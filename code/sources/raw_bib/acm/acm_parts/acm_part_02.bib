@article{10.14778/3401960.3401970,
author = {Kim, Hyeonji and So, Byeong-Hoon and Han, Wook-Shin and Lee, Hongrae},
title = {Natural language to SQL: where are we today?},
year = {2020},
issue_date = {June 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3401960.3401970},
doi = {10.14778/3401960.3401970},
abstract = {Translating natural language to SQL (NL2SQL) has received extensive attention lately, especially with the recent success of deep learning technologies. However, despite the large number of studies, we do not have a thorough understanding of how good existing techniques really are and how much is applicable to real-world situations. A key difficulty is that different studies are based on different datasets, which often have their own limitations and assumptions that are implicitly hidden in the context or datasets. Moreover, a couple of evaluation metrics are commonly employed but they are rather simplistic and do not properly depict the accuracy of results, as will be shown in our experiments. To provide a holistic view of NL2SQL technologies and access current advancements, we perform extensive experiments under our unified framework using eleven of recent techniques over 10+ benchmarks including a new benchmark (WTQ) and TPC-H. We provide a comprehensive survey of recent NL2SQL methods, introducing a taxonomy of them. We reveal major assumptions of the methods and classify translation errors through extensive experiments. We also provide a practical tool for validation by using existing, mature database technologies such as query rewrite and database testing. We then suggest future research directions so that the translation can be used in practice.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {1737–1750},
numpages = {14}
}

@inproceedings{10.1145/3555776.3577862,
author = {Rollo, Federica and Po, Laura and Castellucci, Alessandro},
title = {CEM: an Ontology for Crime Events in Newspaper Articles},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577862},
doi = {10.1145/3555776.3577862},
abstract = {The adoption of semantic technologies for the representation of crime events can help law enforcement agencies (LEAs) in crime prevention and investigation. Moreover, online newspapers and social networks are valuable sources for crime intelligence gathering. In this paper, we propose a new lightweight ontology to model crime events as they are usually described in online news articles. The Crime Event Model (CEM) can integrate specific data about crimes, i.e., where and when they occurred, who is involved (author, victim, and other subjects involved), which is the reason for the occurrence, and details about the source of information (e.g., the news article). Extracting structured data from multiple online sources and interconnecting them in a Knowledge Graph using CEM allow events relationships extraction, patterns and trends identification, and event recommendation.The CEM ontology is available at https://w3id.org/CEMontology.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1762–1765},
numpages = {4},
keywords = {newspaper, crime analysis, lightweight ontology},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3627915.3627925,
author = {Zhou, Zhenhuan and Chen, Yingyu and Wu, Rujing and Tao, Jing},
title = {Ontology-based Case Representation of Mechatronic Product Eco-design and Development of a Cloud-based Case Library},
year = {2023},
isbn = {9798400700590},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627915.3627925},
doi = {10.1145/3627915.3627925},
abstract = {Product life cycle eco-design is challenging, knowledge-intensive and information dependent. This study aims to develop a case library system for eco-design knowledge management and to facilitate eco-design practice of mechatronic product (i.e. manufacturing equipment, construction machinery, vehicles). An ontology-based representation of mechatronic product life cycle eco-design is proposed, which enables the structuring and standardization of related data and the storage of such data in a computer-processable manner. Descriptive features of eco-design case are defined in accordance to the ontology model, based on which the similarity between eco-design case and case queries is calculated using the nearest neighbour method and case retrieval is realized based on the calculated similarity. With the proposed ontology model, a cloud-based eco-design case library is then developed, which provide the benefits of easy deployment and maintenance, and better accessibility.},
booktitle = {Proceedings of the 7th International Conference on Computer Science and Application Engineering},
articleno = {49},
numpages = {7},
keywords = {Cloud-based, Life Cycle Eco-Design, Ontology, Web Service},
location = {Virtual Event, China},
series = {CSAE '23}
}

@inproceedings{10.1145/3411764.3445645,
author = {Wang, Qiaosi and Saha, Koustuv and Gregori, Eric and Joyner, David and Goel, Ashok},
title = {Towards Mutual Theory of Mind in Human-AI Interaction: How Language Reflects What Students Perceive About a Virtual Teaching Assistant},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445645},
doi = {10.1145/3411764.3445645},
abstract = {Building conversational agents that can conduct natural and prolonged conversations has been a major technical and design challenge, especially for community-facing conversational agents. We posit Mutual Theory of Mind as a theoretical framework to design for natural long-term human-AI interactions. From this perspective, we explore a community’s perception of a question-answering conversational agent through self-reported surveys and computational linguistic approach in the context of online education. We first examine long-term temporal changes in students’ perception of Jill Watson (JW), a virtual teaching assistant deployed in an online class discussion forum. We then explore the feasibility of inferring students’ perceptions of JW through linguistic features extracted from student-JW dialogues. We find that students’ perception of JW’s anthropomorphism and intelligence changed significantly over time. Regression analyses reveal that linguistic verbosity, readability, sentiment, diversity, and adaptability reflect student perception of JW. We discuss implications for building adaptive community-facing conversational agents as long-term companions and designing towards Mutual Theory of Mind in human-AI interaction.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {384},
numpages = {14},
keywords = {theory of mind, online education, online community, language analysis, human-AI interaction, conversational agent},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3615366.3625069,
author = {Oliveira, Luiza Bartels and Araujo, Marco Antonio and Dantas, Mario Antonio},
title = {A case study on the development of an ontology for maintenance services of heavy machinery electronic components},
year = {2023},
isbn = {9798400708442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615366.3625069},
doi = {10.1145/3615366.3625069},
abstract = {Inadequate data organization within a company can result in decreased efficiency, increased costs, and longer delivery times. In the context of an electronic maintenance laboratory servicing a mining company internally, the lack of data organization hampers the conversion of information into actionable knowledge, affecting delivery efficiency. This study aims to address these issues by transforming existing data into structured knowledge using the Methontology and OntoForInfoScience methodologies. The ontology model was validated by both experts and a software plug-in and seeks to offers a systematic representation of the domain, facilitating data understanding and utilization, ultimately leading to cost and time savings in deliveries and ongoing process improvements.},
booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing},
pages = {188–191},
numpages = {4},
keywords = {semantic data, ontology, mining, maintenance service., maintenance data, Knowledge acquisition},
location = {La Paz, Bolivia},
series = {LADC '23}
}

@inproceedings{10.1145/3330431.3330449,
author = {Zulfiya, Kaderkeyeva and Gulmira, Bekmanova and Altynbek, Sharipbay},
title = {Ontological model for student's knowledge assessment},
year = {2019},
isbn = {9781450372121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330431.3330449},
doi = {10.1145/3330431.3330449},
abstract = {This article considers the ontological model for knowledge representation on the example of the discipline "Database Theory".Depending on the chosen intelligent system development environment, knowledge must be represented by certain data structures. In this paper, the ontological model is chosen as a model for knowledge representation.},
booktitle = {Proceedings of the 5th International Conference on Engineering and MIS},
articleno = {18},
numpages = {5},
keywords = {sets, ontology, logic, knowledge models, knowledge base, knowledge, artificial intelligence},
location = {Astana, Kazakhstan},
series = {ICEMIS '19}
}

@inproceedings{10.1145/3371158.3371198,
author = {Joshi, Salil Rajeev and Venkatesh, Bharath and Thomas, Dawn and Jiao, Yue and Roy, Shourya},
title = {A Natural Language and Interactive End-to-End Querying and Reporting System},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371198},
doi = {10.1145/3371158.3371198},
abstract = {Natural language query understanding for unstructured textual sources has seen significant progress over the last couple of decades. For structured data, while the ecosystem has evolved with regard to data storage and retrieval mechanisms, the query language has remained predominantly SQL (or SQL-like). Towards making the latter more natural there has been recent research emphasis on Natural Language Interface to DataBases (NLIDB) systems. Piggybacking on the rise of 'deep learning' systems, the state-of-the-art NLIDB solutions over large parallel and standard benchmarks (viz, WikiSQL and Spider) primarily rely on attention based sequence-to-sequence models.Building industry grade NLIDB solutions for making big data ecosystem accessible by truly natural and unstructured querying mechanism presents several challenges. These include lack of availability of parallel corpora, diversity in underlying data schema, wide variability in the nature of queries to context and dialog management in interactive systems. In this paper, we present an end-to-end system Query Enterprise Data (QED) towards making enterprise descriptive analytics and reporting easier and natural. We elaborate in detail how we addressed the challenges mentioned above and novel features such as handling incomplete queries in incremental fashion as well as highlight the role of an assistive user interface that provides a better user experience. Finally, we conclude the paper with observations and lessons learnt from the experience of transferring and deploying a research solution to industry grade practical deployment.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {261–267},
numpages = {7},
keywords = {Semantic Parsing, SQL, Natural Language Understanding, NLIDB, NL2SQL, Information Retrieval},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@inproceedings{10.1145/3357236.3395489,
author = {Papenmeier, Andrea and Sliwa, Alfred and Kern, Dagmar and Hienert, Daniel and Aker, Ahmet and Fuhr, Norbert},
title = { 'A Modern Up-To-Date Laptop' - Vagueness in Natural Language Queries for Product Search},
year = {2020},
isbn = {9781450369749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357236.3395489},
doi = {10.1145/3357236.3395489},
abstract = {With the rise of voice assistants and an increase in mobile search usage, natural language has become an important query language. So far, most of the current systems are not able to process these queries because of the vagueness and ambiguity in natural language. Users have adapted their query formulation to what they think the search engine is capable of, which adds to their cognitive burden. With our research, we contribute to the design of interactive search systems by investigating the genuine information need in a product search scenario. In a crowd-sourcing experiment, we collected 132 information needs in natural language. We examine the vagueness of the formulations and their match to retailer-generated content and user-generated product reviews. Our findings reveal high variance on the level of vagueness and the potential of user reviews as a source for supporting users with rather vague search intents.},
booktitle = {Proceedings of the 2020 ACM Designing Interactive Systems Conference},
pages = {2077–2089},
numpages = {13},
keywords = {vagueness, query formulation, natural language, information retrieval, information need},
location = {Eindhoven, Netherlands},
series = {DIS '20}
}

@article{10.1145/3604561,
author = {Hu, Hengyi and Kerschberg, Larry},
title = {Improving Causal Bayesian Networks Using Expertise in Authoritative Medical Ontologies},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3604561},
doi = {10.1145/3604561},
abstract = {Discovering causal relationships among symptoms is a topical issue in the analysis of observational patient datasets. A Causal Bayesian Network (CBN) is a popular analytical framework for causal inference. While there are many methods and algorithms capable of learning a Bayesian network, they are reliant on the complexity and thoroughness of the algorithm and do not consider prior expertise from authoritative sources. This article proposes a novel method of extracting prior causal knowledge contained in Authoritative Medical Ontologies (AMOs) and using this prior knowledge to orient arcs in a CBN learned from observational patient data. Since AMOs are robust biomedical ontologies containing the collective knowledge of the experts who created them, utilizing the ordering information contained within them produces improved CBNs that provide additional insight into the disease domain.To demonstrate our method, we obtained prior causal ordering information among symptoms from three AMOs: (1) the Medical Dictionary for Regulatory Activities Terminology (MedDRA), (2) the International Classification of Diseases Version 10 Clinical Modification (ICD-10-CM), and (3) Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT). The prior ontological knowledge from these three AMOs is then used to orient arcs in a series of CBNs learned from the National Institutes of Mental Health study on Sequenced Treatment Alternatives to Relieve Depression (STAR*D) patient dataset using the Max-Min Hill-Climbing (MMHC) algorithm. Six distinct CBNs are generated using MMHC: an unmodified baseline model using only the algorithm, three CBNs oriented with ordered-variable pairs from MedDRA, ICD-10-CM, and SNOMED CT, and two more with ordered pairs from a combination of these AMOs. The resulting CBNs modified using ordered-variable pairs significantly change the structure of the network. The agreement between the Modified networks and the Baseline ranges from 50\% to 90\%. A modified network using ordering information from all ontologies obtained an agreement of 50\% (10 out of 20 arcs exist in both the Baseline and Modified models) while maintaining comparable predictive accuracy. This indicates that the Modified CBN reflects the causal claims in the AMOs and agrees with both the AMOs and the observational STAR*D dataset. Furthermore, the Modified models discovered new potentially causal relationships among symptoms in the model, while eliminating weaker edges in a qualitative analysis of the significance of these relationships in existing epidemiological research.},
journal = {ACM Trans. Comput. Healthcare},
month = oct,
articleno = {20},
numpages = {32},
keywords = {ontology evolution, ontology, healthcare information technology, healthcare data, causality, causal networks, causal inference, Bayesian networks, data management, data mining, Patient data}
}

@inproceedings{10.1145/3666015.3666016,
author = {Boudjemila, Chahrazed and Dagnat, Fabien and Mart\'{\i}nez, Salvador},
title = {Maintaining Security Consistency During System Development with Security-Oriented Model Federation},
year = {2024},
isbn = {9798400709913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3666015.3666016},
doi = {10.1145/3666015.3666016},
abstract = {Multi-modeling is an approach within the MDE realm that promotes the development of complex systems by decomposing them in sets of heterogeneous models. These models are defined using different modeling languages and constructed using diverse tools. They represent different but often interdependent views. However, the models of a system are far from being static. They change to accommodate new requirements, functionality improvements, bug fixes, and other evolution events. These changes represent a challenge w.r.t. consistency. This is especially true in security-critical scenarios. Indeed, security information is often integrated within the systems models so that security requirements are met following what is called "security-by-design". In such scenarios, the security concern of the systems models must remain consistent across changes so that security properties continue to hold. In order to tackle this problem, we propose a methodology to enhance the (multi)model-based design phase of a system development process. It comprises the creation of a security federation in which security dependencies between the different models are reified and equipped with security rules expressing security consistency requirements. Then, whenever a model is changed, the security rules are evaluated to monitor the consistency of security across the system models. We evaluate the capabilities of this methodology by a prototype implementation and its application to different use cases.},
booktitle = {Proceedings of the 2024 International Conference on Software and Systems Processes},
pages = {66–76},
numpages = {11},
keywords = {Model-driven engineering, model evolution., model federation, security by design},
location = {M\, Germany},
series = {ICSSP '24}
}

@inproceedings{10.1145/3366030.3366070,
author = {Shaaban, Abdelkader Magdy and Schmittner, Christoph and Gruber, Thomas and Mohamed, A. Baith and Quirchmayr, Gerald and Schikuta, Erich},
title = {Ontology-Based Model for Automotive Security Verification and Validation},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366070},
doi = {10.1145/3366030.3366070},
abstract = {Modern automobiles are considered semi-autonomous vehicles regarding new adaptive technologies. New cars consist of a vast number of electronic units for managing and controlling the functional safety in a vehicle. In the vehicular industry, safety and security are considered two sides for the same coin. Therefore, improving functional safety in the vehicular industry is essential to protect the vehicle from different attack scenarios. This work introduces an ontology-based model for security verification and validation in the vehicular domain. The model performs a series of logical quires and inference rules to ensure that the security requirements are fulfilled. It endeavors to enhance the current security state of a vehicle by selecting additional security requirements that can handle existence security weaknesses and meet the actual security goal.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {73–82},
numpages = {10},
keywords = {Verification and Validation, Threats, Security Requirements, Protection Profile, Ontology, Automotive},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3194104.3194105,
author = {Weigelt, Sebastian and Hey, Tobias and Landh\"{a}u\ss{}er, Mathias},
title = {Integrating a dialog component into a framework for spoken language understanding},
year = {2018},
isbn = {9781450357234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194104.3194105},
doi = {10.1145/3194104.3194105},
abstract = {Spoken language interfaces are the latest trend in human computer interaction. Users enjoy the newly found freedom but developers face an unfamiliar and daunting task. Creating reactive spoken language interfaces requires skills in natural language processing. We show how a developer can integrate a dialog component in a natural language processing system by means of software engineering methods. Our research project PARSE that aims at naturalistic end-user programming in spoken natural language serves as an example. We integrate a dialog component with PARSE without affecting its other components: We modularize the dialog management and introduce dialog acts that bundle a trigger for the dialog and the reaction of the system. We implemented three dialog acts to address the following issues: speech recognition uncertainties, coreference ambiguities, and incomplete conditionals.We conducted a user study with ten subjects to evaluate our approach. The dialog component achieved resolution rates from 23\% to 50\% (depending on the dialog act) and introduces a negligible number of errors. We expect the overall performance to increase even further with the implementation of additional dialog acts.},
booktitle = {Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {1–7},
numpages = {7},
keywords = {programming in natural language, naturalistic programming, natural language processing for software engineering, knowledge-based software engineering, human-computer interaction, enduser programming, dialog systems, dialog integration},
location = {Gothenburg, Sweden},
series = {RAISE '18}
}

@inproceedings{10.1145/3706598.3713789,
author = {Velloso, Eduardo and Hornb\ae{}k, Kasper},
title = {Theorising in HCI using Causal Models},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713789},
doi = {10.1145/3706598.3713789},
abstract = {Although the literature on Human-Computer Interaction (HCI) catalogues many theories, it offers surprisingly few tools for theorising. This paper critiques dominant approaches to engaging with theory and proposes a working model for theorising in HCI. We then present graphical causal modelling as an effective theorising tool. This includes a step-by-step guide to building causal models and examples of their use in different stages of the research process. We explain how causal models help develop method-agnostic representations of research problems using directed acyclic graphs, identify potential confounders, and construct alternative interpretations of data. Finally, we discuss their limitations and challenges for adoption by the HCI community.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {484},
numpages = {17},
keywords = {Causal modelling, HCI theory, directed acyclic graphs},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3314221.3314594,
author = {Campagna, Giovanni and Xu, Silei and Moradshahi, Mehrad and Socher, Richard and Lam, Monica S.},
title = {Genie: a generator of natural language semantic parsers for virtual assistant commands},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314594},
doi = {10.1145/3314221.3314594},
abstract = {To understand diverse natural language commands, virtual assistants today are trained with numerous labor-intensive, manually annotated sentences. This paper presents a methodology and the Genie toolkit that can handle new compound commands with significantly less manual effort. We advocate formalizing the capability of virtual assistants with a Virtual Assistant Programming Language (VAPL) and using a neural semantic parser to translate natural language into VAPL code. Genie needs only a small realistic set of input sentences for validating the neural model. Developers write templates to synthesize data; Genie uses crowdsourced paraphrases and data augmentation, along with the synthesized data, to train a semantic parser. We also propose design principles that make VAPL languages amenable to natural language translation. We apply these principles to revise ThingTalk, the language used by the Almond virtual assistant. We use Genie to build the first semantic parser that can support compound virtual assistants commands with unquoted free-form parameters. Genie achieves a 62\% accuracy on realistic user inputs. We demonstrate Genie’s generality by showing a 19\% and 31\% improvement over the previous state of the art on a music skill, aggregate functions, and access control.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {394–410},
numpages = {17},
keywords = {virtual assistants, training data generation, semantic parsing, data engineering, data augmentation},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3472301.3484327,
author = {Castro, Murillo V. H. B. and Barcellos, Monalessa P. and de A. Falbo, Ricardo and Costa, Simone D.},
title = {Using Ontologies to aid Knowledge Sharing in HCI Design},
year = {2021},
isbn = {9781450386173},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472301.3484327},
doi = {10.1145/3472301.3484327},
abstract = {Developing interactive systems is a challenging task that involves concerns related to the human-computer interaction (HCI), such as usability and user experience. Therefore, HCI design is a core issue when developing such systems. It often involves people with different backgrounds (e.g., Arts, Software Engineering, Design), which makes knowledge transfer a challenging issue. Ontologies have been acknowledged as a successful approach to represent domain knowledge and support knowledge-based solutions. Hence, in this work, we propose to explore ontologies to represent structured knowledge and improve knowledge sharing in HCI design. We briefly present the Human-Computer Interaction Design Ontology (HCIDO), a reference ontology that addresses HCI design aspects that connect HCI and Software Engineering concerns. By making knowledge related to the HCI design domain explicit and structured, HCIDO has helped us to develop KTID, a tool that aims to support capturing and sharing useful knowledge to aid in HCI design. Preliminary results indicate that the tool may be particularly useful for novice HCI designers.},
booktitle = {Proceedings of the XX Brazilian Symposium on Human Factors in Computing Systems},
articleno = {50},
numpages = {7},
keywords = {Ontology, Knowledge, HCI Design},
location = {Virtual Event, Brazil},
series = {IHC '21}
}

@article{10.1145/3729530,
author = {Pradeep, Anagha and Mamidi, Radhika},
title = {Sandar\'{s}ana: A Survey on Sanskrit Computational Linguistics and Digital Infrastructure for Sanskrit},
year = {2025},
issue_date = {October 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {10},
issn = {0360-0300},
url = {https://doi.org/10.1145/3729530},
doi = {10.1145/3729530},
abstract = {Computational Linguistics is an interdisciplinary field of computer science and linguistics that focuses on designing computational models and algorithms for processing, analyzing, and generating human language. Over recent years, this field has made substantial progress. While its primary emphasis tends to center around widely spoken languages, there is equal importance in investigating languages that are not commonly spoken but have contributed immensely to the literature, culture, and philosophy of the society. Thus, this survey article comprehensively delves into the exploration of computational tasks undertaken for Sanskrit, an ancient language of the Indian sub-continent steeped in a wealth of literary heritage. The purpose of this study is to provide an overview of the progress made thus far in the computational analysis of Sanskrit, while also reviewing the current digital infrastructure that supports these efforts. Additionally, our study also identifies potential avenues for future research, serving as a reference for anyone interested in advancing their exploration in this field.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {254},
numpages = {38},
keywords = {Sanskrit computational linguistics, Pundefinedundefinedini, Aundefinedundefinedundefineddhyundefinedyundefined}
}

@inproceedings{10.1145/3652620.3687791,
author = {Shaked, Avi and Messe, Nan and Melham, Tom},
title = {Modelling Tool Extension for Vulnerability Management},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687791},
doi = {10.1145/3652620.3687791},
abstract = {Managing vulnerabilities with respect to the design of systems is essential to securing systems and establishing their trustworthiness. Until now, there has been no modelling tool to support vulnerability management within the context of system design. We present a new, open-source extension of a systems security design and assessment tool. First and foremost, this extension integrates a pertinent vulnerability management domain ontology into the tool's underlying metamodel. Based on the extended metamodel, the enriched tool supports importing information from vulnerability-related knowledge bases as well as capturing new vulnerability information and security rules. This information can then be used in an integrative and scalable form to analyse and reason about the security of systems designs. The extended tool now includes an automated reasoning mechanism for establishing the vulnerability posture of systems designs.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {56–60},
numpages = {5},
keywords = {model driven engineering, threat modelling, vulnerability management, security by design},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3330431.3330438,
author = {Suleimenova, Laura and Zhomartkyzy, Gulnaz and Kumargazhanova, Saule},
title = {Integration data models based on ontology},
year = {2019},
isbn = {9781450372121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330431.3330438},
doi = {10.1145/3330431.3330438},
abstract = {Evaluation of the performance of university staff can be carried out with different goals, consistent with the strategic goals of the institution. To provide an appropriate data set, an appropriate structure must be provided that ensures that neither incomplete nor reliable data will be used. To provide such a structure with the best possible features data from the various available sources should be integrated. The article provides an overview of the existing integration problems and approaches to solving this problem.},
booktitle = {Proceedings of the 5th International Conference on Engineering and MIS},
articleno = {7},
numpages = {5},
keywords = {ontology, ontological model, monitoring, information system, data integration},
location = {Astana, Kazakhstan},
series = {ICEMIS '19}
}

@inproceedings{10.1145/3704268.3748680,
author = {Barron, Ryan C. and Eren, Maksim E. and Stanev, Valentin and Matuszek, Cynthia and Alexandrov, Boian S.},
title = {Topic Modeling and Link-Prediction for Material Property Discovery},
year = {2025},
isbn = {9798400713514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704268.3748680},
doi = {10.1145/3704268.3748680},
abstract = {Link prediction is a key network analysis technique that infers missing or future relations between nodes in a graph, based on observed patterns of connectivity. Scientific literature networks and knowledge graphs are typically large, sparse, and noisy, and often contain missing links, potential but unobserved connections, between concepts, entities, or methods. Here, we present an AI-driven hierarchical link prediction framework that integrates matrix factorization to infer hidden associations and steer discovery in complex material domains. Our method combines Hierarchical Nonnegative Matrix Factorization (HNMFk), Boolean matrix factorization (BNMFk) with automatic model selection. These discrete factors are then fused with Logistic matrix factorization (LMF), we use to construct a three-level topic tree from a 46,862-document corpus focused on 73 transition-metal dichalcogenides (TMDs). This class of materials has been studied in a variety of physics fields and has a multitude of current and potential applications.An ensemble BNMFk + LMF approach fuses discrete interpretability with probabilistic scoring. The resulting HNMFk clusters map each material onto coherent research themes, such as superconductivity, energy storage, and tribology, and highlight missing or weakly connected links between topics and materials, suggesting novel hypotheses for cross-disciplinary exploration. We validate our method by removing publications about superconductivity in well-known superconductors, and demonstrate that the model correctly predicts their association with the superconducting TMD clusters. This highlights the ability of the method to find hidden connections in a graph of material to latent topic associations built from scientific literature. This is especially useful when examining a diverse corpus of scientific documents covering the same class of phenomena or materials but originating from distinct communities and perspectives. The inferred links generating new hypotheses, produced by our method, are exposed through an interactive Streamlit dashboard, designed for scientific discovery.},
booktitle = {Proceedings of the 2025 ACM Symposium on Document Engineering},
articleno = {10},
numpages = {4},
keywords = {LMF, Link Prediction, Matrix completion, NMF, NMFk},
location = {Nottingham, United Kingdom},
series = {DocEng '25}
}

@inproceedings{10.1145/3468791.3469119,
author = {Sima, Ana Claudia and Mendes de Farias, Tarcisio and Anisimova, Maria and Dessimoz, Christophe and Robinson-Rechavi, Marc and Zbinden, Erich and Stockinger, Kurt},
title = {Bio-SODA: Enabling Natural Language Question Answering over Knowledge Graphs without Training Data},
year = {2021},
isbn = {9781450384131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468791.3469119},
doi = {10.1145/3468791.3469119},
abstract = {The problem of natural language processing over structured data has become a growing research field, both within the relational database and the Semantic Web community, with significant efforts involved in question answering over knowledge graphs (KGQA). However, many of these approaches are either specifically targeted at open-domain question answering using DBpedia, or require large training datasets to translate a natural language question to SPARQL in order to query the knowledge graph. Hence, these approaches often cannot be applied directly to complex scientific datasets where no prior training data is available. In this paper, we focus on the challenges of natural language processing over knowledge graphs of scientific datasets. In particular, we introduce Bio-SODA, a natural language processing engine that does not require training data in the form of question-answer pairs for generating SPARQL queries. Bio-SODA uses a generic graph-based approach for translating user questions to a ranked list of SPARQL candidate queries. Furthermore, Bio-SODA uses a novel ranking algorithm that includes node centrality as a measure of relevance for selecting the best SPARQL candidate query. Our experiments with real-world datasets across several scientific domains, including the official bioinformatics Question Answering over Linked Data (QALD) challenge, as well as the CORDIS dataset of European projects, show that Bio-SODA outperforms publicly available KGQA systems by an F1-score of least 20\% and by an even higher factor on more complex bioinformatics datasets.},
booktitle = {Proceedings of the 33rd International Conference on Scientific and Statistical Database Management},
pages = {61–72},
numpages = {12},
keywords = {Ranking, Question Answering, Knowledge Graphs},
location = {Tampa, FL, USA},
series = {SSDBM '21}
}

@article{10.1145/3747585,
author = {Rost, Mattias},
title = {Reclaiming the Computer through LLM-Mediated Computing},
year = {2025},
issue_date = {September - October 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1072-5520},
url = {https://doi.org/10.1145/3747585},
doi = {10.1145/3747585},
journal = {Interactions},
month = aug,
pages = {26–31},
numpages = {6}
}

@inproceedings{10.1145/3485447.3511921,
author = {Ye, Hongbin and Zhang, Ningyu and Deng, Shumin and Chen, Xiang and Chen, Hui and Xiong, Feiyu and Chen, Xi and Chen, Huajun},
title = {Ontology-enhanced Prompt-tuning for Few-shot Learning},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511921},
doi = {10.1145/3485447.3511921},
abstract = {Few-shot Learning (FSL) is aimed to make predictions based on a limited number of samples. Structured data such as knowledge graphs and ontology libraries has been leveraged to benefit the few-shot setting in various tasks. However, the priors adopted by the existing methods suffer from challenging knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder the performance for few-shot learning. In this study, we explore knowledge injection for FSL with pre-trained language models and propose ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the ontology transformation based on the external knowledge graph to address the knowledge missing issue, which fulfills and converts structure knowledge to text. We further introduce span-sensitive knowledge injection via a visible matrix to select informative knowledge to handle the knowledge noise issue. To bridge the gap between knowledge and text, we propose a collective training algorithm to optimize representations jointly. We evaluate our proposed OntoPrompt in three tasks, including relation extraction, event extraction, and knowledge graph completion, with eight datasets. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {778–787},
numpages = {10},
keywords = {Event Extraction, Few-shot Learning, Knowledge Graph Completion, Ontology, Prompt-tuning, Relation Extraction},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1145/3631976,
author = {Cederbladh, Johan and Cicchetti, Antonio and Suryadevara, Jagadish},
title = {Early Validation and Verification of System Behaviour in Model-based Systems Engineering: A Systematic Literature Review},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3631976},
doi = {10.1145/3631976},
abstract = {In the Systems Engineering (SE) domain there has been a paradigm shift from document-based to model-based system development artefacts; in fact, new methodologies are emerging to meet the increasing complexity of current systems and the corresponding growing need of digital workflows. In this regard, Model-Based Systems Engineering (MBSE) is considered as a key enabler by many central players of the SE community. MBSE has reached an adequate level of maturity, and there exist documented success stories in its adoption in industry. In particular, one significant benefit of utilising MBSE when compared to the traditional manual and document-centric workflows is that models are available from early phases of systems development; these enable a multitude of analyses prior any implementation effort together with other relevant capabilities, like the automation of development tasks. Nonetheless, it is noticeable there is a lack of a common understanding for how formal analyses for the verification and validation (V&amp;V) of systems behaviour, specifically in the early phases of development, could be placed in an MBSE setting.In this article, we report on the planning, execution, and results of a systematic literature review regarding the early V&amp;V of systems behaviour in the context of model-based systems engineering. The review aims to provide a structured representation of the state of the art with respect to motivations, proposed solutions, and limitations. From an initial set of potentially relevant 701 peer-reviewed publications we selected 149 primary studies, which we analysed according to a rigorous data extraction, analysis, and synthesis process. Based on our results, early V&amp;V has usually the goal of checking the quality of a system design to avoid discovering flaws when parts are being concretely realised; SysML is a de facto standard for describing the system under study, while the solutions for the analyses tend to be varied; also V&amp;V analyses tend to target varied properties with a slight predominance of functional concerns, and following the variation mentioned so far the proposed solutions are largely context specific; the proposed approaches are usually presented without explicit limitations, while when limitations are discussed, readiness of the solutions, handling of analyses simplifications/assumptions, and languages/tools integration are among the most frequently mentioned issues.Based on the survey results and the standard SE practices, we discuss how the current state-of-the-art MBSE supports early V&amp;V of systems behaviour with a special focus on industrial adoption and identify relevant challenges to be researched further.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {81},
numpages = {67},
keywords = {MBSE, validation, verification, system behaviour, systematic literature review}
}

@inproceedings{10.1145/3708319.3733654,
author = {Ferilli, Stefano},
title = {Knowledge Graph-based User Models and Personalized Access for Cultural Heritage},
year = {2025},
isbn = {9798400713996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708319.3733654},
doi = {10.1145/3708319.3733654},
abstract = {Cultural Heritage is opening up from the professional community to a wider public, generating an increasing demand for culture and an associated economic turnaround. This step requires to differentiate the behavior of Cultural Heritage systems, dealing with a wide variety of backgrounds, expectations, contexts, aims, educational and cultural level, preferences and interests. Computer Science and Artificial Intelligence can play a key role in this landscape, fine-tuning the fruition of cultural items to every kind of stakeholder and even to single users. In this paper we present an approach to personalization of Cultural Heritage fruition based on Knowledge Graphs. An approach to describe user models, and to use them for extracting personalized information, is proposed, and a platform that embeds this approach is described.},
booktitle = {Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {437–441},
numpages = {5},
keywords = {User Models, Personalized Information Access, Knowledge Graphs, Cultural Heritage},
location = {
},
series = {UMAP Adjunct '25}
}

@inproceedings{10.1145/3357223.3362701,
author = {Dogga, Pradeep and Narasimhan, Karthik and Sivaraman, Anirudh and Netravali, Ravi},
title = {A System-Wide Debugging Assistant Powered by Natural Language Processing},
year = {2019},
isbn = {9781450369732},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357223.3362701},
doi = {10.1145/3357223.3362701},
abstract = {Despite advances in debugging tools, systems debugging today remains largely manual. A developer typically follows an iterative and time-consuming process to move from a reported bug to a bug fix. This is because developers are still responsible for making sense of system-wide semantics, bridging together outputs and features from existing debugging tools, and extracting information from many diverse data sources (e.g., bug reports, source code, comments, documentation, and execution traces). We believe that the latest statistical natural language processing (NLP) techniques can help automatically analyze these data sources and significantly improve the systems debugging experience. We present early results to highlight the promise of NLP-powered debugging, and discuss systems and learning challenges that must be overcome to realize this vision.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {171–177},
numpages = {7},
keywords = {systems debugging, natural language processing},
location = {Santa Cruz, CA, USA},
series = {SoCC '19}
}

@inproceedings{10.1145/3715336.3735832,
author = {Calderwood, Alex and Chung, John Joon Young and Sun, Yuqian and Roemmele, Melissa and Kreminski, Max},
title = {Phraselette: A Poet’s Procedural Palette},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735832},
doi = {10.1145/3715336.3735832},
abstract = {According to the recently introduced theory of artistic support tools, creativity support tools exert normative influences over artistic production, instantiating a normative ground that shapes both the process and product of artistic expression. We argue that the normative ground of most existing automated writing tools is misaligned with writerly values and identify a potential alternative frame—material writing support—for experimental poetry tools that flexibly support the finding, processing, transforming, and shaping of text(s). Based on this frame, we introduce Phraselette, an artistic material writing support interface that helps experimental poets search for words and phrases. To provide material writing support, Phraselette is designed to counter the dominant mode of automated writing tools, while offering language model affordances in line with writerly values. We further report on an extended expert evaluation involving 10 published poets that indicates support for both our framing of material writing support and for Phraselette itself.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {2701–2717},
numpages = {17},
keywords = {Creative Writing, Language Models, Poetry, Artistic Support Tool, CST, Material, Search},
location = {
},
series = {DIS '25}
}

@inproceedings{10.1145/3332165.3347899,
author = {Li, Toby Jia-Jun and Radensky, Marissa and Jia, Justin and Singarajah, Kirielle and Mitchell, Tom M. and Myers, Brad A.},
title = {PUMICE: A Multi-Modal Agent that Learns Concepts and Conditionals from Natural Language and Demonstrations},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332165.3347899},
doi = {10.1145/3332165.3347899},
abstract = {Natural language programming is a promising approach to enable end users to instruct new tasks for intelligent agents. However, our formative study found that end users would often use unclear, ambiguous or vague concepts when naturally instructing tasks in natural language, especially when specifying conditionals. Existing systems have limited support for letting the user teach agents new concepts or explaining unclear concepts. In this paper, we describe a new multi-modal domain-independent approach that combines natural language programming and programming-by-demonstration to allow users to first naturally describe tasks and associated conditions at a high level, and then collaborate with the agent to recursively resolve any ambiguities or vagueness through conversations and demonstrations. Users can also define new procedures and concepts by demonstrating and referring to contents within GUIs of existing mobile apps. We demonstrate this approach in PUMICE, an end-user programmable agent that implements this approach. A lab study with 10 users showed its usability.},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {577–589},
numpages = {13},
keywords = {programming by demonstration, natural language programming, multi-modal interaction, end user development},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{10.1145/3360901.3364448,
author = {Szekely, Pedro and Garijo, Daniel and Bhatia, Divij and Wu, Jiasheng and Yao, Yixiang and Pujara, Jay},
title = {T2WML: Table To Wikidata Mapping Language},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364448},
doi = {10.1145/3360901.3364448},
abstract = {The web contains millions of useful spreadsheets and CSV files, but these files are difficult to use in applications because they use a wide variety of data layouts and terminology. We present Table To Wikidata Mapping Language (T2WML), a language that makes it easy to map and link arbitrary spreadsheets and CSV files to the Wikidata data model. The output of T2WML consists of Wikidata statements that can be loaded in the public Wikidata knowledge base or in a Wikidata clone repository, creating an augmented Wikidata knowledge graph that application developers can query using SPARQL.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {267–270},
numpages = {4},
keywords = {wikidata, rdf, knowledge graphs, entity linking},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1145/3430984.3431002,
author = {Sen, Jaydeep and Saha, Diptikalyan and Mittal, Ashish and Sankaranarayanan, Karthik},
title = {Optimizing Interpretation Generation in Natural Language Query Answering for Real Time End Users},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431002},
doi = {10.1145/3430984.3431002},
abstract = {Natural Language Querying over Database is gaining popularity across different use cases. Most common of them is to democratize the process of data analysis and querying of backend data to naive end users especially business users, obviating the need of knowing back end query language. Natural Language Query answering systems have thus seen widespread usage in industry too where business users want to search their own data to make business decisions. However, a common challenge faced by any natural language query answering system is generation of precise interpretations. The research community although tries to handle the problem via asking clarification questions back to the user, in industry setup this remains an ineffective solution due to various practical usage limitations. For example, it is not fair to assume any end user will be aware of the correct option to answer these clarification questions. Moreover, involving clarification questions and user feedbacks makes the system unusable by one shot API calls, which is the most intuitive usage among common use cases in industry like automated report generation. In this paper, we investigate practical ways to address the problem of precise interpretation generation. We propose novel algorithms to make use of existing technologies like Functional Partitioning of Ontology and Lazy Inclusion to solve this problem. We take our previous state-of-the-art paper ATHENA and further extend it to include our proposed methods. We test with 3 benchmark ontologies to empirically demonstrate the huge improvement over state-of-the-art results by factors of at least 400\% in number of interpretation generation and also in the computation time.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science \&amp; Management of Data (8th ACM IKDD CODS \&amp; 26th COMAD)},
pages = {341–349},
numpages = {9},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.1145/3550356.3561548,
author = {Medinacelli, Luis Palacios and Noyrit, Florian and Mraidha, Chokri},
title = {Augmenting model-based systems engineering with knowledge},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561548},
doi = {10.1145/3550356.3561548},
abstract = {This article presents a general approach for the integration of Knowledge Bases into Model-Based Systems Engineering tools. In existing tools, domain-specific modeling languages are well supported. However when it comes to enforcing design constraints, existing approaches are verbose, it is difficult to be complete and consistent, and the reuse of knowledge is only possible in a limited way (mainly through model libraries). Furthermore, current tools usually lack or have limited capability to detect semantic errors, ability to evaluate the models with respect to formal expert knowledge, and the ability to understand what is being designed. Our work addresses these limitations through the semantic annotation of UML models in Papyrus (an MBSE Tool), to attach domain-specific semantics to the models. This integration enables not only reasoning capabilities over the annotated models, but the models can be shared with semantic-compatible tools and stakeholders. Moreover, the models can reuse and integrate knowledge generated outside the tooling environment. The approach's feasibility is demonstrated through an implementation that defines a technology stack, with emphasis on the mapping of UML elements and its counterparts in the ontology. We address the coherence and preservation of the semantics throughout the transformation process, which enable the formalization of constraints coming from the UML's system design. Finally, we illustrate the reasoning capabilities by evaluating expert knowledge via SPARQL queries and SWRL rules.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {351–358},
numpages = {8},
keywords = {semantic interoperability, ontology, model-driven engineering, knowledge based engineering, UML, Papyrus},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3679318.3685501,
author = {Krapp, Eva and Neuhaus, Robin and Hassenzahl, Marc and Laschke, Matthias},
title = {In a Quasi-Social Relationship With ChatGPT. An Autoethnography on Engaging With Prompt-Engineered LLM Personas},
year = {2024},
isbn = {9798400709661},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3679318.3685501},
doi = {10.1145/3679318.3685501},
abstract = {As conversational AI like ChatGPT becomes more sophisticated, understanding emerging quasi-social relationships with it is crucial. Through analytical autoethnography, we explore the nuances of these relationships with two autobiographically designed ChatGPT personas to augment the social needs of the first author: the Endless Enthusiast (always responding positively and encouragingly) and the Socratic Tutor (asking questions to stimulate critical thinking). After six weeks of interaction, we find that for a successful relationship, the non-human counterpart must be authentic about its machine nature and limitations. Using deception to appear more human-like makes the relationship fail. We thus suggest designing machine relationships as complementary to human-human relationships. For authentic interactions, humans should be in control, with the machine authentically assuming a role that "naturally" fits machines. Here, the unique qualities of machines in social interactions offer promising starting points for designing such roles.},
booktitle = {Proceedings of the 13th Nordic Conference on Human-Computer Interaction},
articleno = {79},
numpages = {16},
keywords = {autobiographical research through design, autoethnography, human-AI interaction},
location = {Uppsala, Sweden},
series = {NordiCHI '24}
}

@inbook{10.5555/3712729.3712962,
author = {Blas, Mar\'{\i}a Julia and Gonnet, Silvio},
title = {A Modeling Framework for Complex Systems},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {This paper presents a modeling framework for defining abstractions of real-world complex systems promoting the development of discrete-event simulation models based on DEVS. An ontology, a metamodel, and a reasoner are combined in one single structure to allow an upgrade of an abstraction model to an implementation model. Our motivation is to reduce the effort related to the modeling part when specifying DEVS models for complex systems described from an abstraction of reality built over a research question. Applications include an easier introduction to M&amp;S for students of any scientific field that can define an abstraction model with an easier introduction to DEVS models (from formalization to implementation).},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2809–2820},
numpages = {12}
}

@inproceedings{10.1145/3674213.3674222,
author = {Mori, Kosuke and Kondo, Takao and Teraoka, Fumio},
title = {Detecting Inconsistency between Network Design and Current State Based on Network Ontology Bonsai},
year = {2024},
isbn = {9798400709852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674213.3674222},
doi = {10.1145/3674213.3674222},
abstract = {Generally, a network administrator designs, constructs, and operates an enterprise network. Since inconsistency between the network design understood by the administrator and the actual network configuration might arise due to mistakes or errors, a method for automatically detecting such inconsistency is needed. The following four techniques are necessary for this purpose. (i) A machine-readable notation to represent the network configuration. (ii) A tool to write down the network design using the machine-readable notation. (iii) A tool to automatically detect the current network configuration and write it down in the machine-readable notation. (iv) A tool to compare the two outputs generated in (ii) and (iii). This paper employs the network ontology called Bonsai for (i). Bonsai can represent not only physical configurations but also virtualization technologies such as VLAN and overlay. This paper proposes three tools, nc-design, nc-detect, and nc-diff for (ii)-(iv), and confirms that they work as expected in the test network. This paper also measures their fundamental performance.},
booktitle = {Proceedings of the Asian Internet Engineering Conference 2024},
pages = {76–84},
numpages = {9},
keywords = {network configuration detection, network management, network ontology},
location = {Sydney, NSW, Australia},
series = {AINTEC '24}
}

@article{10.1145/3708492,
author = {Wang, Di and Kogan, Marina},
title = {Resonance+: Operationalizing Protective Action Decision Model for Finding Information Useful for Public Information Officers},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3–4},
url = {https://doi.org/10.1145/3708492},
doi = {10.1145/3708492},
abstract = {Microblogging platforms have been increasingly used by the public in crisis situations, enabling more participatory crisis communication between the official response channels and the affected community. However, the sheer volume of crisis-related messages on social media can make it challenging for officials to find pertinent information and understand the public’s perception of evolving risks. To address this issue, crisis informatics researchers have proposed a variety of technological solutions, but there has been limited examination of the cognitive and perceptual processes and subsequent responses of the affected population. Yet, this information is critical for the crisis response officials to gauge public’s understanding of the event, their perception of event-related risk, and perception of incident response and recovery efforts, in turn enabling the officials to craft crisis communication messaging more effectively. Taking cues from the Protective Action Decision Model, we conceptualize a metric (resonance+) that prioritizes the cognitive and perceptual processes of the affected population, quantifying shifts in collective attention and information exposure for each tweet. Based on resonance+, we develop a principled, scalable pipeline that recommends content relating to people’s cognitive and perceptual processes. Our results suggest that resonance+ is generalizable across different types of natural hazards. We have also demonstrated its applicability for near-real-time scenarios. According to the feedback from the target users, the local public information officers in emergency management, the messages recommended by our pipeline are useful in their tasks of understanding public perception and finding hopeful narratives, potentially leading to more effective crisis communications.},
journal = {Trans. Soc. Comput.},
month = feb,
articleno = {6},
numpages = {37},
keywords = {Crisis informatics, social media data, crisis communication, word embedding, protective action decision model}
}

@inproceedings{10.1145/3600100.3623729,
author = {Hwang, Min Young and Akinci, Burcu and Berges, Mario},
title = {FSBrick: An information model for representing fault-symptom relationships in HVAC systems},
year = {2023},
isbn = {9798400702303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600100.3623729},
doi = {10.1145/3600100.3623729},
abstract = {Current fault diagnosis (FD) methods for Heating, Ventilation, and Air Conditioning (HVAC) systems do not accommodate for system reconfigurations throughout the systems’ operational lifetime. However, system reconfiguration can change the causal relationship between faults and symptoms, which leads to a drop in FD accuracy. In this paper, we present Fault-Symptom Brick (FSBrick), an extension to the Brick metadata schema intended to represent information necessary to propagate system configuration changes onto FD algorithms, and ultimately revise FSRs explicitly or implicitly. We motivate the need to represent FSRs by illustrating their changes when the system reconfigures. Then, we survey existing efforts to represent FSRs within the HVAC sector and adjacent fields, and choose to extend Brick. We introduce the FSBrick architecture and discuss which extensions are added to represent FSRs. To evaluate the coverage of FSBrick, we implement FSBrick on (i) the motivational case study scenario, (ii) Building Automation Systems’ representation of FSRs from 3 HVACs, and (iii) FSRs from 7 FD method papers, and find that FSBrick can represent 88.2\% of faults, 87.7\% of fault severities, and 72.5\% of symptoms. The analyses show that both Brick and FSBrick should be expanded further to cover HVAC component and property information, and mathematical and logical statements used to formulate FSRs in real life and literature. As there is currently no generic and extensible information model to represent FSRs in commercial buildings, FSBrick paves the way to future extensions that would aid the automated revision of FD algorithms upon system reconfiguration.},
booktitle = {Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {69–78},
numpages = {10},
keywords = {causal relationship, fault diagnosis, fault-symptom relationships, ontology, semantic information modeling},
location = {Istanbul, Turkey},
series = {BuildSys '23}
}

@article{10.1145/3589230,
author = {Trentin, Mia and Felicetti, Achille},
title = {Between Written and Visual Communication: CIDOC CRM Ontology for Medieval and Early Modern European Graffiti},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3589230},
doi = {10.1145/3589230},
abstract = {The development of graffiti studies during the last couple of decades highlighted the relevance and potential of graffiti as a complementary source for understanding different aspects of past societies. Moreover, the availability of digital documentation techniques crucially increased data production, showing the widespread presence of graffiti in Medieval and Early Modern contexts across Europe.However, the approach to historical graffiti has not been yet structured. Guidelines, specific analytical tools, and descriptors are still missing due to various reasons. First, graffiti are a multiform and multimodal graphic expression, so texts, signs, and images must be considered together despite their different communicative nature. Secondly, due to their variety in forms and contents, graffiti have been studied from many perspectives (e.g., epigraphy, palaeography, history, art history, maritime studies), following the specific interests of each scholar. Consequently, the numerous and extensive contributions concerning graffiti highlight the lack of shared standards and approaches, hindering data analysis and interoperability. The panorama emerging is fragmentary and unstructured.This article thus aims to offer a first step toward the development of a specific methodology for the analysis and study of Medieval and Early Modern European graffiti. Precisely, a specific ontology adopting CIDOC CRM for Medieval and Early Modern graffiti will be presented, as developed in a preliminary form within the DIGIGRAF project1 with the support of the ARIADNEplus2 network.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {55},
numpages = {18},
keywords = {CIDOC CRM, epigraphy, Medieval Graffiti}
}

@article{10.1145/3656468,
author = {Gray, Colin M. and Chivukula, Shruthi Sai and Johns, Janna and Will, Matthew and Obi, Ike and Li, Ziqing},
title = {Languaging Ethics in Technology Practice},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {2},
url = {https://doi.org/10.1145/3656468},
doi = {10.1145/3656468},
abstract = {Ethics as embodied by technology practitioners resists simple definition—particularly as it relates to the interplay of identity, organizational, and professional complexity. In this article, we use the linguistic notion of languaging as an analytic lens to describe how technology and design practitioners negotiate their conception of ethics as they reflect upon their everyday work. We engaged 12 practitioners in individual co-creation workshops, encouraging them to reflect on their ethical role in their everyday work through a series of generative and evaluative activities. We analyzed these data to identify how each practitioner reasoned about ethics through language and artifacts, finding that practitioners used a range of rhetorical tropes to describe their ethical commitments and beliefs in ways that were complex and sometimes contradictory. Across three cases, we describe how ethics was negotiated through language across three key zones of ecological emergence: the practitioners’ “core” beliefs about ethics, internal and external ecological elements that shaped or mediated these core beliefs, and the ultimate boundaries they reported refusing to cross. Building on these findings, we describe how the languaging of ethics reveals opportunities to definitionally and practically engage with ethics in technology ethics research, practice, and education.},
journal = {ACM J. Responsib. Comput.},
month = jun,
articleno = {15},
numpages = {15},
keywords = {Ethics, technology practice, languaging, rhetoric, co-creation}
}

@inproceedings{10.1145/3698300.3698319,
author = {Zhou, Bo and Chen, Ji and Wu, Shuo},
title = {A Knowledge Graph Construction System Based on Digital Elevation Model},
year = {2024},
isbn = {9798400717512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698300.3698319},
doi = {10.1145/3698300.3698319},
abstract = {In recent years, Integration of knowledge graph technology into various domains has become a prominent trend, fostering the emergence of domain-specific knowledge graph tailored to particular fields. Digital Elevation Model (DEM), as fundamental data for terrain analysis, has widespread applications in surveying, hydrology, geomorphology, and Geographic Information Systems (GIS). However, research on constructing knowledge graph based on DEM remains relatively limited. This study presents a system for constructing knowledge graph based on DEM. In this system, valley and ridge lines are delineated and organized into slope units with distinct boundaries and semantic clarity, refined from DEM. These slope units serve as entities, and their spatial relationships form the connections within the knowledge graph. This system features a user-friendly interface, enabling users to intuitively view terrain data and the knowledge graph. Additionally, it supports natural language Question and Answer (Q&amp;A) functionality, allowing users to query terrain information, explore relationships between slope units, and calculate the shortest paths. By providing new perspectives and tools, this system advances the research and application of terrain data. Continuous improvement and expansion are expected to enhance terrain analysis technology and create innovative opportunities in related disciplines.},
booktitle = {Proceedings of the 2024 7th International Conference on Big Data Technologies},
pages = {45–50},
numpages = {6},
keywords = {Interactive Q&amp;A, Slope units, ridge lines, valley lines},
location = {
},
series = {ICBDT '24}
}

@article{10.1109/TASLP.2020.2980152,
author = {Zhu, Su and Zhao, Zijian and Ma, Rao and Yu, Kai},
title = {Prior Knowledge Driven Label Embedding for Slot Filling in Natural Language Understanding},
year = {2020},
issue_date = {2020},
publisher = {IEEE Press},
volume = {28},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2020.2980152},
doi = {10.1109/TASLP.2020.2980152},
abstract = {Traditional slot filling in natural language understanding (NLU) predicts a one-hot vector for each word. This form of label representation lacks semantic correlation modeling, which leads to severe data sparsity problem, especially when adapting an NLU model to a new domain. To address this issue, a novel label embedding based slot filling framework is proposed in this article. Here, distributed label embedding is constructed for each slot using prior knowledge. Three encoding methods are investigated to incorporate different kinds of prior knowledge about slots: atomic concepts, slot descriptions, and slot exemplars. The proposed label embeddings tend to share text patterns and reuses data with different slot labels. This makes it useful for adaptive NLU with limited data. Also, since label embedding is independent of NLU model, it is compatible with almost all deep learning based slot filling models. The proposed approaches are evaluated on three datasets. Experiments on single domain and domain adaptation tasks show that label embedding achieves significant performance improvement over traditional one-hot label representation as well as advanced zero-shot approaches.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = may,
pages = {1440–1451},
numpages = {12}
}

@proceedings{10.1145/3698322,
title = {EuroPLoP '24: Proceedings of the 29th European Conference on Pattern Languages of Programs, People, and Practices},
year = {2024},
isbn = {9798400716836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3337722.3341860,
author = {Mobramaein, Afshin and Whitehead, Jim},
title = {A methodology for designing natural language interfaces for procedural content generation},
year = {2019},
isbn = {9781450372176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337722.3341860},
doi = {10.1145/3337722.3341860},
abstract = {Procedural Content Generation (PCG) uses algorithmic techniques to create a wide variety of content for games. These generators often have a large number of parameters, making it difficult for non-technical designers to explore the design space of generated artifacts. Natural language interfaces for generators can map natural language keywords to parameter space changes spanning multiple simultaneous parameters and afford use of expressive language. This way, designers can navigate to interesting points in the design space of a generator by describing desired properties of the artifact using a series of natural language descriptors. We present a design methodology that designers can use to develop natural language interfaces for procedural content generation systems. This design methodology begins by defining a design vocabulary that can describe the output of a generator, mapping the vocabulary to a series of parameters, and translating natural language queries to movements in the generator's design space. We further address issues around designer intent understanding, design space exploration and workflows using natural language interfaces in PCG. An example and implementation of our methodology is provided demonstrating its application to existing plug-ins for content creation in the Unity3D engine},
booktitle = {Proceedings of the 14th International Conference on the Foundations of Digital Games},
articleno = {102},
numpages = {9},
keywords = {procedural content generation, natural language interfaces, design methodology},
location = {San Luis Obispo, California, USA},
series = {FDG '19}
}

@inproceedings{10.1145/3236024.3275427,
author = {Hosseini, Mitra Bokaei},
title = {Semantic inference from natural language privacy policies and Android code},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275427},
doi = {10.1145/3236024.3275427},
abstract = {Mobile apps collect dierent categories of personal information to provide users with various services. Companies use privacy policies containing critical requirements to inform users about their data practices. With the growing access to personal information and the scale of mobile app deployment, traceability of links between privacy policy requirements and app code is increasingly important. Automated traceability can be achieved using natural language processing and code analysis techniques. However, such techniques must address two main challenges: ambiguity in privacy policy terminology and unbounded information types provided by users through input elds in GUI. In this work, we propose approaches to interpret abstract terms in privacy policies, identify information types in Android layout code, and create a mapping between them using natural language processing techniques.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {940–943},
numpages = {4},
keywords = {natural Language Processing, Traceability, Requirements Engineering, Privacy},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3706598.3714210,
author = {Xie, Jingyi and Yu, Rui and Zhang, He and Billah, Syed Masum and Lee, Sooyeon and Carroll, John M.},
title = {Beyond Visual Perception: Insights from Smartphone Interaction of Visually Impaired Users with Large Multimodal Models},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714210},
doi = {10.1145/3706598.3714210},
abstract = {Large multimodal models (LMMs) have enabled new AI-powered applications that help people with visual impairments (PVI) receive natural language descriptions of their surroundings through audible text. We investigated how this emerging paradigm of visual assistance transforms how PVI perform and manage their daily tasks. Moving beyond usability assessments, we examined both the capabilities and limitations of LMM-based tools in personal and social contexts, while exploring design implications for their future development. Through interviews with 14 visually impaired users of Be My AI (an LMM-based application) and analysis of its image descriptions from both study participants and social media platforms, we identified two key limitations. First, these systems’ context awareness suffers from hallucinations and misinterpretations of social contexts, styles, and human identities. Second, their intent-oriented capabilities often fail to grasp and act on users’ intentions. Based on these findings, we propose design strategies for improving both human-AI and AI-AI interactions, contributing to the development of more effective, interactive, and personalized assistive technologies.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {62},
numpages = {17},
keywords = {People with visual impairments (PVI); large multimodal models (LMMs), Human-AI interaction, visual question answering (VQA); remote sighted assistance (RSA), Be My Eyes, Be My AI.},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3191697.3214331,
author = {Basman, Antranig},
title = {Critique of ‘Semprola: a semiotic programming language’},
year = {2018},
isbn = {9781450355131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3191697.3214331},
doi = {10.1145/3191697.3214331},
abstract = {We supply a critique of the paper Semprola: A Semiotic Programming Language, suggesting directions in which its work of bringing semiotics to programming can be refined, and supplying opinions on areas where it may be refounded.},
booktitle = {Companion Proceedings of the 2nd International Conference on the Art, Science, and Engineering of Programming},
pages = {214–217},
numpages = {4},
keywords = {Subjective Programming, Semiotics, Ontologies, Information Architecture, Cognitive Ergonomics},
location = {Nice, France},
series = {Programming '18}
}

@inproceedings{10.1145/3652620.3688557,
author = {Manellanga, Rajitha and David, Istvan},
title = {Participatory and Collaborative Modeling of Sustainable Systems: A Systematic Review},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688557},
doi = {10.1145/3652620.3688557},
abstract = {Sustainability has become a key characteristic of modern systems. Unfortunately, the convoluted nature of sustainability limits its understanding and hinders the design of sustainable systems. Thus, cooperation among a diverse set of stakeholders is paramount to sound sustainability-related decisions. Collaborative modeling has demonstrated benefits in facilitating cooperation between technical experts in engineering problems; but fails to include non-technical stakeholders in the modeling endeavor. In contrast, participatory modeling excels in facilitating high-level modeling among a diverse set of stakeholders, often of non-technical profiles; but fails to generate actionable engineering models. To instigate a convergence between the two disciplines, we systematically survey the field of collaborative and participatory modeling for sustainable systems. By analyzing 24 primary studies (published until June 2024), we identify common challenges, cooperation models, modeling formalisms and tools; and recommend future avenues of research.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {645–654},
numpages = {10},
keywords = {collaboration, MDE, model-driven, model-based, participatory modeling, survey, sustainability, systematic literture review},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3600100.3623720,
author = {He, Fang and Wang, Dan and Sun, Yaojie},
title = {Ontology Integration for Building Systems and Energy Storage Systems},
year = {2023},
isbn = {9798400702303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600100.3623720},
doi = {10.1145/3600100.3623720},
abstract = {A building ontology defines the concepts and organization of building data. Such knowledge can be assistance with automatic data access and support data-driven applications in buildings. With technological advances in batteries and energy storage, an increasing number of data-driven building applications now involve both building systems and energy storage systems (ESS), e.g., peak load shaving (PLS). However, existing building ontologies, e.g., Brick, are not designed to include concepts from ESS systems. Given the emergence of building-ESS applications, it has become important to develop ontologies that can cover knowledge about both building and ESS systems. Building systems and ESS systems fall under different industry sectors and there are building ontologies and ESS ontologies that have been developed independently. To maximally reuse existing knowledge, we leverage ontology integration technologies. We present a building-energy storage ontology integration (BESOI) system that can extend a building ontology with appropriate ESS ontologies. Our system handles ambiguity, incoherence, and redundancy problems in ontology integration. We evaluate BESOI on four building-ESS applications by extending Brick, a notable building ontology, with different ESS ontologies. The results show that BESOI can extend the coverage of Brick from 68.09\% to 95.74\% on the concepts of applications.},
booktitle = {Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {212–215},
numpages = {4},
keywords = {Building Application, Energy Storage System, Metadata, Ontology Integration},
location = {Istanbul, Turkey},
series = {BuildSys '23}
}

@inproceedings{10.1145/3266237.3266270,
author = {Peixoto, Mariana Maia and Silva, Carla},
title = {Specifying privacy requirements with goal-oriented modeling languages},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266270},
doi = {10.1145/3266237.3266270},
abstract = {Context: Privacy of personal data is a growing concern regarding users of software systems. In this sense, the literature reports that in order to avoid privacy breaches, there must be systematic approaches to specify privacy requirements from the early activities of software development. Objective: Motivated by this situation, this paper presents a framework of privacy modeling capabilities that must be addressed by requirements modeling languages to better support privacy specification. The capabilities will be used to compare three goal-oriented modeling languages (i*, NFR-Framework and Secure-Tropos). Method: The framework was created with basis on a conceptual foundation and a conceptual model of privacy built from an analysis of a standard, a regulation, guidelines and other bibliographical sources related to privacy. A health care example is used to illustrate how the framework can be used to compare the chosen modeling languages. Results: Fourteen privacy modeling capabilities were defined in the framework and it was observed that the analyzed modeling languages do not fully support them. Conclusions: The proposed framework contributes towards the consolidation of a privacy conceptual foundation that can be used to evaluate modeling languages for privacy in Requirements Engineering. The comparison performed by using this framework indicates Secure-Tropos as the most complete language to model privacy among the analyzed goal-oriented modeling languages.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {112–121},
numpages = {10},
keywords = {requirements modeling, requirements engineering, privacy, goal-oriented languages},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/3583780.3614771,
author = {Zhao, Yizheng},
title = {Highly-Optimized Forgetting for Creating Signature-Based Views of Ontologies},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614771},
doi = {10.1145/3583780.3614771},
abstract = {Uniform interpolation (UI) is a non-standard reasoning service that seeks to project an ontology down to its sub-signature --- given an ontology taking a certain signature, and a subset Σ of "relevant names'' of that signature, compute a new ontology, called a uniform interpolant, that uses only the relevant names while preserving the semantics of the relevant names in the uniform interpolant. UI is of great potential importance since it may be used in a variety of applications where suitable views of ontologies need to be computed. However, this potential can only be fully realized if a highly optimized method for computing such views exists. Previous research has shown that computing uniform interpolants of ELH-ontologies is a computationally extremely hard problem --- a finite uniform interpolant does not always exist for ELH, and if it exists, then there exists one of at most triple exponential size in terms of the original ontology, and that, in the worst case, no shorter interpolant exists. Despite the inherent difficulty of the problem, in this paper, we present a highly optimized forgetting method for computing uniform interpolants of ELH-ontologies, and show however that, with good reduction and inference strategies, such uniform interpolants can be efficiently computed. The method is an improvement of the one presented in our previous work. What sets it apart is its flexibility to treat concept names of different types differently, effectively cutting down on the inferences involved. This treatment is primarily driven by the polarities of the concept names within an ontology. A comprehensive evaluation with a prototypical implementation of the method shows &gt;95\% average success rates over two popular benchmark datasets and demonstrates a clear computational advantage over state-of-the-art systems.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {3444–3452},
numpages = {9},
keywords = {uniform interpolation, ontologies, forgetting, description logics},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3405962.3405977,
author = {G\'{o}mez-Suta, Manuela and Echeverry-Correa, Juli\'{a}n D. and Soto-Mej\'{\i}a, Jos\'{e} A.},
title = {Semi-automatic extraction and validation of concepts in ontology learning from texts in Spanish},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405977},
doi = {10.1145/3405962.3405977},
abstract = {The construction of ontologies from texts in Spanish is a challenge since this language lacks conceptual databases to validate abstract ontology structures as concepts and relations between them. The preceding generates the necessity of using manual evaluation by human experts; carrying high expenses that limit the calibration of algorithm parameters and large-scale evaluations. This document presents a proposal to evaluate abstract ontology structures through the task of semantic clustering of documents, without the expensive necessity of using manual evaluation or conceptual databases. The proposal is not only affordable but also applicable to model data and domains that lack structured knowledge resources. The experiments lead to the extraction and validation of the ontology structures from texts in Spanish regarding the domain of the Colombian armed conflict.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {7–16},
numpages = {10},
keywords = {evaluation, concepts, Spanish, Ontology learning},
location = {Biarritz, France},
series = {WIMS 2020}
}

@article{10.1145/3759454,
author = {Berardinelli, Luca and Muttillo, Vittoriano and Eramo, Romina and Bruneliere, Hugo and Rahimi, Abbas and Cicchetti, Antonio and Giner-Miguelez, Joan and G\'{o}mez, Abel and Potena, Pasqualina and Saadatmand, Mehrdad},
title = {Model Driven Engineering, Artificial Intelligence, and DevOps for Software and Systems Engineering: A Systematic Mapping Study of Synergies and Challenges},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3759454},
doi = {10.1145/3759454},
abstract = {This paper presents a systematic mapping study classifying existing scientific contributions on synergies of Model Driven Engineering (MDE), Artificial Intelligence/Machine Learning (AI/ML), and DevOps, with the overall objective of supporting the continuous development of Cyber-Physical Systems (CPSs). We collected papers from bibliographic sources and selected primary studies to analyse. Then, we characterised and classified the current state of the art, focusing on 1) main aspects already tackled at the intersection of at least two of the three studied areas, and 2) findings emerging from the analysis as a framework for potential future research, notably regarding the integration of the three studied areas. The results reveal that few approaches combine MDE, AI/ML, and DevOps for software and systems engineering. In contrast, several approaches have combined two of them, specifically MDE and DevOps. Approaches combining AI/ML with MDE or DevOps are also becoming more frequent and will most likely continue to progress in the future. These synergies cover a range of engineering activities, from requirements and design to monitoring, maintenance, and evolution. Open research challenges include advancing AI/ML, MDE, and DevOps integration, supporting scalable, data-oriented solutions, proposing new continuous engineering methods, and adapting DevOps practices to diverse systems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = aug,
keywords = {Model-Driven Engineering, DevOps, Continuous Integration, Artificial Intelligence, Machine Learning, Cyber-Physical Systems, Internet of Things, Cloud Computing}
}

@inproceedings{10.1145/3701716.3715478,
author = {Pernisch, Romana and Dobriy, Daniil and Polleres, Axel},
title = {The Massive Problem of Remote Changes in Ontology Reuse},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715478},
doi = {10.1145/3701716.3715478},
abstract = {Reusing existing datasets is a common practice in the Semantic Web, and it is also highly encouraged. Previous work on linking datasets has introduced and analysed different ways of linking but has failed to discuss the meaning and intentions behind the reuse of entities. This problem is aggravated by the fact Knowledge Graphs (KGs) and ontologies change over time. Currently, we lack an analysis of what impact the asymmetric evolution of the reused KGs has. Therefore, in this short paper, we evaluate how severe the problem of impacting remote changes is in practice by analysing the evolution of real-world ontologies. To this end, we collect a large corpus of open biomedical ontologies (759 ontologies) and provide statistics on their evolution, reuse (46.65\%) and impacting changes (33.38\%). We find that these KGs experience enormous amounts of impacting term reuse (7.59\%), and the extent of the problem has been overlooked on a massive scale.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1254–1258},
numpages = {5},
keywords = {kg evolution, kg evolution impact, kg reuse},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1613/jair.1.12789,
author = {Caro-Mart\'{\i}nez, Marta and Jim\'{e}nez-D\'{\i}az, Guillermo and Recio-Garc\'{\i}a, Juan A.},
title = {Conceptual Modeling of Explainable Recommender Systems: An Ontological Formalization to Guide Their Design and Development},
year = {2021},
issue_date = {Sep 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {71},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12789},
doi = {10.1613/jair.1.12789},
abstract = {With the increasing importance of e-commerce and the immense variety of products, users need help to decide which ones are the most interesting to them. This is one of the main goals of recommender systems. However, users’ trust may be compromised if they do not understand how or why the recommendation was achieved. Here, explanations are essential to improve user confidence in recommender systems and to make the recommendation useful.
 Providing explanation capabilities into recommender systems is not an easy task as their success depends on several aspects such as the explanation’s goal, the user’s expectation, the knowledge available, or the presentation method. Therefore, this work proposes a conceptual model to alleviate this problem by defining the requirements of explanations for recommender systems. Our goal is to provide a model that guides the development of effective explanations for recommender systems as they are correctly designed and suited to the user’s needs. Although earlier explanation taxonomies sustain this work, our model includes new concepts not considered in previous works. Moreover, we make a novel contribution regarding the formalization of this model as an ontology that can be integrated into the development of proper explanations for recommender systems.},
journal = {J. Artif. Int. Res.},
month = sep,
pages = {557–589},
numpages = {33},
keywords = {ontologies, knowledge representation}
}

@inproceedings{10.1145/3360901.3364444,
author = {Badenes-Olmedo, Carlos and Redondo-Garc\'{\i}a, Jos\'{e} Luis and Corcho, Oscar},
title = {Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364444},
doi = {10.1145/3360901.3364444},
abstract = {With the ongoing growth in number of digital articles in a wider set of languages and the expanding use of different languages, we need annotation methods that enable browsing multi-lingual corpora. Multilingual probabilistic topic models have recently emerged as a group of semi-supervised machine learning models that can be used to perform thematic explorations on collections of texts in multiple languages. However, these approaches require theme-aligned training data to create a language-independent space. This constraint limits the amount of scenarios that this technique can offer solutions to train and makes it difficult to scale up to situations where a huge collection of multi-lingual documents are required during the training phase. This paper presents an unsupervised document similarity algorithm that does not require parallel or comparable corpora, or any other type of translation resource. The algorithm annotates topics automatically created from documents in a single language with cross-lingual labels and describes documents by hierarchies of multi-lingual concepts from independently-trained models. Experiments performed on the English, Spanish and French editions of JCR-Acquis corpora reveal promising results on classifying and sorting documents by similar content.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {147–153},
numpages = {7},
keywords = {topic models, large-scale text analysis, cross-lingual semantic similarity},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@article{10.1145/3639409,
author = {Geeganage, Dakshi Kapugama and Xu, Yue and Li, Yuefeng},
title = {A Semantics-enhanced Topic Modelling Technique: Semantic-LDA},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3639409},
doi = {10.1145/3639409},
abstract = {Topic modelling is a beneficial technique used to discover latent topics in text collections. But to correctly understand the text content and generate a meaningful topic list, semantics are important. By ignoring semantics, that is, not attempting to grasp the meaning of the words, most of the existing topic modelling approaches can generate some meaningless topic words. Even existing semantic-based approaches usually interpret the meanings of words without considering the context and related words. In this article, we introduce a semantic-based topic model called semantic-LDA that captures the semantics of words in a text collection using concepts from an external ontology. A new method is introduced to identify and quantify the concept–word relationships based on matching words from the input text collection with concepts from an ontology without using pre-calculated values from the ontology that quantify the relationships between the words and concepts. These pre-calculated values may not reflect the actual relationships between words and concepts for the input collection, because they are derived from datasets used to build the ontology rather than from the input collection itself. Instead, quantifying the relationship based on the word distribution in the input collection is more realistic and beneficial in the semantic capture process. Furthermore, an ambiguity handling mechanism is introduced to interpret the unmatched words, that is, words for which there are no matching concepts in the ontology. Thus, this article makes a significant contribution by introducing a semantic-based topic model that calculates the word–concept relationships directly from the input text collection. The proposed semantic-based topic model and an enhanced version with the disambiguation mechanism were evaluated against a set of state-of-the-art systems, and our approaches outperformed the baseline systems in both topic quality and information filtering evaluations.},
journal = {ACM Trans. Knowl. Discov. Data},
month = feb,
articleno = {93},
numpages = {27},
keywords = {Topic modelling, semantics, concepts, disambiguation}
}

@inproceedings{10.5555/3712729.3712749,
author = {Tian, David and Squires, Hazel Y. and Buckley, Charlotte and Gillespie, Duncan and Tattan-Birch, Harry and Shahab, Lion and West, Robert and Brennan, Alan and Brown, Jamie and Purshouse, Robin C.},
title = {Incorporating the Com-B Model for Behavior Change into an Agent-Based Model of Smoking Behaviors: An Object-Oriented Design},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Modeling trajectories in cigarette smoking prevalence, initiation and quitting for populations and subgroups of populations is important for policy planning and evaluation. This paper proposes an agent-based model (ABM) design for simulating the smoking behaviors of a population using the Capability, Opportunity, Motivation - Behavior (COM-B) model. Capability, Opportunity and Motivation are modeled as latent composite attributes which are composed of observable factors associated with smoking behaviors. Three forms of the COM-B model are proposed to explain the transitions between smoking behaviors: initiating regular smoking uptake, making a quit attempt and quitting successfully. The ABM design follows object-oriented principles and extends an existing generic software architecture for mechanism-based modeling. The potential of the model to assess the impact of smoking policies is illustrated and discussed.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {252–263},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3448748.3448796,
author = {Liang, Changwei and Kong, Jiangping and Wu, Xiyu},
title = {A Speech-Driven 3-D Tongue Model with Realistic Movement in Mandarin Chinese},
year = {2021},
isbn = {9781450390002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448748.3448796},
doi = {10.1145/3448748.3448796},
abstract = {In this paper, a new speech driven 3-D geometric tongue model is constructed. The constructed 3-D tongue shape is controlled with control points on 2-D midsagittal tongue curve, and speech-driven inverse estimation based on the constructed model is evaluated by empirical data. X-Ray 2-D vocal tract motion videos are tagged for the midsagittal tongue motion, and static 3-D vocal tracts of 20 phonemes are collected with MRI for the realistic 3-D tongue shape. MFCC are calculated from the videos as acoustic features, and are then used in a LSTM-RNN to predict the control points movement of the tongue shape. Three geometrically intuitive control points are selected to represent and calculate the midsagittal line of the tongue through linear regression. Cross-sections on the central lines of the tongues, whose height, width and angle are then predicted from the midsagittal line, are reconstructed with geometric curves, and the shape of each cross-section are then placed on the midsagittal line to get the overall predicted moving grid of the 3-D tongue. In this 3-D tongue model, acoustic features and realistic tongue motion are mapped directly to preserve more realistic articulatory details, and the control points are intuitive for non-experts to control the model, and the geometric tongue shapes predicted are comparable with realistic tongue dynamics. Based on the proposed method, the speech-driven prediction is evaluated with the realistic data, which proved this proposed method feasible.},
booktitle = {Proceedings of the 2021 International Conference on Bioinformatics and Intelligent Computing},
pages = {297–302},
numpages = {6},
keywords = {3-D tongue model, Mandarin Chinese, realistic dynamics, speech-driven},
location = {Harbin, China},
series = {BIC '21}
}

@article{10.1145/3554733,
author = {Sharma, Neha and Soni, Mukesh and Kumar, Sumit and Kumar, Rajeev and Deb, Nabamita and Shrivastava, Anurag},
title = {Supervised Machine Learning Method for Ontology-based Financial Decisions in the Stock Market},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3554733},
doi = {10.1145/3554733},
abstract = {For changing semantics, ontological and information presentation, as well as computational linguistics for Asian social networks, are one of the most essential platforms for offering enhanced and real-time data mapping, as well as huge data access across diverse big data sources on the web architecture, information extraction mining, statistical modeling and data modeling, database control, and so on. The concept of opinion or sentiment analysis is often used to predict or classify the textual data, sentiment, affect, subjectivity, and other emotional states in online text. Recognizing the message's positive and negative thoughts or opinions by examining the author's goals will aid in a better understanding of the text's content in terms of the stock market. An intelligent ontology and knowledge Asian social network solution can improve the effectiveness of a company's decision making support procedures by deriving important information about users from a wide variety of web sources. However, ontology is concerned primarily with problem-solving knowledge discovery. The utilization of Internet-based modernizations welcomed a significant effect on the Indian stock exchange. News related to the stock market in the most recent decade plays a vital role for the brokers or users. This article focuses on predicting stock market news sentiments based on their polarity and textual information using the concept of ontological knowledge-based Convolution Neural Network (CNN) as a machine learning approach. Optimal features are essential for the sentiment classification model to predict the stock's textual reviews' exact sentiment. Therefore, the swarm-based Artificial Bee Colony (ABC) algorithm is utilized with the Lexicon feature extraction approach using a novel fitness function. The main motivation for combining ABC and CNN is to accelerate model training, which is why the suggested approach is effective in predicting emotions from stock news.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {139},
numpages = {24},
keywords = {Convolution Neural Networks (CNN), Artificial Bee Colony Algorithm (ABC), Lexicon feature extraction, sentiment analysis, opinion mining, Stock market}
}

@inproceedings{10.1145/3600100.3623744,
author = {Mavrokapnidis, Dimitris and Fierro, Gabe and Husmann, Maria and Korolija, Ivan and Rovas, Dimitrios},
title = {SeeQ: A Programming Model for Portable Data-Driven Building Applications},
year = {2023},
isbn = {9798400702303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600100.3623744},
doi = {10.1145/3600100.3623744},
abstract = {This paper introduces SeeQ, a programming model and an abstraction framework that facilitates the development of portable data-driven building applications. Data-driven approaches can provide insights into building operations and guide decision-making to achieve operational objectives. Yet the configuration of such applications per building requires extensive effort and tacit knowledge. In SeeQ, we propose a portable programming model and build a software system that enables self-configuration and execution across diverse buildings. The configuration of each building is captured in a unified data model — in this paper, we work with the Brick ontology without loss of generality. SeeQ focuses on the distinction between the application logic and the configuration of an application against building-specific data inputs and systems. We test the proposed approach by configuring and deploying a diverse range of applications across five heterogeneous real-world buildings. The analysis shows the potential of SeeQ to significantly reduce the efforts associated with the delivery of building analytics.},
booktitle = {Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {159–168},
numpages = {10},
keywords = {Analytics, Brick, Metadata, Ontologies, Portability, Programming, RDF, SHACL, Scalability, Semantic Web},
location = {Istanbul, Turkey},
series = {BuildSys '23}
}

@inproceedings{10.1145/3638067.3638088,
author = {Cardoso, Daiane de Ascen\c{c}\~{a}o and Henriques, Felipe da Rocha and Belloze, Kele Teixeira},
title = {Ontology Visualization in BioPortal: Methodological Triangulation for Analyzing Accessibility, Communicability, and Usability},
year = {2024},
isbn = {9798400717154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638067.3638088},
doi = {10.1145/3638067.3638088},
abstract = {Visualizing biomedical ontologies is an ongoing and highly relevant challenge in dealing with the vast volume of daily data and the miscellaneous use cases of ontologies. BioPortal is the most well-known tool for visualizing biomedical ontologies, bringing together hundreds of ontologies of varying sizes, depths, and complexities. This article aims to establish a methodological triangulation using SIM-SR based on Semiotic Engineering, the WCAG guidelines from the W3C consortium, and ergonomic principles to evaluate the experience of users who utilize screen readers and users who do not use this feature without conducting user experiments. The objective is to investigate and intersect the usability, communicability, and accessibility of BioPortal. Additionally, it seeks to understand how ontologies can be analyzed by as many people as possible, ensuring they are comprehensible and enhancing users’ cognitive understanding of the explored domain.},
booktitle = {Proceedings of the XXII Brazilian Symposium on Human Factors in Computing Systems},
articleno = {60},
numpages = {11},
keywords = {Communicability, Digital Acessibility, Evaluation Methods, Ontology, Semiotic Engineer, Usability},
location = {Macei\'{o}, Brazil},
series = {IHC '23}
}

@inproceedings{10.1145/3368640.3368641,
author = {Danenas, Paulius and Skersys, Tomas and Butleris, Rimantas},
title = {Enhancing the extraction of SBVR business vocabularies and business rules from UML use case diagrams with natural language processing},
year = {2019},
isbn = {9781450372923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368640.3368641},
doi = {10.1145/3368640.3368641},
abstract = {Being among the best-selling and most advanced features of model-driven development, model-to-model transformation could help improving one of the most time- and resource-consuming efforts in the process of model-driven information systems engineering, namely, discovery and specification of business vocabularies and business rules within the problem domain. Nonetheless, despite the relatively high levels of automation throughout the whole systems' model-driven development process, business modeling stage remains among the most under re-searched areas throughout the whole process. In this paper, we introduce a novel natural language processing (NLP) technique to one of our latest developments for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams. This development remains arguably the most comprehensive development of this kind currently available in public. The experiment provided proof that the developed NLP enhancement delivered even better extraction results compared to the already satisfactory performance of the previous development. This work contributes to the research in the areas of model transformations and NLP within the model-driven development of information systems, and beyond.},
booktitle = {Proceedings of the 23rd Pan-Hellenic Conference on Informatics},
pages = {1–8},
numpages = {8},
keywords = {use case diagram, natural language processing, model transformation, business vocabulary, business rules, UML, SBVR},
location = {Nicosia, Cyprus},
series = {PCI '19}
}

@inproceedings{10.1145/3325730.3325757,
author = {Memon, Kamran Ali and Xiaoling, Xia},
title = {Deciphering and Analyzing Software Requirements employing the techniques of Natural Language Processing},
year = {2019},
isbn = {9781450362580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3325730.3325757},
doi = {10.1145/3325730.3325757},
abstract = {Deciphering human language by Requirement Analysts is the key issue in Software Development. Clients communicate their software requirements in raw form. In this paper, we are presenting certain techniques of Natural Language processing which work out greatly to extract information properly and minimizing the bugs that may generate in later parts of Software Development. In today's era, the latest technological development in Artificial Intelligence has enabled machines to process the text to a certain level. Natural Language understanding is so far the most critical problem; the Software community is facing today in requirements gathering. In this study, using the techniques of Natural Language Interpretation, Testers and Software Developers can chalk out the more exact requirements from customers which can improve the Quality of Software to a certain level.},
booktitle = {Proceedings of the 2019 4th International Conference on Mathematics and Artificial Intelligence},
pages = {153–156},
numpages = {4},
keywords = {Software Requirements, Natural Language Understanding, Natural Language Processing, NLP Techniques, Linguistics},
location = {Chegndu, China},
series = {ICMAI '19}
}

@inproceedings{10.1145/3706599.3719925,
author = {Binksmith, Adam and Mansoor, Hamid and Nacenta, Miguel A and Toniolo, Alice},
title = {Designing Progressive Model Elicitation Tools to Support Complex Cognitive Activities},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719925},
doi = {10.1145/3706599.3719925},
abstract = {Externalizations such as sketches and diagrams are effective in helping individuals plan for complex projects and designs. Offloading complex information on a different medium (e.g. paper) frees up cognitive resources for sophisticated thinking. However, there are significant challenges in effectively using externalizations and notations (formalized externalizations). It is difficult to envision formal representations of complex ideas at the beginning and it is also hard to keep track of evolutions within notations. In addition, existing notations may be insufficient or inflexible for our specific purposes. We call instances of such thinking in which humans have to work through complex information and action space as instances of "Progressive Model Elicitation" (PME). We explore ways to support PME processes by presenting a set of design principles and Schematica, a prototype that implements those design principles to support PME. We present illustrative examples of Schematica use to support the process of PME.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {179},
numpages = {8},
keywords = {Externalization, Notation, Diagramming, Mental Model},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3711542.3711571,
author = {undefinedlgen, Bahar and Hattab, Georges},
title = {A Lexical Simplification Framework for Turkish},
year = {2025},
isbn = {9798400717383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711542.3711571},
doi = {10.1145/3711542.3711571},
abstract = {Lexical simplification is a fundamental step towards improving the accessibility, comprehension, and readability of texts, particularly in languages with limited linguistic resources. In this study, we adopt a state-of-the-art lexical simplification approach and propose a lexical simplification framework tailored to Turkish. Our framework leverages a combination of complex word identification tasks and substitution generation through pre-trained language models to identify complex lexical units using selective substitution ranking approaches and algorithms and replace them with simpler alternatives, thereby improving text readability. This work makes three key contributions: (i) a comprehensive study of lexical simplification for Turkish, including the complex word identification subtask; (ii) a rigorous comparison of various language models for candidate generation using the masked language modeling objective; and (iii) an in-depth exploration of the impact of different complexity thresholds and additional parameters on overall performance. Our framework demonstrates a strong capability to balance simplification and contextual preservation, offering an effective solution to lexical simplification in Turkish.},
booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval},
pages = {218–224},
numpages = {7},
keywords = {BERT, Text simplification, artificial intelligence for social good, complex word identification, lexical simplification, low-resource languages, pre-trained language model},
location = {
},
series = {NLPIR '24}
}

@inproceedings{10.1145/3624486.3624489,
author = {Bednar, Peter and Sarnovsky, Martin and Vanko, Jakub Ivan},
title = {Cognitive Architecture for Process industries},
year = {2023},
isbn = {9798400708350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624486.3624489},
doi = {10.1145/3624486.3624489},
abstract = {This paper introduces a Cross-Sectorial Big Data Processing platform which provides tools for the semantic modelling of the data analytical processes and for the automatic generation of data analysis scripts for solving the described problems. The main contribution of this paper is the cognitive component for the automatic extraction of the task definition from the narrative description of the problem based on the Large Language Models (LLMs). We have evaluated the proposed method on five problems from the different domains and found that the automatic extraction of the task definition can have promising results that can be applied to full-automatic data analytics.},
booktitle = {Proceedings of the 3rd Eclipse Security, AI, Architecture and Modelling Conference on Cloud to Edge Continuum},
pages = {15–20},
numpages = {6},
keywords = {Data analytics, Large language models, Ontologies},
location = {Ludwigsburg, Germany},
series = {eSAAM '23}
}

@inproceedings{10.1145/3613904.3642887,
author = {Chen, Liuqing and Jiang, Zhaojun and Xia, Duowei and Cai, Zebin and Sun, Lingyun and Childs, Peter and Zuo, Haoyu},
title = {BIDTrainer: An LLMs-driven Education Tool for Enhancing the Understanding and Reasoning in Bio-inspired Design},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642887},
doi = {10.1145/3613904.3642887},
abstract = {Bio-inspired design (BID) fosters innovations in engineering. Learning BID is crucial for developing multidisciplinary innovation skills of designers and engineers. Current BID education aims to enhance learners’ understanding and analogical reasoning skills. However, it often heavily relies on the teachers’ expertise. When learners pursue independent learning using some educational tools, they face challenges in understanding and reasoning practice within this multidisciplinary field. Additionally, evaluating their learning outcomes comprehensively becomes problematic. Addressing these challenges, we introduce a LLMs-driven BID education method based on a structured ontology and three strategies: enhancing understanding through LLMs-enpowered "learning by asking", assisting reasoning by providing hints and feedback, and assessing learning outcomes through benchmarking against existing BID cases. Implementing the method, we developed BIDTrainer, a BID education tool. User studies indicate that learners using BIDTrainer understood BID knowledge better, reason faster with higher interactivity than the baseline, and BIDTrainer assessed the learning outcomes consistent with experts.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {676},
numpages = {20},
keywords = {Analogy training, Bio-inspired design, Design education, Design evaluation},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3579987.3586572,
author = {Gupta, Tushar and Sural, Shamik},
title = {Ontology-based Evaluation of ABAC Policies for Inter-Organizational Resource Sharing},
year = {2023},
isbn = {9798400700996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579987.3586572},
doi = {10.1145/3579987.3586572},
abstract = {Attribute-based Access Control (ABAC), as the name suggests, determines whether an access request be granted based on the attributes or characteristics of the requesting user, those of the requested resource, and the environmental condition in which the request is generated. An important advantage of such an identity-agnostic model is that access control can be imposed even on users from other organizations if they are able to prove their attributes to the reference monitor of the organization whose resources are being accessed. It would, however, require a mechanism for mapping the attributes and their values among these organizations. We propose an ontology based method for addressing this requirement. Besides meeting the needs of collaborative accesses, we show how such an approach can be made to naturally support hierarchical ABAC policies as well as controlled relaxation during policy enforcement.},
booktitle = {Proceedings of the 9th ACM International Workshop on Security and Privacy Analytics},
pages = {85–94},
numpages = {10},
keywords = {resource sharing, policy relaxation, ontology, digital signature, attribute-based access control, at- tribute hierarchy},
location = {Charlotte, NC, USA},
series = {IWSPA '23}
}

@inproceedings{10.1145/3745238.3745485,
author = {Ji, Tingting and Xu, Jingyun and Chen, Weisong and Lin, Shengkai and Wu, Yican and Li, Wei},
title = {Multivariate data extraction method of enterprise digital internal audit based on knowledge map and LDA model},
year = {2025},
isbn = {9798400712791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745238.3745485},
doi = {10.1145/3745238.3745485},
abstract = {The explosive growth of enterprise data, with diverse data types and complex structure, provides abundant information resources for enterprise's operational decision-making, risk management and compliance review. However, how to extract valuable information from these massive data efficiently and accurately has become a key problem to be solved urgently in the digital internal audit of enterprises. Therefore, this paper proposes a multivariate data extraction method for enterprise digital internal audit based on knowledge map and LDA model. Define ontology and entity, construct multivariate data security knowledge map of enterprise digital internal audit, obtain the thematic correlation between documents and words, and construct LDA model; The features are aggregated to get the entity data vector, and the multivariate data extraction of enterprise digital internal audit is realized. The experimental results show that the information coverage of this method is high, and it shows significant advantages in the cost of multivariate data extraction in enterprise digital internal audit.},
booktitle = {Proceedings of the 2nd Guangdong-Hong Kong-Macao Greater Bay Area International Conference on Digital Economy and Artificial Intelligence},
pages = {1577–1581},
numpages = {5},
keywords = {Enterprise digitalization, Internal audit multivariate data, Data extraction, Knowledge map, LDA model},
location = {
},
series = {DEAI '25}
}

@inproceedings{10.1145/3629527.3652897,
author = {Suman, Shekhar and Chu, Xiaoyu and Niewenhuis, Dante and Talluri, Sacheendra and De Matteis, Tiziano and Iosup, Alexandru},
title = {Enabling Operational Data Analytics for Datacenters through Ontologies, Monitoring, and Simulation-based Prediction},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629527.3652897},
doi = {10.1145/3629527.3652897},
abstract = {Datacenters are key components in the ICT infrastructure supporting our digital society. Datacenter operations are hampered by operational complexity and dynamics, risking to reduce or even offset the performance, energy efficiency, and other datacenter benefits. A promising emerging technology, Operational Data Analytics~(ODA), promises to collect and use monitoring data to improve datacenter operations. However, it is challenging to organize, share, and leverage the massive and heterogeneous data resulting from monitoring datacenters. Addressing this combined challenge, starting from the idea that graphs could provide a good abstraction, in this work we present our early work on designing and implementing a graph-based approach for datacenter ODA. We focus on two main components of datacenter ODA. First, we design, implement, and validate agraph-based ontology for datacenters that captures both high-level meta-data information and low-level metrics of operational data collected from real-world datacenters, and maps them to a graph structure for better organization and further use. Second, we design and implementODAbler, a software framework for datacenter ODA, which combines ODA data with an online simulator to make predictions about current operational decisions and other what-if scenarios. We take the first steps to illustrate the practical use of ODAbler, and explore its potential to support datacenter ODA through graph-based analysis. Our work helps construct the case that graph-based ontologies have great value for datacenter ODA and, further, to improving datacenter operations.},
booktitle = {Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {120–126},
numpages = {7},
keywords = {\%oda monitoring, analysis, datacenter, graph-based ontology, mapping, odabler, opendc, operational data analytics, simulation},
location = {London, United Kingdom},
series = {ICPE '24 Companion}
}

@inproceedings{10.1145/3366424.3382699,
author = {Wang, Chunpei and Zhang, Xiaowang},
title = {Q-BERT: A BERT-based Framework for Computing SPARQL Similarity in Natural Language},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382699},
doi = {10.1145/3366424.3382699},
abstract = {In this paper, we present a pre-trained transformer network Q-BERT, in which siamese network architecture is employed to produce semantically meaningful embeddings of SPARQL queries to be compared via cosine-similarity. A core idea of Q-BERT is to put SPARQL query into the category of natural language to ensure that each entity mention or relation phrase in different knowledge bases has the same vector representation. Moreover, based on Q-BERT, we present a practical approach for the SPARQL query-empty-answer problem by accessing the RDF repository. The experiments on real datasets show the effectiveness and efficiency of Q-BERT.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {65–66},
numpages = {2},
keywords = {Siamese, Semantic Similarity, SPARQL, BERT},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3652620.3688558,
author = {Marchezan, Luciano and Homolka, Marcel and Blokhin, Andrei and Assun\c{c}\~{a}o, Wesley K. G. and Herac, Edvin and Egyed, Alexander},
title = {A Tool for Collaborative Consistency Checking During Modeling},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688558},
doi = {10.1145/3652620.3688558},
abstract = {Consistency checking is widely used to detect inconsistencies in engineering artifacts. In collaborative modeling environments, however, maintaining model consistency is challenging due to the frequent changes introduced by multiple engineers, leading to possibly numerous inconsistencies. These inconsistencies need to be identified and shared among engineers to collaboratively resolve them and prevent cascading problems. Despite extensive research in consistency checking, there remains a lack of tools that support collaborative consistency checking (C3). C3 is defined as the process of checking and fixing inconsistencies during collaborative modeling, whether engineers work synchronously or asynchronously. To address this gap, we introduce a prototype tool integrated into the DesignSpace environment which allows collaborative work during modeling and consistency checking. Our tool is implemented to support a streamlined version of UML (sUML) with various diagrams such as class, use case, sequence, and state-machine. We showcase the tool's features with an example of a robotic arm, highlighting its effectiveness in collaborative consistency checking. We also compare our tool with related work, showing that collaborative features are currently lacking in the proposed tools. A video showcasing the tool is available at https://www.youtube.com/watch?v=9kweKeBHx5Y},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {655–659},
numpages = {5},
keywords = {collaborative modeling, consistency checking, tool demo},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3459637.3482307,
author = {Liu, Zhao and Lu, Chang and Alghamdi, Ghadah and Schmidt, Renate A. and Zhao, Yizheng},
title = {Tracking Semantic Evolutionary Changes in Large-Scale Ontological Knowledge Bases},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482307},
doi = {10.1145/3459637.3482307},
abstract = {This paper is concerned with the problem of computing the semantic difference between different versions of large-scale ontological knowledge bases using a uniform interpolation (UI) approach. The semantic difference between two versions of an ontology are the axioms entailed by one version but not the other version, reflecting the evolutionary changes of the content of the ontology. In general, computing such axioms is not computationally feasible, since there are infinitely many of them. UI is an advanced reasoning technique that seeks to create restricted views of ontologies; it provides an effective means for computing a finite representation of the difference between two ontologies. While existing UI methods are designed for languages that are either more expressive or less expressive than the description logic ELH, the underlying language of typical large-scale ontologies, in this paper, we introduce a practical UI method tailored for the task of computing the semantic difference in large-scale ELH-ontologies. The method is terminating, sound, and can always compute UI results possibly including fresh definer symbols. Two case studies on different versions of the SNOMED CT terminology show that the method has overcome major limitations of existing UI methods and can be used to reveal modeling changes that have occurred over successive releases of SNOMED CT.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {1130–1139},
numpages = {10},
keywords = {uniform interpolation, semantic difference, ontologies, knowledge representation and reasoning, forgetting, description logics},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3640310.3674084,
author = {Bach, Jean-Christophe and Beugnard, Antoine and Champeau, Jo\"{e}l and Dagnat, Fabien and Gu\'{e}rin, Sylvain and Mart\'{\i}nez, Salvador},
title = {10 years of Model Federation with Openflexo: Challenges and Lessons Learned},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674084},
doi = {10.1145/3640310.3674084},
abstract = {In the context of complex system development, heterogeneous modeling responds to the need to integrate several domains. This need requires the use of the most appropriate formalism and tooling for each domain to be efficient. Model federation promotes the semantic interoperability of heterogeneous models by providing the means to reify correspondences between different model elements, add custom behaviors and bridge the gap between technological spaces. As such, it can be used as an infrastructure to address many different system engineering problems. This is what we have been doing for over a decade, as part of a close collaboration between a small software engineering startup and academia. This paper reports on this experience.Concretely, we discuss the context, ambitions, and challenges that led to the inception of our practice of model federation, and we present five use cases experiences, stemming from real industrial and academic needs, and elaborate on lessons learned. In addition, we also report on challenges and lessons learned regarding the development and maintenance of a model-driven model federation tool, the Openflexo framework. Finally, we set up a road map for the future of model federation and Openflexo.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {25–36},
numpages = {12},
keywords = {Experience report, Model federation, Model management},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3555776.3577616,
author = {Bozzato, Loris and Serafini, Luciano},
title = {Ontology-Mediated Data Migration: Deriving Migration Rules by Reasoning on Schema Descriptions},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577616},
doi = {10.1145/3555776.3577616},
abstract = {Migration of data across information systems is a knowledge intensive task: the definition of mappings between systems requires knowledge of the source and target (relational) schemas and their interpretation of the shared domain. Moreover, direct schema mappings need often to be re-defined for each new migration instance, in order to accommodate the variations caused by the change of systems and representation conventions. A possible solution to such problems is the use of an intermediate ontological model, that can be used as a lingua franca for the description of schemas, by defining mappings from and to the ontology. While this helps in making explicit the semantics of the schemas, the problem remains on how to extract a direct mapping from source to target schema from this intermediate representation.In this paper, we present our ongoing work in building an ontology-based migration system in the scenario of banking information systems. In the architecture of the system, an ontology defines an intermediate semantic description for the source and target schemas. We introduce a reasoning method for the automatic extraction of migration rules starting from the semantic descriptions of the schemas. The procedure for computation of migration rules is then implemented via reasoning over an Answer Set Programming encoding.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1724–1731},
numpages = {8},
keywords = {ontology mediated migration, relational schema mapping, data migration},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3465481.3470024,
author = {Merah, Yazid and Kenaza, Tayeb},
title = {Ontology-based Cyber Risk Monitoring Using Cyber Threat Intelligence},
year = {2021},
isbn = {9781450390514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465481.3470024},
doi = {10.1145/3465481.3470024},
abstract = {Efficient cyber risk assessment needs to consider all security alerts provided by cybersecurity solutions deployed in a network. To build a reliable overview of cyber risk, there is a need to adopt continuous monitoring of emerged cyber threats related to that risk. Indeed, the integration of Cyber Threat Intelligence (CTI) into cybersecurity solutions provides valuable information about threats, targets, and potential vulnerabilities. Structured Threat Information eXpression (STIX), as a language for expressing information about cyber threats in a structured and unambiguous manner, is becoming a de facto standard for sharing information about cyber threats. In addition, ontology-based semantic knowledge modeling has become a promising solution that provides a machine-readable language for downstream work in cybersecurity problem-solving. In this paper, we propose an ontology using CTI for risk monitoring. This latter improves an existing ontology, originally proposed to be used within a SIEM (Security Information Event Management), by extending it and aligning it with the STIX concepts.},
booktitle = {Proceedings of the 16th International Conference on Availability, Reliability and Security},
articleno = {88},
numpages = {8},
keywords = {Cyber Threat Intelligence, OWL, Ontology, Risk Assessment},
location = {Vienna, Austria},
series = {ARES '21}
}

@inproceedings{10.1145/3652620.3688223,
author = {Hallak, Yara and Blouin, Dominique and Pautet, Laurent and Saab, Layale and Laborie, Baptiste and Mittal, Rakshit},
title = {Model Management at Renault Virtual Simulation Team: State of Practice, Challenges and Research Directions},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688223},
doi = {10.1145/3652620.3688223},
abstract = {In the automotive industry, new systems are being developed to enhance vehicle safety and driver convenience. These systems are increasingly complex to build and maintain. To develop these systems Renault makes intensive use of simulation and must deal with thousands of models. This huge number of models must be well managed. To manage these models, Renault has developed the SysML-based Model Identity Card (MIC), used with a Model-Based Simulation (MBSi) approach. However, despite this first solution, managing simulation models remains a difficult task.In this paper, we describe the current simulation model management approaches used at Renault, and their shortcomings and challenges in the modelling and simulation of complex automotive systems. We use Advanced Driver Assistance System (ADAS) and its Automatic Emergency Breaking (AEB) sub-system as examples to illustrate the utilization of the MIC and demonstrate current practices. From these examples, we derive main challenges faced by the virtual simulation team and propose research directions to solve them, based on state of the art methodologies for simulation models' validation and verification management.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1005–1014},
numpages = {10},
keywords = {model management, model-based systems engineering, model identity card, model-based simulation, advanced driver assistance system},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3652620.3688256,
author = {Abbasi, Faima and Brimont, Pierre and Pruski, Cedric and Sottet, Jean-S\'{e}bastien},
title = {Understanding Semantic Drift in Model Driven Digital Twins},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688256},
doi = {10.1145/3652620.3688256},
abstract = {Digital twins have revolutionized the industry in recent years by providing virtual representations of physical assets, systems, or processes, and relying on real-time data for effective functioning. These twins enable real-time monitoring, analysis, and simulation of real-world entities through the extensive use of various digital models, including design models, scientific models, and data models that capture the status and behaviour of the corresponding physical entities. However, as the real world evolves, these models must adapt to maintain consistency with their physical counterparts. This adaptation process can lead to semantic drift, a misalignment between the digital representation and the physical reality over time. In this paper, we propose a classification and formalization of different types of semantic drift and review how this concept is understood in the literature on model-driven digital twins. We further illustrate the scenarios associated with each type of semantic drift using an urban mobility use case, explicitly highlighting the practical implications.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {419–430},
numpages = {12},
keywords = {digital twins, semantic drift, scientific models, data models, design models},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3360901.3364439,
author = {Jain, Prateek and Verma, Kunal and Gaikwad, Aniket and Gadde, Pramod},
title = {Understanding Financial Transaction Documents using Natural Language Processing},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364439},
doi = {10.1145/3360901.3364439},
abstract = {In this paper, we share our experiences creating NLP based AI platform for finance - Appzen (http://www.appzen.com). AppZen's auditing technology is being utilized by over 500 enterprise customers including multiple Fortune 500 companies for auditing employee expenses. AppZen's technology can process, analyze and identify relationships between various kinds of transaction documents such as - receipts, invoices, contracts and purchase orders. Each type of transaction document requires custom processing and analysis due to the diversity in language and structure of the document. Contracts typically require deep understanding of the content such as identifying sentence structures, identifying entities and relationships between them compared to receipts and invoices, which are somewhat semi-structured and require a different kind of processing. We elaborate on the challenges we have experienced and use of NLP in conjunction with a lightweight semantic layer to alleviate these challenges.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {255–258},
numpages = {4},
keywords = {nlp, information extraction, financial auditing, feature engineering},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1145/3706598.3713940,
author = {Zhu, Jichen and Sanches, Pedro and Tsaknaki, Vasiliki and van der Maden, Willem and Kaklopoulou, Irene},
title = {The Centers and Margins of Modeling Humans in Well-being Technologies},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713940},
doi = {10.1145/3706598.3713940},
abstract = {This paper critically examines the machine learning (ML) modeling of humans in three case studies of well-being technologies. Through a critical technical approach, it examines how these apps were experienced in daily life (technology in use) to surface breakdowns and to identify the assumptions about the “human” body entrenched in the ML models (technology design). To address these issues, this paper applies agential realism to decenter foundational assumptions, such as body regularity and health/illness binaries, and speculates more inclusive design and ML modeling paths that acknowledge irregularity, human-system entanglements, and uncertain transitions. This work is among the first to explore the implications of decentering theories in computational modeling of human bodies and well-being, offering insights for more inclusive technologies and speculations toward posthuman-centered ML modeling.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {518},
numpages = {16},
keywords = {Decentering, Machine Learning Modeling, Well-being, Diffraction, Agential Realism},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3713985,
author = {Paudel, Shreyasha and Loos, Sabine and Soden, Robert},
title = {Hype versus Historical Continuity: Situating the Rise of AI in Climate and Disaster Risk Modeling},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713985},
doi = {10.1145/3706598.3713985},
abstract = {As governments increasingly adopt Artificial Intelligence (AI) across different application sectors, advocates argue that it will create new disruptions by democratizing access, improving accuracy, and lowering costs. In practice, uncritical adoption of AI tools has been shown to cause significant harms. Our study uses a historical lens to examine the uptake of AI in climate risk management through a study of climate and disaster risk modeling. These techniques originated in the insurance industry, but are now incorporated into many climate and disaster governance processes. Using the concept of ‘insurance logics’, we demonstrate that many of the original aspects of disaster risk modeling remain despite the transfer of risk assessment tools from the insurance industry to the public sector and new techniques made possible by AI. This highlights technological continuity, rather than disruption, as a key driver of contemporary risk modeling practice. Doing so helps to unsettle problematic, though challenging to identify, aspects of supposedly disruptive technologies and create possibilities for alternatives.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {258},
numpages = {17},
keywords = {History, Technological Evolution, Climate Risk, Disaster Risk Models, AI Hype, Responsible AI},
location = {
},
series = {CHI '25}
}

@article{10.1145/3626196,
author = {kumari, Rani and Sah, Dinesh Kumar and Cengiz, Korhan and Ivkovi\'{c}, Nikola and Balaji, Prasanalakshmi},
title = {Automatic graph construction and Exploring different types of LSTMs for Asian Hindi languages for Medical review Sentiment Analysis},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3626196},
doi = {10.1145/3626196},
abstract = {Sentiment Analysis (SA) of medical reviews is crucial for improving healthcare outcomes. However, analyzing sentiment in low-resource languages such as Asian Hindi presents significant challenges. In this study, we propose an automatic graph construction approach to extract relevant features from medical reviews in Asian Hindi languages. We explore different types of Long Short-Term Memory (LSTMs), including traditional LSTMs, bidirectional LSTMs, and attention-based LSTMs, to classify the sentiment of medical reviews. Our proposed approach uses attention-based LSTM architecture and pre-trained Word2Vec embeddings to achieve high accuracy. We compare the proposed approach with existing models using various evaluation metrics, including accuracy, precision, recall, and F1-score. The results demonstrate that our proposed approach outperforms all existing models in terms of accuracy, achieving an accuracy score of 81\%. These findings could have implications for improving healthcare outcomes by enabling better monitoring of patient feedback and identifying areas for improvement in medical services.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
keywords = {Graph construction, Long short-term memory, Deep learning, Hindi language}
}

@inproceedings{10.1145/3652620.3688215,
author = {Fu, Yuhong and Selway, Matt and Grossmann, Georg and Kaur, Karamjit and Stumptner, Markus},
title = {Modelling a Warehouse with SLICER: A Contribution to the MULTI Warehouse Challenge},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688215},
doi = {10.1145/3652620.3688215},
abstract = {In this paper we apply the SLICER multi-level modelling framework on the MULTI 2024 Warehouse Challenge. SLICER was first introduced at the ER 2015 conference[17] and formally specified in [18]. It has been implemented in an object-oriented knowledge representation and reasoning system and the framework identifies additional semantic subcategories for the traditional object-oriented relationship types and uses these distinctions for a new style of formal characterisation of level relationships. The framework is executable via its implementation in a dynamic metamodeling and object logic environment. We first introduce SLICER, then describe its application on the Warehouse Challenge requirements and then discuss the advantages of the SLICER representation.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {828–837},
numpages = {10},
keywords = {multi-level modelling, warehouse challenge, SLICER},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3635059.3635087,
author = {Gasidou, Anastasia and Kotsifakos, Dimitrios and Douligeris, Christos},
title = {Specific modeling issues for designing the transformation of a smart city},
year = {2024},
isbn = {9798400716263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635059.3635087},
doi = {10.1145/3635059.3635087},
abstract = {In this short paper, we present methods and solutions for specific issues of concern to engineers and application technologists related to modeling a smart city's transformation. The main body of developing "structural design modeling" for a smart city utilizes the Unified Modeling Language™ (UML®) conceptual approach. Our approach provides an overview of the specific domains (points - nodes) related to the topic. Our discussion does not delve into significant issues. It does not solidify the proposed approaches but as a research proposal, it remains at a level that could be described as a presentation or an argumentative position. Our short paper heavily relies on diagrams of models. This short paper focuses on specific design issues related to modeling mappings specifically for the design and modeling processes for a smart city.},
booktitle = {Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {181–184},
numpages = {4},
keywords = {Engineering, Informatics, Modeling Patterns, Project, Science, Technology},
location = {Lamia, Greece},
series = {PCI '23}
}

@inproceedings{10.1145/3264746.3264786,
author = {Kim, Gihoon and Choi, Chang and Choi, Junho},
title = {Ontology modeling for APT attack detection in an IoT-based power system},
year = {2018},
isbn = {9781450358859},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3264746.3264786},
doi = {10.1145/3264746.3264786},
abstract = {Smart grid technology is the core technology for the next-generation power grid system with enhanced energy efficiency through decision-making communication between suppliers and consumers enabled by integrating the IoT into the existing grid. This open architecture allowing bilateral information exchange makes it vulnerable to various types of cyberattack. APT attacks, one of the most common cyberattacks, are highly tricky and sophisticated attacks that can circumvent the existing detection technology and attack the targeted system after a certain latent period after intrusion. This paper proposes an ontology-based attack detection system capable of early detection of and response to APT attacks by analyzing their attacking patterns.},
booktitle = {Proceedings of the 2018 Conference on Research in Adaptive and Convergent Systems},
pages = {160–164},
numpages = {5},
keywords = {smart grid, ontology, IoT, APT attack},
location = {Honolulu, Hawaii},
series = {RACS '18}
}

@inproceedings{10.1145/3209219.3213598,
author = {Moon, DeKita G.},
title = {Modeling Learners' Interest with a Domain-Independent Ontology-Based Framework},
year = {2018},
isbn = {9781450355896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209219.3213598},
doi = {10.1145/3209219.3213598},
abstract = {Ontologies are recognized as a promising approach to support the reusability and interoperability of learners' preferences; which is useful for the optimization and flexibility of data and resources. However, little to no research on adaptive learning systems or semantic technologies explore personalized experiences based on the various out-of-school experiences and activities of the users. This research investigates the design, development, and evaluation of an ontology-based framework for students' interests in a math word problem generator that may be applied to various other learning systems and possibly other domains. The cohesiveness of the problems in addition to the usability, usefulness, and the short-term effectiveness of the derived technology will be investigated by comparing the generated questions to numerical and traditional Algebra I problems. We aim to better understand students' interests to identify the role that their interests can play in semantic technologies, further supporting the recent advances in ontology-based educational technologies and personalized math word problem generators.},
booktitle = {Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization},
pages = {345–348},
numpages = {4},
keywords = {semantic technologies, knowledge graphs, human-centered computing, educational technologies, domain ontologies},
location = {Singapore, Singapore},
series = {UMAP '18}
}

@inbook{10.1145/3715668.3735608,
author = {Calderwood, Alex and Kim, Taewook and Sun, Yuqian and Roemmele, Melissa and Chung, John Joon Young and Kreminski, Max},
title = {Supporting Material Writing Practice with Phraselette, a Palette of Phrases},
year = {2025},
isbn = {9798400714863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715668.3735608},
abstract = {We present a demonstration of Phraselette, an artistic support tool designed for compatibility with the writerly values of experimental poets. Following a theory of “material writing support”, we introduce affordances for selecting short spans of text (on the order of a few words) to vary; constraining text generation procedures (some based on language models) to adhere to poetic intent; and searching through large spaces of potential variations for phrases that satisfy users’ constraints in unexpected but evocative ways. Phraselette has been validated through an extended expert evaluation with 10 published poets; we found that, in contrast to the dominant prompting-based approach to interacting with language models as writing support tools, Phraselette is better aligned with experimental poetry practice, providing deeper support for navigating spaces of potential interpretations of poetic text.},
booktitle = {Companion Publication of the 2025 ACM Designing Interactive Systems Conference},
pages = {236–241},
numpages = {6}
}

@inproceedings{10.1145/3589335.3651256,
author = {Chaturvedi, Rochana},
title = {Temporal Knowledge Graph Extraction and Modeling across Multiple Documents for Health Risk Prediction},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651256},
doi = {10.1145/3589335.3651256},
abstract = {Clinical text in electronic health records (EHR) holds vital cues into a patient's journey, often absent in structured EHR data. Evidence-based healthcare decisions demand accurate extraction and modeling of these cues. The goal of our study is to predict Type-II Diabetes by utilizing concept-based models of visit sequences from longitudinal EHR data. We undertake the challenging task of fine-grained temporal information extraction from clinical text using a recent span-based approach with pre-trained transformers. We achieve a new state-of-the-art in end-to-end relation extraction from 2012 clinical temporal relations corpus. We propose to apply our model to a new dataset and extract patient-centric temporal knowledge graphs from their visits-fusing temporal orderings within documents and across visits. Beyond the current focus of our work on Type-II Diabetes risk prediction from EHR, our versatile framework can be extended to other domains including web-based healthcare systems for personalized medicine. It can not only model health outcomes having long progression timelines but also various socio-economic outcomes such as conflict, natural disasters, and financial markets by leveraging news, reports, and social-media text for extracting and modeling irregular time-series and help inform a variety of web-based applications and policies.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1182–1185},
numpages = {4},
keywords = {dynamic graphs, multidocument classification, natural language processing, patient-centric, temporal information extraction, temporal knowledge graphs, time series classification},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3330431.3330453,
author = {Niyazova, R. and Aktayeva, Al. and Davletkireeva, L.},
title = {An ontology based model for user profile building using social network},
year = {2019},
isbn = {9781450372121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330431.3330453},
doi = {10.1145/3330431.3330453},
abstract = {The structure and basic principles of technology for increasing the probability of identifying subjects of information processes of open Internet resources based on ontology methods are considered. Based on this ontology the knowledge base intended for creation of the program systems supporting ensuring information security has been realized. The developed ontological knowledge base has been used when developing the software complex intended for identification of the user of social networks when ensuring information security, monitoring and preventing threats.},
booktitle = {Proceedings of the 5th International Conference on Engineering and MIS},
articleno = {21},
numpages = {4},
keywords = {social network, ontology, knowledge base, identification, cybersecurity, SPARQL},
location = {Astana, Kazakhstan},
series = {ICEMIS '19}
}

@inproceedings{10.1145/3550355.3552449,
author = {Parvizimosaed, Alireza and Roveri, Marco and Rasti, Aidin and Amyot, Daniel and Logrippo, Luigi and Mylopoulos, John},
title = {Model-checking legal contracts with SymboleoPC},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552449},
doi = {10.1145/3550355.3552449},
abstract = {Legal contracts specify requirements for business transactions. As any other requirements specification, contracts may contain errors and violate properties expected by contracting parties. Symboleo was recently proposed as a formal specification language for legal contracts. This paper presents SymboleoPC, a tool for analyzing Symboleo contracts using model checking. It highlights the architecture, implementation and testing of the tool, as well as a scalability evaluation with respect to the size of contracts and properties to be checked through a series of experiments. The results suggest that SymboleoPC can be usefully applied to the analysis of formal specifications of contracts with real-life sizes and structures.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {278–288},
numpages = {11},
keywords = {software requirements specifications, smart contracts, performance analysis, nuXmv, model checking, legal contracts, formal specification languages},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3701551.3703477,
author = {Roy, Soumyadeep and Sundaram, Sowmya S. and Wolff, Dominik and Ganguly, Niloy},
title = {Building Trustworthy AI Models for Medicine: From Theory to Applications},
year = {2025},
isbn = {9798400713293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701551.3703477},
doi = {10.1145/3701551.3703477},
abstract = {AI is emerging as an efficient companion in medicine. While AI holds promise for reducing the cognitive load of researchers and practitioners, its adoption is often hindered by a lack of trust in new AI advancements. We present sophisticated techniques for developing trustworthy artificial intelligence (AI) models in medicine, bridging breakthroughs in AI research with practical healthcare applications. We will discuss in-depth the four stages (Design, Development, Implementation, and Evaluation) involved in the process of building trustworthy AI models customized for the medical domain. We present various techniques for incorporating important Trustworthy AI principles like data privacy, robustness, explainability, interpretability, medical experts-in-the-loop, and risk assessment while developing AI models for medicine. In contrast to prior tutorials, we make the following two key contributions: (i) While explaining the 'Implementation' stage, we cover various real-world healthcare applications developed as part of research projects in academia in collaboration with medical schools in India and Germany. (ii) By including a health informatics professional as one of the tutorial organizers, we provide a fresh and much-needed perspective on the research challenges and mitigation strategies in building AI models for medicine.},
booktitle = {Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining},
pages = {1012–1015},
numpages = {4},
keywords = {knowledge integration in healthcare, medical NLP, trustworthy AI},
location = {Hannover, Germany},
series = {WSDM '25}
}

@inproceedings{10.1145/3701716.3715286,
author = {Kousa, Ilona M.},
title = {Data-Driven Democracy? Using Social Media and AI for Knowledge Co-production in Energy Transition Research},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715286},
doi = {10.1145/3701716.3715286},
abstract = {As global challenges grow increasingly difficult to address, the need for relevant knowledge in policymaking has become more critical than ever. In fields such as sustainability research, discussions about the co-production of knowledge emphasise the involvement of diverse stakeholders in generating relevant information for democratic decision-making processes. Particularly in technology-intensive areas like energy, the voices of experts have traditionally dominated, often marginalising the perspectives of citizens affected by energy policies and leading to epistemic injustices. Social media serves as a central forum for political activity and civic engagement, making it an important research environment for understanding the complexities of knowledge production and sharing. In my dissertation, I explore the opportunities and challenges of integrating citizen voices into the co-production of knowledge through social media using artificial intelligence (AI) based methods. My approach includes analysing stakeholder perspectives across social media, policymakers' speeches, and traditional media, using the discourse on energy justice in the context of sustainable energy transition as empirical material. Additionally, I conduct a comparative analysis of a rule-based ontological classification tool and a large language model (LLM) chatbot as qualitative content analysis tools. I demonstrate that social media data can reveal critical perspectives and marginalised discourses that might otherwise be overlooked. However, social media platforms are also used to spread disinformation and incite polarisation, which undermines their reliability as accurate reflections of prevailing attitudes and opinions in society. In addition, although AI methods can make information processing more effective and support the use of large datasets, it is important to consider their inherent limitations. I address the biases and ethical concerns associated with social media data and AI-based analysis tools and discuss potential solutions to mitigate these challenges.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {701–704},
numpages = {4},
keywords = {computational social sciences, large language models, natural language processing, social media},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3732775.3733586,
author = {Gokdemir, Ozan and Siebenschuh, Carlo and Brace, Alexander and Wells, Azton and Hsu, Brian and Hippe, Kyle and Setty, Priyanka and Ajith, Aswathy and Pauloski, J. Gregory and Sastry, Varuni and Foreman, Sam and Zheng, Huihuo and Ma, Heng and Kale, Bharat and Chia, Nicholas and Gibbs, Thomas and Papka, Michael and Brettin, Thomas and Alexander, Francis and Anandkumar, Anima and Foster, Ian and Stevens, Rick and Vishwanath, Venkatram and Ramanathan, Arvind},
title = {HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights},
year = {2025},
isbn = {9798400718861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3732775.3733586},
doi = {10.1145/3732775.3733586},
abstract = {The volume of scientific literature is growing exponentially, leading to underutilized discoveries, duplicated efforts, and limited cross-disciplinary collaboration. Retrieval-Augmented Generation (RAG) offers a way to assist scientists by improving the factuality of Large Language Models (LLMs) in processing this influx of information. However, scaling RAG to handle millions of articles introduces significant challenges, including the high computational costs associated with parsing documents and embedding scientific knowledge, as well as the algorithmic complexity of aligning these representations with the nuanced semantics of scientific content. To address these issues, we introduce HiPerRAG, a RAG workflow powered by high performance computing (HPC) to index and retrieve knowledge from more than 3.6 million scientific articles. At its core are Oreo, a high-throughput model for multimodal document parsing, and ColTrast, a query-aware encoder fine-tuning algorithm that enhances retrieval accuracy by using contrastive learning and late-interaction techniques. HiPerRAG delivers robust performance on existing scientific question answering (Q/A) benchmarks and two new benchmarks introduced in this work, achieving 90\% accuracy on SciQ and 76\% on PubMedQA—outperforming both domain-specific models like PubMedGPT and commercial LLMs such as GPT-4. Scaling to thousands of GPUs on the Polaris, Sunspot, and Frontier supercomputers, HiPerRAG delivers million document-scale RAG workflows for unifying scientific knowledge and fostering interdisciplinary innovation.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
pages = {1–13},
numpages = {13},
keywords = {HPC, AI, large language models, retrieval-augmented generation, metric learning, neural information retrieval},
location = {FHNW University of Applied Sciences and Arts Northwestern Switzerland, Brugg-Windisch, Switzerland},
series = {PASC '25}
}

@article{10.5555/3241691.3241693,
author = {Gatt, Albert and Krahmer, Emiel},
title = {Survey of the state of the art in natural language generation: core tasks, applications and evaluation},
year = {2018},
issue_date = {January 2018},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {61},
number = {1},
issn = {1076-9757},
abstract = {This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past two decades, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artifical intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of nlp, with an emphasis on different evaluation methods and the relationships between them.},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {65–170},
numpages = {106}
}

@inproceedings{10.1145/3555776.3577719,
author = {Baader, Franz},
title = {Optimal Repairs in Ontology Engineering as Pseudo-Contractions in Belief Change},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577719},
doi = {10.1145/3555776.3577719},
abstract = {The question of how a given knowledge base can be modified such that certain unwanted consequences are removed has been investigated in the area of knowledge engineering under the name of repair and in the area of belief change under the name of contraction. Whereas in the former area the emphasis was more on designing and implementing concrete repair algorithms, the latter area concentrated on characterizing classes of contraction operations by certain postulates they satisfy. In the classical setting, repairs and contractions are subsets of the knowledge base that no longer have the unwanted consequence. This makes these approaches syntax-dependent and may result in removal of more consequences than necessary. To alleviate this problem, gentle repairs and pseudo-constractions have been introduced in the respective research areas, and their connections have been investigated in recent work. Optimal repairs preserve a maximal amount of consequences, but they may not always exist. We show that, if they exist, then they can be obtained by certain pseudo-contraction operations, and thus they comply with the postulates that these operations satisfy. Conversely, under certain conditions, pseudo-contractions are guaranteed to produce optimal repairs.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {983–990},
numpages = {8},
keywords = {description logic, ontology repair, belief change},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3550355.3552421,
author = {Saini, Rijul and Mussbacher, Gunter and Guo, Jin L. C. and Kienzle, J\"{o}rg},
title = {Machine learning-based incremental learning in interactive domain modelling},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552421},
doi = {10.1145/3550355.3552421},
abstract = {In domain modelling, practitioners manually transform informal requirements written in natural language (problem descriptions) to more concise and analyzable domain models expressed with class diagrams. With automated domain modelling support using existing approaches, manual modifications may still be required in extracted domain models and problem descriptions to make them more accurate and concise. For example, educators teaching software engineering courses at universities usually use an incremental approach to build modelling exercises to restrict students in using intended modelling patterns. These modifications result in the evolution of domain modelling exercises over time. To assist practitioners in this evolution, a synergy between interactive support and automated domain modelling is required. In this paper, we propose a bot-assisted approach to allow practitioners perform domain modelling quickly and interactively. Furthermore, we provide an incremental learning strategy empowered by machine learning to improve the accuracy of the bot's suggestions and extracted domain models by analyzing practitioners' decisions over time. We evaluate the performance of our bot using test problem descriptions which shows that practitioners can expect to get useful support from the bot when applied to exercises of similar size and complexity, with precision, recall, and F2 scores over 85\%. Finally, we evaluate our incremental learning strategy where we observe a reduction in the required manual modifications by 70\% and an improvement of F2 scores of extracted domain models by 4.2\% when using our proposed approach and learning strategy together.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {176–186},
numpages = {11},
keywords = {natural language processing (NLP), machine learning (ML), incremental learning, evolution, domain models, decisions, bot},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3571788.3571797,
author = {Georges, Thomas and Rice, Liam and Huchard, Marianne and K\"{o}nig, M\'{e}lanie and Nebut, Cl\'{e}mentine and Tibermacine, Chouki},
title = {Guiding Feature Models Synthesis from User-Stories: An Exploratory Approach},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571797},
doi = {10.1145/3571788.3571797},
abstract = {User-stories are commonly used to define requirements in agile project management. In Software Product Lines (SPL), a user-story corresponds to a feature description (or part of it), that can be shared by several products. In practice, large SPL include a huge number of user-stories, making variability hard to grasp and handle. In this paper we present an exploratory approach that aims to guide the synthesis of Feature Models that capture and structure the commonalities and the variability expressed in these user-stories. The built Feature Models aim to help the project understanding, maintenance and evolution. Our approach first decomposes the user-stories to extract the roles and the features, using natural language processing techniques. In a second step, we group user-stories having the same topics thanks to a clustering method. This contributes to extract more general features. In a third step, we leverage the use of Formal Concept Analysis to extract logical constraints between the features that guide Feature Model synthesis. We illustrate our approach using a dataset from our industrial partner.},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {65–70},
numpages = {6},
keywords = {User Story, Software Product Line, SPL domain engineering, Reengineering, Natural Language Processing, Formal Concept Analysis, Feature Model, Agile Process},
location = {Odense, Denmark},
series = {VaMoS '23}
}

@inproceedings{10.1145/3366424.3384363,
author = {Degbelo, Auriol and Sherpa, Ang},
title = {Open Geodata Reuse: Towards Natural Language Interfaces to Web APIs},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3384363},
doi = {10.1145/3366424.3384363},
abstract = {Open Government datasets have been flooding the Web recently, and Application Programming Interfaces (APIs) are key to the development of services on top of these datasets. An issue of current APIs worldwide is that their learnability is limited. This work has explored the potential of querying APIs using natural language terms to mitigate that issue. A user study with 20 participants has demonstrated that a natural-language-based, along with an order-agnostic approach to API design can produce easily learnable APIs for both novice and experienced API users. These insights can pave the way for a paradigm change on Web-API design for open geodata retrieval and beyond.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {703–710},
numpages = {8},
keywords = {Smart Cities, Open Government Data, Geographic Information Retrieval, API Usability, API Learnability, API Design Conventions},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3677614,
author = {Razin, Yosef S. and Feigh, Karen M.},
title = {Converging Measures and an Emergent Model: A Meta-Analysis of Human-Machine Trust Questionnaires},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
url = {https://doi.org/10.1145/3677614},
doi = {10.1145/3677614},
abstract = {Trust is crucial for technological acceptance, continued usage, and teamwork. However, human-robot trust, and human-machine trust more generally, suffer from terminological disagreement and construct proliferation. By comparing, mapping, and analyzing well-constructed trust survey instruments, this work uncovers a consensus structure of trust in human–machine interaction. To do so, we identify the most frequently cited and best-validated human-machine and human-robot trust questionnaires as well as the best-established factors that form the dimensions and antecedents of such trust. To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between questionnaires. Furthermore, we perform a meta-analysis of the regression models which emerged from the experiments that employed multi-factorial survey instruments. Based on this meta-analysis, we provide the most complete, experimentally validated model of human-machine and human-robot trust to date. This convergent model establishes an integrated framework for future research. It determines the current boundaries of trust measurement and where further investigation and validation are necessary. We close by discussing how to choose an appropriate trust survey instrument and how to design for trust. By identifying the internal workings of trust, a more complete basis for measuring trust is developed that is widely applicable.},
journal = {J. Hum.-Robot Interact.},
month = nov,
articleno = {58},
numpages = {41},
keywords = {Human-Robot Trust, Shared Mental Models}
}

@inproceedings{10.1145/3550356.3559576,
author = {B\'{u}r, M\'{a}rton and Stirewalt, Kurt},
title = {ORM ontologies with executable derivation rules to support semantic search in large-scale data applications},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3559576},
doi = {10.1145/3550356.3559576},
abstract = {A semantic layer maps complex enterprise data into an ontology with abstract business concepts that are well-known to business users. Chief data officers invest significant effort to create and update these ontologies, while data scientists do feature engineering by combining already existing concepts and features of the domain. However, it is a significant challenge to catalogue and maintain the numerous features pertaining to an ontology, which leads to duplicated features and unnecessary complexity. In this work, we propose to combine ontologies captured using the Object-Role Modeling notation with derivation rules defined in a datalog-like language called Rel, which allows the creation of a semantic layer with feature search capability. Our prototype framework uses the RAI Knowledge Graph Management System, which provides automated and incremental derivation rule evaluation.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {81–82},
numpages = {2},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.5555/3213214.3213223,
author = {Amissah, Matthew and Toba, Ange-Lionel and Handley, Holly A. H. and Seck, Mamadou},
title = {Towards a framework for executable systems modeling: an executable systems modeling language (ESysML)},
year = {2018},
isbn = {9781510860186},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The Systems Modeling Language (SysML), which is the de-facto modeling standard in the systems engineering community, consists of a number of independently derived methodologies (i.e. state charts, activity diagrams etc.) which have been co-opted into a single modeling framework. This and the lack of an overarching meta-model that specifies relationships and rules governing the various language constructs precludes their uniform application across diagram types. This has resulted in a large unwieldy and at best semi-formal language specification, with adverse implications for interoperability of modeling tools and model execution. This paper presents an executable language that re-factors the SysML language schema and offers an equivalent textual syntax for model specification in tandem with the existing graphical syntax. This is aimed at supporting the development of time based simulation models useful for decision support and architecture verification and validation in systems engineering.},
booktitle = {Proceedings of the Model-Driven Approaches for Simulation Engineering Symposium},
articleno = {9},
numpages = {12},
keywords = {model driven engineering, model based systems engineering, SysML},
location = {Baltimore, Maryland},
series = {Mod4Sim '18}
}

@inproceedings{10.1145/3589334.3645567,
author = {El Husseini, Wafaa and El Vaigh, Cheikh Brahim and Goasdou\'{e}, Fran\c{c}ois and Jaudoin, H\'{e}l\`{e}ne},
title = {Query Optimization for Ontology-Mediated Query Answering},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645567},
doi = {10.1145/3589334.3645567},
abstract = {Ontology-mediated query answering (OMQA) consists in asking database queries on knowledge bases (KBs); a KB is a set of facts called the KB's database, which is described by domain knowledge called the KB's ontology. A widely-investigated OMQA technique is FO-rewriting: every query asked on a KB is reformulated w.r.t. the KB's ontology, so that its answers are computed by the relational evaluation of the query reformulation on the KB's database. Crucially, because FO-rewriting compiles the domain knowledge relevant to queries into their reformulations, query reformulations may be complex and their optimization is the crux of efficiency. We devise a novel optimization framework for a large set of OMQA settings that enjoy FO-rewriting: conjunctive queries, i.e., the core select-project-join queries, asked on KBs expressed using datalog+/-, description logics, existential rules, OWL, or RDFS. We optimize the query reformulations produced by state-of-the-art FO-rewriting algorithms by computing rapidly, with the help of a KB's database summary, simpler (contained) queries with the same answers that can be evaluated faster by RDBMSs. We show on a well-established OMQA benchmark that time performance is significantly improved by our optimization framework in general, up to three orders of magnitude.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {2138–2148},
numpages = {11},
keywords = {data summarization, existential rules, query optimization},
location = {Singapore, Singapore},
series = {WWW '24}
}

@proceedings{10.1145/3620666,
title = {ASPLOS '24: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
abstract = {Welcome to the third volume of ASPLOS'24: the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. This document is mostly dedicated to the 2024 fall cycle but also provides some statistics summarizing all three cycles.We introduced several notable changes to ASPLOS this year, most of which were discussed in our previous messages from program chairs in Volume 1 and 2, including: (1) significantly increasing the program committee size to over 220 members (more than twice the size of last year); (2) foregoing synchronous program committee (PC) meetings and instead making all decisions online; (3) overhauling the review assignment process; (4) developing an automated submission format violation identifier script that uncovers, e.g., disallowed vertical space manipulations that "squeeze" space; (5) introducing the new ASPLOS role of Program Vice Chairs to cope with the increased number of submissions and the added load caused by foregoing synchronous program committee; and (6) characterizing a systematic problem that ASPLOS is facing in reviewing quantum computing submissions, describing how we addressed it and highlighting how we believe that it should be handled in the future.Assuming readers have read our previous messages, here, we will only describe differences between the current cycle and the previous ones. These include: (1) Finally unifying submission and acceptance paper formatting instructions (forgoing the `jpaper' class) to rid authors of accepted papers from the need to reformat; (2) Describing the methodology we employed to select best papers, which we believe ensures quality and hope will persist; and (3) Reporting the ethical incidents we encountered and how we handled them. In the final, fourth volume, when the outcome of the ASPLOS'24 fall major revisions will become known, we plan to conduct a broader analysis of all the data we have gathered throughout the year.Following are some key statistics of the fall cycle: 340 submissions were finalized (43\% more than last year's fall count and 17\% less than our summer cycle) of which 111 are related to accelerators/FPGAs/GPUs, 105 to machine learning, 54 to security, 50 to datacenter/cloud and 50 to storage/memory; 183 (54\%) submissions were promoted to the second review round; 39 (11.5\%) papers were accepted (of which 19 were awarded artifact evaluation badges); 33 (9.7\%) submissions were allowed to submit major revisions and are currently under review (these will be addressed in the fourth volume of ASPLOS'24 and will be presented in ASPLOS'25 if accepted); 1,368 reviews were uploaded; and 4,949 comments were generated during online discussions, of which 4,070 were dedicated to the submissions that made it to the second review round.This year, in the submission form, we asked authors to specify which of the three ASPLOS research areas are related to their submitted work. Analyzing this data revealed that 80\%, 39\%, and 29\% of the submissions are categorized by their authors as related to architecture, operating systems, and programming languages, respectively, generating the highest difference we have observed across the cycles between architecture and the other two. About 46\% of the fall submissions are "interdisciplinary," namely, were associated with two or more of the three areas.Overall, throughout all the ASPLOS'24 cycles, we received 922 submissions, constituting a 1.54x increase compared to last year. Our reviewers submitted a total of 3,634 reviews containing more than 2.6 million words, and we also generated 12,655 online comments consisting of nearly 1.2 million words. As planned, PC members submitted an average of 15.7 reviews and a median of 15, and external review committee (ERC) members submitted an average of 4.7 and a median of 5.We accepted 170 papers thus far, written by 1100 authors, leading to an 18.4\% acceptance rate, with the aforementioned 33 major revisions still under review. Assuming that the revision acceptance rate will be similar to that of previous cycles, we estimate that ASPLOS'24 will accept nearly 200 (!) papers, namely, 21\%–22\% of the submissions.The ASPLOS'24 program consists of 193 papers: the 170 papers we accepted thus far and, in addition, 23 major revisions from the fall cycle of ASPLOS'23, which were re-reviewed and accepted. The full details are available in the PDF of the front matter.},
location = {La Jolla, CA, USA}
}

@article{10.1145/3579839,
author = {Zaji, Amirhossein and Liu, Zheng and Bando, Takashi and Zhao, Lihua},
title = {Ontology-Based Driving Simulation for Traffic Lights Optimization},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {2157-6904},
url = {https://doi.org/10.1145/3579839},
doi = {10.1145/3579839},
abstract = {Traffic lights optimization is one of the principal components to lessen the traffic flow and travel time in an urban area. The present article seeks to introduce a novel procedure to design the traffic lights in a city using evolutionary-based optimization algorithms in combination with an ontology-based driving behavior simulation framework. Accordingly, an ontology-based knowledge base is introduced to provide a machine-understandable knowledge of roads and intersections, traffic rules, and driving behaviors. Then, a simulation environment is developed to inspect car behavior in real time. To optimize the traffic lights, a sine-based equation was defined for each traffic light, and the total travel time of the vehicles was considered as the cost function in the optimization algorithm. The optimization was performed with 5, 10, 15, 20, 25, and 30 vehicles in the urban areas. Based on the results, in contrast to uncontrolled intersections without traffic lights, optimized traffic lights can significantly contribute to total travel time-saving. To conclude, due to an escalation in the number of vehicles, the significance of optimized traffic lights has encountered an increase, and unoptimized traffic lights could increase total travel time even more than a city deprived of any traffic light.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = mar,
articleno = {39},
numpages = {26},
keywords = {traffic light, autonomous car, driving behavior, knowledge representation, ontology, Evolutionary optimization}
}

@article{10.14778/3611540.3611634,
author = {Halevy, Alon and Choi, Yejin and Floratou, Avrilia and Franklin, Michael J. and Noy, Natasha and Wang, Haixun},
title = {Will LLMs Reshape, Supercharge, or Kill Data Science? (VLDB 2023 Panel)},
year = {2023},
issue_date = {August 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3611540.3611634},
doi = {10.14778/3611540.3611634},
abstract = {Large language models (LLMs) have recently taken the world by storm, promising potentially game changing opportunities in multiple fields. Naturally, there is significant promise in applying LLMs to the management of structured data, or more generally, to the processes involved in data science. At the very least, LLMs have the potential to provide substantial advancements in long-standing challenges that our community has been tackling for decades. On the other hand, they may introduce completely new capabilities that we have only dreamed of thus far. This panel will bring together a few leading experts who have been thinking about these opportunities from various perspectives and fielding them in research prototypes and even in commercial applications.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {4114–4115},
numpages = {2}
}

@inproceedings{10.1145/3640794.3665537,
author = {Seaborn, Katie and Gessinger, Iona and Yoshida, Suzuka and Cowan, Benjamin R. and Doyle, Philip R.},
title = {Cross-Cultural Validation of Partner Models for Voice User Interfaces},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640794.3665537},
doi = {10.1145/3640794.3665537},
abstract = {Recent research has begun to assess people’s perceptions of voice user interfaces (VUIs) as dialogue partners, termed partner models. Current self-report measures are only available in English, limiting research to English-speaking users. To improve the diversity of user samples and contexts that inform partner modelling research, we translated, localized, and evaluated the Partner Modelling Questionnaire (PMQ) for non-English speaking Western (German, n=185) and East Asian (Japanese, n=198) cohorts where VUI use is popular. Through confirmatory factor analysis (CFA), we find that the scale produces equivalent levels of “goodness-to-fit” for both our German and Japanese translations, confirming its cross-cultural validity. Still, the structure of the communicative flexibility factor did not replicate directly across Western and East Asian cohorts. We discuss how our translations can open up critical research on cultural similarities and differences in partner model use and design, whilst highlighting the challenges for ensuring accurate translation across cultural contexts.},
booktitle = {Proceedings of the 6th ACM Conference on Conversational User Interfaces},
articleno = {19},
numpages = {10},
keywords = {conversational user interfaces, cross-cultural research, human-computer interaction, human-machine dialogue, mental models, partner models, speech interfaces, voice user interfaces},
location = {Luxembourg, Luxembourg},
series = {CUI '24}
}

@inproceedings{10.1145/3711542.3711580,
author = {Jiang, Xiaorui and Khan, Kulsoom and Vasantha, Sumithra Thinakara and Haider, Sajjad},
title = {Evidence Extraction for Automated Medical Coding: Preliminary Evaluation},
year = {2025},
isbn = {9798400717383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711542.3711580},
doi = {10.1145/3711542.3711580},
abstract = {Coding clinical texts in standard language such as ICD is an important but tedious and error-prone process. Automated medical coding algorithms suffer problems due to the combined the challenge of handling the significant length of clinical text, the complexity of the huge code hierarchy and the lack of interpretability to ensure user trust. Large language models (LLM) have also been proven struggling with this task in recent studies. Recent efforts have been made to annotate an evidence-supported medical coding dataset. The current study makes the first empirical investigation into how well (small) fine-tuned pretrained language models (PLM) and LLMs could identify the sentences containing medical evidence supporting the assigned codes. Hierarchical sequential sentence classification and GPT-3.5 in the zero-shot setting were tested for evidence sentence extraction. Extra evaluation was performed to investigate how evidence extraction impacts clinical coding and what implications it has towards the future generation algorithms for automated medical coding.},
booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval},
pages = {18–23},
numpages = {6},
keywords = {ICD coding, code evidence, sequential sentence classification, large language model},
location = {
},
series = {NLPIR '24}
}

@inproceedings{10.1145/3706890.3706997,
author = {Nan, Beier and Gu, Jinguang and Qiu, Chen and Wu, Jingyun},
title = {Construction and application of medical history knowledge graph based on UIE model fine-tuning},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3706997},
doi = {10.1145/3706890.3706997},
abstract = {Objective: To achieve the automated extraction of complex medical history knowledge from Chinese electronic medical records, a fine-tuned UIE extraction model is utilized to automatically obtain medical history knowledge and construct a knowledge graph (KG) of medical histories. Method: Taking the current medical history as an example, a fundamental knowledge base of Chinese medical history was first built. Then, based on this knowledge base, a training set was annotated, and the UIE model was fine-tuned using this training set. The fine-tuned UIE model was then used to extract medical history knowledge, which was processed and stored to generate a KG of medical histories. Results: The fine-tuned UIE model achieved entity, relationship, and event extraction tasks. Subsequently, the extracted medical history information was processed and stored, successfully constructing a KG of the current medical history. Conclusion: This method realizes the completion of entity, relation, and event extraction tasks by training only one model, efficiently achieving automatic extraction of specified medical history knowledge from Chinese electronic medical records and constructing a KG of medical histories. It helps organize and analyze complex medical history knowledge for clinical use, offering practical value.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {621–626},
numpages = {6},
keywords = {Constructing KG, Information extraction, Medical history knowledge, Model fine-tuning},
location = {
},
series = {ISAIMS '24}
}

@article{10.1613/jair.1.13939,
author = {\"{O}zcep, \"{O}zg\"{u}r L\"{u}tf\"{u} and Leemhuis, Mena and Wolter, Diedrich},
title = {Embedding Ontologies in the Description Logic ALC by Axis-Aligned Cones},
year = {2024},
issue_date = {Jan 2024},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {78},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13939},
doi = {10.1613/jair.1.13939},
abstract = {This paper is concerned with knowledge graph embedding with background knowledge, taking the formal perspective of logics. In knowledge graph embedding, knowledge— expressed as a set of triples of the form (a R b) (“a is R-related to b”)—is embedded into a real-valued vector space. The embedding helps exploiting geometrical regularities of the space in order to tackle typical inductive tasks of machine learning such as link prediction. Recent embedding approaches also consider incorporating background knowledge, in which the intended meanings of the symbols a, R, b are further constrained via axioms of a theory. Of particular interest are theories expressed in a formal language with a neat semantics and a good balance between expressivity and feasibility. In that case, the knowledge graph together with the background can be considered to be an ontology. This paper develops a cone-based theory for embedding in order to advance the expressivity of the ontology: it works (at least) with ontologies expressed in the description logic ALC, which comprises restricted existential and universal quantifiers, as well as concept negation and concept disjunction. In order to align the classical Tarskian Style semantics for ALC with the sub-symbolic representation of triples, we use the notion of a geometric model of an ALC ontology and show, as one of our main results, that an ALC ontology is satisfiable in the classical sense iff it is satisfiable by a geometric model based on cones. The geometric model, if treated as a partial model, can even be chosen to be faithful, i.e., to reflect all and only the knowledge captured by the ontology. We introduce the class of axis-aligned cones and show that modulo simple geometric operations any distributive logic (such as ALC) interpreted over cones employs this class of cones. Cones are also attractive from a machine learning perspective on knowledge graph embeddings since they give rise to applying conic optimization techniques.},
journal = {J. Artif. Int. Res.},
month = jan,
numpages = {51}
}

@inproceedings{10.1145/3511808.3557184,
author = {Paulus, Alexander and Burgdorf, Andreas and Langer, Tristan and Pomp, Andr\'{e} and Meisen, Tobias and Pol, Sebastian},
title = {PLASMA: A Semantic Modeling Tool for Domain Experts},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557184},
doi = {10.1145/3511808.3557184},
abstract = {In recent years, Knowledge Graphs and Ontology-based Data Management have proven to be particularly effective in the efficient management and consolidation of heterogeneous data sources. In this context, semantic modeling has proven to be a useful approach for creating semantic data annotations. However, automatically generated semantic models usually need to be revised by a domain expert, who is often not familiar with semantic technologies. For addressing this issue, we propose the PLASMA semantic modeling tool, which aims at enabling domain experts to build semantic models from scratch or refine models created by automatic algorithms. We demonstrate the use of the tool and its user interface in two different semantic data management use cases for integrating smart city data in a public funded project, called City Dataspace, and for creating semantic models in an industrial use case at Siemens AG.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {4946–4950},
numpages = {5},
keywords = {semantic refinement, semantic modeling, resource description framework, graphical user interface},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{10.1145/3447735,
author = {Darwish, Kareem and Habash, Nizar and Abbas, Mourad and Al-Khalifa, Hend and Al-Natsheh, Huseein T. and Bouamor, Houda and Bouzoubaa, Karim and Cavalli-Sforza, Violetta and El-Beltagy, Samhaa R. and El-Hajj, Wassim and Jarrar, Mustafa and Mubarak, Hamdy},
title = {A panoramic survey of natural language processing in the Arab world},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/3447735},
doi = {10.1145/3447735},
journal = {Commun. ACM},
month = mar,
pages = {72–81},
numpages = {10}
}

@inproceedings{10.1145/3724154.3724349,
author = {Gao, Juan and Guo, Xiangyun and Xiang, Tianqi},
title = {Research on the Evolutionary Path of Public Opinion in Networked Emergencies Based on Large Models——Taking the Gansu earthquake as an example},
year = {2025},
isbn = {9798400711862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724154.3724349},
doi = {10.1145/3724154.3724349},
abstract = {Construct a rational map of online public opinion on emergencies, reveals the evolutionary paths and developmental contexts of public opinion incidents, providing decision support for government departments in managing and guiding public sentiments. Select Sina Weibo as the data source, use octopus collector to collect Gansu earthquake Weibo data, and then perform data preprocessing. The causal relationships of the events were determined based on a rule-based template matching method, and a multi-round dialogue mechanism based on large language models was used to automatically extract causal event pairs. The K-means++ clustering algorithm was employed to generalize the events and construct a network public opinion event logic graph, analyzing the evolutionary paths of online public sentiments. The research findings indicate that public sentiment regarding the Gansu earthquake is characterized by multiple levels and complex causal relationships. The involvement of multiple causal chains contributes to a high degree of complexity in the evolution of public sentiment. By constructing an event logic graph, the development process of the Gansu earthquake public opinion event has been demonstrated, which can provide technical support for real-time monitoring, in-depth analysis, and precise response of public opinion.},
booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management},
pages = {1206–1211},
numpages = {6},
keywords = {Multiple rounds of dialogue, emergencies, evolutionary pathways, large model, online public opinion},
location = {
},
series = {BDEIM '24}
}

@article{10.1145/3640314,
author = {Zahid, Maryam and Bucaioni, Alessio and Flammini, Francesco},
title = {Model-based Trustworthiness Evaluation of Autonomous Cyber-Physical Production Systems: A Systematic Mapping Study},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3640314},
doi = {10.1145/3640314},
abstract = {The fourth industrial revolution, i.e., Industry 4.0, is associated with Cyber-Physical Systems (CPS), which are entities integrating hardware (e.g., smart sensors and actuators connected through the Industrial Internet of Things) together with control and analytics software used to drive and support decisions at several levels. The latest developments in Artificial Intelligence (AI) and Machine Learning (ML) have enabled increased autonomy and closer human-robot cooperation in the production and manufacturing industry, thus leading to Autonomous Cyber-Physical Production Systems (ACPPS) and paving the way to the fifth industrial revolution (i.e., Industry 5.0). ACPPS are increasingly critical due to the possible consequences of their malfunctions on human co-workers, and therefore, evaluating their trustworthiness is essential. This article reviews research trends, relevant attributes, modeling languages, and tools related to the model-based trustworthiness evaluation of ACPPS. As in many other engineering disciplines and domains, model-based approaches, including stochastic and formal analysis tools, are essential to master the increasing complexity and criticality of ACPPS and to prove relevant attributes such as system safety in the presence of intelligent behaviors and uncertainties.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {157},
numpages = {28},
keywords = {Autonomous cyber-physical production systems, cyber-physical manufacturing systems, industry 4.0, industry 5.0, automation, trustworthiness, models, mapping study}
}

@proceedings{10.1145/3727582,
title = {PROMISE '25: Proceedings of the 21st International Conference on Predictive Models and Data Analytics in Software Engineering},
year = {2025},
isbn = {9798400715945},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Trondheim, Norway}
}

@article{10.1109/TASLP.2024.3497586,
author = {Ma, Hao and Peng, Zhiyuan and Li, Xu and Shao, Mingjie and Wu, Xixin and Liu, Ju},
title = {CLAPSep: Leveraging Contrastive Pre-Trained Model for Multi-Modal Query-Conditioned Target Sound Extraction},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3497586},
doi = {10.1109/TASLP.2024.3497586},
abstract = {Universal sound separation (USS) aims to extract arbitrary types of sounds from real-world recordings. This can be achieved by language-queried target sound extraction (TSE), which typically consists of two components: a query network that converts user queries into conditional embeddings, and a separation network that extracts the target sound accordingly. Existing methods commonly train models from scratch. As a consequence, substantial data and computational resources are required to make the randomly initialized model comprehend sound events and perform separation accordingly. In this paper, we propose to integrate pre-trained models into TSE models to address the above issue. To be specific, we tailor and adapt the powerful contrastive language-audio pre-trained model (CLAP) for USS, denoted as CLAPSep. CLAPSep also accepts flexible user inputs, taking both positive and negative user prompts of uni- and/or multi-modalities for target sound extraction. These key features of CLAPSep can not only enhance the extraction performance but also improve the versatility of its application. We provide extensive experiments on 5 diverse datasets to demonstrate the superior performance and zero- and few-shot generalizability of our proposed CLAPSep with fast training convergence, surpassing previous methods by a significant margin. Full codes and some audio examples are released for reproduction and evaluation.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {4945–4960},
numpages = {16}
}

@inproceedings{10.1145/3701716.3717819,
author = {Jia, Runsong and Zhang, Bowen and M\'{e}ndez, Sergio Jos\'{e} Rodr\'{\i}guez and Omran, Pouya G.},
title = {StructRAG: Structure-Aware RAG Framework with Scholarly Knowledge Graph for Diverse Question Answering},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717819},
doi = {10.1145/3701716.3717819},
abstract = {Recent advances in Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) have shown promise in academic question answering. However, existing approaches often fail to fully utilize document structural information and lack diversity in retrieved contexts. This paper presents StructRAG, a structure-aware RAG framework that leverages scholarly knowledge graphs for enhanced question answering. Our framework features three key innovations: (1) an automated knowledge graph construction pipeline based on Deep Document Model (DDM) that preserves document hierarchical structure, (2) a structure-aware retrieval mechanism that combines semantic relevance with source diversity, and (3) a context-enhanced generation approach that integrates structural metadata for improved answer synthesis. Experimental results on 329 computer science papers demonstrate that StructRAG significantly outperforms vanilla RAG baseline. While maintaining comparable semantic accuracy (91\% vs 90\%), our approach achieves substantially higher diversity in generated answers (Distinct-1: 62\% vs 52\%, Distinct-2: 89\% vs 78\%) and better answer quality across all metrics, with notable improvements in relevance (29\%) and readability (36.5\%). These results demonstrate that StructRAG effectively enhances both the diversity and quality of academic question answering.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2567–2573},
numpages = {7},
keywords = {deep document model, knowledge graph, knowledge graph construction, large language models, retrieval-augmented generation},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3405962.3405983,
author = {Stefanidis, Dimosthenis and Christodoulou, Chrysovalantis and Symeonidis, Moysis and Pallis, George and Dikaiakos, Marios and Pouis, Loukas and Orphanou, Kalia and Lampathaki, Fenareti and Alexandrou, Dimitrios},
title = {The ICARUS Ontology: A general aviation ontology developed using a multi-layer approach},
year = {2020},
isbn = {9781450375429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405962.3405983},
doi = {10.1145/3405962.3405983},
abstract = {The management of aviation data is a great challenge in the aviation industry, as they are complex and can be derived from heterogeneous data sources. To handle this challenge, ontologies can be applied to facilitate the modelling of the data across multiple data sources. This paper presents an aviation domain ontology, the ICARUS ontology, which aims at facilitating the semantic description and integration of information resources that represent the various assets of the ICARUS platform and their use. To present the functionality and usability of the proposed ontology, we present the results of querying the ontology using SPARQL queries through three use case scenarios. As shown from the evaluation, the ICARUS ontology enables the integration and reasoning over multiple sources of heterogeneous aviation-related data, the semantic description of metadata produced by ICARUS, and their storage in a knowledge-base which is dynamically updated and provides access to its contents via SPARQL queries.},
booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
pages = {21–32},
numpages = {12},
keywords = {services, queries, ontology, datasets, aviation},
location = {Biarritz, France},
series = {WIMS 2020}
}

@inproceedings{10.1145/3360901.3364449,
author = {Vu, Binh and Pujara, Jay and Knoblock, Craig A.},
title = {D-REPR: A Language for Describing and Mapping Diversely-Structured Data Sources to RDF},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364449},
doi = {10.1145/3360901.3364449},
abstract = {Publishing data sources to knowledge graphs is a complicated and laborious process as data sources are often heterogeneous, hierarchical and interlinked. As an example, food price datasets may contain product prices of various units at different markets and times, and different providers can have many choices of formats such as CSV, JSON or spreadsheet. Beyond data formats, these datasets may have differing layout, where one dataset may be organized as a row-based table or relational table (prices are in one column), while another may use a matrix table (prices are in one matrix). To address these problems, we present a novel data description language for mapping datasets to RDF. In particular, our language supports specifying the locations of source attributes in the sources, mapping of the attributes to ontologies, and simple rules to join the data of these attributes to output final RDF triples. Unlike existing approaches, our language is not restricted to specific data layouts such as the Nested Relational Model, or to specific data formats, such as spreadsheet. Our broad data description language presents a format-independent solution, allowing interlinking among multiple heterogeneous sources and representing many diverse data structures that existing tools are unable to handle.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {189–196},
numpages = {8},
keywords = {rdf mapping, linked data, knowledge graph},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1145/3622896.3622922,
author = {Li, Min and Xu, Jianliang and Wu, Xiaoquan},
title = {Construction of Traditional Culture Ontology Based on Representation and Role},
year = {2023},
isbn = {9798400708190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622896.3622922},
doi = {10.1145/3622896.3622922},
abstract = {Traditional culture refers to a culture that has evolved from civilization and can reflect the characteristics and spirit of a nation. However, at present, the traditional cultural ontology only focuses on modeling and data organization of a certain type of traditional culture, which is a certain distance from the massive cultural information and urgent cultural needs. This article starts from the perspective of protecting and inheriting traditional culture, and based on representation, incorporates role theory to construct a cultural ontology centered on poetry, calligraphy, and painting. The traditional cultural ontology constructed in this article can further contribute to the field of artificial intelligence.},
booktitle = {Proceedings of the 2023 4th International Conference on Control, Robotics and Intelligent System},
pages = {156–159},
numpages = {4},
keywords = {traditional culture, role theory, representation ontology},
location = {Guangzhou, China},
series = {CCRIS '23}
}

@inproceedings{10.1145/3706890.3706971,
author = {Man, Jianping and Hu, Zhensheng and Liu, Hongze and Yang, Rui and Liu, Jingjing and Chen, Ziyi and Zhou, Yi},
title = {A Model for Epilepsy Named Entity Recognition Based on Chinese EEG Reports},
year = {2025},
isbn = {9798400717826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706890.3706971},
doi = {10.1145/3706890.3706971},
abstract = {It is critical for clinical diagnosis and research to extract and utilize the medical information from electroencephalogram (EEG) reports of epilepsy. With the widespread application of deep learning techniques in health care, particularly in natural language processing tasks, named entity recognition (NER) has emerged as an essential tool for information extraction from medical text. This study proposed an epilepsy NER model for EEG reports to improve the efficiency of epilepsy text processing. 17,606 paragraphs from real Chinese epilepsy EEG reports as data samples, were meticulously annotated with 17 entity types by clinical experts. The model used the BERT and the Global Pointer (GP) algorithm to identify nested and non-nested entities, achieved outstanding performance in the Precision, Recall and F1 score. The experimental results demonstrate that our method significantly enhances the effectiveness of epilepsy entity recognition, providing robust support for the automated extraction and analysis of medical information.},
booktitle = {Proceedings of the 2024 5th International Symposium on Artificial Intelligence for Medicine Science},
pages = {467–472},
numpages = {6},
keywords = {BERT model, EEG reports, Epilepsy, Global Pointer algorithm, Named entity recognition},
location = {
},
series = {ISAIMS '24}
}

@inproceedings{10.1145/3299819.3299830,
author = {Delaney, Steven and Chan, Christopher Chun Ki and Smith, Doug},
title = {Natural Language Processing for Productivity Metrics for Software Development Profiling in Enterprise Applications},
year = {2018},
isbn = {9781450366236},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299819.3299830},
doi = {10.1145/3299819.3299830},
abstract = {In this paper, we utilize ontology-based information extraction for semantic analysis and terminology linking from a corpus of software requirement specification documents from 400 enterprise-level software development projects. The purpose for this ontology is to perform semi-supervised learning on enterprise-level specification documents towards an automated method of defining productivity metrics for software development profiling. Profiling an enterprise-level software development project in the context of productivity is necessary in order to objectively measure productivity of a software development project and to identify areas of improvement in software development when compared to similar software development profiles or benchmark of these profiles. We developed a semi-novel methodology of applying NLP OBIE techniques towards determining software development productivity metrics, and evaluated this methodology on multiple practical enterprise-level software projects.},
booktitle = {Proceedings of the 2018 Artificial Intelligence and Cloud Computing Conference},
pages = {83–87},
numpages = {5},
keywords = {Software Development Productivity, Software Development, Natural Language Processing},
location = {Tokyo, Japan},
series = {AICCC '18}
}

@inproceedings{10.1145/3556223.3556262,
author = {Tramontana, Emiliano and Verga, Gabriella},
title = {Keeping Researchers Updated by Automatically Enriching an Ontology in the Medical Field},
year = {2022},
isbn = {9781450396349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3556223.3556262},
doi = {10.1145/3556223.3556262},
abstract = {Ontologies provide a common and standard dictionary of terms in some domain for researchers to easily exchange data. Forming an ontology requires several years of work performed by human experts, and an ontology for a given domain is thought to be stable for many years. Nevertheless, as scientific articles are continuously published to gather knowledge on recent findings, existing ontologies risk becoming stale, or require further human effort. Moreover, searching new articles without referring to an ontology can be very time consuming and confusing, especially for novice researchers. We propose an approach for automatically relating newly available published articles to existing ontologies. By automatically selecting relevant scientific articles and making them appear besides other data in an ontology, we aim at supporting experienced and novice researchers. Therefore, as knowledge grows and articles are available, the ontology used by researchers will also be automatically connected, allowing them to readily discover new findings. To validate the effectiveness of the proposed approach, we have enriched OBIB, an ontology for biobanking, with a selection of articles extracted from PubMed. The approach is general enough and can be applied to other ontologies or publishers.},
booktitle = {Proceedings of the 10th International Conference on Computer and Communications Management},
pages = {257–262},
numpages = {6},
keywords = {e-learning, PubMed, Ontology enrichment, Ontologies, Knowledge management},
location = {Okayama, Japan},
series = {ICCCM '22}
}

@article{10.1145/3594723,
author = {Koho, Mikko and Coladangelo, L. P. and Ransom, Lynn and Emery, Doug},
title = {Wikibase Model for Premodern Manuscript Metadata Harmonization, Linked Data Integration, and Discovery},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3594723},
doi = {10.1145/3594723},
abstract = {To facilitate discovery of premodern manuscripts in U.S. memory institutions, Digital Scriptorium, a growing consortium of over 35 institutional members representing American libraries, museums, and other cultural heritage institutions, has developed a digital platform for an online national union catalog. The platform will allow low-barrier and efficient collection, aggregation, and enrichment of member metadata and sustainably publish it as Linked Open Data. This article describes the methods and principles behind the data model development and the decision to use Wikibase. The results of the prototype implementation and testing phase demonstrate the practicality and sustainability of Digital Scriptorium’s approach to building an online national union catalog based on Linked Open Data technologies and practices.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {56},
numpages = {25},
keywords = {semantic web, cultural heritage, digital humanities, premodern manuscripts, data interoperability, Wikibase, data modeling, Linked Open Data}
}

@proceedings{10.1145/3699682,
title = {UMAP '25: Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
year = {2025},
isbn = {9798400713132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inbook{10.1145/3191315.3191325,
author = {Christiansen, Henning and Dahl, Ver\'{o}nica},
title = {Natural language processing with (tabled and constraint) logic programming},
year = {2018},
isbn = {9781970001990},
publisher = {Association for Computing Machinery and Morgan \&amp; Claypool},
url = {https://doi.org/10.1145/3191315.3191325},
abstract = {We survey the evolution of natural language processing as it relates to Logic Programming, with particular focus on David Scott Warren's crucial contributions such as tabling, and the relationship with hypothetical reasoning and constraint based programming. These topics lead naturally to a view of parsing as constraint solving, which extends to grammar inference. Our exposition of the subject is intuitive and example-driven, with references to more formal presentations when needed.},
booktitle = {Declarative Logic Programming: Theory, Systems, and Applications},
pages = {477–511},
numpages = {35}
}

@inproceedings{10.1145/3297280.3297661,
author = {Ojino, Ronald},
title = {User's profile ontology-based semantic model for personalized hotel room recommendation in the web of things: student research abstract},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297661},
doi = {10.1145/3297280.3297661},
abstract = {The hotel industry is not exempt from technology disruption and will only keep its competitiveness through strategy [3]including adjusting to the new customers demands, reacting to competitors' innovations and taking advantage of the new technological developments. Personalization can enable an organization to customize its offerings at the individual level. This paper seeks to design and evaluate a novel Web of Things personalization model that effectively utilizes information about a customer's preferences to create a personalized service space.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2314–2316},
numpages = {3},
keywords = {machine learning, ontology, personalization, semantics},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3427423.3427431,
author = {Togatorop, Parmonangan R. and Siagian, Rosa and Nainggolan, Yolanda and Simanungkalit, Kaleb},
title = {Implementation of ontology-based on Word2Vec and DBSCAN for part-of-speech},
year = {2021},
isbn = {9781450376051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427423.3427431},
doi = {10.1145/3427423.3427431},
abstract = {POS tagging is a process of marking text into an appropriate word-class based on word definitions and word relationships. In general, several POS tagging approaches have been applied in Bahasa Indonesia namely rule-based, stochastic, and neural. Besides, there is another approach to POS tagging which has been applied to English, namely the approach using ontology. This approach has not yet been applied to Bahasa Indonesia so we will implement an ontology to conduct POS tagging in Bahasa Indonesia. In this study, the ontology was constructed using the Word2Vec and the DBSCAN clustering method. The Word2Vec model is implemented to extract each word in vector form based on its context and the DBSCAN clustering method is implemented for the classification process of word classes based on word vectors modeled by Word2Vec. The process of POS tagging with ontology is carried out in several stages, namely: data collection using web scraping techniques from Kompas.com and Detik.com online news articles, text preprocessing, Word2Vec feature building, clustering with DBSCAN, ontology construction and evaluation. The experiments carried out in this study were to choose the optimal parameter values from DBSCAN in forming word clusters for ontology construction. Overall, the implementation of ontology with Word2Vec and DBSCAN can do POS tagging with the highest accuracy value of 0.62, the highest precision value of 0.79, the highest recall value of 0.62, and the highest f1-score of 0.67.},
booktitle = {Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology},
pages = {51–56},
numpages = {6},
keywords = {ontology, bahasa indonesia, Word2Vec, POS tagging, DBSCAN},
location = {Malang, Indonesia},
series = {SIET '20}
}

@article{10.1145/3737880,
author = {Subagdja, Budhitama and Shanthoshigaa, D. and Tan, Ah-Hwee},
title = {DisambiguART: A Neural-based Inference Model for Knowledge Graph Disambiguation},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3737880},
doi = {10.1145/3737880},
abstract = {One main challenge in constructing a Knowledge Graph (KG) is to deal with ambiguity. Specifically, an entity in the graph can be assigned with multiple meanings while two or more entities considered to have different meanings may actually be the same. Assigning an entity with the correct meaning may involve re-evaluation of its relevant contexts. This costly operation typically involves searching for other similar entities within the KG such that the context can be determined. In this article, a new model called DisambiguART is proposed leveraging multi-channel matching and inference in a self-organizing neural network for sense disambiguation in KGs. Unlike other disambiguation methods that rely on representation learning to identify the relevant contexts whereby similarities among entities are learned, DisambiguART extends the working principle of multi-channel Adaptive Resonance Theory (ART) to conduct inferences directly over the graph representation through bi-directional interactions of bottom-up activations and top-down matching to find similar entities and select the correct meaning according to the right context. The proposed method is evaluated on the tasks of entity sense disambiguation in three domain KGs (jet engine, biomedical, and kinship) and author name disambiguation in bibliographic KGs, demonstrating the effectiveness and efficiency of DisambiguART against the state-of-the-art methods.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jul,
articleno = {121},
numpages = {29},
keywords = {Knowledge Graphs, Graph Embeddings, Entity Disambiguation, Adaptive Resonance Theory}
}

@inproceedings{10.1145/3338188.3338215,
author = {Yingbo, Dong and Xia, Hou},
title = {Business Modeling and Reasoning Based on Process Ontology},
year = {2019},
isbn = {9781450362931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338188.3338215},
doi = {10.1145/3338188.3338215},
abstract = {In this paper, a method of business modeling and reasoning based on process ontology is proposed to solve the problem that existing in the banking business field such as large amount of data and the complexity to analysis the relationship between data. We use process ontology technology to describe each basic element in the business process, and on this basis, design constraints and inference rules, and carry out corresponding ontology knowledge reasoning by use the inference rules. Practice results show that the establishment of process ontology model can effectively represent the knowledge involved in the business process and discover the hidden information contained in the knowledge.},
booktitle = {Proceedings of the 5th International Conference on Frontiers of Educational Technologies},
pages = {143–147},
numpages = {5},
keywords = {process ontology, knowledge reasoning, inference rules, banking business field},
location = {Beijing, China},
series = {ICFET '19}
}

@inproceedings{10.1145/3459930.3469562,
author = {Lyons, Robert and Low, Geoffrey Ross and Congdon, Clare Bates and Ceruolo, Melissa and Ballesteros, Marissa and Cambria, Steven and DePetrillo, Paolo},
title = {Towards an extensible ontology for streaming sensor data for clinical trials},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459930.3469562},
doi = {10.1145/3459930.3469562},
abstract = {The use of wearable sensors for clinical trials can lead to better data collection and a better patient experience during trials, and can further allow more patients to participate in trials by allowing more remote monitoring and fewer site visits. However, extracting maximum value from the data collected via streaming sensors presents some specific technical challenges, including processing the data in real time, and storing the sensor data in a representation that facilitates the use of biomarker algorithms that can be used and reused with different similar sensors, at different scales, and across different clinical trials. Here we present our initial work on SORBET, a Sensor Ontology for Reusable Biometric Expressions and Transformations. Our design strategy is presented, along with the initial design and examples. While this ontology has been created for the Medidata Sensor Cloud product, it is our hope that others working in this space will join us in extending and hardening this ontology, as we expand it to incorporate more sensors and more needs for clinical trials research.},
booktitle = {Proceedings of the 12th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {48},
numpages = {6},
keywords = {health informatics, ontology engineering, semantic networks},
location = {Gainesville, Florida},
series = {BCB '21}
}

@inproceedings{10.1145/3654522.3654561,
author = {Nguyen, Dien Thanh and Do, Nhon Van and Tran, Tung Hoang},
title = {A Model of Topic for Document Retrieval Systems in the Field of Artificial Intelligence for Information Technology students},
year = {2024},
isbn = {9798400716713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654522.3654561},
doi = {10.1145/3654522.3654561},
abstract = {This article proposes a topic definition, topic modeling and document search techniques related to topics in the field of artificial intelligence, thereby building a document search application based on topic. Document warehouse includes ebooks and papers. The application will serve the need to search for documents by topic for information technology students during the learning and research process, to solve that problem. Realizing that Ontology and document search techniques are a suitable and powerful approach for organizing the knowledge base as well as solving the problem of retrieving documents by topic with high efficiency. From there, the following databases are built: the Ebook and Paper database is stored in a structured form, the topic database is to store a set of topics in the field of artificial intelligence, Ontology database for ebooks and papers, keyphrase graph database to store topic and document representations and semantic similarity matching techniques between documents and topics.},
booktitle = {Proceedings of the 2024 9th International Conference on Intelligent Information Technology},
pages = {376–385},
numpages = {10},
keywords = {Additional Key Words and Phrases: Topic definition, domain ontology, graph matching, topic modeling, topic representation},
location = {Ho Chi Minh City, Vietnam},
series = {ICIIT '24}
}

@inproceedings{10.1145/3643655.3643876,
author = {Rossi, Maria Teresa and Tundo, Alessandro and Mariani, Leonardo},
title = {Towards Model-Driven Dashboard Generation for Systems-of-Systems},
year = {2024},
isbn = {9798400705571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643655.3643876},
doi = {10.1145/3643655.3643876},
abstract = {Configuring and evolving dashboards in complex and large-scale Systems-of-Systems (SoS) can be an expensive and cumbersome task due to the many Key Performance Indicators (KPIs) that are usually collected and have to be arranged in a number of visualizations. Unfortunately, setting up dashboards is still a largely manual and error-prone task requiring extensive human intervention.This short paper describes emerging results about the definition of a model-driven technology-agnostic approach that can automatically transform a simple list of KPIs into a dashboard model, and then translate the model into an actual dashboard for a target dashboard technology. Dashboard customization can be efficiently obtained by solely modifying the abstract model representation, freeing operators from expensive interactions with actual dashboards.},
booktitle = {Proceedings of the 12th ACM/IEEE International Workshop on Software Engineering for Systems-of-Systems and Software Ecosystems},
pages = {9–12},
numpages = {4},
keywords = {automatic dashboard generation, model-driven engineering, model-based dashboard, systems of systems, monitoring dashboard},
location = {Lisbon, Portugal},
series = {SESoS '24}
}

@inproceedings{10.1145/3550356.3556502,
author = {Boubekeur, Younes and Singh, Prabhsimran and Mussbacher, Gunter},
title = {A DSL and model transformations to specify learning corpora for modeling assistants},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3556502},
doi = {10.1145/3550356.3556502},
abstract = {Software engineering undergraduate students spend a significant time learning various topics related to software design, including notably model-driven engineering (MDE), where different types of structural and behavioral models are used to design, implement, and validate an application. MDE instructors spend a lot of time covering modeling concepts, which is more difficult with ever-increasing class sizes. Online resources, such as learning corpora for domain modeling, can aid in this learning process by serving as a more dynamic textbook alternative or as part of a larger interactive application with domain modeling exercises and tutorials. A Learning Corpus (LC) is an extensible list of entries representing possible mistakes that could occur when defining a model, e.g., Missing Abstraction-Occurrence pattern in the case of a domain model. Each LC entry includes progressive levels of feedback, including written responses, quizzes, and references to external resources. To make it easy for instructors to customize the entries as well as add their own, we propose a novel, simple, and intuitive approach based on an internal domain-specific language that supports features such as context-specific information and concise arbitrary metamodel navigation with shorthands. Transformations to source code as well as Markdown and LATEX enable use of the LC entries in different contexts. These transformations as well as the integration of the generated code in a sample Modeling Assistant application verify and validate the LC metamodel and specification.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {95–102},
numpages = {8},
keywords = {model-driven engineering (MDE), model transformation, learning corpus, feedback, domain modeling},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3587259.3627554,
author = {Iglesias-Molina, Ana and Toledo, Jhon and Corcho, Oscar and Chaves-Fraga, David},
title = {Re-Construction Impact on Metadata Representation Models},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627554},
doi = {10.1145/3587259.3627554},
abstract = {Reification in knowledge graphs has been present since the inception of RDF to allow capturing additional information in triples, usually metadata. The need of adopting or changing a metadata representation in a pre-existing graph to enhance the knowledge capture and access can lead to inducing complex structural changes in the graph, according the target representation’s schema. In these situations, it is necessary to decide whether to construct the knowledge graph again from its original sources, or to re-construct it using the current version of the graph. In this paper we conduct an empirical study to analyze which re-construction approach is more suitable for switching the representation approach from the created graph ensuring that the additional represented knowledge is preserved. We study four well-known metadata representations, using mapping languages to construct the graph, and SPARQL CONSTRUCT queries to re-construct it. With this work we aim to provide insights about the impact of re-construction on metadata representations interoperability and the implications of different approaches.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {197–205},
numpages = {9},
keywords = {Declarative Mappings., Knowledge Graphs, Metadata, SPARQL},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3631700.3665182,
author = {Jeromela, Jovan and Conlan, Owen},
title = {Devising Scrutable User Models for Time Management Assistants},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3665182},
doi = {10.1145/3631700.3665182},
abstract = {Intelligent Personal Assistants (IPAs) have become ubiquitous through integration into smartphones, smart speakers, and standalone devices. However, prior studies raised noteworthy usability concerns and determined that IPAs remain primarily used for simple tasks. Such findings contrast with the reported user aspirations for more proactive and truly personalised IPAs. By focusing on the use case of time management, in this paper, we contemplate how scrutability – i.e. the ability of the user to study their assistant and its underlying user model – fits within the vision of more complex IPAs. Furthermore, we describe our ongoing project investigating user interest in and expectations of the scrutability of a proactive calendaring assistant. Lastly, by deliberating on the challenges and benefits of making IPAs scrutable, this paper outlines potential avenues for further research.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {250–255},
numpages = {6},
keywords = {Explainability, Intelligent personal assistants, Proactive dialogue systems, Time management},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@inproceedings{10.1145/3640310.3674096,
author = {Rajbhoj, Asha and Pathan, Ajim and Sant, Tanay and Kulkarni, Vinay and Nistala, Padmalata and Pandey, Rajesh and Narasimhan, Sabarinathan and Thiagarajan, Geetha},
title = {AutoMW: Model-based Automated Medical Writing},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674096},
doi = {10.1145/3640310.3674096},
abstract = {Medical Writing is an art of writing scientific documents which includes regulatory and research-related content. To obtain approval for marketing new medicines, pharmaceutical companies are obligated to provide drug authorities with a huge volume of documents related to clinical trials. Creating these clinical trial documents is a time, effort, and skill-intensive process as the required information exists in fragmented form distributed across various information sources. To overcome these challenges in medical writing, we propose Automated Medical Writing tool (AutoMW). AutoMW enables the digitalization of information from different sources of information using a meta-model-based approach and leverages these models for the automated generation of clinical trial documents as per the regulatory authority document templates. This paper describes the approach and illustrates its utility and efficacy in real-world clinical trial application of two use cases - breast cancer, and diabetes.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {257–267},
numpages = {11},
keywords = {Automated Content Generation, Clinical Trial Documentation, MDE, Medical Writing, NLP},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3696952.3696987,
author = {Hu, Yan and Wang, Hongli},
title = {A Review of Mining User Needs Based on Text Sentiment Analysis Technology and KANO Model},
year = {2024},
isbn = {9798400718076},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696952.3696987},
doi = {10.1145/3696952.3696987},
abstract = {This article reviews the research on user demand mining based on online reviews. Through text sentiment analysis techniques, such as sentiment lexicons, rules, and machine learning methods, user needs are explored. The article emphasizes the value of sentiment analysis of online reviews in mining user needs and explores its relationship with product sales and the application of the KANO model in demand classification. Finally, it summarizes the challenges of current research, future trends, and the importance of combining sentiment analysis with user demand mining.},
booktitle = {Proceedings of the 2024 9th International Conference on Intelligent Information Processing},
pages = {257–264},
numpages = {8},
keywords = {Deep learning, Machine learning, Online reviews, Text analysis},
location = {
},
series = {ICIIP '24}
}

@inproceedings{10.1145/3178372.3179515,
author = {Ginsbach, Philip and Crawford, Lewis and O'Boyle, Michael F. P.},
title = {CAnDL: a domain specific language for compiler analysis},
year = {2018},
isbn = {9781450356442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178372.3179515},
doi = {10.1145/3178372.3179515},
abstract = {Optimizing compilers require sophisticated program analysis and transformations to exploit modern hardware. Implementing the appropriate analysis for a compiler optimization is a time consuming activity. For example, in LLVM, tens of thousands of lines of code are required to detect appropriate places to apply peephole optimizations. It is a barrier to the rapid prototyping and evaluation of new optimizations.  In this paper we present the Compiler Analysis Description Language (CAnDL), a domain specific language for compiler analysis. CAnDL is a constraint based language that operates over LLVM's intermediate representation. The compiler developer writes a CAnDL program, which is then compiled by the CAnDL compiler into a C++ LLVM pass. It provides a uniform manner in which to describe compiler analysis and can be applied to a range of compiler analysis problems, reducing code length and complexity.  We implemented and evaluated CAnDL on a number of real world use cases: eliminating redundant operations; graphics code optimization; identifying static control flow regions. In all cases were we able to express the analysis more briefly than competing approaches.},
booktitle = {Proceedings of the 27th International Conference on Compiler Construction},
pages = {151–162},
numpages = {12},
keywords = {LLVM, constraint programming, optimization},
location = {Vienna, Austria},
series = {CC '18}
}

@inproceedings{10.1145/3579170.3579265,
author = {Ollier, Guillaume and Arnez, Fabio and Adedjouma, Morayo and Lallement, Rapha\"{e}l and Gerasimou, Simos and Mraidha, Chokri},
title = {Towards an Ontological Methodology for Dynamic Dependability Management of Unmanned Aerial Vehicles},
year = {2023},
isbn = {9798400700453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579170.3579265},
doi = {10.1145/3579170.3579265},
abstract = {Dynamic Dependability Management (DDM) is a promising approach to guarantee and monitor the ability of safety-critical Automated Systems (ASs) to deliver the intended service with an acceptable risk level. However, the non-interpretability and lack of specifications of the Learning-Enabled Component (LEC) used in ASs make this mission particularly challenging. Some existing DDM techniques overcome these limitations by using probabilistic environmental perception knowledge associated with predicting behavior changes for the agents in the environment. Ontology-based methods allow using a formal and traceable representation of AS usage scenarios to support the design process of the DDM component of such ASs. This paper presents a methodology to perform this design process, starting from the AS specification stage and including threat analysis and requirements identification. The present paper focuses on the formalization of an ontology modeling language allowing the interpretation of logical usage scenarios, i.e., a formal description of the scenario represented by state variables. The proposed supervisory system also considers the uncertainty estimation and interaction between AS components through the whole perception-planning-control pipeline. This methodology is illustrated in this paper on a use case involving Unmanned Aerial Vehicles (UAVs).},
booktitle = {Proceedings of the DroneSE and RAPIDO: System Engineering for Constrained Embedded Systems},
pages = {12–19},
numpages = {8},
keywords = {Autonomous Systems, Cyber-Physical Systems, Dynamic Risk Management, Real-time Monitoring, Safety-critical Systems},
location = {Toulouse, France},
series = {RAPIDO '23}
}

@article{10.1145/3675781,
author = {Pham, Quoc-Hung and Le, Huu-Loi and Dang Nhat, Minh and Tran T., Khang and Tran-Tien, Manh and Dang, Viet-Hung and Vu, Huy-The and Nguyen, Minh-Tien and Phan, Xuan-Hieu},
title = {Towards Vietnamese Question and Answer Generation: An Empirical Study},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3675781},
doi = {10.1145/3675781},
abstract = {Question-answer generation (QAG) is a challenging task that generates both questions and answers from a given input paragraph context. The QAG task has recently achieved promising results thanks to the appearance of large pre-trained language models, yet, QAG models are mainly implemented in common languages, e.g., English. There still remains a gap in domain and language adaptation of these QAG models to low-resource languages such as Vietnamese. To address the gap, this article presents a large-scale and systematic study of QAG in Vietnamese. To do that, we first implement several QAG models by using the common fine-tuning techniques based on powerful pre-trained language models. We next introduce a set of instructions designed for the QAG task. These instructions are used to fine-tuned the pre-trained language and large language models. Extensive experimental results of both automatic and human evaluation on five benchmark machine reading comprehension datasets show two important points. First, the instruction-tuning method has the potential to enhance the performance of QAG models. Second, large language models trained in English need more data for fine-tuning to work well on the downstream QAG tasks of low-resource languages. We also provide a prototype system to demonstrate how our QAG models actually work. The code for fine-tuning QAG models and instructions are also made available.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {132},
numpages = {28},
keywords = {Natural language processing, question and answer generation, large pre-trained language models, instruction fine-tuning, BARTPho, ViT5, LlaMa2}
}

@inproceedings{10.1145/3703790.3703803,
author = {Papadakis, Nikolaos and Bouloukakis, Georgios and Magoutis, Kostas},
title = {Enabling IoT-enhanced Data Models for Context-aware Hydropower Plants},
year = {2025},
isbn = {9798400712852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3703790.3703803},
doi = {10.1145/3703790.3703803},
abstract = {Hydroelectric power, or hydropower, harnesses the potential energy of water descending from higher to lower elevations to generate electricity. As a well-established and cost-effective renewable energy technology, it not only produces power but also supports significant water management services. The integration of Internet of Things (IoT) technologies in hydropower plants has shown significant potential in enhancing monitoring, efficiency, and control capabilities. However, current implementations often lack a holistic and standardized approach to contextual modeling. To address this gap, this paper presents a comprehensive approach to modeling the structural and operational components of hydropower plants (HPPs) using NGSI-LD data models. We propose detailed NGSI-LD data models that incorporate both static properties (e.g., location, structural attributes), relationships (e.g., component interactions, hierarchical dependencies) and dynamic properties (e.g., real-time sensor data, operational status). These models are designed to facilitate efficient data integration, support decision-making processes, and enable the development of interoperable and replicable IoT applications for smart hydropower plants. We validate our approach through deployment and testing on a federated context broker architecture using real-world data from HPPs.},
booktitle = {Proceedings of the 14th International Conference on the Internet of Things},
pages = {108–116},
numpages = {9},
keywords = {Renewable Energy, Data Models, Internet of Things (IoT), NGSI-LD, Smart Energy, Interoperability, Context-Aware Systems},
location = {
},
series = {IoT '24}
}

@inproceedings{10.1145/3579051.3579074,
author = {Zhou, Baifan and Tan, Zhipeng and Zheng, Zhuoxun and Zhou, Dongzhuoran and Savkovic, Ognjen and Kharlamov, Evgeny},
title = {Towards A Visualisation Ontology for Reusable Visual Analytics},
year = {2023},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579051.3579074},
doi = {10.1145/3579051.3579074},
abstract = {Data analytics including machine learning analytics is essential to extract insights from production data in modern industries. Visual analytics is essential for data analytics for e.g., presenting the data to provide an instinctive perception in exploratory data analysis, facilitating the presentation of data analysis results and the subsequent discussion on that. Visual analytics should allow a transparent common ground for discussion between experts in data analysis projects, given the multidisciplinary background of these experts. However, a standarised and formalised way of describing the knowledge and practice of visualisation is still lacking in the industry, which hamstrings the transparency and reusability of visual analytics. A visualisation ontology which models the nature and procedure of visualisation is well-suited to provide such standardisation. Currently a few studies discuss partially the modelling of visualisation, but insufficiently study the procedure of visualisation tasks, which is important for transparency and reusability especially in an industrial scenario. To this end, we present our ongoing work of development of the visualisation ontology in industrial scenarios at Bosch. We also demonstrate its benefits with case studies and knowledge graph based on our ontology.},
booktitle = {Proceedings of the 11th International Joint Conference on Knowledge Graphs},
pages = {99–103},
numpages = {5},
keywords = {knowledge graph, ontology engineering, visual analytics},
location = {Hangzhou, China},
series = {IJCKG '22}
}

@inproceedings{10.5555/3522802.3522881,
author = {Saleh, Nurul and Bell, David and Sulaiman, Zuharabih},
title = {Hybrid conceptual modeling for simulation: an ontology approach during covid-19},
year = {2022},
publisher = {IEEE Press},
abstract = {The recent outbreak of Covid-19 caused by SARS-CoV-2 infection that started in Wuhan, China, has quickly spread worldwide. Due to the aggressive number of cases, the entire healthcare system has to respond and make decisions promptly to ensure it does not fail. Researchers have investigated the integration between ontology, algorithms and process modeling to facilitate simulation modeling in emergency departments and have produced a Minimal-Viable Simulation Ontology (MVSimO). However, the "minimalism" of the ontology has yet to be explored to cover pandemic settings. Responding to this, modelers must redesign services that are Covid-19 safe and better reflect changing realities. This study proposes a novel method that conceptualizes processes within the domain from a Discrete-Event Simulation (DES) perspective and utilizes prediction data from an Agent-Based Simulation (ABS) model to improve the accuracy of existing models. This hybrid approach can be helpful to support local decision making around resources allocation.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {108},
numpages = {11},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@article{10.1145/3295738,
author = {Bj\o{}rner, Dines},
title = {Domain Analysis and Description Principles, Techniques, and Modelling Languages},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3295738},
doi = {10.1145/3295738},
abstract = {We present a method for analysing and describing domains.By a domain we shall understand a rationally describable segment of a human assisted reality, i.e., of the world, its physical parts: natural [“God-given”] and artifactual [“human-made”], and living species: plants and animals including, notably, humans. These are endurants (“still”), as well as perdurants (“alive”). Emphasis is placed on “human-assistedness,” that is, that there is at least one (human-made) artifact and, therefore, that humans are a primary cause for change of endurant states as well as perdurant behaviours.By a method we shall mean a set of principles of analysis and for selecting and applying a number of techniques and tools in the construction of some artifact, say a domain description. We shall present a method for constructing domain descriptions. Among the tools we shall only be concerned with are the analysis and synthesis languages.Domain science and engineering marks a new area of computing science. Just as we are formalising the syntax and semantics of programming languages, so we are formalising the syntax and semantics of human-assisted domains. Just as physicists are studying the natural physical world, endowing it with mathematical models, so we, computing scientists, are studying these domains, endowing them with mathematical models, A difference between the endeavours of physicists and ours lies in the tools: The physics models are based on classical mathematics, differential equations and integrals, and so on; our models are based on mathematical logic, set theory, and algebra [1].Where physicists thus classically use a variety of differential and integral calculi to model the physical world, we shall be using the analysis and description calculi presented in this article to model primarily artifactual domains.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = mar,
articleno = {8},
numpages = {67},
keywords = {Domain engineering, domain analysis and description calculi, transcendental deduction}
}

@inproceedings{10.1145/3750022.3750459,
author = {Zhai, Rundi and Liu, Jianmin and Miao, Yukai and Chen, Li and Li, Dan and Cui, Baojiang and Zhang, Peng and Zhai, Ennan and Ding, Zishuo},
title = {ConfSum: Towards Automatic Summarization of Network-scale Operational Intents from Device Configurations},
year = {2025},
isbn = {9798400721038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3750022.3750459},
doi = {10.1145/3750022.3750459},
abstract = {When network operators need to understand the high-level intent behind a network's existing device configurations, they must engage in a tedious and error-prone process of manually reverse-engineering the low-level commands. We propose Configuration Intent Summarization (CIS), a new task that aims to automate this process by generating human-readable summaries of the intents embedded across a network's configurations. CIS is challenging due to the diversity of intents, the semantic gap between device-specific configurations and network-wide intents, and the need to reason about interactions between multiple devices' configurations. We present ConfSum, a system that addresses these challenges by leveraging the unique ability of large language models (LLMs) to parse semi-structured configuration files and summarize them in natural language. However, the full CIS task requires reasoning about device interactions and other complexities that are beyond the capabilities of LLMs alone. To enhance the LLM's robustness to these challenges, ConfSum introduces novel techniques for retrieving relevant examples to augment LLM prompts, decomposing the generation process to handle multi-device intents, and integrating with formal validation tools. Our experiments demonstrate that Conf-Sum achieves high intent coverage while generating summaries that match the quality of human experts.},
booktitle = {Proceedings of the 2nd Workshop on Formal Methods Aided Network Operation},
pages = {19–24},
numpages = {6},
keywords = {Formal Methods, Large Language Models, Network management},
location = {Coimbra, Portugal},
series = {FMANO '25}
}

@inproceedings{10.1145/3308560.3316602,
author = {Stein, Daniel and Shterionov, Dimitar and Way, Andy},
title = {Towards language-agnostic alignment of product titles and descriptions: a neural approach},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316602},
doi = {10.1145/3308560.3316602},
abstract = {The quality of e-Commerce services largely depends on the accessibility of product content as well as its completeness and correctness. Nowadays, many sellers target cross-country and cross-lingual markets via active or passive cross-border trade, fostering the desire for seamless user experiences. While machine translation (MT) is very helpful for crossing language barriers, automatically matching existing items for sale (e.g. the smartphone in front of me) to the same product (all smartphones of the same brand/type/colour/condition) can be challenging, especially because the seller’s description can often be erroneous or incomplete. This task we refer to as item alignment in multilingual e-commerce catalogues. To facilitate this task, we develop a pipeline of tools for item classification based on cross-lingual text similarity, exploiting recurrent neural networks (RNNs) with and without pre-trained word-embeddings. Furthermore, we combine our language agnostic RNN classifiers with an in-domain MT system to further reduce the linguistic and stylistic differences between the investigated data, aiming to boost our performance. The quality of the methods as well as their training speed is compared on an in-domain data set for English–German products.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {387–392},
numpages = {6},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3459637.3482466,
author = {Cima, Gianluca and Croce, Federico and Lenzerini, Maurizio},
title = {Query Definability and Its Approximations in Ontology-based Data Management},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482466},
doi = {10.1145/3459637.3482466},
abstract = {Given an input dataset (i.e., a set of tuples), query definability in Ontology-based Data Management (OBDM) amounts to finding a query over the ontology whose certain answers coincide with the tuples in the given dataset. We refer to such a query as a characterization of the dataset with respect to the OBDM system. Our first contribution is to propose approximations of perfect characterizations in terms of recall (complete characterizations) and precision (sound characterizations). A second contribution is to present a thorough complexity analysis of three computational problems, namely verification (check whether a given query is a perfect, or an approximated characterization of a given dataset), existence (check whether a perfect, or a best approximated characterization of a given dataset exists), and computation (compute a perfect, or best approximated characterization of a given dataset).},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {271–280},
numpages = {10},
keywords = {ontology based data management, semantic technologies},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3555776.3577795,
author = {Engelberg, Gal and Fumagalli, Mattia and Kuboszek, Adrian and Klein, Dan and Soffer, Pnina and Guizzardi, Giancarlo},
title = {An Ontology-Driven Approach for Process-Aware Risk Propagation},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577795},
doi = {10.1145/3555776.3577795},
abstract = {Risk Propagation (RP) is a central technique that allows the calculation of the cascading effect of risk within a system. At the current state, there is a lack of risk propagation solutions that can be used to assess the impact of risk at different levels of abstraction, accounting for actors, processes, physical-digital objects, and their relations. To fill this gap, in this paper, we propose a process-aware risk propagation approach that builds on two main components: i. an ontology, which supports functionalities typical of Semantic Web technologies (SWT), and ii. an ad hoc method to calculate the propagation of risk within the given system. We implemented our approach in a proof-of-concept tool, which was validated in the cybersecurity domain.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1742–1745},
numpages = {4},
keywords = {risk propagation, risk analytics, ontology-driven risk propagation},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@article{10.1145/3746626.3746627,
author = {Zampierin, Luca and Frasincar, Flavius},
title = {An Unsupervised Approach Based on Attentional Neural Models for Aspect-Based Sentiment Classification},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1559-6915},
url = {https://doi.org/10.1145/3746626.3746627},
doi = {10.1145/3746626.3746627},
abstract = {Due to the vast amount of reviews available on the Web, in the past decades, a growing share of work has focused on sentiment analysis. Aspect-based sentiment classification is the subtask that seeks to detect the sentiment expressed by the content creators towards a defined target within a sentence. This paper introduces three novel unsupervised attentional neural network models for aspect-based sentiment classification, and tests them on English restaurant reviews. The first model employs an autoencoder-like structure to learn a sentiment embedding matrix where each row of the matrix represents the embedding for one sentiment. To improve the model, a target-based attention mechanism is included that de-emphasizes irrelevant words. Last, a redundancy and a seed regularization term constrain the sentiment embedding matrix. The second model extends the first by including a Bi-LSTM layer in the attention mechanism to exploit contextual information. The third model further adapts a Left-Center-Right separated neural network with Rotatory attention structure from the supervised realm to an unsupervised setting. Although all three models construct meaningful sentiment embeddings, experimental results indicate that the inclusion of the Bi-LSTM in the attention mechanism leads to a more precise attention mechanism and, thus, better predictions. The best model, i.e., the second, outperforms all investigated unsupervised and weakly supervised algorithms for aspect-based sentiment classification from the literature.},
journal = {SIGAPP Appl. Comput. Rev.},
month = jul,
pages = {5–17},
numpages = {13},
keywords = {attentional neural model, unsupervised learning, aspect-based sentiment classification}
}

@inproceedings{10.5555/3716662.3716801,
author = {Wolfe, Robert and Mitra, Tanushree},
title = {The Implications of Open Generative Models in Human-Centered Data Science Work: A Case Study with Fact-Checking Organizations},
year = {2025},
publisher = {AAAI Press},
abstract = {Calls to use open generative language models in academic research have highlighted the need for reproducibility and transparency in scientific research. However, the impact of generative AI extends well beyond academia, as corporations and public interest organizations have begun integrating these models into their data science pipelines. We expand this lens to include the impact of open models on organizations, focusing specifically on fact-checking organizations, which use AI to observe and analyze large volumes of circulating misinformation, yet must also ensure the reproducibility and impartiality of their work. We wanted to understand where fact-checking organizations use open models in their data science pipelines; what motivates their use of open models or proprietary models; and how their use of open or proprietary models can inform research on the societal impact of generative AI. To answer these questions, we conducted an interview study with N=24 professionals at 20 fact-checking organizations on six continents. Based on these interviews, we offer a five-component conceptual model of where fact-checking organizations employ generative AI to support or automate parts of their data science pipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data Delivery, and Data Sharing. We then provide taxonomies of fact-checking organizations' motivations for using open models and the limitations that prevent them for further adopting open models, finding that they prefer open models for Organizational Autonomy, Data Privacy and Ownership, Application Specificity, and Capability Transparency. However, they nonetheless use proprietary models due to perceived advantages in Performance, Usability, and Safety, as well as Opportunity Costs related to participation in emerging generative AI ecosystems. Finally, we propose a research agenda to address limitations of both open and proprietary models. Our research provides novel perspective on open models in data-driven organizations.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1595–1607},
numpages = {13},
location = {San Jose, California, USA},
series = {AIES '24}
}

@inproceedings{10.1145/3736229.3736262,
author = {Roper, Bernard and Packer, Heather},
title = {PROVPub: A Publication Model to Support Research Evaluation and Acknowledgement},
year = {2025},
isbn = {9798400719417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3736229.3736262},
doi = {10.1145/3736229.3736262},
abstract = {The Research Excellence Framework (REF) is vital for assessing the quality and impact of UK academic research, requiring a transparent provenance trail for research outputs. A major challenge is linking publications to their underlying research activities. This paper introduces PROVPub, a provenance model that automates and documents the publication lifecycle, improving traceability and compliance with REF requirements. PROVPub extends existing frameworks like Git2PROV&nbsp;[5] by integrating metadata from institutional repositories, publisher databases, and Crossref. It captures key stages—submission, peer review, acceptance, and publication—to create a structured, verifiable record. This enhances efficiency, reduces administrative burdens, and strengthens research impact assessment. Through case studies, we demonstrate PROVPub’s role in REF compliance and automated research tracking. Our findings indicate that PROVPub enhances transparency and scalability, benefiting institutions seeking to streamline REF submissions and improve research evaluation.},
booktitle = {Proceedings of the ProvenanceWeek 2025},
pages = {11–17},
numpages = {7},
keywords = {Provenance, Research Evaluation, Impact Assessment, REF, PROV},
location = {
},
series = {PW' 25}
}

@inproceedings{10.1145/3318464.3386145,
author = {Liu, Bang and Guo, Weidong and Niu, Di and Luo, Jinwen and Wang, Chaoyue and Wen, Zhen and Xu, Yu},
title = {GIANT: Scalable Creation of a Web-scale Ontology},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386145},
doi = {10.1145/3318464.3386145},
abstract = {Understanding what online users may pay attention to on the web is key to content recommendation and search services. These services will benefit from a highly structured and web-scale ontology of entities, concepts, events, topics and categories. While existing knowledge bases and taxonomies embody a large volume of entities and categories, we argue that they fail to discover properly grained concepts, events and topics in the language style of online users. Neither is a logically structured ontology maintained among these notions. In this paper, we present GIANT, a mechanism to construct a user-centered, web-scale, structured ontology, containing a large number of natural language phrases conforming to user attentions at various granularities, mined from the vast volume of web documents and search click logs. Various types of edges are also constructed to maintain a hierarchy in the ontology. We present our detailed techniques used in GIANT, and evaluate the proposed models and methods as compared to a variety of baselines, as well as deploy the resulted Attention Ontology in real-world applications, involving over a billion users, to observe its effect on content recommendation. The online performance of the ontology built by GIANT proves that it can significantly improve the click-through rate in news feeds recommendation.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {393–409},
numpages = {17},
keywords = {concept mining, document understanding, event mining, ontology creation, user interest modeling},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1109/TASLP.2024.3485547,
author = {Ma, Jun-Yu and Gu, Jia-Chen and Ling, Zhen-Hua and Liu, Quan and Liu, Cong and Hu, Guoping},
title = {Syntax-Augmented Hierarchical Interactive Encoder for Zero-Shot Cross-Lingual Information Extraction},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3485547},
doi = {10.1109/TASLP.2024.3485547},
abstract = {Zero-shot cross-lingual information extraction (IE) aims at constructing an IE model for some low-resource target languages, given annotations exclusively in some rich-resource languages. Recent studies have shown language-universal features can bridge the gap between languages. However, prior work has neither explored the potential of establishing interactions between language-universal features and contextual representations nor incorporated features that can effectively model constituent span attributes and relationships between multiple spans. In this study, a &lt;bold&gt;s&lt;/bold&gt;yntax-augmented &lt;bold&gt;h&lt;/bold&gt;ierarchical &lt;bold&gt;in&lt;/bold&gt;teractive &lt;bold&gt;e&lt;/bold&gt;ncoder (SHINE) is proposed to transfer cross-lingual IE knowledge. The proposed encoder is capable of interactively capturing complementary information between features and contextual information, to derive language-agnostic representations for various cross-lingual IE tasks. Concretely, a multi-level interaction network is designed to hierarchically interact the complementary information to strengthen domain adaptability. Besides, in addition to the well-studied word-level syntax features of part-of-speech and dependency relation, a new span-level syntax feature of constituency structure is introduced to model the constituent span information which is crucial for IE. Experiments across seven languages on three IE tasks and four benchmarks verify the effectiveness and generalization ability of the proposed method.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = oct,
pages = {4795–4809},
numpages = {15}
}

@inproceedings{10.1145/3616131.3616143,
author = {Zhu, Li and Qi, Xiangtao},
title = {Research on The Construction of Ontology-based Music Works Knowledge Base},
year = {2023},
isbn = {9798400707339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616131.3616143},
doi = {10.1145/3616131.3616143},
abstract = {The article formally explores the construction and reasoning of music domain ontology based on ontological methodology, proposes an improved ontology construction method, defines an ontology knowledge representation model, builds up an ontology of music works and its inference rules, achieves ontology-driven knowledge representation, storage, query and reasoning of music works, and makes some quest research for the construction and application of domain ontology.},
booktitle = {Proceedings of the 2023 7th International Conference on Cloud and Big Data Computing},
pages = {81–88},
numpages = {8},
location = {Manchester, United Kingdom},
series = {ICCBDC '23}
}

@inproceedings{10.1145/3535511.3535555,
author = {Monteiro, Lucas Henrique de Assis and Rodrigues, Cleyton M\'{a}rio de Oliveira and Sousa, A\^{e}da Monalliza Cunha de},
title = {An Information System for Law Integrating Ontological Bases with a Legal Reasoner Chatbot},
year = {2022},
isbn = {9781450396981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535511.3535555},
doi = {10.1145/3535511.3535555},
abstract = {Context: The Semantic Web aims to assign meanings to resources available on the internet so that humans and computers can understand them. It can be used in the most diverse contexts, facilitating the development of systems where expert knowledge is formalized through logical-mathematical resources, mitigating potential inconsistencies, and promoting more human-friendly interaction services. Problem: The existence of semantic anomalies (use of rhetorical language, polysemy and inaccuracies) in the Brazilian Legal Domain enables the use of Semantic Web standards and technologies to mitigate these problems. Solution: This work deals with the development of an Information System that uses resources from the Semantic Web for the formal representation and the realization of legal inferences about Crimes Against Property. SI Theory: The Behavioral Decision Theory was approached, mainly in the incorporation of real patterns of decision making. Method: Bibliographic and documentary research methods were used to list the main concepts related to the Criminal Types investigated. The research is prescriptive and has a quali-quantitative approach. Summary of Results: A prototype system is presented, integrating ontologies of Brazilian Law with a chatbot that enables interaction with users in natural language, as well as performing reasoning tasks based on the knowledge formalized in these ontologies. Contributions and Impact in the IS area: The research will contribute to the automation of decision-making processes involving crimes against property, serving as an aid for professionals or law students and for legal simulations by ordinary people. Furthermore, it will serve as a reference for the development of other information systems with similar objectives in other contexts.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Information Systems},
articleno = {44},
numpages = {8},
keywords = {Chatbot, Law, Ontology},
location = {Curitiba, Brazil},
series = {SBSI '22}
}

@inproceedings{10.1145/3543507.3583533,
author = {Song, Miao-Hui and Zhang, Lan and Yuan, Mu and Li, Zichong and Song, Qi and Liu, Yijun and Zheng, Guidong},
title = {CoTel: Ontology-Neural Co-Enhanced Text Labeling},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583533},
doi = {10.1145/3543507.3583533},
abstract = {The success of many web services relies on the large-scale domain-specific high-quality labeled dataset. Insufficient public datasets motivate us to reduce the cost of data labeling while maintaining high accuracy in support of intelligent web applications. The rule-based method and the learning-based method are common techniques for labeling. In this work, we study how to utilize the rule-based and learning-based methods for resource-effective text labeling. We propose CoTel, the first ontology-neural co-enhanced framework for text labeling. We propose critical ontology extraction in the rule-based module and ontology-enhanced loss prediction in the learning-based module. CoTel can integrate explicit labeling rules and implicit labeling models and make them help each other to improve resource efficiency in text labeling tasks. We evaluate CoTel on both public datasets and real applications with three different tasks. Compared with the baseline, CoTel can reduce the time cost by 64.75\% (a 2.84\texttimes{} speedup) and the number of labeling by 62.07\%.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1897–1906},
numpages = {10},
keywords = {active learning, knowledge enhancement, pseudo labeling, text labeling},
location = {Austin, TX, USA},
series = {WWW '23}
}

@article{10.1145/3427201,
author = {Swalens, Janwillem and Koster, Joeri De and Meuter, Wolfgang De},
title = {Chocola: Composable Concurrency Language},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/3427201},
doi = {10.1145/3427201},
abstract = {Programmers often combine different concurrency models in a single program, in each part of the program using the model that fits best. Many programming languages, such as Clojure, Scala, and Java, cater to this need by supporting different concurrency models. However, existing programming languages often combine concurrency models in an ad hoc way, and the semantics of the combinations are not always well defined.This article studies the combination of three concurrency models: futures, transactions, and actors. We show that a naive combination of these models invalidates the guarantees they normally provide, thereby breaking the assumptions of programmers. Hence, we present Chocola: a unified language of futures, transactions, and actors that maintains the guarantees of all three models wherever possible, even when they are combined.We describe and formalize the semantics of this language and prove the guarantees it provides. We also provide an implementation as an extension of Clojure and demonstrated that it can improve the performance of three benchmark applications for relatively little effort from the developer.},
journal = {ACM Trans. Program. Lang. Syst.},
month = jan,
articleno = {17},
numpages = {56},
keywords = {Futures, actor model, software transactional memory}
}

@inproceedings{10.1145/3444757.3485111,
author = {Drissi, Amani and Khemiri, Ahmed and Sassi, Salma and Chbeir, Richard},
title = {A New Automatic Ontology Construction Method Based on Machine Learning Techniques: Application on financial corpus},
year = {2021},
isbn = {9781450383141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444757.3485111},
doi = {10.1145/3444757.3485111},
abstract = {Ontology Learning is a process of (semi)automatically creating, maintaining, and transferring various forms of information into an ontology with minimum human intervention to guarantee a better knowledge representation and sharing. In recent years, the research on automating financial data modeling has become a hot topic among researchers because of the exponential increase of the number of financial documents and the heterogeneous of financial data [19, 20]. So, we highlight the emergence of new computational tools and methods to deal with the automatic modeling and exploration of large financial corpus. That's why, we propose here a solution named Norms2Onto which is a semi-automatic ontology construction method based on machine learning algorithms to facilitate the reading and ease updating of financial data and to guarantee their understanding. Experiments have been conducted to measure the effectiveness of our solution compared to a manual classification made by an domain expert. The results show the superiority of our approach.},
booktitle = {Proceedings of the 13th International Conference on Management of Digital EcoSystems},
pages = {57–61},
numpages = {5},
keywords = {IFRS Accounting Standards, Machine Learning, Ontology Construction, Ontology Learning},
location = {Virtual Event, Tunisia},
series = {MEDES '21}
}

@inproceedings{10.1145/3172871.3172873,
author = {Prasad, Gollapudi VRJ Sai and Chimalakonda, Sridhar and Choppella, Venkatesh},
title = {Towards a Domain-Specific Language for the Renarration of Web Pages},
year = {2018},
isbn = {9781450363983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3172871.3172873},
doi = {10.1145/3172871.3172873},
abstract = {We are interested in the problem of enabling transformation of existing, already published web pages. We call this Renarration of web content. In our earlier work, we had already established the role and importance of renarration for improving Web Accessibility. There are nearly a billion websites on the web, making transformation of pages a domain on its own. In this paper, we present the development of a Domain-Specific Language (DSL) for the purpose of web page transformation. We show how the design and implementation of our DSL is driven by our problem domain, its terminology and its unique requirements. We take up an existing online video-course delivery system, which has accessibility challenges, as a specific case to demonstrate our DSL. We end with insights and reflections for future work in both DSL and web page transformations.},
booktitle = {Proceedings of the 11th Innovations in Software Engineering Conference},
articleno = {3},
numpages = {10},
keywords = {Domain Specific Language (DSL), Renarration, Web Page Transformation},
location = {Hyderabad, India},
series = {ISEC '18}
}

@inproceedings{10.1145/3543712.3543739,
author = {Ouchaou, Linda and Nacer, Hassina and Charoy, Francois and Youcef, Samir},
title = {Ontology-Based Cognitive Service Discovery \&amp; Composition},
year = {2022},
isbn = {9781450396226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543712.3543739},
doi = {10.1145/3543712.3543739},
abstract = {Cloud cognitive computing has received a lot of attention lately especially for tackling real-world problems such as vision, natural language processing, fraud detection, sentiment analysis and speech recognition. This paradigm is based on cloud serverless computing and it provides machine learning based functions to end users. Part of the appeal in adopting this paradigm is its simplicity and the future promises a fast-growing serverless-native ecosystem in which service discovery and composition methods must be provided. However, serverless platforms still lack automated searching methods and the research community’s attention regarding this issue has been limited. In this paper, we propose an ontology-based approach for discovering and composing cognitive functions in serverless platforms in order to automate the searching process and semantically answer users’ requirements. We also carried a set of experiments to verify the correctness and the feasibility of our approach and discuss the influence of cognitive services nature on the outcomes of the discovery and composition approach.},
booktitle = {Proceedings of the 2022 8th International Conference on Computer Technology Applications},
pages = {154–162},
numpages = {9},
keywords = {Cognitive services, Composition, Discovery, FaaS, Ontology., Serverless computing},
location = {Vienna, Austria},
series = {ICCTA '22}
}

@article{10.1145/3594727,
author = {Candela, Gustavo and Pereda, Javier and S\'{a}ez, Dolores and Escobar, Pilar and S\'{a}nchez, Alexander and Torres, Andr\'{e}s Villa and Palacios, Albert A. and McDonough, Kelly and Murrieta-Flores, Patricia},
title = {An Ontological Approach for Unlocking the Colonial Archive},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3594727},
doi = {10.1145/3594727},
abstract = {Cultural Heritage institutions have been exploring new ways of making available their catalogues in digital format. Recently, new approaches have emerged as methods to reuse and make available the contents for computational purposes. This work introduces a methodology to transform digital collections into Linked Open Data following best practices. The framework has been applied to Indigenous and Spanish colonial archives based on the collection Relaciones Geogr\'{a}ficas of Mexico and Guatemala provided by the LLILAS Benson Latin American Studies and Collections. The results of this work are publicly available. This work aims at encouraging Cultural Heritage institutions to publish and reuse their digital collections using advanced methods and techniques.},
journal = {J. Comput. Cult. Herit.},
month = nov,
articleno = {74},
numpages = {18},
keywords = {Linked Open Data, metadata, collections as data, knowledge graph}
}

@article{10.1145/3653977,
author = {Moraitou, Efthymia and Christodoulou, Yannis and Kotis, Konstantinos and Caridakis, George},
title = {An Ontology-Based Framework for Supporting Decision-Making in Conservation and Restoration Interventions for Cultural Heritage},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3653977},
doi = {10.1145/3653977},
abstract = {Decision-making (DM) is the backbone of the Conservation and Restoration (CnR) of Cultural Heritage (CH). The demands of the DM process for information organization and management have raised issues that the CnR community attempts to solve by creating DM-support tools and systems, which, among others, exploit Semantic Web (SW) technologies. Regarding the tools and systems that focus on the DM process of selecting an intervention option (CnR-DM-I), they present benefits, as well as limitations, regarding the (1) completeness of representation of the relevant knowledge in a unified manner, (2) facilitation of recording the CnR-DM-I process per se, in terms of the problem at hand as well as the intervention parameters, requirements, and criteria, and (3) recommendation and further exploration of CnR intervention options in a systematic manner. This work proposes an ontology-based framework as a means to overcome those limitations. The proposed framework (DS-CnRI) sets at its core a formal ontology which provides the necessary entities to represent expert knowledge related to CnR-DM-I. The ontology also includes rules which provide useful inferences to assist the CnR-DM-I process. The proposed framework has been deployed and evaluated in collaboration with conservators. Initial evaluation results show that the framework assists conservators in CnR-DM-I to detect and select the most suitable intervention options, to better understand the limitations of different options, and to document the process of reaching their decision.},
journal = {J. Comput. Cult. Herit.},
month = may,
articleno = {41},
numpages = {24},
keywords = {Decision-support services, semantic Web technologies, knowledge representation, conservation, restoration, cultural heritage}
}

@inproceedings{10.1145/3627043.3659564,
author = {Kitto, Kirsty},
title = {Will a Skills Passport ever get me through the lifelong learning border?: Two critical challenges facing personalised user models for lifelong learning},
year = {2024},
isbn = {9798400704338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627043.3659564},
doi = {10.1145/3627043.3659564},
abstract = {Lifelong personalised learning is often described as the holy grail of the educational data sciences, but work on the topic is sporadic and we are yet to achieve this goal in a meaningful form. In the wake of the skills shortages arising from national responses to COVID-19 this problem has again become a topic of interest. A number of proposals have emerged that some sort of a skills passport would help individuals, educational institutions, and employers to identify training and recruitment needs according to identified skills gaps. And yet, we are a long way from achieving a skills passport that could support lifelong learning despite more than 25 years of work on the topic. This paper draws attention to two of the critical socio-technical challenges facing skills passports, and lifelong learner models in general. This leads to a proposal for how we might move towards a useful skills passport that can cross the “skills sector border”.},
booktitle = {Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {132–142},
numpages = {11},
keywords = {contextualisation, data portability, lifelong learning, personal user model, skills},
location = {Cagliari, Italy},
series = {UMAP '24}
}

@proceedings{10.1145/3663338,
title = {ApPLIED'24: Proceedings of the 2024 Workshop on Advanced Tools, Programming Languages, and PLatforms for Implementing and Evaluating algorithms for Distributed systems},
year = {2024},
isbn = {9798400706707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {ApPLIED aims to bring together distributed system designers and practitioners from academia and industry to share their experiences and perspectives in designing and building distributed systems.},
location = {Nantes, France}
}

@article{10.1145/3691642,
author = {Argotti, Yann and Kenfaoui, Yasmine and Baron, Claude and Abran, Alain and Esteban, Philippe},
title = {An Operational Quality Model of Embedded Software Aligned with ISO 25000},
year = {2024},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1539-9087},
url = {https://doi.org/10.1145/3691642},
doi = {10.1145/3691642},
abstract = {Embedded systems omnipresent in everyday life and industry are mainly composed of hardware and software that must comply with a number of standards and regulations. However, there is no consensus on the quality characteristics and subcharacteristics of embedded software. This article presents the steps for modeling an operational quality model for embedded software aligned with the ISO 25000 series of quality models for traditional computer systems. From a literature review composed of 40 studies on quality modeling for embedded systems and software, 85 of the most frequent quality characteristics and subcharacteristics were first identified, including a subset of 16 referenced or cited in at least 25\% of the literature. Next, the design of a quality model for embedded software aligned with the ISO 25000 series was proposed with 13 characteristics and 27 subcharacteristics. The operational aspect of this quality model for embedded software is addressed next through a set of measures and measurement functions from ISO 25000 to aggregate the results of the quantification of the characteristics and subcharacteristics. A survey involving 25 embedded software specialists is presented next to gauge, using Fleiss's Kappa criteria, their agreement with the proposed quality model. Furthermore, the computed importance weights derived from the survey participants’ individual opinions were compared with those derived from an analysis of 40 embedded software studies, bolstering the credibility of the model. The results of this study suggest that the proposed quality model can serve as a framework for evaluating and understanding the quality characteristics across diverse expertise levels. Furthermore, the convergence between the survey and the literature strengthens the model's credibility by anchoring it in both established literature and practitioners’ agreements.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = nov,
articleno = {9},
numpages = {41},
keywords = {Quality model, quality characteristics, measure, ISO, qualimetry}
}

@article{10.1109/TCBB.2018.2849968,
author = {Zeng, Zexian and Deng, Yu and Li, Xiaoyu and Naumann, Tristan and Luo, Yuan},
title = {Natural Language Processing for EHR-Based Computational Phenotyping},
year = {2019},
issue_date = {January 2019},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {16},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2849968},
doi = {10.1109/TCBB.2018.2849968},
abstract = {This article reviews recent advances in applying natural language processing NLP to Electronic Health Records EHRs for computational phenotyping. NLP-based computational phenotyping has numerous applications including diagnosis categorization, novel phenotype discovery, clinical trial screening, pharmacogenomics, drug-drug interaction DDI, and adverse drug event ADE detection, as well as genome-wide and phenome-wide association studies. Significant progress has been made in algorithm development and resource construction for computational phenotyping. Among the surveyed methods, well-designed keyword search and rule-based systems often achieve good performance. However, the construction of keyword and rule lists requires significant manual effort, which is difficult to scale. Supervised machine learning models have been favored because they are capable of acquiring both classification patterns and structures from data. Recently, deep learning and unsupervised learning have received growing attention, with the former favored for its performance and the latter for its ability to find novel phenotypes. Integrating heterogeneous data sources have become increasingly important and have shown promise in improving model performance. Often, better performance is achieved by combining multiple modalities of information. Despite these many advances, challenges and opportunities remain for NLP-based computational phenotyping, including better model interpretability and generalizability, and proper characterization of feature relations in clinical narratives.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {139–153},
numpages = {15}
}

@inproceedings{10.1145/3639592.3639610,
author = {Chawuthai, Rathachai and Kertkeidkachorn, Natthawut and Racharak, Teeradaj},
title = {Modelling an RDF Knowledge Graph with Transitivity and Symmetry for Bus Route Path Finding},
year = {2024},
isbn = {9798400716225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639592.3639610},
doi = {10.1145/3639592.3639610},
abstract = {A key property of Linked Data is the representation and publication of data as an inter-connected labelled graph where different resources linked to each other form a network of meaningful information. A problem of path finding can be seen as searching important relationships between resources, such as, looking for chains of intermediate nodes. In this paper, we tackle this problem in the context of public transport navigation system, where we aim to find candidates of bus route path given two bus stations. We model a novel lightweight bus network as Resource Description Framework (RDF) triples of directed bus lines and walking paths between connected stations. Indeed, we demonstrate that lightweight bus network can be achieved by exploiting the sub-property of RDF Schema (RDFS) and the transitivity and symmetry provided by Web Ontology Language (OWL). We also perform a scalability test of our approach using a real-world bus network in Bangkok, Thailand. Various patterns of SPARQL Protocol and RDF Query Language (SPARQL) query statements are validated, showing the usefulness of the RDF model. The further step of this paper is to work with bus schedules and travel time analysis in order to select some proper candidates for users through an application.},
booktitle = {Proceedings of the 2023 6th Artificial Intelligence and Cloud Computing Conference},
pages = {126–134},
numpages = {9},
keywords = {Graph Traversal, Knowledge Graph, Path Finding, Semantic Web, Transport Navigation System},
location = {Kyoto, Japan},
series = {AICCC '23}
}

@inproceedings{10.1145/3669754.3669784,
author = {Jayawardena, Lasal and Yapa, Prasan},
title = {Improving Quality and Domain-Relevancy of Paraphrase Generation with Graph-Based Retrieval Augmented Generation},
year = {2024},
isbn = {9798400717055},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3669754.3669784},
doi = {10.1145/3669754.3669784},
abstract = {Paraphrase generation is a fundamental area of research in Natural Language Processing (NLP) and Natural Language Generation (NLG), due to its sequence-to-sequence (Seq2Seq) nature. Paraphrasing, spanning across various domains, poses challenges for simpler model architectures due to the extensive knowledge required to generate paraphrases. The added constraint of generating diverse paraphrases further complicates the task for models trained on existing datasets. We present a methodology that leverages Graph-Based Retrieval Augmented Generation (G-RAG), capable of utilizing both entity and phrasal knowledge to address this issue. We demonstrate through experiments that this approach enables both complex models like Large Language models (LLMs) and smaller Seq2Seq models to generate more diverse paraphrases without compromising semantic similarity. Furthermore, this approach’s capacity to integrate domain-specific knowledge makes it particularly effective across different domains, enhancing its applicability in varied contexts. The results are further corroborated by human evaluation and extensive quantitative analysis focusing on semantic similarity, lexical diversity, syntactic diversity, and grammatical correctness to gauge high-quality paraphrases.},
booktitle = {Proceedings of the 2024 10th International Conference on Computing and Artificial Intelligence},
pages = {196–208},
numpages = {13},
keywords = {Graph-based Knowledge, Large Language Models, Natural Language Processing, Paraphrase Generation, Sequence-to-Sequence Models},
location = {Bali Island, Indonesia},
series = {ICCAI '24}
}

@inproceedings{10.1145/3631700.3653062,
author = {Purificato, Erasmo and Boratto, Ludovico and De Luca, Ernesto William},
title = {Paradigm Shifts in User Modeling: A Journey from Historical Foundations to Emerging Trends},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3653062},
doi = {10.1145/3631700.3653062},
abstract = {The presented tutorial aims to serve as a comprehensive roadmap for the UMAP community into the current user modeling research, focusing on the paradigm shifts that have transformed the research landscape in recent times. We will provide a complete overview of the large, long-standing, and ever-growing research fields of user modeling and user profiling, both from a historical and a technical point of view. We will then examine the definitions associated with each key term in this research domain, aiming to eliminate ambiguity and confusion in their usage. As the core of our tutorial, we present in-depth the paradigm shifts that have occurred in recent years, especially due to technological evolution, as well as the current research directions and novel trends in the field. In particular, we illustrate and discuss the advances in the following topics: implicit and explicit user profiling, user behavior modeling, user representation, and beyond-accuracy perspectives. The audience will be engaged in discussions during the whole presentation to foster the development of an interactive event. Detailed information and resources about the tutorial are available on the website: https://link.erasmopurif.com/tutorial-umap24.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {13–16},
numpages = {4},
keywords = {Paradigm Shifts, User Modeling, User Profiling},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@article{10.14778/3229863.3236248,
author = {Jammi, Manasa and Sen, Jaydeep and Mittal, Ashish and Verma, Sagar and Pahuja, Vardaan and Ananthanarayanan, Rema and Lohia, Pranay and Karanam, Hima and Saha, Diptikalyan and Sankaranarayanan, Karthik},
title = {Tooling framework for instantiating natural language querying system},
year = {2018},
issue_date = {August 2018},
publisher = {VLDB Endowment},
volume = {11},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3229863.3236248},
doi = {10.14778/3229863.3236248},
abstract = {Recent times have seen a growing demand for natural language querying (NLQ) interfaces to retrieve information from the structured data sources such as knowledge bases. Using this interface, business users can directly interact with a database without the knowledge of the query language or the data schema. Our earlier work describes a natural language query engine called ATHENA which has several shortcoming around ease of use and compatibility with data stores, formats and flows. In this demonstration paper, we present a tooling framework to address these challenges so that one can instantiate an NLQ system with utmost ease. Our framework makes it easy and practically applicable to all NLIDB scenarios involving different sources of structured data, file formats, and ontologies to enable natural language querying on top of them with minimal human configuration. We present the tool design and the solution to the challenges towards building such a system and demonstrate its applicability in the medical domain.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2014–2017},
numpages = {4}
}

@inproceedings{10.1145/3733723.3742469,
author = {Cuzzocrea, Alfredo},
title = {Vector Databases for Modelling, Managing and Querying Big Scientific Data: Models, Issues, Paradigms},
year = {2025},
isbn = {9798400714627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733723.3742469},
doi = {10.1145/3733723.3742469},
abstract = {Inspired by the emergence of scientific data in fields like as astronomy, climate research, and genomics, which presents significant challenges for conventional database systems. This vision paper investigates the adaptation of vector databases in order to describe, handle, and query large-scale scientific data. Moreover, we propose a road-map for embedding-centric data infrastructures, along with an analysis of current advancements and identification of important future research directions, including explainability and scalability. This research provides a foundational basis for next-generation interdisciplinary scientific discovery.},
booktitle = {Proceedings of the 37th International Conference on Scalable Scientific Data Management},
articleno = {24},
numpages = {5},
keywords = {Vector Databases, Advanced Scientific Data Representation, Advanced Scientific Data Management, Big Scientific Data},
location = {
},
series = {SSDBM '25}
}

@inproceedings{10.1145/3644523.3644610,
author = {Si, Tiange},
title = {A Hybrid Data-Knowledge-Driven Domain Ontology Construction Method for China's Financial Domain},
year = {2024},
isbn = {9798400709517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644523.3644610},
doi = {10.1145/3644523.3644610},
abstract = {The rapid increase of data in China's financial domain has brought difficulties in data organization and management, and the traditional flat organization of financial big data ignores the rich knowledge association in the data, and the study of knowledge association in the financial domain has received more and more attention from academia. Financial ontology can meet the demand of the financial industry for data quality and program rigor, and reflect the structure of the financial capital market and the associations between different entities. This study investigates and compares various ontology construction methods at home and abroad, and proposes a data-knowledge hybrid-driven approach to construct domain ontologies, which realizes the collaboration among people, knowledge and data with the core of "knowledge generation-data expansion-quality control". Under the guidance of this method, this study invited a research group of experts in the financial field to successfully construct the first financial ontology applicable to China's financial system by using financial data provided by several financial and economic information providers, such as Wind, CSMAR, CNRDS, etc., which provides exploration and guidance for the development of informatization in China's financial field, and on the basis of which a more comprehensive knowledge graph can be constructed in the future to further promote the construction of information resources in China's financial domain.},
booktitle = {Proceedings of the 2023 4th International Conference on Computer Science and Management Technology},
pages = {480–486},
numpages = {7},
location = {Xi'an, China},
series = {ICCSMT '23}
}

@inproceedings{10.1145/3534678.3539453,
author = {Geng, Yuxia and Chen, Jiaoyan and Zhang, Wen and Xu, Yajing and Chen, Zhuo and Z. Pan, Jeff and Huang, Yufeng and Xiong, Feiyu and Chen, Huajun},
title = {Disentangled Ontology Embedding for Zero-shot Learning},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539453},
doi = {10.1145/3534678.3539453},
abstract = {Knowledge Graph (KG) and its variant of ontology have been widely used for knowledge representation, and have shown to be quite effective in augmenting Zero-shot Learning (ZSL). However, existing ZSL methods that utilize KGs all neglect the intrinsic complexity of inter-class relationships represented in KGs. One typical feature is that a class is often related to other classes in different semantic aspects. In this paper, we focus on ontologies for augmenting ZSL, and propose to learn disentangled ontology embeddings guided by ontology properties to capture and utilize more fine-grained class relationships in different aspects. We also contribute a new ZSL framework named DOZSL, which contains two new ZSL solutions based on generative models and graph propagation models, respectively, for effectively utilizing the disentangled ontology embeddings. Extensive evaluations have been conducted on five benchmarks across zero-shot image classification (ZS-IMGC) and zero-shot KG completion (ZS-KGC). DOZSL often achieves better performance than the state-of-the-art, and its components have been verified by ablation studies and case studies. Our codes and datasets are available at https://github.com/zjukg/DOZSL.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {443–453},
numpages = {11},
keywords = {disentangled representation learning, knowledge graph, ontology, zero-shot learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3428757.3429091,
author = {Cardinale, Yudith and Cornejo-Lupa, Mar\'{\i}a A. and Ticona-Herrera, Regina and Barrios-Aranibar, Dennis},
title = {A Methodological Approach to Compare Ontologies: Proposal and Application for SLAM Ontologies},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429091},
doi = {10.1145/3428757.3429091},
abstract = {Representation of the knowledge related to any domain with flexible and well-defined models, such as ontologies, provides the base to develop efficient and interoperable solutions. Hence, a proliferation of ontologies in many domains is unleashed. It is necessary to define how to compare such ontologies to decide which one is the most suitable for specific needs of users/developers. Since the emerging developing of ontologies, several studies have proposed criteria to evaluate them. Nevertheless, there is still a lack of practical and reproducible guidelines to drive a comparative evaluation of ontologies as a systematic process. In this paper, we propose a methodological process to qualitatively and quantitatively compare ontologies at Lexical, Structural, and Domain Knowledge levels, considering Correctness and Quality perspectives. Since the evaluation methods of our proposal are based in a golden-standard, it can be customized to compare ontologies in any domain. To show the suitability of our proposal, we apply our methodological approach to conduct a comparative study of ontologies in the robotic domain, in particularly for the Simultaneous Localization and Mapping (SLAM) problem. With this study case, we demonstrate that with this methodological comparative process, we are able to identify the strengths and weaknesses of ontologies, as well as the gaps still needed to fill in the target domain (SLAM for our study case).},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {223–233},
numpages = {11},
keywords = {Autonomous and Mobile Robots, Ontologies Evaluation, Ontology, SLAM},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@article{10.1145/3277006.3277017,
author = {Deutch, Daniel and Frost, Nave and Gilad, Amir},
title = {Natural Language Explanations for Query Results},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/3277006.3277017},
doi = {10.1145/3277006.3277017},
abstract = {Multiple lines of research have developed Natural Language (NL) interfaces for formulating database queries. We build upon this work, but focus on presenting a highly detailed form of the answers in NL. The answers that we present are importantly based on the provenance of tuples in the query result, detailing not only the results but also their explanations. We develop a novel method for transforming provenance information to NL, by leveraging the original NL query structure. Furthermore, since provenance information is typically large and complex, we present two solutions for its effective presentation as NL text: one that is based on provenance factorization, with novel desiderata relevant to the NL case, and one that is based on summarization.},
journal = {SIGMOD Rec.},
month = sep,
pages = {42–49},
numpages = {8}
}

@inproceedings{10.1145/3695080.3695151,
author = {Jiang, Xinran and Zhang, Wenxue},
title = {"Construction of Conceptual Layer of Knowledge Graph of State-target Differentiation \&amp; Treatment based on Ontology and Prot\'{e}g\'{e}"},
year = {2024},
isbn = {9798400710223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3695080.3695151},
doi = {10.1145/3695080.3695151},
abstract = {As an innovative strategy, "State-target Differentiation \&amp; Treatment" (STDT) combines traditional Chinese medicine and Western medicine, making it a prominent development direction in the current Traditional Chinese Medicine (TCM) diagnosis and treatment system. With the deepening of research in this field, a significant number of research achievements was coming out. However, massive scattered data lack of integration and structuring. Therefore, domain knowledge storage and relational reasoning become pressing issues in the field of STDT. This paper obtains raw data from literature retrieval database and domain monograph of STDT. By analyzing the existing semantic framework of TCM and the clinical characteristics of STDT, we summarize the knowledge structure of STDT theory in three aspects "class, data properties, and object properties " in the form of table. Finally, the ontology of STDT knowledge graph is constructed with Protege, a commonly used ontology development tool. From the therapeutic process of macro-control of physical state to the knowledge of the micro-pharmacological level of TCM targets, the visualized display of the STDT knowledge graph realizes the combination of "state" and "target". The utilization of ontology technology enables the blending, recombination, and reasoning of the domain knowledge of STDT, laying the foundation and offering reference for the subsequent construction of the domain knowledge base.},
booktitle = {Proceedings of the 2024 International Conference on Cloud Computing and Big Data},
pages = {414–422},
numpages = {9},
location = {Dali, China},
series = {ICCBD '24}
}

@inproceedings{10.1145/3672608.3707826,
author = {Jungmann, Michelle and Lazarova-Molnar, Sanja},
title = {Fusing Expert Knowledge and Internet of Things Data for Digital Twin Models: Addressing Uncertainty in Expert Statements},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707826},
doi = {10.1145/3672608.3707826},
abstract = {Extracting Digital Twin models by fusing expert knowledge with Internet of Things data remains a challenging and open research area. Existing literature offers very limited approaches for seamless and systematic extraction of Digital Twin models from these combined sources. In this paper, we address the research gap by proposing a novel approach that considers and integrates the uncertainty inherent in human expert knowledge into the extraction processes of Digital Twin models. Given that experts possess unique experiences, contextual understandings and judgements, their knowledge can be highly divergent, complex, ambiguous, and even incorrect or incomplete. Consequently, not all expert knowledge statements should be equally weighted in the resulting simulation models. Our contributions include a comprehensive literature review on the uncertainty in expert knowledge and the proposal of an approach to integrate this uncertainty in the extraction of Digital Twin models from fused expert knowledge and IoT data. We demonstrate our approach through a case study in reliability assessment.1},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {874–881},
numpages = {8},
keywords = {ACM proceedings, digital twins, fusion of data and expert knowledge, uncertainty in expert knowledge, industry 4.0},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1145/3477314.3507301,
author = {Martin, Tomas and Fuentes, Victor and Valtchev, Petko and Diallo, Abdoulaye Banir\'{e} and Lacroix, Ren\'{e}},
title = {Generalized graph pattern discovery in linked data with data properties and a domain ontology},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507301},
doi = {10.1145/3477314.3507301},
abstract = {Nowadays, in many practical situations, analytical tasks need to be performed on complex heterogeneous data, often described by a domain ontology (DO). Such cases abound in life science fields such as agro-informatics, where observations and measures on animals/plants are logged for subsequent mining. The data is naturally structured as graph(s), unlabelled and missing some values, hence it fits well pattern mining. In our own precision farming project aimed at decision support for dairy cow management, we mine for knowledge in milk production data. In one task, we aim at contrast patterns explaining the relative impact of independent production factors. To that end, ontologically-generalized graph patterns (OGPs), a variety of generalized graph patterns, where vertices and edges are labelled by DO classes and properties, respectively, were defined. A mining methodology was also designed that reconciles OWL DOs, abstraction from RDF graphs and literals in data. To address the well-known cost-related limitations of graph mining -exacerbated here by class/property specializations and data properties- we split the mining task into (1) mining of generic object property topology patterns and (2) label refinement. Those focus on two sorts of OGPs, called topologies and class stars, respectively, which, after being mined separately, get (3) assembled into fully-fledged OGPs.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1890–1899},
numpages = {10},
keywords = {generalized patterns, graph data, ontologies, pattern mining},
location = {Virtual Event},
series = {SAC '22}
}

@article{10.1145/3148239,
author = {Bertossi, Leopoldo and Milani, Mostafa},
title = {Ontological Multidimensional Data Models and Contextual Data Quality},
year = {2018},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3148239},
doi = {10.1145/3148239},
abstract = {Data quality assessment and data cleaning are context-dependent activities. Motivated by this observation, we propose the Ontological Multidimensional Data Model (OMD model), which can be used to model and represent contexts as logic-based ontologies. The data under assessment are mapped into the context for additional analysis, processing, and quality data extraction. The resulting contexts allow for the representation of dimensions, and multidimensional data quality assessment becomes possible. At the core of a multidimensional context, we include a generalized multidimensional data model and a Datalog± ontology with provably good properties in terms of query answering. These main components are used to represent dimension hierarchies, dimensional constraints, and dimensional rules and define predicates for quality data specification. Query answering relies on and triggers navigation through dimension hierarchies and becomes the basic tool for the extraction of quality data. The OMD model is interesting per se beyond applications to data quality. It allows for a logic-based and computationally tractable representation of multidimensional data, extending previous multidimensional data models with additional expressive power and functionalities.},
journal = {J. Data and Information Quality},
month = jan,
articleno = {14},
numpages = {36},
keywords = {Datalog±, Ontology-based data access, query answering, weakly-sticky programs}
}

@article{10.1145/3674501,
author = {Zhao, Xiaoyan and Deng, Yang and Yang, Min and Wang, Lingzhi and Zhang, Rui and Cheng, Hong and Lam, Wai and Shen, Ying and Xu, Ruifeng},
title = {A Comprehensive Survey on Relation Extraction: Recent Advances and New Frontiers},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {11},
issn = {0360-0300},
url = {https://doi.org/10.1145/3674501},
doi = {10.1145/3674501},
abstract = {Relation extraction (RE) involves identifying the relations between entities from underlying content. RE serves as the foundation for many natural language processing (NLP) and information retrieval applications, such as knowledge graph completion and question answering. In recent years, deep neural networks have dominated the field of RE and made noticeable progress. Subsequently, the large pre-trained language models (PLMs) have taken the state-of-the-art RE to a new level. This survey provides a comprehensive review of existing deep learning techniques for RE. First, we introduce RE resources, including datasets and evaluation metrics. Second, we propose a new taxonomy to categorize existing works from three perspectives, i.e., text representation, context encoding, and triplet prediction. Third, we discuss several important challenges faced by RE and summarize potential techniques to tackle these challenges. Finally, we outline some promising future directions and prospects in this field. This survey is expected to facilitate researchers’ collaborative efforts to address the challenges of real-world RE systems.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {293},
numpages = {39},
keywords = {Relation extraction, deep learning, pre-trained language models, low-resource relation extraction}
}

@inproceedings{10.1145/3701716.3715487,
author = {Mitra, Aniket and Venugopal, Vinu E.},
title = {Stat-n-Ball: Enhancing Probabilistic Knowledge Graph Embeddings with Geometric and Confidence-Aware Models},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715487},
doi = {10.1145/3701716.3715487},
abstract = {Region-based Knowledge Graph Embedding (R-KGE) models, which represent entities as convex shapes (e.g., balls) and relations as geometric transformations in vector space, offer a promising approach for explainable and accurate reasoning over ontologies. However, existing R-KGE models assume perfect reliability of Knowledge Graphs (KGs), which is often unrealistic as real-world KGs are noisy and incomplete. To address this, Probabilistic Knowledge Graphs (P-KGs) associate axioms with confidence scores, capturing the uncertainty of their truthfulness. We propose Stat-n-Ball, a novel R-KGE framework that incorporates confidence scores by representing axioms' certainty as overlapping volumes between entities in vector space. Our approach enhances the geometric representation of KGs, enabling accurate link prediction and confidence estimation in probabilistic settings. Experimental evaluations on standard P-KG datasets demonstrate that Stat-n-Ball achieves atleast a 2\texttimes{} improvement in entity association detection and a minimum 5\% reduction in confidence prediction error compared to state-of-the-art models. These results underscore its effectiveness in handling noisy and uncertain KGs while preserving logical and semantic integrity.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1194–1198},
numpages = {5},
keywords = {n-ball embedding, noisy knowledge graphs, probabilistic knowledge graph embedding},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3236405.3236427,
author = {Li, Yang},
title = {Feature and variability extraction from natural language software requirements specifications},
year = {2018},
isbn = {9781450359450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236405.3236427},
doi = {10.1145/3236405.3236427},
abstract = {Extracting feature and variability from requirement specifications is an indispensable activity to support systematic integration related single software systems into Software Product Line (SPL). Performing variability extraction is time-consuming and inefficient, since massive textual requirements need to be analyzed and classified. Despite the improvement of automatically features and relationships extraction techniques, existing approaches are not able to provide high accuracy and applicability in real-world scenarios. The aim of my doctoral research is to develop an automated technique for extracting features and variability which provides reliable solutions to simplify the work of domain analysis. I carefully analyzed the state of the art and identified main limitations so far: accuracy and automation. Based on these insights, I am developing a methodology to address this challenges by making use of advanced Natural Language Processing (NLP) and machine learning techniques. In addition, I plan to design reasonable case study to evaluate the proposed approaches and empirical study to investigate usability in practice.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 2},
pages = {72–78},
numpages = {7},
keywords = {feature identification, requirement documents, reverse engineering, software product lines, variability extraction},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inbook{10.5555/3712729.3712835,
author = {Tolk, Andreas},
title = {Hybrid Modeling Integrating Artificial Intelligence and Modeling \&amp; Simulation Paradigms},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {This paper discusses the complementary relationship between Modeling and Simulation (M&amp;S) and Artificial Intelligence (AI) methods like machine learning. While M&amp;S uses algorithms to model system behavior from input parameters, AI learns patterns from correlation in data. The paper argues that hybrid models combining M&amp;S and AI can be more powerful than either alone. It provides a conceptual framework showing how M&amp;S and AI can be integrated in sequential, parallel, complementary or competitive configurations. Several example applications are given where AI enhances M&amp;S and vice versa, such as using AI to optimize simulation parameters, generate synthetic training data for AI from simulations, interpret AI model behavior through simulation, and automate aspects of simulation development with AI assistance. The potential benefits of hybrid AI/M&amp;S modeling span improved accuracy, efficiency, trustworthiness and cross-disciplinary collaboration. The paper calls for further research developing a solid theoretical foundation for merging these complementary paradigms.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1271–1280},
numpages = {10}
}

@inproceedings{10.1145/3624007.3624060,
author = {Correa Restrepo, Camilo and Robin, Jacques and Mazo, Raul},
title = {Generating Constraint Programs for Variability Model Reasoning: A DSL and Solver-Agnostic Approach},
year = {2023},
isbn = {9798400704062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624007.3624060},
doi = {10.1145/3624007.3624060},
abstract = {Verifying and configuring large Software Product Lines (SPL) requires automation tools. Current state-of-the-art approaches involve translating variability models into a formalism accepted as input by a constraint solver. There are currently no standards for variability modeling languages (VML). There is also a variety of constraint solver input languages. This has resulted in a multiplication of ad-hoc architectures and tools specialized for a single pair of VML and solver, fragmenting the SPL community. To overcome this limitation, we propose a novel architecture based on model-driven code generation, where the syntax and semantics of VMLs can be declaratively specified as data, and a standard, human-readable, formal pivot language is used between the VML and the solver input language. This architecture is the first to be fully generic by being agnostic to both VML and the solver paradigm. To validate the genericity of the approach, we have implemented a prototype tool together with declarative specifications for the syntax and semantics of two different VMLs and two different solver families. One VML is for classic, static SPL, and the other for run-time reconfigurable dynamic SPL with soft constraints to be optimized during configuration. The two solver families are Constraint Satisfaction Programs (CSP) and Constraint Logic Programs (CLP).},
booktitle = {Proceedings of the 22nd ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {138–152},
numpages = {15},
keywords = {Automated Reasoning, Configuration Automation, Generic Architecture, Software Product Lines},
location = {Cascais, Portugal},
series = {GPCE 2023}
}

@inbook{10.1145/3672608.3707884,
author = {Zampierin, Luca and Frasincar, Flavius},
title = {An Unsupervised Approach for Aspect-Based Sentiment Classification Using Attentional Neural Models},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707884},
abstract = {With the vast amount of reviews available on the Web, in the past decades, a growing share of literature has focused on sentiment analysis. Aspect-based sentiment classification is the subtask that seeks to detect the sentiment expressed by the content creators towards a defined target typically within a sentence. This paper introduces two novel unsupervised attentional neural network models for aspect-based sentiment classification, and tests them on English restaurant reviews. The first model employs an autoencoder-like structure to learn a sentiment embedding matrix where each row of the matrix represents the embedding for one sentiment. To improve the model, a target-based attention mechanism is included that de-emphasizes irrelevant words. Last, a redundancy and a seed regularization term constrain the sentiment embedding matrix. The second model extends the first by including a Bi-LSTM layer in the attention mechanism to exploit contextual information. Although both models construct meaningful sentiment embeddings, experimental results indicate that the inclusion of the Bi-LSTM in the attention mechanism leads to a more precise attention mechanisms and, thus, better predictions. The best model, i.e., the second, outperforms all investigated unsupervised and weakly supervised algorithms for aspect-based sentiment classification from the literature.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {2069–2077},
numpages = {9}
}

@article{10.1145/3623379,
author = {Saidi, Rakia and Jarray, Fethi},
title = {Stacking of BERT and CNN Models for Arabic Word Sense Disambiguation},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {11},
issn = {2375-4699},
url = {https://doi.org/10.1145/3623379},
doi = {10.1145/3623379},
abstract = {We propose a new approach for Arabic Word Sense Disambiguation (AWSD) by hybridization of single-layer Convolutional Neural Network (CNN) with contextual representation (BERT). WSD is the task of automatically detecting the correct meaning of a word used in a given context. WSD can be performed as a classification task, and the context is generally a short sentence. Kim [26] proved that combining a CNN with an RNN (recurrent neural network) provides a good result for text classification. Here, we use a concatenation of BERT models as a word embedding to get simultaneously the target and context representation. Our approach improves the performance of WSD in Arabic languages. The experimental results show that our model outperforms the state-of-the-art approaches and improves the accuracy of 96.42\% on the Arabic WordNet dataset.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {247},
numpages = {14},
keywords = {Word sense disambiguation, Arabic text, supervised approach, transformer, BERT, convolutional neural network}
}

@proceedings{10.1145/3689217,
title = {LAMPS '24: Proceedings of the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis},
year = {2024},
isbn = {9798400712098},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {This volume contains papers presented at the 1st ACM Workshop on Large AI Systems and Models with Privacy and Safety Analysis (LAMPS), which was held on October 14, 2024, in Salt Lake City, USA.This year we received 18 paper submissions from Australia, China, Italy, Luxembourg, Singapore, South Korea, USA, and Vietnam, and 11 high-quality papers were accepted. Each contributed paper was rigorously peer-reviewed by reviewers who were drawn from a pool of expert technical committee members in machine learning security and privacy. Each paper received two detailed review comments and one meta review comments that summarise the weaknesses/issues to be addressed in the camera-ready revision or future work.},
location = {Salt Lake City, UT, USA}
}

@article{10.1145/3150227,
author = {Bergmayr, Alexander and Breitenb\"{u}cher, Uwe and Ferry, Nicolas and Rossini, Alessandro and Solberg, Arnor and Wimmer, Manuel and Kappel, Gerti and Leymann, Frank},
title = {A Systematic Review of Cloud Modeling Languages},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3150227},
doi = {10.1145/3150227},
abstract = {Modern cloud computing environments support a relatively high degree of automation in service provisioning, which allows cloud service customers (CSCs) to dynamically acquire services required for deploying cloud applications. Cloud modeling languages (CMLs) have been proposed to address the diversity of features provided by cloud computing environments and support different application scenarios, such as migrating existing applications to the cloud, developing new cloud applications, or optimizing them. There is, however, still much debate in the research community on what a CML is, and what aspects of a cloud application and its target cloud computing environment should be modeled by a CML. Furthermore, the distinction between CMLs on a fine-grain level exposing their modeling concepts is rarely made. In this article, we investigate the diverse features currently provided by existing CMLs. We classify and compare them according to a common framework with the goal to support CSCs in selecting the CML that fits the needs of their application scenario and setting. As a result, not only features of existing CMLs are pointed out for which extensive support is already provided but also in which existing CMLs are deficient, thereby suggesting a research agenda.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {22},
numpages = {38},
keywords = {Cloud computing, domain-specific languages, modeling}
}

@inproceedings{10.1145/3447548.3467138,
author = {Hao, Junheng and Lei, Chuan and Efthymiou, Vasilis and Quamar, Abdul and \"{O}zcan, Fatma and Sun, Yizhou and Wang, Wei},
title = {MEDTO: Medical Data to Ontology Matching Using Hybrid Graph Neural Networks},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467138},
doi = {10.1145/3447548.3467138},
abstract = {Medical ontologies are widely used to describe and organize medical terminologies and to support many critical applications on healthcare databases. These ontologies are often manually curated (e.g., UMLS, SNOMED CT, and MeSH) by medical experts. Medical databases, on the other hand, are often created by database administrators, using different terminology and structures. The discrepancies between medical ontologies and databases compromise interoperability between them. Data to ontology matching is the process of finding semantic correspondences between tables in databases to standard ontologies. Existing solutions such as ontology matching have mostly focused on engineering features from terminological, structural, and semantic model information extracted from the ontologies. However, this is often labor intensive and the accuracy varies greatly across different ontologies. Worse yet, the ontology capturing a medical database is often not given in practice. In this paper, we propose MEDTO, a novel end-to-end framework that consists of three innovative techniques: (1) a lightweight yet effective method that bootstrap a semantically rich ontology from a given medical database, (2) a hyperbolic graph convolution layer that encodes hierarchical concepts in the hyperbolic space, and (3) a heterogeneous graph layer that encodes both local and global context information of a concept. Experiments on two real-world medical datasets matching against SNOMED CT show significant improvements compared to the state-of-the-art methods. MEDTO also consistently achieves competitive results on a benchmark from the Ontology Alignment Evaluation Initiative.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2946–2954},
numpages = {9},
keywords = {graph neural network, medical data, ontology matching},
location = {Virtual Event, Singapore},
series = {KDD '21}
}

@proceedings{10.1145/3722565,
title = {FMSys: Proceedings of the 2nd International Workshop on Foundation Models for Cyber-Physical Systems \&amp; Internet of Things},
year = {2025},
isbn = {9798400716089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irvine, CA, USA}
}

@inproceedings{10.1145/3561877.3561896,
author = {Wang, Xiaoning and Zhang, Yang},
title = {Research on CGAN-BERT and RGAN-BERT Models for Short Text Classification based on Semi-Supervised Model},
year = {2022},
isbn = {9781450396837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3561877.3561896},
doi = {10.1145/3561877.3561896},
abstract = {With the rapid development of artificial intelligence, a large number of short texts cause a certain degree of information redundancy. Text classification technology can help people classify and process information, and has important applications in the fields of recommendation system, public opinion monitoring and information retrieval. However, short text information in different fields has the characteristics of industry professionalism and fast updating of language style. The resulting problems such as poor applicability of the model and bottlenecks in annotation make the effect of traditional classification methods in short text analysis limited. Therefore, we propose semi supervised text classification models CGAN-BERT and RGAN-BERT based on convolutional neural network and cyclic neural network respectively. The newly designed generator and discriminator are more conducive to the game process. We evaluate our model and other classical models on several public data sets. The experimental results show that our proposed model is better than other models.},
booktitle = {Proceedings of the 5th International Conference on Information Science and Systems},
pages = {118–124},
numpages = {7},
keywords = {Recurrent Neural Network, convolutional neural networks, gan, natural language processing, text classification},
location = {Beijing, China},
series = {ICISS '22}
}

@inproceedings{10.1145/3568562.3568602,
author = {Agafonov, Anton and Ponomarev, Andrew},
title = {An Experiment on Localization of Ontology Concepts in Deep Convolutional Neural Networks},
year = {2022},
isbn = {9781450397254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568562.3568602},
doi = {10.1145/3568562.3568602},
abstract = {Deep neural networks have recently evolved into a powerful AI tool, reaching near-human performance level in many tasks, and in some tasks even surpassing it. However, a significant drawback of neural networks is the lack of explainability and interpretability — it is hard to say why a neural network arrived to a certain conclusion. This significantly limits application of neural networks in critical tasks and undermines trust in human-AI collaboration. It has been recently shown that internal representations constructed by a neural network can often be aligned with a domain ontology. This opens a promising way to provide explanations of a neural network in human terms. In this paper, we discuss the results of the experiment aimed at understanding what layers of a neural network are the most perspective for the alignment with given ontology concept. To do so, we build concept localization maps for XTRAINS — a synthetic dataset consisting of images and their ontological annotations. The importance of such maps is that they can be used for the development of efficient concept alignment heuristics. The experiment mostly supports the intuition that high-level concepts are localized mostly in the activations of last layers of a neural network (near its head), while lower-level concepts might be better extracted from middle layers.},
booktitle = {Proceedings of the 11th International Symposium on Information and Communication Technology},
pages = {82–87},
numpages = {6},
keywords = {XAI, explainable AI, neural networks, neuro-symbolic intelligence, ontologies},
location = {Hanoi, Vietnam},
series = {SoICT '22}
}

@inproceedings{10.1145/3701716.3715172,
author = {Dew, Rebecca and Li, Mingzhao and Liu, Weidong and Baratha Raj, Sandya},
title = {Developing a Comprehensive Task Framework for Effective Workforce Analysis},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715172},
doi = {10.1145/3701716.3715172},
abstract = {Effective workforce analysis, planning, and management require a deep understanding of the tasks and skills associated with different roles. This paper introduces a novel methodology for developing a comprehensive task framework that leverages Large Language Models (LLMs) and large-scale job ads data. We first propose an innovative approach to task taxonomy design, which involves the decomposition and reconstruction of tasks into a hierarchical structure based on action-object pairings, systematically refined using LLMs. The methodology extends to integrating the taxonomy with occupation and skill linkages derived from job ads, ensuring alignment with real-world workforce dynamics. Finally, we demonstrate the practical value of this framework through a visual analytics system that enables interactive exploration and analysis of tasks, occupations, and associated skills, highlighting its potential to transform workforce analysis. Demo video: https://bit.ly/41txBZK},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2819–2822},
numpages = {4},
keywords = {gpt, large language model, visual analytics, workforce analysis},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3716368.3735295,
author = {Gautam, Ashish and Patton, Robert and Potok, Thomas and Kannan, Ramakrishnan and Aimone, James and Severa, William},
title = {AI-Powered Knowledge Graphs for Neuromorphic and Energy-Efficient Computing},
year = {2025},
isbn = {9798400714962},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3716368.3735295},
doi = {10.1145/3716368.3735295},
abstract = {The surge in scientific literature obscures breakthroughs and hinders the discovery of new research paths. We propose an artificial intelligence (AI) powered framework using large language models (LLMs) and knowledge graphs (KGs) to automate parts of scientific discovery, focusing on energy-efficient AI circuits. Our hybrid approach combines LLMs, structured data, and ontology-based reasoning to construct a comprehensive knowledge graph that integrates insights across computational neuroscience, spiking neuron models, learning rules, architectural motifs, and neuromorphic device technologies. This multi-domain representation enables the generation of hypotheses that connect biological function with implementable, energy-efficient hardware architectures. Using KG embeddings and graph neural networks, the framework generates hypotheses for novel circuits, validates them through optimization on exascale HPC systems, and with tools like SuperNeuro and Fugu, the most promising designs will be prototyped in hardware. This open-source system aims to accelerate discoveries and bridging neuroscience with hardware innovation, drive collaboration, and unlock new opportunities in low-power AI computing.},
booktitle = {Proceedings of the Great Lakes Symposium on VLSI 2025},
pages = {996–1001},
numpages = {6},
keywords = {Knowledge graphs, neuromorphic computing, energy-efficient circuits, large language models, hypothesis generation, spiking neural networks, architectural synthesis, scientific discovery automation, STDP, cortical microcircuits.},
location = {
},
series = {GLSVLSI '25}
}

@inproceedings{10.1145/3652620.3688206,
author = {Rabbi, Fazle},
title = {A Model-Based Framework for Exploring Conflict Dynamics},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688206},
doi = {10.1145/3652620.3688206},
abstract = {This paper introduces a novel framework for conflict analysis that leverages advanced visual modeling techniques. By employing comparative analysis, key variables influencing armed conflicts are identified and analyzed. The framework includes a meta-model representing domain concepts such as the goals and strategies of conflicting parties, escalating stages, and impacts of conflicts.Conflict escalation is a complex process characterized by interactions between opposing parties. This paper presents a structured model that outlines how conflicts evolve and intensify over time. We adapt a meta-modeling framework called the Diagram Predicate Framework (DPF) to represent conflict-related concepts and extend it to support abstract view generation. This framework facilitates the analysis of conflict trends and the study of dynamics across various levels of abstraction.A computational model based on category theory is proposed for trend analysis, enabling the extraction of patterns of conflict evolution and the comparison of strategies and goals at different escalation stages. Categorical operations such as pullback and limit construction are employed to compute conflict evolution and identify common structures among conflict instances, providing insights into conflict dynamics across diverse zones.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {745–754},
numpages = {10},
keywords = {conflict analysis, computational journalism, category theory, metamodeling},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.1145/3527838,
author = {Akram, Muhammad Waseem and Salman, Muhammad and Bashir, Muhammad Farrukh and Salman, Syed Muhammad Saad and Gadekallu, Thippa Reddy and Javed, Abdul Rehman},
title = {A Novel Deep Auto-Encoder Based Linguistics Clustering Model for Social Text},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3527838},
doi = {10.1145/3527838},
abstract = {The wide adoption of media and social media has increased the amount of digital content to an enormous level. Natural language processing (NLP) techniques provide an opportunity to extract and explore meaningful information from a large amount of text. Among natural languages, Urdu is one of the widely used languages worldwide for spoken and written communications. Due to its wide adopt-ability, digital content in the Urdu language is increasing briskly, especially with social media and online NEWS feeds. Government agencies and advertisers must filter and understand the content to analyze the trends and cohorts in their interest and national prerogative. Clustering is considered a baseline and one of the first steps in natural language understanding. There are many state-of-the-art clustering techniques specifically for English, French, and Arabic, but no significant research has been conducted in Urdu language processing. Doing it for short text segments is challenging because of limited features and the absence of meaningful language discourse and nuance. Many rule-based NLP techniques are adopted to overcome these issues, relying on human-designed features and rules. Therefore, these methods do not promise remarkable results. Alongside NLP, deep learning techniques are pretty efficient in capturing contextual information with minimal noise compared to other traditional methods. By taking on this challenging job, we develop a deep learning-based technique for Urdu short text clustering for the very first time without a human-designed feature. In this paper, we propose a method of short text clustering using a deep neural network that automatically learns feature representations and clustering assignments simultaneously. This method learns clustering objectives by converting the high dimensional feature space to a low dimensional feature space. Our experiments on the Urdu NEWS headlines dataset show remarkable results compared to state-of-the-art methods.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
keywords = {Clustering, Urdu, Social media, Text, Low resource language}
}

@inproceedings{10.1145/3606305.3606321,
author = {Ivanova, Tatyana Ivanova},
title = {Semi-automatic ontology development for supporting personalized tutoring},
year = {2023},
isbn = {9798400700477},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606305.3606321},
doi = {10.1145/3606305.3606321},
abstract = {Many researches have working the last two decades on ontology learning, using NLP, data mining, and machine learning, but experiments show, that every ontology learning method have his precision and recall less than 1, and automatically developed ontologies need from human evaluation and modification. So, participation of experts is a must in ontology development to guarantee high quality and the semi-automatic ontology development is the only approach having potential to ensure cheaper development of high-quality ontologies. In this research we will discuss semi-automatic support of ontology engineering process, based on automated knowledge extraction from text, semi-structured or structured sources. Most of automated ontology development methods and algorithms are domain – dependent. We analyze specifics of semi-automatic ontology development in the e-learning domain and propose software architecture of semi-automatic ontology development framework for educational ontologies.},
booktitle = {Proceedings of the 24th International Conference on Computer Systems and Technologies},
pages = {180–185},
numpages = {6},
location = {Ruse, Bulgaria},
series = {CompSysTech '23}
}

@inproceedings{10.1145/3383583.3398529,
author = {Scharpf, Philipp and Schubotz, Moritz and Youssef, Abdou and Hamborg, Felix and Meuschke, Norman and Gipp, Bela},
title = {Classification and Clustering of arXiv Documents, Sections, and Abstracts, Comparing Encodings of Natural and Mathematical Language},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398529},
doi = {10.1145/3383583.3398529},
abstract = {In this paper, we show how selecting and combining encodings of natural and mathematical language affect classification and clustering of documents with mathematical content. We demonstrate this by using sets of documents, sections, and abstracts from the arXiv preprint server that are labeled by their subject class (mathematics, computer science, physics, etc.) to compare different encodings of text and formulae and evaluate the performance and runtimes of selected classification and clustering algorithms. Our encodings achieve classification accuracies up to 82.8\% and cluster purities up to 69.4\% (number of clusters equals number of classes), and 99.9\% (unspecified number of clusters) respectively. We observe a relatively low correlation between text and math similarity, which indicates the independence of text and formulae and motivates treating them as separate features of a document. The classification and clustering can be employed, e.g., for document search and recommendation. Furthermore, we show that the computer outperforms a human expert when classifying documents. Finally, we evaluate and discuss multi-label classification and formula semantification.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {137–146},
numpages = {10},
keywords = {document classification, document clustering, information retrieval, machine learning, mathematical information retrieval},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3571473.3571502,
author = {Castro, Murillo and Barcellos, Monalessa},
title = {An Ontology to support Knowledge Management Solutions for Human-Computer Interaction Design},
year = {2023},
isbn = {9781450399999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571473.3571502},
doi = {10.1145/3571473.3571502},
abstract = {Developing interactive systems is a challenging task that involves concerns related to the human-computer interaction (HCI), such as usability and user experience. Therefore, HCI design is a core issue to the quality of such systems. HCI design often involves people with different backgrounds (e.g., Arts, Software Engineering, Design). This makes knowledge transfer a challenging issue due to the lack of a common conceptualization about HCI design, leading to semantic interoperability problems, such as ambiguity and imprecision when interpreting shared information. Ontologies have been acknowledged as a successful approach to represent domain knowledge and support knowledge-based solutions. Hence, in this work, we propose to explore the use of ontologies to represent structured knowledge of HCI design and improve knowledge sharing in this context. We developed the Human-Computer Interaction Design Ontology (HCIDO), which is part of the Human-Computer Interaction Ontology Network (HCI-ON) and is connected to the Software Engineering Ontology Network (SEON). By making knowledge related to the HCI design domain explicit and structured, HCIDO helped us to develop KTID, a tool that aims to support capturing and sharing knowledge to aid in HCI design by allowing HCI designers to annotate information about design choices in design artifacts shared with HCI design stakeholders. Preliminary results indicate that the tool can be particularly useful for novice HCI designers.},
booktitle = {Proceedings of the XXI Brazilian Symposium on Software Quality},
articleno = {33},
numpages = {10},
keywords = {HCI Design, Knowledge Management, Ontology, User Interface},
location = {Curitiba, Brazil},
series = {SBQS '22}
}

@inproceedings{10.1145/3644713.3644729,
author = {Meftah, Mohammed Charaf Eddine and Kazar, Okba},
title = {A new ontological approach for multilingual scientific research},
year = {2024},
isbn = {9798400709036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644713.3644729},
doi = {10.1145/3644713.3644729},
abstract = {Abstract— Scientific research is all the actions undertaken to produce and develop scientific knowledge. The representation of this knowledge can take various forms: it can be publications, reports, patents, etc. This knowledge can be incorporated into scientific social networks and applications. The scientific social networks in their current form depend on the title, the keywords, and the ontology to compare and link the relationship between different scientific research; there may be different scientific research with the same keywords; the same scientific research may have different titles. In addition, scientific research may be written in different languages, but current scientific social networks do not take into account the multiple languages of researchers and research. they cannot link the relationship and compare scientific research written in different languages. To solve these problems, this paper proposes the use of a multi-lingual ontology to determine and describe scientific research. This work uses ontology in the context of a conceptual indexation, by separating the concept from the term, thus, the result of this work according to the proposed approach will make it possible to express the concept (knowledge extra-linguistic), in different languages, this will allow comparing and linking the relationship between scientific research written in different languages and measuring the percentage of similarity and difference between them. Dealing with multilingualism in scientific research is a very important contribution to this field.},
booktitle = {Proceedings of the 7th International Conference on Future Networks and Distributed Systems},
pages = {114–124},
numpages = {11},
location = {Dubai, United Arab Emirates},
series = {ICFNDS '23}
}

@article{10.14778/3659437.3659461,
author = {Kayali, Moe and Lykov, Anton and Fountalis, Ilias and Vasiloglou, Nikolaos and Olteanu, Dan and Suciu, Dan},
title = {Chorus: Foundation Models for Unified Data Discovery and Exploration},
year = {2024},
issue_date = {April 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {8},
issn = {2150-8097},
url = {https://doi.org/10.14778/3659437.3659461},
doi = {10.14778/3659437.3659461},
abstract = {We apply foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMS) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. We investigate the fundamental characteristics of this approach including generalizability to several foundation models and the impact of non-determinism on the outputs. All in all, this suggests a future direction in which disparate data management tasks can be unified under foundation models.},
journal = {Proc. VLDB Endow.},
month = apr,
pages = {2104–2114},
numpages = {11}
}

@article{10.1145/3604550,
author = {Liu, Yaochen and Li, Qiuchi and Wang, Benyou and Zhang, Yazhou and Song, Dawei},
title = {A Survey of Quantum-cognitively Inspired Sentiment Analysis Models},
year = {2023},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3604550},
doi = {10.1145/3604550},
abstract = {Quantum theory, originally proposed as a physical theory to describe the motions of microscopic particles, has been applied to various non-physics domains involving human cognition and decision-making that are inherently uncertain and exhibit certain non-classical, quantum-like characteristics. Sentiment analysis is a typical example of such domains. In the last few years, by leveraging the modeling power of quantum probability (a non-classical probability stemming from quantum mechanics methodology) and deep neural networks, a range of novel quantum-cognitively inspired models for sentiment analysis have emerged and performed well. This survey presents a timely overview of the latest developments in this fascinating cross-disciplinary area. We first provide a background of quantum probability and quantum cognition at a theoretical level, analyzing their advantages over classical theories in modeling the cognitive aspects of sentiment analysis. Then, recent quantum-cognitively inspired models are introduced and discussed in detail, focusing on how they approach the key challenges of the sentiment analysis task. Finally, we discuss the limitations of the current research and highlight future research directions.},
journal = {ACM Comput. Surv.},
month = aug,
articleno = {15},
numpages = {37},
keywords = {Quantum-cognitively inspired models, non-classical probability from quantum mechanics methodology, sentiment analysis, sarcasm detection, emotion recognition}
}

@inproceedings{10.1145/3554364.3559139,
author = {de Freitas, Alexandre A. C. and Scalser, Murilo B. and Costa, Simone D. and Barcellos, Monalessa P.},
title = {Towards an ontology-based approach to develop software systems with adaptive user interface},
year = {2022},
isbn = {9781450395069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3554364.3559139},
doi = {10.1145/3554364.3559139},
abstract = {The new ways of manipulating computers, smartphones and other devices have brought challenges such as the need to ensure a good usability when different user types use the same system. Adaptive user interface (AUI) systems are a possible solution. They change the user interface to better meet the needs of different users. However, developing such systems is not trivial. It is necessary to capture the users' characteristics and preferences and constantly adapt the system accordingly. In this paper, we discuss the use of ontologies to support the development of AUI systems. We argue that by providing structured knowledge about such systems, ontologies help understand how they work and offer a basis to structure them, identify the necessary adaptations and implement mechanisms to make them happen in run-time. We have explored the use of ontologies from an ontology network to develop a social network about academic subjects that automatically adapts its interface according to the low vision and colorblind user's needs and usage characteristics. The first version of an ontology-based process to guide the development of AUI systems raised from this experience.},
booktitle = {Proceedings of the 21st Brazilian Symposium on Human Factors in Computing Systems},
articleno = {43},
numpages = {7},
keywords = {adaptive user interface, ontology, ontology network},
location = {Diamantina, Brazil},
series = {IHC '22}
}

@inproceedings{10.1145/3459637.3482387,
author = {Wisniewski, Dawid and Potoniec, Jedrzej and Lawrynowicz, Agnieszka},
title = {SeeQuery: An Automatic Method for Recommending Translations of Ontology Competency Questions into SPARQL-OWL},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482387},
doi = {10.1145/3459637.3482387},
abstract = {Ontology authoring is a complicated and error-prone process since the knowledge being modeled is expressed using logic-based formalisms, in which logical consequences of the knowledge have to be foreseen. To make that process easier, competency questions (CQs), being questions expressed in natural language are often stated to trace both the correctness and completeness of the ontology at a given time. However, CQs have to be translated into a formal language, like ontology query language (SPARQL-OWL), to query the ontology. Since the translation step is time-consuming and requires familiarity with the query language used, in this paper, we propose an automatic method named SeeQuery, which recommends SPARQL-OWL queries being translations of CQs stated against a given ontology. It consists of a pipeline of transformations based on template matching and filling, being motivated by the biggest to date publicly available CQ to SPARQL-OWL datasets. We provide a detailed description of SeeQuery and evaluate the method on a separate set of 2 ontologies with their CQs. It is, to date, the only automatic method available for recommending SPARQL-OWL queries out of CQs. The source code of SeeQuery is available at: https://github.com/dwisniewski/SeeQuery.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {2119–2128},
numpages = {10},
keywords = {automatic translation, competency questions, ontology authoring, semantic similarity, sparql-owl},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3460210.3493568,
author = {Alharbi, Reham and Tamma, Valentina and Grasso, Floriana},
title = {Characterising the Gap Between Theory and Practice of Ontology Reuse},
year = {2021},
isbn = {9781450384575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460210.3493568},
doi = {10.1145/3460210.3493568},
abstract = {Ontology reuse is a complex process that requires the support of methodologies and tools to minimise errors and to keep the ontologies consistent and coherent. Although the vast majority of ontology engineering methodologies include a reuse phase, and reuse has been investigated for different tasks and purposes (e.g.ontology integration), this body of work does not seem to translate into practice, neither in the form of strict criteria for reuse, nor as a set of community proposed guidelines. In this paper, we report the salient results from a study aimed at ontology developers and practitioners, whose objective is to gain an insight into the gap between the theory and the practice of ontology reuse. Thefocus of our study is to gain practitioners' views on i) their preferred reuse approaches; ii) the types of ontologies they tend to reuse (e.g. specific domain ontologies or upper-level ontologies)iii) what reporting information they deem useful when deciding which ontology to reuse; iv) what are the main reasons deterring them from reusing an ontology. Our findings confirm and extend established results from the literature, but in addition, the study provides a fresh view on the practice of reuse with an explicit focus on highly experienced developers and moderately experienced ones. The study corroborates the need for a comprehensive set of recommendations, that are widely accepted by the community, and are possibly implemented in development tools.},
booktitle = {Proceedings of the 11th Knowledge Capture Conference},
pages = {217–224},
numpages = {8},
keywords = {challenges to ontology reuse, ontology development methodologies, ontology engineering, ontology reuse},
location = {Virtual Event, USA},
series = {K-CAP '21}
}

@article{10.1145/3657299,
author = {Xia, Bolun (Namir) and Rawte, Vipula and Gupta, Aparna and Zaki, Mohammed},
title = {FETILDA: Evaluation Framework for Effective Representations of Long Financial Documents},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {7},
issn = {1556-4681},
url = {https://doi.org/10.1145/3657299},
doi = {10.1145/3657299},
abstract = {In the financial sphere, there is a wealth of accumulated unstructured financial data, such as the textual disclosure documents that companies submit on a regular basis to regulatory agencies, such as the Securities and Exchange Commission. These documents are typically very long and tend to contain valuable soft information about a company’s performance that is not present in quantitative predictors. It is therefore of great interest to learn predictive models from these long textual documents, especially for forecasting numerical key performance indicators. In recent years, there has been great progress in natural language processing via pre-trained language models (LMs) learned from large corpora of textual data. This prompts the important question of whether they can be used effectively to produce representations for long documents, as well as how we can evaluate the effectiveness of representations produced by various LMs. Our work focuses on answering this critical question, namely, the evaluation of the efficacy of various LMs in extracting useful soft information from long textual documents for prediction tasks. In this article, we propose and implement a deep learning evaluation framework that utilizes a sequential chunking approach combined with an attention mechanism. We perform an extensive set of experiments on a collection of 10-K reports submitted annually by U.S. banks, and another dataset of reports submitted by U.S. companies, to investigate thoroughly the performance of different types of language models. Overall, our framework using LMs outperforms strong baseline methods for textual modeling as well as for numerical regression. Our work provides better insights into how utilizing pre-trained domain-specific and fine-tuned long-input LMs for representing long documents can improve the quality of representation of textual data and, therefore, help in improving predictive analyses.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jun,
articleno = {182},
numpages = {27},
keywords = {Text regression, language models, long text documents, financial documents, 10-K reports}
}

@inproceedings{10.1145/3432291.3432306,
author = {Contreras, Jennifer O. and Ballera, Melvin A. and Festijo, Enrique D.},
title = {Ontology Learning using Hybrid Machine Learning Algorithms for Disaster Risk Management},
year = {2020},
isbn = {9781450375733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3432291.3432306},
doi = {10.1145/3432291.3432306},
abstract = {Disaster is inevitable but manageable thru careful planning, preparation and immediate response strategies. During typhoons, earthquakes and other calamities, agreement about language is vital to understand each other well to avoid high number of deaths, delay in access to basic needs and slow response time. However, some of the people involved in this domain find it hard to coordinate and respond to different emergency situations due to lack of familiarization and knowledge about the different terms or concepts. In disaster risk management, the consistency and reusability of the sharing of information is important to avoid possible risks. Due to this reason, an ontology is incorporated to aid in the disaster management process. The use of ontology enables quick retrieving and incorporating "consistent data" and information related to disaster management which plays an important for making decisions efficiently. This paper aims to implement and evaluate the accuracy of Support Vector Machine (SVM) and Neural Network (NN) learning-based ontology for disaster risk management to enhance the classification of concepts (keywords) generated for the domain ontology. The experiment shows that the hybrid SVM and NN machine learning algorithm outperformed the accuracy of SVM and NN based on the precision, recall and F-Measure criterion.},
booktitle = {Proceedings of the 2020 3rd International Conference on Signal Processing and Machine Learning},
pages = {13–20},
numpages = {8},
keywords = {Disaster Risk Management, Hybrid Algorithm, Machine Learning, Natural Language Processing, Neural Network, Ontology Learning, Support Vector Machine},
location = {Beijing, China},
series = {SPML '20}
}

@inproceedings{10.1145/3707292.3707356,
author = {Zhao, Pei and Zhang, Longxing and Zhao, Jiawen},
title = {Complete the exploration of low-resource knowledge graph completion based on large model technology},
year = {2025},
isbn = {9798400707308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707292.3707356},
doi = {10.1145/3707292.3707356},
abstract = {In the construction and application of knowledge graph, it is a realistic research problem to complete the knowledge in the low resource field. Traditional methods that rely on manual annotation and rules are not only costly, but also have limitations in coverage and scalability. To solve this problem, this paper proposes a large model technique, combining fine-tuning and knowledge transfer strategies. Firstly, to improve the ability of fine-tuning of the large model, the rich knowledge learned by the large model in the high resource field to assist the completion of the low resource knowledge graph through knowledge transfer technology to make up for the shortage of direct extraction. The experimental results show that this method can effectively improve the completion rate and accuracy of the knowledge graph, especially in the completion of entity relations and attribute filling. Furthermore, we explore the impact of different fine-tuning strategies and knowledge transfer methods on the completion effect, providing experimental empirical and theoretical support for future studies on similar issues.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Intelligent Information Processing},
pages = {140–145},
numpages = {6},
keywords = {Fine-tuning, Knowledge graph, Large model technology, Low resource, Transfer learning},
location = {
},
series = {AIIIP '24}
}

@article{10.1613/jair.1.13511,
author = {Artale, Alessandro and Kontchakov, Roman and Kovtunova, Alisa and Ryzhikov, Vladislav and Wolter, Frank and Zakharyaschev, Michael},
title = {First-Order Rewritability and Complexity of Two-Dimensional Temporal Ontology-Mediated Queries},
year = {2022},
issue_date = {Dec 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {75},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13511},
doi = {10.1613/jair.1.13511},
abstract = {Aiming at ontology-based data access to temporal data, we design two-dimensional temporal ontology and query languages by combining logics from the (extended) DL-Lite family with linear temporal logic LTL over discrete time (Z,&lt;). Our main concern is first-order rewritability of ontology-mediated queries (OMQs) that consist of a 2D ontology and a positive temporal instance query. Our target languages for FO-rewritings are two-sorted FO(&lt;) -- first-order logic with sorts for time instants ordered by the built-in precedence relation &lt; and for the domain of individuals---its extension FO(&lt;, ≡) with the standard congruence predicates t ≡ 0 (mod n), for any fixed n &gt; 1, and FO(RPR) that admits relational primitive recursion. In terms of circuit complexity, FO(&lt;, ≡)- and FO(RPR)-rewritability guarantee answering OMQs in uniform AC0 and NC1, respectively. We proceed in three steps. First, we define a hierarchy of 2D DL-Lite/LTL ontology languages and investigate the FO-rewritability of OMQs with atomic queries by constructing projections onto 1D LTL OMQs and employing recent results on the FO-rewritability of propositional LTL OMQs. As the projections involve deciding consistency of ontologies and data, we also consider the consistency problem for our languages. While the undecidability of consistency for 2D ontology languages with expressive Boolean role inclusions might be expected, we also show that, rather surprisingly, the restriction to Krom and Horn role inclusions leads to decidability (and ExpSpace-completeness), even if one admits full Booleans on concepts. As a final step, we lift some of the rewritability results for atomic OMQs to OMQs with expressive positive temporal instance queries. The lifting results are based on an in-depth study of the canonical models and only concern Horn ontologies.},
journal = {J. Artif. Int. Res.},
month = dec,
numpages = {69}
}

@inproceedings{10.1145/3652620.3686251,
author = {Zavada, \'{A}rmin and Marussy, Krist\'{o}f and Moln\'{a}r, Vince},
title = {From Transpilers to Semantic Libraries: Formal Verification With Pluggable Semantics},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3686251},
doi = {10.1145/3652620.3686251},
abstract = {In the field of model-based systems engineering, there is an increasing demand for the application of formal methods. However, this requires expertise in formal methods, which cannot be expected from systems engineers. While several attempts have been made to bridge this gap, there are still open questions. (1) With the trend shifting towards ontological languages, systems are modeled as classes of 4D occurrences, rather than a 3D system evolving with time, which hinders the application of state-of-the-art model checking algorithms. (2) Ontological reasoning cannot handle the state space explosion problem, and can even make it harder for verifiers to operate efficiently. (3) When operationalizing ontological languages, we need to validate the conformance of the two semantics, even in the presence of optimizations. (4) On top of all, these challenges must be solved for every new engineering language, version, or variant. In this paper, we propose a new approach to address the aforementioned challenges. To validate its feasibility, we present a prototype tool and evaluate it on a SysML model.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {311–317},
numpages = {7},
keywords = {model-based systems engineering, kernel modeling language, formal verification, declarative interpretation, metaprogramming, semantic libraries, operational libraries},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3708036.3708130,
author = {Cao, Wanghua and Xia, Huan and Xie, Rongdong and Hu, Jiangyu},
title = {Research on the Construction of a Knowledge Graph for Miao Medicine Based on the BERT-BiLSTM-CRF Model},
year = {2025},
isbn = {9798400709999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708036.3708130},
doi = {10.1145/3708036.3708130},
abstract = {This study focuses on constructing a knowledge graph for Miao medicine to promote the understanding of traditional Miao cultural practices, enhance public health capabilities, and preserve this intangible cultural heritage. The BERT-BiLSTM-CRF model, combined with the manual review, was utilized for entity recognition, while SBERT technology was used for entity alignment. As a result, a comprehensive knowledge graph of Miao medicine was constructed, including 11,111 nodes and 143,751 relationships. This knowledge graph systematically organizes the complex knowledge system of Miao medicine, including diseases, symptoms, drugs, and their interrelations. It supports efficient knowledge querying, reasoning, and discovery, contributing valuable insights into applying knowledge graph technologies in traditional medicine. Moreover, this research offers a framework that can be adapted for constructing knowledge graphs in other conventional medical systems.},
booktitle = {Proceedings of the 2024 5th International Conference on Computer Science and Management Technology},
pages = {555–561},
numpages = {7},
keywords = {BERT-BiLSTM-CRF model, Miao medicine, intangible cultural heritage, knowledge graph},
location = {
},
series = {ICCSMT '24}
}

@inproceedings{10.5555/3291291.3291311,
author = {Boyer, John M.},
title = {Natural language question answering in the financial domain},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {This paper describes a natural language question answering system focused on answering financial domain questions using a daily updated corpus of financial reports. Financial entity types of interest included company stocks, country bonds, currencies, industries, commodities, and diversified assets. Financial questions of interest included explanatory and factual questions about entities as well as financial outlook for entities.An important architectural divergence emerged between the approach required for answering financial outlook questions versus the approach for answering other financial information questions. The financial domain focus also introduced additional challenges to open domain natural language processing that were addressed in the areas of document ingestion, question classification accuracy, question analysis techniques, speed of machine learning, answer ranking by linguistic confidence versus temporality, and system accuracy assessment.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {189–200},
numpages = {12},
keywords = {financial domain, financial information retrieval, financial sentiment analysis, question analysis, question answering systems, question classification, text analytics},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3531073.3531081,
author = {Fu, Bo and Steichen, Ben},
title = {Impending Success or Failure? An Investigation of Gaze-Based User Predictions During Interaction with Ontology Visualizations},
year = {2022},
isbn = {9781450397193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531073.3531081},
doi = {10.1145/3531073.3531081},
abstract = {Designing and developing innovative visualizations to assist humans in the process of generating and understanding complex semantic data has become an important element in supporting effective human-ontology interaction, as visual cues are likely to provide clarity, promote insight, and amplify cognition. While recent research has indicated potential benefits of applying novel adaptive technologies, typical ontology visualization techniques have traditionally followed a one-size-fits-all approach that often ignores an individual user's preferences, abilities, and visual needs. In an effort to realize adaptive ontology visualization, this paper presents a potential solution to predict a user's likely success and failure in real time, and prior to task completion, by applying established machine learning models on eye gaze generated during an interactive session. These predictions are envisioned to inform future adaptive ontology visualizations that could potentially adjust its visual cues or recommend alternative visualizations in real time to improve individual user success. This paper presents findings from a series of experiments to demonstrate the feasibility of gaze-based success and failure predictions in real time that can be achieved with a number of off-the-shelf classifiers without the need of expert configurations in the presence of mixed user backgrounds and task domains across two commonly used fundamental ontology visualization techniques.},
booktitle = {Proceedings of the 2022 International Conference on Advanced Visual Interfaces},
articleno = {7},
numpages = {9},
keywords = {Eye Tracking, Ontology Visualization, Predictive Analytics},
location = {Frascati, Rome, Italy},
series = {AVI '22}
}

@inproceedings{10.1145/3464385.3464744,
author = {Calvanese, Diego and Ding, Linfang and Mosca, Alessandro and Xiao, Guohui},
title = {Realizing Ontology-based Reusable Interfaces for Data Access via Virtual Knowledge Graphs},
year = {2021},
isbn = {9781450389778},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464385.3464744},
doi = {10.1145/3464385.3464744},
abstract = {In this paper, we present a comprehensive framework, which we call VKG-UI, for realizing ontology-based reusable user interfaces (UIs) for data access via virtual knowledge graphs (VKGs). The VKG approach uses an ontology to model the domain of interest and to hide the heterogeneity of the underlying data sources. Reusable UIs can be built by relying on queries that are issued to the VKG system and that use the high level vocabulary from the ontology layer. This use of VKGs allows for decoupling the data from the UIs, and brings great reusability in designing the latter. To illustrate our approach, we introduce significant use cases with various types of UIs, including programming, graphic, natural language, and voice interfaces.},
booktitle = {Proceedings of the 14th Biannual Conference of the Italian SIGCHI Chapter},
articleno = {35},
numpages = {5},
keywords = {data access, ontology, user interface, virtual knowledge graph},
location = {Bolzano, Italy},
series = {CHItaly '21}
}

@inproceedings{10.1145/3487553.3524723,
author = {Yang, Sean T. and Howe, Bill},
title = {Surj: Ontological Learning for Fast, Accurate, and Robust Hierarchical Multi-label Classification},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524723},
doi = {10.1145/3487553.3524723},
abstract = {We consider multi-label classification in the context of complex hierarchical relationships organized into an ontology. These situations are ubiquitous in learning problems on the web and in science, where rich domain models are developed but labeled data is rare. Most existing solutions model the problem as a sequence of simpler problems: one classifier for each level in the hierarchy, or one classifier for each label. These approaches require more training data, which is often unavailable in practice: as the ontology grows in size and complexity, it becomes unlikely to find training examples for all expected combinations. In this paper, we learn offline representations of the ontology using a graph autoencoder and separately learn to classify input records, reducing dependence on training data: Since the relationships between labels are encoded independently of training data, the model can make predictions even for underrepresented labels, naturally generalize to DAG-structured ontologies, remain robust to low-data regimes, and, with minor offline retraining, tolerate evolving ontologies. We show empirically that our label predictions respect the hierarchy (predicting a descendant implies predicting its ancestors) and propose a method of evaluating hierarchy violations that properly ignores irrelevant violations. Our main result is that our model outperforms all state-of-the-art models on 17 of 20 datasets across multiple domains by a significant margin, even with limited training data.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1106–1114},
numpages = {9},
keywords = {Graph Learning, Hierarchical Multi-label classification, Ontology Learning},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3659211.3659257,
author = {Tang, Hailin and Feng, Jun and Zhou, Siyuan},
title = {Generic Ontologies for Digital Watersheds},
year = {2024},
isbn = {9798400716669},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659211.3659257},
doi = {10.1145/3659211.3659257},
abstract = {Digital Watershed represents an effective strategy for addressing floods and mitigating their associated risks. However, a notable challenge lies in the absence of a universally applicable modeling approach for constructing digital watersheds. The wealth of data and knowledge about watersheds is currently managed in a fragmented manner, impeding a comprehensive and cohesive understanding of the subject. This paper addresses the fragmented control landscape in watershed management by introducing generic ontologies, including water conservancy object ontology, model ontology, rainfall and runoff scene-mode ontology, and event ontology. These ontologies standardize the representation of water conservancy objects, hydrological models, and expert knowledge while also defining structured representations for physical events, scheduling rules, and business processes, which contribute to breaking the paradigm of "one watershed, one system" and facilitate integrated flood prediction and scheduling.},
booktitle = {Proceedings of the 2023 4th International Conference on Big Data Economy and Information Management},
pages = {260–266},
numpages = {7},
location = {Zhengzhou, China},
series = {BDEIM '23}
}

@proceedings{10.1145/3627043,
title = {UMAP '24: Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
year = {2024},
isbn = {9798400704338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3627673.3679662,
author = {Balsebre, Pasquale and Huang, Weiming and Cong, Gao and Li, Yi},
title = {City Foundation Models for Learning General Purpose Representations from OpenStreetMap},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679662},
doi = {10.1145/3627673.3679662},
abstract = {Pre-trained Foundation Models (PFMs) have ushered in a paradigm-shift in AI, due to their ability to learn general-purpose representations that can be readily employed in downstream tasks. While PFMs have been successfully adopted in various fields such as NLP and Computer Vision, their capacity in handling geospatial data remains limited. This can be attributed to the intrinsic heterogeneity of such data, which encompasses different types, including points, segments and regions, as well as multiple information modalities. The proliferation of Volunteered Geographic Information initiatives, like OpenStreetMap, unveils a promising opportunity to bridge this gap. In this paper, we present CityFM, a self-supervised framework to train a foundation model within a selected geographical area. CityFM relies solely on open data from OSM, and produces multimodal representations, incorporating spatial, visual, and textual information. We analyse the entity representations generated by our foundation models from a qualitative perspective, and conduct experiments on road, building, and region-level downstream tasks. In all the experiments, CityFM achieves performance superior to, or on par with, application-specific algorithms.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {87–97},
numpages = {11},
keywords = {contrastive learning, foundation models, geospatial data},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3460120.3485353,
author = {Christian, Ryan and Dutta, Sharmishtha and Park, Youngja and Rastogi, Nidhi},
title = {An Ontology-driven Knowledge Graph for Android Malware},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3485353},
doi = {10.1145/3460120.3485353},
abstract = {We present MalONT2.0 -- an ontology for malware threat intelligence [4]. New classes (attack patterns, infrastructural resources to enable attacks, malware analysis to incorporate static analysis, and dynamic analysis of binaries) and relations have been added following a broadened scope of core competency questions. MalONT2.0 allows researchers to extensively capture all requisite classes and relations that gather semantic and syntactic characteristics of an android malware attack. This ontology forms the basis for the malware threat intelligence knowledge graph, MalKG, which we exemplify using three different, non-overlapping demonstrations. Malware features have been extracted from openCTI reports on android threat intelligence shared on the Internet and written in the form of unstructured text. Some of these sources are blogs, threat intelligence reports, tweets, and news articles. The smallest unit of information that captures malware features is written as triples comprising head and tail entities, each connected with a relation. In the poster and demonstration, we discuss MalONT2.0 and MalKG.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2435–2437},
numpages = {3},
keywords = {inference, knowledge graphs, malware, ontology},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/3724154.3724251,
author = {Yu, Jie and Yao, Minghui},
title = {Multidimensional model for stock prediction using investors’ commentary},
year = {2025},
isbn = {9798400711862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724154.3724251},
doi = {10.1145/3724154.3724251},
abstract = {Forecasting share prices has always been a major concern in the financial sector, as several factors and investor's comments influence share price movements have a tangible impact on the stock market. The research is the quantification of investor comments as indicators of emotional tendencies and constructs multidimensional feature sets to predict stock prices based on stock fundamentals, technical aspects, and information aspects. To enhance the correlation between the emotional tendencies of stock investors and other characteristics, meanwhile, to extract implicit information from these characteristics, we have created a sliding window structure with Pearson correlation, and channel convolution attention modules(CCAM). This paper develops a Multidimensional Fusion Model (MFM) consisting of four components: the text-only model, the one-dimensional price model I, the multidimensional price model II, and the data fusion module. Multi-source heterogeneous data is used to construct the dataset, which contains numerical data on stock fundamentals and technical aspects, and text data on information aspects, which collects investor comments during a moving window period. Expert knowledge is used to mark the emotional polarity of the text in the batch. The constructed text-only model aims to reveal the implicit relationship between the comment texts. To quantify the emotional tendency of massive stock investors on a given day, the Bert/ Enhanced Representation through Knowledge Integration (ERNIE) of word embedding method and the Convolutional Neural Networks (CNN)/ Deep Pyramid-CNN (DPCNN)/ Region-CNN (RCNN) classifier are used models. Model I (CEEMDAN-WOA-BiLSTM) analyses unidimensional stock closing prices and interprets stock price fluctuations as digital signals, whereas Model II (CCAM-Attention-BiLSTM based on Pearson correlation data integration) utilizes multidimensional data encompassing emotional tendencies. On this basis, a constrained Data Fusion Module (DFM) is proposed to process the prediction results of Model I and Model II. For the Stock Index dataset 000881.SZ, covering the period 2017-2023. The BERT-DPCNN-MFM model is one of the most efficient MFMs produced, it obtained 0.0947, 0.0699, and 0.0090 results in RMSE, MAE, and MSE, respectively. Simulation results verify the effectiveness of the model, while proving that the model can reflect the short-term fluctuation of stock prices.},
booktitle = {Proceedings of the 2024 5th International Conference on Big Data Economy and Information Management},
pages = {582–593},
numpages = {12},
keywords = {CCAM-Attention-BiLSTM, Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN), Pearson correlation, Stock predictors, emotional tendencies},
location = {
},
series = {BDEIM '24}
}

@inproceedings{10.1145/3587259.3627559,
author = {Martorana, Margherita and Kuhn, Tobias and Siebes, Ronald and Van Ossenbruggen, Jacco},
title = {Advancing data sharing and reusability for restricted access data on the Web: introducing the DataSet-Variable Ontology},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627559},
doi = {10.1145/3587259.3627559},
abstract = {In response to the increasing volume of research data being generated, more and more data portals have been designed to facilitate data findability and accessibility. However, a significant portion of this data remains confidential or restricted due to its sensitive nature, such as patient data or census microdata. While maintaining confidentiality prohibits its public release, the emergence of portals supporting rich metadata can help enable researchers to at least discover the existence of restricted access data, empowering them to assess the suitability of the data before requesting access. Existing standards, such as CSV on the Web and RDF Data Cube, have been adopted to facilitate data management, integration, and re-use of data on the Web. However, the current landscape still lacks adequate standards not only to effectively describe restricted access data while preserving confidentiality but also to facilitate its discovery. In this work, we investigate the relationship between the structural, statistical, and semantic elements of restricted access tabular data, and we explore how such relationship can be formally modeled in a way that is Findable, Accessible, Interoperable, and Reusable. We introduce the DataSet-Variable Ontology (DSV), that by combining CSV on the Web and RDF Data Cube standards, leveraging semantic technologies and Linked Data principles, and introducing variable-level metadata, aims to capture high-quality metadata to support the management and re-use of restricted access data on the Web. As evaluation, we conducted a case study where we applied DSV to four different datasets from different statistical governmental agencies. We employed a set of competency questions to assess the ontology’s ability to support knowledge discovery and data exploration. By describing high-quality metadata, both at the dataset- and variable levels, while maintaining data privacy, this novel ontology facilitates data interoperability, discovery, and re-use and it empowers researchers to manage, integrate, and analyze complex restricted access data sources.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {83–91},
numpages = {9},
keywords = {FAIR principles, Privacy-preserving web data, Restricted access data, Semantic Web, Variable-level metadata},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@article{10.1145/3625299,
author = {Hou, Wenjun and Bai, Bing and Cai, Chenyang},
title = {CR-TransR: A Knowledge Graph Embedding Model for Cultural Domain},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3625299},
doi = {10.1145/3625299},
abstract = {As a combination of information computing technology and the cultural field, cultural computing is gaining more attention. The knowledge graph is also gradually applied as a particular data structure in the cultural area. Based on the domain knowledge graph data of the Beijing Municipal Social Science Project “Mining and Utilization of Cultural Resources in the Ancient Capital of Beijing,” this article proposes a graph representation learning model CR-TransR that integrates cultural attributes. Through the analysis of the data in the cultural field of the ancient capital of Beijing, a cultural feature dictionary is constructed, and a domain-specific feature matrix is constructed in the form of word vector splicing. The feature matrix is used to constrain the embedding graph model TransR, and then the feature matrix and the TransR model are jointly trained to complete the embedded expression of the knowledge graph. Finally, a comparative experiment is carried out on the Beijing ancient capital cultural knowledge graph dataset and the effects of the classic graph embedding algorithms TransE, TransH, and TransR. At the same time, we try to reproduce the embedding method with the core idea of neighbor node information aggregation as the core idea, and CRTransR are compared. The experimental tasks include link prediction and triplet classification, and the experimental results show that the CRTransR model performs better.},
journal = {J. Comput. Cult. Herit.},
month = feb,
articleno = {12},
numpages = {11},
keywords = {Beijing ancient capital culture, knowledge graph, cultural calculation, graph embedding}
}

@proceedings{10.1145/3745533,
title = {CAMMIC '25: Proceedings of the 2025 5th International Conference on Applied Mathematics, Modelling and Intelligent Computing},
year = {2025},
isbn = {9798400713873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3423334.3431452,
author = {Laddada, Wissame and Duchateau, Fabien and Favetta, Franck and Moncla, Ludovic},
title = {Ontology-Based Approach for Neighborhood and Real Estate Recommendations},
year = {2020},
isbn = {9781450381604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423334.3431452},
doi = {10.1145/3423334.3431452},
abstract = {Suggesting services or products to people is a task that should be handled by recommendation systems due to the important increase of information and the multitude of user criteria. In fact, when expressing wishes for a product, a user is influenced by his/her tastes or priorities. These influential characteristics tend to be challenging regarding their integration into recommendation systems, because interaction between the products/services and the user has to be captured through its preferences. Recommendation systems for neighborhood and real estate search are no exception, and to achieve reliable recommendation, we developed an ontology NAREO (Neighborhood And Real Estate Ontology) where environment characteristics related to user preferences are modeled with other geo-semantic descriptions. This ontology can be enriched by SWRL (Semantic Web Rule Language) rules that enhance the semantics of our knowledge base and allow reasoning process through built-ins. To illustrate a use case, we provide a basic set of predefined rules for the recommendation context. User preferences are managed through SPARQL queries taking into account the result of inferences.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL Workshop on Location-Based Recommendations, Geosocial Networks, and Geoadvertising},
articleno = {4},
numpages = {10},
keywords = {Ontology, Recommendation Systems, SWRL reasoning, Spatial modeling},
location = {Seattle, WA, USA},
series = {LocalRec'20}
}

@inproceedings{10.1145/3538969.3544453,
author = {Charalambous, Markos and Farao, Aristeidis and Kalantzantonakis, George and Kanakakis, Panagiotis and Salamanos, Nikos and Kotsifakos, Evangelos and Froudakis, Evangellos},
title = {Analyzing Coverages of Cyber Insurance Policies Using Ontology},
year = {2022},
isbn = {9781450396707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538969.3544453},
doi = {10.1145/3538969.3544453},
abstract = {In an era where all the transactions, businesses and services are becoming digital and online, the data assets and the services protection are of utmost importance. Cyber-insurance companies are offering a wide range of coverages, but they also have exclusions. Customers of these companies need to be able to understand the terms and conditions of the related contracts and furthermore they need to be able to compare various offerings in order to determine the most appropriate solutions for their needs. The research in the area is very limited while at the same time the related market is growing, giving every potential solution a high value. In this paper, we propose a methodology and a prototype system that will help customers to compare contracts based on a pre-defined ontology that is describing cyber-insurance terms. After a first preliminary analysis and validation, our approach accuracy is averaging at almost 50\%, giving a promising initial evaluation. Fine tuning, larger data set assessment and ontology refinement will be our next steps to improve the accuracy of our tool. Real user evaluation will follow, in order to evaluate the tool in real world cases.},
booktitle = {Proceedings of the 17th International Conference on Availability, Reliability and Security},
articleno = {108},
numpages = {7},
keywords = {Coverages, Cyber-insurance, Exclusions, Ontology, Premium, Weakest link},
location = {Vienna, Austria},
series = {ARES '22}
}

@proceedings{10.1145/3628034,
title = {EuroPLoP '23: Proceedings of the 28th European Conference on Pattern Languages of Programs},
year = {2023},
isbn = {9798400700408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irsee, Germany}
}

@inproceedings{10.1145/3656766.3656929,
author = {Xu, Lianzheng and Fu, Deqian and Qiu, Jianlong},
title = {Trusted Non-intrusive Data Exchange based on Ontology in Logistics Industry},
year = {2024},
isbn = {9798400716478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656766.3656929},
doi = {10.1145/3656766.3656929},
abstract = {The logistics industry is becoming increasingly important in our daily lives, leading to a growing demand for digitalization within the sector. However, due to concerns about data privacy, logistics entities have formed natural information silos, which have made logistics data difficult to exchange and share. To address this issue, this paper proposes an ontology-based logistics data exchange model that utilizes blockchain technology to establish user trust among information silos. The model in this paper fuses non-intrusive data access and Ontology-Based Data Access (OBDA) to achieve both precise control over the data access process and interconnection among heterogeneous systems. The proposed model creates a new trustworthy and controllable solution for the circulation of logistics data within the logistics industry, without the need for the secondary development of logistics information systems.},
booktitle = {Proceedings of the 2023 3rd International Conference on Big Data, Artificial Intelligence and Risk Management},
pages = {986–991},
numpages = {6},
location = {Chengdu, China},
series = {ICBAR '23}
}

@article{10.1145/3626961,
author = {Roy, Chiradeep and Nourani, Mahsan and Arya, Shivvrat and Shanbhag, Mahesh and Rahman, Tahrima and Ragan, Eric D. and Ruozzi, Nicholas and Gogate, Vibhav},
title = {Explainable Activity Recognition in Videos using Deep Learning and Tractable Probabilistic Models},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {2160-6455},
url = {https://doi.org/10.1145/3626961},
doi = {10.1145/3626961},
abstract = {We consider the following video activity recognition (VAR) task: given a video, infer the set of activities being performed in the video and assign each frame to an activity. Although VAR can be solved accurately using existing deep learning techniques, deep networks are neither interpretable nor explainable and as a result their use is problematic in high stakes decision-making applications (in healthcare, experimental Biology, aviation, law, etc.). In such applications, failure may lead to disastrous consequences and therefore it is necessary that the user is able to either understand the inner workings of the model or probe it to understand its reasoning patterns for a given decision. We address these limitations of deep networks by proposing a new approach that feeds the output of a deep model into a tractable, interpretable probabilistic model called a dynamic conditional cutset network that is defined over the explanatory and output variables and then performing joint inference over the combined model. The two key benefits of using cutset networks are: (a) they explicitly model the relationship between the output and explanatory variables and as a result, the combined model is likely to be more accurate than the vanilla deep model and (b) they can answer reasoning queries in polynomial time and as a result, they can derive meaningful explanations by efficiently answering explanation queries. We demonstrate the efficacy of our approach on two datasets, Textually Annotated Cooking Scenes (TACoS), and wet lab, using conventional evaluation measures such as the Jaccard Index and Hamming Loss, as well as a human-subjects study.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = dec,
articleno = {29},
numpages = {32},
keywords = {Temporal models, dynamic Bayesian networks, cutset networks, tractable probabilistic models}
}

@article{10.1109/TCBB.2024.3377928,
author = {Palacio, Ana Le\'{o}n and S., Alberto Garc\'{\i}a and Rom\'{a}n, Jos\'{e} Fabi\'{a}n Reyes and Costa, Mireia and Pastor, Oscar},
title = {The Delfos Platform: A Conceptual Model-Based Solution for the Enhancement of Precision Medicine},
year = {2024},
issue_date = {Sept.-Oct. 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2024.3377928},
doi = {10.1109/TCBB.2024.3377928},
abstract = {The use in the clinical practice of the vast amount of genomic data generated by current sequencing technologies constitutes a bottleneck for the progress of Precision Medicine (PM). Various problems inherent to the genomics domain (i.e., dispersion, heterogeneity, discrepancies, lack of standardization, and data quality issues) remain unsolved. In this paper, we present the Delfos platform, a conceptual model-based solution developed following a rigorous methodological and ontological background, whose main aim is to minimize the impact of these problems when transferring the research results to clinical practice. This paper presents the SILE method that provides methodological support for the Delfos platform, the Conceptual Schema of the Genome that provides a shared understanding of the domain, and the technological architecture behind the implementation of the platform. This paper also exemplifies the use of the Delfos platform through two use cases that involve the study of the DNA variants associated with the risk of developing Dilated Cardiomyopathies and Neuroblastoma.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = mar,
pages = {1242–1253},
numpages = {12}
}

@inproceedings{10.5555/3539845.3539968,
author = {Herrmann, Martin and Witt, Christian and Lake, Laureen and Guneshka, Stefani and Heinzemann, Christian and Bonarens, Frank and Feifel, Patrick and Funke, Simon},
title = {Using ontologies for dataset engineering in automotive AI applications},
year = {2022},
isbn = {9783981926361},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {Basis of a robust safety strategy for an automated driving function based on neural networks is a detailed description of its input domain, i.e. a description of the environment, in which the function is used. This is required to describe its functional system boundaries and to perform a comprehensive safety analysis. Moreover, it allows to tailor datasets specifically designed for safety related validation tests. Ontologies fulfill the task to gather expert knowledge and model information to enable computer aided processing, while using a notion understandable for humans. In this contribution, we propose a methodology for domain analysis to build up an ontology for perception of autonomous vehicles including characteristic features that become important when dealing with neural networks. Additionally, the method is demonstrated by the creation of a synthetic test dataset for an Euro NCAP-like use case.},
booktitle = {Proceedings of the 2022 Conference \&amp; Exhibition on Design, Automation \&amp; Test in Europe},
pages = {526–531},
numpages = {6},
keywords = {artificial intelligence, autonomous driving, dataset engineering, neural network, ontology},
location = {Antwerp, Belgium},
series = {DATE '22}
}

@inproceedings{10.1145/3318464.3386139,
author = {Quamar, Abdul and Lei, Chuan and Miller, Dorian and Ozcan, Fatma and Kreulen, Jeffrey and Moore, Robert J. and Efthymiou, Vasilis},
title = {An Ontology-Based Conversation System for Knowledge Bases},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3386139},
doi = {10.1145/3318464.3386139},
abstract = {Domain-specific knowledge bases (KB), carefully curated from various data sources, provide an invaluable reference for professionals. Conversation systems make these KBs easily accessible to professionals and are gaining popularity due to recent advances in natural language understanding and AI. Despite the increasing use of various conversation systems in open-domain applications, the requirements of a domain-specific conversation system are quite different and challenging. In this paper, we propose an ontology-based conversation system for domain-specific KBs. In particular, we exploit the domain knowledge inherent in the domain ontology to identify user intents, and the corresponding entities to bootstrap the conversation space. We incorporate the feedback from domain experts to further refine these patterns, and use them to generate training samples for the conversation model, lifting the heavy burden from the conversation designers. We have incorporated our innovations into a conversation agent focused on healthcare as a feature of the IBM Micromedex product.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {361–376},
numpages = {16},
keywords = {conversation systems, knowledge bases, ontology-driven},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3570748.3570760,
author = {Kuchii, Kanta and Kondo, Takao and Teraoka, Fumio},
title = {KANVAS: A Network Information Sharing Framework Based on Network Ontology Bonsai},
year = {2022},
isbn = {9781450399814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570748.3570760},
doi = {10.1145/3570748.3570760},
abstract = {Demands for acquiring Internet behavior are increasing for Internet-scale network understanding such as inter-AS path management and traffic engineering. Although there are several efforts to make Internet behavior public, most of the public information is not structured and it is hard for applications to use such information. This paper proposes a network information sharing framework called KANVAS. It defines a network ontology called Bonsai which models network structure from viewpoints of physical, logical, service, and operation network structures. Bonsai can express network virtualization technologies such as link aggregation (LAG), VLAN, L2 over L3 tunneling, and virtual routing and forwarding (VRF). Applications can access network information via useful API. As a first step of development of KANVAS and Bonsai, this paper describes network information sharing within a single domain focusing on failure localization and throughput monitoring as examples. Evaluation results on a PoC system show that the time for failure localization is short enough and a throughput monitoring tool can choose appropriate monitoring points.},
booktitle = {Proceedings of the 17th Asian Internet Engineering Conference},
pages = {79–87},
numpages = {9},
keywords = {fault localization, network management, network ontology, traffic monitoring},
location = {Hiroshima, Japan},
series = {AINTEC '22}
}

@proceedings{10.1145/3649158,
title = {SACMAT 2024: Proceedings of the 29th ACM Symposium on Access Control Models and Technologies},
year = {2024},
isbn = {9798400704918},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 29th ACM Symposium on Access Control Models and Technologies (SACMAT 2024). This year's symposium continues its tradition of being the premier venue for presenting research results and experience reports on cutting edge advances on access control, including models, systems, applications, and theory, while also embracing an expanded focus on the general area of computer and information security and privacy. The overarching goal of the symposium is to share novel access control and computer security solutions that fulfill the needs of emerging applications and environments, and also to identify new directions for future research and development. ACM SACMAT provides researchers and also practitioners with a unique opportunity to share their perspectives with others interested in the various aspects of access control and computer security.},
location = {San Antonio, TX, USA}
}

@inproceedings{10.1145/3377170.3377279,
author = {Wilson, R. S. I. and Ginige, Athula and Goonetillake, J. S. and Indika, W. A.},
title = {User Needs-driven Enrichment of Ontology: A case study in Sri Lankan Agriculture},
year = {2020},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377170.3377279},
doi = {10.1145/3377170.3377279},
abstract = {This study describes the mobile-based user needs-driven knowledge management system that supports the decision making process by considering user needs and preferences. Agriculture is one of the domains, in which, users seek specific information and knowledge relevant to their needs rather than searching and accessing general information from the Web, books, magazines or other information sources. Thus, the conceptualized solution was created by applying participatory sensing, natural language processing and ontology theories and techniques in a novel way in order to satisfy the user needs. The user-centered agriculture ontology that has been developed in our previous work is extended to make an up-to-date knowledge base by capturing user needs and preferences through participatory sensing. The methods of ontology evolution from unstructured data were analyzed to build a technique to enrich the user-centered ontology. The Modified Delphi method is used for verifying the correctness and relevancy of the ontology and the application-based evaluation is applied for checking the functional correctness of the system.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {581–586},
numpages = {6},
keywords = {Agriculture, Knowledge base, Ontology enrichment, Participatory sensing, Question Answering, User needs in context},
location = {Shanghai, China},
series = {ICIT '19}
}

@inproceedings{10.1145/3724979.3725047,
author = {Zhang, Ying and Liu, Yuan and Pan, Xiaoyong and Shen, Hongbin},
title = {Ocean archaea PPI prediction with pretraining models},
year = {2025},
isbn = {9798400712203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724979.3725047},
doi = {10.1145/3724979.3725047},
abstract = {Protein-Protein Interaction (PPI) provides important insights into the metabolic mechanisms of different biological processes. Although PPIs in some organisms have been investigated systematically, PPIs in the ocean archaea remain largely unexplored. But such species have special investigation value since their adaptation to extreme living conditions may generate unique PPIs. In this paper, we aim to characterize and predict PPIs in ocean archaea to advance understanding of their metabolic networks. First, we collect all ocean archaea PPIs with high confidence from STRING database and analyze the PPI network features, including centrality and enrichment analysis. The functional enrichment results of the largest connecting subgraph in the PPI network show most PPIs in our constructed dataset is related to the translation and transcription processes. Then, we generate an equal number of negative PPI pairs, whose members have either different subcellular locations or GO terms. We also use the generated dataset to test the performance of three pretraining methods and their ensemble methods in the binary PPI prediction task. Our results suggest the ensemble methods could be applied to further improve models’ performance.  Fine-tuned models trained on the ocean archaea dataset are expected to predict the other ocean archaea PPIs that are not included in the STRING database and get more understanding about the ocean archaea PPI universe.},
booktitle = {Proceedings of the 2025 5th International Conference on Bioinformatics and Intelligent Computing},
pages = {440–445},
numpages = {6},
keywords = {Binary PPI prediction, Ocean archaea, PPI network, Pretraining model},
location = {
},
series = {BIC '25}
}

@inproceedings{10.1145/3589334.3645584,
author = {Shi, Jingchuan and Dong, Hang and Chen, Jiaoyan and Wu, Zhe and Horrocks, Ian},
title = {Taxonomy Completion via Implicit Concept Insertion},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645584},
doi = {10.1145/3589334.3645584},
abstract = {beginabstract High quality taxonomies play a critical role in various domains such as e-commerce, web search and ontology engineering. While there has been extensive work on expanding taxonomies from externally mined data, there has been less attention paid to enriching taxonomies by exploiting existing concepts and structure within the taxonomy. In this work, we show the usefulness of this kind of enrichment, and explore its viability with a new taxonomy completion system ICON (I mplicit CON cept Insertion). ICON generates new concepts by identifying implicit concepts based on the existing concept structure, generating names for such concepts and inserting them in appropriate positions within the taxonomy. ICON integrates techniques from entity retrieval, text summary, and subsumption prediction; this modular architecture offers high flexibility while achieving state-of-the-art performance. We have evaluated ICON on two e-commerce taxonomies, and the results show that it offers significant advantages over strong baselines including recent taxonomy completion models and the large language model, ChatGPT.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {2159–2169},
numpages = {11},
keywords = {ontology engineering, pre-trained language model, taxonomy completion, taxonomy enrichment, text summarisation},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3278293.3278302,
author = {Zhao, Grace and Zhang, Xiaowen},
title = {Domain-Specific Ontology Concept Extraction and Hierarchy Extension},
year = {2018},
isbn = {9781450365512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278293.3278302},
doi = {10.1145/3278293.3278302},
abstract = {The domain-specific vernaculars and notations have been a hurdle to automatic ontology building and augmentation, since most of the ontology learning methods are essentially based on the natural language studies and lexicosyntactic pattern explorations. This paper proposes two robust approaches to ontology hierarchical enhancement, in particular, adding new terms to the ontology graph. We designed our learning models from a computational vantage point, examining the inter-relationship between documents, ontology dictionary terms, and the graph structure of the seed ontology. We then take advantage of late studies of neural networks and machine learning to perform classification over the inter-related data, and insert the new term at the most desirable nodal place on the domain ontology graph.},
booktitle = {Proceedings of the 2nd International Conference on Natural Language Processing and Information Retrieval},
pages = {60–64},
numpages = {5},
keywords = {Domain Knowledge Learning, Ontology Engineering, Ontology Hierarchy Extension, Supervised Machine Learning},
location = {Bangkok, Thailand},
series = {NLPIR '18}
}

@inproceedings{10.1145/3685651.3686700,
author = {Haverinen, Henry and Janhunen, Tomi and P\"{a}iv\"{a}rinta, Tero and Lempinen, Sami and Kaartinen, Suvi and Meril\"{a}, Sami},
title = {Automating Cybersecurity Compliance in DevSecOps with Open Information Model for Security as Code},
year = {2024},
isbn = {9798400709845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3685651.3686700},
doi = {10.1145/3685651.3686700},
abstract = {Software development teams meet increasing requirements to implement cybersecurity management in compliance with standards and regulations. However, adopting a compliant cybersecurity management system and DevSecOps practices as part of a software development process has turned out to be tedious and expensive in practice. Open-source communities and open ecosystems, which lack tools and realistic practices for compliant cybersecurity management, face these difficulties as well. This paper suggests a set of requirements and a solution that are based on long-term experience in adopting standard compliant DevSecOps processes in industry. The proposed solution, called Cyberismo, facilitates the adoption of compliance and cybersecurity management, improves collaboration on cybersecurity in company internal projects, cross-company projects, and open-source projects, and automates the compliance and cybersecurity management in software development by way of an open information model representation format, and an open-source tool to manage the information model. As the information model uses a simple plain text format that can be managed by automated DevSecOps tool chains, it can be understood as an instance of the Everything as Code and Security as Code paradigms. The proposed solution is designed as modular, tailorable to the organisation and its existing tools, and flexible enough to model both process- and technology-related information. It automates both the validation of how compliance requirements have been met and the gathering and archiving of evidence of compliance. The information model is mapped to a logic program conforming to the Answer Set Programming (ASP) paradigm for knowledge representation. The mapping enables flexible query evaluation and reasoning, including the calculation of performance measures and automated policy checks. However, developers, product owners and other end-users of the solution do not necessarily need to know how to write logic programs, as logic programs can be encapsulated in content modules made available for the users. By putting the ease of adoption of compliant DevSecOps processes by the practitioners in the spotlight, this paper concludes that it is both necessary and possible to meet all the proposed requirements.},
booktitle = {Proceedings of the 4th Eclipse Security, AI, Architecture and Modelling Conference on Data Space},
pages = {93–102},
numpages = {10},
keywords = {Cybersecurity management, DevOps, DevSecOps, Everything as Code, Security as Code, answer set programming, compliance, knowledge representation and reasoning, open source security, process adoption},
location = {Mainz, Germany},
series = {eSAAM '24}
}

@inproceedings{10.1145/3564719.3568689,
author = {Butting, Arvid and Kirchhof, J\"{o}rg Christian and Kleiss, Anno and Michael, Judith and Orlov, Radoslav and Rumpe, Bernhard},
title = {Model-Driven IoT App Stores: Deploying Customizable Software Products to Heterogeneous Devices},
year = {2022},
isbn = {9781450399203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564719.3568689},
doi = {10.1145/3564719.3568689},
abstract = {Internet of Things (IoT) devices and the software they execute are often strongly coupled with vendors preinstalling their software at the factory. Future IoT applications are expected to be distributed via app stores. A strong coupling between hard- and software hinders the rise of such app stores. Existing model-driven approaches for developing IoT applications focus largely on the behavior specification and message exchange but generate code that targets a specific set of devices. By raising the level of abstraction, models can be utilized to decouple hard- and software and adapt to various infrastructures. We present a concept for a model-driven app store that decouples hardware and software development of product lines of IoT applications.},
booktitle = {Proceedings of the 21st ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {108–121},
numpages = {14},
keywords = {App Store, Architecture Description Language, Internet of Things, Low-Code, Model-Driven Engineering},
location = {Auckland, New Zealand},
series = {GPCE 2022}
}

@article{10.1109/TCBB.2024.3429234,
author = {Mundotiya, Rajesh Kumar and Priya, Juhi and Kuwarbi, Divya and Singh, Teekam},
title = {Enhancing Generalizability in Biomedical Entity Recognition: Self-Attention PCA-CLS Model},
year = {2024},
issue_date = {Nov.-Dec. 2024},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {21},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2024.3429234},
doi = {10.1109/TCBB.2024.3429234},
abstract = {One of the primary tasks in the early stages of data mining involves the identification of entities from biomedical corpora. Traditional approaches relying on robust feature engineering face challenges when learning from available (un-)annotated data using data-driven models like deep learning-based architectures. Despite leveraging large corpora and advanced deep learning models, domain generalization remains an issue. Attention mechanisms are effective in capturing longer sentence dependencies and extracting semantic and syntactic information from limited annotated datasets. To address out-of-vocabulary challenges in biomedical text, the PCA-CLS (Position and Contextual Attention with CNN-LSTM-Softmax) model combines global self-attention and character-level convolutional neural network techniques. The model's performance is evaluated on eight distinct biomedical domain datasets encompassing entities such as genes, drugs, diseases, and species. The PCA-CLS model outperforms several state-of-the-art models, achieving notable F&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$_{1}$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;mml:math&gt;&lt;mml:msub&gt;&lt;mml:mrow/&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;&lt;inline-graphic xlink:href="mundotiya-ieq1-3429234.gif"/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt;-scores, including 88.19% on BC2GM, 85.44% on JNLPBA, 90.80% on BC5CDR-chemical, 87.07% on BC5CDR-disease, 89.18% on BC4CHEMD, 88.81% on NCBI, and 91.59% on the s800 dataset.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jul,
pages = {1934–1941},
numpages = {8}
}

@inproceedings{10.1109/ASE56229.2023.00019,
author = {Phokela, Kanchanjot Kaur and Sikand, Samarth and Singi, Kapil and Dey, Kuntal and Sharma, Vibhu Saujanya and Kaulgud, Vikrant},
title = {Smart Prompt Advisor: Multi-Objective Prompt Framework for Consistency and Best Practices},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00019},
doi = {10.1109/ASE56229.2023.00019},
abstract = {Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions, and basic control and data flow are met.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1846–1848},
numpages = {3},
keywords = {prompt engineering, artificial intelligence, deep learning, LLM, ontology},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inproceedings{10.1145/3575813.3595190,
author = {Mavrokapnidis, Dimitris and Fierro, Gabe and Korolija, Ivan and Rovas, Dimitrios},
title = {A Programming Model for Portable Fault Detection and Diagnosis},
year = {2023},
isbn = {9798400700323},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575813.3595190},
doi = {10.1145/3575813.3595190},
abstract = {Portable applications support the write once, deploy everywhere paradigm. This paradigm is particularly attractive in building applications, where current practice involves the manual deployment and configuration of such applications, requiring significant engineering effort and concomitant costs. This is a tedious and error-prone process which does not scale well. Notwithstanding recent advances in semantic data modelling that allow a unified representation of buildings, we still miss a paradigm for deploying portable building applications at scale. This paper introduces a portable programming model for such applications, which we examine in the context of Fault-Detection and Diagnosis (FDD). In particular, we look at the separation of the FDD logic and the configuration with specific data inputs. We architect a software system that enables their self-configuration and execution across various building configurations, expressed in terms of Brick metadata models. Our initial results from authoring and executing APAR (AHU Performance Assessment Rules) on multiple AHUs of two museums demonstrate the potential of our model to reduce repetitive tasks and deployment costs of FDD applications.},
booktitle = {Proceedings of the 14th ACM International Conference on Future Energy Systems},
pages = {127–131},
numpages = {5},
keywords = {Brick, FDD, Metadata, Ontologies, Portability, Programming, RDF, SHACL, Scalability, Semantic Web},
location = {Orlando, FL, USA},
series = {e-Energy '23}
}

@inproceedings{10.1145/3587716.3587723,
author = {Huang, Xinyi and Cheng, Lianglun and Deng, Jianfeng and Wang, Tao},
title = {Binocular attention-based stacked BiLSTM NER model for Supply chain management event knowledge graph construction},
year = {2023},
isbn = {9781450398411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587716.3587723},
doi = {10.1145/3587716.3587723},
abstract = {Extracting fine-grained event ontology knowledge based on supply chain management (SCM) related corpus and constructing knowledge graph (KG) has important guiding significance and knowledge support for the efficient implementation and development of SCM in manufacturing enterprises. Recently, research on the KG of SCM has not gained sufficient attention. This paper aims to propose an event logical KG construction approach for SCM. Specifically, a stacked BiLSTM entity recognition model based on the binocular attention mechanism is proposed, called the SBBAN model. Firstly, the character feature attention mechanism is used to infer the key information that contributes greatly to entity recognition in the text sequence. Character weighted features and character features splicing are used as new character input features. Then the deep semantic abstract features of text sequence are obtained by stacked BiLSTM. In addition, a self-attention mechanism is added to obtain the deep context relevant features. Experimental results show that the model shows better performance in in comparison with the state-of-the-art algorithms to complete the matching of event argument entities and offer knowledge support for SCM.},
booktitle = {Proceedings of the 2023 15th International Conference on Machine Learning and Computing},
pages = {40–46},
numpages = {7},
keywords = {event logic knowledge graph, named entity recognition, stacked BiLSTM, supply chain management ontology},
location = {Zhuhai, China},
series = {ICMLC '23}
}

@article{10.1145/3329124,
author = {McDaniel, Melinda and Storey, Veda C.},
title = {Evaluating Domain Ontologies: Clarification, Classification, and Challenges},
year = {2019},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3329124},
doi = {10.1145/3329124},
abstract = {The number of applications being developed that require access to knowledge about the real world has increased rapidly over the past two decades. Domain ontologies, which formalize the terms being used in a discipline, have become essential for research in areas such as Machine Learning, the Internet of Things, Robotics, and Natural Language Processing, because they enable separate systems to exchange information. The quality of these domain ontologies, however, must be ensured for meaningful communication. Assessing the quality of domain ontologies for their suitability to potential applications remains difficult, even though a variety of frameworks and metrics have been developed for doing so. This article reviews domain ontology assessment efforts to highlight the work that has been carried out and to clarify the important issues that remain. These assessment efforts are classified into five distinct evaluation approaches and the state of the art of each described. Challenges associated with domain ontology assessment are outlined and recommendations are made for future research and applications.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {70},
numpages = {44},
keywords = {Ontology, applied ontology, assessment, domain ontology, evaluation, metrics, ontology application, ontology development, task-ontology fit}
}

@inproceedings{10.1145/3543434.3543590,
author = {Ortiz-Rodriguez, Fernando and Tiwari, Sanju and Panchal, Ronak and Medina-Quintero, Jose Melchor and Barrera, Ruben},
title = {MEXIN: Multidialectal Ontology supporting NLP approach to improve government electronic communication with the Mexican Ethnic Groups},
year = {2022},
isbn = {9781450397490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543434.3543590},
doi = {10.1145/3543434.3543590},
abstract = {The government services usually target all citizens, but sometimes physical services nor technology-based services do not cover all people. This research aims to tackle services given to underrepresented citizens in Mexico (Indigenous people) and apply NLP techniques supported by ontologies to achieve accurate translation to most dialects spoken in Mexico. The scope of this paper only tests with Mayan dialect spoken primarily in the Mexican peninsula.},
booktitle = {Proceedings of the 23rd Annual International Conference on Digital Government Research},
pages = {461–463},
numpages = {3},
keywords = {NLP, Ontologies, Semantic Web, eGovernment},
location = {Virtual Event, Republic of Korea},
series = {dg.o '22}
}

@inproceedings{10.5555/3712729.3712933,
author = {Jungmann, Michelle and Lazarova-Molnar, Sanja},
title = {Fusing Expert Knowledge and Data for Simulation Model Discovery in Digital Twins: A Case Study from Reliability Modeling},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Integrating expert knowledge in data-driven Digital Twins can lead to better-informed underlying models. Achieving systematic integration, however, remains a complex challenge. In this study, we propose an initial approach for hybrid model extraction by systematically fusing expert knowledge statements with Internet of Things data from manufacturing systems, such as event and state logs. We outline two main strategies to facilitate the fusion of data and expert knowledge in a systematic way. We, furthermore, present a case study in reliability assessment of manufacturing systems showcasing our methodology within this specific domain. Using our four fusion algorithms, we automatically extract reliability models from both data and expert knowledge statements. Finally, we conduct a comprehensive analysis of the results and draw conclusions regarding the efficacy of the fusion algorithms for Digital Twin model extractions.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2463–2474},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3590777.3590783,
author = {Putrevu, Venkata Sai Charan and Chunduri, Hrushikesh and Putrevu, Mohan Anand and Shukla, Sandeep},
title = {A Framework for Advanced Persistent Threat Attribution using Zachman Ontology},
year = {2023},
isbn = {9781450398299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590777.3590783},
doi = {10.1145/3590777.3590783},
abstract = {Advanced Persistent Threat (APT) is a type of cyber attack that infiltrates a targeted organization and exfiltrates sensitive data over an extended period of time or to cause sabotage. Recently, there has been a trend of nation states backing APT groups in order to further their political and financial interests, making the APT attribution process increasingly important. The APT attribution process involves identifying the actors behind an attack and their motivations, using a method of logical inference called abductive reasoning to determine the most likely explanation for a set of observations. While various attribution methods and frameworks have been proposed by the security community, many of them lack granularity and are dependent on the skills of practitioners rather than a standardized process. This can hinder both the understandability and reproducibility of attribution efforts as this process is practiced but not engineered. To address these issues, we propose a new framework for the APT attribution process based on the Zachman ontology, which offers greater granularity by posing specific primitive questions at various levels of the attribution process. This allows for more accurate conclusions about the attackers and their motivations, helping organizations to better protect themselves against future attacks.},
booktitle = {Proceedings of the 2023 European Interdisciplinary Cybersecurity Conference},
pages = {34–41},
numpages = {8},
keywords = {APT, Attribution Framework, Cyber Criminology, Cyber Investigation., Zachman Ontology},
location = {Stavanger, Norway},
series = {EICC '23}
}

@inproceedings{10.1145/3718491.3718494,
author = {Tang, Yanfei and Wang, Hui and Luo, Huilan and Li, Qin},
title = {A fuzzy expert system based recommendation model for nuclear wastewater research collaborators},
year = {2025},
isbn = {9798400710865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718491.3718494},
doi = {10.1145/3718491.3718494},
abstract = {In order to help scholars in the field of nuclear wastewater explore potential research collaborators more efficiently, this paper proposes a fuzzy expert system-based research collaborator recommendation model (FCR-NWR) to address the problems of existing collaboration recommendation models in terms of textual representation, incomplete extraction of scholars' features, and the lack of diversified information and reasoning decision-making methods. The model fully obtains text features through multilevel text analysis, combines with new methods to calculate scholars' comprehensive academic ability and scientific research cooperation intensity, and finally utilizes the expert system to simulate the knowledge and decision-making process of human experts for collaborator recommendation. Through this model, scholars in the field of nuclear wastewater can integrate into the academic community faster, find potential collaborators, and promote academic co-development. The experimental results show that FCR-NWR outperforms traditional methods in terms of precision, recall and F1 value.},
booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum},
pages = {13–16},
numpages = {4},
keywords = {collaboration recommendation, decision simulation, expert system, fuzzy set, hybrid modeling},
location = {
},
series = {AIBDF '24}
}

@inproceedings{10.5555/3712729.3712889,
author = {Lei\ss{}au, Madlene and Laroque, Christoph},
title = {Breaking Barriers in Semiconductor Simulations: An Automated Low-Code Framework for Model-Structure Synchronisation and Large-Scale Simulation Studies},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {The paradigm shift towards Industry 4.0 and the emerging trends of Industry 5.0 present ongoing challenges in production planning and control. In response to these dynamics, discrete event-driven simulation methods are gaining prominence as an operational decision-support-tool, particularly in the semiconductor industry. This paper introduces an automated low-code framework designed to synchronize model structures across simulation tool boundaries for extensive simulation studies, using the Semiconductor Manufacturing Testbed 2020 as a test reference, and aims to serve as a helpful tool for simulation experiments. Key aspects include model structure synchronization, Design of Experiments, and the distributed execution of large-scale simulation studies.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1919–1930},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@article{10.1145/3345517,
author = {Abdulhameed, Tiba Zaki and Zitouni, Imed and Abdel-Qader, Ikhlas},
title = {Wasf-Vec: Topology-based Word Embedding for Modern Standard Arabic and Iraqi Dialect Ontology},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3345517},
doi = {10.1145/3345517},
abstract = {Word clustering is a serious challenge in low-resource languages. Since words that share semantics are expected to be clustered together, it is common to use a feature vector representation generated from a distributional theory-based word embedding method. The goal of this work is to utilize Modern Standard Arabic (MSA) for better clustering performance of the low-resource Iraqi vocabulary. We began with a new Dialect Fast Stemming Algorithm (DFSA) that utilizes the MSA data. The proposed algorithm achieved 0.85 accuracy measured by the F1 score. Then, the distributional theory-based word embedding method and a new simple, yet effective, feature vector named Wasf-Vec word embedding are tested. Wasf-Vec word representation utilizes a word’s topology features. The difference between Wasf-Vec and distributional theory-based word embedding is that Wasf-Vec captures relations that are not contextually based. The embedding is followed by an analysis of how the dialect words are clustered within other MSA words. The analysis is based on the word semantic relations that are well supported by solid linguistic theories to shed light on the strong and weak word relation representations identified by each embedding method. The analysis is handled by visualizing the feature vector in two-dimensional (2D) space. The feature vectors of the distributional theory-based word embedding method are plotted in 2D space using the t-sne algorithm, while the Wasf-Vec feature vectors are plotted directly in 2D space. A word’s nearest neighbors and the distance-histograms of the plotted words are examined. For validation purpose of the word classification used in this article, the produced classes are employed in Class-based Language Modeling (CBLM). Wasf-Vec CBLM achieved a 7\% lower perplexity (pp) than the distributional theory-based word embedding method CBLM. This result is significant when working with low-resource languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {22},
numpages = {27},
keywords = {2D visualizing, Arabic language, Topology, Word embedding, class-based language modeling, dialect, morphology, orthographic, phonology, words classification, words features, words ontology}
}

@proceedings{10.1145/3563359,
title = {UMAP '23 Adjunct: Adjunct Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
year = {2023},
isbn = {9781450398916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@inproceedings{10.1145/3722237.3722341,
author = {Zhang, Baoliang and Xu, Yinghui and Li, Chengyuan and Jiang, Yuan and Cui, Wenchao and Yue, Hongxu and Hui, Chen and Hu, Ziwei},
title = {Research on Intelligent Management and Assisted Decision Making Technology of Electric Power Research Enterprise Based on Knowledge Engineering},
year = {2025},
isbn = {9798400712692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722237.3722341},
doi = {10.1145/3722237.3722341},
abstract = {Currently, the internal management knowledge of electric power research enterprises is difficult to integrate and structured organization, resulting in greatly reduced decision-making efficiency. Knowledge Graph(KG), as an efficient knowledge organization, helps to connect the complex knowledge network. However, for a Q&amp;A platform based solely on knowledge graphs, it can be challenging to interpret and analyze natural language input, especially when considering historical Q&amp;A data. In this paper, we combine the KG with a Large Language Model(LLM), choose the ERNIE model fine-tuned by the text of electric power domain knowledge, and use the TransE method to integrate the KG into the model's structure to build a knowledge-based Q&amp;A system that assists researchers in decision-making. The experimental results show that the ERNIE model combined with KG outperforms the comparison model in terms of precision, recall, and F1 value in entity and relation classification tasks, and the performance is further improved by increasing the number of attention heads.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education},
pages = {590–600},
numpages = {11},
keywords = {Knowledge Graph embedding, Knowledge Quizzing, Large Language Model},
location = {
},
series = {ICAIE '24}
}

@inproceedings{10.1145/3701716.3715167,
author = {Wang, Ludi and Song, Dongze and Cui, Qiang and Chen, Xueqing and Zhou, Yuanchun and Cui, Wenjuan and Du, Yi},
title = {AutoDive+: An Adaptive Model Enhanced Multimodal Online Annotation Tool},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715167},
doi = {10.1145/3701716.3715167},
abstract = {The demand for data by machine learning models, especially pre-trained large-scale models, has surged dramatically in recent times, resulting in an urgent need for efficient data annotation. Despite the prevalence of annotation tools, they grapple with challenges such as high data conversion costs, limited scalability, and inefficiencies in annotating multimodal data. To tackle these challenges, this paper introduces AutoDive+, an advanced data annotation tool empowered by active learning mechanisms and integrated automatic extraction models. Expanding upon the capabilities of its predecessor, AutoDive, which excels in direct text annotation within PDF documents, AutoDive+ undergoes extensive architectural enhancements. It not only exploits high-value resource ranking modules to boost annotation efficiency across entity and content annotation but also boasts scalable interfaces for seamless integration of new annotation modes and modalities. Furthermore, our novel model community concept facilitates adaptable integration of appropriate automatic extraction and annotation models. We substantiate the effectiveness of our proposed modules through user studies spanning diverse domains. Our vision entails the establishment of an annotation task-driven open community, bolstered by the support of AutoDive+. A live demo of Autodive+ is available at http://autodive.sciwiki.cn/, and a video demo http://autodive.sciwiki.cn/introVideo/introduce-v2.0.mp4. The source code is available at https://github.com/Autodive.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2919–2922},
numpages = {4},
keywords = {human in the loop, intelligent extraction, multimodal annotation tool},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3517804.3524166,
author = {Lutz, Carsten and Przybylko, Marcin},
title = {Efficiently Enumerating Answers to Ontology-Mediated Queries},
year = {2022},
isbn = {9781450392600},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3517804.3524166},
doi = {10.1145/3517804.3524166},
abstract = {We study the enumeration of answers to ontology-mediated queries (OMQs) where the ontology is a set of guarded TGDs or formulated in the description logic ELI and the query is a conjunctive query (CQ). In addition to the traditional notion of an answer, we propose and study two novel notions of partial answers that can take into account nulls generated by existential quantifiers in the ontology. Our main result is that enumeration of the traditional complete answers and of both kinds of partial answers is possible with linear-time preprocessing and constant delay for OMQs that are both acyclic and free-connex acyclic. We also provide partially matching lower bounds. Similar results are obtained for the related problems of testing a single answer in linear time and of testing multiple answers in constant time after linear time preprocessing. In both cases, the border between tractability and intractability is characterized by similar, but slightly different acyclicity properties.},
booktitle = {Proceedings of the 41st ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
pages = {277–289},
numpages = {13},
keywords = {constant delay, description logic, enumeration, ontology-mediated queries, partial answers, tuple generating dependencies},
location = {Philadelphia, PA, USA},
series = {PODS '22}
}

@inproceedings{10.5555/3213214.3213217,
author = {Deatcu, Christina and Folkerts, Hendrik and Pawletta, Thorsten and Durak, Umut},
title = {Design patterns for variability modeling using SES ontology},
year = {2018},
isbn = {9781510860186},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The System Entity Structure (SES) is a high level approach for variability modeling, particularly in simulation engineering, which is under continuous development. In this context, an enhanced framework is introduced that supports dynamic variability evolution using the SES approach. However, the main focus is to start a discussion about a set of design patterns, which were developed to analyze the tree design and computing aspects of System Entity Structures. As development of our MATLAB-based SES toolbox for construction and pruning of SES trees proceeded, the necessity to have some generalized examples for testing and verification came more and more into awareness. We propose a set of design patterns that, if completely representable and computable by a certain tool, support all aspects of SES theory. In addition, the patterns give users substantial support for developing SES models for other applications.},
booktitle = {Proceedings of the Model-Driven Approaches for Simulation Engineering Symposium},
articleno = {3},
numpages = {12},
keywords = {MATLAB/simulink, SES, model generation, simulation engineering, system entity structure, variability modeling, versatile systems},
location = {Baltimore, Maryland},
series = {Mod4Sim '18}
}

@article{10.1145/3408316,
author = {Cornejo-Lupa, Mar\'{\i}a A. and Ticona-Herrera, Regina P. and Cardinale, Yudith and Barrios-Aranibar, Dennis},
title = {A Survey of Ontologies for Simultaneous Localization and Mapping in Mobile Robots},
year = {2020},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3408316},
doi = {10.1145/3408316},
abstract = {Autonomous robots are playing important roles in academic, technological, and scientific activities. Thus, their behavior is getting more complex, particularly, in tasks related to mapping an environment and localizing themselves. These tasks comprise the Simultaneous Localization and Mapping (SLAM) problem. Representation of knowledge related to the SLAM problem with a standard, flexible, and well-defined model, provides the base to develop efficient and interoperable solutions. As many existing works demonstrate, Semantic Web seems to be a clear approach, since they have formulated ontologies, as the base data model to represent such knowledge. In this article, we survey the most popular and recent SLAM ontologies with our aim being threefold: (i) propose a classification of SLAM ontologies according to the main knowledge needed to model the SLAM problem; (ii) identify existing ontologies for classifying, comparing, and contrasting them, in order to conceptualize SLAM domain for mobile robots; and (iii) pin-down lessons to learn from existing solutions in order to design better solutions and identify new research directions and further improvements. We compare the identified SLAM ontologies according to the proposed classification and, finally, we explore new data fields to enrich existing ontologies and highlight new possibilities in terms of performance and efficiency for SLAM solutions.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {103},
numpages = {26},
keywords = {SLAM, Web ontologies, robots, semantic web}
}

@inproceedings{10.5555/3712729.3712866,
author = {Winkels, Jan and \"{O}zkul, Felix and Sutherland, Robin and L\"{o}hn, Jannik and Wenzel, Sigrid and Rehof, Jakob},
title = {Component-Based Synthesis of Structural Variants of Simulation Models for Changeable Material Flow Systems},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {Despite relevant research endeavors, modeling efforts related to the building of discrete-event simulation models for planning changeable material flow systems still limit their practical application. This is because simulation experts have to model many possible structural variants and compare them based on key performance indicators such as throughput, workload or investment costs, while also ensuring sufficient system changeability. This article presents a methodology for reducing efforts for structural variation during the experimental phase of a simulation study. Starting from a valid initial simulation model, structural variants of this simulation model are automatically generated by applying component-based software synthesis which uses combinatorial logic; thereby, a range of simulation models is provided for the user. This paper presents the outlined methodology using a case study and places it in the research context of reducing efforts associated with the design and execution of simulation experiments.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1657–1668},
numpages = {12},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@proceedings{10.1145/3715340,
title = {VaMoS '25: Proceedings of the 19th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2025},
isbn = {9798400714412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@proceedings{10.1145/3685651,
title = {eSAAM '24: Proceedings of the 4th Eclipse Security, AI, Architecture and Modelling Conference on Data Space},
year = {2024},
isbn = {9798400709845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Mainz, Germany}
}

@proceedings{10.1145/3650105,
title = {FORGE '24: Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {FORGE aims to bring researchers, practitioners, and educators from the AI and Software Engineering community to solve the new challenges we meet in the era of foundation models.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3412841.3442142,
author = {Gala, Alexander Pinto-De la and Cardinale, Yudith and Dongo, Irvin and Ticona-Herrera, Regina},
title = {Towards an ontology for urban tourism},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442142},
doi = {10.1145/3412841.3442142},
abstract = {Nowadays, diffusion and preservation of cultural heritage are being supported by technology on the Web. Thus, the online availability of urban tourism information, as part of cultural heritage, has been of enormous relevance to activate the tourism in many countries. The necessity of a well-defined and standard model for representing this knowledge is being managed by semantic web technologies, such as ontologies. However, current proposals represent partial knowledge of cultural heritage. In this context, this work proposes an ontology for indoor and outdoor environments of a city to represent the cultural heritage knowledge based on the UNESCO definitions. This ontology has a three-level architecture (Upper, Middle, and Lower ontologies) in accordance with a purpose of modularity and levels of specificity. To demonstrate the utility and suitability of our proposal, we have developed a parser to map and convert a museum repository (in CSV format) to RDF triples. With this case of study, we demonstrated that, by using our ontology, it is possible to represent the knowledge of urban tourism domains of a city.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1887–1890},
numpages = {4},
keywords = {automatic population, ontology, urban tourism},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3543507.3584185,
author = {Gautam, Nikita and Shumway, David and Kowalcyk, Megan and Khanal, Sarthak and Caragea, Doina and Caragea, Cornelia and Mcginty, Hande and Dorevitch, Samuel},
title = {Leveraging Existing Literature on the Web and Deep Neural Models to Build a Knowledge Graph Focused on Water Quality and Health Risks},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3584185},
doi = {10.1145/3543507.3584185},
abstract = {A knowledge graph focusing on water quality in relation to health risks posed by water activities (such as diving or swimming) is not currently available. To address this limitation, we first use existing resources to construct a knowledge graph relevant to water quality and health risks using KNowledge Acquisition and Representation Methodology (KNARM). Subsequently, we explore knowledge graph completion approaches for maintaining and updating the graph. Specifically, we manually identify a set of domain-specific UMLS concepts and use them to extract a graph of approximately 75,000 semantic triples from the Semantic MEDLINE database (which contains head-relation-tail triples extracted from PubMed). Using the resulting knowledge graph, we experiment with the KG-BERT approach for graph completion by employing pre-trained BERT/RoBERTa models and also models fine-tuned on a collection of water quality and health risks abstracts retrieved from the Web of Science. Experimental results show that KG-BERT with BERT/RoBERTa models fine-tuned on a domain-specific corpus improves the performance of KG-BERT with pre-trained models. Furthermore, KG-BERT gives better results than several translational distance or semantic matching baseline models.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {4161–4171},
numpages = {11},
keywords = {BERT, Knowledge graph, diving, health risks, knowledge graph completion, water quality, water recreation},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3550356.3556509,
author = {Tong, Li and Yiting, Wang and Congkai, Geng},
title = {Detection of anomalous modeling behavior: a goal-driven data mining approach},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3556509},
doi = {10.1145/3550356.3556509},
abstract = {Precise and timely detection of anomalous modeling behaviors is critical for both teaching and application of modeling methods. Existing methods usually focus on evaluating the modeling results rather than mining the knowledge hidden in the modeling process. In this paper, we propose to monitor and analyze the modeling process in order to timely detect anomalous modeling behaviors, potentially contributing to a comprehensive assessment of the modeling practice. Specifically, we propose to systematically build a goal model for characterizing the normal modeling behaviors, which establishes the connections between modelers' high-level modeling behaviors and low-level modeling operations. On top of such a goal model, we propose a data mining-based approach to semi-automatically validate the design of the goal model and explore other normal modeling behaviors. Then, we propose to automatically detect anomalous modeling behaviors by capturing normal modeling behaviors obtained from the goal model and actual modeling sequences. We have developed and deployed a data-flow diagram modeling platform, which implemented our proposed approach. We have conducted an experiment with 57 participants, the preliminary results of which show that our approach can effectively detect modelers' anomalous behaviors. The experiment results are beneficial for not only assessing the modelers' performance but also identifying the usability issues of the modeling tool.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {142–145},
numpages = {4},
keywords = {anomalous modeling behavior detection, data mining, goal modeling},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@article{10.1145/3616111,
author = {Mehmood, Faiza and Shahzadi, Rehab and Ghafoor, Hina and Asim, Muhammad Nabeel and Ghani, Muhammad Usman and Mahmood, Waqar and Dengel, Andreas},
title = {EnML: Multi-label Ensemble Learning for Urdu Text Classification},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3616111},
doi = {10.1145/3616111},
abstract = {Exponential growth of electronic data requires advanced multi-label classification approaches for the development of natural language processing (NLP) applications such as recommendation systems, drug reaction detection, hate speech detection, and opinion recognition/mining. To date, several machine and deep learning–based multi-label classification methodologies have been proposed for English, French, German, Chinese, Arabic, and other developed languages. Urdu is the 11th largest language in the world and has no computer-aided multi-label textual news classification approach. Unlike other languages, Urdu is lacking multi-label text classification datasets that can be used to benchmark the performance of existing machine and deep learning methodologies. With an aim to accelerate and expedite research for the development of Urdu multi-label text classification–based applications, this article provides multiple contributions as follows: First, it provides a manually annotated multi-label textual news classification dataset for the Urdu language. Second, it benchmarks the performance of traditional machine learning approaches particularly by adapting three data transformation approaches along with three top-performing machine learning classifiers and four algorithm adaptation-based approaches. Third, it benchmarks performance of 16 existing deep learning approaches and the four most widely used language models. Finally, it provides an ensemble approach that reaps the benefits of three different deep learning architectures to precisely predict different classes associated with a particular Urdu textual document. Experimental results reveal that proposed ensemble approach performance values (87\% accuracy, 92\% F1-score, and 8\% hamming loss) are significantly higher than adapted machine and deep learning–based approaches.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {227},
numpages = {31},
keywords = {Multi-label Urdu text classification, multi-label Urdu news dataset, traditional machine learning, deep learning, language models, multi-label ensemble learning, data transformation methods}
}

@inproceedings{10.5555/3492252.3492274,
author = {Fassbinder, Marcelo and Fassbinder, Aracele and Fioravanti, Maria Lydia and Barbosa, Ellen Francine},
title = {Towards an educational design pattern language to support the development of open educational resources in videos for the MOOC context},
year = {2021},
publisher = {The Hillside Group},
address = {USA},
abstract = {The creation and adoption of Massive Open Online Courses (MOOCs) can bring many benefits and impact on education, such as put forward diversity in education; enhance student's learning by encouraging and engaging them for lifelong learning; connect with more individuals in informal contexts creating opportunities to transition to formal higher education or lifelong learning activities; force a re-conceptualization of higher education through the use of online study; enhance teachers' skills from developing Open Educational Resources (OERs) and adopting learner-centered pedagogical approaches and active learning strategies; among others. In such perspective, several studies have investigated the potential benefits about the use of videos as a support for the process of teaching in virtual learning environments and, particularly, in the context of MOOCs. However, video production for MOOCs still presents several challenges that need to be better investigated. It is because, in general, educators and MOOC teams (e.g. educators, learning designers, and educational technologists) are still using ad hoc decision-making procedures based on empirical knowledge obtained from their experiences with face-to-face courses or even traditional virtual courses. There are also gaps about what adaptations need to be performed to video formats and what are the attributes and steps needed to support the production and validation of videos for the MOOC context. In addition, since MOOCs are part of the Open Educational Movement, as well as the OERs, it is also important to reflect on the construction of OERs in the form of videos for the MOOC context. There is, therefore, a need for research that investigates the current theoretical panorama involving video construction for MOOCs, in order to propose strategies empirically validated and useful to support and guide MOOC teams during the development of videos more theoretically informed. Considering such context, the main objective of this paper is to present a set of patterns and move towards the development of an educational design pattern language able to support MOOC teams in the construction of OERs in the form of videos. The paper presents a life cycle for OERs production in the form of videos. From such cycle activities, we extracted nine patterns that are presented in the form of patlets (problem-solution pair). The patterns were also collected and refined from a literature review on guidelines about the production of MOOCs, OERs, and educational videos. The patterns are divided into five categories related to the life cycle: analysis, planning development, evaluation, and distribution. These patterns are expected to offer a compelling alternative to guide MOOC teams in designing OERs in the form of videos to enhance learning experiences, increase student engagement in the course, and emphasize self-directed learning, which are requirements for quality in MOOCs.},
booktitle = {Proceedings of the 26th Conference on Pattern Languages of Programs},
articleno = {18},
numpages = {10},
keywords = {MOOCs, educational design patterns, educational videos, learning design, patterns},
location = {Urbana, Illinois},
series = {PLoP '19}
}

@inproceedings{10.1145/3726302.3729988,
author = {Li, Wanli and Qian, Tieyun and Song, Yi and Zhang, Zeyu and Li, Jiawei and Chen, Zhuang and Zou, Lixin},
title = {Generative Meta-Learning for Zero-Shot Relation Triplet Extraction},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3729988},
doi = {10.1145/3726302.3729988},
abstract = {Zero-shot Relation Triplet Extraction (ZeroRTE) aims to extract relation triplets from texts containing unseen relation types. This capability benefits various downstream information retrieval (IR) tasks. The primary challenge lies in enabling models to generalize effectively to unseen relation categories. Existing approaches typically leverage the knowledge embedded in pre-trained language models to accomplish the generalization process. However, these methods focus solely on fitting the training data during training, without specifically improving the model's generalization performance, resulting in limited generalization capability. For this reason, we explore the integration of bi-level optimization (BLO) with pre-trained language models for learning generalized knowledge directly from the training data, and propose a generative meta-learning framework which exploits the 'learning-to-learn' ability of meta-learning to boost the generalization capability of generative models.Specifically, we introduce a BLO approach that simultaneously addresses data fitting and generalization. This is achieved by constructing an upper-level loss to focus on generalization and a lower-level loss to ensure accurate data fitting. Building on this, we subsequently develop three generative meta-learning methods, each tailored to a distinct category of meta-learning. Extensive experimental results demonstrate that our framework performs well on the ZeroRTE task. Our code is available at https://github.com/leeworry/TGM-MetaLearning.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1371–1381},
numpages = {11},
keywords = {meta-learning, pre-trained language model, relation triplet extraction, zero-shot learning},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3707292.3707392,
author = {Duan, Jinjian and Wei, Deming and Wang, Meiqing and Chen, Gaohui and Zhang, Yang and Gao, Yanhua},
title = {Knowledge-based Retrieval Methods for Enhancing Aerospace Model Software Documentation},
year = {2025},
isbn = {9798400707308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707292.3707392},
doi = {10.1145/3707292.3707392},
abstract = {Addressing the critical challenges of poor structure, weak relevance, and limited reusability in aerospace model software development documents, we propose a knowledge-based retrieval method. This method constructs a knowledge base by analyzing key document features, including storage methods, file formats, content structure, inter-document associations, and naming conventions. The system implements structured document management and associates documents based on software configuration information. By partitioning the retrieval domain, constructing semantic vectors, and applying multi-dimensional weight configurations, the method enables efficient retrieval in domains characterized by dense information and frequent term repetition. An aerospace model software document retrieval system has been developed and preliminarily implemented in an aerospace enterprise, demonstrating its effectiveness in improving retrieval efficiency and enhancing the reuse of development knowledge.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Intelligent Information Processing},
pages = {372–378},
numpages = {7},
keywords = {Intelligent Retrieval 2, Knowledge Base 4, Knowledge Reuse 3, Model Software Document 1},
location = {
},
series = {AIIIP '24}
}

@inproceedings{10.1145/3545947.3576365,
author = {Katyshev, Alexander and Anikin, Anton and Sychev, Oleg},
title = {Using Transformer Models for Knowledge Graph Construction in Computer Science Education},
year = {2023},
isbn = {9781450394338},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545947.3576365},
doi = {10.1145/3545947.3576365},
abstract = {The volume of information that can be used in the development of knowledge bases that can be used in education is constantly increasing. Also, this amount of data is very difficult to process and store. When designing a knowledge base to optimize the educational process, it is important to use ontologies. At the moment, the creation of an ontological knowledge model is the most promising option for storing and processing information. The article describes effective approaches for generating an ontological model using machine learning models based on the Transformer model.},
booktitle = {Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 2},
pages = {1421},
numpages = {1},
keywords = {concepts, machine learning, neural networks, ontological graph, ontologies, relations between concepts, semantics, transformers},
location = {Toronto ON, Canada},
series = {SIGCSE 2023}
}

@article{10.1145/3479012,
author = {Troncoso, Alvaro R. Ortiz},
title = {Ontology-Based Approach to Creating Semantic Wikis},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3479012},
doi = {10.1145/3479012},
abstract = {Maintaining a semantic wiki is challenging. Coping with increasingly complex wikis led to the development of a methodical approach for simplifying the creation and maintenance of semantic wikis. The methodical approach used involves modeling the semantic relationships underlying the information in the wiki as an ontology, and then programmatically creating the wiki from the ontology constructs. A methodical approach for creating and maintaining wikis greatly simplifies the creation and maintenance of semantic wikis. Furthermore, reusing vocabularies and taxonomies throughout several projects becomes manageable. The approach was implemented as open source software that maps ontology constructs to the wiki artifacts, and practical applications are discussed.},
journal = {J. Comput. Cult. Herit.},
month = apr,
articleno = {28},
numpages = {7},
keywords = {Semantic wikis, collaborative research environment, domain-driven software development}
}

@article{10.1145/3589338,
author = {Storey, Veda C. and Lukyanenko, Roman and Castellanos, Arturo},
title = {Conceptual Modeling: Topics, Themes, and Technology Trends},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {14s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3589338},
doi = {10.1145/3589338},
abstract = {Conceptual modeling is an important part of information systems development and use that involves identifying and representing relevant aspects of reality. Although the past decades have experienced continuous digitalization of services and products that impact business and society, conceptual modeling efforts are still required to support new technologies as they emerge. This paper surveys research on conceptual modeling over the past five decades and shows how its topics and trends continue to evolve to accommodate emerging technologies, while remaining grounded in basic constructs. We survey over 5,300 papers that address conceptual modeling topics from the 1970s to the present, which are collected from 35 multidisciplinary journals and conferences, and use them as the basis from which to analyze the progression of conceptual modeling. The important role that conceptual modeling should play in our evolving digital world is discussed, and future research directions proposed.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {317},
numpages = {38},
keywords = {Conceptual modeling, digital world, database, information systems, information technology, structured literature review, clustering analysis}
}

@article{10.1145/3487066,
author = {Zorrilla, Asier L\'{o}pez and Torres, M. In\'{e}s},
title = {A Multilingual Neural Coaching Model with Enhanced Long-term Dialogue Structure},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3487066},
doi = {10.1145/3487066},
abstract = {In this work we develop a fully data driven conversational agent capable of carrying out motivational coaching sessions in Spanish, French, Norwegian, and English. Unlike the majority of coaching, and in general well-being related conversational agents that can be found in the literature, ours is not designed by hand-crafted rules. Instead, we directly model the coaching strategy of professionals with end users. To this end, we gather a set of virtual coaching sessions through a Wizard of Oz platform, and apply state of the art Natural Language Processing techniques. We employ a transfer learning approach, pretraining GPT2 neural language models and fine-tuning them on our corpus. However, since these only take as input a local dialogue history, a simple fine-tuning procedure is not capable of modeling the long-term dialogue strategies that appear in coaching sessions. To alleviate this issue, we first propose to learn dialogue phase and scenario embeddings in the fine-tuning stage. These indicate to the model at which part of the dialogue it is and which kind of coaching session it is carrying out. Second, we develop a global deep learning system which controls the long-term structure of the dialogue. We also show that this global module can be used to visualize and interpret the decisions taken by the the conversational agent, and that the learnt representations are comparable to dialogue acts. Automatic and human evaluation show that our proposals serve to improve the baseline models. Finally, interaction experiments with coaching experts indicate that the system is usable and gives rise to positive emotions in Spanish, French and English, while the results in Norwegian point out that there is still work to be done in fully data driven approaches with very low resource languages.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jul,
articleno = {16},
numpages = {47},
keywords = {Dialogue system, coaching, multilingual, transfer learning, explainable artificial intelligence}
}

@inproceedings{10.1145/3625007.3627305,
author = {Mu, Wenchuan and Lim, Kwan Hui},
title = {Modelling Text Similarity: A Survey},
year = {2024},
isbn = {9798400704093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625007.3627305},
doi = {10.1145/3625007.3627305},
abstract = {Online social networking services such as Twitter and Instagram have become pervasive platforms for engaging in discussions on a wide array of topics. These platforms cater to both mainstream subjects, like music and movies, as well as more specialized areas, such as politics. With the growing volume of textual data generated on these platforms, the ability to define and identify similar texts becomes crucial for effective investigation and clustering. In this paper, we explore the challenges and significance of text similarity regression models in the context of online social networking services. We delve into the methods and techniques employed to define and find similarities among texts, enabling the extraction of meaningful patterns and insights. Specifically, we categorize text similarity regression models into four distinct types: set-theoretic, sequence-theoretic, real-vector, and end-to-end methods. This categorization is based on the mathematical formalisation of similarity used by each model. Ultimately, our survey aims to provide a comprehensive overview of the interlinkages between independently proposed methods for text similarity. By understanding the strengths and weaknesses of these methods, researchers can make informed decisions when designing novel approaches and algorithms. We hope this survey serves as a valuable resource for advancing the state-of-the-art in addressing the complex problem of text similarity.},
booktitle = {Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {698–705},
numpages = {8},
keywords = {modelling and simulation, deep learning and embeddings, algorithms and techniques},
location = {Kusadasi, Turkiye},
series = {ASONAM '23}
}

@inproceedings{10.1145/3183440.3195087,
author = {Atchison, Abigail and Anderson, Haley and Berardi, Christina and Best, Natalie and Firmani, Cristiano and German, Rene and Linstead, Erik},
title = {A topic analysis of the R programming language},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3195087},
doi = {10.1145/3183440.3195087},
abstract = {We leverage Latent Dirichlet Allocation to analyze R source code from 10,051 R packages in order to better understand the topic space of scientific computing. Our method is able to identify several generic programming concepts and, more importantly, identify concepts that are highly specific to scientific and high performance computing applications.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {183–184},
numpages = {2},
keywords = {R, machine learning, topic modeling},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3466933.3466946,
author = {Helfer, Gilson Augusto and Costa, Adilson Ben da and Bavaresco, Rodrigo Simon and Barbosa, Jorge Luis Vict\'{o}ria},
title = {Tellus-Onto: uma ontologia para classifica\c{c}\~{a}o e infer\^{e}ncia de solos na agricultura de precis\~{a}o: Tellus-Onto: an ontology for soil classification and inference in precision agriculture},
year = {2021},
isbn = {9781450384919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466933.3466946},
doi = {10.1145/3466933.3466946},
abstract = {Soil analysis laboratories demand large volumes of data used in precision agriculture. Among them, parameters that represent soil fertility such as texture and organic matter guide the fertilization process. However, this process can take time, thus limiting its usefulness. Therefore, this article proposes an ontology called Tellus-Onto that extends the state of the art in the classification of Brazilian soils according to the organic and textural composition. A series of axioms and semantic rules provided consultations and inferences about their instantiated basis. In order to test the ontology, we added 98 soil sample results and their classifications were inferred precisely and automatically. Laborat\'{o}rios de an\'{a}lises de solos demandam volumes grandes de dados empregados na agricultura de precis\~{a}o. Dentre eles, par\^{a}metros que representam fertilidade de solos como textura e mat\'{e}ria org\^{a}nica orientam o processo de aduba\c{c}\~{a}o. No entanto, este processo pode se tornar demorado, limitando assim sua utilidade. Sendo assim, este artigo prop\~{o}e uma ontologia denominada Tellus-Onto que estende o estado da arte na classifica\c{c}\~{a}o de solos brasileiros de acordo com a composi\c{c}\~{a}o org\^{a}nica e textural. Uma s\'{e}rie de axiomas e regras sem\^{a}nticas foram empregadas para proporcionar a realiza\c{c}\~{a}o de consultas e infer\^{e}ncias sobre sua base instanciada. Para testar a ontologia foram instanciados 98 resultados de amostras de solos e inferidos suas classifica\c{c}\~{o}es de modo preciso e autom\'{a}tico.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Information Systems},
articleno = {13},
numpages = {7},
keywords = {agricultura de precis\~{a}o, classifica\c{c}\~{a}o de solos, ontology, precision agriculture ontologia, soil classification},
location = {Uberl\^{a}ndia, Brazil},
series = {SBSI '21}
}

@inproceedings{10.1145/3428757.3429143,
author = {Rocha, Rodrigo and Bion, Danillo and Azevedo, Ryan and Gomes, Arthur and Cordeiro, Diogo and Leandro, Renan and Silva, Israel and Freitas, Fred},
title = {A Syntactic and Semantic Assessment of a Global Software Engineering Domain Ontology},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429143},
doi = {10.1145/3428757.3429143},
abstract = {Globalization has allowed organizations to intensify the search for solutions that minimize challenges, reduce costs and optimize processes. In this way, global software development has emerged as an attempt to use the best resources for its limitations.In distributed environments, the use of Ontologies brings some benefits such as a uniform understanding of information among teams and ease of communication, as well as making for the lack of a reference model that can be applied in a distributed context.This work aims to propose a viable form of validation for DKDonto a domain ontology developed for Global Software Engineering. The validation allowed a broader and more targeted assessment, different from its original validation, which was carried out in a controlled environment, limited to answering questions already known by the knowledge base itself.The main result of this work is a satisfactory evaluation of the ontology, enabling it to be used and shared by companies or institutions, as well as the presentation of a set of methods and ways to evaluate and verify domain ontologies to be used in different domains.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {253–262},
numpages = {10},
keywords = {Evaluation, Global Software Development, Ontology},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1145/3586183.3606715,
author = {Zaidi, Ali and Turbeville, Kelsey and Ivan\v{c}i\'{c}, Kristijan and Moss, Jason and Gutierrez Villalobos, Jenny and Sagar, Aravind and Li, Huiying and Mehra, Charu and Li, Sixuan and Hutchins, Scott and Kumar, Ranjitha},
title = {Learning Custom Experience Ontologies via Embedding-based Feedback Loops},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606715},
doi = {10.1145/3586183.3606715},
abstract = {Organizations increasingly rely on behavioral analytics tools like Google Analytics to monitor their digital experiences. Making sense of the data these tools capture, however, requires manual event tagging and filtering — often a tedious process. Prior approaches have trained machine learning models to automatically tag interaction data, but draw from fixed digital experience vocabularies which cannot be easily augmented or customized. This paper introduces a novel machine learning interaction pattern that generates customized tag predictions for organizations. The approach employs a general user experience word embedding to bootstrap an initial set of predictions, which can then be refined and customized by users to adapt the underlying vector space, iteratively improving the quality of future predictions. The paper presents a needfinding study that grounds the design choices of the system, and describes a real-world deployment as part of UserTesting.com that demonstrates the efficacy of the approach.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {111},
numpages = {13},
keywords = {Sankey diagrams, UX research, clickstream analytics, sequence alignment, usability testing},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{10.1145/3589132.3625618,
author = {Wang, Renzhong and Najafabadi, Maryam and Zhang, Chiqun and Chen, Long-Qi and Olenina, Tanya and Yankov, Dragomir},
title = {GPT Applications in Relevance Model Training in Map Search},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625618},
doi = {10.1145/3589132.3625618},
abstract = {Understanding map queries and retrieving correct entity results are the two main relevance tasks in Map search. They are usually performed by a set of task specific machine learning models. Collecting large amount of high quality labelled data for training such models is a time-consuming and labor-intensive process. Although various methods have been studied for producing pseudo data labels, they are limited in their effectiveness when applied across different languages or tasks. The recently released Large Language models (LLMs), including ChatGPT and GPT-4 (GPT for short), have demonstrated state-of-the-art performance in text understanding by using simple prompt instructions with only a handful of examples for in-context learning. In this paper, we explore GPT as a cost-effective alternative for both data labeling and synthetic data generation, where we subsequently use data obtained from this approach to train various task specific models such as maps intent detection, address detection, address parsing, geo-entity ranking, and rank scores calibration. GPT demonstrates strong potential in generating otherwise hard-to-synthesize data. We observe significant accuracy and relevance improvement across all task specific models when trained or fine-tuned on data generated by GPT. Lastly, we propose a general framework combining labeled data from GPT with other sources and a prompt fine-tune structure to guide GPT model in completing a given task.},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {68},
numpages = {4},
keywords = {GPT, query processing, maps service, information retrieval},
location = {Hamburg, Germany},
series = {SIGSPATIAL '23}
}

@inproceedings{10.1145/3368691.3368745,
author = {Sharipbay, Altynbek and Razakhova, Bibigul and Mukanova, Assel and Yergesh, Banu and Yelibayeva, Gaziza},
title = {Syntax parsing model of Kazakh simple sentences},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368745},
doi = {10.1145/3368691.3368745},
abstract = {This paper proposes a syntactic analysis of Kazakh simple sentences taking into account their semantics. To do this, first, the syntactic rules of sentences are described using formal grammar, then parsing trees and ontological models are built to determine the semantics of their components and the relationships between them. As a formal grammar used Chomsky's context-free grammar, and ontological models were built in the environment of Protege.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {54},
numpages = {5},
keywords = {formal grammar, ontological model, parsing of Kazakh language, parsing tree, simple sentences, syntactic rules of sentences},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/3563357.3564083,
author = {Fierro, Gabe and Saha, Avijit and Shapinsky, Tobias and Steen, Matthew and Eslinger, Hannah},
title = {Application-driven creation of building metadata models with semantic sufficiency},
year = {2022},
isbn = {9781450398909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563357.3564083},
doi = {10.1145/3563357.3564083},
abstract = {Semantic metadata models such as Brick, RealEstateCore, Project Haystack, and BOT promise to simplify and lower the cost of developing software for smart buildings, enabling the widespread deployment of energy efficiency applications. However, creating these models remains a challenge. Despite recent advances in creating models from existing digital representations like point labels and architectural models, there is still no feedback mechanism to ensure that the human input to these methods results in a model that can actually support the desired software.In this paper, we introduce the notion of semantic sufficiency, a practical principle for semantic metadata model creation that asserts that a model is "finished" when it contains the metadata necessary to support a given set of applications. To support semantic sufficiency, we design a standard representation for capturing application metadata requirements and a templating system for generating common metadata model components with limited user input. We then construct an iterative model creation workflow that integrates metadata requirements to direct the model creation effort, and present several novel optimizations that increase the model utility while minimizing the effort by a human operator. These new abstractions for model creation and validation lower model development costs and ensure the utility of the resulting model, thus facilitating the adoption of intelligent building applications.},
booktitle = {Proceedings of the 9th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {228–237},
numpages = {10},
keywords = {applications, brick, metadata, ontology, semantics},
location = {Boston, Massachusetts},
series = {BuildSys '22}
}

@inproceedings{10.1145/3492547.3492680,
author = {Bekmanova, Gulmira and Nazyrova, Aizhan and Sharipbay, Altynbek and Suvorovsky, Oleg and Somzhurek, Baubek},
title = {Two approaches of improving e-learning models qualities},
year = {2021},
isbn = {9781450390446},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492547.3492680},
doi = {10.1145/3492547.3492680},
abstract = {ABSTRACTThe proposed two approaches of improving e-learning models qualities let us redesign existing models and e-learning systems.Model for organizing blended and distance learning involves the creation of an individual learning path, which makes it flexible. Ontological model used like a tool for optimization and redesign existing system modules.},
booktitle = {The 7th International Conference on Engineering \&amp; MIS 2021},
articleno = {83},
numpages = {3},
keywords = {Blended learning, artificial intelligence, distance learning, e-learning, e-learning system optimization, ontological model, personalized training},
location = {Almaty, Kazakhstan},
series = {ICEMIS'21}
}

@inproceedings{10.1145/3485447.3511924,
author = {Xiang, Yue and Wu, Xuan and Lu, Chang and Zhao, Yizheng},
title = {Creating Signature-Based Views for Description Logic Ontologies with Transitivity and Qualified Number Restrictions},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511924},
doi = {10.1145/3485447.3511924},
abstract = {Developing ontologies for the Semantic Web is a time-consuming and error-prone task that typically requires the investment of considerable manpower and resources, as well as collaborative efforts. A potentially better idea is to reuse the “off-the-shelf” ontologies, whenever possible, somehow as per certain demands and requirements. A promising way to achieve ontology reuse is through creating views of ontologies, analogous to creating views of databases, with the resulting views focusing on specific topics and content of the original ontologies. This paper explores the problem of creating views of ontologies using a uniform interpolation approach. In particular, we develop a novel and practical uniform interpolation method for creating signature-based views for ontologies specified in the description logic , a very expressive description logic for which uniform interpolation has not been fully addressed. The method is terminating and sound, and computes uniform interpolants of -ontologies by eliminating from the input ontologies the names not used in the view using a forgetting procedure. This makes it the first and so far the only approach to eliminate both concept and (non-transitive) role names from -ontologies. Despite the inherent difficulty of uniform interpolation for this level of expressivity, an empirical evaluation with a prototypical implementation show very good success rates on a corpus of real-world ontologies, and demonstrates clear algorithmic advantage over the state-of-the-art system LETHE. This is extremely useful from the semantic web perspective, as it provides knowledge engineers with a powerful tool to create views of ontologies for ontology reuse.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {808–817},
numpages = {10},
keywords = {Description Logics, Forgetting, Knowledge Representation and Reasoning, Ontologies, The Semantic Web, Uniform Interpolation},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3736733.3736737,
author = {Fletcher, George and Nahurna, Olha and Prytula, Matvii and Stoyanovich, Julia},
title = {CREDAL: Close Reading of Data Models},
year = {2025},
isbn = {9798400719592},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3736733.3736737},
doi = {10.1145/3736733.3736737},
abstract = {Data models are foundational to the creation of data and any data-driven system. Every algorithm, ML model, statistical model, and database depends on a data model to function. As such, data models are rich sites for examining the material, social, and political conditions shaping technical systems. Inspired by literary criticism, we propose close readings of data models—treating them as artifacts to be analyzed like texts. This practice highlights the materiality, genealogy, techne, closure, and design of data systems.While literary theory teaches that no single reading is "correct," systematic guidance is vital—especially for those in computing and data science, where sociopolitical dimensions are often overlooked. To address this gap, we introduce the CREDAL methodology for close readings of data models. We describe its iterative development and share results from a qualitative evaluation, demonstrating its usability and value for critical data studies.},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {4},
numpages = {7},
location = {Intercontinental Berlin, Berlin, Germany},
series = {HILDA '25}
}

@article{10.1145/3565364,
author = {Aameri, Bahar and Gr\"{u}ninger, Michael},
title = {Reducible Theories and Amalgamations of Models},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {1529-3785},
url = {https://doi.org/10.1145/3565364},
doi = {10.1145/3565364},
abstract = {Within knowledge representation in artificial intelligence, a first-order ontology is a theory in first-order logic that axiomatizes the concepts in some domain. Ontology verification is concerned with the relationship between the intended models of an ontology and the models of the axiomatization of the ontology. In particular, we want to characterize the models of an ontology up to isomorphism and determine whether or not these models are equivalent to the intended models of the ontology. Unfortunately, it can be quite difficult to characterize the models of an ontology up to isomorphism. In the first half of this article, we review the different metalogical relationships between first-order theories and identify which relationship is needed for ontology verification. In particular, we will demonstrate that the notion of logical synonymy is needed to specify a representation theorem for the class of models of one first-order ontology with respect to another. In the second half of the article, we discuss the notion of reducible theories and show we can specify representation theorems by which models are constructed by amalgamating models of the constituent ontologies.},
journal = {ACM Trans. Comput. Logic},
month = jan,
articleno = {9},
numpages = {24},
keywords = {Amalgamations of models, reducible theories, synonymous theories, relative interpretation, model theory, first-order logic}
}

@inproceedings{10.1145/3417990.3421406,
author = {Chammard, Thomas Boyer and Regalia, Blake and Karban, Robert and Gomes, Ivan},
title = {Assisted authoring of model-based systems engineering documents},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421406},
doi = {10.1145/3417990.3421406},
abstract = {In systems engineering practices, system design and analysis have historically been performed using a document-centric approach where stakeholders produce a number of documents that represent their views on a system under development. Given the ad-hoc, disparate, and informal nature of natural language documents, these views become quickly inconsistent. Rigor in engineering work is also lost in the transition from model-based engineering design and analysis to engineering documents. Once the documents are delivered, the engineering portion of the work is disconnected. In the Open Model Based Engineering Environment (OpenMBEE), Cross-References (aka transclusions) synthesize relevant engineering information where model elements are not simply hyperlinked, but de-referenced in place in a document, upgrading a document-based process with model-based engineering technology. Those Cross-References are nowadays partially created manually, putting a burden on the engineer who is authoring the document. This paper presents an approach which can assist the engineer by providing machine-generated suggestions for Cross-References using language processing, graph analysis, and clustering technologies on model data managed by the OpenMBEE infrastructure.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {36},
numpages = {7},
keywords = {MBSE, OpenMBEE, SysML, clustering, entity linking, graph analysis, natural language processing},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3443467.3443723,
author = {Yuan, Ming and Yang, Shulin and Gu, Mengdie and Gu, Huijie},
title = {Analysis and Research on Book Recommendation Model Based on Big Data},
year = {2021},
isbn = {9781450387811},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443467.3443723},
doi = {10.1145/3443467.3443723},
abstract = {In the context of big data, how to define user behavior models and provide personalized reading services for readers by mining large amounts of user data is a problem that the current reading platform needs to optimize urgently. First, we need to analyze the user behavior model to construct the research status and existing problems, in order to provide large data personalized services, targeted user behavior model based on reading platform construction strategies and construction methods, and designs a user logging library is utilized to extract the user interest in dominant and recessive demand ontology of personalized service plan. The user behavior model based on ontology can be technically seamlessly connected with the big data analysis platform, so as to provide real-time and accurate services, which can effectively deal with the challenges of "knowledge trek", "information overload" and "emotional loss" faced by the personalized service of the reading platform in the current big data environment.},
booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
pages = {21–25},
numpages = {5},
keywords = {Big Data, Linked Data, Ontology, Personalized Service, User Behavior Model},
location = {Xiamen, China},
series = {EITCE '20}
}

@inproceedings{10.1145/3711896.3737444,
author = {Difallah, Djellel},
title = {WikiRAG: Revisiting Wikidata KGC Datasets with Community Updates and Retrieval-Augmented Generation},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737444},
doi = {10.1145/3711896.3737444},
abstract = {Link prediction is an important task for knowledge graph completion and curation, and it has received significant attention from the research community. However, researchers often train and evaluate new models on small or outdated datasets that do not reflect the current state of knowledge, thereby disregarding new information and the rich textual content often linked to knowledge graphs. As a result, many opportunities to leverage these dimensions are missed. We introduce WikiRAG, a framework for knowledge completion and evaluation derived from Wikidata and Wikipedia, which enables research integrating retrieval techniques and large language models. Our framework combines the following contributions: (i) We revisit the Wikidata5M dataset by updating it to reflect the current state of Wikidata and providing automated tools for its periodic maintenance. (ii) We enrich the dataset with long-form textual content sourced from Wikipedia, enabling research that goes beyond traditional graph structures and shallow text methods toward dense retrieval techniques. (iii) We propose a simple yet effective baseline that leverages retrieval-augmented generation, demonstrating the utility of the dataset and integrating language model capabilities for link prediction. The revised dataset, coined Wikidata5M-RE, shows that the original graph grew by roughly 50\% in the number of edges, while 10\% of the edges have been removed. A comparative analysis of classic methods demonstrates that these changes can impact downstream task evaluation. Finally, our evaluation of WikiRAG's KGC method shows an improvement of up to 9\% in link prediction accuracy over state-of-the-art baselines, setting the stage for a new avenue in knowledge completion that uses deep information extraction. The source code, data, and other artifacts have been made available on the project website: https://github.com/colab-nyuad/WikiRAG},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {5391–5401},
numpages = {11},
keywords = {benchmarking, knowledge graph completion, large language models, retrieval augmented generation, wikidata},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3487664.3487781,
author = {Martinez-Gil, Jorge and Yin, Shaoyi and K\"{u}ng, Josef and Morvan, Franck},
title = {Matching Large Biomedical Ontologies Using Symbolic Regression},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487781},
doi = {10.1145/3487664.3487781},
abstract = {The problem of ontology matching consists of finding the semantic correspondences between two ontologies that, although belonging to the same domain, have been developed separately. Matching methods are of great importance since they allow us to find the pivot points from which an automatic data integration process can be established. Unlike the most recent developments based on deep learning, this study presents our research on the development of new methods for ontology matching that are accurate and interpretable at the same time. For this purpose, we rely on a symbolic regression model specifically trained to find the mathematical expression that can solve the ground truth accurately, with the possibility of being understood by a human operator and forcing the processor to consume as little energy as possible. The experimental evaluation results show that our approach seems to be promising.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {162–167},
numpages = {6},
keywords = {Information Integration, Ontology Matching, Similarity Measures},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3674658.3674660,
author = {Kohli, Nikita and Tomal, Jabed and Lin, Wenjun and Yan, Yan},
title = {PentaPen: Combining Penalized Models to Identify Important SNPs on Whole-genome Arabidopsis thaliana Data},
year = {2024},
isbn = {9798400717666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674658.3674660},
doi = {10.1145/3674658.3674660},
abstract = {In the rapidly advancing field of genomics, the identification of Single Nucleotide Polymorphisms (SNPs) plays a crucial role in understanding complex phenotypic traits. This study introduces “PentaPen”, an innovative computational workflow which combines the strengths of five penalized models to achieve improved accuracy in SNP detection. We compare the performance of PentaPen with existing models, highlighting its advantages in solving problems arising from when the number of predictors exceeds the number of samples. Beyond model comparison, we provide insights into PentaPen’s effectiveness in utilizing all SNPs as input, streamlines data pre-processing, and leverages parallel computation, enabling the workflow a considerable stride in SNP detection. Furthermore, a thorough evaluation and comparison of computational complexities signifies competitive edge of the workflow over individual penalized models. As future research directions, we propose applications of PentaPen to plant-specific characteristics and suggest further explorations to assess the robustness of its findings. In summary, this manuscript presents the genomics community with a tool that combines computational efficiency with high-precision SNP detection, making a strong contribution to the field of genomic research.},
booktitle = {Proceedings of the 2024 16th International Conference on Bioinformatics and Biomedical Technology},
pages = {9–16},
numpages = {8},
keywords = {Genome-Wide Association Study, Single Nucleotide Polymorphism, SNP Identification, Machine Learning, High Dimensional Data, Regression, Classification},
location = {
},
series = {ICBBT '24}
}

@inproceedings{10.1145/3640310.3674085,
author = {Ben Chaaben, Meriem and Ben Sghaier, Oussama and Dhaouadi, Mouna and Elrasheed, Nafisa and Darif, Ikram and Jaoua, Imen and Oakes, Bentley and Syriani, Eugene and Hamdaqa, Mohammad},
title = {Toward Intelligent Generation of Tailored Graphical Concrete Syntax},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674085},
doi = {10.1145/3640310.3674085},
abstract = {In model-driven engineering, the concrete syntax of a domain-specific modeling language (DSML) is fundamental as it constitutes the primary point of interaction between the user and the DSML. Nevertheless, the conventional one-size-fits-all approach to concrete syntax often undermines the effectiveness of DSMLs, as it fails to accommodate the diverse constraints and specific requirements inherent to diverse users and usage contexts. Such shortcomings can lead to a significant decline in the performance, usability, and efficiency of DSMLs. This vision paper proposes a conceptual framework to generate concrete syntax intelligently. Our framework considers multiple concerns of users and aims to align the concrete syntax with the context of the DSML usage. Additionally, we detail a baseline process to employ our framework in practice, leveraging large language models to expedite the generation of tailored concrete syntax. We illustrate the potential of our vision with two concrete examples and discuss the shortcomings and research challenges of current intelligent generation techniques.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {160–171},
numpages = {12},
keywords = {Artificial Intelligence, Concrete Syntax, Domain-specific Modeling Languages, Large Language Models},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.1145/3640457.3687114,
author = {Anelli, Vito Walter and Ferrara, Antonio and Musto, Cataldo and Narducci, Fedelucio and Ragone, Azzurra and Zanker, Markus},
title = {Sixth Knowledge-aware and Conversational Recommender Systems Workshop (KaRS)},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3687114},
doi = {10.1145/3640457.3687114},
abstract = {Recommender systems, though widely used, often struggle to engage users effectively. While deep learning methods have enhanced connections between users and items, they often neglect the user’s perspective. Knowledge-based approaches, utilizing knowledge graphs, offer semantic insights and address issues like knowledge graph embeddings, hybrid recommendation, and interpretable recommendation. More recently, neural-symbolic systems, combining data-driven and symbolic techniques, show promise in recommendation systems, especially when used with knowledge graphs. Moreover, content features become vital in conversational recommender systems, which demand multi-turn dialogues. Recent literature highlights increasing interest in this area, particularly with the emergence of Large Language Models (LLMs), which excel in understanding user queries and generating recommendations in natural language. Sixth Knowledge-aware and Conversational Recommender Systems (KaRS) Workshop aims to disseminate advancements and discuss about challenges and opportunities.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {1245–1249},
numpages = {5},
keywords = {conversational agents, knowledge graphs, large language models, natural language processing, neuro-symbolic, recommender systems},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3438872.3439091,
author = {Pei, Pei and Xuejing, Ding and Deqing, Zhang},
title = {Construction of Curriculum Knowledge Map based on Ontology},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439091},
doi = {10.1145/3438872.3439091},
abstract = {With the rapid development of science and technology, knowledge update is accelerating, which puts forward higher requirements for teachers and students in Colleges and universities, and needs to grasp the latest development trends of curriculum knowledge related fields faster, more comprehensive and more accurate. Many schools have digitized their educational resources. However, the traditional sharing of educational resources lacks a unified knowledge representation structure, which makes the sharing and reuse of learning resources unsatisfactory. Curriculum is the core of school knowledge teaching. It evaluates the curriculum system and discipline system in order to achieve certain teaching objectives. Curriculum knowledge includes explicit knowledge and tacit knowledge.Knowledge map is a kind of model which can describe knowledge in semantic and knowledge level. Its purpose is to acquire, organize and present knowledge in a general and intuitive way, to search and match knowledge quickly, so as to improve the utilization of knowledge by learners and knowledge workers. Curriculum knowledge has obvious ontology characteristics, and there are many inconveniences and shortcomings in the presentation of traditional knowledge map. Using ontology technology to construct curriculum knowledge map can not only reflect the relationship between knowledge modules, but also realize the mining and representation of more knowledge relations and types such as tacit knowledge to a certain extent.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {259–265},
numpages = {7},
keywords = {curriculum knowledge, knowledge map, ontology, ontology construction},
location = {Shanghai, China},
series = {RICAI '20}
}

@inproceedings{10.1145/3514094.3534137,
author = {Franklin, Jade S. and Bhanot, Karan and Ghalwash, Mohamed and Bennett, Kristin P. and McCusker, Jamie and McGuinness, Deborah L.},
title = {An Ontology for Fairness Metrics},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534137},
doi = {10.1145/3514094.3534137},
abstract = {Recent research has revealed that many machine-learning models and the datasets they are trained on suffer from various forms of bias, and a large number of different fairness metrics have been created to measure this bias. However, determining which metrics to use, as well as interpreting their results, is difficult for a non-expert due to a lack of clear guidance and issues of ambiguity or alternate naming schemes between different research papers. To address this knowledge gap, we present the Fairness Metrics Ontology (FMO), a comprehensive and extensible knowledge resource that defines each fairness metric, describes their use cases, and details the relationships between them. We include additional concepts related to fairness and machine learning models, enabling the representation of specific fairness information within a resource description framework (RDF) knowledge graph. We evaluate the ontology by examining the process of how reasoning-based queries to the ontology were used to guide the fairness metric-based evaluation of a synthetic data model.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {265–275},
numpages = {11},
keywords = {bias, fairness metric, machine learning evaluation, rdf knowledge graph},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@inbook{10.1145/3677389.3702536,
author = {Zielinski, Andrea and Hirzel, Simon and Arnold-Keifer, Sonja},
title = {Enhancing Digital Libraries with Automated Definition Generation},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702536},
abstract = {Scientific domains encompass many concepts that require a concise term definition to enable a common understanding among researchers, in particular for interdisciplinary fields. In digital libraries, information access and sharing is often facilitated by terminology databases. However, building up such resources is expensive to produce manually and requires expert knowledge. Automatically generating definitions for scientific terms has become a hot research topic recently that can reduce the manual burden. However, current methods heavily rely on large language models (LLMs) that store factual knowledge in their parameters, so that knowledge cannot be easily updated for emerging scientific terms. Furthermore, a major shortcoming of these models is that they are prone to hallucination and their output is difficult to control. To bridge these gaps, we propose to address the task of definition generation through guided abstractive summarization, incorporating key information from external resources. At test time, we augment the model with retrieved abstracts from Scopus and use automatically extracted topics and keywords as guidance, both essential for definition generation. To this aim, our approach takes into account two relevant sub-tasks in the process, a) predicting the topic class and b) generating hypernym candidates for the term. Our proposed pipelined approach for automatic guided definition generation achieves significant performance improvement over the standard baselines as well as relevant prior works on this problem. We use BLEU, ROUGE and BERTScore to automatically evaluate the quality of the systems on our benchmark and carry out a human evaluation to assess fluency, relevancy, coherence and factuality of the output. Our experiments show that LLMs can provide fluent and coherent definitions, and are often on par with human created definitions. Yet, there is still room for improvement on identifying relevant content and improving factual correctness.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {57},
numpages = {11}
}

@inproceedings{10.1145/3319499.3328238,
author = {Ousmer, Mehdi and Vanderdonckt, Jean and Buraga, Sabin},
title = {An ontology for reasoning on body-based gestures},
year = {2019},
isbn = {9781450367455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319499.3328238},
doi = {10.1145/3319499.3328238},
abstract = {Body-based gestures, such as acquired by Kinect sensor, today benefit from efficient tools for their recognition and development, but less for automated reasoning. To facilitate this activity, an ontology for structuring body-based gestures, based on user, body and body parts, gestures, and environment, is designed and encoded in Ontology Web Language according to modelling triples (subject, predicate, object). As a proof-of-concept and to feed this ontology, a gesture elicitation study collected 24 participants X 19 referents for IoT tasks = 456 elicited body-based gestures, which were classified and expressed according to the ontology.},
booktitle = {Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
articleno = {17},
numpages = {6},
keywords = {Microsoft Kinect, gesture interaction, natural gestures, ontology web language, resource description file},
location = {Valencia, Spain},
series = {EICS '19}
}

@inproceedings{10.1145/3555776.3577696,
author = {Heng, Samedi and Tsilionis, Konstantinos and Wautelet, Yves},
title = {Building User Stories and Behavior Driven Development Scenarios with a Strict Set of Concepts: Ontology, Benefits and Primary Validation},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577696},
doi = {10.1145/3555776.3577696},
abstract = {Behavior Driven Development (BDD) offers a way to express scenarios, written in structured natural language, on how the system should act to fulfill a requirement. Such a test scenario is written together with the requirement; this way these two can be conceived in unison, nested into each other. Lots of templates have been written to construct BDD scenarios and various practices were born out of usage. We fail to find documentation on templates. A strict set of templates with a clear definition of the used keywords aligned with the intends of BDD has been proposed recently in the form of an ontology. The present paper (i) evaluates the ontology on existing BDD scenarios found in the GitHub repository (exogenous validation) and (ii) merges an ontology for user story elements' representation with one for expressing BDD scenarios to evaluate its ability to guide the writing of BDD scenarios (endogenous validation). By linking both through strictly-identified concepts, we (i) provide guidance to the practitioner in the agile requirements engineering phase and (ii) with the adequate tagging of elements during the requirements engineering process we get meta-data allowing to suggest treatments to the BDD scenario (e.g. test automation or forward engineering into a software architecture or even code).},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1422–1429},
numpages = {8},
keywords = {behavior driven development, user stories, test scenarios, agile development, scrum},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3445969.3450429,
author = {Chukkapalli, Sai Sree Laya and Aziz, Shaik Barakhat and Alotaibi, Nouran and Mittal, Sudip and Gupta, Maanak and Abdelsalam, Mahmoud},
title = {Ontology driven AI and Access Control Systems for Smart Fisheries},
year = {2021},
isbn = {9781450383196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445969.3450429},
doi = {10.1145/3445969.3450429},
abstract = {Increasing number of internet connected devices has paved a path for smarter ecosystems in various sectors such as agriculture, aquaculture, manufacturing, healthcare, etc. Especially, integrating technologies like big data, artificial intelligence (AI), blockchain, etc. with internet connected devices has increased efficiency and productivity. Therefore, fishery farmers have started adopting smart fisheries technologies to better manage their fish farms. Despite their technological advancements smart fisheries are exposed and vulnerable to cyber-attacks that would cause a negative impact on the ecosystem both physically and economically.Therefore in this paper, we present a smart fisheries ecosystem where the architecture describes various interactions that happen between internet connected devices. We develop a smart fisheries ontology based on the architecture and implement Attribute Based Access Control System (ABAC) where access to resources of smart fisheries is granted by evaluating the requests. We also discuss how access control decisions are made in multiple use case scenarios of a smart fisheries ecosystem. Furthermore, we elaborate on some AI applications that would enhance the smart fisheries ecosystem.},
booktitle = {Proceedings of the 2021 ACM Workshop on Secure and Trustworthy Cyber-Physical Systems},
pages = {59–68},
numpages = {10},
keywords = {access control, artificial intelligence, cybersecurity, ontology, smart fisheries},
location = {Virtual Event, USA},
series = {SAT-CPS '21}
}

@inproceedings{10.1145/3733102.3733152,
author = {Vielhauer, Claus and Loewe, Fabian and Pilgermann, Michael},
title = {Towards Modeling Hidden \&amp; Steganographic Malware Communication based on Images},
year = {2025},
isbn = {9798400718878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733102.3733152},
doi = {10.1145/3733102.3733152},
abstract = {Recently, an increasing number of IT security incidents involving malware, which makes use of hidden and steganographic channels for malicious communication (a.k.a. as "stegomalware"), can be observed in the wild. Especially the use of images to hide malicious code is rising. In consideration of this shift, a new model is proposed in this paper, which aims to help security professionals to identify and analyze incidents revolving around steganographic malware in the future. The model focuses on practical aspects of steganalysis of communication data to elaborate linking properties to previous code analysis knowledge. The model features two distinct roles that interact with a knowledge base which stores malware features and helps building a context for the incident. For evaluation, two image steganography malware types are chosen from popular databases (malpedia and MITRE ATT&amp;CK®), which are analyzed in multiple steps including steganalysis and code analysis. It is conceptually shown, how the extracted features can be stored in a knowledge base for later use to identify stegomalware from communication data without the need of a thorough code analysis. This allows to uncover previously hidden meta-information about the examined malicious programs, enrich the incident’s forensic context traces and thus allows for thorough forensic insights, including attribution and improved preventive security measures in the future.},
booktitle = {Proceedings of the 2025 ACM Workshop on Information Hiding and Multimedia Security},
pages = {52–63},
numpages = {12},
keywords = {Media forensics, stegomalware, image steganography, attribution},
location = {
},
series = {IH&amp;MMSEC '25}
}

@article{10.1109/TCBB.2022.3170301,
author = {Feng, Yuhao and Qi, Lei and Tian, Weidong},
title = {PhenoBERT: A Combined Deep Learning Method for Automated Recognition of Human Phenotype Ontology},
year = {2022},
issue_date = {March-April 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3170301},
doi = {10.1109/TCBB.2022.3170301},
abstract = {Automated recognition of Human Phenotype Ontology (HPO) terms from clinical texts is of significant interest to the field of clinical data mining. In this study, we develop a combined deep learning method named PhenoBERT for this purpose. PhenoBERT uses BERT, currently the state-of-the-art NLP model, as its core model for evaluating whether a clinically relevant text segment (CTS) could be represented by an HPO term. However, to avoid unnecessary comparison of a CTS with each of ∼14,000 HPO terms using BERT, we introduce a two-levels CNN module consisting of a series of CNN models organized at two levels in PhenoBERT. For a given CTS, the CNN module produces only a short list of candidate HPO terms for BERT to evaluate, significantly improving the computational efficiency. In addition, BERT is able to assign an ancestor HPO term to a CTS when recognition of the direct HPO term is not successful, mimicking the process of HPO term assignment by human. In two benchmarks, PhenoBERT outperforms four traditional dictionary-based methods and two recently developed deep learning-based methods in two benchmark tests, and its advantage is more obvious when the recognition task is more challenging. As such, PhenoBERT is of great use for assisting in the mining of clinical text data.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = apr,
pages = {1269–1277},
numpages = {9}
}

@inproceedings{10.1145/3644116.3644311,
author = {Zhang, Xiyan and Hu, Chenping and Zhang, Lei and Huang, Zhisheng and Qin, Hongyun},
title = {Analysis of the mechanism of antipsychotics induced abnormal ECG using Medical Ontologies and Medical Knowledge Graphs},
year = {2024},
isbn = {9798400708138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644116.3644311},
doi = {10.1145/3644116.3644311},
abstract = {Objective: Our aim is to understand the underlying mechanisms of antipsychotics-induced ECG abnormalities by using artificial intelligence (AI) method and information processing technique. Methods: This study employed the SNOMED CT (Systematized Nomenclature of Medicine - Clinical Terms) and the knowledge graph of brain science, to find relevant concepts associated with ECG abnormalities and antipsychotics usage. Results: Approximately 30 relevant publications were found, only 10 articles were included in this comprehensive analysis study. Our findings suggest that the underlying mechanisms of antipsychotic-induced ECG abnormalities include immediate, medium and lasting effects on the predisposing level, the autonomic nervous system, cardiomyocytes, and the blood vessel levels. Conclusions: The potential mechanisms underpinning antipsychotics-induced ECG abnormalities include predisposing, the autonomic nervous system, cardiomyocytes, and the blood vessel levels by use of the NOMED CT and the knowledge graph of brain science.},
booktitle = {Proceedings of the 2023 4th International Symposium on Artificial Intelligence for Medicine Science},
pages = {1146–1151},
numpages = {6},
location = {Chengdu, China},
series = {ISAIMS '23}
}

@article{10.1145/3733234,
author = {Zhao, Chuang and Tang, Hui and Zhao, Hongke and Li, Xiaomeng},
title = {Beyond Sequential Patterns: Rethinking Healthcare Predictions with Contextual Insights},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3733234},
doi = {10.1145/3733234},
abstract = {Healthcare predictions, such as readmission prediction, stand as a cornerstone of societal well-being, exerting a profound influence on individual health outcomes and communal vitality. Existing research primarily employs advanced graph neural networks and sequential algorithms for patient modeling, with a focus on discerning the connections and sequential patterns inherent in Electronic Health Records (EHRs). However, the heterogeneity of entity interactions, the locality of EHR data, and the oversight of target relevance hinder further improvements. To address these limitations, we introduce a novel framework Beyond Sequential Patterns (BSP), which facilitates precise healthcare predictions by incorporating tri-contextual information. Specifically, we establish a symptom-driven hypergraph network with four semantic hyperedges tailored to the intricacies of the healthcare scenario, such as ontology. This serves as a global context, tracking the heterogeneous entity collaboration within and across patients. Moreover, we construct an extensive knowledge graph leveraging existing medical databases and large language models. By sampling and refining knowledge subgraphs as local context, we bolster the semantic associations of medical entities from closed-set EHR data to the open world. Finally, we introduce the candidate context, an explicit entity-relation loss. It enforces the neighbor consistency between the target and the representation during optimization, thus accounting for correlations among targets. Extensive experiments and rigorous robustness analysis on five tasks derived from four large medical datasets underscore the BSP’s superiority over the leading baselines, with improvements of 11\%, 3\%, 11\%, 3.5\%, and 2\% across five tasks, demonstrating the efficacy of incorporating diverse contexts.},
journal = {ACM Trans. Inf. Syst.},
month = jul,
articleno = {107},
numpages = {32},
keywords = {Healthcare prediction, Hypergraph learning, Large language model}
}

@inproceedings{10.1145/3590777.3590785,
author = {Ukegbu, Chibuzo and Neupane, Ramesh and Mehrpouyan, Hoda},
title = {Ontology-based Framework for Boundary Verification of Safety and Security Properties in Industrial Control Systems},
year = {2023},
isbn = {9781450398299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3590777.3590785},
doi = {10.1145/3590777.3590785},
abstract = {As part of Industrial Control Systems (ICS), the control logic controls the physical processes of critical infrastructures such as power plants and water and gas distribution. The Programmable Logic Controller (PLC) commonly manages these processes through actuators based on information received from sensor readings. Therefore, boundary checking is essential in ICS because sensor readings and actuator values must be within the safe range to ensure safe and secure ICS operation. In this paper, we propose an ontology-based approach to provide the knowledge required to verify the boundaries of ICS components with respect to their safety and security specifications. For the proof of concept, the formal model of the Programmable Logic Controller (PLC) is created in UPPAAL and validated in UPPAAL-API. Then, the proposed boundary verification algorithm is used to import the required information from the safety/security ontology},
booktitle = {Proceedings of the 2023 European Interdisciplinary Cybersecurity Conference},
pages = {47–52},
numpages = {6},
keywords = {Control Systems, Formal Verification, Security Properties},
location = {Stavanger, Norway},
series = {EICC '23}
}

@inproceedings{10.1145/3429523.3429534,
author = {Wu, Jifang and Lv, Jianghua and Guo, Haoming and Ma, Shilong},
title = {Ontology Matching by Jointly Encoding Terminological Description and Network Structure},
year = {2020},
isbn = {9781450375276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429523.3429534},
doi = {10.1145/3429523.3429534},
abstract = {Ontology matching is usually performed to find semantic correspondences between the entity elements of different ontologies to enable interoperability. Current research on ontology matching has largely focused on representation learning. However, there still exist two limitations. Firstly, they are only used in the element level matching phase, ignoring relations of the entity. Secondly, the final alignment threshold is usually determined manually within these methods. It is difficult for an expert to adjust the threshold value and even more for non-expert user. To address these issues, we propose an alternative ontology matching framework, which models the matching process by embedding techniques with jointly encoding ontology terminological description and network structure. We further improve our iterative final alignment method by introducing an automatic adjustment of threshold method. Finally, we perform an experimental evaluation and compare it with state-of-the-art ontology matching systems on four Ontology Alignment Evaluation Initiative (OAEI) datasets. Our approach performs better than most of the systems and achieves a competitive performance. Moreover, we obtained F-measure values of 93.8\% and 90.8\% on the OAEI Large Biomedical Ontologies FMA-NCI and FMA-SNOMED subtasks.},
booktitle = {Proceedings of the 2020 5th International Conference on Cloud Computing and Internet of Things},
pages = {77–85},
numpages = {9},
keywords = {Ontology matching, final alignment, graph attention-based autoencoder, network embedding, semantic similarity},
location = {Okinawa, Japan},
series = {CCIOT '20}
}

@inproceedings{10.1145/3651671.3651779,
author = {Liu, Lijuan and Shi, Li},
title = {An Intelligent Risk Mining Method by Whole Process-Oriented Risk Analysis Model},
year = {2024},
isbn = {9798400709234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651671.3651779},
doi = {10.1145/3651671.3651779},
abstract = {This paper proposes a risk intelligent analysis method oriented to the whole process of events and applications, it is used to solve the problem of difficult and incomplete identification of risks, to dig out the risks behind it, and to provide help to ensure the safety of the public. Specifically, through step-by-step mining of incidents and application usage risks, combined with the event evolutionary graph, the model is run to calculate the similarity of text risks for comprehensive analysis, the innovative point is to propose a risk weighting model for the subject and object entities. Experiment shows that the method can reflect the complete risk of events and applications, and has improved recall and precision. The advantage of this method is the integration of multiple perspectives and disciplines, which has guiding significance in practice.},
booktitle = {Proceedings of the 2024 16th International Conference on Machine Learning and Computing},
pages = {616–620},
numpages = {5},
keywords = {Whole Process-Oriented analysis, artificial intelligence, event evolutionary graph, risk mining},
location = {Shenzhen, China},
series = {ICMLC '24}
}

@inproceedings{10.1145/3366424.3383755,
author = {Geng, Qian and Deng, Siyu and Jia, Danping and Jin, Jian},
title = {Cross-domain Ontology Construction and Alignment from Online Product Reviews},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3383755},
doi = {10.1145/3366424.3383755},
abstract = {Online customer product reviews often contain detailed sentiment attitudes towards different aspects of products and these online opinions help potential consumers to be familiar with products. The introduction of well-constructed domain ontology from online product reviews helps potential consumers to obtain relevant information about products quickly. Nonetheless, they may compare products in multiple domains for purchase decisions. On this basis, the comparison of products in different domains induces that ontology alignment becomes a fundamental task to form a cross-domain ontology. In this paper, a series of natural language processing approaches are applied to construct two domain ontologies from online product reviews automatically. Next, a new ontology alignment method is proposed for consumers to make purchase decisions regarding cross-domain product comparisons, in which a semantic-based algorithm and a structure-based algorithm are integrated to form a cross-domain ontology. Categories of experiments were conducted on reviews of smartphone and digital camera. Compared with benchmarked alignment tools, the proposed method performed 5\% better in terms of F1 on ontology alignment. Finally, a case study with a customer friendly website is illustrated to present how the alignment of cross-domain ontology help consumers on purchase decision support.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {401–408},
numpages = {8},
keywords = {ontology alignment, ontology construction, product comparison, product review, purchase decision making.},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3641859,
author = {Che, Shangkun and Liu, Hongyan and Liu, Shen},
title = {Tagging Items with Emerging Tags: A Neural Topic Model Based Few-Shot Learning Approach},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3641859},
doi = {10.1145/3641859},
abstract = {The tagging system has become a primary tool to organize information resources on the Internet, which benefits both users and the platforms. To build a successful tagging system, automatic tagging methods are desired. With the development of society, new tags keep emerging. The problem of tagging items with emerging tags is an open challenge for an automatic tagging system, and it has not been well studied in the literature. We define this problem as a tag-centered cold-start problem in this study and propose a novel neural topic model based few-shot learning method named NTFSL to solve the problem. In our proposed method, we innovatively fuse the topic modeling task with the few-shot learning task, endowing the model with the capability to infer effective topics to solve the tag-centered cold-start problem with the property of interpretability. Meanwhile, we propose a novel neural topic model for the topic modeling task to improve the quality of inferred topics, which helps enhance the tagging performance. Furthermore, we develop a novel inference method based on the variational auto-encoding framework for model inference. We conducted extensive experiments on two real-world datasets, and the results demonstrate the superior performance of our proposed model compared with state-of-the-art machine learning methods. Case studies also show the interpretability of the model.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {102},
numpages = {37},
keywords = {Few-shot learning, neural topic model, automatic tagging, generative probabilistic model, classification, deep learning}
}

@article{10.1145/3660521,
author = {Zeng, Kaisheng and Jin, Hailong and Lv, Xin and Zhu, Fangwei and Hou, Lei and Zhang, Yi and Pang, Fan and Qi, Yu and Liu, Dingxiao and Li, Juanzi and Feng, Ling},
title = {XLORE 3: A Large-Scale Multilingual Knowledge Graph from Heterogeneous Wiki Knowledge Resources},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {1046-8188},
url = {https://doi.org/10.1145/3660521},
doi = {10.1145/3660521},
abstract = {In recent years, knowledge graph (KG) has attracted significant attention from academia and industry, resulting in the development of numerous technologies for KG construction, completion, and application. XLORE is one of the largest multilingual KGs built from Baidu Baike and Wikipedia via a series of knowledge modeling and acquisition methods. In this article, we utilize systematic methods to improve XLORE's data quality and present its latest version, XLORE 3, which enables the effective integration and management of heterogeneous knowledge from diverse resources. Compared with previous versions, XLORE 3 has three major advantages: (1) We design a comprehensive and reasonable schema, namely XLORE ontology, which can effectively organize and manage entities from various resources. (2) We merge equivalent entities in different languages to facilitate knowledge sharing. We provide a large-scale entity linking system to establish the associations between unstructured text and structured KG. (3) We design a multi-strategy knowledge completion framework, which leverages pre-trained language models and vast amounts of unstructured text to discover missing and new facts. The resulting KG contains 446 concepts, 2,608 properties, 66 million entities, and more than 2 billion facts. It is available and downloadable online at , providing a valuable resource for researchers and practitioners in various fields.},
journal = {ACM Trans. Inf. Syst.},
month = aug,
articleno = {145},
numpages = {47},
keywords = {Knowledge graph, knowledge management, knowledge fusion, knowledge completion, schema construction, entity typing, entity alignment, entity linking}
}

@inproceedings{10.1145/3589334.3645648,
author = {Jackermeier, Mathias and Chen, Jiaoyan and Horrocks, Ian},
title = {Dual Box Embeddings for the Description Logic EL++},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645648},
doi = {10.1145/3589334.3645648},
abstract = {OWL ontologies, whose formal semantics are rooted in Description Logic (DL), have been widely used for knowledge representation. Similar to Knowledge Graphs (KGs), ontologies are often incomplete, and maintaining and constructing them has proved challenging. While classical deductive reasoning algorithms use the precise formal semantics of an ontology to predict missing facts, recent years have witnessed growing interest in inductive reasoning techniques that can derive probable facts from an ontology. Similar to KGs, a promising approach is to learn ontology embeddings in a latent vector space, while additionally ensuring they adhere to the semantics of the underlying DL. While a variety of approaches have been proposed, current ontology embedding methods suffer from several shortcomings, especially that they all fail to faithfully model one-to-many, many-to-one, and many-to-many relations and role inclusion axioms. To address this problem and improve ontology completion performance, we propose a novel ontology embedding method named Box2EL for the DL EL++, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles), and models inter-concept relationships using a bumping mechanism. We theoretically prove the soundness of Box2EL and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets on the tasks of subsumption prediction, role assertion prediction, and approximating deductive reasoning.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {2250–2258},
numpages = {9},
keywords = {description logic, link prediction, ontology completion, ontology embedding, web ontology language},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{10.1145/3277591,
author = {Bounhas, Ibrahim},
title = {On the Usage of a Classical Arabic Corpus as a Language Resource: Related Research and Key Challenges},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3277591},
doi = {10.1145/3277591},
abstract = {This article presents a literature review of computer-science-related research applied on hadith, a kind of Arabic narration which appeared in the 7th century. We study and compare existent works in several fields of Natural Language Processing (NLP), Information Retrieval (IR), and Knowledge Extraction (KE). Thus, we illicit their main drawbacks and identify some perspectives, which may be considered by the research community. We also study the characteristics of these types of documents, by enumerating the advantages/limits of using hadith as a language resource. Moreover, our study shows that previous studies used different collections of hadiths, thus making it hard to compare their results objectively. Besides, many preprocessing steps are recurrent through these applications, thus wasting a lot of time. Consequently, the key issues for building generic language resources from hadiths are discussed, taking into account the relevance of related literature and the wide community of researchers that are interested in these narrations. The ultimate goal is to structure hadith books for multiple usages, thus building common collections which may be exploited in future applications.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {23},
numpages = {45},
keywords = {Hadith processing, hadith knowledge extraction, hadith mining, hadith retrieval}
}

@inproceedings{10.1145/3459930.3469524,
author = {Mohan, Sunil and Angell, Rico and Monath, Nicholas and McCallum, Andrew},
title = {Low resource recognition and linking of biomedical concepts from a large ontology},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459930.3469524},
doi = {10.1145/3459930.3469524},
abstract = {Tools to explore scientific literature are essential for scientists, especially in biomedicine, where about a million new papers are published every year. Many such tools provide users the ability to search for specific entities (e.g. proteins, diseases) by tracking their mentions in papers. PubMed, the most well known database of biomedical papers, relies on human curators to add these annotations. This can take several weeks for new papers, and not all papers get tagged. Machine learning models have been developed to facilitate the semantic indexing of scientific papers. However their performance on the more comprehensive ontologies of biomedical concepts does not reach the levels of typical entity recognition problems studied in NLP. In large part this is due to their low resources, where the ontologies are large, there is a lack of descriptive text defining most entities, and labeled data can only cover a small portion of the ontology. In this paper, we develop a new model that overcomes these challenges by (1) generalizing to entities unseen at training time, and (2) incorporating linking predictions into the mention segmentation decisions. Our approach achieves new state-of-the-art results for the UMLS ontology in both traditional recognition/linking (+8 F1 pts) as well as semantic indexing-based evaluation (+10 F1 pts).},
booktitle = {Proceedings of the 12th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {54},
numpages = {10},
keywords = {UMLS, biomedical concept recognition, deep learning, named entity recognition and linking},
location = {Gainesville, Florida},
series = {BCB '21}
}

@article{10.1145/3494560,
author = {Abulaish, Muhammad and Fazil, Mohd and Zaki, Mohammed J.},
title = {Domain-Specific Keyword Extraction Using Joint Modeling of Local and Global Contextual Semantics},
year = {2022},
issue_date = {August 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4681},
url = {https://doi.org/10.1145/3494560},
doi = {10.1145/3494560},
abstract = {Domain-specific keyword extraction is a vital task in the field of text mining. There are various research tasks, such as spam e-mail classification, abusive language detection, sentiment analysis, and emotion mining, where a set of domain-specific keywords (aka lexicon) is highly effective. Existing works for keyword extraction list all keywords rather than domain-specific keywords from a document corpus. Moreover, most of the existing approaches perform well on formal document corpuses but fail on noisy and informal user-generated content in online social media. In this article, we present a hybrid approach by jointly modeling the local and global contextual semantics of words, utilizing the strength of distributional word representation and contrasting-domain corpus for domain-specific keyword extraction. Starting with a seed set of a few domain-specific keywords, we model the text corpus as a weighted word-graph. In this graph, the initial weight of a node (word) represents its semantic association with the target domain calculated as a linear combination of three semantic association metrics, and the weight of an edge connecting a pair of nodes represents the co-occurrence count of the respective words. Thereafter, a modified PageRank method is applied to the word-graph to identify the most relevant words for expanding the initial set of domain-specific keywords. We evaluate our method over both formal and informal text corpuses (comprising six datasets), and show that it performs significantly better in comparison to state-of-the-art methods. Furthermore, we generalize our approach to handle the language-agnostic case, and show that it outperforms existing language-agnostic approaches.},
journal = {ACM Trans. Knowl. Discov. Data},
month = jan,
articleno = {70},
numpages = {30},
keywords = {Text mining, information extraction, domain-specific keyword extraction, language-agnostic keyword extraction}
}

@proceedings{10.1145/3631700,
title = {UMAP Adjunct '24: Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Cagliari, Italy}
}

@inproceedings{10.1145/3664476.3670470,
author = {Teixeira De Castro, Hugo and Hussain, Ahmed and Blanc, Gregory and El Hachem, Jamal and Blouin, Dominique and Leneutre, Jean and Papadimitratos, Panos},
title = {A Model-based Approach for Assessing the Security of Cyber-Physical Systems},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3670470},
doi = {10.1145/3664476.3670470},
abstract = {Cyber-Physical Systems (CPSs) complexity has been continuously increasing to support new life-impacting applications, such as Internet of Things (IoT) devices or Industrial Control Systems (ICSs). These characteristics introduce new critical security challenges to both industrial practitioners and academics. This work investigates how Model-Based System Engineering (MBSE) and attack graph approaches could be leveraged to model secure Cyber-Physical System solutions and identify high-impact attacks early in the system development life cycle. To achieve this, we propose a new framework that comprises (1) an easily adoptable modeling paradigm for Cyber-Physical System representation, (2) an attack-graph-based solution for Cyber-Physical System automatic quantitative security analysis, based on the MulVAL security tool, (3) a set of Model-To-Text (MTT) transformation rules to bridge the gap between SysML and MulVAL. We illustrated the validity of our proposed framework through an autonomous ventilation system example. A Denial of Service (DoS) attack targeting an industrial communication protocol was identified and displayed as attack graphs. In future work, we intend to connect the approach to dynamic security databases for automatic countermeasure selection.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {121},
numpages = {10},
keywords = {Critical Infrastructures, Risk Analysis, Security and Privacy for Cyber-Physical Systems, Security by Design., Threats and Attack Modelling, Usable Security and Privacy},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3316615.3316682,
author = {Rodzman, Shaiful Bakhtiar bin and Suhaili, Siti Suhaima binti and Ismail, Normaly Kamal and Rahman, Nurazzah Abd and Aljunid, Syed Ahmad and Omar, Aslida binti},
title = {Domain Specific Classification of Malay Based Complaints using the Complaint Concept Ontologies},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3316682},
doi = {10.1145/3316615.3316682},
abstract = {The complaint from users is an effective method to identify the quality of services and facilities provided by an organization. The efficiency to respond to users' complaint also depends on an effective workflow. By having an effective method and workflow, the action taken by the management to improve the quality of services and facilities can be done immediately and effectively. One of the ways is by classifying the complaints that will isolate related complaints. This paper presents the implementation of the classification system that combines the application of Complaint Concept Ontologies in Malay language as classifier rules with the BM25 model of Information Retrieval system. Experiments showed the semantic based elements such as Malay ontology may bring the improvement of the classification of the Malay Complaint. The result yielded showed that the proposed classifier produced better result in four category compared to BM25 original score that only produced better result in one category. OBMCS also outperformed the LDA model in all eight categories on the Recall, Precision and F-measure metrics. The finding proven the proposed system is very useful, especially to the Malay complaint in regards of classification for documents in the domain area.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {481–486},
numpages = {6},
keywords = {Malay complaint, bm25 model, ontology based classification, semantic classification},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@inproceedings{10.1145/3520084.3520102,
author = {Tramontana, Emiliano and Verga, Gabriella},
title = {Ontology Enrichment with Text Extracted from Wikipedia},
year = {2022},
isbn = {9781450395519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520084.3520102},
doi = {10.1145/3520084.3520102},
abstract = {As biobanks require storing a large amount of data, the use of ontologies offer an effective solution to properly organise data and for data management. However, the specialised jargon embedded into an ontology, especially in the biomedical field, may constitute a difficulty for the people outside the proper domain. Our solution to this is to enhance ontology usability by automatically enriching an ontology. In this article we illustrate an enrichment process that allows us to expand in a controlled way the terms within an ontology. Our proposed enrichment process automatically adds in the ontological structure information extracted from external sources, in order to offer a more complete and clear knowledge of the domain and let users more easily query the ontology. Our enrichment process carefully selects from the wealth of information available in Wikipedia.},
booktitle = {Proceedings of the 2022 5th International Conference on Software Engineering and Information Management},
pages = {113–117},
numpages = {5},
keywords = {Additional Key Words and Phrases: Ontologies, Biobanks, OBIB, Ontology enrichment, Wikipedia},
location = {Yokohama, Japan},
series = {ICSIM '22}
}

@article{10.1145/3744558,
author = {Gang, Liu and Wenli, Yang and Tongli, Wang and Zhihao, He},
title = {Corpus Fusion and Text Summarization Extraction for Multi-Feature Enhanced Entity Alignment},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {9},
issn = {2375-4699},
url = {https://doi.org/10.1145/3744558},
doi = {10.1145/3744558},
abstract = {Cross-lingual entity alignment endeavors to identify semantically similar entities within a knowledge graph, facilitating knowledge complementarity and enriching cross-lingual knowledge. In the context of knowledge-driven tasks such as cross-lingual question answering and knowledge recommendation, cross-lingual entity alignment can effectively enhancing the performance of these applications built upon cross-lingual knowledge graphs. However, the current methodologies exhibit constraints in efficiently extracting and combining features of multiple entities, rendering them unable to fully harness the wealth of extensive information provided by the knowledge graph. To address this challenge, we propose CFSE, a novel multi-feature enhanced fusion model, which includes deep extraction of complex entity relationship, name, and attribute features. Complex entity relationship features are extracted based on corpus fusion and RotatE model. Additionally, an algorithm based on BERT for multilingual text summarization was introduced to extract entity name and attribute features. Through comprehensive entity feature extraction, CFSE not only further improves the alignment accuracy, but also helps to maximize the depth mining of knowledge graph information. The effectiveness of CFSE in cross-lingual entity alignment applications was demonstrated through experimental results on the DBP15K dataset.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = sep,
articleno = {89},
numpages = {15},
keywords = {Cross-lingual entity alignment, rotate, BERT, feature fusion, multilingual text summarization extraction}
}

@inbook{10.1145/3718491.3718581,
author = {Chen, Hongyv and He, Juan},
title = {Exploring the diverse interests of tourism participants: an analysis based on Topic model and BiLSTM-CRF},
year = {2025},
isbn = {9798400710865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718491.3718581},
abstract = {The rapid development of social network and new technology has led to great changes in tourism. Natural language processing (NLP) has advantages in obtaining information about the vast amount of content generated by online users about travel services and products. In this paper, Gansu province is taken as a sample to establish a corpus knowledge base combining user generated content (UGC) and literature specialty. And a variety of knowledge aggregation methods, such as unsupervised machine learning and knowledge graph methods, are used to effectively organize the massive data in the corpus. Finally, this paper analyzes destination image, tourist preference and tourist behavior patterns. This method effectively addresses the needs of tourism visitors and organization information retrieval. Make some contributions to destination management and tourist behavior recognition.},
booktitle = {Proceedings of the 4th Asia-Pacific Artificial Intelligence and Big Data Forum},
pages = {550–555},
numpages = {6}
}

@article{10.1145/3677376,
author = {Zou, Jie and Sun, Aixin and Long, Cheng and Kanoulas, Evangelos},
title = {Knowledge-Enhanced Conversational Recommendation via Transformer-Based Sequential Modeling},
year = {2024},
issue_date = {November 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {6},
issn = {1046-8188},
url = {https://doi.org/10.1145/3677376},
doi = {10.1145/3677376},
abstract = {In conversational recommender systems (CRSs), conversations usually involve a set of items and item-related entities or attributes, e.g., director is a related entity of a movie. These items and item-related entities are often mentioned along the development of a dialog, leading to potential sequential dependencies among them. However, most of existing CRSs neglect these potential sequential dependencies. In this article, we first propose a Transformer-based sequential conversational recommendation method, named TSCR, to model the sequential dependencies in the conversations to improve CRS. In TSCR, we represent conversations by items and the item-related entities, and construct user sequences to discover user preferences by considering both the mentioned items and item-related entities. Based on the constructed sequences, we deploy a Cloze task to predict the recommended items along a sequence. Meanwhile, in certain domains, knowledge graphs formed by the items and their related entities are readily available, which provide various different kinds of associations among them. Given that TSCR does not benefit from such knowledge graphs, we then propose a knowledge graph enhanced version of TSCR, called TSCRKG. In specific, we leverage the knowledge graph to offline initialize our model TSCRKG, and augment the user sequence of conversations (i.e., sequence of the mentioned items and item-related entities in the conversation) with multi-hop paths in the knowledge graph. Experimental results demonstrate that our TSCR model significantly outperforms state-of-the-art baselines, and the enhanced version TSCRKG further improves recommendation performance on top of TSCR.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {162},
numpages = {27},
keywords = {Conversational recommendation, sequential recommendation, recommender system, transformer}
}

@inproceedings{10.1145/3508546.3508644,
author = {Liu, Lijuan and Guo, Chengyu},
title = {Retrieval and Evaluation of Target Component Based on Ontology Knowledge},
year = {2022},
isbn = {9781450385053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508546.3508644},
doi = {10.1145/3508546.3508644},
abstract = {Software reuse most focuses on component based software development (CBSD). However, it's not so accurate and efficient in the process, to solve this problem, this paper proposes an intelligent knowledge-driven method of target component retrieval and evaluation. This method is based on Ontology component description. With the help of the knowledge graph, a semantic mapping is formed between the component to be queried and the component description library, so the entity component is located. In order to measure the component matching performance, it gives multi-angle evaluation of queried candidate components by an evaluation index system. Based on component query information, it makes the searching target clearer, extends the semantic scope of components to be queried. In order to assembly components, a multi agent system (MAS) is also established. The result shows that this method not only makes the component retrieval process higher recall and precision, but also makes the component retrieval process more intelligent by meeting the assembly requirement.},
booktitle = {Proceedings of the 2021 4th International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {98},
numpages = {6},
keywords = {MAS, Ontology, artificial intelligence, knowledge graph, software reuse},
location = {Sanya, China},
series = {ACAI '21}
}

@inproceedings{10.1145/3605763.3625244,
author = {Ellerhold, Christian and Schnagl, Johann and Schreck, Thomas},
title = {Enterprise Cyber Threat Modeling and Simulation of Loss Events for Cyber Risk Quantification},
year = {2023},
isbn = {9798400702594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3605763.3625244},
doi = {10.1145/3605763.3625244},
abstract = {In today's enterprise landscape, effective risk management has emerged as a vital cornerstone. This importance has escalated significantly due to the widespread transition from traditional on-premise infrastructures to dynamic cloud environments. Many organizations rely on qualitative approaches for internal IT and cyber risk management; however, these approaches have notable drawbacks, such as a lack of accuracy and comparability. In this paper, we propose a novel approach to address these limitations by using the Factor Analysis of Information Risk (FAIR) methodology in conjunction with MITRE ATT&amp;CK to model realistic cyberattacks on organizations and measure quantitative risk. We describe how this approach can be used to create an enterprise cyber threat model, providing a case study for a cloud scenario to demonstrate its usage and to illustrate its potential benefits.  Our model has demonstrated its practical applicability in enterprise settings as we thoroughly evaluated its effectiveness within two prominent German companies. This allowed us to gain valuable insight into how our proposed approach can enhance an organization's risk management strategies. Our research demonstrates the value of using a quantitative approach like FAIR over qualitative risk assessment methods. Overall, our approach provides a more comprehensive understanding of the risks organizations are facing and offers guidance on implementing effective risk management strategies. This research can help organizations improve their risk management practices and reduce the potential negative impact of cyberattacks.},
booktitle = {Proceedings of the 2023 on Cloud Computing Security Workshop},
pages = {17–29},
numpages = {13},
keywords = {cloud computing, cyber risk quantification, enterprise threat model, factor analysis of information risk (fair), mitre att&amp;ck, quantitative risk assessment, unified kill chain},
location = {Copenhagen, Denmark},
series = {CCSW '23}
}

@inproceedings{10.1145/3493700.3493703,
author = {Uceda-Sosa, Rosario and Mihindukulasooriya, Nandana and Kumar, Atul and Bansal, Sahil and Nagar, Seema},
title = {Domain specific ontologies from Linked Open Data (LOD)},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493703},
doi = {10.1145/3493700.3493703},
abstract = {Logical and probabilistic reasoning tasks that require a deeper knowledge of semantics are increasingly relying on general purpose ontologies such as Wikidata and DBpedia. However, tasks such as entity disambiguation and linking may benefit from domain-specific knowledge graphs, which make it more efficient to consume the knowledge and easier to extend with proprietary content. We discuss our experience bootstrapping one such ontology for IT with a domain-agnostic pipeline, and extending it using domain-specific glossaries.},
booktitle = {Proceedings of the 5th Joint International Conference on Data Science \&amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {105–109},
numpages = {5},
keywords = {IT Operations, Knowledge Graphs, Ontologies},
location = {Bangalore, India},
series = {CODS-COMAD '22}
}

@inproceedings{10.1145/3466933.3466951,
author = {Aguiar, Camila Zacch\'{e} de and Zanetti, F\'{e}lix and Souza, Vitor E. Silva},
title = {Source Code Interoperability based on Ontology},
year = {2021},
isbn = {9781450384919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466933.3466951},
doi = {10.1145/3466933.3466951},
abstract = {The different ways of representing a source code in different programming languages create a heterogeneous context. In addition, the use of multiple programming languages in a single source code (polyglot programming) brings a wide choice of terms from different languages, libraries and structures. These facts prevent the direct exchange of information between source codes of different programming languages, requiring specialized knowledge of the programming languages involved. In this article, we present an ontology-based method for source code interoperability that provides an alternative to mitigate heterogeneity problems, aiming to semantically represent the source code written in different programming languages and apply it from different perspectives in a unified way. In this sense, the method is applied in a lab experiment with the objective of validating its methodological aspects, instantiating their respective phases in different subdomains (object orientation and object/relational mapping) and programming languages (Java and Python) in the code smells detection perspective. In addition, the code smell detector produced is evaluated with a set of real-world software projects written in Java and Python.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Information Systems},
articleno = {18},
numpages = {8},
keywords = {applied ontology, code smell detection, interoperabity, ontology, source code},
location = {Uberl\^{a}ndia, Brazil},
series = {SBSI '21}
}

@inproceedings{10.1145/3460866.3461772,
author = {C\'{e}rin, Christophe and Andres, Fr\'{e}d\'{e}ric and Geldwerth-Feniger, Danielle},
title = {Towards an emulation tool based on ontologies and data life cycles for studying smart buildings},
year = {2021},
isbn = {9781450384650},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460866.3461772},
doi = {10.1145/3460866.3461772},
abstract = {In this paper, we share our vision to study a complex Information Technology (IT) system handling a massive amount of data in the context of 'smart buildings.' One technique for analyzing complex IT systems relies on emulation, where the final software system is fully deployed on real architectures, and is evaluated in considering "small" instances of situations the system is supposed to solve. We propose a software architecture for studying the ecosystem of 'smart buildings'. This software architecture is built: 1) on top of ontologies for the description of smart buildings; 2) on a special tool for mastering the life cycle of data produced by sensors and actuators inside the buildings.We assume that it is equally important to model both the building's components and the flow of data produced inside the building. We use existing software components for both goals and to make real our concerns. According to a translational methodology, we also discuss use cases for illustrating the potential of our approach and the particular challenges associated with making the two main components of our emulation tool inter-operate.Therefore, our main contribution is to propose a comprehensive, ambitious and realistic research plan to guide communities. The paper illustrates how computer scientists and smart buildings domain scientists may communicate to address and solve specific research problems related to Big Data in emergent distributed environments. We are also guessing that experimental results that can demonstrate the practicality of the proposed combination of tools could be devised in the future, based on our broad vision. The paper is, first and foremost, a visionary paper.},
booktitle = {Proceedings of the International Workshop on Big Data in Emergent Distributed Environments},
articleno = {8},
numpages = {15},
keywords = {big data tools, data life cycle, emulation principles, ontology, smart buildings, systems and methods},
location = {Virtual Event, China},
series = {BiDEDE '21}
}

@inproceedings{10.1145/3425329.3425377,
author = {Fei, Huang and Youling, Chen and Dongsheng, Xu},
title = {Formal Description of Manufacturing Process based on Domain Ontology Construction},
year = {2020},
isbn = {9781450387873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425329.3425377},
doi = {10.1145/3425329.3425377},
abstract = {In order to solve the problem of process knowledge sharing, integration and reuse in the field of machinery manufacturing due to the complexity, dispersion and diversity of process knowledge. Taking into account the advantages of ontology in knowledge representation, this paper proposes an ontology-based knowledge management framework in the production line. On the basis of inductively analyzing the attributes of the mechanical manufacturing process attributes and intra-process and interprocess relationships, an improved conceptual ontology expression model of the 4-tuple process is proposed.},
booktitle = {Proceedings of the 2nd World Symposium on Software Engineering},
pages = {246–251},
numpages = {6},
keywords = {Process knowledge, domain ontology, ontology modeling, semantic analysis},
location = {Chengdu, China},
series = {WSSE '20}
}

@inproceedings{10.1145/3652620.3688213,
author = {Atkinson, Colin and K\"{u}hne, Thomas and Lange, Arne},
title = {Misconceptions about Potency-Based Deep Instantiation},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688213},
doi = {10.1145/3652620.3688213},
abstract = {Multi-level modeling languages differ in their approaches for controlling the properties of model elements over multiple modeling levels. Over the years the original approach for deeply characterizing model elements, the potency-based deep instantiation mechanism, has received a number of criticisms related to its flexibility, level stability, ontological soundness, type safety, and ability to reduce accidental complexity. However, some of these criticisms are founded on misconceptions and thus cannot usefully inform multi-level modeling language designs. In this paper we identify and clarify these misconceptions in order to help guide future considerations of language feature trade-offs and design choices.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {810–817},
numpages = {8},
keywords = {multi-level modeling, deep instantiation, ontologies, accidental complexity, type safety, potency},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@proceedings{10.1145/3584684,
title = {ApPLIED 2023: Proceedings of the 5th workshop on Advanced tools, programming languages, and PLatforms for Implementing and Evaluating algorithms for Distributed systems},
year = {2023},
isbn = {9798400701283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Orlando, FL, USA}
}

@article{10.1145/3756009,
author = {Cheng, Qing and Zeng, Zefan and Hu, Xingchen and Si, Yuehang and Liu, Zhong},
title = {A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects},
year = {2025},
issue_date = {February 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3756009},
doi = {10.1145/3756009},
abstract = {Event Causality Identification (ECI) has become an essential task in Natural Language Processing (NLP), focused on automatically detecting causal relationships between events within texts. This comprehensive survey systematically investigates fundamental concepts and models, developing a systematic taxonomy and critically evaluating diverse models. We begin by defining core concepts, formalizing the ECI problem, and outlining standard evaluation protocols. Our classification framework divides ECI models into two primary tasks: Sentence-level Event Causality Identification (SECI) and Document-level Event Causality Identification (DECI). For SECI, we review models employing feature pattern-based matching, machine learning classifiers, deep semantic encoding, prompt-based fine-tuning, and causal knowledge pre-training, alongside data augmentation strategies. For DECI, we focus on approaches utilizing deep semantic encoding, event graph reasoning, and prompt-based fine-tuning. Special attention is given to recent advancements in multi-lingual and cross-lingual ECI, as well as zero-shot ECI leveraging Large Language Models (LLMs). We analyze the strengths, limitations, and unresolved challenges associated with each approach. Extensive quantitative evaluations are conducted on four benchmark datasets to rigorously assess the performance of various ECI models. We conclude by discussing future research directions and highlighting opportunities to advance the field further.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {59},
numpages = {37},
keywords = {Natural language processing, event, causality, information extraction, representation learning, knowledge reasoning}
}

@inproceedings{10.1145/3630138.3630488,
author = {Yin, Xuefeng and Luo, Jianfei},
title = {The Statistic Study of Thai Middle School Students' Acquisition in Chinese Negative Structures},
year = {2024},
isbn = {9781450399951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630138.3630488},
doi = {10.1145/3630138.3630488},
abstract = {Among the large amounts of Chinese negatives, bu and mei are the most frequently-used with complicated syntactic function and differences on the aspects of semantics and pragmatics becoming the key and hard one in Chinese teaching. However, in Thailand language, the negative ไม่ is a quite simple syntactic structure representing almost all negative usage. This paper based on the former theoretical study takes two grades 72 students as the object of the study, making relative 20 questions in 10 syntactic conditions, conducting the operation of the average on the subjective effects and in 10 syntactic categories. Relative cross-contrast data analysis on the average accuracy between every two types of syntactic conditions included as well. Statistics show that students in Thai mother tongue always make biased errors about bu and mei for lacking enough correspondence in their mother tongue. According to the pairwise comparison inside of the category of simple and complicated syntactic condition, we observe that there contains a difficult level of acquisition about bu and mei from easy to difficult which is fixed collocation &lt; simple syntactic condition &lt; volitive and mental verbs &lt; complicated syntactic condition.},
booktitle = {Proceedings of the 2023 International Conference on Power, Communication, Computing and Networking Technologies},
articleno = {70},
numpages = {8},
location = {Wuhan, China},
series = {PCCNT '23}
}

@inproceedings{10.1145/3436756.3437037,
author = {Basse, Adrien and Diatta, Baboucar and Deme, Cherif Bachir and Ndiaye, Ndeye Massata},
title = {Ontology-Based Framework For Automatic Generation Of SQL Assessment},
year = {2021},
isbn = {9781450388276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436756.3437037},
doi = {10.1145/3436756.3437037},
abstract = {Assessment measures learner progresses in acquisition of knowledge and skills. It helps teachers to refine their teaching strategies and better deliver knowledge and skills. Likewise, It makes it possible not only to simulate learners’ thinking but to measure the value or quality of their work. Creating effective assessment of learning is a difficult task for teachers. This article presents an automatic question generating system for individual assessment of learners’ practical knowledge.},
booktitle = {Proceedings of the 12th International Conference on Education Technology and Computers},
pages = {152–156},
numpages = {5},
keywords = {SQL language, assessment, ontology, question generation},
location = {London, United Kingdom},
series = {ICETC '20}
}

@inproceedings{10.1145/3659677.3659992,
author = {Saada, Hajer and Orizio, Riccardo and Sebastio, Stefano},
title = {Modeling and Conducting Security Risk Assessment of Smart Airport Infrastructures with SecRAM},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659677.3659992},
doi = {10.1145/3659677.3659992},
abstract = {Despite the COVID-19 pandemic caused flights and routes cut back inverting an otherwise astonishing growth trend, air passenger traffic is increasing every year. As a result, airports are going through a continuous digital transformation enhancing their infrastructure to keep up their growth and to offer passengers an improved and seamless experience. On the other hand, a far-reaching digital infrastructure poses new cyber-security challenges. In response to such a novel and connected ecosystem adopted by airports, this paper presents a security risk assessment to identify and mitigate the consequences of attacks threatening the digital passenger process of smart airports. Also, we propose an extension of SecRAM, a risk assessment methodology proposed by the SESAR Joint Undertaking for the Air Traffic Management (ATM), to enable a continuous assessment as the threat landscape evolves.},
booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
articleno = {59},
numpages = {7},
keywords = {Model-Based System Engineering, SecRAM, Security Risk Assessment, Smart Airports},
location = {Meknes, AA, Morocco},
series = {NISS '24}
}

@article{10.1145/3487919,
author = {Pfannem\"{u}ller, Martin and Breitbach, Martin and Weckesser, Markus and Becker, Christian and Schmerl, Bradley and Sch\"{u}rr, Andy and Krupitzer, Christian},
title = {REACT-ION: A Model-based Runtime Environment for Situation-aware Adaptations},
year = {2021},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {4},
issn = {1556-4665},
url = {https://doi.org/10.1145/3487919},
doi = {10.1145/3487919},
abstract = {Trends such as the Internet of Things lead to a growing number of networked devices and to a variety of communication systems. Adding self-adaptive capabilities to these communication systems is one approach to reducing administrative effort and coping with changing execution contexts. Existing frameworks can help reducing development effort but are neither tailored toward the use in communication systems nor easily usable without knowledge in self-adaptive systems development. Accordingly, in previous work, we proposed REACT, a reusable, model-based runtime environment to complement communication systems with adaptive behavior. REACT addresses heterogeneity and distribution aspects of such systems and reduces development effort. In this article, we propose REACT-ION—an extension of REACT for situation awareness. REACT-ION offers a context management module that is able to acquire, store, disseminate, and reason on context data. The context management module is the basis for (i) proactive adaptation with REACT-ION and (ii) self-improvement of the underlying feedback loop. REACT-ION can be used to optimize adaptation decisions at runtime based on the current situation. Therefore, it can cope with uncertainty and situations that were not foreseeable at design time. We show and evaluate in two case studies how REACT-ION’s situation awareness enables proactive adaptation and self-improvement.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = dec,
articleno = {12},
numpages = {29},
keywords = {Self-adaptive systems, model-based, runtime environment, framework, situation awareness}
}

@inproceedings{10.1145/3463677.3463730,
author = {Avgerinos Loutsaris, Michalis and Lachana, Zoi and Alexopoulos, Charalampos and Charalabidis, Yannis},
title = {Legal Text Processing: Combing two legal ontological approaches through text mining},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463730},
doi = {10.1145/3463677.3463730},
abstract = {The globalization of communication networks and the possibilities offered by the information and communication technologies (ICTs) significantly change the public sector's operation and services. Digital Governance is now integrated into administrations' policies and programs at all levels: local, regional, national, European. At the national level, there is a requirement to provide electronic public services according to citizens' needs while, in the sense of globalization, at the European level, there are many programs (e.g., the Europe 2005 and i2010 program) emphasizing the Digital Governance world (or better Digital Governance community) that indicates rapid changes not only in the sense of the change in the public sector's systems but also in the mentality that the public sector operates. On the other hand, Digital Governance's evolution affects societies intensively, emphasizing the importance of cross-border interaction and information sharing between them. [6]. Concerning the legal informatics domain, this can result in changing governments' operations in many ways [2]. By now, the massive amount of each country's legal information currently remains fragmented across multiple national databases and systems or even better legal databases. Most of these legal databases result from the significant advancements in the “legal informatics” research field that observed since governments have started to promote the development of legal information systems [9]. This research contributes to this purpose by developing an open and automated legal system capable of providing any EU country's legal information based on the existing ontologies.},
booktitle = {Proceedings of the 22nd Annual International Conference on Digital Government Research},
pages = {522–532},
numpages = {11},
keywords = {big open legal data, legal information systems, legal ontologies},
location = {Omaha, NE, USA},
series = {dg.o '21}
}

@inproceedings{10.1145/3718751.3718912,
author = {Fang, Yuxuan and Hu, Xiaochen and Chen, Shangyin},
title = {Research on the influencing factors of creation model of knowledge graphs for design under the perspective of graph workflow},
year = {2025},
isbn = {9798400709753},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3718751.3718912},
doi = {10.1145/3718751.3718912},
abstract = {Based on the analysis of SAPAD-AHP model, this study explores the influencing factors of graph structure in the construction of knowledge graph creation platform, and through the construction of the collaborative platform, the study is carried out in teaching and research practice. With the development of the interdisciplinary system and the establishment of many emerging disciplines, design as the intersection of science, engineering and liberal arts combined, its information grooming and knowledge production mode has also undergone an important transformation, and the processing capacity in the face of a variety of information intersection has greatly stimulated the demand for the construction of personal knowledge bases and knowledge graphs. The current solutions relying on tree structure and linear structure have considerable limitations. In this paper, we analyze the mapping of "Behavior-Product-Meaning" of the existing solutions through SAPAD method, and obtain the user's needs through cluster analysis, and filter the multi-layer core meaning clusters by calculating the weight of the general meaning clusters through the AHP method, so as to obtain the user's core needs. Based on the results of the requirements study, the study proposes a design scheme for personal knowledge graph construction based on graph structure, which for the first time connects graph structure and text content in tandem at the interaction aspect, integrates and simplifies the workflow, and examines its information combing and image editing capabilities in teaching and research practice, providing a valuable reference for the design of knowledge graph solutions for the design category.},
booktitle = {Proceedings of the 2024 4th International Conference on Big Data, Artificial Intelligence and Risk Management},
pages = {971–984},
numpages = {14},
keywords = {collaborative platform, graph workflow, knowledge graph},
location = {
},
series = {ICBAR '24}
}

@inproceedings{10.1145/3711542.3711550,
author = {Druselmann, Maria and Harbusch, Karin},
title = {A Dataset of Semantically Related Multiword Terms of the Electrical Engineering Domain},
year = {2025},
isbn = {9798400717383},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711542.3711550},
doi = {10.1145/3711542.3711550},
abstract = {This paper presents the EEMWT dataset, a collection of 1873 triplets of co-hyponymic multiword terms of the electrical engineering domain. Each triplet combines an anchor term with closely and distantly related co-hyponymic terms. The degree of semantic relatedness is determined by the presence or absence of shared domain-specific semantic features. The primary purpose of this dataset is to serve as a tool for evaluating domain-specific language representation models and embedding vector pooling techniques by assessing the semantic relatedness of co-hyponymic multiword terms. The novelty of the dataset lies in the development approach, which replaces intuition-based scaling of relatedness with linguistically justified semantic-feature-based judgment. Furthermore, the traditional method of calculating interrater agreement rate is replaced with a statistical analysis of vector space distance between low and highly related terms. These innovations in dataset construction and evaluation have the potential to significantly reduce the costs associated with expert questionnaires in dataset development.},
booktitle = {Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval},
pages = {258–264},
numpages = {7},
keywords = {dataset of semantically related terms, domain-specific language representation model, multiword term, semantic relatedness},
location = {
},
series = {NLPIR '24}
}

@inproceedings{10.5555/3398761.3398925,
author = {van den Berg, Line and Atencia, Manuel and Euzenat, J\`{e}rome},
title = {Agent Ontology Alignment Repair through Dynamic Epistemic Logic},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Ontology alignments enable agents to communicate while preserving heterogeneity in their information. Alignments may not be provided as input and should be able to evolve when communication fails or when new information contradicting the alignment is acquired. In the Alignment Repair Game (ARG) this evolution is achieved via adaptation operators. ARG was evaluated experimentally and the experiments showed that agents converge towards successful communication and improve their alignments. However, whether the adaptation operators are formally correct, complete or redundant is still an open question. In this paper, we introduce a formal framework based on Dynamic Epistemic Logic that allows us to answer this question. This framework allows us (1) to express the ontologies and alignments used, (2) to model the ARG adaptation operators through announcements and conservative upgrades and (3) to formally establish the correctness, partial redundancy and incompleteness of the adaptation operators in ARG.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1422–1430},
numpages = {9},
keywords = {agent communication, alignment repair, dynamic epistemic logic, ontology alignment},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@article{10.1145/3665252.3665262,
author = {Doan, AnHai},
title = {Technical Perspective: Unicorn: A Unified Multi-Tasking Matching Model},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/3665252.3665262},
doi = {10.1145/3665252.3665262},
abstract = {Data integration has been a long-standing challenge for data management. It has recently received significant attention due to at least three main reasons. First, many data science projects require integrating data from disparate sources before analysis can be carried out to extract insights. Second, many organizations want to build knowledge graphs, such as Customer 360s, Product 360s, and Supplier 360s, which capture all available information about the customers, products, and suppliers of an organization. Building such knowledge graphs often requires integrating data from multiple sources. Finally, there is also an increasing need to integrate a massive amount of data to create training data for AI models, such as large language models.},
journal = {SIGMOD Rec.},
month = may,
pages = {43},
numpages = {1}
}

@inproceedings{10.1145/3184558.3191654,
author = {Dimanidis, Anastasios and Chatzidimitriou, Kyriakos C. and Symeonidis, Andreas L.},
title = {A Natural Language Driven Approach for Automated Web API Development: Gherkin2OAS},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191654},
doi = {10.1145/3184558.3191654},
abstract = {Speeding up the development process of Web Services, while adhering to high quality software standards is a typical requirement in the software industry. This is why industry specialists usually suggest "driven by" development approaches to tackle this problem. In this paper, we propose such a methodology that employs Specification Driven Development and Behavior Driven Development in order to facilitate the phases of Web Service requirements elicitation and specification. Furthermore, we introduce gherkin2OAS, a software tool that aspires to bridge the aforementioned development approaches. Through the suggested methodology and tool, one may design and build RESTful services fast, while ensuring proper functionality.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1869–1874},
numpages = {6},
keywords = {behavior driven development, gherkin, open API specification, restful API},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3613372.3613378,
author = {Dalcin, Guilherme and Bolzan, Willian and Lazzari, Luan and Farias, Kleinner},
title = {Recommendation of UML Model Conflicts: Unveiling the Biometric Lens for Conflict Resolution},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3613378},
doi = {10.1145/3613372.3613378},
abstract = {Model merging assumes a pivotal role in numerous model-centric software development tasks, e.g., evolving UML models to add new features or even reconciling UML models developed collaboratively by distributed development teams. Usually, UML model elements to-be-merged conflict with each other. Unfortunately, resolving conflicts remains a highly cognitive and error-prone task. Today, wearable devices capable of capturing biometric data are a reality. Recent studies indicate that the developer’s cognitive indicators may affect developers while performing development tasks. However, the current literature has neglected the recommendation of conflicts sensitive to the cognitive activities of software developers. This study, therefore, introduces BACR, a biometric-aware approach to recommend UML model conflicts using machine learning. BACR helps UML model merging to push a step forward, recommending model conflicts based on appropriate biometric indicators and using a behavior sequence transformer model. Our approach is based on four scientific institutions. It represents the first effort in supporting the prioritization of cognitively relevant UML model conflicts by developers, mitigating the risk of making incorrect decisions and preventing potential downstream issues.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {83–88},
numpages = {6},
keywords = {Biometrics, Cognitive Load, Model Merging, Software Modeling},
location = {Campo Grande, Brazil},
series = {SBES '23}
}

@inproceedings{10.1145/3442442.3451388,
author = {Tian, Ke and Chen, Hua},
title = {aiai at the FinSim-2 task: Finance Domain Terms Automatic Classification Via Word Ontology and Embedding},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451388},
doi = {10.1145/3442442.3451388},
abstract = {This paper describes the method that we submitted to the FinSim-2 task on learning similarities for the financial domain. This task aims to automatically classify the Financial domain terms into the most relevant hypernym (or top-level) concept in an external ontology. This paper shows the result of experiments using the Catboost, Attention-LSTM, BERT, RoBERTa to develop an automatic finance domain classifier via word ontology and embedding. The experiment result demonstrates that each model could be an effective method to tackle the FinSim-2 task, respectively.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {320–322},
numpages = {3},
keywords = {Attention, BERT, Catboost, FinSim-2 task, LSTM, Ontology, RoBERTa, Word2vec},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@article{10.14778/3583140.3583148,
author = {F\"{u}rst, Jonathan and Argerich, Mauricio Fadel and Cheng, Bin},
title = {VersaMatch: Ontology Matching with Weak Supervision},
year = {2023},
issue_date = {February 2023},
publisher = {VLDB Endowment},
volume = {16},
number = {6},
issn = {2150-8097},
url = {https://doi.org/10.14778/3583140.3583148},
doi = {10.14778/3583140.3583148},
abstract = {Ontology matching is crucial to data integration for across-silo data sharing and has been mainly addressed with heuristic and machine learning (ML) methods. While heuristic methods are often inflexible and hard to extend to new domains, ML methods rely on substantial and hard to obtain amounts of labeled training data. To overcome these limitations, we propose VersaMatch, a flexible, weakly-supervised ontology matching system. VersaMatch employs various weak supervision sources, such as heuristic rules, pattern matching, and external knowledge bases, to produce labels from a large amount of unlabeled data for training a discriminative ML model. For prediction, VersaMatch develops a novel ensemble model combining the weak supervision sources with the discriminative model to support generalization while retaining a high precision. Our ensemble method boosts end model performance by 4 points compared to a traditional weak-supervision baseline. In addition, compared to state-of-the-art ontology matchers, VersaMatch achieves an overall 4-point performance improvement in F1 score across 26 ontology combinations from different domains. For recently released, in-the-wild datasets, VersaMatch beats the next best matchers by 9 points in F1. Furthermore, its core weak-supervision logic can easily be improved by adding more knowledge sources and collecting more unlabeled data for training.},
journal = {Proc. VLDB Endow.},
month = feb,
pages = {1305–1318},
numpages = {14}
}

@inproceedings{10.1145/3643834.3661498,
author = {Sivertsen, Christian and L\o{}vlie, Anders Sundnes},
title = {Exploring Aesthetic Qualities of Deep Generative Models through Technological (Art) Mediation},
year = {2024},
isbn = {9798400705830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643834.3661498},
doi = {10.1145/3643834.3661498},
abstract = {Deep Generative Models (DGM) have had a great impact both on visual art and broader visual culture. In this research-through-design project we investigate the use of a DGM for helping museum visitors explore the aesthetics of Edvard Munch’s art. We designed and built an interactive drawing table that allows a user to explore a StyleGAN model trained on sketches by Edvard Munch. The paper makes two novel contributions: 1. It presents a system that allows users to interact with a DGM by drawing on paper (rather than the typical text prompts used by most current systems). 2. We demonstrate how this mode and quality of interaction establish a unique perspective on Munch’s drawings as a practice. Through qualitative evaluation, we discuss how this setup led users towards a specific hermeneutic drawing strategy that enables building competency with the model and by proxy the data it is trained on. We suggest that the resulting interaction may contribute to an "education of attention" helping museum visitors to become attentive to certain visual qualities in Munch’s drawing practice. Finally, we discuss how the concepts of technological mediation and relationality are useful for designing how the output of a DGM is understood by its users.},
booktitle = {Proceedings of the 2024 ACM Designing Interactive Systems Conference},
pages = {2738–2752},
numpages = {15},
keywords = {aesthetics, deep generative model, drawing, fine art, interaction design, machine learning, postphenomenology, stylegan},
location = {Copenhagen, Denmark},
series = {DIS '24}
}

@inproceedings{10.1145/3600211.3604702,
author = {Rismani, Shalaleh and Moon, AJung},
title = {What does it mean to be a responsible AI practitioner: An ontology of roles and skills},
year = {2023},
isbn = {9798400702310},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600211.3604702},
doi = {10.1145/3600211.3604702},
abstract = {With the growing need to regulate AI systems across a wide variety of application domains, a new set of occupations has emerged in the industry. The so-called responsible Artificial Intelligence (AI) practitioners or AI ethicists are generally tasked with interpreting and operationalizing best practices for ethical and safe design of AI systems. Due to the nascent nature of these roles, however, it is unclear to future employers and aspiring AI ethicists what specific function these roles serve and what skills are necessary to serve the functions. Without clarity on these, we cannot train future AI ethicists with meaningful learning objectives. In this work, we examine what responsible AI practitioners do in the industry and what skills they employ on the job. We propose an ontology of existing roles alongside skills and competencies that serve each role. We created this ontology by examining the job postings for such roles over a two-year period (2020-2022) and conducting expert interviews with fourteen individuals who currently hold such a role in the industry. Our ontology contributes to business leaders looking to build responsible AI teams and provides educators with a set of competencies that an AI ethics curriculum can prioritize.},
booktitle = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {584–595},
numpages = {12},
keywords = {Competency Framework, Education, Responsible AI Practitioner},
location = {Montr\'{e}al, QC, Canada},
series = {AIES '23}
}

@inproceedings{10.1145/3494322.3494338,
author = {Hviid, Jakob and Johansen, Aslak and Caleb Sangogboye, Fisayo and Kj\ae{}rgaard, Mikkel Baun},
title = {OPM: An Ontology-Based Package Manager for Building Operating Systems},
year = {2022},
isbn = {9781450385664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494322.3494338},
doi = {10.1145/3494322.3494338},
abstract = {The energy sector is experiencing new challenges with the move to green energy. One of these challenges is keeping a stable energy grid when transitioning the production to unpredictable energy generation from green sources. Demand Response (DR) can mitigate some of the lack of predictability by influencing the consumer’s load profile. Unfortunately, the cost of implementing DR, and the required infrastructure, vastly overshadows the benefits for the consumer, thereby negating the incentive to invest. Therefore, reducing the initial cost of investment is a critical factor for the success of DR. Building Operating Systems (BOS) is one possible avenue to achieve DR functionality in buildings. This paper seeks to reduce initial investment costs of BOSes, by introducing an ontology-based package manager (OPM), that dynamically resolves dependencies and installs services. An ontology-based approach to dependency resolution allows for loosely defined dependencies but also takes the context of the service into account, as well as requirements in terms of sensor availability and physical layout of the building. The OPM is evaluated by deploying a BOS and accompanying services for occupancy prediction. By significantly reducing deployment complexity, results show considerable time savings, and thereby cost reductions, on deployment and maintenance activities.},
booktitle = {Proceedings of the 11th International Conference on the Internet of Things},
pages = {118–125},
numpages = {8},
keywords = {OWL, Package manager, building operating systems, containerization, dependency resolution, deployment, ontology},
location = {St.Gallen, Switzerland},
series = {IoT '21}
}

@inproceedings{10.1145/3583780.3615051,
author = {Wang, Mengying and Guan, Sheng and Ma, Hanchao and Bian, Yiyang and Che, Haolai and Daundkar, Abhishek and Sehirlioglu, Alp and Wu, Yinghui},
title = {Selecting Top-k Data Science Models by Example Dataset},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615051},
doi = {10.1145/3583780.3615051},
abstract = {Data analytical pipelines routinely involve various domain-specific data science models. Such models require expensive manual or training effort and often incur expensive validation costs (e.g., via scientific simulation analysis). Meanwhile, high-value models remain to be ad-hocly created, isolated, and underutilized for a broad community. Searching and accessing proper models for data analysis pipelines is desirable yet challenging for users without domain knowledge. This paper introduces ModsNet, a novel MODel SelectioN framework that only requires an Example daTaset. (1) We investigate the following problem: Given a library of pre-trained models, a limited amount of historical observations of their performance, and an "example" dataset as a query, return k models that are expected to perform the best over the query dataset. (2) We formulate a regression problem and introduce a knowledge-enhanced framework using a model-data interaction graph. Unlike traditional methods, (1) ModsNet uses a dynamic, cost-bounded "probe-and-select" strategy to incrementally identify promising pre-trained models in a strict cold-start scenario (when a new dataset without any interaction with existing models is given). (2) To reduce the learning cost, we develop a clustering-based sparsification strategy to prune unpromising models and their interactions. (3) We showcase of ModsNet built on top of a crowdsourced materials knowledge base platform. Our experiments verified its effectiveness, efficiency, and applications over real-world analytical pipelines.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {2686–2695},
numpages = {10},
keywords = {GNN-based recommendation, knowledge graph, model selection},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3670105.3670139,
author = {Lin, Wangqun and Xu, Jing and Tian, Yu and Peng, Baoyun and Li, Yan and Ge, Yawei},
title = {Cognitive Intelligence: Driven by Knowledge Graph and Big Model Collaboration},
year = {2024},
isbn = {9798400716751},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3670105.3670139},
doi = {10.1145/3670105.3670139},
abstract = {Abstract: Cognitive intelligence is primarily characterized by the understanding, reasoning, cognition, and decision-making of complex things. It is a higher-order form of artificial intelligence development. This paper provides an in-depth analysis of two representative technologies, knowledge graph and big model, which promote the development of cognitive intelligence. Firstly, we systematically sorts out the characteristics, advantages, and shortcomings of these two technologies. Secondly, we proposes technical approaches and main methods for the mutual enhancement of knowledge graph and big model. Finally, we provides the main direction for the integrated development of knowledge graph and big model to promote the development of cognitive intelligence. We hope our work can provide reference and inspiration for relevant engineers and technical researchers.CCS Concepts: .Computing methodologies → Artificial intelligence; Knowledge representation and reasoning},
booktitle = {Proceedings of the 2024 5th International Conference on Computing, Networks and Internet of Things},
pages = {204–209},
numpages = {6},
keywords = {artificial intelligence, big model, cognitive intelligence, knowledge graph},
location = {Tokyo, Japan},
series = {CNIOT '24}
}

@inproceedings{10.1145/3503161.3548098,
author = {Wang, Xiao and Gan, Tian and Wei, Yinwei and Wu, Jianlong and Meng, Dai and Nie, Liqiang},
title = {Micro-video Tagging via Jointly Modeling Social Influence and Tag Relation},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548098},
doi = {10.1145/3503161.3548098},
abstract = {The last decade has witnessed the proliferation of micro-videos on various user-generated content platforms. According to our statistics, around 85.7\% of micro-videos lack annotation. In this paper, we focus on annotating micro-videos with tags. Existing methods mostly focus on analyzing video content, neglecting users' social influence and tag relation. Meanwhile, existing tag relation construction methods suffer from either deficient performance or low tag coverage. To jointly model social influence and tag relation, we formulate micro-video tagging as a link prediction problem in a constructed heterogeneous network. Specifically, the tag relation (represented by tag ontology) is constructed in a semi-supervised manner. Then, we combine tag relation, video-tag annotation, and user follow relation to build the network. Afterward, a better video and tag representation are derived through Behavior Spread modeling and visual and linguistic knowledge aggregation. Finally, the semantic similarity between each micro-video and all candidate tags is calculated in this video-tag network. Extensive experiments on industrial datasets of three verticals verify the superiority of our model compared with several state-of-the-art baselines.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {4478–4486},
numpages = {9},
keywords = {behavior spread, micro-video tagging, ontology construction},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3442381.3450042,
author = {Geng, Yuxia and Chen, Jiaoyan and Chen, Zhuo and Pan, Jeff Z. and Ye, Zhiquan and Yuan, Zonggang and Jia, Yantao and Chen, Huajun},
title = {OntoZSL: Ontology-enhanced Zero-shot Learning},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3450042},
doi = {10.1145/3442381.3450042},
abstract = {Zero-shot Learning (ZSL), which aims to predict for those classes that have never appeared in the training data, has arisen hot research interests. The key of implementing ZSL is to leverage the prior knowledge of classes which builds the semantic relationship between classes and enables the transfer of the learned models (e.g., features) from training classes (i.e., seen classes) to unseen classes. However, the priors adopted by the existing methods are relatively limited with incomplete semantics. In this paper, we explore richer and more competitive prior knowledge to model the inter-class relationship for ZSL via ontology-based knowledge representation and semantic embedding. Meanwhile, to address the data imbalance between seen classes and unseen classes, we developed a generative ZSL framework with Generative Adversarial Networks (GANs). Our main findings include: (i) an ontology-enhanced ZSL framework that can be applied to different domains, such as image classification (IMGC) and knowledge graph completion (KGC); (ii) a comprehensive evaluation with multiple zero-shot datasets from different domains, where our method often achieves better performance than the state-of-the-art models. In particular, on four representative ZSL baselines of IMGC, the ontology-based class semantics outperform the previous priors e.g., the word embeddings of classes by an average of 12.4 accuracy points in the standard ZSL across two example datasets (see Figure&nbsp;4).},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3325–3336},
numpages = {12},
keywords = {Generative Adversarial Networks, Image Classification, Knowledge Graph Completion, Ontology, Zero-shot Learning},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inbook{10.1145/3581906.3581911,
author = {Bilidas, Dimitris},
title = {Ontologies and Linked Data},
year = {2023},
isbn = {9798400707407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3581906.3581911},
booktitle = {Geospatial Data Science: A Hands-on Approach for Building Geospatial Applications Using Linked Data Technologies},
pages = {53–66},
numpages = {14}
}

@proceedings{10.1145/3660853,
title = {AICCONF '24: Proceedings of the Cognitive Models and Artificial Intelligence Conference},
year = {2024},
isbn = {9798400716928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {undefinedstanbul, Turkiye}
}

@inproceedings{10.1145/3404709.3404770,
author = {Mohsen, Wa'el and Aref, Mostafa and ElBahnasy, Khaled},
title = {Scaled Scrum Framework for Cooperative Domain Ontology Evolution},
year = {2020},
isbn = {9781450375337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404709.3404770},
doi = {10.1145/3404709.3404770},
abstract = {The field of research in ontology engineering appears to be mature, considering the vast number of contemporary methods and instruments for the formalization and application of knowledge representation models. However, the evolutionary aspects of ontologies are still little understood and supported. This is especially important in distributed and collaborative settings like the Semantic web, where ontologies naturally co-operate with their user communities. Various organizations and teams are building common ground in this context. Ontology is instrumental in this process through the formal description of shared knowledge. Such semanticity constitutes a sound basis for defining, sharing (business) objectives and interests and eventually developing useful collaborative services and systems. In this "complex" and dynamic environment, a collaborative model for process change requires more powerful methodologies for engineering, argumentation and negotiation. Software Engineering provides teamwork, team management, feedback management, versioning, merging, and evolving software artifacts with a wealth of techniques and tools. Many of these techniques can be used again in an ontology engineering environment. This paper examines how this problem can be resolved using Scrum and Nexus frameworks, which are among the most robust models for software development.},
booktitle = {Proceedings of the 6th International Conference on Frontiers of Educational Technologies},
pages = {135–143},
numpages = {9},
keywords = {Collaborative Evolution, Inter-organizational Ontology, Nexus, Ontology Evolution, Scrum},
location = {Tokyo, Japan},
series = {ICFET '20}
}

@article{10.1145/3429251,
author = {Joy, Jeevamol and Raj, Nisha S. and V. G., Renumol},
title = {Ontology-based E-learning Content Recommender System for Addressing the Pure Cold-start Problem},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1936-1955},
url = {https://doi.org/10.1145/3429251},
doi = {10.1145/3429251},
abstract = {E-learning recommender systems are gaining significance nowadays due to its ability to enhance the learning experience by providing tailor-made services based on learner preferences. A Personalized Learning Environment (PLE) that automatically adapts to learner characteristics such as learning styles and knowledge level can recommend appropriate learning resources that would favor the learning process and improve learning outcomes. The pure cold-start problem is a relevant issue in PLEs, which arises due to the lack of prior information about the new learner in the PLE to create appropriate recommendations. This article introduces a semantic framework based on ontology to address the pure cold-start problem in content recommenders. The ontology encapsulates the domain knowledge about the learners as well as Learning Objects (LOs). The semantic model that we built has been experimented with different combinations of the key learner parameters such as learning style, knowledge level, and background knowledge. The proposed framework utilizes these parameters to build natural learner groups from the learner ontology using SPARQL queries. The ontology holds 480 learners’ data, 468 annotated learning objects with 5,600 learner ratings. A multivariate k-means clustering algorithm, an unsupervised machine learning technique for grouping similar data, is used to evaluate the learner similarity computation accuracy. The learner satisfaction achieved with the proposed model is measured based on the ratings given by the 40 participants of the experiments. From the evaluation perspective, it is evident that 79\% of the learners are satisfied with the recommendations generated by the proposed model in pure cold-start condition.},
journal = {J. Data and Information Quality},
month = apr,
articleno = {16},
numpages = {27},
keywords = {Learner profile, learning object, personalized learning environment, pure cold-start problem, content recommenders, ontology, multivariate clustering}
}

@inproceedings{10.1145/3701716.3715178,
author = {Wu, Junfeng and He, Jing and Liu, Hai and Zheng, Zhaoqi and Cao, Yichen and Chen, Xingguo and Zou, Bingjie and Zou, Ruiping and Zhou, Guohua and Sturgess, David and van Zundert, Andr\'{e}},
title = {From Literature to Lab: Hardware-Independent Autonomous Chemical Synthesis with Reinforcement Learning},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715178},
doi = {10.1145/3701716.3715178},
abstract = {Chemical synthesis, a fundamental process in chemical engineering, traditionally requires extensive manual intervention and expertise, particularly in interpreting scientific literature and translating it into executable workflows. Autonomous systems offer the potential to revolutionize this field by enabling robots to autonomously read scientific literature and form general synthesis workflows. However, a key challenge lies in integrating diverse hardware components, requiring adaptable architectures that can seamlessly operate across various platforms. Furthermore, these systems must be robust, allowing for real-time error correction, and accessible to non-programmers. This paper presents Autonomous Chemical Synthesis System (ACSS), an adaptable architecture for chemical execution systems designed to address these challenges. ACSS enables robots to autonomously read scientific literature and form general synthesis workflows. We achieve hardware independence through protocol-integration reinforcement learning, enabling seamless integration across platforms. Chemical code and hardware descriptions are compiled and converted into robotic instructions for execution. Natural language error correction allows non-programmers to easily adjust the system. Our key contributions are: (1) Literature to Lab: directly extracting synthesis protocols from scientific articles; (2) Hardware Independence: developing an adaptive, reinforcement learning-based model for diverse hardware setups. ACSS has successfully synthesized 12 diverse compounds from literature, including lidocaine (a painkiller), Dess-Martin periodinane (an oxidizing agent), and alkyl fluoride (a fluorinating agent). This demonstration highlights the potential of NLP and reinforcement learning for efficient and accessible chemical research and production. Click here to find out more on YouTube.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2923–2926},
numpages = {4},
keywords = {large language model, natural language processing, reinforcement learning, robot chemist, task dependency graph},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3674558.3674597,
author = {Mezhuyev, Vitaliy and Hofmann, Paul},
title = {Expert System for Bainite Design: the Approach to Enrich Physical Models with Information Derived from Knowledge Models},
year = {2024},
isbn = {9798400716386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674558.3674597},
doi = {10.1145/3674558.3674597},
abstract = {The development of a physical model begins with a knowledge model, initially existing as ideas in the mind of a researcher. A transition from knowledge models to strict mathematical formalisms is a challenging process, and may not always be feasible, particularly in the early stages of research. Another problem comes when many experts are participating in the development of new physical knowledge, which may result in inconsistency. To contribute to this domain, the paper presents the development of an expert system (ES), created to capture expert knowledge for the design of a new physical material, namely, the bainite steel. The ES combines physical properties and rules in a unique knowledge model and enriches them by derived from data probabilities. The proposed approach enables users to validate expert knowledge and find contradictions in the logical rules, giving the possibility of mapping them back to physical models.},
booktitle = {Proceedings of the 2024 10th International Conference on Computer Technology Applications},
pages = {270–275},
numpages = {6},
keywords = {Bainite steel, Expert system, Material design, Physical modelling, Probabilistic programming},
location = {Vienna, Austria},
series = {ICCTA '24}
}

@inproceedings{10.1145/3652620.3688245,
author = {Burattini, Samuele and Zimmermann, Antoine and Picone, Marco and Ricci, Alessandro},
title = {Towards Linked Data for Ecosystems of Digital Twins},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688245},
doi = {10.1145/3652620.3688245},
abstract = {Due to either the inherent complexity of the domain or the evolving nature of systems, we can envision solutions that digitalize assets in a complex domain using an ecosystem of distributed Digital Twins instead of a single monolithic one. To effectively tackle interoperability in such ecosystems, this paper advocates for the introduction of a representation based on Semantic Web technologies enabling the discovery of both Digital Twin structure - i.e. the static information about the asset model and offered services - and state - i.e. the data and metrics collected at runtime - to support the management of ecosystems and the creation of application mashups. A review of the state of the art suggests that currently investigated ways to describe a Digital Twin are not sufficient to achieve this objective. A proposal of key requirements for a Digital Twin representation is outlined leading to the proposal of a core ontology and a Linked Data approach for state management.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {332–337},
numpages = {6},
keywords = {digital twins, semantic web, interoperability, ontologies},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3322640.3326725,
author = {El Ghosh, Mirna and Abdulrab, Habib},
title = {The Application of ODCM for Building Well-Founded Legal Domain Ontologies: A Case Study in the Domain of Carriage of Goods by Sea},
year = {2019},
isbn = {9781450367547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322640.3326725},
doi = {10.1145/3322640.3326725},
abstract = {The ontology engineering community is facing several key challenges about the development of domain ontologies. One major challenge is the building of well-founded domain ontologies. This concept has raised recently and it refers to ontologies that are grounded in validated foundational ontologies. This paper addresses the effective contribution of ontology-driven conceptual modeling process (ODCM) for developing such ontologies in the legal domain. A case study in the domain of carriage of goods by sea is presented.},
booktitle = {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law},
pages = {204–208},
numpages = {5},
keywords = {OntoUML, Ontology-Driven Conceptual Modeling, UFO, legal ontologies, well-founded ontologies},
location = {Montreal, QC, Canada},
series = {ICAIL '19}
}

@inproceedings{10.1145/3705328.3748165,
author = {Balloccu, Giacomo and Boratto, Ludovico and Fenu, Gianni and Marras, Mirko and Medda, Giacomo and Murgia, Giovanni},
title = {GreenFoodLens: Sustainability Labels for Food Recommendation},
year = {2025},
isbn = {9798400713644},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705328.3748165},
doi = {10.1145/3705328.3748165},
abstract = {Most food recommender systems aim to boost user engagement by analyzing recipe ingredients and users’ past choices. Even though consumers are paying more attention to sustainability, such as carbon and water footprints, there remains a notable lack of public corpora that combine detailed user–recipe interactions with reliable environmental impact data. This gap makes it hard to build recommendation tools that both match people’s tastes and help reduce ecological damage. To this end, we present GreenFoodLens, a resource that enriches HUMMUS, one of the largest corpora for food recommendation, with environmental impact estimates derived from the hierarchical taxonomy of the SU-EATABLE-LIFE project. We achieved this result through a multi-step process involving human annotations, iterative labeling assessments, knowledge refinement, and constrained generation techniques with large language models. Finally, we evaluate recommendation baselines on HUMMUS augmented with GreenFoodLens labels and find that models are driven by popularity signals, which may exacerbate the environmental impact of users’ recipe choices. These experiments demonstrate the practical benefit of GreenFoodLens for benchmarking and advancing sustainability-aware recommendation research. The resource is available at .},
booktitle = {Proceedings of the Nineteenth ACM Conference on Recommender Systems},
pages = {764–773},
numpages = {10},
keywords = {Sustainability, Food Recommendation, Recipe Recommendation, Large Language Model, Constrained Generation, Human Labeling.},
location = {
},
series = {RecSys '25}
}

@inproceedings{10.1145/3726122.3726228,
author = {Kayumova, Kamola and Akbarova, Shakhnoza and Bobokeldiyeva, Maftuna and Abdukarimova, Gulchehra and Khaydarova, Umida and Xasanova, Zarina},
title = {Systematic Mapping of Computational Linguistics in Distributed Knowledge Based Systems and Management},
year = {2025},
isbn = {9798400711701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726122.3726228},
doi = {10.1145/3726122.3726228},
abstract = {Advancements in computational linguistics have enabled the formulation of multiple natural language processing frameworks with considerable semantic accuracy, knowledge representation, and real-time adaptability benefits. Emerging research on distributed knowledge-based systems is challenging traditional conceptions of language processing and data integration, and in the process, opening up windows of opportunity for enhancing the scalability associated with knowledge management in decentralized environments. As little is known about where computational linguistics integration is gaining momentum beyond academic research and software engineering, the purpose of this systematic mapping study is to map in what areas of distributed knowledge management it is perceived to gain traction. Drawing on data from 150 systematically selected research articles and trend analysis in computational linguistics applications, we identify a long tail of application domains and methodological approaches in which a total of 42 unique computational models operate, including techniques such as knowledge graph embeddings, transformer-based architectures, and multimodal language models. Our findings reveal a strong, positive correlation coefficient (r = 0.82) between natural language processing adoption and knowledge retrieval efficiency in distributed systems. However, existing frameworks do not passively comply. Rather, their linguistic adaptability and semantic interpretation mechanisms are integrated into the core functionality of distributed knowledge networks. The study concludes by identifying key research gaps, reflecting on the application of machine learning-enhanced linguistic models in the field of knowledge management, and proposing suggestions for future research directions in distributed data processing. The findings enrich understandings of the workings of computational linguistics methodologies in experiences of real-time knowledge extraction and intelligent information retrieval.},
booktitle = {Proceedings of the 8th International Conference on Future Networks \&amp; Distributed Systems},
pages = {727–733},
numpages = {7},
location = {
},
series = {ICFNDS '24}
}

@inproceedings{10.1145/3639233.3639242,
author = {Miskell, Cameron and Diaz, Richard and Ganeriwala, Parth and Slhoub, Khaled and Nembhard, Fitzroy},
title = {Automated Framework to Extract Software Requirements from Source Code},
year = {2024},
isbn = {9798400709227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639233.3639242},
doi = {10.1145/3639233.3639242},
abstract = {Software maintenance and innovation are constant challenges across industries, especially as programming languages evolve with technology. Similarly, poor lexicon quality degrades program comprehension, increasing the effort required by developers to improve existing software products. To address these challenges, we propose a novel automated framework that extracts software requirements directly from source code using a baseline AI language model applied to a Java code base. Leveraging natural language processing techniques, the framework validates programs and generates easily readable requirements by analyzing file contents. The framework enhances agility and flexibility by providing comprehensive documentation for existing software systems. It caters to both experienced and less-experienced developers, offering an intuitive graphical user interface and enabling efficient identification and resolution of errors. The resulting output facilitates natural interaction through language processing. By automating the extraction process, the framework allows developers to better understand software systems, make informed decisions, and adapt to evolving needs.},
booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
pages = {130–134},
numpages = {5},
keywords = {AI language model, Extracting functional requirements, Legacy code, Natural Language Processing, Software evolution, Software verification},
location = {Seoul, Republic of Korea},
series = {NLPIR '23}
}

@inproceedings{10.1145/3617695.3617722,
author = {Yang, Yi and Yu, Dekuang and Zhu, Yangyang and Qin, Yuxuan and Wang, Erhao},
title = {Design of General Chronic Disease Retrieval Model Framework Based on Chinese Medical Knowledge Graph},
year = {2023},
isbn = {9798400708015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617695.3617722},
doi = {10.1145/3617695.3617722},
abstract = {This article proposes a method for constructing a chronic disease retrieval model based on the Chinese medical knowledge graph. By combining the Chinese medical knowledge graph with classification retrieval, a chronic disease classification retrieval model based on the medical knowledge graph is constructed, which mainly includes three aspects: constructing medical knowledge graph for retrieval, designing hierarchical classification rules, scheming sorting strategies and display methods. The proposed hierarchical classification retrieval model mechanism and related strategies are conducive to the effective organization of health information, solving the current problems of multi-source heterogeneity and semantic ambiguity, improving the efficiency and quality of user retrieval, has a certain promoting effect on the development of theories and methods related to health information services.},
booktitle = {Proceedings of the 2023 7th International Conference on Big Data and Internet of Things},
pages = {195–200},
numpages = {6},
keywords = {Chinese medical knowledge graph, Chronic diseases, construction method, retrieval model framework},
location = {Beijing, China},
series = {BDIOT '23}
}

@inproceedings{10.1145/3404709.3404769,
author = {Mohsen, Wa'el and Aref, Mostafa and ElBahnasy, Khaled},
title = {Blockchain as a Platform for Collaborative Ontology Evolution},
year = {2020},
isbn = {9781450375337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404709.3404769},
doi = {10.1145/3404709.3404769},
abstract = {The Semantic Web is an incomplete dream so far, but a revolutionary platform as Blockchain could be the solution to this, not optimal reality. There is still little understanding of, and support for, the evolutionary aspects of ontologies. This is particularly crucial in distributed and collaborative settings such as the Semantic Web, where ontologies naturally co-evolve with their communities of use. In this setting, different organizations and teams collaboratively build a common ground of the domain. In this "complex" and dynamic setting, a collaborative change process model requires more powerful engineering, argumentation and negotiation methodologies. Blockchain offers a robust framework for teams' collaboration and ontology versioning globally between an infinite number of teams. Blockchain is an example of a distributed computing system with high Byzantine fault tolerance. This makes blockchains potentially suitable for the recording of evolution events, ontology records, and other records management activities, such as ontology evolution, transaction processing and ontology documenting provenance. In this paper, after briefly summarizing the significant features of Blockchain, we describe blockchain-empowered solutions for building an evolution model based on blockchain technology and its artifacts.},
booktitle = {Proceedings of the 6th International Conference on Frontiers of Educational Technologies},
pages = {183–190},
numpages = {8},
keywords = {Blockchain, Collaborative Evolution, Consensus, Distributed Computing, Inter-organizational Ontology, Ontology Evolution, validation and Evaluation},
location = {Tokyo, Japan},
series = {ICFET '20}
}

@article{10.1145/3574135,
author = {Zhang, Bolin and Tu, Zhiying and Hang, Shaoshi and Chu, Dianhui and Xu, Xiaofei},
title = {Conco-ERNIE: Complex User Intent Detect Model for Smart Healthcare Cognitive Bot},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3574135},
doi = {10.1145/3574135},
abstract = {The outbreak of Covid-19 has exposed the lack of medical resources, especially the lack of medical personnel. This results in time and space restrictions for medical services, and patients cannot obtain health information all the time and everywhere. Based on the medical knowledge graph, healthcare bots alleviate this burden effectively by providing patients with diagnosis guidance, pre-diagnosis, and post-diagnosis consultation services in the way of human-machine dialogue. However, the medical utterance is more complicated in language structure, and there are complex intention phenomena in semantics. It is a challenge to detect the single intent, multi-intent, and implicit intent of a patient’s utterance. To this end, we create a high-quality annotated Chinese Medical query (utterance) dataset, CMedQ (about 16.8k queries in medical domain which includes single, multiple, and implicit intents). It is hard to detect intent on such a complex dataset through traditional text classification models. Thus, we propose a novel detect model Conco-ERNIE, using concept co-occurrence patterns to enhance the representation of pre-trained model ERNIE. These patterns are mined using Apriori algorithm and will be embedded via Node2Vec. Their features will be aggregated with semantic features into Conco-ERNIE by using an attention module, which can catch user explicit intents and also predict user implicit intents. Experiments on CMedQ demonstrates that Conco-ERNIE achieves outstanding performance over baseline. Based on Conco-ERNIE, we develop an intelligent healthcare bot, MedicalBot. To provide knowledge support for MedicalBot, we also build a Chinese medical graph, CMedKG (about 45k entities and 283k relationships).},
journal = {ACM Trans. Internet Technol.},
month = feb,
articleno = {21},
numpages = {24},
keywords = {Intent detection, healthcare bot, cognitive service, medical knowledge graph}
}

@inproceedings{10.1145/3548608.3559289,
author = {Wang, Guodong and Liu, Guohua},
title = {A unified modeling method of product demand and manufacturing capability in the textile industry Internet},
year = {2022},
isbn = {9781450397179},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3548608.3559289},
doi = {10.1145/3548608.3559289},
abstract = {Through the analysis of multiple cloud manufacturing models, we discussed the definition of the current textile industry Internet manufacturing model, and proposed the concept of the textile industry knowledge graph for the standardized expression and sharing of demand and manufacturing resources under the textile industry Internet manufacturing model; A unified modeling and packaging method for manufacturing requirements and manufacturing capabilities based on knowledge graph technology with standard process flow as the interface is proposed.},
booktitle = {Proceedings of the 2022 2nd International Conference on Control and Intelligent Robotics},
pages = {690–694},
numpages = {5},
keywords = {Industrial Internet, cloud manufacturing, knowledge graph, meta-model, ontology},
location = {Nanjing, China},
series = {ICCIR '22}
}

@inproceedings{10.1145/3424978.3425102,
author = {Li, Guoxuan},
title = {Improving Biomedical Ontology Matching Using Domain-specific Word Embeddings},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425102},
doi = {10.1145/3424978.3425102},
abstract = {Biomedical ontology is an effective carrier of biomedical knowledge. In real-world applications, many biomedical ontologies describe knowledge in the same field. In order to make full use of existing knowledge, it is necessary to carry out knowledge fusion to obtain a unified knowledge structure. For this reason, it becomes particularly important to find mapping entities that refer to the same object in different ontologies. At present, a large number of automatic matching systems engineers features and match entities by the name of the entity, the ontology structure and external resources. These methods have achieved encouraging results, but they ignore the semantic information of the entity labels. On the other hand, the representation learning method has already shined in many areas of natural language processing. The word vector obtained by the word embedding method contains semantic information of words. However, a separate representation learning method cannot fully capture the structural information of the ontology. In this paper, we propose a method which combines the representation learning method as a component with traditional feature engineering methods to improve the performance of the matching systems. We tested our method on real-world datasets. Experimental results show that our method can improve the recall and F1-measure of the existing matching systems.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {120},
numpages = {5},
keywords = {Artificial intelligence, Biomedical ontology matching, Feature engineering, Word embedding},
location = {Sanya, China},
series = {CSAE '20}
}

@inproceedings{10.1145/3563657.3596027,
author = {Rakib, Mohammad Abu Nasir and Scidmore, Jeremy and Ginsberg, Justin and Torres, Cesar},
title = {Thermoplastic Kilnforms: Extending Glass Kilnforming Techniques to Thermoplastic Materials using Ontology-Driven Design},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596027},
doi = {10.1145/3563657.3596027},
abstract = {The ecology of thermoplastic materials is rapidly evolving, enabling an exciting landscape of functional, aesthetic, and interactive forms. Despite their utility in fused filament fabrication (FFF), an even larger and untapped design space exists for thermoplastics. In this work, we introduce a design method that leverages similarities with a more mature medium (glass) to guide a material-centered exploration of a new medium (thermoplastics). Through a collaboration between domain experts in thermoplastics and glass, we synthesized an ontology of kilnforming techniques and developed an annotated portfolio of thermoplastic kilnforms that capture generative design directions for altering the phenomenological qualities of plastic, prototyping metamaterials, and composite forms, and engaging with other material practices. We discuss how material parallels can continue to expand the role of thermoplastics as a design material and how ontology-driven design can serve as a means of localizing, questioning, and generating material knowledge.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {263–281},
numpages = {19},
keywords = {composites, material exploration, thermoforming},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{10.1145/3571884.3597131,
author = {Aicher, Annalena Bea and Kornm\"{u}ller, Daniel and Minker, Wolfgang and Ultes, Stefan},
title = {Self-imposed Filter Bubble Model for Argumentative Dialogues},
year = {2023},
isbn = {9798400700149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571884.3597131},
doi = {10.1145/3571884.3597131},
abstract = {During their information seeking people tend to filter out all the parts of the available information that do not fit their existing beliefs or opinions. In this paper we present a model for this “Self-imposed Filter Bubble” (SFB) consisting of four dimensions. Thereby, we aim to 1) estimate the probability of the user being caught in an SFB and consequently, 2) identify suitable clues to reduce this probability in the further course of a dialogue. Using an exemplary implementation in an argumentative dialogue system, we demonstrate the validity and applicability of this model in an online user study with 102 participants. These findings serve as a basis for developing a system strategy to break the user’s SFB and contribute to a sustainable and profound reflection on a topic from all viewpoints.},
booktitle = {Proceedings of the 5th International Conference on Conversational User Interfaces},
articleno = {23},
numpages = {11},
keywords = {Computational Argumentation, Confirmation Bias, Cooperative Argumentative Dialogue Systems (ADS), Echo Chambers, User Modeling},
location = {Eindhoven, Netherlands},
series = {CUI '23}
}

@inproceedings{10.1145/3529190.3535693,
author = {Adhikari, Ajaya and Wenink, Edwin and van der Waa, Jasper and Bouter, Cornelis and Tolios, Ioannis and Raaijmakers, Stephan},
title = {Towards FAIR Explainable AI: a standardized ontology for mapping XAI solutions to use cases, explanations, and AI systems},
year = {2022},
isbn = {9781450396318},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529190.3535693},
doi = {10.1145/3529190.3535693},
abstract = {Several useful taxonomies have been published that survey the eXplainable AI (XAI) research field. However, these taxonomies typically do not show the relation between XAI solutions and several use case aspects, such as the explanation goal or the task context. In order to better connect the field of XAI research with concrete use cases and user needs, we designed the ASCENT (Ai System use Case Explanation oNTology) framework, which is a new ontology and corresponding metadata standard with three complementary modules for different aspects of an XAI solution: one for aspects of AI systems, another for use case aspects, and yet another for explanation properties. The descriptions of XAI solutions in this framework include whether the XAI solution has a positive, negative, inconclusive or unresearched relation with use case elements. Descriptions in ASCENT thus emphasize the (user) evaluation of XAI solutions in order to support finding validated practices for application in industry, as well as being helpful for identifying research gaps. Describing XAI solutions according to the proposed common metadata standard is an important step towards the FAIR (Findable, Accessible, Interoperable, Reusable) usage of XAI solutions.},
booktitle = {Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments},
pages = {562–568},
numpages = {7},
keywords = {ASCENT, FAIR, XAI ontology, user-centered},
location = {Corfu, Greece},
series = {PETRA '22}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00024,
author = {Rajbhoj, Asha and Nistala, Padmalata and Kulkarni, Vinay and Soni, Shivani and Pathan, Ajim},
title = {DocToModel: Automated Authoring of Models from Diverse Requirements Specification Documents},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00024},
doi = {10.1109/ICSE-SEIP58684.2023.00024},
abstract = {Early stages of Software Development Life Cycle (SDLC) namely requirement elicitation and requirements analysis have remained document-centric in the industry for market-driven, complex, large-scale business applications and products. The documentation typically runs into hundreds of Natural Language (NL) text documents which requirements engineers need to sift looking for the relevant information and also maintain these documents in-sync over time - a time-consuming and error-prone activity. Much of this difficulty can be overcome if the information is available in a structured form that is amenable to automated processing. Purposive models offer a way out. However, for easy adoption by industry practitioners, these models must be populated from NL text documents in a largely automated manner. This task is characterized by high variability with several documents containing different information conforming to different structures and styles. As a result, purposive information extractors need to be developed for each project/ product. Moreover, being an open-ended space there is no upper bound on the information extractors that need to be developed. To overcome this difficulty, we propose a document structure agnostic and meta-model agnostic tool, DocToModel, for the automated authoring of models from NL text documents. It provides a pattern mapping language to specify a mapping of structured and unstructured document information to meta-model elements, and a pattern interpreter to automate model authoring. The configurable and extensible architecture of DocToModel makes it generic and amenable to easy repurposing for other NL documents. This paper, describes the approach and illustrates its utility and efficacy on multiple real-world case studies.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {199–210},
numpages = {12},
keywords = {meta-model, automated model authoring, model extraction, document parser, NLP, meta-model pattern, pattern interpreter},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.1145/3729434.3729456,
author = {B\"{o}hm, Karsten},
title = {Deep and Surface Representation of Competences in Academic Curricula: A new approach to address human consumers and formal structures using Generative AI},
year = {2025},
isbn = {9798400712630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3729434.3729456},
doi = {10.1145/3729434.3729456},
abstract = {The curricular design of programs in Higher Education is increasingly oriented towards competence-based learning goals in order to provide more specific and explicit qualifications. At the same time the descriptions of competences are mostly based on textual descriptions often in informal style targeted towards different audiences making descriptions vague and harder to compare. Formal models for the specification of competences exist for some time and even semantic models are being developed, e.g., the European Learning Model. These different approaches lead to a gap between formal and informal competence descriptions that require manual efforts of curriculum designer to maintain. This research borrows the concept of Deep and Surface representation models from the field of linguistics to unify the different approaches. Additionally, it focuses on the functionality of Generative Artificial Intelligence in the form of Large Language Models to close the aforementioned gap. It demonstrates the potential of the technology in both directions and discusses application potential of the concept.},
booktitle = {Proceedings of the 6th International Conference on Modern Educational Technology},
pages = {78–85},
numpages = {8},
keywords = {Curricular Design, European Learning Model, Generative Artificial Intelligence, Higher Education, LLM, Semantic Web},
location = {
},
series = {ICMET '24}
}

@inproceedings{10.1145/3643666.3648580,
author = {Miranda, Darliane and Ara\'{u}jo, Jo\~{a}o and Liebel, Grischa},
title = {A Conceptual Model For Web Accessibility Requirements In Agile Development},
year = {2024},
isbn = {9798400705694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643666.3648580},
doi = {10.1145/3643666.3648580},
abstract = {Accessibility is the practice of making content and functionality accessible to all users, regardless of their abilities. Although accessibility is a highly relevant quality attribute, it is often treated as an afterthought in software development, unfortunately excluding people with disabilities from using many web-based systems. Specifically in agile development, sprints focus on new features and quality attributes, such as accessibility, are often not considered sufficiently. In these cases, using conceptual models to understand and analyze requirements that developers have formulated as a set of related user stories is a research opportunity. To increase agile professionals' focus on accessibility, we built a conceptual model for web accessibility, identifying artifacts and concepts used in agile development to specify accessibility. We discuss how this model can be used as a guide to better integrate accessibility considerations into agile software development. Researchers can use the result to define resources that are not currently covered or improve underutilized practices. We plan to use the conceptual model in the next steps to adapt existing agile artifacts and create support tools for web accessibility in agile development.},
booktitle = {Proceedings of the 1st IEEE/ACM Workshop on Multi-Disciplinary, Open, and RElevant Requirements Engineering},
pages = {15–21},
numpages = {7},
keywords = {accessibility requirements, agile development, conceptual model, requirements engineering},
location = {Lisbon, Portugal},
series = {MO2RE 2024}
}

@inproceedings{10.1145/3417990.3421414,
author = {Partridge, Chris and Mitchell, Andrew and da Silva, Marco and Soto, Oscar Xiberta and West, Matthew and Khan, Mesbah and de Cesare, Sergio},
title = {Implicit requirements for ontological multi-level types in the UNICLASS classification},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421414},
doi = {10.1145/3417990.3421414},
abstract = {In the multi-level type modeling community, claims that most enterprise application systems use ontologically multi-level types are ubiquitous. To be able to empirically verify this claim one needs to be able to expose the (often underlying) ontological structure and show that it does, indeed, make a commitment to multi-level types. We have not been able to find any published data showing this being done. From a top-level ontology requirements perspective, checking this multi-level type claim is worthwhile. If the datasets for which the top-level ontology is required are ontologically committed to multi-level types, then this is a requirement for the top-level ontology. In this paper, we both present some empirical evidence that this ubiquitous claim is correct as well as describing the process we used to expose the underlying ontological commitments and examine them. We describe how we use the bCLEARer process to analyse the UNICLASS classifications making their implicit ontological commitments explicit. We show how this reveals the requirements for two general ontological commitments; higher-order types and first-class relations. This establishes a requirement for a top-level ontology that includes the UNICLASS classification to be able to accommodate these requirements. From a multi-level type perspective, we have established that the bCLEARer entification process can identify underlying ontological commitments to multi-level type that do not exist in the surface linguistic structure. So, we have a process that we can reuse on other datasets and application systems to help empirically verify the claim that ontological multi-level types are ubiquitous.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {87},
numpages = {8},
keywords = {UNICLASS, bCLEARer approach, first class relations, higher order types, top-level ontology},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@proceedings{10.1145/3686812,
title = {ICCMS '24: Proceedings of the 2024 16th International Conference on Computer Modeling and Simulation},
year = {2024},
isbn = {9798400717215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dalian, China}
}

@inproceedings{10.1145/3524481.3527229,
author = {Liu, Yu and Yandrapally, Rahulkrishna and Kalia, Anup K. and Sinha, Saurabh and Tzoref-Brill, Rachel and Mesbah, Ali},
title = {CrawLabel: computing natural-language labels for UI test cases},
year = {2022},
isbn = {9781450392860},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524481.3527229},
doi = {10.1145/3524481.3527229},
abstract = {End-to-end test cases that exercise the application under test via its user interface (UI) are known to be hard for developers to read and understand; consequently, diagnosing failures in these tests and maintaining them can be tedious. Techniques for computing natural-language descriptions of test cases can help increase test readability. However, so far, such techniques have been developed for unit test cases; they are not applicable to end-to-end test cases.In this paper, we focus on the problem of computing natural-language labels for the steps of end-to-end UI test cases for web applications. We present two techniques that apply natural-language processing to information available in the browser document object model (DOM). The first technique is an instance of a supervised approach in which labeling-relevant DOM attributes are ranked via manual analysis and fed into label computation. However, supervised approach requires a training dataset. So we propose the second technique, which is unsupervised: it leverages probabilistic context-free grammar learning to compute dominant DOM attributes automatically. We implemented these techniques, along with two simpler baseline techniques, in a tool called CrawLabel (available as a plugin to Crawljax, a state-of-the-art UI test-generation tool for web applications) and evaluated their effectiveness on open-source web applications. Our results indicate that the supervised approach can achieve precision, recall, and Fl-score of 83.38, 60.64, and 66.40, respectively. The unsupervised approach, although less effective, is competitive, achieving scores of 72.37, 58.12, and 59.77. We highlight key results and discuss the implications of our findings.},
booktitle = {Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test},
pages = {103–114},
numpages = {12},
location = {Pittsburgh, Pennsylvania},
series = {AST '22}
}

@inproceedings{10.1145/3583780.3615036,
author = {Dong, Hang and Chen, Jiaoyan and He, Yuan and Liu, Yinan and Horrocks, Ian},
title = {Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615036},
doi = {10.1145/3583780.3615036},
abstract = {Discovering entity mentions that are out of a Knowledge Base (KB) from texts plays a critical role in KB maintenance, but has not yet been fully explored. The current methods are mostly limited to the simple threshold-based approach and feature-based classification, and the datasets for evaluation are relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL) method which can identify mentions that do not have corresponding KB entities by matching them to a special NIL entity. To better utilize BERT, we propose new techniques including NIL entity representation and classification, with synonym enhancement. We also apply KB Pruning and Versioning strategies to automatically construct out-of-KB datasets from common in-KB EL datasets. Results on five datasets of clinical notes, biomedical publications, and Wikipedia articles in various domains show the advantages of BLINKout over existing methods to identify out-of-KB mentions for the medical ontologies, UMLS, SNOMED CT, and the general KB, WikiData.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {452–462},
numpages = {11},
keywords = {WikiData, biomedical ontologies, entity linking, knowledge base enrichment, language models},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3502223.3502235,
author = {Zhu, Rui and Shimizu, Cogan and Stephen, Shirly and Zhou, Lu and Cai, Ling and Mai, Gengchen and Janowicz, Krzysztof and Schildhauer, Mark and Hitzler, Pascal},
title = {SOSA-SHACL: Shapes Constraint for the Sensor, Observation, Sample, and Actuator Ontology},
year = {2022},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502235},
doi = {10.1145/3502223.3502235},
abstract = {The explosive growth of the Linked Data on the Web has greatly facilitated collecting data from remote sensors, from air quality sensors spread out across a city, to seismograph stations spread across the entire world. Integrating these heterogeneous data can be quite challenging; however one can achieve this through the use of available W3C standards to create a knowledge graph. For this use case, the W3C also provides a standard, the Sensor, Observation, Sample, Actuator (SOSA) Ontology, that allows for the semantic encoding of sensors and their observations. However, even with the guidance of this standard, it may be difficult to produce a correct graph with high fidelity from heterogeneous sources. In this paper we present a set of (data) shape constraints, called SOSA-SHACL, for the SOSA ontology using a data validation language, namely the W3C standard SHACL (Shape Constraint Language). These constraints enable us to evaluate whether the modeled observations in our Knowledge Graph comply with the SOSA recommendations. Furthermore, we show through several case studies how the closed world assumption plays a role in the process of designing such shape constraints, especially as SOSA is based on the open world assumption.},
booktitle = {Proceedings of the 10th International Joint Conference on Knowledge Graphs},
pages = {99–107},
numpages = {9},
keywords = {RDF validation, knowledge graph quality assessment and refinement, sensors and observations},
location = {Virtual Event, Thailand},
series = {IJCKG '21}
}

@inproceedings{10.1145/3628034.3628037,
author = {Amiri, Amirali and Ntentos, Evangelos and Zdun, Uwe and Geiger, Sebastian},
title = {Tool Support for Learning Architectural Guidance Models from Architectural Design Decision Models},
year = {2024},
isbn = {9798400700408},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628034.3628037},
doi = {10.1145/3628034.3628037},
abstract = {This paper presents an approach to architectural knowledge management that does not assume existing architectural design decisions or pattern applications are documented as architectural knowledge, but benefits from more existing data. We drew inspiration from manual qualitative research methods for mining patterns and architectural knowledge and created a guideline model of which the ADD models are instances. We evaluated our approach on 11 cases from the gray literature. We found that it can provide suitable recommendations after modeling only a single case and reaches theoretical saturation and recommendations with low to very low errors after only 6-8 cases. Our approach shows that creating a reusable architectural design space is possible based only on limited case data. Our approach not only provides a novel approach to architectural knowledge management but can also be used as a tool for pattern mining.},
booktitle = {Proceedings of the 28th European Conference on Pattern Languages of Programs},
articleno = {3},
numpages = {14},
keywords = {Architectural Design Decisions, Architectural Knowledge Management, Design Patterns},
location = {Irsee, Germany},
series = {EuroPLoP '23}
}

@inproceedings{10.1145/3549037.3561272,
author = {Stang, J\o{}rgen and Walther, Dirk and Myrseth, Per},
title = {Data quality as a microservice: an ontology and rule based approach for quality assurance of sensor data in manufacturing machines},
year = {2022},
isbn = {9781450394598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549037.3561272},
doi = {10.1145/3549037.3561272},
abstract = {The manufacturing industry is continuously looking for production improvements resulting in high quality production, reduced waste and competitive advantages. In this article, ontologies, semantic rule logic and microservices have been deployed to suggest a system for quality assurance of manufacturing machine data. The existing upper ontology for manufacturing service description has been used to define both the physical assets as well as the data quality requirements. The system is used to both operationalize data quality monitoring by semantic technology as well as enabling up-front modelling of data quality requirements. The approach is illustrated by a specific speed-feed case for manufacturing machines but could easily be extended to other manufacturing use-cases or even to other industries.},
booktitle = {Proceedings of the 2nd International Workshop on Software Engineering and AI for Data Quality in Cyber-Physical Systems/Internet of Things},
pages = {3–9},
numpages = {7},
keywords = {Data Quality, IoT, Manufacturing Machines, Microservices, Ontolologies, Sensor Data},
location = {Singapore, Singapore},
series = {SEA4DQ 2022}
}

@article{10.1145/3458027,
author = {Bromander, Siri and Swimmer, Morton and Muller, Lilly Pijnenburg and J\o{}sang, Audun and Eian, Martin and Skj\o{}tskift, Geir and Borg, Fredrik},
title = {Investigating Sharing of Cyber Threat Intelligence and Proposing A New Data Model for Enabling Automation in Knowledge Representation and Exchange},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3458027},
doi = {10.1145/3458027},
abstract = {For a strong, collective defense in the digital domain, we need to produce, consume, analyze, and share cyber threat intelligence. With an increasing amount of available information, we need automation to ensure adequate efficiency. We present the results from a questionnaire investigating the use of standards and standardization and how practitioners share and use cyber threat intelligence (CTI). We propose a strict data model for CTI that enables consumption of all relevant data, data validation, and analysis of consumed content. The main contribution of this article is insight into how CTI is shared and used by practitioners, and the strictness of the data model that enforces input of information and enables automation and deduction of new knowledge.},
journal = {Digital Threats},
month = oct,
articleno = {6},
numpages = {22},
keywords = {Cyber threat intelligence, security, knowledge graph, ontology}
}

@inproceedings{10.1145/3698205.3733934,
author = {Chen, Shijun (Cindy)},
title = {Conceptualizing Online Feedback Engagement from a Sociomaterial Perspective: An Iceberg Model},
year = {2025},
isbn = {9798400712913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698205.3733934},
doi = {10.1145/3698205.3733934},
abstract = {This study conceptualizes online feedback engagement through a sociomaterial lens, exploring how learners' feedback engagement is dynamically shaped by the entanglement of human and non-human actors in digital environments. While prior research has examined cognitive, behavioral, and affective dimensions of feedback engagement in face-to-face learning environments, few studies have explored how technological affordances and sociocultural values mediate these forms of engagement. Drawing on a sociomaterial perspective, this study proposes a multidimensional framework of feedback engagement comprising cognitive, behavioral, relational, and collaborative dimensions. By synthesizing existing literature and integrating insights from recent empirical studies involving digital feedback tools, the paper highlights how engagement is not solely a learner-driven phenomenon but is co-constructed through sociomaterial arrangements. The framework advances current understandings of online feedback engagement and offers implications for the design of pedagogically sound feedback practices in technology-mediated learning contexts.},
booktitle = {Proceedings of the Twelfth ACM Conference on Learning @ Scale},
pages = {251–255},
numpages = {5},
keywords = {feedback, feedback engagement, online learning, sociomaterialism},
location = {Palermo, Italy},
series = {L@S '25}
}

@inproceedings{10.1145/3636243.3636256,
author = {Doughty, Jacob and Wan, Zipiao and Bompelli, Anishka and Qayum, Jubahed and Wang, Taozhi and Zhang, Juran and Zheng, Yujia and Doyle, Aidan and Sridhar, Pragnya and Agarwal, Arav and Bogart, Christopher and Keylor, Eric and Kultur, Can and Savelka, Jaromir and Sakr, Majd},
title = {A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education},
year = {2024},
isbn = {9798400716195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3636243.3636256},
doi = {10.1145/3636243.3636256},
abstract = {There is a constant need for educators to develop and maintain effective up-to-date assessments. While there is a growing body of research in computing education on utilizing large language models&nbsp;(LLMs) in generation and engagement with coding exercises, the use of LLMs for generating programming MCQs has not been extensively explored. We analyzed the capability of GPT-4 to produce multiple-choice questions (MCQs) aligned with specific learning objectives (LOs) from Python programming classes in higher education. Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs from high-level course context and module-level LOs. We evaluated 651 LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python courses. We found that GPT-4 was capable of producing MCQs with clear language, a single correct choice, and high-quality distractors. We also observed that the generated MCQs appeared to be well-aligned with the LOs. Our findings can be leveraged by educators wishing to take advantage of the state-of-the-art generative models to support MCQ authoring efforts.},
booktitle = {Proceedings of the 26th Australasian Computing Education Conference},
pages = {114–123},
numpages = {10},
keywords = {Assessments, Automated Content Generation, Automatic Generation, GPT-4, LLMs, LOs, Large Language Models, Learning Objectives, MCQs, Multiple-choice Questions},
location = {Sydney, NSW, Australia},
series = {ACE '24}
}

@inproceedings{10.1145/3655532.3655555,
author = {Liu, Caihong and Liu, Changhui},
title = {Sentiment Analysis of Movie Reviews Based on Sentiment Dictionary and Deep Learning Models},
year = {2024},
isbn = {9798400708039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3655532.3655555},
doi = {10.1145/3655532.3655555},
abstract = {As the Internet era has progressed, platforms such as Douban Movies have spawned a great number of evaluations with personal biases. However, these assessments lack a set length, and the text's expression is varied, not constrained to grammar-related constraints. The expressive style is less formal. As a result, mining and assessing these opinions has substantial economic worth. This experiment utilized a novel sentiment lexicon to adapt informal vocabulary in movie reviews. To improve the accuracy of sentiment analysis in movie reviews, it was integrated with the Albert-BiLSTM-Attention model. The results of six rounds of comparative experiments show that the method suggested in this paper has improved average precision, average recall, and average F1 score in the sentiment classification of this dataset. The suggested model can be used to achieve precise sentiment analysis for film reviews, offering pertinent support and advice for the production team's upcoming films.},
booktitle = {Proceedings of the 2023 6th International Conference on Robot Systems and Applications},
pages = {144–148},
numpages = {5},
keywords = {Albert-BiLSTM, Attention mechanisms, deep learning, sentiment lexicon},
location = {Wuhan, China},
series = {ICRSA '23}
}

@inproceedings{10.1145/3511047.3537690,
author = {Gena, Cristina and Damiano, Rossana and Mattutino, Claudio and Mazzei, Alessandro and Brighenti, Stefania and Nazzario, Matteo and Meirone, Andrea and Quarato, Camilla and Miraglio, Elisabetta and Ricciardiello, Giulia and Petriglia, Francesco and Liscio, Federica and Piccinni, Giuseppe and Mazzotta, Loredana and Pecone, Cesare and Ricci, Valeria},
title = {Ontologies and Open Data for Enriching Personalized Social Moments in Human Robot Interaction},
year = {2022},
isbn = {9781450392327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511047.3537690},
doi = {10.1145/3511047.3537690},
abstract = {This paper describes our proposal for enriching personalized social moments and dialogues between human and robot in the context of the Sugar, Salt \&amp; Pepper laboratory. The lab focused on the use of the Pepper robot in a therapeutic context to promote autonomies and functional acquisitions in highly functioning (Asperger) children with autism. This paper is focused on a post-hoc work aimed at improving the robot's autonomous dialogue strategies. In particular we are integrating the robot's dialogue with a knowledge base to have the robot able to move and reason on an ontology, and thus enriching its dialogue's strategies. For instance, the taxonomic structure of the ontology could allow Pepper to drive the focus of the conversation to related topics or to more general or specific topics, and, in general, it could improve its capability to manage the conversation and disambiguate the input from the user.},
booktitle = {Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {151–154},
numpages = {4},
keywords = {Adaptivity, HRI, Human Behavior Understanding, Social Robots},
location = {Barcelona, Spain},
series = {UMAP '22 Adjunct}
}

@inproceedings{10.1145/3404709.3404771,
author = {Mohsen, Wa'el and Aref, Mostafa and ElBahnasy, Khaled},
title = {Cooperative Domain Ontology Reduction Based on Power Sets},
year = {2020},
isbn = {9781450375337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404709.3404771},
doi = {10.1145/3404709.3404771},
abstract = {Ontology is widely used in the areas of knowledge engineering, web-based data mining, and others. The process of developing and evolving inter-organizational domain ontologies is easy to get much redundant information. PowerSets can be used to reduce the attributes of ontologies. In this paper, "Rule Finding Uniqueness," RFU is proposed for learning a set of rules in order to refine an ontology. The algorithm's primary goal is to generate unique rules that not only cover the initial set but also enhance reasoning. The claimed technique compresses Ontologies after it is already built or during the evolving process of the inter-organizational cooperative domain ontology. The proposed method can also be used to strengthen automatic and semi-automatic operations to develop and evolve ontologies. We can consider this approach as a maintenance operation that could be done periodically based on the ontology evolution frequency rate.},
booktitle = {Proceedings of the 6th International Conference on Frontiers of Educational Technologies},
pages = {196–203},
numpages = {8},
keywords = {Attributes, Inter-organizational domain ontology, Ontology Reductio, Power Sets},
location = {Tokyo, Japan},
series = {ICFET '20}
}

@inproceedings{10.1145/3428757.3429110,
author = {Khiat, Abderrahmane and Elias, Mirette and Foldenauer, Ann Christina and Koehm, Michaela and Blumenstein, Irina and Napolitano, Giulio},
title = {Towards an Ontology Representing Characteristics of Inflammatory Bowel Disease},
year = {2021},
isbn = {9781450389228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428757.3429110},
doi = {10.1145/3428757.3429110},
abstract = {Inflammatory bowel disease (IBD) is a chronic disease characterized by numerous, hard to predict periods of relapse and remission. "Digital twin" approaches, leveraging personalized predictive models, would significantly enhance therapeutic decision-making and cost-effectiveness. However, the associated computational and statistical methods require high quality data from a large population of patients. Such a comprehensive repository is very challenging to build, though, and none is available for IBD. To overcome this, a promising approach is to employ a knowledge graph, which is built from the available data and would help predicting IBD episodes and delivering more relevant personalized therapy at the lowest cost. In this research, we present a knowledge graph developed on the basis of patient records which are collected from one of the largest German gastroentologic outpatient clinic. First, we designed IBD ontology that encompasses the vocabulary, specifications and characteristics associated by physicians with IBD patients, such as disease classification schemas (e.g., Montreal Classification of IBD), status of the disease activity, and medications. Next, we defined the mappings between ontology entities and database variables. Physicians and project members participating in the Fraunhofer MED2ICIN project, validated the ontology and the knowledge graph. Furthermore, the knowledge graph has been validated against the competency questions compiled by physicians.},
booktitle = {Proceedings of the 22nd International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {216–222},
numpages = {7},
keywords = {IBD, Mappings, Ontology, Ontop, VoCoReg, knowledge graph},
location = {Chiang Mai, Thailand},
series = {iiWAS '20}
}

@inproceedings{10.1145/3466933.3466953,
author = {Ribeiro, Elivaldo Lozer Fracalossi and Souza, Marlo and Claro, Daniela Barreiro},
title = {MIDAS-OWL: An Ontology for Interoperability between Data and Service Cloud Layers},
year = {2021},
isbn = {9781450384919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466933.3466953},
doi = {10.1145/3466933.3466953},
abstract = {As different cloud computing services have emerged over the years, the diversity of technologies and the lack of standardization has given rise to an interoperability problem in cloud computing. Cloud computing services include those such as Software as a Service (SaaS), Platform as a Service (PaaS), Infrastructure as a Service (IaaS), and Data as a Service (DaaS). In this context, interoperability enables a service to communicate with another service transparently. Among the solutions proposed in the literature, a middleware can be used to intermediate such communication and to mitigate the lack of interoperability in cloud computing. For instance, the middleware MIDAS (Middleware for DaaS and SaaS) provides transparent interoperability between SaaS and DaaS. Although MIDAS current version promotes syntactic interoperability, semantic interoperability is only superficially addressed. In collaboration with this project, we develop an OWL-based ontology to formally represent the communication between SaaS and DaaS, and discuss its strengths in providing semantic interoperability on MIDAS. We conduct a set of experiments to validate our ontology. We evaluate intrinsic (consistency, correctness, acceptance) and extrinsic (integration between ontology and MIDAS) issues. Results provide evidence that a semantic MIDAS interoperability can be enhanced by our ontology.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Information Systems},
articleno = {20},
numpages = {8},
keywords = {cloud computing, cloud services, ontology, semantic interoperability},
location = {Uberl\^{a}ndia, Brazil},
series = {SBSI '21}
}

@inproceedings{10.1145/3665689.3665768,
author = {Deng, Qiwen and Han, Yuexia and Sun, Jianfei},
title = {A Joint Framework for Predicting Disease-Gene Interactions Based on Pre-trained Models and Graph Attention Networks},
year = {2024},
isbn = {9798400716645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665689.3665768},
doi = {10.1145/3665689.3665768},
abstract = {The study of disease-gene interactions is crucial in biomedical research. Identifying genes associated with diseases can provide critical insights into disease mechanisms, facilitate early diagnosis, and contribute to the development of targeted therapies. In this paper, we propose a novel framework for predicting disease-gene interactions called the PRGAT-DG, which utilizes pre-trained language models and graph attention networks to extract semantic and graph structure features respectively. Moreover, we introduce residual structure to alleviate the problem of excessive smoothing. Experimental results on a dataset released by Stanford University demonstrate the remarkable predictive accuracy of our framework, showcasing its superiority compared to other existing methods. This research holds significant implications for advancing our understanding of disease-gene interaction mechanisms and accelerating the development of relevant therapeutics.},
booktitle = {Proceedings of the 2024 4th International Conference on Bioinformatics and Intelligent Computing},
pages = {474–478},
numpages = {5},
location = {Beijing, China},
series = {BIC '24}
}

@inproceedings{10.1145/3550356.3561604,
author = {Biglari, Raheleh and Denil, Joachim},
title = {Model validity and tolerance quantification for real-time adaptive approximation},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561604},
doi = {10.1145/3550356.3561604},
abstract = {Designing a Cyber-physical system (CPS) including modeling the control components and services is a challenging issue. Models and simulations at run-time play a crucial role to implement these control and prediction components.Real-time constraints raise the complexity of designing an efficient CPS system. Having detailed models in making decisions and/or numerous predictions in different contexts is computationally expensive and difficult to schedule on the computational infrastructure.Inspired by substitutability, one strategy for dealing with complex CPS and the contradiction of better real-time performance and reduced cost in CPS is to employ approximated models and switch to the most suited model adaptively at run-time.However, using an approximate model raises the uncertainty on the model's predictions. Nonetheless, the model is appropriate when the uncertainty is within bound. This bound is defined as tolerance which is the permitted amount of uncertainty.In this paper, we propose a method for quantifying the tolerance of cyber-physical systems, where we can switch between the original model and approximated models and how to identify more appropriate models.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {668–676},
numpages = {9},
keywords = {adaptation, approximation, cyber-physical systems, model validity, real-time systems, tolerance quantification, uncertainty},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3701716.3715532,
author = {Mu\~{n}oz, Carlos and Mendoza, Marcelo and Lobel, Hans and Keith, Brian},
title = {Imitating Human Reasoning to Extract 5W1H in News},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715532},
doi = {10.1145/3701716.3715532},
abstract = {Extracting key information from news articles is crucial for advancing search systems. Historically, the 5W1H framework, which organises information based on 'Who', 'What', 'When', 'Where', 'Why', and 'How', has been a predominant method in digital journalism empowering search tools. The rise of Large Language Models (LLMs) has sparked new research into their potential for performing such information extraction tasks effectively. Our study examines a novel approach to employing LLMs in the 5W1H extraction process, particularly focusing on their capacity to mimic human reasoning. We introduce two innovative Chain-of-Thought (COT) prompting techniques to extract 5W1H in news: extractive reasoning and question-level reasoning. The former directs the LLM to pinpoint and highlight essential details from texts, while the latter encourages the model to emulate human-like reasoning at the question-response level. Our research methodology includes experiments with leading LLMs using prompting strategies to ascertain the most effective approach. The results indicate that COT prompting significantly outperforms other methods. In addition, we show that the effectiveness of LLMs in such tasks depends greatly on the nature of the questions posed.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1199–1203},
numpages = {5},
keywords = {5w1h, imitative reasoning, llm, news},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3550356.3561554,
author = {Diaconescu, Ada and Houze, Etienne and Dessalles, Jean-Louis and Vangheluwe, Hans and Franceschini, Romain},
title = {Multi-scale model-based explanations for cyber-physical systems: the urban traffic case},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561554},
doi = {10.1145/3550356.3561554},
abstract = {Automated control in Cyber-Physical Systems (CPS) generates behaviours that may surprise non-expert users. Relevant explanations are required to maintain user trust. Large CPS (e.g., autonomous car networks and smart grids) raise additional scaleability issues for the explanatory processes and complexity issues for generated explanations. We propose a multi-scale system modelling and explanation technique to address these concerns. The idea is to increase the scale, or abstraction level, of the modelled CPS, whenever possible without loss of salient information, so as to produce smaller system representations and hence to reduce the complexity of the explanatory process and of the generated explanations. We illustrate our proposal via an urban traffic case study, modelling traffic at two different scales (i.e., modelling individual cars at a lower-scale; and traffic jams at a higher-scale). We show how a multi-scale explanatory process can use the lower- and higher-scale models to generate either longer (more detailed) explanations, or shorter (more abstract) explanations, respectively. This proof-of-concept illustration offers a basis for further research towards a comprehensive multi-scale explanatory solution for CPS.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {684–691},
numpages = {8},
keywords = {cyber-physical system, multi-scale model and explanation, traffic simulation},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3655693.3655722,
author = {Chetwyn, Robert Andrew and Eian, Martin and J\o{}sang, Audun},
title = {Modelling Indicators of Behaviour for Cyber Threat Hunting via Sysmon},
year = {2024},
isbn = {9798400716515},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3655693.3655722},
doi = {10.1145/3655693.3655722},
abstract = {Hunting for threats is of capital importance for security teams. Establishing multifaceted contexts around the evolving behaviours of threat actors is paramount for enabling threat hunting teams to tell the malicious from the benign. The MITRE ATT&amp;CK framework is the state-of-art knowledge base for referencing how threat actors conduct their tactics, techniques and procedures. Despite the abstract concepts of techniques being well defined, it is challenging to hunt from an abstract technique concept to security event data. In this work, we develop a data driven knowledge base of threat actor behaviours called Indicators of Behaviour, that use semantic reasoning to infer threat actor behaviours. Unlike generalised techniques in MITRE ATT&amp;CK, these behaviours can be queried from a low level indicator and the behaviour itself. We use MITRE’s Caldera platform to emulate threat actor behaviours and Sysmon for capturing security events and defining the knowledge base’s semantics. By utilising this approach, the semantic reasoner aids threat hunting teams by inferring threat actor behaviour chains from individual interconnected events.},
booktitle = {Proceedings of the 2024 European Interdisciplinary Cybersecurity Conference},
pages = {95–104},
numpages = {10},
keywords = {Caldera, MITRE ATT&amp;CK, TTP, Threat Actor Behaviour, Threat Hunting},
location = {Xanthi, Greece},
series = {EICC '24}
}

@inproceedings{10.1145/3411764.3445396,
author = {Zhang, Xiaoyu and Chandrasegaran, Senthil and Ma, Kwan-Liu},
title = {ConceptScope: Organizing and Visualizing Knowledge in Documents based on Domain Ontology},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445396},
doi = {10.1145/3411764.3445396},
abstract = {Current text visualization techniques typically provide overviews of document content and structure using intrinsic properties such as term frequencies, co-occurrences, and sentence structures. Such visualizations lack conceptual overviews incorporating domain-relevant knowledge, needed when examining documents such as research articles or technical reports. To address this shortcoming, we present ConceptScope, a technique that utilizes a domain ontology to represent the conceptual relationships in a document in the form of a Bubble Treemap visualization. Multiple coordinated views of document structure and concept hierarchy with text overviews further aid document analysis. ConceptScope facilitates exploration and comparison of single and multiple documents respectively. We demonstrate ConceptScope by visualizing research articles and transcripts of technical presentations in computer science. In a comparative study with DocuBurst, a popular document visualization tool, ConceptScope was found to be more informative in exploring and comparing domain-specific documents, but less so when it came to documents that spanned multiple disciplines.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {19},
numpages = {13},
keywords = {Knowledge Representation, Ontology, Visualization},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3347146.3359069,
author = {Li, Jingjing and Wang, Wenlu and Ku, Wei-Shinn and Tian, Yingtao and Wang, Haixun},
title = {SpatialNLI: A Spatial Domain Natural Language Interface to Databases Using Spatial Comprehension},
year = {2019},
isbn = {9781450369091},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347146.3359069},
doi = {10.1145/3347146.3359069},
abstract = {A natural language interface (NLI) to databases is an interface that translates a natural language question to a structured query that is executable by database management systems (DBMS). However, an NLI that is trained in the general domain is hard to apply in the spatial domain due to the idiosyncrasy and expressiveness of the spatial questions. Inspired by the machine comprehension model, we propose a spatial comprehension model that is able to recognize the meaning of spatial entities based on the semantics of the context. The spatial semantics learned from the spatial comprehension model is then injected to the natural language question to ease the burden of capturing the spatial-specific semantics. With our spatial comprehension model and information injection, our NLI for the spatial domain, named SpatialNLI, is able to capture the semantic structure of the question and translate it to the corresponding syntax of an executable query accurately. We also experimentally ascertain that SpatialNLI outperforms state-of-the-art methods.},
booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {339–348},
numpages = {10},
keywords = {Natural Language Interface, Spatial Data Science},
location = {Chicago, IL, USA},
series = {SIGSPATIAL '19}
}

@article{10.1145/3742891,
author = {Saleh, Alaa and Morabito, Roberto and Dustdar, Schahram and Tarkoma, Sasu and Pirttikangas, Susanna and Lov\'{e}n, Lauri},
title = {Towards Message Brokers for Generative AI: Survey, Challenges, and Opportunities},
year = {2025},
issue_date = {January 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3742891},
doi = {10.1145/3742891},
abstract = {In today’s digital world, GenAI is becoming increasingly prevalent by enabling unparalleled content generation capabilities for a wide range of advanced applications. This surge in adoption has sparked a significant increase in demand for data-centric GenAI models spanning the distributed edge-cloud continuum, placing increasing demands on communication infrastructures, highlighting the necessity for robust communication solutions. Central to this need are message brokers, which serve as essential channels for data transfer within various system components. This survey aims at delving into a comprehensive analysis of traditional and modern message brokers based on a variety of criteria, highlighting their critical role in enabling efficient data exchange in distributed AI systems. Furthermore, we explore the intrinsic constraints that the design and operation of each message broker might impose, highlighting their impact on real-world applicability. Finally, this study explores the enhancement of message broker mechanisms tailored to GenAI environments. It considers key factors such as scalability, semantic communication, and distributed inference that can guide future innovations and infrastructure advancements in the realm of GenAI data communication.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {20},
numpages = {37},
keywords = {Generative AI, message brokers, publish/subscribe paradigm, brokerless, edge computing, large language models}
}

@inproceedings{10.1145/3589335.3651500,
author = {Guan, Yong and Liu, Dingxiao and Ma, Jinchen and Peng, Hao and Wang, Xiaozhi and Hou, Lei and Li, Ru},
title = {Event GDR: Event-Centric Generative Document Retrieval},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651500},
doi = {10.1145/3589335.3651500},
abstract = {Generative document retrieval, an emerging paradigm in information retrieval, learns to build connections between documents and identifiers within a single model, garnering significant attention. However, there are still two challenges: (1) neglecting inner-content correlation during document representation; (2) lacking explicit semantic structure during identifier construction. Nonetheless, events have enriched relations and well-defined taxonomy, which could facilitate addressing the above two challenges. Inspired by this, we propose Event GDR, an event-centric generative document retrieval model, integrating event knowledge into this task. Specifically, we utilize an exchange-then-reflection method based on multi-agents for event knowledge extraction. For document representation, we employ events and relations to model the document to guarantee the comprehensiveness and inner-content correlation. For identifier construction, we map the events to well-defined event taxonomy to construct the identifiers with explicit semantic structure. Our method achieves significant improvement over the baselines on two datasets, and also hopes to provide insights for future research.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {975–978},
numpages = {4},
keywords = {event knowledge, generative document retrieval, large language model},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3349341.3349360,
author = {Zou, Xiaohui and Zou, Shunpeng and Wang, Xiaoqun},
title = {Smart System Studied: New Approaches to Natural Language Understanding},
year = {2019},
isbn = {9781450371506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349341.3349360},
doi = {10.1145/3349341.3349360},
abstract = {This paper aims to focus on the smart system as optimized expert knowledge acquisition system as new approaches to natural language understanding system. This method can finish fine processing for any text segment instantly. The module's precision machining can adopt big production method that combines on the line first, complete coverage and accurate grasp each language point and knowledge point and original point even their respective combination. Its characteristics are teachers and students can use the text analyzed method to do the fine processing of the same knowledge module, and only in Chinese or English, through the selection of keywords and terminology and knowledge modules that can be used as the menu to be selected as the way to achieve knowledge with the system. The result is the learning environment that enables human-computer collaboration system namely smart system to optimize the expert knowledge acquisition and the natural language understanding as a research field that has great significance to human beings. Its significance is that this learning environment software based on the National Excellent Courses by using the language chess with the feature of the introduction on the knowledge big production mode for the textual knowledge module finishing at Peking University.},
booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
pages = {1–6},
numpages = {6},
keywords = {Natural Language Understanding, Smart System Studied},
location = {Wuhan, Hubei, China},
series = {AICS 2019}
}

@inproceedings{10.1145/3408308.3427979,
author = {He, Fang and Xu, Cheng and Xu, Yanhui and Hong, Dezhi and Wang, Dan},
title = {EnergonQL: A Building Independent Acquisitional Query Language for Portable Building Analytics},
year = {2020},
isbn = {9781450380614},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408308.3427979},
doi = {10.1145/3408308.3427979},
abstract = {Emerging building analytics heavily rely on data-driven machine learning algorithms. However, writing these analytics is still challenging: developers not only need to know what data is required but also where this data is in each individual building when writing applications. To bridge this gap between analytics and the actual resources in buildings, we present EnergonQL, a building independent acquisitional data query language that extracts data for building analytics with a declarative query processor. EnergonQL provides logic views of building resources that universally apply to all buildings, thus allowing portable building analytics across buildings. We evaluate EnergonQL with four different building analytics and show that with EnergonQL the line-of-code and development efforts can be effectively reduced.},
booktitle = {Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {266–269},
numpages = {4},
keywords = {Declarative query language, Smart buildings, data analytics},
location = {Virtual Event, Japan},
series = {BuildSys '20}
}

@inproceedings{10.1145/3546157.3546164,
author = {Mohseni, Maryam and Maher, Mary Lou},
title = {A Framework for Exploring Computational Models of Novelty in Unstructured Text},
year = {2022},
isbn = {9781450396257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3546157.3546164},
doi = {10.1145/3546157.3546164},
abstract = {Novelty modeling in unstructured text data is a research topic within the Natural Language Processing (NLP) Community. Effective novelty models can play a key role in providing relevant and interesting content to the users which is the central goal in many applications including education and recommender systems. This paper presents a framework for comparing different approaches and applications of computational models of novelty in unstructured text data. We focus on computational models that apply methods such as natural language processing and information theory. The framework provides an ontology for computational novelty with respect to the source of text data, methods for representing the data, and models for measuring novelty. We explore the value of the framework by applying it to research on computational novelty in news articles, research publications, books, and recipes. This framework is independent of the type of data in the items and can be used as a tool for researchers to study, compare, and extend existing computational novelty models and applications.},
booktitle = {Proceedings of the 6th International Conference on Information System and Data Mining},
pages = {36–45},
numpages = {10},
keywords = {Computational models of novelty, NLP, Recommender systems, Surprise, Unstructured text},
location = {Silicon Valley, CA, USA},
series = {ICISDM '22}
}

@inproceedings{10.5555/3522802.3522965,
author = {Wilsdorf, Pia and Fischer, Nadine and Haack, Fiete and Uhrmacher, Adelinde M.},
title = {Exploiting provenance and ontologies in supporting best practices for simulation experiments: a case study on sensitivity analysis},
year = {2022},
publisher = {IEEE Press},
abstract = {Simulation studies are intricate processes and user support for conducting more consistent, systematic, and efficient simulation studies is needed. Simulation experiments as one crucial part of a simulation study can benefit from semi-automatic method selection, parameterization, and execution. However, this largely depends on the context in which the experiment is conducted. Context information about a simulation study can be provided in form of provenance that documents which artifacts contributed in developing a simulation model. We present an approach that exploits provenance to support best practices for simulation experiments. The approach relies on 1) explicitly specified provenance information, 2) an ontology of methods, 3) best practices rules, and 4) integration with a previously developed experiment generation pipeline. We demonstrate our approach by conducting a sensitivity analysis experiment within a cell biological simulation study.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {194},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3442381.3449881,
author = {Li, Jiaqi and Wu, Xuan and Lu, Chang and Deng, Wenxing and Zhao, Yizheng},
title = {Computing Views of OWL Ontologies for the Semantic Web},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449881},
doi = {10.1145/3442381.3449881},
abstract = {This paper tackles the problem of computing views of OWL ontologies using a forgetting-based approach. In traditional relational databases, a view is a subset of a database, whereas in ontologies, a view is more than a subset; it contains not only axioms contained in the original ontology, but may also contain newly-derived axioms entailed by the original ontology (implicitly contained in the original ontology). Specifically, given an ontology , the signature of is the set of all the names in , and a view of is a new ontology obtained from using only part of ’s signature, namely the target signature, while preserving all logical entailments up to the target signature. Computing views of OWL ontologies is useful for Semantic Web applications such as ontology-based query answering, in a way that the view can be used as a substitute of the original ontology to answer queries formulated with the target signature, and information hiding, in the sense that it restricts users from viewing certain information of an ontology. Forgetting is a form of non-standard reasoning concerned with eliminating from an ontology a subset of its signature, namely the forgetting signature, in such a way that all logical entailments are preserved up to the target signature. Forgetting can thus be used as a means for computing views of OWL ontologies — the solution of forgetting a set of names from an ontology is the view of for the target signature . In this paper, we present a forgetting-based method for computing views of OWL ontologies specified in the description logic , the basic extended with role hierarchy, nominals and inverse roles. The method is terminating and sound. Despite the method not being complete, an evaluation with a prototype implementation of the method on a corpus of real-world ontologies has shown very good success rates. This is very useful from the perspective of the Semantic Web, as it provides knowledge engineers with a powerful tool for creating views of OWL ontologies.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2624–2635},
numpages = {12},
keywords = {Description Logics, Forgetting, Ontology, Semantics Web},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3209914.3209940,
author = {Akg\"{u}n, Ayhan and Ayvaz, Serkan},
title = {An Approach for Information Discovery Using Ontology In Semantic Web Content},
year = {2018},
isbn = {9781450364218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209914.3209940},
doi = {10.1145/3209914.3209940},
abstract = {Information searching techniques are rapidly developing as the World Wide Web (WWW) evolves. Along with the development of information technologies, the need for acquiring domain knowledge bases, accessing data sources and discovering insights increases. The advancements in knowledge discovery, information management and artificial intelligence require faster data processing, storing more data and developing more intelligent applications. This study provides an information discovery and data integration approach for linked open data in the semantic web. Using semantics embedded in ontologies, data available in knowledge bases can be enhanced to better serve the information needs of users. The entity relationships between resources and resource hierarchies represented as linked open data in semantic web provide semantically rich insights about the data and facilitates knowledge discovery. Graph theory methods can be utilized to enrich the features of data sets in semantic web. In this study, we propose an approach for integrating isolated data sources with semantic web by using ontologies to make them available for information discovery and enhancing the features of semantic data by using graph theory techniques.},
booktitle = {Proceedings of the 1st International Conference on Information Science and Systems},
pages = {250–255},
numpages = {6},
keywords = {Graph, information retrieval, ontology, semantic web},
location = {Jeju, Republic of Korea},
series = {ICISS '18}
}

@article{10.1145/3447523,
author = {Maree, Mohammed and Rattrout, Amjad and Altawil, Muhanad and Belkhatir, Mohammed},
title = {Multi-modality Search and Recommendation on Palestinian Cultural Heritage Based on the Holy-Land Ontology and Extrinsic Semantic Resources},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3447523},
doi = {10.1145/3447523},
abstract = {The Cultural Heritage (CH) sector and its associated tourism services have been affected notably by the advancement of the Internet as well as the explosive growth of smartphones and other handheld devices. These days, visitors can access reliable CH content using Web and mobile-based interfaces. However, conventional CH systems still lack the ability to provide meaningful semantically overt results that precisely meet user information needs in this domain. In addition, they often ignore the user search context and experience, which hinders their ability to adapt their behavior to the preferences, tasks, interests, and other user functionalities. In this article, we aim to address the issue of designing a precision-oriented multilingual and multi-criteria semantic-based mobile recommender system specifically targeting Palestine's CH, a country with great historical and cultural importance. We aim to better facilitate users’ access to CH content by providing them with multiple search functionalities. In this context, a user can search for relevant information using keywords (a.k.a. tags) or sentence-like queries and the system retrieves all relevant documents based on their semantic similarity. A second option is to search using current location information to retrieve correlated historical places and events. Finally, starting from a picture of interest, a third option makes it possible to extract captions describing its content that can be used to search for additional contextually relevant information. Additionally, the proposed system aims at personalizing users’ experience through progressively delivering output that meets their information needs based on a number of parameters such as users' logging data, interests, previous searches, and location-based information. A prototype of the proposed system has been developed and tested using Android smartphones and a manually constructed ontology enriched with CH links to the Art \&amp; Architecture Thesaurus (AAT) and DBpedia. By comparing our system with similar systems in this domain, findings demonstrate that it provides additional search features and functionalities to users. The proposed Holy-Land ontology is the first of its kind attempting to encode knowledge about Palestine's CH. It plays a crucial role in our proposal, serving as a pivotal entity in the combination of language-based, location-based, and visual-based retrieval strategies.},
journal = {J. Comput. Cult. Herit.},
month = jul,
articleno = {29},
numpages = {23},
keywords = {Cultural heritage, content-based image retrieval, hybrid recommendation, knowledge-based search, manually constructed ontology, semantic similarity}
}

@inproceedings{10.1145/3433174.3433617,
author = {Thinyane, Mamello and Christine, Debora},
title = {SMART Citizen Cyber Resilience (SC2R) Ontology},
year = {2021},
isbn = {9781450387514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433174.3433617},
doi = {10.1145/3433174.3433617},
abstract = {Adverse cyber incidents are some of the top risks currently facing the global community. Cybersecurity frameworks and models formulated to mitigate these risks are typically framed from the organizational perspective of governments and the private sector. Further, they are traditionally techno-centric solutions framed from a cybersecurity perspective towards prevention of risks; only recently are they incorporating resilience perspectives towards anticipation of risks and positive adaptation during adverse cyber incidents. This research makes advances on human-centric cyber resilience - from the perspective of citizens and centered on citizens’ multi-dimensional experience of adverse cyber incidents. It considers the goal of cyber resilience, from the capabilitarian perspective, as enhancing the cyber capabilities of individuals and achieving positive adaptation in the face of adverse cyber incidents. This paper presents an ontology that is formulated to formalize individuals’ cyber resilience. The paper motivates the need for such an ontology and discusses the constitutive concepts. Finally, it shows how this ontology can support the realization of cyber resilience for individual citizens.},
booktitle = {13th International Conference on Security of Information and Networks},
articleno = {14},
numpages = {8},
keywords = {Cyber resilience, Cybersecurity, Human-centric cybersecurity, Ontologies},
location = {Merkez, Turkey},
series = {SIN 2020}
}

@inproceedings{10.5555/3463952.3463986,
author = {Bourahla, Yasser and Atencia, Manuel and Euzenat, J\'{e}r\^{o}me},
title = {Knowledge Improvement and Diversity under Interaction-Driven Adaptation of Learned Ontologies},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {When agents independently learn knowledge, such as ontologies, about their environment, it may be diverse, incorrect or incomplete. This knowledge heterogeneity could lead agents to disagree, thus hindering their cooperation. Existing approaches usually deal with this interaction problem by relating ontologies, without modifying them, or, on the contrary, by focusing on building common knowledge. Here, we consider agents adapting ontologies learned from the environment in order to agree with each other when cooperating. In this scenario, fundamental questions arise: Do they achieve successful interaction? Can this process improve knowledge correctness? Do all agents end up with the same ontology? To answer these questions, we design a two-stage experiment. First, agents learn to take decisions about the environment by classifying objects and the learned classifiers are turned into ontologies. In the second stage, agents interact with each other to agree on the decisions to take and modify their ontologies accordingly. We show that agents indeed reduce interaction failure, most of the time they improve the accuracy of their knowledge about the environment, and they do not necessarily opt for the same ontology.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {242–250},
numpages = {9},
keywords = {knowledge diversity, multi-agent learning, multi-agent social simulation, ontologies},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3704268.3742700,
author = {Hussein, Hassan and Oelen, Allarad and Auer, S\"{o}ren},
title = {A Hybrid, Neuro-symbolic Approach for Scholarly Knowledge Organization},
year = {2025},
isbn = {9798400713514},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704268.3742700},
doi = {10.1145/3704268.3742700},
abstract = {The rapid development of generative AI leveraging neural models, particularly with the introduction of large language models (LLMs), has fundamentally advanced natural language processing and generation. However, such neural models are non-deterministic, opaque, and tend to confabulate. Knowledge Graphs (KGs) on the other hand contain factual information represented in a symbolic way for humans and machines following formal knowledge representation formalisms. However, the creation and curation of KGs is time-consuming, cumbersome, and resource-demanding. A key research challenge now is how to synergistically combine both formalisms with the human in the loop (Hybrid AI) to obtain structured and machine-processable knowledge in a scalable way. We introduce an approach for a tight integration of Humans, Neural Models (LLM), and Symbolic Representations (KG) for the semiautomatic creation and curation of Scholarly Knowledge Graphs. Our approach, while demonstrated in the scholarly context, establishes generalizable principles for neuro-symbolic integration that can be adapted to other domains. We implement and integrate our approach comprising an intelligent user interface and prompt templates for interaction with an LLM in the Open Research Knowledge Graph. We perform a thorough analysis of our approach and implementation with a user evaluation to assess the merits of the neuro-symbolic, hybrid approach for organizing scholarly knowledge.},
booktitle = {Proceedings of the 2025 ACM Symposium on Document Engineering},
articleno = {16},
numpages = {10},
location = {Nottingham, United Kingdom},
series = {DocEng '25}
}

@article{10.14778/3565838.3565850,
author = {Bellomarini, Luigi and Benedetto, Davide and Brandetti, Matteo and Sallinger, Emanuel},
title = {Exploiting the Power of Equality-Generating Dependencies in Ontological Reasoning},
year = {2022},
issue_date = {September 2022},
publisher = {VLDB Endowment},
volume = {15},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3565838.3565850},
doi = {10.14778/3565838.3565850},
abstract = {Equality-generating dependencies (EGDs) allow to fully exploit the power of existential quantification in ontological reasoning settings modeled via Tuple-Generating Dependencies (TGDs), by enabling value-assignment or forcing the equivalence of fresh symbols. These capabilities are at the core of many common reasoning tasks, including graph traversals, clustering, data matching and data fusion, and many more related real-world scenarios.However, the interplay of TGDs and EGDs is known to lead to undecidability or intractability of query answering in tractable Datalog+/- fragments, like Warded Datalog+/-, for which, in the sole presence of TGDs, query answering is PTIME in data complexity. Restrictions of equality constraints, like separable EGDs, have been studied, but all achieve decidability at the cost of limited expressive power, which makes them unsuitable for the mentioned tasks.This paper introduces the class of "harmless" EGDs, that subsume separable EGDs and allow to model a very broad class of tasks. We contribute a sufficient syntactic condition for testing harmlessness, an undecidable task in general. We argue that in Warded Datalog+/- with harmless EGDs, ontological reasoning is decidable and PTIME. From such theoretical underpinnings, we develop novel chase-based techniques for reasoning with harmless EGDs and present an implementation within the Vadalog system, a state-of-the-art Datalog-based reasoner. We provide full-scale experimental evaluation and comparative analysis.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {3976–3988},
numpages = {13}
}

@inproceedings{10.1145/3357766.3359541,
author = {Seifer, Philipp and H\"{a}rtel, Johannes and Leinberger, Martin and L\"{a}mmel, Ralf and Staab, Steffen},
title = {Empirical study on the usage of graph query languages in open source Java projects},
year = {2019},
isbn = {9781450369817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357766.3359541},
doi = {10.1145/3357766.3359541},
abstract = {Graph data models are interesting in various domains, in part because of the intuitiveness and flexibility they offer compared to relational models. Specialized query languages, such as Cypher for property graphs or SPARQL for RDF, facilitate their use. In this paper, we present an empirical study on the usage of graph-based query languages in open-source Java projects on GitHub. We investigate the usage of SPARQL, Cypher, Gremlin and GraphQL in terms of popularity and their development over time. We select repositories based on dependencies related to these technologies and employ various popularity and source-code based filters and ranking features for a targeted selection of projects. For the concrete languages SPARQL and Cypher, we analyze the activity of repositories over time. For SPARQL, we investigate common application domains, query use and existence of ontological data modeling in applications that query for concrete instance data. Our results show, that the usage of graph query languages in open-source projects increased over the last years, with SPARQL and Cypher being by far the most popular. SPARQL projects are more active in terms of query related artifact changes and unique developers involved, but Cypher is catching up. Relatively few applications use SPARQL to query for concrete instance data: A majority of those applications employ multiple different ontologies, including project and domain specific ones. Common application domains are management systems and data visualization tools.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {152–166},
numpages = {15},
keywords = {Cypher, Empirical Study, GitHub, GraphQL, Graphs, Gremlin, Query Languages, SPARQL},
location = {Athens, Greece},
series = {SLE 2019}
}

@article{10.1145/3549553,
author = {Kirchhof, J\"{o}rg Christian and Kleiss, Anno and Rumpe, Bernhard and Schmalzing, David and Schneider, Philipp and Wortmann, Andreas},
title = {Model-driven Self-adaptive Deployment of Internet of Things Applications with Automated Modification Proposals},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3549553},
doi = {10.1145/3549553},
abstract = {Today’s Internet of Things (IoT) applications are mostly developed as a bundle of hardware and associated software. Future cross-manufacturer app stores for IoT applications will require that the strong coupling of hardware and software is loosened. In the resulting IoT applications, a quintessential challenge is the effective and efficient deployment of IoT software components across variable networks of heterogeneous devices. Current research focuses on computing whether deployment requirements fit the intended target devices instead of assisting users in successfully deploying IoT applications by suggesting deployment requirement relaxations or hardware alternatives. This can make successfully deploying large-scale IoT applications a costly trial-and-error endeavor. To mitigate this, we have devised a method for providing such deployment suggestions based on search and backtracking. This can make deploying IoT applications more effective and more efficient, which, ultimately, eases reducing the complexity of deploying the software surrounding us.},
journal = {ACM Trans. Internet Things},
month = sep,
articleno = {30},
numpages = {30},
keywords = {Internet of Things, deployment, model-driven engineering, architecture description languages}
}

@inproceedings{10.1145/3373722.3373782,
author = {Vinogradov, Andrei and Kurshev, Evgeny and Vlasova, Natalia and Podobryaev, Alexey},
title = {Information extraction tasks in public administration domain: ISIDA-T natural language processing system},
year = {2020},
isbn = {9781450376709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373722.3373782},
doi = {10.1145/3373722.3373782},
abstract = {This article represents approaches of the artificial intelligence methods, used in public administration. An overview of various technologies of artificial intelligence applications in the field of public administration and related fields is given. All of these research directions are particularly relevant to the task of digital technologies (including artificial intelligence) growth to create an efficient and competitive digital economics in Russia. Among the modern intellectual technologies that allow solving the widest range of tasks, an important role plays technologies related to natural language text processing - nonstructured NL-texts are essential segment of data, used for analysis and decision-making tasks (in terms of data volume, of course, video data have a leading position, however, they are usually suitable for solving tactical but not strategic management tasks). The article provides an overview of the existing methods of natural language processing and their practical application to the tasks of public administration. An integrated approach to the natural language processing tools using for solving practical problems in the field of public administration is considered on the example of the ISIDA-T system for extracting information from natural language texts, developed at the Artificial Intelligence Research Center PSI RAS. The system under consideration is distinguished by a modular approach to the pre-processing of unstructured text and the possibility of manual adjustment for a specific extraction task. This technological solution gives the necessary flexibility and ease of use. The system consists of configurable text preprocessing, linguistic analysis, target information retrieval and output of the results in user-friendly form modules. One of the important components of the system is an integrated knowledge resource that allows quickly and efficiently adjust the system to the specifics of the relevant subject area. An approach to the application of the ISIDA-T system to the task of fact information extraction (as the most important for analyzing the situation and subsequent management decisions) is proposed using the example of information extraction about resignations and appointments from news feeds.},
booktitle = {Proceedings of the XI International Scientific Conference Communicative Strategies of the Information Society},
articleno = {15},
numpages = {10},
keywords = {Artificial intelligence, Digital economy, Information extraction, Natural language processing, Public administration},
location = {St. Petersburg, Russian Federation},
series = {CSIS'2019}
}

@inproceedings{10.1145/3372782.3406258,
author = {McGill, Monica M. and Decker, Adrienne},
title = {Construction of a Taxonomy for Tools, Languages, and Environments across Computing Education},
year = {2020},
isbn = {9781450370929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372782.3406258},
doi = {10.1145/3372782.3406258},
abstract = {The sheer number of tools, languages, and environments (TLEs) used in computing education has proliferated in the last few years as more tools are developed to meet new demands of the growing amount of K-12 computing education that has been undertaken. However, there is little formalized language at either the K-12 or post-secondary level that provides for a way to classify these TLEs for discussing research and for classifying in databases.In this research study, we step through a formal process for building a taxonomy for TLEs. As part of the supporting research, we first discuss the importance of taxonomies and classification systems in computing education, provide a formal method for building a taxonomy, and provide working definitions of TLEs based on previous literature. This is followed by a systematic literature review using a widely-accepted methodology for finding articles that have examined TLEs in primary, secondary, and post-secondary computing education. This literature review focuses on studies that looked at multiple TLEs and specifically attempted to classify or categorize them. We then propose a new taxonomy for classifying TLEs and provide definitions and samples for each category. This is followed by a discussion of the next steps in vetting the taxonomy and the challenges and issues that need to be considered when evaluating it for classifying TLEs in computing education.},
booktitle = {Proceedings of the 2020 ACM Conference on International Computing Education Research},
pages = {124–135},
numpages = {12},
keywords = {classification, computing, education, environments, k-12, languages, literature review, ontology, post-secondary, primary, secondary, taxonomy, tools},
location = {Virtual Event, New Zealand},
series = {ICER '20}
}

@inproceedings{10.5555/3522802.3522970,
author = {Shuttleworth, David and Padilla, Jose J.},
title = {Towards semi-automatic model specification},
year = {2022},
publisher = {IEEE Press},
abstract = {This paper presents a natural language understanding (NLU) approach to transition a description of a phenomenon towards a simulation specification. As multidisciplinary endeavors using simulations increase, the need for teams to better communicate and make non-modelers active participants on the process increases. We focus on semi-automating the model conceptualization process towards the creation of a specification as it is one of the most challenging steps in collaborations. The approach relies on NLU processing of narratives, create a model that captures concepts and relationships, and finally provide a specification of a simulation implementation. An initial definition set and grammatical rules are proposed to formalize this process. These are followed by a Design of Experiments (DoE) to test the NLU model accuracy and a test case that generates Agent-Based Model (ABM) conceptualizations and specifications. We provide a discussion on the advantages and limitations of using NLUs for model conceptualization and specification processes.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {199},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3651671.3651773,
author = {Sore, Safiatou S. and Ouedraogo, T. Frederic T.Fr\'{e}dric and Bikienga, Moustapha M. and Traore, Yaya Y.},
title = {Towards a More Generic and Elastic Metadata Management Model in a Data Lake Environment},
year = {2024},
isbn = {9798400709234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3651671.3651773},
doi = {10.1145/3651671.3651773},
abstract = {The evolution of the vast amount of heterogeneous data sources is leading to the emergence of several new concepts. One of the best-known concepts that is emerging as a new and trending topic in the big data space is the data lake. This is a central repository that stores heterogeneous data sources in their native format, without any predefined schema. In the absence of an enforced schema, effective metadata management based on metadata models remains an active research topic to address the problems associated with the data lake: the "data swamp". The analysis of existing metadata models shows that there is no comprehensive model among them. In this paper, we present a generic and scalable metadata model, which refers to the ability to dynamically provision computing resources based on demand and to resize resources as needed during metadata integration. Our approach will be based on a functional architecture of the data lake, along with a set of features that promote the generality of the metadata model.CCS CONCEPTS: Information systems→ Data management systems→ Information integration → Entity resolution},
booktitle = {Proceedings of the 2024 16th International Conference on Machine Learning and Computing},
pages = {44–51},
numpages = {8},
keywords = {data lake, elasticity, metadata, scalability},
location = {Shenzhen, China},
series = {ICMLC '24}
}

@inproceedings{10.1145/3404835.3462904,
author = {Zhang, Chiqun and Evans, Michael R. and Lepikhin, Max and Yankov, Dragomir},
title = {Fast Attention-based Learning-To-Rank Model for Structured Map Search},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3462904},
doi = {10.1145/3404835.3462904},
abstract = {Recent works show that Transformer-based learning-to-rank (LTR) approaches can outperform previous well-established ranking methods, such as gradient-boosted decision trees (GBDT), on document and passage re-ranking problems. A common assumption in these works is that the query and the result documents are comprised of purely textual information without explicit structure. In map search, the relevance of results is determined based on rich heterogeneous features - textual features derived from the query and the results, geospatial features such as proximity of a result to the user, structured features reflecting the address format of the result, and the perceived structure of the query. In this work, we propose a novel deep neural network LTR architecture, capable of seamlessly handling heterogeneous inputs, similar to GBDT-based methods. At the same time, unlike GBDT, the architecture does not require human input via (numerous) carefully-crafted features. Instead, features are inferred through a self-attention mechanism. Our model implements two lightweight attention layers optimized for ranking: the first layer computes query-result similarities, the second implements listwise ranking inference. We perform evaluation on several single language and one multilingual dataset. Our model outperforms by a wide margin other Transformer-based ranking architectures and has equal or better performance than GBDT models. Equally important, runtime inference is orders of magnitude faster than other Transformer architectures, significantly reducing hardware serving costs. The model is a low-cost alternative suitable to power ranking in industrial map search engines across a variety of languages and markets.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {942–951},
numpages = {10},
keywords = {geocoding, information retrieval, language understanding, map search, ranking system},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3543873.3587659,
author = {Donald, Andy and Galanopoulos, Apostolos and Curry, Edward and Mu\~{n}oz, Emir and Ullah, Ihsan and Waskow, M. A. and Dabrowski, Maciej and Kalra, Manan},
title = {Towards a Semantic Approach for Linked Dataspace, Model and Data Cards},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587659},
doi = {10.1145/3543873.3587659},
abstract = {The vast majority of artificial intelligence practitioners overlook the importance of documentation when building and publishing models and datasets. However, due to the recent trend in the explainability and fairness of AI models, several frameworks have been proposed such as Model Cards, and Data Cards, among others, to help in the appropriate re-usage of those models and datasets. In addition, because of the introduction of the dataspace concept for similar datasets in one place, there is potential that similar Model Cards, Data Cards, Service Cards, and Dataspace Cards can be linked to extract helpful information for better decision-making about which model and data can be used for a specific application. This paper reviews the case for considering a Semantic Web approach for exchanging Model/Data Cards as Linked Data or knowledge graphs in a dataspace, making them machine-readable. We discuss the basic concepts and propose a schema for linking Data Cards and Model Cards within a dataspace. In addition, we introduce the concept of a dataspace card which can be a starting point for extracting knowledge about models and datasets in a dataspace. This helps in building trust and reuse of models and data among companies and individuals participating as publishers or consumers of such assets.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {1468–1473},
numpages = {6},
keywords = {AI Documentation, Data Cards, Dataspace Cards, Model Cards, Semantic Web, Service Cards},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1145/3550356.3561581,
author = {Vanommeslaeghe, Yon and Ceulemans, David and van Acker, Bert and Denil, Joachim and Derammelaere, Stijn and De Meulenaere, Paul},
title = {Validation and uncertainty in model-based design space exploration: an experience report},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561581},
doi = {10.1145/3550356.3561581},
abstract = {Model-based systems engineering (MBSE) techniques can help manage the growing complexity in the design and development of cyber-physical systems, and can even allow for the optimization of a system under design in simulation. However, models are always an abstraction of the real-world systems they represent. This introduces uncertainty at the model level, which affects the validity of simulation results, and thus also the results of the optimization. This, together with variations in real-world system parameters, significantly complicates the validation of simulation and optimization results. In this experience report, we first use a descriptive process model to describe our efforts to validate the results of a model-based design space exploration (DSE) process given this uncertainty. After this, we discuss lessons learned and insights gained, and identify future challenges. We present a possible prescriptive process model for future validation efforts, which specifically takes into account uncertainty.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {702–711},
numpages = {10},
keywords = {cyber-physical systems, design space exploration, model-based systems engineering, uncertainty, validation},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3412841.3441942,
author = {Teixeira, Milene Santos and Maran, Vin\'{\i}cius and Dragoni, Mauro},
title = {The interplay of a conversational ontology and AI planning for health dialogue management},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441942},
doi = {10.1145/3412841.3441942},
abstract = {Health dialogue systems are required to respect some special requirements such as predictability and reliability. While knowledge based approaches still seem to be the most appropriate for these systems, the automated generation of reliable policies remains an open problem. This work proposes an approach that integrates a conversational ontology (Convology) and Artificial Intelligence planning with the aim of automating the generation of a dialogue manager capable of handling goal-oriented dialogues for the health domain. The resulting dialogue manager is aimed to be integrated into a suitable architecture that provides the natural language components. We illustrate our approach by describing how it has been implemented into PuffBot, a multi-turn goal-oriented conversational agent for supporting patients affected by asthma.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {611–619},
numpages = {9},
keywords = {automated planning, conversational ontology, dialogue management, health dialogue},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3366030.3366111,
author = {Jirkovsk\'{y}, V\'{a}clav and \v{S}ebek, Ond\v{r}ej and Kadera, Petr and Burget, Pavel and Knoch, S\"{o}nke and Becker, Tilman},
title = {Facilitation of Domain-Specific Data Models Design using Semantic Web Technologies for Manufacturing},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366111},
doi = {10.1145/3366030.3366111},
abstract = {Modern manufacturing faces a challenge of integrating data models from various sources/domains which may differ both semantically and technically when particular domain specific data models are designed by different users and stored in different formats. This paper introduces an approach for facilitating the design of domain-specific data models using semantic web technologies. In this approach, all the information required for managing the production (including a description of a product, processes involved in the production, and existing resources and their specifications) is captured in an ontology. The proposed Product, Process, and Resource (PPR) ontology defines fundamental conceptualization of the production that can be easily applied to the arbitrary domain. Application of the PPR ontology is demonstrated in the case of simple truck assembling by means of robots. Capturing the knowledge in the form of ontology provides the advantage of employing supporting tools such as reasoners for consistency checking or query languages for information extraction. The paper demonstrates the utilization of SQWRL for searching resources suitable to manipulate given truck parts on the basis of semantic matching between properties of particular elements.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {649–653},
numpages = {5},
keywords = {Data Model Design, Industrial Automation, Ontology, Semantic Matchmaking},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3437120.3437311,
author = {E. Samaridi, Nikoletta and N. Karanikolas, Nikitas and C. Papakitsos, Evangelos and Papoutsidakis, Michail},
title = {Designing a Greek Electronic Dictionary based on Ontology},
year = {2021},
isbn = {9781450388979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437120.3437311},
doi = {10.1145/3437120.3437311},
abstract = {In this paper we examine the design of a conceptual dictionary of the modern Greek language with intelligent features, which will offer possibilities of semantic integration and interoperability in an automatic and secure way, connecting heterogeneous systems and approaches in the field of engineering and technologies under the light of a standard and standardized organization and coding of data that come from structured and semi-structured information sources.},
booktitle = {Proceedings of the 24th Pan-Hellenic Conference on Informatics},
pages = {223–225},
numpages = {3},
keywords = {binary relations, concepts, dictionary, hierarchy, inheritance, interoperability, ontology, semantic integration, taxonomy},
location = {Athens, Greece},
series = {PCI '20}
}

@article{10.1145/3557890,
author = {Cunningham, Jay and Benabdallah, Gabrielle and Rosner, Daniela and Taylor, Alex},
title = {On the Grounds of Solutionism: Ontologies of Blackness and HCI},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3557890},
doi = {10.1145/3557890},
abstract = {Why is the solution the end point to a problem? While many in HCI and design have examined the impulse to solve problems–the solutionist or techno-solutionist mindset–we examine the logic that binds the solution and the problem together as a pair. Focusing on the timely and consequential problem of systemic racial injustice, we think through the paradoxical possibility that the pairing of the problem and solution (so often treated as the default in design and HCI) perpetuates the very conditions we seek to improve. With Calvin Warren’s profound Afro-pessimism, we recognize how the tools used to solve structural inequities around Black life are constructed with inequities themselves. The problem-solution, therefore, is a dead end. We use this paradox as an invitation to rethink ongoing efforts to seek equity and justice more broadly, setting out a fragile but hopeful path for HCI and design.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
articleno = {20},
numpages = {17},
keywords = {Solutionism, solutions, design problems, theory}
}

@article{10.1145/3402440,
author = {Goy, Annamaria and Colla, Davide and Magro, Diego and Accornero, Cristina and Loreto, Fabrizio and Radicioni, Daniele Paolo},
title = {Building Semantic Metadata for Historical Archives through an Ontology-driven User Interface},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3402440},
doi = {10.1145/3402440},
abstract = {Historical archives represent an immense wealth, the potential of which is endangered by the lack of effective management and access tools. We believe that this issue can be faced by providing archive catalogs with a semantic layer, containing rich semantic metadata, representing the content of documents in a full-fledged formal machine-readable format. In this article, we present the contribution offered in this direction by the PRiSMHA project, in which the conceptual vocabulary of the semantic layer is represented by computational ontologies. However, acquiring semantic knowledge represents a well-known bottleneck for knowledge-based systems; to solve this problem, PRiSMHA relies on a crowdsourcing collaborative model, i.e., an online community of users who collaborate in building semantic representations of the content of archival documents. In this perspective, this article aims at answering the following research question: Starting from the axioms characterizing concepts in the computational ontology underlying the system, how can we derive a user interface enabling users to formally represent the content of archival documents by exploiting the conceptual vocabulary provided by the ontology?Our solution includes the following steps: (a) a manually defined configuration, acting as a pre-filter, to hide “unsuited” classes, properties, and relations; (b) an algorithm, combining heuristics and reasoning, which extracts from the ontology all and only the “compatible” properties and relations, given an entity (event) type; and (c) a set of strategies to rank, group, and present the entity (event) properties and relations, based on the results of a study with users. This integrated solution enabled us to design an ontology-driven user interface enabling users to characterize entities, and in particular (historical) events, on the basis of the vocabulary provided by the ontology.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {25},
numpages = {36},
keywords = {Ontology-driven user interfaces, computational ontologies, crowdsourcing platform, historical archives}
}

@inproceedings{10.1145/3410566.3410610,
author = {Zorgati, Hela and Djemaa, Raoudha Ben and Amor, Ikram Amous Ben and Sedes, Florence},
title = {QoC enhanced semantic IoT model},
year = {2020},
isbn = {9781450375030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410566.3410610},
doi = {10.1145/3410566.3410610},
abstract = {The miniaturization of computers, coupled with a constant increase in computing power, led to the emergence of new sources of context information. We are facing a new paradigm, the Internet of Things (IoT). Today, this latter improves the quality of life in multiple areas. However, the heterogeneity of objects used in such environments makes their interoperability difficult. In addition, the observations produced by context providers (connected objects) are generated with different vocabularies and data formats. This heterogeneity of technologies in the IoT world makes it necessary to adopt generic solutions. Therefore, it is important to transform the raw data from these context producers into knowledge and information based on ontologies. The use of ontologies solves the challenges of heterogeneity and interoperability of IoT systems. In this paper, we propose a semantic IoT model that aims to overcome the semantic interoperability challenges introduced by the variety of objects potentially used in IoT systems. Furthermore, we enhanced this ontology with quality of context meta-data. These meta-data helps in dealing with imperfection and inconsistency of the collected IoT data.},
booktitle = {Proceedings of the 24th Symposium on International Database Engineering \&amp; Applications},
articleno = {5},
numpages = {7},
keywords = {QoC meta-data, internet of things, ontology reuse, semantic model},
location = {Seoul, Republic of Korea},
series = {IDEAS '20}
}

@inproceedings{10.5555/3373669.3373686,
author = {Finidori, Helene},
title = {Configuring patterns and pattern languages for systemic inquiry and design},
year = {2020},
publisher = {The Hillside Group},
address = {USA},
abstract = {This paper builds on work relating to pattern languages for social change, such as in the papers titled Fourth generation pattern languages - patterns as epistemic threads for systemic orientation, and Pattern Literacy in support of Systems Literacy presented to the Systems Science and Pattern Language communities between 2015 and 2017.It is part of an endeavor to bring pattern thinking and systems thinking, or pattern science and systems science, closer to each other, in order to further introduce pattern thinking and pattern language in the design, assessment and orientation of our socio-technological and socioenvironmental systems, large or small, to better address the societal issues of our time. It complements several initiatives to put pattern languages at the service of sustainability and societal change, and to introduce pattern thinking and pattern language into systems thinking and systemic design.My broader aim is to enhance the innate patterning capability of human beings and thus an overall pattern literacy in support of systems literacy. Pattern literacy manifests our ability to grasp, learn, assemble, represent and mobilize patterns to make-sense of, converse about and shape our world(s). Systems literacy manifests our ability to interrogate and attempt to understand the relationships among systems wholes and parts, and the mechanisms that affect and shape our world(s), in part or as a whole.In this paper, I explore how a systemic approach to patterns and pattern language could support systemic inquiry and systemic design, and more generally the advancement of pattern language.In particular, I discuss the extension of the act of design to encompass the systemic inquiry that motivates a design and the on-going monitoring of the fitness of a design to its intended purpose. I examine the multiple facets and understandings of the concept of pattern and show how they can be reconciled to include both the inquiry or observational/informational aspects and the design aspects of patterns in a larger systems framework. In this light, I reexamine the appropriateness of the pattern expressed in problem-solution form in the context of complex systems, and the notion of generativity, and I propose ways forward for extended definitions and pattern forms.},
booktitle = {Proceedings of the 25th Conference on Pattern Languages of Programs},
articleno = {18},
numpages = {32},
keywords = {action research, boundary objects, complex adaptive modeling, complex systems, participatory inquiry, pattern language, pattern literacy, semiotics},
location = {Portland, Oregon},
series = {PLoP '18}
}

@article{10.1145/3641552,
author = {Tseng, Tiffany and Davidson, Matt J. and Morales-Navarro, Luis and Chen, Jennifer King and Delaney, Victoria and Leibowitz, Mark and Beason, Jazbo and Shapiro, R. Benjamin},
title = {Co-ML: Collaborative Machine Learning Model Building for Developing Dataset Design Practices},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
url = {https://doi.org/10.1145/3641552},
doi = {10.1145/3641552},
abstract = {Machine learning (ML) models are fundamentally shaped by data, and building inclusive ML systems requires significant considerations around how to design representative datasets. Yet, few novice-oriented ML modeling tools are designed to foster hands-on learning of dataset design practices, including how to design for data diversity and inspect for data quality. To this end, we outline a set of four data design practices (DDPs) for designing inclusive ML models and share how we designed a tablet-based application called Co-ML to foster learning of DDPs through a collaborative ML model building experience. With Co-ML, beginners can build image classifiers through a distributed experience where data is synchronized across multiple devices, enabling multiple users to iteratively refine ML datasets in discussion and coordination with their peers. We deployed Co-ML in a 2-week-long educational AIML Summer Camp, where youth ages 13–18 worked in groups to build custom ML-powered mobile applications. Our analysis reveals how multi-user model building with Co-ML, in the context of student-driven projects created during the summer camp, supported development of DDPs including incorporating data diversity, evaluating model performance, and inspecting for data quality. Additionally, we found that students’ attempts to improve model performance often prioritized learnability over class balance. Through this work, we highlight how the combination of collaboration, model testing interfaces, and student-driven projects can empower learners to actively engage in exploring the role of data in ML systems.},
journal = {ACM Trans. Comput. Educ.},
month = apr,
articleno = {25},
numpages = {37},
keywords = {Machine learning, collaboration, computing education, data science}
}

@inproceedings{10.1145/3275245.3275269,
author = {Renault, Laylla D.C. and Barcellos, Monalessa Perini and de Almeida Falbo, Ricardo},
title = {Using an Ontology-based Approach for Integrating Applications to support Software Processes},
year = {2018},
isbn = {9781450365659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275245.3275269},
doi = {10.1145/3275245.3275269},
abstract = {Software organizations use several applications to support their software processes. To properly support the software processes, applications should be integrated at different layers (data, service, and process). Moreover, the integration should cover semantic aspects. Therefore, an approach that provides guidelines on how to perform integration at different layers addressing semantic aspects can be helpful. This paper presents an extension of the Ontology-based Approach for Semantic Integration (OBA-SI), focusing on semantic integration at process layer. This extension establishes relationships between integration at data, service and process layers, and uses task ontologies and a process ontology to guide integration at process layer. It was used to provide an integrated solution involving applications supporting the Issue Management and Software Configuration Management processes.},
booktitle = {Proceedings of the XVII Brazilian Symposium on Software Quality},
pages = {220–229},
numpages = {10},
keywords = {Ontology, process integration, semantics, system integration},
location = {Curitiba, Brazil},
series = {SBQS '18}
}

@inproceedings{10.1145/3703187.3703290,
author = {Ma, Xiangfei and Li, Lin},
title = {Geological Disaster Named Entity Recognition with Small Samples Based on Data Augmentation and Prompt Engineering},
year = {2024},
isbn = {9798400707254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3703187.3703290},
doi = {10.1145/3703187.3703290},
abstract = {This paper uses a large language model to perform generative data enhancement on the original small sample data by performing random synonym replacement and random mask filling operations. In accordance with the reasoning logic of the large language model, three prompt templates are designed and the reasons are explored. Experiments show that when the parameters remain unchanged, the data enhanced by this method has been greatly improved under the three prompt templates, alleviating the difficulty of low resources of geological disaster data. And by comparing the performance of different instructions under different learning rates, the fine-tuning learning rate range suitable for the field of geological disasters is summarized. The limitation is that it is constrained by local computing resources, which reduces the parameter scale of LLM, and the recognition performance is low for extremely long or complex texts.},
booktitle = {Proceedings of the 2024 7th International Conference on Computer Information Science and Artificial Intelligence},
pages = {613–617},
numpages = {5},
keywords = {Data Augmentation, Geological Disasters, LLMs, Named Entity Recognition, Prompt Engineering},
location = {
},
series = {CISAI '24}
}

@inproceedings{10.1145/3347317.3357245,
author = {Lagrue, Sylvain and Chetcuti-Sperandio, Nathalie and Delorme, Fabien and Thi, Chau Ma and Thi, Duyen Ngo and Tabia, Karim and Benferhat, Salem},
title = {An Ontology Web Application-based Annotation Tool for Intangible Culture Heritage Dance Videos},
year = {2019},
isbn = {9781450369107},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3347317.3357245},
doi = {10.1145/3347317.3357245},
abstract = {Collecting dance videos, preserving and promoting them after enriching the collected data has been significant actions in preserving Intangible culture heritage in South-East Asia. Whereas techniques for the conceptual modeling of the expressive semantics of dance videos are very complex, they are crucial to exploit effectively the video semantics. This paper proposes an ontology web-based dance video annotation system for representing the semantics of dance videos at different granularity levels. Especially, the system incorporates both syntactic and semantic features of pre-built dance ontology system in order to not only use the available semantic web system but also to create unity for users when annotating videos to minimize conflicts.},
booktitle = {Proceedings of the 1st Workshop on Structuring and Understanding of Multimedia HeritAge Contents},
pages = {75–81},
numpages = {7},
keywords = {inconsistency-tolerant query answering, knowledge representation, ontologies, video annotation},
location = {Nice, France},
series = {SUMAC '19}
}

@inproceedings{10.1145/3712716.3712719,
author = {Skipanes, Mads and Pratama, Nardiena and Porter, Kyle and Demartini, Gianluca},
title = {Fast Synthetic Data Generation for Case-Specific Entity Extraction in Criminal Investigations},
year = {2025},
isbn = {9798400710766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712716.3712719},
doi = {10.1145/3712716.3712719},
abstract = {In major criminal investigations, the manual analysis of police reports for the categorization of entities is a resource-intensive task prone to human error. Recent advances in Named Entity Recognition (NER) models offer promising solutions for automating this process, potentially reducing both time and error rates.This paper demonstrates the effectiveness of fine-tuning a NER model using a publicly shared synthetic dataset inspired by real case files. Notably, we leverage a large language model (LLM) for generating both the synthetic data and the annotations used for training. This approach enables investigators to rapidly develop case-specific models tailored to ongoing investigations. To structure this effort, we propose an ontology for entity extraction in criminal cases, focusing on key entities, such as persons, seized items, communication profiles, vehicles, locations, organizations, and financial profiles. Our model achieves an average weighted F1-score of 94.2\% on the synthetic dataset.For further validation, we manually annotated a small dataset of confidential data from two homicide cases, achieving an average weighted F1-score of 81.6\%. Our results demonstrate that our approach can at times generalize well to real case files.},
booktitle = {Proceedings of the Digital Forensics Doctoral Symposium},
articleno = {2},
numpages = {8},
keywords = {Criminal Investigation, Artificial Intelligence, Named Entity Recognition, Large Language Models, Synthetic Data, Data Management, Information Analysis.},
location = {
},
series = {DFDS '25}
}

@inproceedings{10.1145/3307363.3307381,
author = {Liu, Bin and Wu, Junfeng and Yao, Li and Ding, Zheyuan},
title = {Ontology-based Fault Diagnosis: A Decade in Review},
year = {2019},
isbn = {9781450366199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307363.3307381},
doi = {10.1145/3307363.3307381},
abstract = {Fault diagnosis is a critical activity in the comprehensive support for equipment operation. Relevant research about it is becoming more and more popular. Ontology-based fault diagnosis is an important method for diagnosing faults. Due to the expressive ability, knowledge sharing and knowledge reuse based on deep semantics, and supporting for logical reasoning, ontology-based fault diagnosis has received more and more attention from researchers in the past decade. The fault diagnosis ontology describes the core concepts involved in diagnosing faults and the relationship among the concepts. Its quality and usage determine the efficiency and effectiveness of fault diagnosis, thus has a great impact on fault diagnosis. However, its knowledge source and usage have not been analyzed comprehensively to give us a deep insight into this area yet. This paper investigates and studies the ontology-based fault diagnosis during the past decade, from the perspective of knowledge source and usage of fault diagnosis ontology. In summary, it can be seen that combining ontology with other methods improves the efficiency and the effect of fault diagnosis, and ontology-based fault diagnosis needs to be integrated into the whole process of fault diagnosis. To make knowledge of fault diagnosis complete, machine learning shall be used for the ontological engineering.},
booktitle = {Proceedings of the 11th International Conference on Computer Modeling and Simulation},
pages = {112–116},
numpages = {5},
keywords = {Fault diagnosis, Knowledge-based system, Ontology},
location = {North Rockhampton, QLD, Australia},
series = {ICCMS '19}
}

@article{10.1145/3665252.3665263,
author = {Fan, Ju and Tu, Jianhong and Li, Guoliang and Wang, Peng and Du, Xiaoyong and Jia, Xiaofeng and Gao, Song and Tang, Nan},
title = {Unicorn: A Unified Multi-Tasking Matching Model},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/3665252.3665263},
doi = {10.1145/3665252.3665263},
abstract = {Data matching, which decides whether two data elements (e.g., string, tuple, column, or knowledge graph entity) are the "same" (a.k.a. a match), is a key concept in data integration. The widely used practice is to build task-specific or even dataset-specific solutions, which are hard to generalize and disable the opportunities of knowledge sharing that can be learned from different datasets and multiple tasks. In this paper, we propose Unicorn, a unified model for generally supporting common data matching tasks. Building such a unified model is challenging due to heterogeneous formats of input data elements and various matching semantics of multiple tasks. To address the challenges, Unicorn employs one generic Encoder that converts any pair of data elements (a, b) into a learned representation, and uses a Matcher, which is a binary classifier, to decide whether a matches b. To align matching semantics of multiple tasks, Unicorn adopts a mixture-of-experts model that enhances the learned representation into a better representation. We conduct extensive experiments using 20 datasets on 7 well-studied data matching tasks, and find that our unified model can achieve better performance on most tasks and on average, compared with the state-of-the-art specific models trained for ad-hoc tasks and datasets separately. Moreover, Unicorn can also well serve new matching tasks with zero-shot learning.},
journal = {SIGMOD Rec.},
month = may,
pages = {44–53},
numpages = {10}
}

@article{10.1145/3453475,
author = {Dwivedi, Vimal and Pattanaik, Vishwajeet and Deval, Vipin and Dixit, Abhishek and Norta, Alex and Draheim, Dirk},
title = {Legally Enforceable Smart-Contract Languages: A Systematic Literature Review},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3453475},
doi = {10.1145/3453475},
abstract = {Smart contracts are a key component of today’s blockchains. They are critical in controlling decentralized autonomous organizations (DAO). However, smart contracts are not yet legally binding nor enforceable; this makes it difficult for businesses to adopt the DAO paradigm. Therefore, this study reviews existing Smart Contract Languages (SCL) and identifies properties that are critical to any future SCL for drafting legally binding contracts. This is achieved by conducting a Systematic Literature Review (SLR) of white- and grey literature published between 2015 and 2019. Using the SLR methodology, 45 Selected and 28 Supporting Studies detailing 45 state-of-the-art SCLs are selected. Finally, 10 SCL properties that enable legally compliant DAOs are discovered, and specifications for developing SCLs are explored.},
journal = {ACM Comput. Surv.},
month = jun,
articleno = {110},
numpages = {34},
keywords = {Blockchain, decentralized autonomous organization, expressiveness, smart contract language, suitability, systematic literature review}
}

@article{10.1145/3517336,
author = {Kumar, Amit and Esmaili, Nazanin and Piccardi, Massimo},
title = {Neural Topic Model Training with the REBAR Gradient Estimator},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3517336},
doi = {10.1145/3517336},
abstract = {Topic modelling is an important approach of unsupervised machine learning that allows automatically extracting the main “topics” from large collections of documents. In addition, topic modelling is able to identify the topic proportions of each individual document, which can be helpful for organizing the collections. Many topic modelling algorithms have been proposed to date, including several that leverage advanced techniques such as variational inference and deep autoencoders. However, to date topic modelling has made limited use of reinforcement learning, a framework that has obtained vast success in many other unsupervised learning tasks. For this reason, in this article we propose training a neural topic model using a reinforcement learning objective and minimizing the objective with the recently-proposed REBAR gradient estimator. Experiments performed over two probing datasets have shown that the proposed model has achieved improvements over all the compared models in terms of both model perplexity and topic coherence, and produced topics that appear qualitatively informative and consistent.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {107},
numpages = {18},
keywords = {Topic models, deep neural networks, variational autoencoders, variational-autoencoder topic models, reinforcement learning, REBAR}
}

@inproceedings{10.1145/3539618.3591753,
author = {Jiang, Hao and Li, Chuanzhen and Cai, Juanjuan and Wang, Jingling},
title = {RCENR: A Reinforced and Contrastive Heterogeneous Network Reasoning Model for Explainable News Recommendation},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591753},
doi = {10.1145/3539618.3591753},
abstract = {Existing news recommendation methods suffer from sparse and weak interaction data, leading to reduced effectiveness and explainability. Knowledge reasoning, which explores inferential trajectories in the knowledge graph, can alleviate data sparsity and provide explicitly recommended explanations. However, brute-force pre-processing approaches used in conventional methods are not suitable for fast-changing news recommendation. Therefore, we propose an explainable news recommendation model: the Reinforced and Contrastive Heterogeneous Network Reasoning Model for Explainable News Recommendation (RCENR), consisting of NHN-R2 and MR&amp;CO frameworks. The NHN-R2 framework generates user/news subgraphs to enhance recommendation and extend the dimensions and diversity of reasoning. The MR&amp;CO framework incorporates contrastive learning with a reinforcement-based strategy for self-supervised and efficient model training. Experiments on the MIND dataset show that RCENR is able to improve recommendation accuracy and provide diverse and credible explanations.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1710–1720},
numpages = {11},
keywords = {contrastive learning, explainable recommendation, knowledge reasoning, markov decision process, news recommendation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/3274410,
author = {Piscopo, Alessandro and Simperl, Elena},
title = {Who Models the World? Collaborative Ontology Creation and User Roles in Wikidata},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {CSCW},
url = {https://doi.org/10.1145/3274410},
doi = {10.1145/3274410},
abstract = {Wikidata is a collaborative knowledge graph which is central to many academic and industry IT projects. Its users are responsible for maintaining the schema that organises this knowledge into classes, properties, and attributes, which together form the Wikidata 'ontology'. In this paper, we study the relationship between different Wikidata user roles and the quality of the Wikidata ontology. To do so we first propose a framework to evaluate the ontology as it evolves. We then cluster editing activities to identify user roles in monthly time frames. Finally, we explore how each role impacts the ontology. Our analysis shows that the Wikidata ontology has uneven breadth and depth. We identified two user roles: contributors and leaders. The second category is positively associated to ontology depth, with no significant effect on other features. Further work should investigate other dimensions to define user profiles and their influence on the knowledge graph.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {141},
numpages = {18},
keywords = {collaborative knowledge engineering, ontologies, user roles, wikidata}
}

@proceedings{10.1145/3624486,
title = {eSAAM '23: Proceedings of the 3rd Eclipse Security, AI, Architecture and Modelling Conference on Cloud to Edge Continuum},
year = {2023},
isbn = {9798400708350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Ludwigsburg, Germany}
}

@inproceedings{10.1145/3543873.3587645,
author = {Meyer zum Felde, Hendrik and Kollenstart, Maarten and Bellebaum, Thomas and Dalmolen, Simon and Brost, Gerd},
title = {Extending Actor Models in Data Spaces},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587645},
doi = {10.1145/3543873.3587645},
abstract = {In today’s internet almost any party can share sets of data with each other. However, creating frameworks and regulated realms for the sharing of data is very complex when multiple parties are involved and complicated regulation comes into play. As solution data spaces were introduced to enable participating parties to share data among themselves in an organized, regulated and standardized way. However, contract data processors, acting as data space participants, are currently unable to execute data requests on behalf of their contract partners. Here we show that an on-behalf-of actor model can be easily added to existing data spaces. We demonstrate how this extension can be realized using verifiable credentials. We provide a sample use case, a detailed sequence diagram and discuss necessary architectural adaptations and additions to established protocols. Using the extensions explained in this work numerous real life use cases which previously could technically not be realized can now be covered. This enables future data spaces to provide more dynamic and complex real world use cases.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {1447–1451},
numpages = {5},
keywords = {Actor Models, Contract Data Processing, Data Spaces, Gaia-X, International Data Spaces, On-behalf-of Model, Self-Sovereign Identities},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@article{10.1145/3696427,
author = {Rani, Nanda and Saha, Bikash and Maurya, Vikas and Shukla, Sandeep Kumar},
title = {TTPXHunter: Actionable Threat Intelligence Extraction as TTPs from Finished Cyber Threat Reports},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
url = {https://doi.org/10.1145/3696427},
doi = {10.1145/3696427},
abstract = {Understanding the modus operandi of adversaries aids organizations to employ efficient defensive strategies and share intelligence in the community. This knowledge is often present in unstructured natural language text within threat analysis reports. A translation tool is needed to interpret the modus operandi explained in the sentences of the threat report and convert it into a structured format. This research introduces a methodology named TTPXHunter for automated extraction of threat intelligence in terms of Tactics, Techniques, and Procedures (TTPs) from finished cyber threat reports. It leverages cyber domain-specific state-of-the-art natural language model to augment sentences for minority class TTPs and refine pinpointing the TTPs in threat analysis reports significantly. We create two datasets: an augmented sentence-TTP dataset of  (39,296) sentence samples and a  (149)  real-world cyber threat intelligence report-to-TTP dataset. Further, we evaluate TTPXHunter on the augmented sentence and report datasets. The TTPXHunter achieves the highest performance of  (92.42\%)  f1-score on the augmented dataset, and it also outperforms existing state-of-the-art TTP extraction method by achieving an f1-score of  (97.09\%)  when evaluated over the report dataset. TTPXHunter significantly improves cybersecurity threat intelligence by offering quick, actionable insights into attacker behaviors. This advancement automates threat intelligence analysis and provides a crucial tool for cybersecurity professionals to combat cyber threats.},
journal = {Digital Threats},
month = dec,
articleno = {37},
numpages = {19},
keywords = {Threat Intelligence, TTP Extraction, MITRE ATT&amp;CK, Natural Language Processing, Threat Intelligence Extraction, TTP Classification, Cyber Security and AI, Cyber Security Threats, NLP, Cybersecurity}
}

@article{10.1109/TASLP.2023.3267832,
author = {Thulke, David and Daheim, Nico and Dugast, Christian and Ney, Hermann},
title = {Task-Oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3267832},
doi = {10.1109/TASLP.2023.3267832},
abstract = {This paper summarizes our contributions to the document-grounded dialog tasks at the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In both iterations the task consists of three subtasks: first detect whether the current turn is knowledge seeking, second select a relevant knowledge document, and third generate a response grounded on the selected document. For DSTC9 we proposed different approaches to make the selection task more efficient. The best method, Hierarchical Selection, actually improves the results compared to the original baseline and gives a speedup of 24x. In the DSTC10 iteration of the task, the challenge was to adapt systems trained on written dialogs to perform well on noisy automatic speech recognition transcripts. Therefore, we proposed data augmentation techniques to increase the robustness of the models as well as methods to adapt the style of generated responses to fit well into the proceeding dialog. Additionally, we proposed a noisy channel model that allows for increasing the factuality of the generated responses. In addition to summarizing our previous contributions, in this work, we also report on a few small improvements and reconsider the automatic evaluation metrics for the generation task which have shown a low correlation to human judgments.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {733–741},
numpages = {9}
}

@inproceedings{10.1145/3335656.3335701,
author = {Pan, Jiabin and Mou, Naixia and Liu, Wenbao},
title = {Emotion Analysis of Tourists Based on Domain Ontology},
year = {2019},
isbn = {9781450360906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335656.3335701},
doi = {10.1145/3335656.3335701},
abstract = {The big data of tourism has exploded with the rapid development of social media, providing a new data source for the emotion analysis of tourism. Based on online comments, this paper proposes an emotion analysis model that combines tourism domain ontology and semantic-based method to mine the fine-grained emotion of tourists and designs specific formulas to quantify the emotion of tourists. Finally, the Palace Museum is used as an example to verify the validity of the model. The analysis results show that: 1) Tourists pay more attention to the attributes such as "scenery", "tourist flow", "ticket", etc. in their travel activities. 2) The emotional score of the attributes such as "lodging environment", "scenery", "culture", "environment quality", etc. are higher, but the attributes such as "safety", "tourist flow", "toilet" and cost-related attributes are lower. The main reasons are: "low security", "massive tourists", "less and small toilets" and "high costs". 3) Due to the excessive number of tourists during the holiday, which leads poor travel experience to the tourists, the emotional score of tourists are lower in the 5th, 7th, 8th and 10th months. The analysis results can provide reference for tourists' travel decisions and the development and optimization of tourism.},
booktitle = {Proceedings of the 2019 International Conference on Data Mining and Machine Learning},
pages = {146–150},
numpages = {5},
keywords = {Domain ontology, Emotion analysis, Online comment, Tourism, the Palace Museum},
location = {Hong Kong, Hong Kong},
series = {ICDMML 2019}
}

@inproceedings{10.1145/3584371.3612950,
author = {Le, Nguyen Quoc Khanh and Kha, Quang Hien},
title = {A Sequence-Based Prediction Model of Vesicular Transport Proteins Using Ensemble Deep Learning},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3612950},
doi = {10.1145/3584371.3612950},
abstract = {This study aims to employ computational methods for the accurate identification of vesicular transport proteins. The identification of these proteins holds great significance in enhancing our understanding of their protein family structure, thereby enabling the design of more effective drug targets for individuals afflicted with endocrine disorders. In recent times, researchers in the field of biology have increasingly sought to leverage deep learning techniques to address this challenge. In order to further enhance the classification performance, we investigated the following models incorporating distinct features: (1) We devised a novel protein feature called AAC_PSSM by amalgamating amino acid composition (AAC) and position-specific scoring matrix (PSSM) features. Subsequently, a gated recurrent unit (GRU) model was employed to learn such features; (2) An ensemble model was constructed by combining the existing GRU model with the model of a neural network featuring the AAC feature; (3) Random forest analysis was conducted using the pseudo-amino acid composition (PseAAC) feature; (4) Furthermore, we explored a natural language processing (NLP) approach by considering the protein sequence as a natural language and applying various neural network architectures. Upon analyzing the results obtained from the different models, it was observed that the ensemble model incorporating PSSM and AAC features exhibited the highest sensitivity of 81.03\% and accuracy of 82.43\%. Notably, our proposed model surpassed the performance of state-of-the-art models addressing the same problem and datasets, thus establishing its superiority.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {106},
numpages = {6},
keywords = {deep learning, nesemble learning, gate recurrent unit, position-specific scoring matrix, protein sequence, vesicular transport},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1145/3584371.3613056,
author = {Kalbasi, Reza and Nickerson, David and Atalag, Koray},
title = {PhysioMed: A Semantic Web based Framework for Linking Healthcare Information with Computational Physiology Models},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3613056},
doi = {10.1145/3584371.3613056},
abstract = {This research focuses on the integration of computational physiology models and Electronic Health Records (EHR) to improve the accuracy and applicability of computational models in clinical settings. Computational physiology models provide quantified insights into biological processes, while EHRs store digital health information. By combining these two domains, the research aims to create a foundation for personalized and predictive clinical decision support systems.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {85},
numpages = {1},
keywords = {semantic web, health informatics, biomedical ontology, collaborative ontology development},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1145/3412841.3442057,
author = {Vsesviatska, Oleksandra and Tietz, Tabea and Hoppe, Fabian and Sprau, Mirjam and Meyer, Nils and Dess\`{\i}, Danilo and Sack, Harald},
title = {ArDO: an ontology to describe the dynamics of multimedia archival records},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442057},
doi = {10.1145/3412841.3442057},
abstract = {Cultural heritage institutions store and digitize large amounts of multimedia data inside archives to make archival records findable by archivists, scientists, and general public. Cataloging standards vary from archive to archive and, therefore, the sharing and use of this data are limited. To solve this issue, linked open data (LOD) is rising as an essential paradigm to open and provide access to the archival resources. Archives which are opened to the world knowledge benefit from external connections by enabling the application of automated approaches to process archival records, helping all stakeholders to gain valuable insights. In this paper, we present the Archive Dynamics Ontology (ArDO) - an ontology designed for describing the hierarchical nature of archival multimedia data, as well as its application on the example of archival resources about the Weimar Republic. Furthermore, ArDO semantically organizes multimedia archival resources in form of texts, images, audios, and videos by representing the dynamics related to their classification over time. ArDO tracks the changes of a specific hierarchical classification schema referred to as systematics adopted to organize archival resources under semantically defined keywords.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1855–1863},
numpages = {9},
keywords = {multimedia archive, ontology design, ontology dynamics},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3503162.3503170,
author = {Mor, Annu and Kumar, Mukesh and Chaudhury, Santanu},
title = {Smart City Umbrella Ontology :Context -Driven Framework For Traffic Planning},
year = {2022},
isbn = {9781450395960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503162.3503170},
doi = {10.1145/3503162.3503170},
abstract = {Presently, huge public and private data sets are available from different government sources regarding transportation services in modern cities.The heterogeneous data-sets address day- to- day operations in transportation research,and these challenges can be effectively done with assistance of ontologies. Ontology is used for accessing and exploiting data from sensors to surveys in a semantically inter-operable way is demanding,in recent years. In this paper, an ontological framework is designed for smart cities by integration of entities such as surface transport network,road geometry, topology,and traffic monitoring sensors.The Smart City Umbrella Ontology (SCUO) is designed using standardized traffic guidelines in the context of Indian surface transportation.The paper focuses on ontology design by argumentation of road network nomenclature and relationship among entities.A mechanism for linking ontologies is implemented through Resource Description Framework(RDF) with a set of relations.The proposed framework can be used to provide services to commuters via specific applications in intelligent system of transportation by administration,and city stakeholders for design,policy-making purposes of traffic planning.},
booktitle = {Proceedings of the 13th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {83–90},
numpages = {8},
keywords = {Transportation ontologies, linking ontologies, resource description framework, smart city umbrella ontology},
location = {Virtual Event, India},
series = {FIRE '21}
}

@inproceedings{10.1145/3417990.3418744,
author = {Gogolla, Martin and Selic, Bran},
title = {On teaching descriptive and prescriptive modeling},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3418744},
doi = {10.1145/3417990.3418744},
abstract = {Models may be used for purposes relating (a) to understanding, predicting, and communicating model aspects, and (b) to implementing the model and capturing the design intent. Models that are primarily used for understanding, predicting and communicating are referred to as descriptive models, whereas models mainly used for implementation are called prescriptive models. This contribution focuses on teaching both the common and the distinguishing aspects of the two model categories. We start with an example for a general descriptive and prescriptive model, independent of particular software modeling languages, and continue to discuss an example demonstrating how UML and OCL can be applied for specifying both a descriptive and a prescriptive model. Finally, we discuss essentials to be learned from this teaching venture.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {23},
numpages = {9},
keywords = {UML and OCL model, descriptive model, prescriptive model},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3391274.3393636,
author = {Manda, Prashanti and SayedAhmed, Saed and Mohanty, Somya D.},
title = {Automated ontology-based annotation of scientific literature using deep learning},
year = {2020},
isbn = {9781450379748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3391274.3393636},
doi = {10.1145/3391274.3393636},
abstract = {Representing scientific knowledge using ontologies enables data integration, consistent machine-readable data representation, and allows for large-scale computational analyses. Text mining approaches that can automatically process and annotate scientific literature with ontology concepts are necessary to keep up with the rapid pace of scientific publishing. Here, we present deep learning models (Gated Recurrent Units (GRU) and Long Short Term Memory (LSTM)) combined with different input encoding formats for automated Named Entity Recognition (NER) of ontology concepts from text. The Colorado Richly Annotated Full Text (CRAFT) gold standard corpus was used to train and test our models. Precision, Recall, F-1, and Jaccard semantic similarity were used to evaluate the performance of the models. We found that GRU-based models outperform LSTM models across all evaluation metrics. Surprisingly, considering the top two probabilistic predictions of the model for each instance instead of the top one resulted in a substantial increase in accuracy. Inclusion of ontology semantics via subsumption reasoning yielded modest performance improvement.},
booktitle = {Proceedings of The International Workshop on Semantic Big Data},
articleno = {5},
numpages = {6},
keywords = {automated annotation, deep learning, named entity recognition, ontologies},
location = {Portland, Oregon},
series = {SBD '20}
}

@inproceedings{10.1145/3486622.3493933,
author = {Rajaonarivo, Landy and Mine, Tsunenori and Arakawa, Yutaka},
title = {Automatic Generation of Event Ontology from Social Network and Mobile Positioning Data},
year = {2022},
isbn = {9781450391153},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486622.3493933},
doi = {10.1145/3486622.3493933},
abstract = {The study of mobile positioning data makes it possible to detect whether an event has happened at a particular place during a given period. However, determining the nature and details of the event is a challenge, especially if the event is not widely known, as is the case for local events. We propose an approach to determining the nature of local events by generating an ontology in a completely automatic way from social network data and data on people’s movements and by querying this generated ontology. This approach uses entity discovery techniques, filtering systems and information enrichment via Open Data, as well as a system for matching discovered entities and ontology elements. Evaluation via a survey allowed us to validate approximately that the information presented in the ontology is reliable, makes sense and answers our questions.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {87–94},
numpages = {8},
keywords = {automatic ontology generation, data mining, event ontology, recommendation engine, social network data},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@inproceedings{10.1145/3318396.3318442,
author = {Ul Haq, Sami and Qamar, Usman},
title = {Ontology Based Test Case Generation for Black Box Testing},
year = {2019},
isbn = {9781450362672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318396.3318442},
doi = {10.1145/3318396.3318442},
abstract = {Software systems are not considered complete unless properly tested and verified. In existing literature, a growing interest on establishment of automated testing techniques has been observed. However, tedious manual process of test case generation largely depends upon domain knowledge and formalized representation of user requirements. The advent of semantic web engineering has led the foundation for developing ontologies as a mean to express information and knowledge semantics regarding particular domain efficiently. In software testing, ontologies can be significantly helpful to automate testing phase as they encode domain knowledge in machine interpretable format. We have proposed automatic test case generation framework that involves ontology-based requirement specification and learning based methods for conducting black box testing. Our approach integrates knowledge-based system (ontology) with learning-based testing algorithm to automate: generation of test cases, test execution and test verdict construction. Proposed framework includes, requirement ontology to formalize requirement specification, Dialogue Manager that enables selection of available test cases and Learning Based Testing to generate counter examples of test cases through system learning. The contribution of this paper is to enable 1) requirement elicitation and specification using ontologies 2) test data selection from existing ontologies and 3) automatic test case generation from existing test cases. To represent the applicability of this research, ontology for requirement elicitation and specification is developed. Framework proposed in this research paper is an effort to provide software testing tools to save time, cost and efforts during test design phase.},
booktitle = {Proceedings of the 2019 8th International Conference on Educational and Information Technology},
pages = {236–241},
numpages = {6},
keywords = {Black box testing, Learning based testing, Ontologies, Requirement Engineering, Test case generation},
location = {Cambridge, United Kingdom},
series = {ICEIT 2019}
}

@article{10.1145/3354584,
author = {Lilis, Yannis and Savidis, Anthony},
title = {A Survey of Metaprogramming Languages},
year = {2019},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3354584},
doi = {10.1145/3354584},
abstract = {Metaprogramming is the process of writing computer programs that treat programs as data, enabling them to analyze or transform existing programs or generate new ones. While the concept of metaprogramming has existed for several decades, activities focusing on metaprogramming have been increasing rapidly over the past few years, with most languages offering some metaprogramming support and the amount of metacode being developed growing exponentially. In this article, we introduce a taxonomy of metaprogramming languages and present a survey of metaprogramming languages and systems based on the taxonomy. Our classification is based on the metaprogramming model adopted by the language, the phase of the metaprogram evaluation, the metaprogram source location, and the relation between the metalanguage and the object language.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {113},
numpages = {39},
keywords = {Metaprogramming, aspect-oriented programming, generative programming, macro systems, meta-object protocols, multistage languages, reflection}
}

@inproceedings{10.1145/3711896.3737221,
author = {Shi, Binpeng and Luo, Yu and Wang, Jingya and Zhao, Yongxin and Zhang, Shenglin and Hao, Bowen and Zhao, Chenyu and Sun, Yongqian and Zhang, Zhi and Sun, Ronghua and Li, Haihua and Song, Wei and Chen, Xiaolong and Miao, Jingbo and Pei, Dan},
title = {FlowXpert: Expertizing Troubleshooting Workflow Orchestration with Knowledge Base and Multi-Agent Coevolution},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737221},
doi = {10.1145/3711896.3737221},
abstract = {Incident management remains a critical yet challenging task for large-scale cloud services. Most cloud service providers abstract troubleshooting into predefined workflows for different incidents, offering step-by-step guidance. However, manually crafting workflows is resource-consuming and knowledge-intensive, hindering large-scale deployment. Most automated techniques for workflow orchestration rely on large language models (LLMs) to handle complex tasks but overlook key aspects of troubleshooting, including complex expertise, domain requirements, and the reliability of AI feedback. These limitations undermine workflow quality. Therefore, we propose FlowXpert, a novel framework for troubleshooting workflow orchestration. Leveraging LLMs, it first builds a knowledge base centered on incident-aware nodes to precisely depict expertise. Then, fed into AI feedback and synthetic preference data, reinforcement learning is applied to refine the workflow generator and evaluator. To assess troubleshooting workflows, we introduce OpsFlowBench based on Huawei Cloud's datacenter switch operation documents. Benchmark tests under the tailored STEPScore metric validate its effectiveness. Furthermore, during a 10-week deployment in Huawei Cloud's datacenter network, FlowXpert provided valuable support to both on-call engineers and AI executors, as evidenced by empirical data and case study.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {4839–4850},
numpages = {12},
keywords = {incident management, large language model, troubleshooting, workflow orchestration},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3276604.3276623,
author = {Coulon, Fabien and Degueule, Thomas and van der Storm, Tijs and Combemale, Benoit},
title = {Shape-diverse DSLs: languages without borders (vision paper)},
year = {2018},
isbn = {9781450360296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276604.3276623},
doi = {10.1145/3276604.3276623},
abstract = {Domain-Specific Languages (DSLs) manifest themselves in remarkably diverse shapes, ranging from internal DSLs embedded as a mere fluent API within a programming language, to external DSLs with dedicated syntax and tool support. Although different shapes have different pros and cons, combining them for a single language is problematic:&nbsp;language designers usually commit to a particular shape early in the design process, and it is hard to reconsider this choice later. In this new ideas paper, we envision a language engineering approach enabling (i) language users to manipulate language constructs in the most appropriate shape according to the task at hand, and (ii) language designers to combine the strengths of different technologies for a single DSL. We report on early experiments and lessons learned building , our prototype approach to this problem. We illustrate its applicability in the engineering of a simple shape-diverse DSL implemented conjointly in Rascal, EMF, and Java. We hope that our initial contribution will raise the awareness of the community and encourage future research.},
booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {215–219},
numpages = {5},
keywords = {domain-specific language, shape-diverse dsl},
location = {Boston, MA, USA},
series = {SLE 2018}
}

@inproceedings{10.1145/3292500.3330838,
author = {Hao, Junheng and Chen, Muhao and Yu, Wenchao and Sun, Yizhou and Wang, Wei},
title = {Universal Representation Learning of Knowledge Bases by Jointly Embedding Instances and Ontological Concepts},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330838},
doi = {10.1145/3292500.3330838},
abstract = {Many large-scale knowledge bases simultaneously represent two views of knowledge graphs (KGs): an ontology view for abstract and commonsense concepts, and an instance view for specific entities that are instantiated from ontological concepts. Existing KG embedding models, however, merely focus on representing one of the two views alone. In this paper, we propose a novel two-view KG embedding model, JOIE, with the goal to produce better knowledge embedding and enable new applications that rely on multi-view knowledge. JOIE employs both cross-view and intra-view modeling that learn on multiple facets of the knowledge base. The cross-view association model is learned to bridge the embeddings of ontological concepts and their corresponding instance-view entities. The intra-view models are trained to capture the structured knowledge of instance and ontology views in separate embedding spaces, with a hierarchy-aware encoding technique enabled for ontologies with hierarchies. We explore multiple representation techniques for the two model components and investigate with nine variants of JOIE. Our model is trained on large-scale knowledge bases that consist of massive instances and their corresponding ontological concepts connected via a (small) set of cross-view links. Experimental results on public datasets show that the best variant of JOIE significantly outperforms previous models on instance-view triple prediction task as well as ontology population on ontology-view KG. In addition, our model successfully extends the use of KG embeddings to entity typing with promising performance.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1709–1719},
numpages = {11},
keywords = {knowledge graph, ontology learning, relational embeddings},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3442391.3442407,
author = {Sree-Kumar, Anjali and Planas, Elena and Claris\'{o}, Robert},
title = {Validating Feature Models With Respect to Textual Product Line Specifications},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442407},
doi = {10.1145/3442391.3442407},
abstract = {Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents.},
booktitle = {Proceedings of the 15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {15},
numpages = {10},
keywords = {Feature Model Validation, Machine Learning, Natural Language Processing, Requirements Engineering, Software Product Line},
location = {Krems, Austria},
series = {VaMoS '21}
}

@inproceedings{10.1145/3677052.3698671,
author = {Sarmah, Bhaskarjit and Mehta, Dhagash and Hall, Benika and Rao, Rohan and Patel, Sunil and Pasquali, Stefano},
title = {HybridRAG: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698671},
doi = {10.1145/3677052.3698671},
abstract = {Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models (LLMs) even using the current best practices to use Retrieval Augmented Generation (RAG) (referred to as VectorRAG techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called HybridRAG, of the Knowledge Graphs (KGs) based RAG techniques (called GraphRAG) and VectorRAG techniques to enhance question-answer (Q&amp;A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q&amp;A format, and hence provide a natural set of pairs of ground-truth Q&amp;As, we show that HybridRAG which retrieves context from both vector database and KG outperforms both traditional VectorRAG and GraphRAG individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {608–616},
numpages = {9},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inproceedings{10.1145/3573428.3573599,
author = {Zhang, Luyi and Li, Ren and Xiao, Qiao},
title = {A Prompt-based Few-shot Machine Reading Comprehension Model for Intelligent Bridge Management},
year = {2023},
isbn = {9781450397148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573428.3573599},
doi = {10.1145/3573428.3573599},
abstract = {Bridge inspection reports are an important data source in the bridge management process, and they contain a large amount of fine-grained information. However, the research on machine reading comprehension (MRC) methods for this field is insufficient, and annotating large scale domain-specific corpus is time-consuming. This paper presented a novel prompt-based few-shot MRC approach for intelligent bridge management. The proposed model uses the pretrained model MacBERT as backbone. The prompt templates are designed based on some domain-specific heuristic rules. The experimental results show that our model outperforms the baseline models in different few-shot settings. The proposed model can provide technical support for the construction of automatic question answering system in the field of bridge management.},
booktitle = {Proceedings of the 2022 6th International Conference on Electronic Information Technology and Computer Engineering},
pages = {946–950},
numpages = {5},
keywords = {Bridge inspection, Few-shot, Machine reading comprehension, Prompt},
location = {Xiamen, China},
series = {EITCE '22}
}

@inproceedings{10.1109/JCDL.2019.00066,
author = {Wu, Mei Mei and Liu, Ying-Hsang},
title = {Exploring ontologies for collection protection in second sino-japanese war},
year = {2020},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00066},
doi = {10.1109/JCDL.2019.00066},
abstract = {The actions are usually trivial, implicit yet significant for the protection of collection during the wartime. Records of those actions, events, and particularly, the related persons, organizations may be scattered but deserve further attention since the records are precious to human knowledge protection efforts. Searching, identifying and collecting the related resources from the scattered data sets is not as complicated as developing an ontology for this domain of collection protection during the wartime period. This study has selected 1939--41, in the Second Sino-Japanese War during WWII when there were figures working on the collection protection projects that have been documented in the historical archives and many other resources, including oral history, journals, digital documentation, conference papers, literature reviews, etc. The preliminary ontology models for the protection of wartime period collection have been developed based on one of the oral history. The current research has further tuned and finalized the ontology of collection protection during the wartime period by analyzing the multi-sets of data resources. The implications of the research results are two folds: firstly, it could shed lights on the classification system for a digital library of this domain, and secondly, it could benefit the historical researchers for providing new clues in this line of collection protection during the wartime period research.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {351–352},
numpages = {2},
keywords = {collection protection, cultural heritage, digital scholarship, ontologies},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@article{10.1145/3660639,
author = {Sharma, Mandar and Gogineni, Ajay Kumar and Ramakrishnan, Naren},
title = {Neural Methods for Data-to-text Generation},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3660639},
doi = {10.1145/3660639},
abstract = {The neural boom that has sparked natural language processing (NLP) research throughout the last decade has similarly led to significant innovations in data-to-text (D2T) generation. This survey offers a consolidated view into the neural D2T paradigm with a structured examination of the approaches, benchmark datasets, and evaluation protocols. This survey draws boundaries separating D2T from the rest of the natural language generation (NLG) landscape, encompassing an up-to-date synthesis of the literature, and highlighting the stages of technological adoption from within and outside the greater NLG umbrella. With this holistic view, we highlight promising avenues for D2T research that focus not only on the design of linguistically capable systems but also on systems that exhibit fairness and accountability.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {89},
numpages = {46},
keywords = {Narration, data-to-text, data-to-text generation, natural language generation}
}

@inproceedings{10.1145/3366030.3366097,
author = {Stenzer, Alexander},
title = {Query Relaxation using Spreading-Activation and SKOS-Ontologies},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366097},
doi = {10.1145/3366030.3366097},
abstract = {Digital libraries and archives adopting the Linked Open Data (LOD) approach add descriptive metadata to the objects stored in their inventory in order to facilitate searching and sorting. In many cases the available metadata terms are organized as a controlled vocabulary. Technically, the controlled vocabularies are often provided in the form of SKOS ontologies thus allowing to apply semantic web technologies to establish inter-vocabulary relations and ask queries.However, how can a search over the contents of different digital libraries each of which relies on their own vocabulary be performed without sacrificing recall or having to align the underlying ontologies beforehand?In this paper we present an approach based on query relaxation to solving this problem. Considering the graph nature of ontologies for controlled vocabularies we propose to use a spreading activation algorithm to relax and subsequently transform SPARQL queries in a way that makes them suitable for other vocabularies.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {330–334},
numpages = {5},
keywords = {Query, Relaxation, SKOS, Spreading-Activation, Transformation},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3243082.3243106,
author = {Sacenti, Juarez A. P. and Willrich, Roberto and Fileto, Renato},
title = {Hybrid Recommender System Based on Multi-Hierarchical Ontologies},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3243106},
doi = {10.1145/3243082.3243106},
abstract = {Recommender Systems (RSs) are usually based in User Profiles (UP) to identify items of interest to a user, among the items of a usually vast collection. Traditional RSs are mostly based on ratings of items made by users and do not attempt to estimate the reasons that led the user to access these items. Furthermore, such systems may suffer from the lack of rating data, the so-called data sparsity. This paper proposes a hybrid recommender system that considers, besides the ratings of the users, a feature description analysis of the items accessed by the users. This analysis is based on ontological UP, described in accordance with a set of ontologies, one per feature. The use of ontologies provides a weak coupling between the proposed RS and the domain of the item to be recommended. The effectiveness of our proposal is demonstrated and evaluated in the movie domain using the MovieLens dataset. The experiments demonstrated an improvement in the quality of the recommendations and a greater tolerance to the data sparsity, compared to state-of-art systems.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {149–156},
numpages = {8},
keywords = {Hibrid filtering, Ontology, Recommender systems},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@inproceedings{10.1145/3447568.3448553,
author = {Djebouri, Djamila and Keskes, Nabil},
title = {Exploitation of ontological approaches in Big Data: A State of the Art},
year = {2021},
isbn = {9781450376556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447568.3448553},
doi = {10.1145/3447568.3448553},
abstract = {The emergence of web technologies is generating a data deluge called Big Data. All this data is in fact a gold mine to be exploited. However, we are confronted with huge volumes of heterogeneous data (various formats) and varied data (various sources) and in continuous expansion. To deal with this, some research works have introduced ontologies: this is the purpose of this paper. We present the Big Data concept on the one hand and the ontology concept on the other. We first recalled the definitions of Big Data, its main dimensions known by the 3 V (volume, velocity, variety), the fields of application as well as the various problems related to it. We reviewed the different solutions proposed as well as the existing tools by using the NoSQL and the Map-Reduce paradigm implemented in Hadoop and Spark.We then looked at the concept of ontology, starting by recalling the definition of ontology, so an ontology is a conceptual model to represent reality and on which it is possible to develop systems that can be shared and reused. Ontologies are used to represent a domain and reason about its entities.Finally, we presented and discussed some research works that combined ontologies and Big Data. We have found that there is a very abundant literature that deals with big data and ontologies separately, but few studies combine the two concepts together. We will therefore focus on the latter in order to enrich the scientific literature in the domain.},
booktitle = {Proceedings of the 10th International Conference on Information Systems and Technologies},
articleno = {45},
numpages = {6},
keywords = {Big Data, HADOOP, Knowledge Base, Map-Reduce, Semantic Web, Spark, ontology},
location = {Lecce, Italy},
series = {ICIST '20}
}

@book{10.1145/3382097,
author = {Allemang, Dean and Hendler, Jim and Gandon, Fabien},
title = {Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL},
year = {2020},
isbn = {9781450376174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {3},
volume = {33},
abstract = {Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.}
}

@inproceedings{10.1145/3688671.3688734,
author = {Kivrakidis, Ioannis and Rigas, Emmanouil and Bassiliades, Nick},
title = {Towards Enriching the Electric Vehicle Knowledge Graph by Linking it to DBpedia},
year = {2024},
isbn = {9798400709821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688671.3688734},
doi = {10.1145/3688671.3688734},
abstract = {The automotive industry is focusing on Electric Vehicles (EVs) for their efficiency in reducing oil consumption and emissions. However, the EV market’s diversity in battery capacities, classifications, and connector types creates a lack of standardization. Researchers are exploring advanced data and knowledge management methods, with Knowledge Graphs (KGs) emerging as a promising solution. KGs represent data in a way that reflects human understanding, promoting natural human-machine interactions and enhancing AI’s insights. Their structure and constraints are defined by a vocabulary or ontology. This paper presents ongoing work towards enriching an existing Electric Vehicle Knowledge Graph (EVKG), which is specified by the Electric Vehicle Ontology (EVO). To achieve this, we utilized the Python programming language to create labels for each resource in the EVKG. These labels were then used to match and link these resources to their corresponding entries in DBpedia through the use of the owl:sameAs and rdfs:seeAlso properties. To ensure the matching was as accurate as possible, we developed two algorithms: one employing string matching and the other using word vectorization and distance techniques.},
booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence},
articleno = {38},
numpages = {4},
keywords = {Linked Data, Ontology, Knowledge Graph, EVO Ontology, Electric Vehicles},
location = {
},
series = {SETN '24}
}

@inproceedings{10.1145/3243250.3243253,
author = {Telli, Abdelmoutia and Chau, Ma Thi and Bourahla, Mustapha and Tabia, Karim and Benferhat, Salem},
title = {An Ontology for Classifying Vietnamese Dance Movements},
year = {2018},
isbn = {9781450364829},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243250.3243253},
doi = {10.1145/3243250.3243253},
abstract = {This paper proposes an OWL ontology called "VDM" (Vietnamese Dance Movements), to define taxonomy of dance movement classes and their relationships for the traditional Vietnamese dances taking into account the semantics of its art and its cultural anthropologists. The "VDM" terminology can be used to describe elementary movements (poses) as a dataset ontology importing the ontology "VDM". These poses are results of dance sequences segmentation (using segmentation techniques). The ontology "VDM" is supported by classification rules, which are developed with the OWL complementary language SWRL (Semantic Web Rule Language) to entail movement phrases, which are basic movements with complete meaning. The dataset ontology containing pose descriptions can be queried using the query language SQWRL (Semantic Query Web-enhanced Rule Language).},
booktitle = {Proceedings of the International Conference on Pattern Recognition and Artificial Intelligence},
pages = {23–29},
numpages = {7},
keywords = {Dance Labanotation, Description Logics, Ontology, Semantic Web Technologies, Traditional Vietnamese Dance},
location = {Union, NJ, USA},
series = {PRAI 2018}
}

@article{10.1145/3375628,
author = {Hernich, Andr\'{e} and Lutz, Carsten and Papacchini, Fabio and Wolter, Frank},
title = {Dichotomies in Ontology-Mediated Querying with the Guarded Fragment},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {1529-3785},
url = {https://doi.org/10.1145/3375628},
doi = {10.1145/3375628},
abstract = {We study ontology-mediated querying in the case where ontologies are formulated in the guarded fragment of first-order logic (GF) or extensions thereof with counting and where the actual queries are (unions of) conjunctive queries. Our aim is to classify the data complexity and Datalog rewritability of query evaluation depending on the ontology O, where query evaluation w.r.t. O is in PTime (resp. Datalog rewritable) if all queries can be evaluated in PTime w.r.t. O (resp. rewritten into Datalog under O), and coNP-hard if at least one query is coNP-hard w.r.t. O. We identify several fragments of GF that enjoy a dichotomy between Datalog-rewritability (which implies PTime) and coNP-hardness as well as several other fragments that enjoy a dichotomy between PTime and coNP-hardness, but for which PTime does not imply Datalog-rewritability. For the latter, we establish and exploit a connection to constraint satisfaction problems. We also identify fragments for which there is no dichotomy between PTime and coNP. To prove this, we establish a non-trivial variation of Ladner’s theorem on the existence of NP-intermediate problems. Finally, we study the decidability of whether a given ontology enjoys PTime query evaluation, presenting both positive and negative results, depending on the fragment.},
journal = {ACM Trans. Comput. Logic},
month = feb,
articleno = {20},
numpages = {47},
keywords = {Ontology-based data access, dichotomies, query evaluation}
}

@inproceedings{10.1145/3472307.3484166,
author = {Ayimdji Tekemetieu, Armel and Pigot, H\'{e}l\`{e}ne and Bottari, Carolina and Gagnon-Roy, Mireille and Giroux, Sylvain},
title = {Modeling an Adaptive Resident-System Interaction for Cognitive Assistance in Ambient Assisted Living},
year = {2021},
isbn = {9781450386203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472307.3484166},
doi = {10.1145/3472307.3484166},
abstract = {In the last decade, advances in the field of ambient assisted living (AAL) have changed the way services can be provided in smart homes. New possibilities are now offered for addressing complex interaction problems between the technology and users with special needs. Within this context, this study addresses human-computer interactions for cognitive assistance to people with Traumatic Brain Injury (TBI). Interdisciplinary research combining computer science and occupational therapy was conducted to model the interaction between an AAL system (AALS) and a person with cognitive impairments due to TBI residing in an AAL environment. Cognitive assistance is modelled as an interactive exchange where an AALS spreads assistance cues that should induce appropriate behaviours from an assisted person as responses. After an assistance cue is delivered, the AALS should evaluate the user reaction and, stop the interaction if the intended reaction is observed or resume by adapting the assistance. To do so, evidence-based cognitive rehabilitation and speech acts are used to model the interaction content i.e., assistive cues and user feedback, while an ontology formalizes the semantics of this knowledge in a computer readable format. A behavioural model based on behaviour trees informed by the ontological model is then used to enable the cognitive assistant to plan the sequence of cues to be delivered adaptively depending on the circumstances, the assistance goal and the user's behaviour. To show that the proposed interaction model can help an AALS to provide adaptive assistance to people with cognitive impairments, it is exemplified on a cooking safety assistant designed for people with TBI.},
booktitle = {Proceedings of the 9th International Conference on Human-Agent Interaction},
pages = {183–192},
numpages = {10},
keywords = {Ambient intelligence, Cognitive assistance, Ontology-based decision support, Speech acts, Traumatic Brain Injury, interactive information retrieval},
location = {Virtual Event, Japan},
series = {HAI '21}
}

@inproceedings{10.1109/ASONAM55673.2022.10068618,
author = {Al-Shareef, Sarah and Alharbi, Rahaf and Alharbi, Rawan and Almfarriji, Raghad and Alsharif, Maram and Alharthi, Rasha and Althaqafi, Lamia},
title = {Investigating Community Detection in Arabic Scholarly Network Using Ontology-Based Semantic Expansion},
year = {2023},
isbn = {9781665456616},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASONAM55673.2022.10068618},
doi = {10.1109/ASONAM55673.2022.10068618},
abstract = {Clustering researchers in communities is an important task to support a range of techniques for analyzing and making sense of the research environment and helps researchers find people in the same field of interest to collaborate. In computer science, ontology is commonly used to capture knowledge about a particular area using relevant concepts and relations. This study investigates the use of overlapping community detection algorithms on a multilayered Arabic scholarly network to detect communities of researchers who share their research interests. Two researchers can share an interest if they co-authored a publication or share some keywords in their publications. The set of keywords is expanded via semantic search within a cross-domain ontology, e.g. DBpedia, allowing more researchers with indirect relationships to be connected. A 2-layer scholarly network was constructed by retrieving the scholarly data of faculty members from three colleges at Umm AlQura University (UQU) with rich Arabic publications. Four versions of this network were tested: unweighted, weighted, semantically expanded, and reduced semantically expanded. It was found that weights have an insignificant role in community detection within this study. In addition, a semantically expanded network does have better clustering potentials but only if was performed selectively. Otherwise, the expanded network might suffer from generic and non-discriminative keywords, making the community detection task more challenging. To our knowledge, this is the first investigation into detecting communities within an Arabic scholarly network.},
booktitle = {Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {96–103},
numpages = {8},
keywords = {community detection, semantic annotation, Arabic scholarly data, overlapping community detection, multilayered complex network, social network analysis, DBpedia},
location = {Istanbul, Turkey},
series = {ASONAM '22}
}

@inproceedings{10.1145/3696500.3696562,
author = {Ke, Lanxin and Jian, Li and Liao, Wang and Chen, Yibin and Cai, Yangqi and Ye, Linmei},
title = {Hierarchical Multi-label Classification Model Research for Scenic Area Tourism Resources},
year = {2024},
isbn = {9798400710278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696500.3696562},
doi = {10.1145/3696500.3696562},
abstract = {Research on tourism resource demand offers critical decision support for the protection, development, and marketing of tourism resources. It also improves personalized experiences and satisfaction for tourists, thus fostering the advancement of the tourism industry into broader and higher realms. It also improves personalized experiences and satisfaction for tourists, thus fostering the advancement of the tourism industry into broader and higher realms. The objective of this study is to extract characteristics of 3A-level and above scenic spots nationwide, facilitating the integration and utilization of tourism resources in the country. The objective of this study is to extract characteristics of 3A-level and above scenic spots nationwide, facilitating the integration and utilization of tourism information. This facilitates the automation and efficiency of tourism resource classification and provides suggestions for improving services at these scenic areas. In response to the abundant resources of major scenic areas across the country, a new approach has been developed. In response to the abundant resources of major scenic areas across the country, this study introduces a hierarchical multi-label classification model for tourism resources. tourism resource classification theme system issued by the Ministry of Culture and Tourism, and leveraging FastText for pre-training on scenic area introductory texts, this research combines the advantages of the hierarchical multi-label classification model with the theme system of the Ministry of Culture and Tourism. introductory texts, this research combines the traditional LSTM model with an attention-based Transformer model. Additionally, a Graph-Convolutional Network (GCN) is used to classify tourism resources. Convolutional Network (GCN) is employed as a hierarchical structure-aware encoder to construct the MiLCT, a hierarchical multi-label classification model, enabling sophisticated multi-label classification to be applied to the text. model, enabling sophisticated multi-label classification of tourism scenic area resource data. Experimental comparisons demonstrate that, with increasing classification levels, the proposed model outperforms those lacking GCN and Transformer components in terms of micro-Precision, micro-Recision, micro-Recision, and micro-Recision. micro-Precision, micro-Recall, and micro-F1 scores, this indicates that the hierarchical structural information of the model can significantly enhance its performance. In comparison to the hierarchical multi-label correlation model, the proposed model demonstrates enhanced performance across the evaluated metrics, indicating its efficacy in integrating multimodal features and providing more comprehensive and accurate data characterization.},
booktitle = {Proceedings of the 2024 International Conference on Big Data and Digital Management},
pages = {369–379},
numpages = {11},
location = {Shanghai, China},
series = {ICBDDM '24}
}

@inproceedings{10.1145/3297662.3365824,
author = {Orozco, Adrian Taboada and Mouakher, Amira and Ben Sassi, Imen and Nicolle, Christophe},
title = {An Ontology-Based Thermal Comfort Management System In Smart Buildings},
year = {2020},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365824},
doi = {10.1145/3297662.3365824},
abstract = {Achieving thermal comfort for occupants in buildings has been the main focus of several studies in recent years. The challenging issue of the building envelope is to save energy and achieve a high comfortable environment simultaneously. To calculate the thermal comfort level in a living space, environmental factors such as indoor air temperature, mean radiant temperature, air velocity, and humidity are needed. The latter parameters are aggregated through the well known PMV index. In this paper, we introduce a wireless sensor network (WSN)-based comfort measurement approach, called OnCom, using a dedicated ontology and the emotional state analysis of the occupant to reach the "adequate" indoor thermal comfort. The main thrust of OnCom stands on the smooth connection of human emotions with the thermal sensations. Carried out experiments showed that emotions, unveiled from tweets, have been efficiently used to mitigate user thermal discomfort.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {300–307},
numpages = {8},
keywords = {Ontology, Sentiment Analysis, Smart Buildings, Thermal Comfort, Wireless Sensor Networks (WSN)},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@article{10.1613/jair.1.12631,
author = {Silverman, Greg M. and Sahoo, Himanshu S. and Ingraham, Nicholas E. and Lupei, Monica and Puskarich, Michael A. and Usher, Michael and Dries, James and Finzel, Raymond L. and Murray, Eric and Sartori, John and Simon, Gyorgy and Zhang, Rui and Melton, Genevieve B. and Tignanelli, Christopher J. and Pakhomov, Serguei VS},
title = {NLP Methods for Extraction of Symptoms from Unstructured Data for Use in Prognostic COVID-19 Analytic Models},
year = {2022},
issue_date = {Jan 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {72},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12631},
doi = {10.1613/jair.1.12631},
abstract = {Statistical modeling of outcomes based on a patient's presenting symptoms (symptomatology) can help deliver high quality care and allocate essential resources, which is especially important during the COVID-19 pandemic. Patient symptoms are typically found in unstructured notes, and thus not readily available for clinical decision making. In an attempt to fill this gap, this study compared two methods for symptom extraction from Emergency Department (ED) admission notes. Both methods utilized a lexicon derived by expanding The Center for Disease Control and Prevention's (CDC) Symptoms of Coronavirus list. The first method utilized a word2vec model to expand the lexicon using a dictionary mapping to the Uni ed Medical Language System (UMLS). The second method utilized the expanded lexicon as a rule-based gazetteer and the UMLS. These methods were evaluated against a manually annotated reference (f1-score of 0.87 for UMLS-based ensemble; and 0.85 for rule-based gazetteer with UMLS). Through analyses of associations of extracted symptoms used as features against various outcomes, salient risks among the population of COVID-19 patients, including increased risk of in-hospital mortality (OR 1.85, p-value &lt; 0.001), were identified for patients presenting with dyspnea. Disparities between English and non-English speaking patients were also identified, the most salient being a concerning finding of opposing risk signals between fatigue and in-hospital mortality (non-English: OR 1.95, p-value = 0.02; English: OR 0.63, p-value = 0.01). While use of symptomatology for modeling of outcomes is not unique, unlike previous studies this study showed that models built using symptoms with the outcome of in-hospital mortality were not significantly different from models using data collected during an in-patient encounter (AUC of 0.9 with 95\% CI of [0.88, 0.91] using only vital signs; AUC of 0.87 with 95\% CI of [0.85, 0.88] using only symptoms). These findings indicate that prognostic models based on symptomatology could aid in extending COVID-19 patient care through telemedicine, replacing the need for in-person options. The methods presented in this study have potential for use in development of symptomatology-based models for other diseases, including for the study of Post-Acute Sequelae of COVID-19 (PASC).},
journal = {J. Artif. Int. Res.},
month = jan,
pages = {429–474},
numpages = {46},
keywords = {Natural language processing, COVID-19, Information extraction, UMLS}
}

@article{10.1145/3423166,
author = {Varela-Vaca, \'{A}ngel Jes\'{u}s and Quintero, Antonia M. Reina},
title = {Smart Contract Languages: A Multivocal Mapping Study},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3423166},
doi = {10.1145/3423166},
abstract = {Blockchain is a disruptive technology that has attracted the attention of the scientific community and companies, as proven by the exponential growth of publications on this topic in recent years. This growing interest is mainly due to the promise that the use of blockchain enables it to be verified, without including any trusted intermediaries, that the information received from the network is authentic and up-to-date. In this respect, blockchain is a distributed database that can be seen as a ledger that records all transactions that have ever been executed. In this context, smart contracts are pieces of software used to facilitate, verify, and enforce the negotiation of a transaction on a blockchain platform. These pieces of software are implemented by using programming languages, which are sometimes provided by the blockchain platforms themselves. This study aims to (1) identify and categorise the state-of-the-art related to smart contract languages, in terms of the existing languages and their main features, and (2) identify new research opportunities. The review has been conducted as a multivocal mapping study that follows the guidelines proposed by Garousi et&nbsp;al. for conducting multivocal literature reviews, as well as the guidelines proposed by Kitchenham and Charters for conducting mapping studies. As a result of the implementation of the review protocol, 4,119 papers were gathered, and 109 of them were selected for extraction. The contributions of this article are twofold: (1) 101 different smart contract languages have been identified and classified according to a variety of criteria; (2) a discussion on the findings and their implications for future research have been outlined. As a conclusion, it could be stated that a rigorous and replicable overview of the state-of-the-art of smart contract languages has been provided that can benefit not only researchers but also practitioners in the field, thanks to its multivocal nature.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {3},
numpages = {38},
keywords = {Smart contract language, blockchain, multivocal literature mapping study, systematic literature review}
}

@inproceedings{10.1145/3331076.3331103,
author = {Mansour, Elio and Chbeir, Richard and Arnould, Philippe},
title = {EQL-CE: an event query language for connected environments},
year = {2019},
isbn = {9781450362498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331076.3331103},
doi = {10.1145/3331076.3331103},
abstract = {Recent advances in sensor technology and information processing have allowed connected environments to impact various application domains. In order to detect events in these environments, existing works rely on the sensed data. However, these works are not re-usable since they statically define the targeted events (i.e., the definitions are hard to modify when needed). Here, we present a generic framework for event detection composed of (i) a representation of the environment; (ii) an event detection mechanism; and (iii) an Event Query Language (EQL) for user/framework interaction. This paper focuses on detailing the EQL which allows the definition of the data model components, handles instances of each component, protects the security/privacy of data/users, and defines/detects events. We also propose a query optimizer in order to handle the dynamicity of the environment and spatial/temporal constraints. We finally illustrate the EQL and conclude the paper with some future works.},
booktitle = {Proceedings of the 23rd International Database Applications \&amp; Engineering Symposium},
articleno = {7},
numpages = {10},
keywords = {event query language, internet of things, sensor networks},
location = {Athens, Greece},
series = {IDEAS '19}
}

@inproceedings{10.1145/3337722.3341871,
author = {Gandhi, Sagar and Harrison, Brent},
title = {Guided open story generation using probabilistic graphical models},
year = {2019},
isbn = {9781450372176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3337722.3341871},
doi = {10.1145/3337722.3341871},
abstract = {In this work, we present an approach for performing computational storytelling in open domain based on Author Goals. Author Goals are constraints placed on a story event directed by the author of the system. There are two challenges present in this type of story generation: (1) automatically acquiring a model of story progression, and (2) guiding the progress of story progression in light of different goals. We propose a novel approach to story generation based on probabilistic graphical models and Loopy Belief Propagation (LBP) that addresses both of these problems. We show the applicability of our technique through a case study on the Visual Storytelling (VIST) 2017 dataset. We use image descriptions as author goals. This empirical analysis suggests that our approach is able to utilize goals information to better automatically generate stories.},
booktitle = {Proceedings of the 14th International Conference on the Foundations of Digital Games},
articleno = {79},
numpages = {7},
keywords = {belief propagation, computational storytelling, natural language generation, probabilistic graphical models},
location = {San Luis Obispo, California, USA},
series = {FDG '19}
}

@inproceedings{10.1145/3444757.3485110,
author = {Khemiri, Ahmed and Drissi, Amani and Tissaoui, Anis and Sassi, Salma and Chbeir, Richard},
title = {Learn2Construct: An automatic ontology construction based on LDA from texual data},
year = {2021},
isbn = {9781450383141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444757.3485110},
doi = {10.1145/3444757.3485110},
abstract = {In recent years, the research on Ontology Learning has become a hot topic among researchers because of the exponential increase of the number of documents and textual data not only on the web but also in digital libraries. This has participated to the emergence of new computational tools and methods to deal with the automatic organization, representation, retrieval and exploration of large corpus in order to have a good way of organizing and managing huge volumes of data. LDA-based approaches have proven to provide the best result [18][16] [4]. However, they suffers to several limitations related to concept and relation extraction, as well as coping with the corpus evolution. In order to cope with these problems, we propose here a new solution named Learn2Construct which is an automatic ontology construction method based on topic modeling. Experiments have been conducted to measure the effectiveness of our solution and compare it to existing ones. The results obtained are more than satisfactory.},
booktitle = {Proceedings of the 13th International Conference on Management of Digital EcoSystems},
pages = {49–56},
numpages = {8},
keywords = {LDA, Latent Dirichlet Allocation, Ontology Learning, Ontology based on topic modeling, Text classification, Topic modeling},
location = {Virtual Event, Tunisia},
series = {MEDES '21}
}

@inproceedings{10.1145/3440749.3442607,
author = {Ismail, Qossay and Saleh, Osama and Hashayka, Mohammed and Awad, Ahmed and Hawash, Amjad and Othman, Othman},
title = {Improve the Firewall Accuracy By using Dynamic Ontology},
year = {2021},
isbn = {9781450388863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440749.3442607},
doi = {10.1145/3440749.3442607},
abstract = {Data is considered an important asset for organizations, companies, and even people. Crucial decisions depend mainly on data. Exchanging data is essential in order to negotiate ideas, thoughts, and decisions. Networks are the communication channels of data exchange although data is exposed to different attacks, threats, and loss. Because of this, data security has become a key concern for different parties through their daily data manipulation. There are different ways to ensure data security. Paying attention to network threats, data encryption, and using strong passwords are all examples. However, a firewall represents the first defense line against malicious traffic throughout the network. Firewalls have a set of rules to be applied in the time of data exchange between inside and outside of data networks. Some of the firewalls apply such rules in a sequential manner, which degrades the performance of the firewall. In this work, we are utilizing a dynamic ontology of different firewall rules managed by SPARQL queries, so that the rules are applied faster, and thus, increasing the firewall performance. Experimental results show that our proposed methodology totally eliminates the anomalies in the firewall rules as a result of conducting longest matching with proper rules from the dynamically constructed ontology.},
booktitle = {Proceedings of the 4th International Conference on Future Networks and Distributed Systems},
articleno = {16},
numpages = {5},
keywords = {Anomalies, Correlation, DOM, False Negative, False Positive, Firewall, Generalization, IP, Jena, Ontology, Port, RDF, Redundancy, Rule, SPARQL, Shadowing, Software Defined Network.},
location = {St.Petersburg, Russian Federation},
series = {ICFNDS '20}
}

@inproceedings{10.1145/3701716.3717809,
author = {Pellegrino, Maria Angela and Tuozzo, Gabriele},
title = {From Quality Reports to Knowledge Graphs: a Case Study on CSV-to-KG Transformation},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717809},
doi = {10.1145/3701716.3717809},
abstract = {The construction of Knowledge Graphs (KGs) often demands substantial manual effort and domain expertise, especially when converting structured data formats like CSV files into KGs. Recent advancements in Large Language Models (LLMs) offer promising avenues to simplify this process through prompt engineering.This study investigates various prompting strategies-zero-shot, one-shot, prompt chaining, and a hybrid approach-to enable LLMs to automate the creation of KGs from CSV files. Using a dataset containing quality metrics for 2,026 KGs generated by KGHeartBeat, the paper assesses the performance of GPT-4o, GPT-o1 mini, Claude 3.5 Sonnet, and Gemini 1.5 pro, across different prompt configurations. The findings reveal that the hybrid approach consistently produces the most accurate and complete KGs, effectively addressing challenges related to scalability and complexity.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1626–1632},
numpages = {7},
keywords = {comparison, csv converter, data quality, empirical investigation, knowledge graph, llms, prompt engineering, quality assessment},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3487553.3527164,
author = {Verspoor, Karin},
title = {Why Bother Enabling Biomedical Literature Analysis with Semantics?},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3527164},
doi = {10.1145/3487553.3527164},
abstract = {These days, ELMo&nbsp;[3], BERT&nbsp;[1], BART&nbsp;[2] and other similarly cutely-named models appear to have dramatically advanced the state of the art in basically every problem in natural language processing and information retrieval. It can leave a researcher wondering whether there is more to language processing than deploying or fine-tuning contextual word embeddings. What of formal semantics and knowledge representation? What value do these bring to text analysis, either in modelling or in task definitions? In this talk, I will try to explore these questions, from the perspective of my long-running experiences in biomedical information extraction and literature exploration. Perhaps we can shift the academic conversation from a one-model-fits-all solution for individual tasks to a more nuanced consideration of complex, multi-faceted problems in which such models certainly can play a critical role but aren’t necessarily “all you need”&nbsp;[4].},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {822},
numpages = {1},
keywords = {knowledge representation, language models, ontologies, semantics, text mining, word embeddings},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3701551.3703500,
author = {Yoon, Soojin and Ko, Sungho and Kim, Tongyoung and Kang, SeongKu and Yeo, Jinyoung and Lee, Dongha},
title = {Unsupervised Robust Cross-Lingual Entity Alignment via Neighbor Triple Matching with Entity and Relation Texts},
year = {2025},
isbn = {9798400713293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701551.3703500},
doi = {10.1145/3701551.3703500},
abstract = {Cross-lingual entity alignment (EA) enables the integration of multiple knowledge graphs (KGs) across different languages, providing users with seamless access to diverse and comprehensive knowledge. Existing methods, mostly supervised, face challenges in obtaining labeled entity pairs. To address this, recent studies have shifted towards self-supervised and unsupervised frameworks. Despite their effectiveness, these approaches have limitations: (1) Relation passing: mainly focusing on the entity while neglecting the semantic information of relations, (2) Isomorphic assumption: assuming isomorphism between source and target graphs, which leads to noise and reduced alignment accuracy, and (3) Noise vulnerability: susceptible to noise in the textual features, especially when encountering inconsistent translations or Out-of-Vocabulary (OOV) problems. In this paper, we propose ERAlign, an unsupervised and robust cross-lingual EA pipeline that jointly performs &lt;u&gt;E&lt;/u&gt;ntity-level and &lt;u&gt;R&lt;/u&gt;elation-level &lt;u&gt;Align&lt;/u&gt;ment by neighbor triple matching strategy using semantic textual features of relations and entities. Its refinement step iteratively enhances results by fusing entity-level and relation-level alignments based on neighbor triple matching. The additional verification step examines the entities' neighbor triples as the linearized text. This Align-then-Verify pipeline rigorously assesses alignment results, achieving near-perfect alignment even in the presence of noisy textual features of entities. Our extensive experiments demonstrate that the robustness and general applicability of ERAlign improved the accuracy and effectiveness of EA tasks, contributing significantly to knowledge-oriented applications.},
booktitle = {Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining},
pages = {184–193},
numpages = {10},
keywords = {cross-lingual entity alignment, knowledge graph, neighbor triple matching, optimal transport, pretrained language models},
location = {Hannover, Germany},
series = {WSDM '25}
}

@inproceedings{10.1145/3460210.3493552,
author = {Sartini, Bruno and van Erp, Marieke and Gangemi, Aldo},
title = {Marriage is a Peach and a Chalice: Modelling Cultural Symbolism on the Semantic Web},
year = {2021},
isbn = {9781450384575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460210.3493552},
doi = {10.1145/3460210.3493552},
abstract = {In this work, we fill the gap in the Semantic Web in the context of Cultural Symbolism. Building upon earlier work in citesartini_towards_2021, we introduce the Simulation Ontology, an ontology that models the background knowledge of symbolic meanings, developed by combining the concepts taken from the authoritative theory of Simulacra and Simulations of Jean Baudrillard with symbolic structures and content taken from "Symbolism: a Comprehensive Dictionary'' by Steven Olderr. We re-engineered the symbolic knowledge already present in heterogeneous resources by converting it into our ontology schema to create HyperReal, the first knowledge graph completely dedicated to cultural symbolism. A first experiment run on the knowledge graph is presented to show the potential of quantitative research on symbolism.},
booktitle = {Proceedings of the 11th Knowledge Capture Conference},
pages = {201–208},
numpages = {8},
keywords = {knowledge graph, linked data, ontology, semantic web, symbolism},
location = {Virtual Event, USA},
series = {K-CAP '21}
}

@article{10.1145/3558000,
author = {Kleyko, Denis and Rachkovskij, Dmitri and Osipov, Evgeny and Rahimi, Abbas},
title = {A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part II: Applications, Cognitive Models, and Challenges},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3558000},
doi = {10.1145/3558000},
abstract = {This is Part&nbsp;II of the two-part comprehensive survey devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Holographic Reduced Representations&nbsp;[321, 326] is an influential HDC/VSA model that is well known in the machine learning domain and often used to refer to the whole family. However, for the sake of consistency, we use HDC/VSA to refer to the field.Part&nbsp;I of this survey&nbsp;[222] covered foundational aspects of the field, such as the historical context leading to the development of HDC/VSA, key elements of any HDC/VSA model, known HDC/VSA models, and the transformation of input data of various types into high-dimensional vectors suitable for HDC/VSA. This second part surveys existing applications, the role of HDC/VSA in cognitive computing and architectures, as well as directions for future work. Most of the applications lie within the Machine Learning/Artificial Intelligence domain; however, we also cover other applications to provide a complete picture. The survey is written to be useful for both newcomers and practitioners.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {175},
numpages = {52},
keywords = {Artificial intelligence, machine learning, distributed representations, cognitive architectures, cognitive computing, applications, analogical reasoning, hyperdimensional computing, vector symbolic architectures, holographic reduced representations, tensor product representations, matrix binding of additive terms, binary spatter codes, multiply-add-permute, sparse binary distributed representations, sparse block codes, modular composite representations, geometric analogue of holographic reduced representations}
}

@inproceedings{10.1145/3581783.3613434,
author = {Yang, Yuchen},
title = {Encoding and Decoding Narratives: Datafication and Alternative Access Models for Audiovisual Archives},
year = {2023},
isbn = {9798400701085},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581783.3613434},
doi = {10.1145/3581783.3613434},
abstract = {Situated in the intersection of audiovisual archives, computational methods, and immersive interactions, this work probes the increasingly important accessibility issues from a two-fold approach. Firstly, the work proposes an ontological data model to handle complex descriptors (metadata, feature vectors, etc.) with regard to user interactions. Secondly, this work examines text-to-video retrieval from an implementation perspective by proposing a classifier-enhanced workflow to deal with complex and hybrid queries and a training data augmentation workflow to improve performance. This work serves as the foundation for experimenting with novel public-facing access models to large audiovisual archives.},
booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
pages = {9355–9359},
numpages = {5},
keywords = {audiovisual archive, computational archive, experimental museology, text-to-video encoding},
location = {Ottawa ON, Canada},
series = {MM '23}
}

@inproceedings{10.1145/3167132.3167227,
author = {Bernard, Camille and Villanova-Oliver, Marl\`{e}ne and Gensel, J\'{e}r\^{o}me and Dao, Hy},
title = {Modeling changes in territorial partitions over time: ontologies TSN and TSN-change},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167227},
doi = {10.1145/3167132.3167227},
abstract = {Territories are governed, administered and observed from partitions of space into territorial units. All these territorial partitions change over time, for political or administrative reasons. In this paper, we present two innovative ontologies - Territorial Statistical Nomenclature (TSN) and TSN-Change - for the modeling of territorial partitions over time, adopting a Linked Data (LD) approach understandable by both humans and machines. These ontologies are innovative as they are generic, enabling the publication of any territorial partition into the LD Web. Above all, they allow for rich descriptions of changes, from one partition version to another, through Multi-Levels Change Graphs that link together changes that happen at different territorial levels (e.g., major regions or districts levels). These RDF graphs constitute a knowledge base to extract patterns of change and simulate scenarios.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {866–875},
numpages = {10},
keywords = {change ontology, space and time ontology, territorial statistical nomenclature, web of linked data},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3297280.3297367,
author = {Arruda, Mayke Ferreira and Bulc\~{a}o-Neto, Renato Freitas},
title = {Toward a lightweight ontology for privacy protection in IoT},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297367},
doi = {10.1145/3297280.3297367},
abstract = {The literature asserts that the design of an ontology-based privacy model is an essential starting point to address privacy risks in IoT, where connected devices are increasingly capable of monitoring human activities. Due to the omnipresence of data privacy concerns in IoT, we highlight the need for privacy ontologies that combine an expressive vocabulary with extension points but that do not overload the processing of privacy policies data. This paper presents IoT-Priv as a lightweight privacy layer upon IoT basic concepts such as device, sensor, and service. We introduce privacy requirements guiding the IoT-Priv ontology design, match these requirements to the respective privacy terms modeled, and show how to use IoT-Priv through a usage scenario. Finally, we evaluate static metrics and response times of spatial and temporal query filters over instances of privacy policies. Results open the way for the creation of scalable, privacy-enabled systems.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {880–888},
numpages = {9},
keywords = {evaluation, internet of things, ontology, privacy, requirements},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3589335.3651572,
author = {Hsu, Chi-Yang and Cox, Kyle and Xu, Jiawei and Tan, Zhen and Zhai, Tianhua and Hu, Mengzhou and Pratt, Dexter and Chen, Tianlong and Hu, Ziniu and Ding, Ying},
title = {Thought Graph: Generating Thought Process for Biological Reasoning},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651572},
doi = {10.1145/3589335.3651572},
abstract = {We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28\% and LLM baselines by 5.38\% based on cosine similarity to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {537–540},
numpages = {4},
keywords = {bioinformatics, biological process., gene ontology, large language model, natural language processing, semantic web},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3494322.3494337,
author = {Hviid, Jakob and Johansen, Aslak and Fierro, Gabe and Kj\ae{}rgaard, Mikkel Baun},
title = {Service Portability and Information Discovery in Building Operating Systems using Semantic Modeling},
year = {2022},
isbn = {9781450385664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494322.3494337},
doi = {10.1145/3494322.3494337},
abstract = {To achieve cost-efficient IoT based Building Operating Systems (BOS), portable building services are needed. Most previous work has gone into hardware abstraction, while service abstraction has been neglected. This paper presents an information discovery mechanism for BOSs that is based on an ontology which integrates with other semantic models used in the space. The built environment is characterized by extreme heterogeneity; no buildings are entirely alike. Equipment is replaced or updated, control systems and building functionality evolve, as applications, models, forecasters, and controllers improve. For services deployed in such settings to operate at a scale, they must be robust to change. Describing service interfaces using a semantic model, together with the physical context in a building, enables applications to query for their service dependencies. Applications then depend on an abstract query, instead of specific services. For evaluation, nine services running on models of the service ecosystems of three concrete buildings, are implemented, demonstrated, and discussed. Results show services have successfully been made portable and adapts to the changing environments of buildings. Merging and splitting services without code changes to depending services also work as intended, as well as increasing system resilience, by arbitrating similar services.},
booktitle = {Proceedings of the 11th International Conference on the Internet of Things},
pages = {110–117},
numpages = {8},
keywords = {Dependency Resolution., Information Discovery, Ontology, Service Abstraction, Service Discovery},
location = {St.Gallen, Switzerland},
series = {IoT '21}
}

@inproceedings{10.1145/3397125.3397134,
author = {Alaa, Rana and Gawich, Mariam and Fern\'{a}ndez-Veiga, Manuel},
title = {Personalized Recommendation for Online Retail Applications Based on Ontology Evolution},
year = {2020},
isbn = {9781450377492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397125.3397134},
doi = {10.1145/3397125.3397134},
abstract = {The upcoming generation of World Wide Web is signified in semantic web technology that allows future applications to grasp and connect with numerous knowledge bases. Due to its exclusive function in modeling specific domain, Ontology has been playing an essential role in semantic web development. Recommender systems are an indispensable part of online site, which makes their use of high value in recommending items to users according to their interests. The semantic recommender systems recently aim to accomplish the website ontologies to generate semantic recommendations for users' profiles. Therefore, ontology-based semantic recommender systems are used to develop web recommendation. In this paper a recommendation system architecture based on ontology is proposed to give semantic recommendations for each user profile. The proposed system architecture applies two recommendation techniques, content-based filtering and collaborative filtering.},
booktitle = {Proceedings of the 2020 6th International Conference on Computer and Technology Applications},
pages = {12–16},
numpages = {5},
keywords = {Semantic recommender system, intelligent personalization, ontology, ontology evolution, reasoning},
location = {Antalya, Turkey},
series = {ICCTA '20}
}

@inproceedings{10.1145/3555776.3577657,
author = {Ichida, Alexandre Yukio and Meneguzzi, Felipe},
title = {Modeling a Conversational Agent using BDI Framework},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577657},
doi = {10.1145/3555776.3577657},
abstract = {Building conversational agents to help humans in domain-specific tasks is challenging since the agent needs to understand the natural language and act over it while accessing domain expert knowledge. Modern natural language processing techniques led to an expansion of conversational agents, with recent pretrained language models achieving increasingly accurate language recognition results using ever-larger open datasets. However, the black-box nature of such pretrained language models obscures the agent's reasoning and its motivations when responding, leading to unexplained dialogues. We develop a belief-desire-intention (BDI) agent as a task-oriented dialogue system to introduce mental attitudes similar to humans describing their behavior during a dialogue. We compare the resulting model with a pipeline dialogue model by leveraging existing components from dialogue systems and developing the agent's intention selection as a dialogue policy. We show that combining traditional agent modelling approaches, such as BDI, with more recent learning techniques can result in efficient and scrutable dialogue systems.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {856–863},
numpages = {8},
keywords = {belief-desire-intention, task-oriented dialogue systems, autonomous agent, machine learning},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3627341.3630381,
author = {Hong, Sheng and Lai, Yizhong and Li, Yuzhou and You, Yang and Ou, Shuai and Han, Fei and Wu, Tiejun},
title = {A Reference Model for Information Security Applications of Generative Adversarial Network Variants},
year = {2023},
isbn = {9798400708701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627341.3630381},
doi = {10.1145/3627341.3630381},
abstract = {Information security stemming from Generative Adversarial Network (GAN) variants has garnered significant attention. However, a complete reference model targeting this security problem has yet to be established. This paper selects several GAN variants as the research subject and proposes a reference model framework for the information security applications of adversarial generative network variants. The proposed framework is derived using the NIST information security reference model methodology. By conducting a comprehensive analysis of the structure and information security risks of GAN variants, this paper classifies information security attacks on information systems of GAN variants into three categories and maps them onto the security target reference model. The resulting security application reference model can serve as a basis and reference for improving system confidentiality, integrity, and availability, as well as facilitating the design, analysis, and verification of security against malicious attacks. Moreover, the research method employed in this paper is also applicable to information security research of other types of information systems. Therefore, the proposed reference model framework can serve as a valuable contribution to the field of information security and advance the development of effective countermeasures against adversarial generative network variants.},
booktitle = {Proceedings of the 2023 International Conference on Computer, Vision and Intelligent Technology},
articleno = {7},
numpages = {8},
keywords = {Adversarial Generative Network Variant, Information Security, Security Reference Model, System Security},
location = {Chenzhou, China},
series = {ICCVIT '23}
}

@article{10.1145/3494572,
author = {Musso, Marta and Arnold, Kerstin and Nanni, Federico and Cannelli, Beatrice},
title = {What Is in a &lt;unittitle&gt;? Cross-lingual Topic Detection \&amp; Information Retrieval in Archives Portal Europe},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3494572},
doi = {10.1145/3494572},
abstract = {Archives Portal Europe (APE, ) is the portal of European archives, an aggregator that connects on a single research point the catalogues and digitised archival material of all archives in and about Europe. It currently hosts material from more than 30 countries and from a variety of archival institutions (such as State archives, city archives, university and parish archives, private institutions, and more). It is maintained by the Archives Portal Europe Foundation, an international consortium of State archives and other archival institutions that aim to connect the archival material of single institutions into one digital repository to allow universal access to the archival heritage of Europe, promoting new forms of archival research beyond national or local boundaries. One of the research tools made available by Archives Portal Europe is by topics; however, these are currently maintained manually by the archivists, and the vast amount of archival material ingested in the portal makes it impossible to have a comprehensive body of topics that describe the whole of the APE repository. Archives are traditionally not organised by their subject content, but around the entity (person, organization, body) that created and/or collected the documents in the course of their activities. While this is an undisputed pillar of archival management, the availability of online digital repositories for archival research requires new tools for digital archival research, particularly when different archival traditions from different countries and different types of institutions are merged into a unique research portal. Topic detection becomes a fundamental tool to guide archival research and to allow archives to be accessible to potentially world-wide users in a situation where national and linguistics barriers blur or are re-defined. This article presents the preliminary results and plan for future iterations of an AI tool for automated topic detection in a multi- lingual environment, where human-created taxonomies act as bases for the algorithms to aggregate relevant material around a specific topic. The development is based on supervised machine learning, with a combination of human inputs in different languages, and of the usage of Wikipedia pages to model the relevant vocabulary and entities.},
journal = {J. Comput. Cult. Herit.},
month = mar,
articleno = {25},
numpages = {23},
keywords = {Digital Archives, Digital Research, Automated topic modelling, search engines, Archives Portal Europe}
}

@inproceedings{10.1145/3297280.3297507,
author = {Laadhar, Amir and Ghozzi, Faiza and Megdiche, Imen and Ravat, Franck and Teste, Olivier and Gargouri, Faiez},
title = {Partitioning and local matching learning of large biomedical ontologies},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297507},
doi = {10.1145/3297280.3297507},
abstract = {Conventional ontology matching systems are not well-tailored to ensure sufficient quality alignments for large ontology matching tasks. In this paper, we propose a local matching learning strategy to align large and complex biomedical ontologies. We define a novel partitioning approach that breakups large ontology alignment task into a set of local sub-matching tasks. We perform a machine learning approach for each local sub-matching task. We build a local machine learning model for each sub-matching task without any user involvement. Each local matching learning model automatically provides adequate matching settings for each local sub-matching task. Our results show that: (i) partitioning approach outperforms existing techniques, (ii) local matching while using a specific machine learning model for each sub-matching task yields to promising results and (iii) the combination between partitioning and machine learning increases the overall result.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2285–2292},
numpages = {8},
keywords = {machine learning, ontology matching, ontology partitioning, semantic web},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1145/3606702,
author = {Adamou, Alessandro and Picca, Davide and Hou, Yumeng and Loreto Granados-Garc\'{\i}a, Paula},
title = {The Facets of Intangible Heritage in Southern Chinese Martial Arts: Applying a Knowledge-driven Cultural Contact Detection Approach},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3606702},
doi = {10.1145/3606702},
abstract = {Investigating the intangible nature of a cultural domain can take multiple forms, addressing, for example, the aesthetic, epistemic, and social dimensions of its phenomenology. The context of Southern Chinese martial arts is of particular significance, as it carries immaterial components of all these aspects: The technical and stylistic framework of a martial art system; the imagery associated to movements; and the transmission of knowledge orally, practically, or through influence, are but examples of intangible characteristics that can and should be captured, not unlike cultural artifacts. The latter case—the one of formalizing cultural influence through its various forms of evidenceis emblematic as well as largely untrodden ground. A previous attempt at detecting cultural influence computationally was made in the context of Roman archaeology, though the binding of that early effort with the domain model was tight; also, there has hardly been any prior dedicated effort to model the martial arts domain through ontologies. In this article, we present the realization of the full cycle of a computational approach to investigating cultural contact in Southern Chinese martial arts. The entire approach is predicated upon the usage of standards and techniques of the Semantic Web and formal knowledge. Starting from a modular domain ontology, which models martial arts independently of the goal of capturing cultural influence, we perform knowledge extraction from archival material from the Hong Kong Martial Arts Living Archive and generate a dataset of the results modeled after said ontology. Then, we combine the resulting knowledge base with a rule model that represents ways to infer knowledge of potential contact between cultures based on the evidence present in the knowledge base. The results offer an insight into how an inference-based computational model can be applied to detect interesting facts even in the as-yet underexplored domain of intangible cultural heritage. The implemented workflow shows that the full-cycle employment of semantic technologies can offer the ground truth required for largely different approaches, such as statistical and machine learning ones, to operate.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {63},
numpages = {27},
keywords = {Intangible cultural heritage, digitization, Semantic Web, embodied knowledge, knowledge representation, ontologies, inferencing}
}

@article{10.1145/3548458,
author = {Chen, Mu-Yen and Lai, Yi-Wei},
title = {Using Fuzzy Clustering with Deep Learning Models for Detection of COVID-19 Disinformation},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3548458},
doi = {10.1145/3548458},
abstract = {Since the beginning of 2020, the COVID-19 pandemic has killed millions of people around the world, leading to a worldwide panic that has fueled the rapid and widespread dissemination of COVID-19-related disinformation on social media. The phenomenon, described by the World Health Organization (WHO) as an "indodemic" presents a serious challenge to governments and public health authorities, but the spread of misinformation has made human detection less efficient than the rate of spread. While there have been many studies developing automated detection techniques for COVID-19 fake news, the results often refer to high accuracy but rarely to model detection time. This research uses fuzzy theory to extract features and uses multiple deep learning model frameworks to detect Chinese and English COVID-19 misinformation. With the reduction of text features, the detection time of the model is significantly reduced, and the model accuracy does not drop excessively. This study designs two different feature extraction methods based on fuzzy classification and compares the results with different deep learning models. BiLSTM was found to provide the best detection results for COVID-19 misinformation by directly using deep learning models, with 99\% accuracy in English and 86\% accuracy in Chinese. Applying fuzzy clustering to English COVID-19 fake news data features maintains 99\% accuracy while reducing detection time by 10\%. For Chinese misinformation, detection time is reduced by 15\% at the cost of an 8\% drop in accuracy.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
keywords = {Misinformation detection, COVID-19, Deep learning model, Fuzzy clustering}
}

@article{10.1145/3191832,
author = {Bienvenu, Meghyn and Kikot, Stanislav and Kontchakov, Roman and Podolskii, Vladimir V. and Zakharyaschev, Michael},
title = {Ontology-Mediated Queries: Combined Complexity and Succinctness of Rewritings via Circuit Complexity},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {65},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/3191832},
doi = {10.1145/3191832},
abstract = {We give solutions to two fundamental computational problems in ontology-based data access with the W3C standard ontology language OWL&nbsp;2&nbsp;QL: the succinctness problem for first-order rewritings of ontology-mediated queries (OMQs) and the complexity problem for OMQ answering. We classify OMQs according to the shape of their conjunctive queries (treewidth, the number of leaves) and the existential depth of their ontologies. For each of these classes, we determine the combined complexity of OMQ answering and whether all OMQs in the class have polynomial-size first-order, positive existential, and nonrecursive datalog rewritings. We obtain the succinctness results using hypergraph programs, a new computational model for Boolean functions, which makes it possible to connect the size of OMQ rewritings and circuit complexity.},
journal = {J. ACM},
month = aug,
articleno = {28},
numpages = {51},
keywords = {Ontology-based data access, computational complexity, ontology-mediated query, query rewriting, succinctness}
}

@inproceedings{10.1145/3579051.3579070,
author = {Gu, Zhenzhen and Lanti, Davide and Mosca, Alessandro and Xiao, Guohui and Xiong, Jing and Calvanese, Diego},
title = {Ontology-based Data Federation},
year = {2023},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579051.3579070},
doi = {10.1145/3579051.3579070},
abstract = {Ontology-based data access (OBDA) is a well-established approach to information management which facilitates the access to a (single) relational data source through the mediation of a high-level ontology, and the use of a declarative mapping linking the data layer to the ontology. We formally introduce here the notion of ontology-based data federation (OBDF) to denote a framework that combines OBDA with a data federation layer where multiple, possibly heterogeneous sources are virtually exposed as a single relational database. We discuss opportunities and challenges of OBDF, and provide techniques to deliver efficient query answering in an OBDF setting. Such techniques are validated through an extensive experimental evaluation based on the Berlin SPARQL Benchmark.},
booktitle = {Proceedings of the 11th International Joint Conference on Knowledge Graphs},
pages = {10–19},
numpages = {10},
keywords = {Data federation, OBDA, Query optimization},
location = {Hangzhou, China},
series = {IJCKG '22}
}

@inproceedings{10.1145/3274192.3274204,
author = {Paim, Polianna and Prietch, Soraia and Duarte, Anderson},
title = {CoDesign in the Exploratory Phase of an Assistive Technology product Design to support the Teaching-Learning Process of Brazilian-Portuguese Language for Visual Persons},
year = {2018},
isbn = {9781450366014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274192.3274204},
doi = {10.1145/3274192.3274204},
abstract = {Libras is a communication and expression language of many Visual Person (VP) in Brazil. In National High School Examination (ENEM), for candidates to a vacancy in higher education institutions, the essay must be written in Portuguese, not taking into account the first language of the candidate. This paper presents an adapted framework that groups concepts of CoDesign, technology adoption, HCI life cycle and Semantic Numbers, and also shows the results of first phase of research. This phase consists of a contextual analysis of an Assistive Technology project for Portuguese teaching as a second language for VP. The adapted framework aims to reach objectives, working with interested parts, to be accessible and potentially adopted by them. As results we have defined Interested Parts Diagram, discussed responses applied surveys and elaborated Personas. Also, it was possible to observe the relation of the PV with the Portuguese, their wishes and difficulties with ENEM.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {12},
numpages = {9},
keywords = {Assistive Technology, CoDesign, Semantic Numbers, Visual People},
location = {Bel\'{e}m, Brazil},
series = {IHC '18}
}

@inproceedings{10.1145/3360901.3364424,
author = {Chen, Jieying and Alghamdi, Ghadah and Schmidt, Renate A. and Walther, Dirk and Gao, Yongsheng},
title = {Ontology Extraction for Large Ontologies via Modularity and Forgetting},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364424},
doi = {10.1145/3360901.3364424},
abstract = {We are interested in the computation of ontology extracts based on forgetting from large ontologies in real-world scenarios. Such scenarios require nearly all of the terms in the ontology to be forgotten, which poses a significant challenge to forgetting tools. In this paper we show that modularization and forgetting can be combined beneficially in order to compute ontology extracts. While a module is a subset of axioms of a given ontology, the solution of forgetting (also known as a uniform interpolant) is a compact representation of the ontology limited to a subset of the signature. The approach introduced in this paper uses an iterative workflow of four stages: (i)~extension of the given signature and, if needed partitioning, (ii)~modularization, (iii)~forgetting, and (iv)~evaluation by domain expert. For modularization we use three kinds of modules: locality-based, semantic and minimal subsumption modules. For forgetting three tools are used: NUI, LETHE and FAME. An evaluation on the SNOMED CT and NCIt ontologies for standard concept name lists showed that precomputing ontology modules reduces the number of terms that need to be forgotten. An advantage of the presented approach is high precision of the computed ontology extracts.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {45–52},
numpages = {8},
keywords = {description logics, forgetting, knowledge abstraction, knowledge management, knowledge representation and reasoning, ontology abstraction, ontology modularity, uniform interpolation},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.5555/3615924.3623629,
author = {Jiang, Hao and Tsiounis, Konstantinos and Kontogiannis, Kostas},
title = {Digital Twin Models for Resource Oriented Service Systems},
year = {2023},
publisher = {IBM Corp.},
address = {USA},
abstract = {Resource-oriented service computing has emerged as one of the major paradigms for building large scale distributed systems. These systems can grow very complex and require access to diverse re-sources. In order to shorten the development time of such systems, we can consider the use of digital twin models. We propose a frame-work whereby a resource-oriented service computing system can be represented as a collection of models that denote three major perspectives, namely structure, behavior and intent of stakeholders’ goals. Furthermore, in order for the digital twin model to be of practical use, the models denoting the different perspectives have to be integrated. In this respect, we propose an architecture which allows these models to exchange information using open source topic-based publish-subscribe systems, and discuss the elements of its associated run-time engine.},
booktitle = {Proceedings of the 33rd Annual International Conference on Computer Science and Software Engineering},
pages = {226–229},
numpages = {4},
keywords = {Resource Oriented Systems, Service Computing, REST, Digital Twin Models, DevOps, Middleware.},
location = {Las Vegas, NV, USA},
series = {CASCON '23}
}

@inproceedings{10.1145/3638209.3638213,
author = {Glass, Ayse and Noennig, Jorg R. and Bek, Burak and Glass, Roman and Menges, Eylul K. and Okhrin, Iryna and Baddam, Pramod and Sanchez, Mariela Rossana and Senthil, Gunalan and J\"{a}kel, Ren\'{e}},
title = {Innovative Urban Design Simulation: Utilizing Agent-Based Modelling through Reinforcement Learning},
year = {2024},
isbn = {9798400709067},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638209.3638213},
doi = {10.1145/3638209.3638213},
abstract = {Data-driven design for cities is improving the quality of everyday life of citizens and optimizes the usage of resources. A new aspect is artificial intelligence, which Smart Cities could greatly benefit from. A central problem for urban designers is the unavailability of data to make relevant decisions. Agent-based simulations enable a view of the dynamic properties of the urban system, generating data in its course. However, the simulation must remain sufficiently simple to remain in the realm of computability. The research question of this paper is: How can we make agents behave more realistically to analyze citizens’ mobility behavior? To solve this problem, we first created a simulated virtual environment, where agents can move freely in a small part of a city, the harbor area in Hamburg, Germany. We assumed that happiness is a crucial motivating factor for the movement of citizens. A survey of 130 citizens provided the weights that govern the simulated environment and the happiness score assignation of places. As an AI method, we then used Reinforcement Learning as a general model and Q-learning as an algorithm to generate a baseline. Through randomly traversing the model environment a baseline was created. We are in the process of enhancing Reinforcement Learning with a Deep Q-Network to make the actors learn. Early experiments show a significant improvement over a tabular Q-learning approach. This paper contributes to the literature of urban planning, and data-driven architectural design. The main contribution is replacing the inefficient search for a global maximum of the happiness function, with an efficient local solution global maximum. This has implications for further research in the generation of synthetic data through simulations.},
booktitle = {Proceedings of the 2023 6th International Conference on Computational Intelligence and Intelligent Systems},
pages = {20–25},
numpages = {6},
keywords = {agent-based modeling, artificial intelligence, city simulations, smart cities, synthetic data, urban design},
location = {Tokyo, Japan},
series = {CIIS '23}
}

@article{10.1109/TCBB.2022.3181300,
author = {Tian, Zhen and Fang, Haichuan and Teng, Zhixia and Ye, Yangdong},
title = {GOGCN: Graph Convolutional Network on Gene Ontology for Functional Similarity Analysis of Genes},
year = {2022},
issue_date = {March-April 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3181300},
doi = {10.1109/TCBB.2022.3181300},
abstract = {The measurement of gene functional similarity plays a critical role in numerous biological applications, such as gene clustering, the construction of gene similarity networks. However, most existing approaches still rely heavily on traditional computational strategies, which are not guaranteed to achieve satisfactory performance. In this study, we propose a novel computational approach called &lt;bold&gt;GOGCN&lt;/bold&gt; to measure gene functional similarity by modeling the Gene Ontology (&lt;bold&gt;GO&lt;/bold&gt;) through Graph Convolutional Network (&lt;bold&gt;GCN&lt;/bold&gt;). GOGCN is a graph-based approach that performs sufficient representation learning for terms and relations in the GO graph. First, GOGCN employs the GCN-based knowledge graph embedding (KGE) model to learn vector representations (i.e., embeddings) for all entities (i.e., terms). Second, GOGCN calculates the semantic similarity between two terms based on their corresponding vector representations. Finally, GOGCN estimates gene functional similarity by making use of the pair-wise strategy. During the representation learning period, GOGCN promotes semantic interaction between terms through GCN, thereby capturing the rich structural information of the GO graph. Further experimental results on various datasets suggest that GOGCN is superior to the other state-of-the-art approaches, which shows its reliability and effectiveness.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jun,
pages = {1053–1064},
numpages = {12}
}

@inproceedings{10.1145/3608251.3608255,
author = {Li, Jun and Zhou, Yong and Liu, Xin and Liu, Jinsong and Li, Qing and Kong, Xiangjun and Niu, Guankai},
title = {Study on the Reference Architecture and Multi-Dimensional Fusion Framework of Production Equipment Models},
year = {2023},
isbn = {9798400707919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3608251.3608255},
doi = {10.1145/3608251.3608255},
abstract = {With the accelerated integration of the new generation information technology and manufacturing industry, the performance of production equipment continues to be optimized, evolved and upgraded iteratively. The production equipment model presents a multi-dimensional, multi-level, full life cycle integration development trend, which has become the key to improve the innovative capability and digital management level of production equipment. Aiming at the problems of multiple types of production equipment models, such as complex structure, difficult integration and low application efficiency, this study analyzed the relationship and interaction mechanism among production equipment models from the perspectives of life cycle, hierarchical structure and multi-dimension, and built a production equipment model architecture. Based on the model architecture of production equipment, a general multi-dimensional fusion framework of production equipment models was proposed, which takes the mechanism model as the core. This study can provide reference for the construction, integration and application of production equipment models.},
booktitle = {Proceedings of the 2023 15th International Conference on Computer Modeling and Simulation},
pages = {172–183},
numpages = {12},
location = {Dalian, China},
series = {ICCMS '23}
}

@inproceedings{10.1145/3419604.3419619,
author = {Sebbaq, Hanane and el Faddouli, Nour-eddine and Bennani, Samir},
title = {Recommender System to Support MOOCs Teachers: Framework based on Ontology and Linked Data},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419619},
doi = {10.1145/3419604.3419619},
abstract = {The proliferation of Massive Open Online Courses (MOOCs) has generated conflicting opinions about their quality. In this paper, we aim at improving the quality of MOOCs through assisting teachers and designers from the initiation phase of MOOCs. For this purpose, we propose a recommendation system Framework based on the knowledge about teachers and MOOCs. Our approach aims to overcome the problems of traditional recommendation systems, by using and integrating different techniques: modeling via ontologies, semantic web technologies, extracting and integrating Linked Data from different sources, ontology mapping and semantic similarity measures.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {18},
numpages = {7},
keywords = {Linked Data, MOOC, Ontology, Ontology mapping, Recommender System, Semantic Web, Semantic similarity},
location = {Rabat, Morocco},
series = {SITA'20}
}

@article{10.1145/3637427,
author = {Atrey, Akanksha and Zakaria, Camellia and Balan, Rajesh and Shenoy, Prashant},
title = {W4-Groups: Modeling the Who, What, When and Where of Group Behavior via Mobility Sensing},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CSCW1},
url = {https://doi.org/10.1145/3637427},
doi = {10.1145/3637427},
abstract = {Human social interactions occur in group settings of varying sizes and locations, depending on the type of social activity. The ability to distinguish group formations based on their purposes transforms how group detection mechanisms function. Not only should such tools support the effective detection of serendipitous encounters, but they can derive categories of relation types among users. Determining who is involved, what activity is performed, and when and where the activity occurs are critical to understanding group processes in greater depth, including supporting goal-oriented applications (e.g., performance, productivity, and mental health) that require sensing social factors. In this work, we propose W4-Groups that captures the functional perspective of variability and repeatability when automatically constructing short-term and long-term groups via multiple data sources (e.g., WiFi and location check-in data). We design and implement W4-Groups to detect and extract all four group features who-what-when-where from the user's daily mobility patterns. We empirically evaluate the framework using two real-world WiFi datasets and a location check-in dataset, yielding an average of 92\% overall accuracy, 96\% precision, and 94\% recall. Further, we supplement two case studies to demonstrate the application of W4-Groups for next-group activity prediction and analyzing changes in group behavior at a longitudinal scale, exemplifying short-term and long-term occurrences.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {150},
numpages = {29},
keywords = {group modeling, next activity prediction, social interactions, user mobility}
}

@inproceedings{10.1145/3629264.3629273,
author = {Xiao, Zheng and Cao, Haowei and Zheng, Dongwei},
title = {Model construction and implementation of digital twin data for body workshop},
year = {2023},
isbn = {9798400700576},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629264.3629273},
doi = {10.1145/3629264.3629273},
abstract = {Data is very important for Digital Twin Workshop (DTW). Data in DTW is not only to store it, but also needs to be associated with different physical information. Knowledge graph (KG) can acquire, integrate and utilize the information of the whole life of cycle process. This paper proposes the construction of digital twin data (DTD) by creation process and feature extraction methods of knowledge graph. The DTD combines the actual production data and the time-varying characteristics of geometry, motion and behavior model of the body workshop. It is able to realize the integration with DTW and graph database storage.},
booktitle = {Proceedings of the 2023 7th International Conference on Computing and Data Analysis},
pages = {1–7},
numpages = {7},
keywords = {Data model, Digital twin workshop, Knowledge graph},
location = {Guiyang, China},
series = {ICCDA '23}
}

@inbook{10.1145/3581906.3581912,
author = {Nikolaou, Charalampos},
title = {Geospatial Ontologies},
year = {2023},
isbn = {9798400707407},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3581906.3581912},
booktitle = {Geospatial Data Science: A Hands-on Approach for Building Geospatial Applications Using Linked Data Technologies},
pages = {67–84},
numpages = {18}
}

@inproceedings{10.1145/3447568.3448547,
author = {Zarri, Gian Piero},
title = {Using a High-Level Conceptual Model as a Support for the Generalized World Entities (GWEs) Paradigm},
year = {2021},
isbn = {9781450376556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447568.3448547},
doi = {10.1145/3447568.3448547},
abstract = {This paper concerns the Generalized World Entities (GWEs) paradigm, an extension of the ordinary IoT/WoT (Internet of Things/Web of Things) approach. GWEs offer a unified way to seamlessly model i) conceptual representations of physical objects, humans, robots and low-level events and ii) higher level of abstractions corresponding to structured situations/behaviors implying mutual relationships among low level entities. The GWEs approach is currently implemented in terms of NKRL, the "Narrative Knowledge Representation Language", which is both a Knowledge Representation language and a Computer Science environment. It is expected to represent a significant contribution with respect to bridge the gap between the recognition of entities at sensor level and their description/management at conceptual level.},
booktitle = {Proceedings of the 10th International Conference on Information Systems and Technologies},
articleno = {39},
numpages = {8},
keywords = {Generalized World Entities, IoT/WoT, high-level abstraction entities, inferences, ontologies, physical entities, seamless conceptual modelling, semantic approach},
location = {Lecce, Italy},
series = {ICIST '20}
}

@inproceedings{10.1145/3167132.3167343,
author = {Besbes, Ghada and Baazaoui-Zghal, Hajer},
title = {Fuzzy ontologies for search results diversification: application to medical data},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167343},
doi = {10.1145/3167132.3167343},
abstract = {Fuzzy ontologies offer an efficient representation of uncertain information in natural language and this representation allows a better interpretation of user queries and documents. Integrating fuzzy ontologies in a search results diversification process may improve the quality of returned documents since diversification helps covering the maximum of user's needs. In this context, we propose an ontology based diversification approach for search results applied to medical domain. The proposal first analyses the query in order to extract medical concepts. A contextual ontology fuzzification is then applied in order to offer an understanding of the user's information needs and finally a fuzzy search result diversification is performed in order to improve the ranking quality of returned documents. We perform a thorough experimental evaluation of our proposal with CLEF e-health 2016 topics. Evaluation results show a major improvement in precision and ranking.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1968–1975},
numpages = {8},
keywords = {fuzzy ontology, information retrieval, medical data, search results diversification, semantic web},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3503047.3503077,
author = {Nast, Benjamin and Sandkuhl, Kurt},
title = {Meta-Model and Tool Support for the Organizational Aspects of Internet-of-Things Development Methods: Organizational Aspects of IoT Development Methods},
year = {2022},
isbn = {9781450385862},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503047.3503077},
doi = {10.1145/3503047.3503077},
abstract = {The Internet-of-Things (IoT) has long become reality and contributes to the digital transformation of many industrial domains. IoT technologies are at the core of industry 4.0 application scenarios, contribute to cyber-physical system implementation, smart connected products and new business models exploiting their potential. There is plenty of work on how to specify, design and implement IoT solutions, but a lot of enterprises struggle to create business value from IoT technology because they have difficulties to define the organizational integration. Methodologies for model-driven engineering (MDE) of IoT solutions should encompass both, organizational and system development and integration, but existing model-based approaches focus on the technical perspective. The paper proposes a modeling approach integrated into enterprise modeling techniques to compensate for this lack. The enterprise modeling language 4EM is extended by adding a method component for IoT modeling. The main contributions of this paper are (a) a summary of the state-of-research in the field, (b) an industrial case for model-based IoT development, and (c) a meta-model and tool support for IoT modelling.},
booktitle = {Proceedings of the 3rd International Conference on Advanced Information Science and System},
articleno = {27},
numpages = {6},
keywords = {Internet-of-Things, Modelling methodologies},
location = {Sanya, China},
series = {AISS '21}
}

@inproceedings{10.1145/3340531.3412768,
author = {Zhou, Lu and Shimizu, Cogan and Hitzler, Pascal and Sheill, Alicia M. and Estrecha, Seila Gonzalez and Foley, Catherine and Tarr, Duncan and Rehberger, Dean},
title = {The Enslaved Dataset: A Real-world Complex Ontology Alignment Benchmark using Wikibase},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412768},
doi = {10.1145/3340531.3412768},
abstract = {Ontology alignment has taken a critical place for helping heterogeneous resources to interoperate. It has been studied for over a decade, and over that time many alignment systems and methods have been developed by researchers to find simple 1:1 equivalence matches between two ontologies. However, very few alignment systems focus on finding complex correspondences. Even if the complex alignment systems are developed, the performance of finding complex relations still has a lot of room for improvement. One reason for this limitation may be that there are still few applicable alignment benchmarks that contain such complex relationships that can raise researchers' interests. In this paper, we propose a real-world dataset from the Enslaved project as a potential complex alignment benchmark. The benchmark consists of two resources, the Enslaved Ontology along with a Wikibase repository holding a large number of instance data from the Enslaved project, as well as a manually created reference alignment between them. The alignment was developed in consultation with domain experts in the digital humanities. The alignment not only includes simple 1:1 equivalence correspondences, but also more complex m:n equivalence and subsumption correspondences and are provided in both Expressive and Declarative Ontology Alignment Language (EDOAL) format and rule syntax. The Enslaved benchmark has been incorporated into the Ontology Alignment Evaluation Initiative (OAEI) 2020 and is completely free for public use to assist the researchers in developing and evaluating their complex alignment algorithms.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {3197–3204},
numpages = {8},
keywords = {benchmark, knowledge graph, ontology alignment, wikibase},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}


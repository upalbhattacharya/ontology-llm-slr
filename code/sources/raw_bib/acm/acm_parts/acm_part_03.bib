@inproceedings{10.1145/3167132.3167343,
author = {Besbes, Ghada and Baazaoui-Zghal, Hajer},
title = {Fuzzy ontologies for search results diversification: application to medical data},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167343},
doi = {10.1145/3167132.3167343},
abstract = {Fuzzy ontologies offer an efficient representation of uncertain information in natural language and this representation allows a better interpretation of user queries and documents. Integrating fuzzy ontologies in a search results diversification process may improve the quality of returned documents since diversification helps covering the maximum of user's needs. In this context, we propose an ontology based diversification approach for search results applied to medical domain. The proposal first analyses the query in order to extract medical concepts. A contextual ontology fuzzification is then applied in order to offer an understanding of the user's information needs and finally a fuzzy search result diversification is performed in order to improve the ranking quality of returned documents. We perform a thorough experimental evaluation of our proposal with CLEF e-health 2016 topics. Evaluation results show a major improvement in precision and ranking.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1968–1975},
numpages = {8},
keywords = {semantic web, search results diversification, medical data, information retrieval, fuzzy ontology},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3510455.3512771,
author = {Weyssow, Martin and Sahraoui, Houari and Liu, Bang},
title = {Better modeling the programming world with code concept graphs-augmented multi-modal learning},
year = {2022},
isbn = {9781450392242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510455.3512771},
doi = {10.1145/3510455.3512771},
abstract = {The progress made in code modeling has been tremendous in recent years thanks to the design of natural language processing learning approaches based on state-of-the-art model architectures. Nevertheless, we believe that the current state-of-the-art does not focus enough on the full potential that data may bring to a learning process in software engineering. Our vision articulates on the idea of leveraging multi-modal learning approaches to modeling the programming world. In this paper, we investigate one of the underlying idea of our vision whose objective based on concept graphs of identifiers aims at leveraging high-level relationships between domain concepts manipulated through particular language constructs. In particular, we propose to enhance an existing pretrained language model of code by joint-learning it with a graph neural network based on our concept graphs. We conducted a preliminary evaluation that shows gain of effectiveness of the models for code search using a simple joint-learning method and prompts us to further investigate our research vision.},
booktitle = {Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {21–25},
numpages = {5},
keywords = {multi-modal learning, concept graphs, code search, code modeling},
location = {Pittsburgh, Pennsylvania},
series = {ICSE-NIER '22}
}

@inproceedings{10.1145/3706598.3713491,
author = {Yeh, Catherine and Ren, Donghao and Assogba, Yannick and Moritz, Dominik and Hohman, Fred},
title = {Exploring Empty Spaces: Human-in-the-Loop Data Augmentation},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713491},
doi = {10.1145/3706598.3713491},
abstract = {Data augmentation is crucial to make machine learning models more robust and safe. However, augmenting data can be challenging as it requires generating diverse data points to rigorously evaluate model behavior on edge cases and mitigate potential harms. Creating high-quality augmentations that cover these “unknown unknowns” is a time- and creativity-intensive task. In this work, we introduce Amplio, an interactive tool to help practitioners navigate “unknown unknowns” in unstructured text datasets and improve data diversity by systematically identifying empty data spaces to explore. Amplio includes three human-in-the-loop data augmentation techniques: Augment with Concepts, Augment by Interpolation, and Augment with Large Language Model. In a user study with 18 professional red teamers, we demonstrate the utility of our augmentation methods in helping generate high-quality, diverse, and relevant model safety prompts. We find that Amplio enabled red teamers to augment data quickly and creatively, highlighting the transformative potential of interactive augmentation workflows.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {847},
numpages = {19},
keywords = {Human-in-the-loop data augmentation, interactive visualization, data diversity, sparse autoencoders, language models},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3649921.3650001,
author = {Zhou, Hongwei and Zhu, Jichen and Mateas, Michael and Wardrip-Fruin, Noah},
title = {The Eyes, the Hands and the Brain: What can Text-to-Image Models Offer for Game Design and Visual Creativity?},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3650001},
doi = {10.1145/3649921.3650001},
abstract = {Text-to-image models such as DALL-E, Stable Diffusion, and Midjourney have seen a boom in development and adoption in both commercial and hobbyist spaces. This paper is a theoretical analysis aimed at informing the development of games that help improve critical literacy around text-to-image models. It asks: what assumptions and perspectives do text-to-image models have on visual creativity, and how do we bring that out through games? We propose a theory to differentiate between seeing an image through the expression of color, shapes and lines, and seeing an image through the recognition of concepts and ideas. These two ways of seeing are two different ways of orienting the player/user to their visual creativity. While traditional painting mechanics emphasize the former, text-to-image interfaces emphasize the latter. We deploy this perspective to study games with traditional painting interactions and games with text-to-image interactions. This paper hopes to contribute to design both broadly for games about visual creativity, and narrowly for gameplay with text-to-image models — specifically, how the latter fosters a different type of visual creativity than traditional painting interactions.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {23},
numpages = {13},
keywords = {Game Design, Painting, Stable Diffusion, Text-to-Image, Visual Creativity},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@inproceedings{10.1145/3417990.3418741,
author = {Boubekeur, Younes and Mussbacher, Gunter and McIntosh, Shane},
title = {Automatic assessment of students' software models using a simple heuristic and machine learning},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3418741},
doi = {10.1145/3417990.3418741},
abstract = {Software models are increasingly popular. To educate the next generation of software engineers, it is important that they learn how to model software systems well, so that they can design them effectively in industry. It is also important that instructors have the tools that can help them assess students' models more effectively. In this paper, we investigate how a tool that combines a simple heuristic with machine learning techniques can be used to help assess student submissions in model-driven engineering courses. We apply our proposed technique to first identify submissions of high quality and second to predict approximate letter grades. The results are comparable to human grading and a complex rule-based technique for the former and surprisingly accurate for the latter.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {20},
numpages = {10},
keywords = {heuristics, grading, domain modeling, assessment, Umple},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3613372.3614202,
author = {Luz, Carlos and Oliveirajr, Edson and Steinmacher, Igor},
title = {A Conceptual Model to Support Teaching of Software Engineering Controlled (Quasi-)Experiments},
year = {2023},
isbn = {9798400707872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613372.3614202},
doi = {10.1145/3613372.3614202},
abstract = {Throughout controlled experimentation, it is possible to provide evidence of the software being developed. In the academic environment, Experimentation in Software Engineering (ESE) is essential to understanding cause-effect relations, enabling a vision of the development process, and taking action on actual events in the software industry. As much as the experimentation processes have been used in industry and academia, there is a lack of formalization of the principles of ESE teaching and artifacts that can be useful to support it in higher education. One of the means to contribute to such a topic would be the design of a conceptual model, which is widely discussed in the literature, thus applying empirical methods for a better understanding of the context and representation of ESE teaching. Thus, in this paper, we developed a conceptual model to support the teaching of controlled experiments and quasi-experiments. To design the conceptual model, we carried out an analysis of metadata from controlled experiments and quasi-experiments in the literature and conducted a survey to collect data from instructors who teach ESE. We evaluated the model with the Technology Acceptance Model (TAM). Results consist of a feasible conceptual model aiming to standardize the basic concepts of ESE and further support the production and reuse of ESE materials.},
booktitle = {Proceedings of the XXXVII Brazilian Symposium on Software Engineering},
pages = {236–245},
numpages = {10},
keywords = {Concepts, Conceptual Modeling, Controlled Experimentation, TAM Model, Teaching of Controlled Experimentation},
location = {Campo Grande, Brazil},
series = {SBES '23}
}

@proceedings{10.1145/3565472,
title = {UMAP '23: Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
year = {2023},
isbn = {9781450399326},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Limassol, Cyprus}
}

@inproceedings{10.1145/3411764.3445522,
author = {Levy, Ariel and Agrawal, Monica and Satyanarayan, Arvind and Sontag, David},
title = {Assessing the Impact of Automated Suggestions on Decision Making: Domain Experts Mediate Model Errors but Take Less Initiative},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445522},
doi = {10.1145/3411764.3445522},
abstract = {Automated decision support can accelerate tedious tasks as users can focus their attention where it is needed most. However, a key concern is whether users overly trust or cede agency to automation. In this paper, we investigate the effects of introducing automation to annotating clinical texts&nbsp;—&nbsp;a multi-step, error-prone task of identifying clinical concepts (e.g., procedures) in medical notes, and mapping them to labels in a large ontology. We consider two forms of decision aid: recommending which labels to map concepts to, and pre-populating annotation suggestions. Through laboratory studies, we find that 18 clinicians generally build intuition of when to rely on automation and when to exercise their own judgement. However, when presented with fully pre-populated suggestions, these expert users exhibit less agency: accepting improper mentions, and taking less initiative in creating additional annotations. Our findings inform how systems and algorithms should be designed to mitigate the observed issues.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {72},
numpages = {13},
keywords = {text tagging, ontology, mental model, human-AI teams, clinical annotation, agency},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.1145/3726302.3730324,
author = {Dammu, Preetam Prabhu Srikar and Naidu, Himanshu and Shah, Chirag},
title = {Dynamic-KGQA: A Scalable Framework for Generating Adaptive Question Answering Datasets},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730324},
doi = {10.1145/3726302.3730324},
abstract = {As question answering (QA) systems advance alongside the rapid evolution of foundation models, the need for robust, adaptable, and large-scale evaluation benchmarks becomes increasingly critical. Traditional QA benchmarks are often static and publicly available, making them susceptible to data contamination and memorization by large language models (LLMs). Consequently, static benchmarks may overestimate model generalization and hinder a reliable assessment of real-world performance. In this work, we introduce Dynamic-KGQA, a scalable framework for generating adaptive QA datasets from knowledge graphs (KGs), designed to mitigate memorization risks while maintaining statistical consistency across iterations. Unlike fixed benchmarks, Dynamic-KGQA generates a new dataset variant on every run while preserving the underlying distribution, enabling fair and reproducible evaluations. Furthermore, our framework provides fine-grained control over dataset characteristics, supporting domain-specific and topic-focused QA dataset generation. Additionally, Dynamic-KGQA produces compact, semantically coherent subgraphs that facilitate both training and evaluation of KGQA models, enhancing their ability to leverage structured knowledge effectively. To align with existing evaluation protocols, we also provide static large-scale train/test/validation splits, ensuring comparability with prior methods. By introducing a dynamic, customizable benchmarking paradigm, Dynamic-KGQA enables a more rigorous and adaptable evaluation of QA systems.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3498–3508},
numpages = {11},
keywords = {benchmark, dynamic evaluation, kgqa, knowledge graphs, large language models, question answering},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3307363.3307387,
author = {Ding, Zheyuan and Yao, Li and Liu, Bin and Wu, Junfeng},
title = {Review of the Application of Ontology in the Field of Image Object Recognition},
year = {2019},
isbn = {9781450366199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307363.3307387},
doi = {10.1145/3307363.3307387},
abstract = {Image object recognition is an important research field in computer vision. It has a wide application prospect and practical significance in the information age. Although the current image recognition technology has achieved high accuracy in some tasks, the computer has many deficiencies in the automatic recognition of images such as fine-grained recognition, recognition of complex scenes. In these tasks, some issues exist like insufficient precision, complex high-level semantics which is difficult to identify and so on. This paper reviews the application of ontology in image object recognition. It is found that combining ontology knowledge model and traditional image recognition technology can improve recognition accuracy, enhance high-level semantic recognition ability, reduce the demand of the large number of training samples, and improve the scalability of the image recognition system. Otherwise, this paper also summarizes the frontier research of ontology applied in the field of image object recognition and the difficulties of deep integration of different technologies and ontology.},
booktitle = {Proceedings of the 11th International Conference on Computer Modeling and Simulation},
pages = {142–146},
numpages = {5},
keywords = {ontology, image object recognition},
location = {North Rockhampton, QLD, Australia},
series = {ICCMS '19}
}

@article{10.1145/3473973,
author = {Xie, Qianqian and Zhu, Yutao and Huang, Jimin and Du, Pan and Nie, Jian-Yun},
title = {Graph Neural Collaborative Topic Model for Citation Recommendation},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3473973},
doi = {10.1145/3473973},
abstract = {Due to the overload of published scientific articles, citation recommendation has long been a critical research problem for automatically recommending the most relevant citations of given articles. Relational topic models (RTMs) have shown promise on citation prediction via joint modeling of document contents and citations. However, existing RTMs can only capture pairwise or direct (first-order) citation relationships among documents. The indirect (high-order) citation links have been explored in graph neural network–based methods, but these methods suffer from the well-known explainability problem. In this article, we propose a model called Graph Neural Collaborative Topic Model that takes advantage of both relational topic models and graph neural networks to capture high-order citation relationships and to have higher explainability due to the latent topic semantic structure. Experiments on three real-world citation datasets show that our model outperforms several competitive baseline methods on citation recommendation. In addition, we show that our approach can learn better topics than the existing approaches. The recommendation results can be well explained by the underlying topics.},
journal = {ACM Trans. Inf. Syst.},
month = nov,
articleno = {48},
numpages = {30},
keywords = {collaborative filtering, relational topic models, graph neural networks, Explainable citation recommendation}
}

@inproceedings{10.1145/3463677.3463750,
author = {Panchal, Ronak and Swaminarayan, Priya and Tiwari, Sanju and Ortiz-Rodriguez, Fernando},
title = {AISHE-Onto: A Semantic Model for Public Higher Education Universities},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463750},
doi = {10.1145/3463677.3463750},
abstract = {The Electronic Government is a challenging field for the Semantic Web and the ontologies play a key role in the development of the Semantic Web. This paper explains the terms of the university through university ontology. We will focus on creating a university ontology. Here an ontology-based case study has been implemented for Public Higher Education (AISHE-Onto). SPARQL queries have been applied to make reasoning with the proposed ontology. As a result, a successful query interface has been provided to search academic data by the AISHE-Onto semantic portal.},
booktitle = {Proceedings of the 22nd Annual International Conference on Digital Government Research},
pages = {545–547},
numpages = {3},
keywords = {Egovernment, Ontologies, Public Administration, Semantic Web},
location = {Omaha, NE, USA},
series = {dg.o '21}
}

@article{10.1145/3412843,
author = {van Rozen, Riemer},
title = {Languages of Games and Play: A Systematic Mapping Study},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3412843},
doi = {10.1145/3412843},
abstract = {Digital games are a powerful means for creating enticing, beautiful, educational, and often highly addictive interactive experiences that impact the lives of billions of players worldwide. We explore what informs the design and construction of good games to learn how to speed-up game development. In particular, we study to what extent languages, notations, patterns, and tools, can offer experts theoretical foundations, systematic techniques, and practical solutions they need to raise their productivity and improve the quality of games and play. Despite the growing number of publications on this topic there is currently no overview describing the state-of-the-art that relates research areas, goals, and applications. As a result, efforts and successes are often one-off, lessons learned go overlooked, language reuse remains minimal, and opportunities for collaboration and synergy are lost. We present a systematic map that identifies relevant publications and gives an overview of research areas and publication venues. In addition, we categorize research perspectives along common objectives, techniques, and approaches, illustrated by summaries of selected languages. Finally, we distill challenges and opportunities for future research and development.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {123},
numpages = {37},
keywords = {tools, systematic map, patterns, notations, languages, game design, Game development}
}

@inproceedings{10.1145/3485447.3511949,
author = {Fan, Lu and Li, Qimai and Liu, Bo and Wu, Xiao-Ming and Zhang, Xiaotong and Lv, Fuyu and Lin, Guli and Li, Sen and Jin, Taiwei and Yang, Keping},
title = {Modeling User Behavior with Graph Convolution for Personalized Product Search},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511949},
doi = {10.1145/3485447.3511949},
abstract = {User preference modeling is a vital yet challenging problem in personalized product search. In recent years, latent space based methods have achieved state-of-the-art performance by jointly learning semantic representations of products, users, and text tokens. However, existing methods are limited in their ability to model user preferences. They typically represent users by the products they visited in a short span of time using attentive models and lack the ability to exploit relational information such as user-product interactions or item co-occurrence relations. In this work, we propose to address the limitations of prior arts by exploring local and global user behavior patterns on a user successive behavior graph, which is constructed by utilizing short-term actions of all users. To capture implicit user preference signals and collaborative patterns, we use an efficient jumping graph convolution to explore high-order relations to enrich product representations for user preference modeling. Our approach can be seamlessly integrated with existing latent space based methods and be potentially applied in any product retrieval method that uses purchase history to model user preferences. Extensive experiments on eight Amazon benchmarks demonstrate the effectiveness and potential of our approach. The source code is available at https://github.com/floatSDSDS/SBG .},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {203–212},
numpages = {10},
keywords = {Graph Convolution, Personalized Product Search, User Preference Modeling},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3377713.3377755,
author = {Sun, Xuan and Jiang, Longquan and Zhang, Minghuan and Wang, Cheng and Chen, Ying},
title = {Unsupervised Learning for Product Ontology from Textual Reviews on E-Commerce Sites},
year = {2020},
isbn = {9781450372619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377713.3377755},
doi = {10.1145/3377713.3377755},
abstract = {On modern e-commerce sites, textual reviews are rich in sentiment and opinions about a product, in particular, its specific attributes. Product ontologies, consisting of a taxonomic categorization of lists of such attributes and product types, are useful for a wide range of applications, such as opinion mining and sentiment analysis. Unfortunately, with a paucity of fine-grained hierarchical categorization of products, many smaller sites feature only coarse high-level categories. We present the Iterative Bootstrapping Process (IBP), an unsupervised learning method for such fine-grained hierarchical categorization of products using coarse categories from Chinese product textual reviews. Results show that our model can extract useful product attributes and still can achieve a high accuracy on the task for categorizing unseen products.},
booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
pages = {260–264},
numpages = {5},
keywords = {taxonomy, machine learning, cluster, Ontology},
location = {Sanya, China},
series = {ACAI '19}
}

@inproceedings{10.1145/3434780.3436609,
author = {Ferjaoui, Dhekra and Cheniti Belcadhi, Lilia},
title = {A Conceptual Model for Personalized Learning based on Educational Robots},
year = {2021},
isbn = {9781450388504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434780.3436609},
doi = {10.1145/3434780.3436609},
abstract = {Over a previous couple of years, Distance learning has successfully overcome the shortcomings of traditional methods of teaching and learning, likewise increases student interaction and diversities of opinion, online instructors could also be from any location across the world. So students have the chance to settle on a learning strategy most suited to their abilities, while education is streamlined to satisfy the requirements of the individual in question. Due to the on-going technological change, we are witnessing, Robots are getting an integral component of our society and have great potential in being utilized as an academic technology by providing students with a highly interactive and hands-on learning experience. Indeed, Robotics promises to inspire a replacement generation of learning. With the aim of understanding how students can use robots to review, we created and implemented a learning scenario through an ontological conceptual Model for Personalized Learning supported Educational Robots. This model enables us to supply inferences over learning data and supply personalized learning resources, adapted to the progress of the scholar within the learning process.},
booktitle = {Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {29–33},
numpages = {5},
keywords = {robots, personalized learning, ontological model, learning scenario, Distance learning},
location = {Salamanca, Spain},
series = {TEEM'20}
}

@article{10.1145/3657631,
author = {Biancofiore, Giovanni Maria and Deldjoo, Yashar and Noia, Tommaso Di and Di Sciascio, Eugenio and Narducci, Fedelucio},
title = {Interactive Question Answering Systems: Literature Review},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3657631},
doi = {10.1145/3657631},
abstract = {Question-answering systems are recognized as popular and frequently effective means of information seeking on the web. In such systems, information seekers can receive a concise response to their queries by presenting their questions in natural language. Interactive question answering is a recently proposed and increasingly popular solution that resides at the intersection of question answering and dialogue systems. On the one hand, the user can ask questions in normal language and locate the actual response to her inquiry; on the other hand, the system can prolong the question-answering session into a dialogue if there are multiple probable replies, very few, or ambiguities in the initial request. By permitting the user to ask more questions, interactive question answering enables users to interact with the system and receive more precise results dynamically.This survey offers a detailed overview of the interactive question-answering methods that are prevalent in current literature. It begins by explaining the foundational principles of question-answering systems, hence defining new notations and taxonomies to combine all identified works inside a unified framework. The reviewed published work on interactive question-answering systems is then presented and examined in terms of its proposed methodology, evaluation approaches, and dataset/application domain. We also describe trends surrounding specific tasks and issues raised by the community, so shedding light on the future interests of scholars. Our work is further supported by a GitHub page synthesizing all the major topics covered in this literature study.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {239},
numpages = {38},
keywords = {Question answering, natural language processing, interactive systems, human-computer interaction, artificial intelligence, large language model}
}

@inproceedings{10.1145/3555962.3555966,
author = {Cherian, Sherimon Puliprathu and Sherimon, Vinu and Agith, Align Elsa and Thomas, Anu Achankunju},
title = {Design and Development of Alzheimer Disease Ontology},
year = {2022},
isbn = {9781450396578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555962.3555966},
doi = {10.1145/3555962.3555966},
abstract = {Alzheimer is one of the common causes of dementia, which is affecting millions of people around the world. This research study proposes a framework for the knowledge representation of Alzheimer disease using Ontology. The case study is conducted in Sultanate of Oman. In Oman, it is suspected that around 5000 people are affected by this disease. The correct statistics is still unknown as public are not fully aware of the symptoms of this disease yet. So, the reported cases in the hospitals are less, even though it is believed that the actual number of patients is quite large. In this research study, Ontologies are used for knowledge representation of Alzheimer disease. Ontologies are best for semantically representing the knowledge. It is defined as a formal, explicit specification of a shared conceptualization. To develop efficient clinical decision support systems, Ontologies can be used to integrate the patient health record with data from different sources. Ontologies manage and specify the knowledge from the medical domain. Preliminary design of the ontology is given in this paper.},
booktitle = {Proceedings of the 2022 6th International Conference on Cloud and Big Data Computing},
pages = {19–23},
numpages = {5},
location = {Birmingham, United Kingdom},
series = {ICCBDC '22}
}

@inproceedings{10.1145/3529372.3530957,
author = {Koch, In\^{e}s},
title = {Integration of models for linked data in cultural heritage and contributions to the FAIR principles},
year = {2022},
isbn = {9781450393454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529372.3530957},
doi = {10.1145/3529372.3530957},
abstract = {Incorporating linked data-based models into the process of describing cultural objects is increasingly important for cultural heritage. Communities such as libraries, archives, and museums have developed and adopted models specific to their contexts. Without a trivial solution, choosing models to support more general applications is challenging. This Ph.D. aims to analyze existing solutions and practices in these domains and propose validated solutions for the discovery, access, interoperability, and reuse of cultural objects, following the FAIR principles. Transversal to the base models used, this research intends to adopt solutions that balance the simplicity of the models with the satisfaction of the requirements.},
booktitle = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
articleno = {51},
numpages = {2},
keywords = {semantic web, linked open data, data integration, cultural heritage, FAIR principles},
location = {Cologne, Germany},
series = {JCDL '22}
}

@proceedings{10.1145/3608251,
title = {ICCMS '23: Proceedings of the 2023 15th International Conference on Computer Modeling and Simulation},
year = {2023},
isbn = {9798400707919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Dalian, China}
}

@inproceedings{10.1145/3579051.3579060,
author = {Alrabbaa, Christian and Hieke, Willi},
title = {Explaining Non-Entailment by Model Transformation for the Description Logic EL},
year = {2023},
isbn = {9781450399876},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579051.3579060},
doi = {10.1145/3579051.3579060},
abstract = {Reasoning results computed by description logic systems can be hard to comprehend. When an ontology does not entail an expected subsumption relationship, generating an explanation of this non-entailment becomes necessary. In this paper, we use countermodels to explain non-entailments. More precisely, we devise relevant parts of canonical models of ontologies that serve as explanations and discuss the computational complexity of extracting these parts by means of model transformations. Furthermore, we provide an implementation of these transformations and evaluate it using real ontologies.},
booktitle = {Proceedings of the 11th International Joint Conference on Knowledge Graphs},
pages = {1–9},
numpages = {9},
keywords = {Model Transformation, Explainable AI, Description Logics},
location = {Hangzhou, China},
series = {IJCKG '22}
}

@proceedings{10.1145/3701551,
title = {WSDM '25: Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining},
year = {2025},
isbn = {9798400713293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 18th ACM International Conference on Web and Data Mining, this time taking place in Hannover, Germany.Many volunteers have helped to provide excellent content for the conference, most of all the 3 PC Chairs, Meeyoung Cha (Max Planck Institute for Security and Privacy in Germany), Francine Moens (KU Leuven, Belgium) and Marc Najork (Google DeepMind, USA), who selected more than 100 papers from over 600 submissions for presentation at the conference.This exciting WSDM 2025 main conference program was extended by interesting demonstrations like Lightning IR for fine-tuning and inference of transformer-based language models for information retrieval, WildlifeLookup, a chatbot designed to facilitate wildlife management and Ventana a la Verdad, a chatbot for navigating Colombian civil conflict archives. Several workshops ranging from language models to trust and verification on the Web complemented this program and provided important opportunities for the discussion of exciting new ideas and approaches.WSDM Cup 2025 challenged more than 1000 competitors to create useful and efficient auto-rater models that can accurately reflect human evaluations. This competition focused on the creation of multilingual auto-rater models using chatbot arena data, with exciting solutions. The WSDM 2025 Industry Day featured talks from leading companies on practical applications of web search and data mining. Some notable examples included zero-shot image moderation in Google Ads, advancing Voice AI for E-commerce at Amazon, and Fact-checking of multilingual podcasts. Finally, WSDM Day talks focused on medical research questions and topics, applying language models, knowledge graphs and other AI based approaches.For participants aiming to enter new research areas, the WSDM 2025 tutorials covered a broad range of topics, including Robust Information Retrieval, Building Trustworthy AI Models for Medicine, Unifying Bias and Unfairness in Information Retrieval in the LLM Era, Advances in Vector Search and quite a few more. And finally, three excellent invited speakers (Slav Petrov from Google DeepMind and co-lead on Gemini, Roberto Navigli from Sapienza University and Babelscape and Mario Fritz from CISPA) provided exciting insights into the state of AI, about what large language models really understand, as well as privacy and security aspects of neural models.},
location = {Hannover, Germany}
}

@inproceedings{10.1145/3407982.3407989,
author = {Ivanova, Tatyana and Popov, Miroslav},
title = {Ontology Evaluation and Multilingualism},
year = {2020},
isbn = {9781450377683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407982.3407989},
doi = {10.1145/3407982.3407989},
abstract = {Many ontologies have been developed recently, and evaluation is important for researchers or users when searching ontologies needed in their work. Ontology evaluation also is important phase of ontology development, refinement or evolution process. Automatically-developed ontologies as a result of ontology learning also need from evaluation. So, ontology evaluation is important both for selecting ontologies, meeting some requirements, and as a part of ontology life cycle. In this paper we propose deep analysis of ontology evaluation and discuss how multilingualism affects ontology evaluation. We classify ontology evaluation approaches, methods and metrics according to several dimensions. Our analysis and conclusions will be helpful for development of good ontologies and for finding or selection of needed ontologies.},
booktitle = {Proceedings of the 21st International Conference on Computer Systems and Technologies},
pages = {215–222},
numpages = {8},
keywords = {Ontology evaluation approaches, Ontology evaluation, Ontology assessment, Ontology, Multilingualism},
location = {Ruse, Bulgaria},
series = {CompSysTech '20}
}

@inproceedings{10.1145/3368691.3368697,
author = {U., Sreeja M. and Kovoor, Binsu C.},
title = {Object driven semantic multi-video summarisation based on ontology},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368697},
doi = {10.1145/3368691.3368697},
abstract = {Multi-Video summarisation frameworks aim to extract representative frames from a collection of videos. In the proposed framework, a multi-video summarisation model is implemented that detects key frames from a collection of related videos on the basis of user query object and ontology inference approach. The framework also develops a novel large-scale ontology for video genre identification based on the characteristics of genre-specific videos. The presence of ontologies aids in generating semantically relevant summaries compared to traditional approaches. Quantitative evaluation ensures that maximum information from the video collection on the basis of query is retrieved. Additionally, the ontology-based query inference approach reduces the computation time significantly. Qualitative results prove that the summary generated is concise, informative and semantically relevant.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {6},
numpages = {6},
keywords = {semantic, query, ontology, multi-video, genre-specific},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/3417990.3418742,
author = {Boubekeur, Younes and Mussbacher, Gunter},
title = {Towards a better understanding of interactions with a domain modeling assistant},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3418742},
doi = {10.1145/3417990.3418742},
abstract = {The enrolment of software engineering students has increased rapidly in the past few years following industry demand. At the same time, model-driven engineering (MDE) continues to become relevant to more domains like embedded systems and machine learning. It is therefore important to teach students MDE skills in an effective manner to prepare them for future careers in academia and industry. The use of interactive online tools can help instructors deliver course material to more students in a more efficient manner, allowing them to offload repetitive or tedious tasks to these systems and focus on other teaching activities that cannot be easily automated. Interactive online tools can provide students with a more engaging learning experience than static resources like books or written exercises. Domain modeling with class diagrams is a fundamental modeling activity in MDE. While there exist multiple modeling tools that allow students to build a domain model, none of them offer an interactive learning experience. In this paper, we explore the interactions between a student modeler and an interactive domain modeling assistant with the aim of better understanding the required interaction. We illustrate desired interactions with three examples and then formalize them in a metamodel. Based on the metamodel, we explain how to form a corpus of learning material that supports the assistant interactions.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {21},
numpages = {10},
keywords = {learning corpus, feedback, domain model, class diagram, chatbot},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@proceedings{10.1145/3551902,
title = {EuroPLop '22: Proceedings of the 27th European Conference on Pattern Languages of Programs},
year = {2022},
isbn = {9781450395946},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Irsee, Germany}
}

@inproceedings{10.1145/3573128.3609346,
author = {Sotudeh, Sajad and Goharian, Nazli},
title = {OntG-Bart: Ontology-Infused Clinical Abstractive Summarization},
year = {2023},
isbn = {9798400700279},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573128.3609346},
doi = {10.1145/3573128.3609346},
abstract = {Automating the process of clinical text summarization could save clinicians' reading time and reduce their fatigue, acknowledging the necessity of human professionals in the loop. This paper addresses clinical text summarization, aiming to incorporate ontology concept relationships via a Graph Neural Network (GNN) into the summarization process. Specifically, we propose a model, extending Bart's encoder-decoder framework with GNN encoder and multi-head attentional layers for decoder, producing ontology-aware summaries. This GNN interacts with the textual encoder, influencing their mutual representations. The model's effectiveness is validated on two real-world radiology datasets. We also present an ablation study to elucidate the impact of varied graph configurations and an error analysis aimed at pinpointing potential areas for future improvements.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2023},
articleno = {17},
numpages = {4},
keywords = {abstractive summarization, clinical text summarization, neural networks, text summarization},
location = {Limerick, Ireland},
series = {DocEng '23}
}

@inproceedings{10.1145/3401832.3402681,
author = {Lembo, Domenico and Li, Yunyao and Popa, Lucian and Scafoglieri, Federico Maria},
title = {Ontology mediated information extraction in financial domain with Mastro System-T},
year = {2020},
isbn = {9781450380300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401832.3402681},
doi = {10.1145/3401832.3402681},
abstract = {Information extraction (IE) refers to the task of turning text documents into a structured form, in order to make the information contained therein automatically processable. Ontology Mediated Information Extraction (OMIE) is a new paradigm for IE that seeks to exploit the semantic knowledge expressed in ontologies to improve query answering over unstructured data (properly raw text). In this paper we present Mastro System-T, an OMIE tool born from a joint collaboration between the University of Rome "La Sapienza" and IBM Research Almaden and its first application in a financial domain, namely to facilitate the access to and the sharing of data extracted from the EDGAR system.},
booktitle = {Proceedings of the Sixth International Workshop on Data Science for Macro-Modeling},
articleno = {3},
numpages = {6},
keywords = {ontology mediated information extraction, ontology based data access, ontology, information extraction, financial domain},
location = {Portland, Oregon},
series = {DSMM '20}
}

@inproceedings{10.1145/3318236.3318257,
author = {Shah, Unnati and Patel, Sankita and Jinwala, Devesh},
title = {An Ontological Approach to Specify Conflicts among Non-Functional Requirements},
year = {2019},
isbn = {9781450362450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318236.3318257},
doi = {10.1145/3318236.3318257},
abstract = {It is a usual practice for a user to narrate the Non-Functional Requirements (NFRs) in natural language and the requirements engineers manually try to express the same, using semi-formal or formal language notations. However, inaccurate and the laborious manual approach may fail to detect all potential NFRs and conflicts among them. Existing solutions for specifying NFRs are based on a graphical representation that requires manual efforts. Furthermore, they do not take into account the classification of the types of conflicting NFRs that helps to prioritize NFRs. In addition, these approaches are not used in industrial practice due to three main reasons viz. 1) High manual inference 2) Sharing and reusing work can be difficult and 3) No support for machine understanding. Therefore, the aim of our research is to formally specify conflicting NFRs from available natural language NFRs by means of ontological representation that helps requirements analysts prioritize the NFRs at an early stage of requirements engineering.},
booktitle = {Proceedings of the 2019 2nd International Conference on Geoinformatics and Data Analysis},
pages = {145–149},
numpages = {5},
keywords = {Specification, Requirements Engineering, Ontology, Non-functional Requirements, Conflict},
location = {Prague, Czech Republic},
series = {ICGDA '19}
}

@inproceedings{10.1145/3194104.3194108,
author = {Blersch, Martin and Landh\"{a}u\ss{}er, Mathias and Mayer, Thomas},
title = {Semi-automatic generation of active ontologies from web forms for intelligent assistants},
year = {2018},
isbn = {9781450357234},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194104.3194108},
doi = {10.1145/3194104.3194108},
abstract = {Intelligent assistants are becoming widespread. A popular method for creating intelligent assistants is modeling the domain (and thus the assistant's capabilities) as Active Ontology. Adding new functionality requires extending the ontology or building new ones; as of today, this process is manual.We describe an automated method for creating Active Ontologies for arbitrary web forms. Our approach leverages methods from natural language processing and data mining to synthesize the ontologies. Furthermore, our tool generates the code needed to process user input.We evaluate the generated Active Ontologies in three case studies using web forms from the UIUC Web Integration Repository, namely from the domains airfare, automobile, and book search. First, we examine how much of the generation process can be automated and how well the approach identifies domain concepts and their relations. Second, we test how well the generated Active Ontologies handle end-user input to perform the desired actions. In our evaluation, Easier automatically generates 65\% of the Active Ontologies' sensor nodes; the generated ontology for airfare search correctly answers 70\% of the queries.},
booktitle = {Proceedings of the 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {28–34},
numpages = {7},
location = {Gothenburg, Sweden},
series = {RAISE '18}
}

@inproceedings{10.1145/3447568.3448533,
author = {Alnahdi, Amany},
title = {Mobile Application Development Ontology},
year = {2021},
isbn = {9781450376556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447568.3448533},
doi = {10.1145/3447568.3448533},
abstract = {Ontologies are structural knowledge components constructed to clarify concepts in a specific domain of knowledge. They are formed to represent the concepts and the relationships between these concepts in that domain. This paper proposes a Mobile Application Development Ontology that conceptualizes the knowledge in the domain of mobile application development. The use of mobile applications is flourishing as mobile devices use is pervasive. The proposed ontology aims to provide a terminology glossary reference for stakeholders in the domain of mobile application development; students, researchers, educators, mobile application analysts, and mobile application developers. The constructed ontology was visualized using a D3 library visualization tool. Applications used for the built ontology include educational purposes and glossaries for classifying concepts in the domain of mobile application development.},
booktitle = {Proceedings of the 10th International Conference on Information Systems and Technologies},
articleno = {25},
numpages = {6},
keywords = {Semantic Web, Pedagogy, Ontology, Mobile Applications},
location = {Lecce, Italy},
series = {ICIST '20}
}

@inproceedings{10.1145/3650105.3652291,
author = {Khakzad Shahandashti, Kimya and Sivakumar, Mithila and Mohajer, Mohammad Mahdi and Boaye Belle, Alvine and Wang, Song and Lethbridge, Timothy},
title = {Assessing the Impact of GPT-4 Turbo in Generating Defeaters for Assurance Cases},
year = {2024},
isbn = {9798400706097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3650105.3652291},
doi = {10.1145/3650105.3652291},
abstract = {Assurance cases (ACs) are structured arguments that allow verifying the correct implementation of the created systems' non-functional requirements (e.g., safety, security). This allows for preventing system failure. The latter may result in catastrophic outcomes (e.g., loss of lives). ACs support the certification of systems in compliance with industrial standards, e.g., DO-178C and ISO 26262. Identifying defeaters ---arguments that challenge these ACs --- is crucial for enhancing ACs' robustness and confidence. To automatically support that task, we propose a novel approach that explores the potential of GPT-4 Turbo, an advanced Large Language Model (LLM) developed by OpenAI, in identifying defeaters within ACs formalized using the Eliminative Argumentation (EA) notation. Our preliminary evaluation assesses the model's ability to comprehend and generate arguments in this context and the results show that GPT-4 turbo is very proficient in EA notation and can generate different types of defeaters.},
booktitle = {Proceedings of the 2024 IEEE/ACM First International Conference on AI Foundation Models and Software Engineering},
pages = {52–56},
numpages = {5},
keywords = {large language models, assurance cases, assurance defeaters, system certification, FM for Requirement Engineering},
location = {Lisbon, Portugal},
series = {FORGE '24}
}

@article{10.1145/3736164,
author = {Albilali, Eman and Al-Twairesh, Nora and Hosny, Manar},
title = {AKER: Arabic Knowledge-enriched Reader for Machine Reading Comprehension},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3736164},
doi = {10.1145/3736164},
abstract = {Machine reading comprehension aims at understanding a passage and answer a given question by selecting a span from the passage. Recently, pre-trained language models achieved state-of-the-art results on Arabic machine reading comprehension, yet a broad body of works suggests that BERT-based variant models fail to encode and associate common sense facts and world knowledge. To alleviate this weakness, we propose an Arabic knowledge enriched reader model, which fuses external knowledge into contextual representation using an attention and gating mechanism. We learn and generate Arabic knowledge graph embeddings that represent information from Arabic Wikidata and utilize this representation when fusing knowledge. We adopted a knowledge graph embedding scoring function to select the most relevant concepts to the context from the knowledge graph. We evaluated our approach on multiple Arabic machine reading comprehension datasets. Despite leveraging a comparatively smaller pre-trained language model, our approach significantly outperforms large language models in Arabic machine reading comprehension across multiple benchmark datasets, achieving substantial gains in both EM and F1 scores.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {62},
numpages = {29},
keywords = {Machine reading comprehension, question answering, large language model, knowledge base}
}

@article{10.1109/TASLP.2023.3268730,
author = {Yang, Dongchao and Yu, Jianwei and Wang, Helin and Wang, Wen and Weng, Chao and Zou, Yuexian and Yu, Dong},
title = {Diffsound: Discrete Diffusion Model for Text-to-Sound Generation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3268730},
doi = {10.1109/TASLP.2023.3268730},
abstract = {Generating sound effects that people want is an important topic. However, there are limited studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a token-decoder, and a vocoder. The framework first uses the token-decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the token-decoder significantly influences the generation performance. Thus, we focus on designing a good token-decoder in this study. We begin with the traditional autoregressive (AR) token-decoder. However, the AR token-decoder always predicts the mel-spectrogram tokens one by one in order, which may introduce the unidirectional bias and accumulation of errors problems. Moreover, with the AR token-decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR token-decoders, we propose a non-autoregressive token-decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound model predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained by iteration. Our experiments show that our proposed Diffsound model not only produces better generation results when compared with the AR token-decoder but also has a faster generation speed, &lt;italic&gt;i.e.&lt;/italic&gt;, MOS: 3.56 &lt;italic&gt;v.s&lt;/italic&gt; 2.786.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = apr,
pages = {1720–1733},
numpages = {14}
}

@inproceedings{10.1145/3550355.3552426,
author = {Lathouwers, Sophie and Zaytsev, Vadim},
title = {Modelling program verification tools for software engineers},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552426},
doi = {10.1145/3550355.3552426},
abstract = {In software engineering, models are used for many different things. In this paper, we focus on program verification, where we use models to reason about the correctness of systems. There are many different types of program verification techniques which provide different correctness guarantees. We investigate the domain of program verification tools, and present a concise megamodel to distinguish these tools. We also present a data set of almost 400 program verification tools. This data set includes the category of verification tool according to our megamodel, practical information such as input/output format, repository links, and more. The categorisation enables software engineers to find suitable tools, investigate similar alternatives and compare them. We also identify trends for each level in our megamodel based on the categorisation. Our data set, publicly available at https://doi.org/10.4121/20347950, can be used by software engineers to enter the world of program verification and find a verification tool based on their requirements.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {98–108},
numpages = {11},
keywords = {program verification, megamodelling, formal methods},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@article{10.1145/3485466,
author = {Ronzino, Paola and Toth, Anna and Falcidieno, Bianca},
title = {Documenting the Structure and Adaptive Reuse of Roman Amphitheatres through the CIDOC CRMba Model},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3485466},
doi = {10.1145/3485466},
abstract = {This article addresses an important aspect of the built heritage documentation, which concerns encoding information about a building in a formal way, making it available for reuse by the research community. Formal ontologies allow structuring and integrating information from heterogeneous sources without loss of semantic information. In the field of Cultural Heritage (CH), the CIDOC Conceptual Reference Model (CRM) ontology is well known and widely accepted as it provides definitions and a formal structure to describe the implicit and explicit concepts and relationships used in the CH documentation. One of its extensions, the CRMba model, has been specifically designed to document information on a built structure and its components. In this work, we have applied the CRMba model to the documentation of Roman architectures, in particular, Roman amphitheatres, demonstrating how the semantic model allows encoding information about the structure of the building and its evolution over time and space, stressing on the concepts of “empty spaces” and “functional spaces” defined by form, and focusing on the relationship between form and function. The aim of the work is to explore the potentiality of the model and to provide, through a series of examples supported by graphs, standard encoding procedures to be reused by scholars dealing with similar case studies.},
journal = {J. Comput. Cult. Herit.},
month = apr,
articleno = {36},
numpages = {23},
keywords = {CIDOC CRM, amphitheatre, Buildings archaeology}
}

@inproceedings{10.1145/3447568.3448526,
author = {Hadj-Mbarek, Asma and Maalel, Ahmed},
title = {Towards a Collection Data Approach to Crisis Situation Based on Ontologies},
year = {2021},
isbn = {9781450376556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447568.3448526},
doi = {10.1145/3447568.3448526},
abstract = {Disasters bring unforeseen situations to the public, requiring a quick and immediate response from official organizations. During these periods, access to rapidly changing information plays an important role in decision-making. However, the transmissions of information hinder this task and ultimately delay response operations. Social networks such as Twitter and Facebook create new opportunities to address this type of real-world problems. The involved actors use these channels at the time of crisis to share various information, whether they are requesting assistance, or describing an event. In this article, we will present the first steps of validation of a collect data approach from Twitter. This approach is based on a domain ontology.},
booktitle = {Proceedings of the 10th International Conference on Information Systems and Technologies},
articleno = {18},
numpages = {6},
keywords = {Twitter, Ontology, Data collection, Crisis situation},
location = {Lecce, Italy},
series = {ICIST '20}
}

@article{10.1145/3453172,
author = {Gil, Yolanda and Garijo, Daniel and Khider, Deborah and Knoblock, Craig A. and Ratnakar, Varun and Osorio, Maximiliano and Vargas, Hern\'{a}n and Pham, Minh and Pujara, Jay and Shbita, Basel and Vu, Binh and Chiang, Yao-Yi and Feldman, Dan and Lin, Yijun and Song, Hayley and Kumar, Vipin and Khandelwal, Ankush and Steinbach, Michael and Tayal, Kshitij and Xu, Shaoming and Pierce, Suzanne A. and Pearson, Lissa and Hardesty-Lewis, Daniel and Deelman, Ewa and Silva, Rafael Ferreira Da and Mayani, Rajiv and Kemanian, Armen R. and Shi, Yuning and Leonard, Lorne and Peckham, Scott and Stoica, Maria and Cobourn, Kelly and Zhang, Zeya and Duffy, Christopher and Shu, Lele},
title = {Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3453172},
doi = {10.1145/3453172},
abstract = {Major societal and environmental challenges involve complex systems that have diverse multi-scale interacting processes. Consider, for example, how droughts and water reserves affect crop production and how agriculture and industrial needs affect water quality and availability. Preventive measures, such as delaying planting dates and adopting new agricultural practices in response to changing weather patterns, can reduce the damage caused by natural processes. Understanding how these natural and human processes affect one another allows forecasting the effects of undesirable situations and study interventions to take preventive measures. For many of these processes, there are expert models that incorporate state-of-the-art theories and knowledge to quantify a system's response to a diversity of conditions. A major challenge for efficient modeling is the diversity of modeling approaches across disciplines and the wide variety of data sources available only in formats that require complex conversions. Using expert models for particular problems requires integration of models with third-party data as well as integration of models across disciplines. Modelers face significant heterogeneity that requires resolving semantic, spatiotemporal, and execution mismatches, which are largely done by hand today and may take more than 2 years of effort.We are developing a modeling framework that uses artificial intelligence (AI) techniques to reduce modeling effort while ensuring utility for decision making. Our work to date makes several innovative contributions: (1) an intelligent user interface that guides analysts to frame their modeling problem and assists them by suggesting relevant choices and automating steps along the way; (2) semantic metadata for models, including their modeling variables and constraints, that ensures model relevance and proper use for a given decision-making problem; and (3) semantic representations of datasets in terms of modeling variables that enable automated data selection and data transformations. This framework is implemented in the MINT (Model INTegration) framework, and currently includes data and models to analyze the interactions between natural and human systems involving climate, water availability, agricultural production, and markets. Our work to date demonstrates the utility of AI techniques to accelerate modeling to support decision-making and uncovers several challenging directions for future work.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jul,
articleno = {11},
numpages = {49},
keywords = {remote sensing data, regional-level decision-making, model metadata, integrated modeling, Intelligent user interfaces}
}

@inproceedings{10.1145/3417990.3421410,
author = {Jeusfeld, Manfred A. and Almeida, Jo\~{a}o Paulo A. and Carvalho, Victorio A. and Fonseca, Claudenir M. and Neumayr, Bernd},
title = {Deductive reconstruction of MLT* for multi-level modeling},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421410},
doi = {10.1145/3417990.3421410},
abstract = {In the last two decades, about a dozen proposals were made to extend object-oriented modeling by multiple abstraction levels. One group of proposals designates explicit levels to objects and classes. The second group uses the powertype pattern to implicitly establish levels. From this group, we consider two proposals, DeepTelos and MLT*. Both have been defined via axioms and both give a central role to the powertype pattern. In this paper, we reconstruct MLT* with the deductive axiomatization style used for DeepTelos. The resulting specification is executed in a deductive database to check MLT* multi-level models for errors and complete them with derived facts that do not have to be explicitly asserted by modelers. This leverages the rich rules of MLT* with the deductive approach underlying DeepTelos. The effort also allows us to clearly establish the relation between DeepTelos and MLT*, in an attempt to clarify the relations between approaches in this research domain. As a byproduct, we supply MLT-Telos as a fully operational deductive implementation of MLT* to the research community.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {83},
numpages = {10},
keywords = {powertype, object-oriented modeling, multi-level modeling, deeptelos, datalog, conceptbase, MLT*},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@article{10.1145/3747290,
author = {Dahou, Abdelghani and Dahou, Abdelhalim Hafedh and Cheragui, Mohamed Amine and Abdedaiem, Amin and Al-Qaness, Mohammed A. A. and Abd Elaziz, Mohamed and Ewees, Ahmed A. and Zheng, Zhonglong},
title = {A Survey on Dialect Arabic Processing and Analysis: Recent Advances and Future Trends},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3747290},
doi = {10.1145/3747290},
abstract = {Advances in language models have enabled significant strides in developing language technologies tailored for analyzing and processing Dialectical Arabic (DA), which exhibits unique linguistic features and variations compared to standard Arabic. This progress has sparked a surge of interest in various research tasks within the Arabic Natural Language Processing (ANLP) domain, encompassing areas such as sentiment analysis, dialect identification, normalization and classification, fake news detection, and part-of-speech tagging. The primary objective of this survey paper is to provide a comprehensive overview of the advancements made in dialectical ANLP from 2014 to 2024. A thorough analysis is undertaken, covering a corpus of approximately 200 research papers, to offer insights into the latest developments, resources, and applications concerning dialectical Arabic. By identifying and discussing the challenges and opportunities for future research, this study aspires to serve as a valuable reference for researchers, practitioners, and enthusiasts interested in the subject matter. Central to the investigation are the recent strides in natural language processing techniques that pertain to dialectical Arabic, namely DA sentiment analysis, DA identification, DA classification, DA normalization, DA part-of-speech tagging, and the role of DA in fake news detection, among other applications. Each research category is meticulously examined, providing a comprehensive understanding of their respective contributions, significance, encountered challenges, and the availability of pertinent datasets. This exhaustive survey paper encompasses existing studies within dialectical Arabic research categories. As a result, readers are presented with a detailed reference source in pursuing advancements and innovations within this field.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {84},
numpages = {45},
keywords = {Deep learning, arabic dialect, sentiment analysis, classification, POS tag, datasets, fake news}
}

@article{10.1145/3569458,
author = {Angermeier, Daniel and Wester, Hannah and Beilke, Kristian and Hansch, Gerhard and Eichler, J\"{o}rn},
title = {Security Risk Assessments: Modeling and Risk Level Propagation},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2378-962X},
url = {https://doi.org/10.1145/3569458},
doi = {10.1145/3569458},
abstract = {Security risk assessment is an important task in systems engineering. It is used to derive security requirements for a secure system design and to evaluate design alternatives as well as vulnerabilities. Security risk assessment is also a complex and interdisciplinary task, where experts from the application domain and the security domain have to collaborate and understand each other. Automated and tool-supported approaches are desired to help manage the complexity. However, the models used for system engineering usually focus on functional behavior and lack security-related aspects. Therefore, we present our modeling approach that alleviates communication between the involved experts and features steps of computer-aided modeling to achieve consistency and avoid omission errors. We demonstrate our approach with an example. We also describe how to model impact rating and attack feasibility estimation in a modular fashion, along with the propagation and aggregation of these estimations through the model. As a result, experts can make local decisions or changes in the model, which in turn provides the impact of these decisions or changes on the overall risk profile. Finally, we discuss the advantages of our model-based method.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = feb,
articleno = {8},
numpages = {25},
keywords = {threat modeling, secure design, model-based, security engineering, risk analysis, Security risk assessment}
}

@article{10.1145/3610896,
author = {Boovaraghavan, Sudershan and Patidar, Prasoon and Agarwal, Yuvraj},
title = {TAO: Context Detection from Daily Activity Patterns Using Temporal Analysis and Ontology},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
url = {https://doi.org/10.1145/3610896},
doi = {10.1145/3610896},
abstract = {Translating fine-grained activity detection (e.g., phone ring, talking interspersed with silence and walking) into semantically meaningful and richer contextual information (e.g., on a phone call for 20 minutes while exercising) is essential towards enabling a range of healthcare and human-computer interaction applications. Prior work has proposed building ontologies or temporal analysis of activity patterns with limited success in capturing complex real-world context patterns. We present TAO, a hybrid system that leverages OWL-based ontologies and temporal clustering approaches to detect high-level contexts from human activities. TAO can characterize sequential activities that happen one after the other and activities that are interleaved or occur in parallel to detect a richer set of contexts more accurately than prior work. We evaluate TAO on real-world activity datasets (Casas and Extrasensory) and show that our system achieves, on average, 87\% and 80\% accuracy for context detection, respectively. We deploy and evaluate TAO in a real-world setting with eight participants using our system for three hours each, demonstrating TAO's ability to capture semantically meaningful contexts in the real world. Finally, to showcase the usefulness of contexts, we prototype wellness applications that assess productivity and stress and show that the wellness metrics calculated using contexts provided by TAO are much closer to the ground truth (on average within 1.1\%), as compared to the baseline approach (on average within 30\%).},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = sep,
articleno = {87},
numpages = {32},
keywords = {Behavioral context recognition, activity recognition, deep Learning, ontology}
}

@inproceedings{10.1145/3302425.3302495,
author = {Liu, Xiao and Gao, Feng},
title = {An Approach for Learning Ontology from Relational Database},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302495},
doi = {10.1145/3302425.3302495},
abstract = {As a conceptual model and modeling tool, ontology can describe knowledge systems at the semantic level, it can also effectively solve the problem of information integration and sharing. In the research of information integration based on ontology, how to transform relational database data into ontology is an important research direction. Since relational databases are widely used to store data, this paper proposes a new method(WN_Graph) for learning ontology from relational database. Compared with the existing methods, WN_Graph combines intermediate conceptual graph model with WordNet to get more hierarchical relationships of concepts. Therefore, more rich semantic relationships of relational databases are extracted. We use data set from the medical field to verify and analyze the proposed method.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {58},
numpages = {6},
keywords = {WordNet, Relational Database, Ontology, Graph Model},
location = {Sanya, China},
series = {ACAI '18}
}

@article{10.1109/TASLP.2022.3153255,
author = {Gao, Silin and Takanobu, Ryuichi and Bosselut, Antoine and Huang, Minlie},
title = {End-to-End Task-Oriented Dialog Modeling With Semi-Structured Knowledge Management},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3153255},
doi = {10.1109/TASLP.2022.3153255},
abstract = {Current task-oriented dialog (TOD) systems mostly manage structured knowledge (e.g. databases and tables) to guide the goal-oriented conversations. However, they fall short of handling dialogs which also involve unstructured knowledge (e.g. reviews and documents). In this article, we formulate a task of modeling TOD grounded on a fusion of structured and unstructured knowledge. To address this task, we propose a TOD system with semi-structured knowledge management, SeKnow, which extends the belief state to manage knowledge with both structured and unstructured contents. Furthermore, we introduce two implementations of SeKnow based on a non-pretrained sequence-to-sequence model and a pretrained language model, respectively. Both implementations use the end-to-end manner to jointly optimize dialog modeling grounded on structured and unstructured knowledge. We conduct experiments on a modified version of MultiWOZ 2.1 dataset, Mod-MultiWOZ 2.1, where dialogs are processed to involve semi-structured knowledge. Experimental results show that SeKnow has strong performances in both end-to-end dialog and intermediate knowledge management, compared to existing TOD systems and their extensions with pipeline knowledge management schemes.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = feb,
pages = {2173–2187},
numpages = {15}
}

@inproceedings{10.1145/3307339.3342148,
author = {Lu, Qiuhao and de Silva, Nisansa and Kafle, Sabin and Cao, Jiazhen and Dou, Dejing and Nguyen, Thien Huu and Sen, Prithviraj and Hailpern, Brent and Reinwald, Berthold and Li, Yunyao},
title = {Learning Electronic Health Records through Hyperbolic Embedding of Medical Ontologies},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3342148},
doi = {10.1145/3307339.3342148},
abstract = {Unplanned intensive care units (ICU) readmissions and in-hospital mortality of patients are two important metrics for evaluating the quality of hospital care. Identifying patients with higher risk of readmission to ICU or of mortality can not only protect those patients from potential dangers, but also reduce the high costs of healthcare. In this work, we propose a new method to incorporate information from the Electronic Health Records (EHRs) of patients and utilize hyperbolic embeddings of a medical ontology (i.e., ICD-9) in the prediction model. The results prove the effectiveness of our method and show that hyperbolic embeddings of ontological concepts give promising performance.},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {338–346},
numpages = {9},
keywords = {readmission prediction, mortality prediction, medical ontology, graph embedding},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1145/3297280.3297631,
author = {Fathalla, Said and Vahdati, Sahar and Auer, S\"{o}ren and Lange, Christoph},
title = {The scientific events ontology of the OpenResearch.org curation platform},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297631},
doi = {10.1145/3297280.3297631},
abstract = {Scholarly events, such as conferences play a key role in scholarly communication from many research fields, such as computer science. We describe a systematic redesign of the OpenResearch Scientific Events Ontology (OR-SEO) that is used as a schema for the event pages on OpenResearch.org curation platform. OR-SEO is now in use in thousands of event pages on OpenResearch, which enables users to create events wiki-pages without going into the details of the implementation of the ontology. We syntactically and semantically validated OR-SEO to conform to the W3C standards. It has been published through a persistent URL following W3C best practices for publishing Linked data and has been registered at Linked Open Vocabularies.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2311–2313},
numpages = {3},
keywords = {knowledge engineering, linked data, scholarly communication, scientific events modeling, semantic MediaWiki},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1145/3471907,
author = {Lano, K. and Kolahdouz-Rahimi, S. and Fang, S.},
title = {Model Transformation Development Using Automated Requirements Analysis, Metamodel Matching, and Transformation by Example},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3471907},
doi = {10.1145/3471907},
abstract = {In this article, we address how the production of model transformations (MT) can be accelerated by automation of transformation synthesis from requirements, examples, and metamodels. We introduce a synthesis process based on metamodel matching, correspondence patterns between metamodels, and completeness and consistency analysis of matches. We describe how the limitations of metamodel matching can be addressed by combining matching with automated requirements analysis and model transformation by example (MTBE) techniques.We show that in practical examples a large percentage of required transformation functionality can usually be constructed automatically, thus potentially reducing development effort. We also evaluate the efficiency of synthesised transformations.Our novel contributions are: The concept of correspondence patterns between metamodels of a transformation.Requirements analysis of transformations using natural language processing (NLP) and machine learning (ML).Symbolic MTBE using “predictive specification” to infer transformations from examples.Transformation generation in multiple MT languages and in Java, from an abstract intermediate language.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {18},
numpages = {71},
keywords = {requirements engineering, automated software engineering, model-driven engineering, Model transformations}
}

@inproceedings{10.1145/3643664.3648211,
author = {Frattini, Julian and Fucci, Davide and Torkar, Richard and Mendez, Daniel},
title = {A Second Look at the Impact of Passive Voice Requirements on Domain Modeling: Bayesian Reanalysis of an Experiment},
year = {2024},
isbn = {9798400705670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643664.3648211},
doi = {10.1145/3643664.3648211},
abstract = {The quality of requirements specifications may impact subsequent, dependent software engineering (SE) activities. However, empirical evidence of this impact remains scarce and too often superficial as studies abstract from the phenomena under investigation too much. Two of these abstractions are caused by the lack of frameworks for causal inference and frequentist methods which reduce complex data to binary results. In this study, we aim to demonstrate (1) the use of a causal framework and (2) contrast frequentist methods with more sophisticated Bayesian statistics for causal inference. To this end, we reanalyze the only known controlled experiment investigating the impact of passive voice on the subsequent activity of domain modeling. We follow a framework for statistical causal inference and employ Bayesian data analysis methods to re-investigate the hypotheses of the original study. Our results reveal that the effects observed by the original authors turned out to be much less significant than previously assumed. This study supports the recent call to action in SE research to adopt Bayesian data analysis, including causal frameworks and Bayesian statistics, for more sophisticated causal inference.},
booktitle = {Proceedings of the 1st IEEE/ACM International Workshop on Methodological Issues with Empirical Studies in Software Engineering},
pages = {27–33},
numpages = {7},
keywords = {requirements engineering, requirements quality, controlled experiment, bayesian data analysis},
location = {Lisbon, Portugal},
series = {WSESE '24}
}

@article{10.1109/TASLP.2022.3224285,
author = {Wang, Zhong-Qiu and Wichern, Gordon and Watanabe, Shinji and Le Roux, Jonathan},
title = {STFT-Domain Neural Speech Enhancement With Very Low Algorithmic Latency},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3224285},
doi = {10.1109/TASLP.2022.3224285},
abstract = {Deep learning based speech enhancement in the short-time Fourier transform (STFT) domain typically uses a large window length such as 32 ms. A larger window can lead to higher frequency resolution and potentially better enhancement. This however incurs an algorithmic latency of 32 ms in an online setup, because the overlap-add algorithm used in the inverse STFT (iSTFT) is also performed using the same window size. To reduce this inherent latency, we adapt a conventional dual-window-size approach, where a regular input window size is used for STFT but a shorter output window is used for overlap-add, for STFT-domain deep learning based frame-online speech enhancement. Based on this STFT-iSTFT configuration, we employ complex spectral mapping for frame-online enhancement, where a deep neural network (DNN) is trained to predict the real and imaginary (RI) components of target speech from the mixture RI components. In addition, we use the DNN-predicted RI components to conduct frame-online beamforming, the results of which are used as extra features for a second DNN to perform frame-online postfiltering. The frequency-domain beamformer can be easily integrated with our DNNs and is designed to not incur any algorithmic latency. Additionally, we propose a future-frame prediction technique to further reduce the algorithmic latency. Evaluation on noisy-reverberant speech enhancement shows the effectiveness of the proposed algorithms. Compared with Conv-TasNet, our STFT-domain system can achieve better enhancement performance for a comparable amount of computation, or comparable performance with less computation, maintaining strong performance at an algorithmic latency as low as 2 ms.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {397–410},
numpages = {14}
}

@inproceedings{10.1145/3167132.3167298,
author = {Belmonte, Javier and Dugerdil, Philippe},
title = {Program understanding using ontologies and dynamic analysis},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167298},
doi = {10.1145/3167132.3167298},
abstract = {No maintenance activity can be performed without understanding at least the part of the program that needs to be modified. Therefore, considering its cost, helping developers to understand programs is a must. Consequently, our research aims at building a business-related model of the program semantics, which is grounded in Perkinsfi research in psychology. After a short reminder of our model, whose performance in helping developers to understand programs has been presented elsewhere, this paper presents the automatic instantiation of the model. This rests on the ontology technology as well as on an innovative dynamic analysis technique. We present a use case to evaluate the performance of our technique.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1552–1559},
numpages = {8},
keywords = {reverse engineering, program understanding, ontology, dynamic analysis},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3550356.3561539,
author = {Acosta, Maribel and Hahner, Sebastian and Koziolek, Anne and K\"{u}hn, Thomas and Mirandola, Raffaela and Reussner, Ralf},
title = {Uncertainty in coupled models of cyber-physical systems},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561539},
doi = {10.1145/3550356.3561539},
abstract = {The development of cyber-physical systems typically involves the association between multiple coupled models that capture different aspects of the system and the environment where it operates. Due to the dynamic aspect of the environment, unexpected conditions and uncertainty may impact the system. In this work, we tackle this problem and propose a taxonomy for characterizing uncertainty in coupled models. Our taxonomy extends existing proposals to cope with the particularities of coupled models in cyber-physical systems. In addition, our taxonomy discusses the notion of uncertainty propagation to other parts of the system. This allows for studying and (in some cases) quantifying the effects of uncertainty on other models in a system even at design time. We show the applicability of our uncertainty taxonomy in real use cases motivated by our envisioned scenario of automotive development.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {569–578},
numpages = {10},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3701716.3716841,
author = {Kejriwal, Mayank and McGuinness, Deborah L. and Lieberman, Henry},
title = {Commonsense AI in the History of the Web},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3716841},
doi = {10.1145/3701716.3716841},
abstract = {Machine common sense (MCS)-the challenge of enabling computers to grasp everyday human knowledge-has been a grand challenge in Artificial Intelligence (AI) since the 1950s. While recent advances in large language models have led to impressive progress, there is still no consensus on how much common sense today's AI actually possesses. In this brief review, we revisit the historical development of MCS in the context of the Web, examining how the Web's evolution-from early knowledge representation efforts to knowledge graphs, the Semantic Web, and crowdsourcing-has shaped MCS research. We argue that key breakthroughs in Web technologies were instrumental in addressing longstanding challenges of scale and coverage in commonsense reasoning. At the same time, MCS research has influenced the development of core Web applications, including intelligent agents, plausibility-based reasoning, and robust evaluation of black-box AI systems.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {837–840},
numpages = {4},
keywords = {conceptnet, cyc, llms, machine common sense},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3318299.3318365,
author = {Benarab, Achref and Rafique, Fahad and Sun, Jianguo},
title = {An Ontology Embedding Approach Based on Multiple Neural Networks},
year = {2019},
isbn = {9781450366007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318299.3318365},
doi = {10.1145/3318299.3318365},
abstract = {In this paper, we present a low-dimensional vector representation method for the concepts and instances of an ontology. The main idea is to transform the ontological entities into digestible data for machine learning and deep learning algorithms that only use digital inputs. The generated vectors will represent the semantics contained in the source ontology. We use the semantic relationships connecting the concepts as a landmark to train expert neural networks using the noise contrastive estimation technique to project them into a vector space specific to this relationship with weightings dependent on their frequency. The resulting vectors are then combined and fed into an autoencoder to generate a denser representation. The generated representation vectors can be used to find the semantically similar ontology entities, allowing creating a semantic network automatically. Thus, semantically similar ontology entities will have relatively close corresponding vector representations in the projection space.},
booktitle = {Proceedings of the 2019 11th International Conference on Machine Learning and Computing},
pages = {186–190},
numpages = {5},
keywords = {multiple neural networks, feature representation, continuous vector representations, concept embeddings, autoencoders, Ontology embeddings},
location = {Zhuhai, China},
series = {ICMLC '19}
}

@inproceedings{10.1145/3585967.3585994,
author = {Jiang, Haoyu},
title = {Short-Text Semantic Similarity Model of BERT-Based Siamese Network},
year = {2023},
isbn = {9781450398466},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585967.3585994},
doi = {10.1145/3585967.3585994},
abstract = {People convey their emotions and thoughts through words, the medium of human thoughts. Up against the vigorous development of streaming media, the calculation of text similarity is imperative in the field of natural language processing. Any text-related field is inseparable from text semantic similarity. The calculation of text semantic similarity plays a key role in document management, document classification, and document relevance. Besides, popular natural language processing tasks in some trendy fields, such as artificial intelligence, human-machine translation, problem system, intelligent chat system, and nomenclature recognition, are intertwined with text semantic similarity calculation. In recent years, many excellent researchers have studied the algorithms and models of text semantic similarity from different dimensions. In this paper, a new short-text cosine similarity calculation model of the BERT-based Siamese network is proposed.},
booktitle = {Proceedings of the 2023 10th International Conference on Wireless Communication and Sensor Networks},
pages = {145–149},
numpages = {5},
keywords = {Siamese Network, Short-Text Semantic Similarity, Cosine Similarity, BERT},
location = {Chengdu, China},
series = {icWCSN '23}
}

@inproceedings{10.1145/3291280.3291786,
author = {Jearanaiwongkul, Watanee and Anutariya, Chutiporn and Andres, Frederic},
title = {An Ontology-based Approach to Plant Disease Identification System},
year = {2018},
isbn = {9781450365680},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291280.3291786},
doi = {10.1145/3291280.3291786},
abstract = {Disease identification in plants is an important issue for farmers in terms of plant production and the reduction of losses in crop field. To deal with this issue, a number of systems and techniques for identifying plant diseases have been proposed. However, most of them mainly concentrate on image-based pattern recognition rather than focusing on the observed abnormalities of plant diseases. In other words, they have not employed ontology for semantic detection of plant disease. Current systems do not support farmers to find disease names w.r.t. the semantics of an infected plant. In this work, we proposed an ontology-based approach to modeling plant diseases and demonstrate our approach by developing a rice disease ontology. The ontology helps identifying plant diseases from existing symptoms on plants. To construct the ontology, we reused the domain knowledge related to symptoms of plant diseases from reliable sources. The proposed ontology is modeled in Web Ontology Language (OWL). We also develope a system architecture compatible with the modeled ontology. Finally, we illustrate the usage of our ontology in DL Query for disease retrieval, given that a farmer's observation has already been transformed into an OWL concept. The retrieved results of DL Query make use of subsumption relationship among concepts and properties defined in the ontology.},
booktitle = {Proceedings of the 10th International Conference on Advances in Information Technology},
articleno = {20},
numpages = {8},
keywords = {Knowledge representation, Ontology, Plant disease identification},
location = {Bangkok, Thailand},
series = {IAIT '18}
}

@inproceedings{10.1145/3644523.3644561,
author = {Wang, Linyao and Zhao, Yan and Mao, Yinxuan and Lu, Zhiang},
title = {Evaluation Index System for SysML System Design Model},
year = {2024},
isbn = {9798400709517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644523.3644561},
doi = {10.1145/3644523.3644561},
abstract = {Model-based Systems Engineering (MBSE) is an advanced approach to complex product design and development. The fundamental concept of MBSE is to develop and use consistent models of the system under development, referred to as named system models, as a cross-sectional and cross-discipline platform for information transfer. With the implementation and popularization of MBSE, the industry has put forward higher requirements for the quality of system models. Due to the lack of system model evaluation theory, the quality of the model is opaque, and different stakeholders have different evaluation results for the same model product. This defeats the purpose of using models instead of documents, which is to ensure a consistent exchange of information, and then seriously hinders the use and delivery of the model. Therefore, this study analyzes the characteristics of the system and model and the application of the system model, forming an evaluation index system for SysML system models. It provides practitioners using SysML with systematic theory and practical general methods to solve these problems, and provides a reference for practitioners using different languages to evaluate the quality of MBSE models.},
booktitle = {Proceedings of the 2023 4th International Conference on Computer Science and Management Technology},
pages = {200–206},
numpages = {7},
location = {Xi'an, China},
series = {ICCSMT '23}
}

@inproceedings{10.1145/3318396.3318422,
author = {Tapia-Leon, Mariela and Santana-Perez, Idafen and Poveda-Villal\'{o}n, Mar\'{\i}a and Espinoza-Arias, Paola and Chicaiza, Janneth and Corcho, Oscar},
title = {Extension of the BiDO Ontology to Represent Scientific Production},
year = {2019},
isbn = {9781450362672},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318396.3318422},
doi = {10.1145/3318396.3318422},
abstract = {The SPAR Ontology Network is a suite of complementary ontology modules to describe the scholarly publishing domain. BiDO Standard Bibliometric Measures is part of its set of ontologies. It allows describing of numerical and categorical bibliometric data such as h-index, author citation count, journal impact factor. These measures may be used to evaluate scientific production of researchers. However, they are not enough. In a previous study, we determined the lack of some terms to provide a more complete representation of scientific production. Hence, we have built an extension using the NeOn Methodology to restructure the BiDO ontology. With this extension, it is possible to represent and measure the number of documents from research, the number of citations from a paper and the number of publications in high impact journals according to its area and discipline.},
booktitle = {Proceedings of the 2019 8th International Conference on Educational and Information Technology},
pages = {166–172},
numpages = {7},
keywords = {BiDO, Ontology, RDF, SPAR Ontology Network, SPARQL, Scholarly Publishing, Scientific Production},
location = {Cambridge, United Kingdom},
series = {ICEIT 2019}
}

@inproceedings{10.1145/3606305.3606320,
author = {Ivanova, Tatyana Ivanova},
title = {Collaborative methodology for semantic modeling of learning domain knowledge},
year = {2023},
isbn = {9798400700477},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3606305.3606320},
doi = {10.1145/3606305.3606320},
abstract = {Technologies for structured representation of knowledge are very useful for learning and for organization of personalized tutoring. Concept maps (CMs) have been used in many different ways to support learning. They are good tool for regular note-taking, visualization of the structure of learning content, or assisting in discussions and problem-solving activities. On the other hand, ontologies can benefit personalization and adaption of the tutoring process to the needs of specific learners or groups. This research discusses benefits of joint usage of both concept maps and ontologies in e-learning. We propose a collaborative methodology for semantic modeling of learning domain knowledge, showing how to use both CMs and ontologies to improve learning.},
booktitle = {Proceedings of the 24th International Conference on Computer Systems and Technologies},
pages = {174–179},
numpages = {6},
location = {Ruse, Bulgaria},
series = {CompSysTech '23}
}

@inproceedings{10.1145/3284557.3284714,
author = {Brecher, Christian and Kusmenko, Evgeny and Lindt, Achim and Rumpe, Bernhard and Storms, Simon and Wein, Stephan and von Wenckstern, Michael and Wortmann, Andreas},
title = {Multi-Level Modeling Framework for Machine as a Service Applications Based on Product Process Resource Models},
year = {2018},
isbn = {9781450366281},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3284557.3284714},
doi = {10.1145/3284557.3284714},
abstract = {At present, manufacturing processes are highly tailored to a specific product. Changes in product requirements therefore lead to big manual efforts for adapting the manufacturing process and reconfiguring production resources accordingly. Existing approaches do not cope well with this complexity. This hinders agile, customer-oriented manufacturing. A promising approach for automated assembling processes is the Machine as a Service paradigm, which aims for providing production resources on demand. This requires a consistent and pervasive formalization of product specifications, the corresponding manufacturing resources and their interdependencies. Thus, our first contribution is a generic and extensible multi-level and modular modeling framework to formalize products and available resources. Our framework is scalable for large companies and enables reuse for cross-company collaboration and supplier integration. Thereby, the static relationship between product, process and resource is avoided by describing product features and resource skills in separate models. Our framework uses the standardized SysML/UML. Our second contribution is the ability of our framework to integrate different standards. For demonstration, we apply our multi-level approach to a flexible assembly of terminal boxes for transmission gears and show the integration of standards by embedding the eCl@ss classification.},
booktitle = {Proceedings of the 2nd International Symposium on Computer Science and Intelligent Control},
articleno = {4},
numpages = {9},
keywords = {Asset Administration Shell, Industry 4.0 Components, Integration of Ontologies, Language Adaption and Aggregation, Machine as a Service, Multi-level Modeling, PPR Models},
location = {Stockholm, Sweden},
series = {ISCSIC '18}
}

@inproceedings{10.1145/3297280.3297508,
author = {Pikus, Yevgen and Wei\ss{}enberg, Norbert and Holtkamp, Bernhard and Otto, Boris},
title = {Semi-automatic ontology-driven development documentation: generating documents from RDF data and DITA templates},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297508},
doi = {10.1145/3297280.3297508},
abstract = {For a data-driven economy, digitization of product information throughout the entire product lifecycle is key to agility and efficiency of product-related processes. Documenting products and their development, e.g., creating requirement specifications, is an indispensable, time-consuming and resource-intensive activity in large organizations. A vast amount of related information often emerges across several siloing lifecycle tools, and only a portion of it is available in the post-hoc documentation. Additionally, numerous product lines and versions additionally increase the documentation effort. To tackle these issues in a research project, we developed a semi-automatic end-to-end documentation system, able to generate documents based on templates and structured data. As a use case for document generation, we employ the RDF-based lifecycle tool integration standard OSLC and add extended publishing information. In order to generate target documents, we leverage DITA, an established digital publishing standard. A pilot implementation demonstrates that the approach is able to extract distributed lifecycle data and to generate several types of documents in multiple formats. Since the method can also be used to generate documents from arbitrary RDF graphs, the results can be generalized to other domains beyond software development. We believe that the results support the change from a document-driven to a data-driven documentation paradigm in large organizations.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2293–2302},
numpages = {10},
keywords = {digital publishing, linked data, ontology, product documentation, tool integration},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3341105.3374230,
author = {Ojino, Ronald Ochieng},
title = {Towards an ontology for personalized hotel room recommendation: student research abstract},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374230},
doi = {10.1145/3341105.3374230},
abstract = {This paper presents the design of an ontology based on user profile that allows personalizing guests' hotel rooms and services. The ontology being developed using NeON methodology, takes into consideration the maximum number of concepts associated with hotel guest profile and hotel room. The ontology will also provide a sound representation of comfort metrics for hotel rooms to support recommendation.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {2060–2063},
numpages = {4},
keywords = {NeON methodology, comfort metrics, ontology, semantics},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3351556.3351581,
author = {Stewart, Radovesta and Simeonov, Stanislav and Pavlov, Radoslav},
title = {Development of base ontology for a digital library of the Bulgarian museums' collections},
year = {2019},
isbn = {9781450371933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351556.3351581},
doi = {10.1145/3351556.3351581},
abstract = {This paper gives a further look into the process of ontology engineering for the needs of the Bulgarian museums' digital collections. The representation of the data model and a skeleton for a digital library offers a universal solution that can be used for the digitalization of movable cultural heritage ensuring its compatibility with the existing legislation in the domain. The main purpose of the base ontology is to unify and extend the usability of accumulated knowledge stored in the museum collection as well as information retrieval and query processing. The development of its structure has been proceeded following the bottom-up model due to the requirements of the front and backend users forming an important step for the standardization of I.T. solutions in the work of Bulgarian museums.},
booktitle = {Proceedings of the 9th Balkan Conference on Informatics},
articleno = {5},
numpages = {4},
keywords = {Digital libraries, Digitalization, Museum database, Ontologies},
location = {Sofia, Bulgaria},
series = {BCI'19}
}

@inproceedings{10.1145/3672608.3707798,
author = {Ehl, Marco and Ahmadian, Amir Shayan and Gro\ss{}er, Katharina and Elsofi, Duaa Adel Ali and Herrmann, Marc and Specht, Alexander and Schneider, Kurt and J\"{u}rjens, Jan},
title = {Supporting Software Engineers in IT Security and Privacy through Automated Knowledge Discovery},
year = {2025},
isbn = {9798400706295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672608.3707798},
doi = {10.1145/3672608.3707798},
abstract = {Security and privacy are increasingly essential concepts in software engineering. New threats and corresponding countermeasures are continuously discovered. Concurrently, projects are becoming more complex and are exposed to a greater number of threats. This presents a significant challenge for software engineers. As a result, security and privacy are often neglected due to a lack of knowledge, limited time, and financial constraints. While systematic literature reviews exist to address the increasing volume of publications, software engineers still require up-to-date knowledge of current threats and measures. This paper presents an automated, time-efficient, and cost-effective method for discovering knowledge from state-of-the-art literature and project artifacts, such as design documents. The presented method utilizes Large Language Models (LLMs) for data extraction and is demonstrated through a prototypical implementation and evaluation. This evaluation involves security and privacy in open-access scientific publications and project documentation from European Union research and development projects. The extracted knowledge is used to populate a quality model that is specifically designed to provide software engineers with information that helps them apply the findings. This quality model offers software engineers valuable, up-to-date insights into security and privacy, bridging the gap between scientific research and practical applications.},
booktitle = {Proceedings of the 40th ACM/SIGAPP Symposium on Applied Computing},
pages = {1647–1656},
numpages = {10},
keywords = {security, privacy, quality model, knowledge discovery, large language model},
location = {Catania International Airport, Catania, Italy},
series = {SAC '25}
}

@inproceedings{10.1145/3706598.3714255,
author = {Meng, Han and Zhang, Renwen and Wang, Ganyi and Yang, Yitian and Qin, Peinuan and Lee, Jungup and Lee, Yi-Chieh},
title = {Deconstructing Depression Stigma: Integrating AI-driven Data Collection and Analysis with Causal Knowledge Graphs},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714255},
doi = {10.1145/3706598.3714255},
abstract = {Mental-illness stigma is a persistent social problem, hampering both treatment-seeking and recovery. Accordingly, there is a pressing need to understand it more clearly, but analyzing the relevant data is highly labor-intensive. Therefore, we designed a chatbot to engage participants in conversations; coded those conversations qualitatively with AI assistance; and, based on those coding results, built causal knowledge graphs to decode stigma. The results we obtained from 1,002 participants demonstrate that conversation with our chatbot can elicit rich information about people’s attitudes toward depression, while our AI-assisted coding was strongly consistent with human-expert coding. Our novel approach combining large language models (LLMs) and causal knowledge graphs uncovered patterns in individual responses and illustrated the interrelationships of psychological constructs in the dataset as a whole. The paper also discusses these findings’ implications for HCI researchers in developing digital interventions, decomposing human psychological constructs, and fostering inclusive attitudes.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {641},
numpages = {21},
keywords = {Social Stigma, Depression, Causal Knowledge Graph, AI-assisted Coding, Chatbot, Large Language Model},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3589335.3641296,
author = {Todorov, Konstantin and Fafalios, Pavlos and Dietze, Stefan and Dimitrov, Dimitar},
title = {Beyond Facts: 4th International Workshop on Computational Methods for Online Discourse Analysis},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3641296},
doi = {10.1145/3589335.3641296},
abstract = {Expressing opinions and interacting with others on the Web has led to the production of an abundance of online discourse data, such as claims and viewpoints on controversial topics, their sources and contexts (events, entities). This data constitutes a valuable source of insights for studies into misinformation spread, bias reinforcement, echo chambers or political agenda setting. Computational methods, mostly from the field of NLP, have emerged that tackle a wide range of tasks in this context, including argument and opinion mining, claim detection, checkworthiness detection, stance detection or fact verification. However, computational models require robust definitions of classes and concepts under investigation. Thus, these computational tasks require a strong interdisciplinary and epistemological foundation, specifically with respect to the underlying definitions of key concepts such as claims, arguments, stances, check-worthiness or veracity. This requires a highly interdisciplinary approach combining expertise from fields such as communication studies, computational linguistics and computer science. As opposed to facts, claims are inherently more complex. Their interpretation strongly depends on the context and a variety of intentional or unintended meanings, where terminology and conceptual understandings strongly diverge across communities. From a computational perspective, in order to address this complexity, the synergy of multiple approaches, coming both from symbolic (knowledge representation) and statistical AI seem to be promising to tackle such challenges. This workshop aims at strengthening the relations between these communities, providing a forum for shared works on the modeling, extraction and analysis of discourse on the Web. It will address the need for a shared understanding and structured knowledge about discourse data in order to enable machine-interpretation, discoverability and reuse, in support of scientific or journalistic studies into the analysis of societal debates on the Web. Beyond research into information and knowledge extraction, data consolidation and modeling for knowledge graphs building, the workshop targets communities focusing on the analysis of online discourse, relying on methods from machine learning, natural language processing, large language models and Web data mining.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1418–1421},
numpages = {4},
keywords = {computational fact-checking, computational journalism, intent detection, knowledge graphs, language models, mis- and dis-information, online discourse analysis, social web mining, stance and viewpoint discovery},
location = {Singapore, Singapore},
series = {WWW '24}
}

@article{10.1145/3457608,
author = {Vera-Olivera, Harley and Guo, Ruizhe and Huacarpuma, Ruben Cruz and Da Silva, Ana Paula Bernardi and Mariano, Ari Melo and Holanda, Maristela},
title = {Data Modeling and NoSQL Databases - A Systematic Mapping Review},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457608},
doi = {10.1145/3457608},
abstract = {Modeling is one of the most important steps in developing a database. In traditional databases, the Entity Relationship (ER) and Unified Modeling Language (UML) models are widely used. But how are NoSQL databases being modeled? We performed a systematic mapping review to answer three research questions to identify and analyze the levels of representation, models used, and contexts where the modeling process occurred in the main categories of NoSQL databases. We found 54 primary studies where we identified that conceptual and logical levels received more attention than the physical level of representation. The UML, ER, and new notation based on ER and UML were adapted to model NoSQL databases, in the same way, formats such as JSON, XML, and XMI were used to generate schemas through the three levels of representation. New contexts such as benchmark, evaluations, migration, and schema generation were identified, as well as new features to be considered for modeling NoSQL databases, such as the number of records by entities, CRUD operations, and system requirements (availability, consistency, or scalability). Additionally, a coupling and co-citation analysis was carried out to identify relevant works and researchers.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {116},
numpages = {26},
keywords = {Data modeling, NoSQL databases, systematic mapping}
}

@inproceedings{10.1145/3406865.3418595,
author = {Polovina, Simon and Polovina, Rubina and Kemp, Neil and Pu, Ken},
title = {MOVE: Measuring Ontologies in Value-seeking Environments: CSCW for Human Adaptation},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418595},
doi = {10.1145/3406865.3418595},
abstract = {The interest in sharing the Data-Information-Knowledge-Wisdom (DIKW) continuum has been amplified by the latest multi-scale social changes including but not limited to pandemics, economic crises, climate change, and racial issues. This workshop aims to inspire research and discussion on measuring sharing of the DIKW continuum, including through computer-mediated methods, represented by its ontologies. The implied suggestion is that there are ways to improve human adaptation by social technologies that enable rapidly finding solutions for complex global situations. We therefore invite research on (1) ontologies as a medium that enables comparing and measuring the DIKW continuum, (2) ontologies and their convergence or divergence with the values that motivate and determine DIKW sharing, (3) properties and dynamics of ontologies shared via social technologies in their relation to human adaptation.},
booktitle = {Companion Publication of the 2020 Conference on Computer Supported Cooperative Work and Social Computing},
pages = {475–482},
numpages = {8},
keywords = {human adaptation, knowledge, knowledge sharing, ontology},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.1145/3338906.3340446,
author = {Lohia, Pranay and Kannan, Kalapriya and Srivastava, Biplav and Mehta, Sameep},
title = {Design diagrams as ontological source},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340446},
doi = {10.1145/3338906.3340446},
abstract = {beginabstract In custom software development projects, it is frequently the case that the same type of software is being built for different customers. The deliverables are similar because they address the same market (e.g., Telecom, Banking) or have similar functions or both. However, most organisations do not take advantage of this similarity and conduct each project from scratch leading to lesser margins and lower quality. Our key observation is that the similarity among the projects alludes to the existence of a veritable domain of discourse whose ontology, if created, would make the similarity across the projects explicit. Design diagrams are an integral part of any commercial software project deliverables as they document crucial facets of the software solution. We propose an approach to extract ontological information from UML design diagrams (class and sequence diagrams) and represent it as domain ontology in a convenient representation. This ontology not only helps in developing a better understanding of the domain but also fosters software reuse for future software projects in that domain. Initial results on extracting ontology from thousands of model from public repository show that the created ontologies are accurate and help in better software reuse for new solutions. endabstract},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {863–873},
numpages = {11},
keywords = {Ontology Extraction, Semantic Representation, Software Re-use},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1145/3588947,
author = {Balsebre, Pasquale and Yao, Dezhong and Cong, Gao and Huang, Weiming and Hai, Zhen},
title = {Mining Geospatial Relationships from Text},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588947},
doi = {10.1145/3588947},
abstract = {A geospatial Knowledge Graph (KG) is a heterogeneous information network, capable of representing relationships between spatial entities in a machine-interpretable format, and has tremendous applications in logistics and social networks. Existing efforts to build a geospatial KG, have mainly used sparse spatial relationships, e.g., a district located inside a city, which provide only marginal benefits compared to a traditional database. In spite of the substantial advances in the tasks of link prediction and knowledge graph completion, identifying geospatial relationships remains challenging, particularly due to the fact that spatial entities are represented with single-point geometries, and textual attributes are frequently missing. In this study, we present GTMiner, a novel framework capable of jointly modeling Geospatial and Textual information to construct a knowledge graph, by mining three useful spatial relationships from a geospatial database, in an end-to-end fashion. The system is divided into three components: (1) a Candidate Selection module, to efficiently select a small number of candidate pairs; (2) a Relation Prediction component to predict spatial relationships between the entities; (3) a KG Refinement procedure, to improve both coverage and correctness of a geospatial knowledge graph. We carry out experiments on four cities' geospatial databases, from publicly-available sources and compare with existing algorithms for link prediction and geospatial data integration. Finally, we conduct an ablation study to motivate our design choices and an efficiency analysis to show that the time required by GTMiner for training and inference is comparable, or even shorter, than existing solutions.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {93},
numpages = {26},
keywords = {area, geokg, geometry, geospatial, graph, information, interest, kg, knowledge, language, learning, model, of, ontology, poi, point, pois, prediction, relation, relationship, relationships, representation, spatial, text}
}

@inproceedings{10.1145/3570991.3571028,
author = {Kumar, Suresh and Kumar P, Sreenivasa},
title = {Using domain ontology to identify consistent and inconsistent cases from LSTM-generated transfer type AWPs},
year = {2023},
isbn = {9781450397971},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570991.3571028},
doi = {10.1145/3570991.3571028},
booktitle = {Proceedings of the 6th Joint International Conference on Data Science \&amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)},
pages = {289–290},
numpages = {2},
location = {Mumbai, India},
series = {CODS-COMAD '23}
}

@inproceedings{10.1145/3425329.3425332,
author = {Chen, Siming and Xiao, Liang and Cheng, Mo},
title = {A Semantic-based Multi-agent Dynamic Interaction Model},
year = {2020},
isbn = {9781450387873},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425329.3425332},
doi = {10.1145/3425329.3425332},
abstract = {Due to the autonomy of agents and their ability to perceive the environment, multi-agent systems have been widely used in many fields. The design of multi-agent systems requires the support of interactive models. The traditional multi-agent interaction model has certain feasibility in solving specific tasks. However, in a distributed environment, a relatively static multi-agent interaction model is not sufficient to support a dynamically changing interaction process. Frequent data interactions will also consume resources of multi-agent systems, thereby reducing agent performance. In this study, we propose a semantic-based multiagent dynamic interaction model (MADIM). MADIM uses semantic ontology to map the objects in the interaction model, and defines the interaction protocol through the rule description language. This model is attached with dynamically configurable semantic templates and interaction rule base. We added a reusable dynamic resolution engine component to MADIM to provide dynamic resolution services for the semantic information in the model. MADIM supports dynamic interactive behavior and has good interoperability and interpretability. Our model provides a flexible solution to the multi-agent interaction process. Finally, we verified the feasibility of the model design scheme through a simple example.},
booktitle = {Proceedings of the 2nd World Symposium on Software Engineering},
pages = {101–108},
numpages = {8},
keywords = {Grouping, Interaction model, Multi-agent system, Protocol, Semantic rules},
location = {Chengdu, China},
series = {WSSE '20}
}

@inproceedings{10.1145/3491102.3501998,
author = {Cambo, Scott Allen and Gergle, Darren},
title = {Model Positionality and Computational Reflexivity: Promoting Reflexivity in Data Science},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501998},
doi = {10.1145/3491102.3501998},
abstract = {Data science and machine learning provide indispensable techniques for understanding phenomena at scale, but the discretionary choices made when doing this work are often not recognized. Drawing from qualitative research practices, we describe how the concepts of positionality and reflexivity can be adapted to provide a framework for understanding, discussing, and disclosing the discretionary choices and subjectivity inherent to data science work. We first introduce the concepts of model positionality and computational reflexivity that can help data scientists to reflect on and communicate the social and cultural context of a model’s development and use, the data annotators and their annotations, and the data scientists themselves. We then describe the unique challenges of adapting these concepts for data science work and offer annotator fingerprinting and position mining as promising solutions. Finally, we demonstrate these techniques in a case study of the development of classifiers for toxic commenting in online communities.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {572},
numpages = {19},
keywords = {Computational reflexivity, annotator fingerprinting, critical data studies, data science, human-centered data science, human-centered machine learning, model positionality, position mining},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{10.1145/3607720.3607797,
author = {Iman, el kodssi and Sbai Hanae},
title = {A study of extending BPMN for IoT-aware process modeling},
year = {2023},
isbn = {9798400700194},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607720.3607797},
doi = {10.1145/3607720.3607797},
abstract = {A fundamental obstacle to automatic business process detection is the lack of modeling concepts that explicitly express Internet of Things elements as components of a business process model. In order to present a framework for discovering business process models from sensor data, we have studied and compared in our previous publications associated with the modeling of processes in an intelligent environment via the application of process mining. The application of this technique is associated with process modeling, which does not include IoT elements, namely IoT information and devices as output. In this paper, we presented a semantic framework built on our extended BPMN ontology model for IoT.},
booktitle = {Proceedings of the 6th International Conference on Networking, Intelligent Systems \&amp; Security},
articleno = {68},
numpages = {4},
location = {Larache, Morocco},
series = {NISS '23}
}

@inproceedings{10.1145/3658644.3690306,
author = {Wen, Rui and Li, Zheng and Backes, Michael and Zhang, Yang},
title = {Membership Inference Attacks Against In-Context Learning},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3690306},
doi = {10.1145/3658644.3690306},
abstract = {Adapting Large Language Models (LLMs) to specific tasks introduces concerns about computational efficiency, prompting an exploration of efficient methods such as In-Context Learning (ICL). However, the vulnerability of ICL to privacy attacks under realistic assumptions remains largely unexplored. In this work, we present the first membership inference attack tailored for ICL, relying solely on generated texts without their associated probabilities. We propose four attack strategies tailored to various constrained scenarios and conduct extensive experiments on four popular large language models. Empirical results show that our attacks can accurately determine membership status in most cases, e.g., 95\% accuracy advantage against LLaMA, indicating that the associated risks are much higher than those shown by existing probability-based attacks. Additionally, we propose a hybrid attack that synthesizes the strengths of the aforementioned strategies, achieving an accuracy advantage of over 95\% in most cases. Furthermore, we investigate three potential defenses targeting data, instruction, and output. Results demonstrate combining defenses from orthogonal dimensions significantly reduces privacy leakage and offers enhanced privacy assurances.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {3481–3495},
numpages = {15},
keywords = {in-context learning, large language models, membership inference attacks},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3706598.3713375,
author = {Shen, Hanshu and Shen, Lyukesheng and Wu, Wenqi and Zhang, Kejun},
title = {IdeationWeb: Tracking the Evolution of Design Ideas in Human-AI Co-Creation},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713375},
doi = {10.1145/3706598.3713375},
abstract = {Due to the remarkable content generation capabilities, large language models (LLMs) have demonstrated potential in supporting early-stage conceptual design. However, current interaction paradigms often struggle to effectively facilitate multi-round idea exploration and selection, leading to random outputs, unclear iterations, and cognitive overload. To address these challenges, we propose a human-AI co-ideation framework aimed at tracking the evolution of design ideas. This framework leverages a structured idea representation, an analogy-based reasoning mechanism and interactive visualization techniques. It guides both designers and AI to systematically explore design spaces. We also develop a prototype system, IdeationWeb, which integrates an intuitive, mind map-like visual interface and interactive methods to support co-ideation. Our user study validates the framework’s feasibility, demonstrating enhanced collaboration and creativity between humans and AI. Furthermore, we identified collaborative design patterns from user behaviors, providing valuable insights for future human-AI interaction design.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {146},
numpages = {19},
keywords = {Human-AI co-ideation, Human-AI interaction, Creativity support, Large language models, Design space},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3178248.3178257,
author = {Elstermann, Matthes and Krenn, Florian},
title = {The Semantic Exchange Standard for Subject-Oriented Process Models},
year = {2018},
isbn = {9781450353601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178248.3178257},
doi = {10.1145/3178248.3178257},
abstract = {After the first general concept about using ontologies for the exchange of subject-oriented business process models in 2014, a concrete proposal for using the Web Ontology Language (OWL) and the semantic web technology framework as a concrete and direct means was made in 2017 including a proof of concept and proposal for a first standard. Based upon this work a standardization committee has formed and further developed the concept, brought in or reduced definitions, and discussed and agreed on specific controversial topics. In this paper we describe the progress made in 2017 and the current state of the OWL-Standard for the Subject-Oriented Parallel Activity Specification Schema (PASS) modeling language. We present the current state and argue for the made conventions.},
booktitle = {Proceedings of the 10th International Conference on Subject-Oriented Business Process Management},
articleno = {5},
numpages = {8},
keywords = {OWL, PASS, ontologies, subject-oriented process models},
location = {Linz, Austria},
series = {S-BPM One '18}
}

@article{10.1145/3722231,
author = {Cimino, Gaetano and Deufemia, Vincenzo},
title = {SIGFRID: Unsupervised, Platform-Agnostic Interference Detection in IoT Automation Rules},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {2},
url = {https://doi.org/10.1145/3722231},
doi = {10.1145/3722231},
abstract = {Smart home technology has profoundly changed modern living by interconnecting devices, services, dataflows, and user interactions into integrated, automated environments. Homeowners can easily program smart devices using conditional IF-THEN rules, where triggers prompt corresponding actions. However, as smart homes incorporate more multifunctional devices, conflicting trigger-action rules can simultaneously control devices in inconsistent ways, causing unexpected and potentially unsafe interference situations. This article introduces Sigfrid, a novel interference detection approach using scene interaction graphs constructed through Large Language Models (LLMs). To enhance LLM reasoning, we propose a new prompt engineering methodology that integrates automated and manual editing techniques to formulate queries for deriving causal insights in the smart home domain. Interferences are identified through efficient exploration of the graph constructed from the extracted relations. We evaluate Sigfrid on real-world If-This-Then-That (IFTTT) and SmartThings rule sets, demonstrating its superiority over state-of-the-art methods by more than 21\% in F1-score.},
journal = {ACM Trans. Internet Things},
month = apr,
articleno = {13},
numpages = {33},
keywords = {IoT, trigger-action platforms, interference detection, behavioral modeling, smart home}
}

@inproceedings{10.1145/3487553.3524927,
author = {Dupuy, Jean and Guille, Adrien and Jacques, Julien},
title = {Anchor Prediction: A Topic Modeling Approach},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524927},
doi = {10.1145/3487553.3524927},
abstract = {Networks of documents connected by hyperlinks, such as Wikipedia, are ubiquitous. Hyperlinks are inserted by the authors to enrich the text and facilitate the navigation through the network. However, authors tend to insert only a fraction of the relevant hyperlinks, mainly because this is a time consuming task. In this paper we address an annotation, which we refer to as anchor prediction. Even though it is conceptually close to link prediction or entity linking, it is a different task that require developing a specific method to solve it. Given a source document and a target document, this task consists in automatically identifying anchors in the source document, i.e words or terms that should carry a hyperlink pointing towards the target document. We propose a contextualized relational topic model, CRTM, that models directed links between documents as a function of the local context of the anchor in the source document and the whole content of the target document. The model can be used to predict anchors in a source document, given the target document, without relying on a dictionary of previously seen mention or title, nor any external knowledge graph. Authors can benefit from CRTM, by letting it automatically suggest hyperlinks, given a new document and the set of target document to connect to. It can also benefit to readers, by dynamically inserting hyperlinks between the documents they’re reading. Experiments conducted on several Wikipedia corpora (in English, Italian and German) highlight the practical usefulness of anchor prediction and demonstrate the relevancy of our approach.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {1310–1318},
numpages = {9},
keywords = {Anchor prediction, Annotation, Document network, Topic modeling},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1145/3744554,
author = {K\"{u}hn, Ramona and Mitrovi\'{c}, Jelena and Granitzer, Michael},
title = {Computational Approaches to the Detection of Lesser-Known Rhetorical Figures: A Systematic Survey and Research Challenges},
year = {2025},
issue_date = {January 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3744554},
doi = {10.1145/3744554},
abstract = {Rhetorical figures play a major role in everyday communication, making text and speech more interesting, memorable, or persuasive through their association between form and meaning. Computational detection of rhetorical figures plays an important part in thorough understanding of complex communication patterns. In this survey, we provide a comprehensive overview of computational approaches to lesser-known rhetorical figures. We explore the linguistic and computational perspectives on rhetorical figures and highlight their significance in the field of Natural Language Processing. We present different figures in detail and investigate datasets, definitions, rhetorical functions, and detection approaches. We identify challenges such as dataset scarcity, language limitations, and reliance on rule-based methods.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {42},
numpages = {36},
keywords = {Rhetorical figures, computational rhetoric, rhetorical figure detection, rhetorical datasets}
}

@inproceedings{10.1145/3167132.3167140,
author = {Proen\c{c}a, Diogo and Borbinha, Jos\'{e}},
title = {Using enterprise architecture model analysis and description logics for maturity assessment},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167140},
doi = {10.1145/3167132.3167140},
abstract = {A Maturity Model represents a path towards an increasingly organized and systematic way of doing business. It is therefore a widely used technique valuable to assess certain aspects of organizations, as for example business processes. A maturity assessment can enable stakeholders to clearly identify strengths and improvement points, and prioritize actions in order to reach higher maturity levels. Doing maturity assessments can range from simple self-assessment questionnaires to full-blown assessment methods, such as those recommended by the ISO/IEC 15504 or the SEI CMMI. A main caveat of these assessments is the resources they encompass. In addition, many times the lack of automation renders benchmarks not possible. Assuming that the wide spread of Enterprise Architecture practices is making the modeling of business domains a fact, and considering the recent state of the art on the representation of those models as ontologies, this paper proposes how existing semantic technology can be used to automate maturity models assessment methods.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {102–109},
numpages = {8},
keywords = {OWL, description logics; archimate, enterprise architecture, maturity model, ontology},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3477314.3508383,
author = {Islam, Raisa and Cerny, Tomas and Shin, Dongwan},
title = {Ontology-based user privacy management in smart grid},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3508383},
doi = {10.1145/3477314.3508383},
abstract = {A smart grid system is one of the most complex cyber-physical systems consisting of power generation, distribution, consumption, customer domains, and millions of connected end devices. The widespread implementation of the smart grid raises concerns about the privacy of the data it collects. Since users' personal and non-personal data are going to be accessed by different entities involved in the smart grid and by third parties, the privacy concern can be a big obstacle in the adoption of the smart grid among people. Hence, there is an urgent need to provide the user with a privacy solution to support selective sharing of their usage data with different entities. In this paper, we propose an ontology-based user privacy management approach that will enable the user to release their data based on sensitivity and privacy factors, thus making an informed privacy decision on their usage data sharing. Green Button Initiative is a smart grid application that allows users to download and share their energy usage data with third parties. We present a proof-of-concept implementation extending the Green Button Initiative to test the feasibility of the proposed approach. Lastly, we discuss the results of a user study to investigate the effectiveness of our proposed approach.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {174–182},
numpages = {9},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3184558.3191533,
author = {Obeid, Charbel and Lahoud, Inaya and El Khoury, Hicham and Champin, Pierre-Antoine},
title = {Ontology-based Recommender System in Higher Education},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191533},
doi = {10.1145/3184558.3191533},
abstract = {Academic advising is limited in its ability to assist students in identifying academic pathways. Selecting a major and a university is a challenging process rife with anxiety. Students at high school are not sure how to match their interests with their working future or major. Therefore, high school students need guidance and support. Moreover, students need to filter, prioritize and efficiently get appropriate information from the web in order to solve the problem of information overload. This paper represents an approach for developing ontology-based recommender system improved with machine learning techniques to orient students in higher education. The proposed recommender system is an assessment tool for students' vocational strengths and weaknesses, interests and capabilities. The main objective of our ontology-based recommender system is to identify the student requirements, interests, preferences and capabilities to recommend the appropriate major and university for each one.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1031–1034},
numpages = {4},
keywords = {education, ontology-based, recommender system},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3615887.3627754,
author = {Rawsthorne, Helen Mair and Abadie, Nathalie and Kergosien, Eric and Duch\^{e}ne, C\'{e}cile and Saux, \'{E}ric},
title = {Automatic Nested Spatial Entity and Spatial Relation Extraction From Text for Knowledge Graph Creation: A Baseline Approach and a Benchmark Dataset},
year = {2023},
isbn = {9798400703492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615887.3627754},
doi = {10.1145/3615887.3627754},
abstract = {Automatically extracting geographic information from text is the key to harnessing the vast amount of spatial knowledge that only exists in this unstructured form. The fundamental elements of spatial knowledge include spatial entities, their types and the spatial relations between them. Structuring the spatial knowledge contained within text as a geospatial knowledge graph, and disambiguating the spatial entities, significantly facilitates its reuse. The automatic extraction of geographic information from text also allows the creation or enrichment of gazetteers. We propose a baseline approach for nested spatial entity and binary spatial relation extraction from text, a new annotated French-language benchmark dataset on the maritime domain that can be used to train algorithms for both extraction tasks, and benchmark results for the two tasks carried out individually and end-to-end. Our approach involves applying the Princeton University Relation Extraction system (PURE), made for flat, generic entity extraction and generic binary relation extraction, to the extraction of nested, spatial entities and spatial binary relations. By extracting nested spatial entities and the spatial relations between them, we have more information to aid entity disambiguation. In our experiments we compare the performance of a pretrained monolingual French BERT language model with that of a pretrained multilingual BERT language model, and study the effect of including cross-sentence context. Our results reveal very similar results for both models, although the multilingual model performs slightly better in entity extraction, and the monolingual model has slightly better relation extraction and end-to-end performances. We observe that increasing the amount of cross-sentence context improves the results for entity extraction whereas it has the opposite effect on relation extraction.},
booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on Geospatial Humanities},
pages = {21–30},
numpages = {10},
keywords = {binary spatial relation, deep learning, geographic information, language model, maritime data, nested spatial entity, neural network, spatial knowledge},
location = {Hamburg, Germany},
series = {GeoHumanities '23}
}

@inproceedings{10.1145/3239438.3239470,
author = {Su, Chuan-Jun and Huang, Shi-Feng and Li, Yi},
title = {Case Based Reasoning Driven Ontological Intelligent Health Projection System},
year = {2018},
isbn = {9781450363891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239438.3239470},
doi = {10.1145/3239438.3239470},
abstract = {For the past few years, health-related issues are getting more and more attention. With regular health screening, disease detection and subsequent medical attention can proceed with far lesser chance of missing the critical period. The projection of health related issues from health screening has been studied by using statistical analysis methods. However, the results generally fail to alert the subjects regarding the potential risks involved due to lack of certainty. The potential health issues derived from statistical analysis of health screening data and medical general guidelines can only be presented to the subjects with probability and lack of supportive hard evidence. The efficacy of health screening consequently fails to be realized. In order to confront the dilemma, we have developed an Intelligent Health Projection System (IHPS) for providing an evidential health status projection and a more motivated health plan to the subjects. This study focuses on modelling Type 2 Diabetes Mellitus (T2DM) and associated factors as an example. Other cases can be analogously implemented. The IHPS adaptively provides the projection of potential risk of getting T2DM by exploring the similarity between the subject's health screening data and previous T2DM patients' cases. The main building blocks, the cases that serve as a knowledge base for IHPS are modelled using ontology technology. As the main functionality of IHPS, the T2DM projection uses the traces left by previous T2DM patients and works on top of case-based reasoning mechanism. The proposed IHPS aims to promote better self-health management by enhancing a subject's comprehension on risks revealed in health screening result. The cases retrieved can not only being used for risk projection of a subject but also serving as evidences for physicians to provide more accurate and convincing health advice.},
booktitle = {Proceedings of the 2nd International Conference on Medical and Health Informatics},
pages = {185–194},
numpages = {10},
keywords = {Analytic Hierarchy Process, Case-based Reasoning, Entropy, Health Projection, Ontology, Type 2 Diabetes Mellitus},
location = {Tsukuba, Japan},
series = {ICMHI '18}
}

@inproceedings{10.1145/3358528.3358550,
author = {Liang, Yan and Wen, Zepeng and Liu, Li and Li, Gongliang and Guo, Bing},
title = {Towards A Goal-driven Dynamic Business Process Ontology},
year = {2019},
isbn = {9781450371926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358528.3358550},
doi = {10.1145/3358528.3358550},
abstract = {Business process Modelling plays an important role in supporting the whole life-cycle management of business processes. With the frequent changes of the environment and the urgent requirements of business collaboration, sharing, inter-operation, there is an increasing need to dynamically form business process models based on business goals and the underlying criteria. The traditional modelling methods, which focus on the activities with a fixed execution order, are not suitable for the situation. In this paper, a goal-driven dynamic business process ontology (GDBPO) is introduced. The ontology consists of four parts, business process ontology, goal ontology, business rule ontology and decision-making ontology. It can support decomposing the highlevel business goals into the refined operational level activity goals which are aligned with business rules and the available activities to dynamically construct the business process. Moreover, the proposed ontology can explicitly represent the knowledge within processes and be employed as a knowledge base for reasoning and decision-making.},
booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
pages = {295–299},
numpages = {5},
keywords = {Decision-making, Dynamic business process, Goal-driven, Ontology},
location = {Jinan, China},
series = {ICBDT '19}
}

@inproceedings{10.1145/3529372.3533285,
author = {Oelen, Allard and Stocker, Markus and Auer, S\"{o}ren},
title = {TinyGenius: intertwining natural language processing with microtask crowdsourcing for scholarly knowledge graph creation},
year = {2022},
isbn = {9781450393454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529372.3533285},
doi = {10.1145/3529372.3533285},
abstract = {As the number of published scholarly articles grows steadily each year, new methods are needed to organize scholarly knowledge so that it can be more efficiently discovered and used. Natural Language Processing (NLP) techniques are able to autonomously process scholarly articles at scale and to create machine readable representations of the article content. However, autonomous NLP methods are by far not sufficiently accurate to create a high-quality knowledge graph. Yet quality is crucial for the graph to be useful in practice. We present TinyGenius, a methodology to validate NLP-extracted scholarly knowledge statements using microtasks performed with crowdsourcing. The scholarly context in which the crowd workers operate has multiple challenges. The explainability of the employed NLP methods is crucial to provide context in order to support the decision process of crowd workers. We employed TinyGenius to populate a paper-centric knowledge graph, using five distinct NLP methods. In the end, the resulting knowledge graph serves as a digital library for scholarly articles.},
booktitle = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
articleno = {5},
numpages = {5},
keywords = {crowdsourcing microtasks, intelligent user interfaces, knowledge graph validation, scholarly knowledge graphs},
location = {Cologne, Germany},
series = {JCDL '22}
}

@inproceedings{10.1145/3485447.3511946,
author = {Nguyen, Vinh and Yip, Hong Yung and Bajaj, Goonmeet and Wijesiriwardene, Thilini and Javangula, Vishesh and Parthasarathy, Srinivasan and Sheth, Amit and Bodenreider, Olivier},
title = {Context-Enriched Learning Models for Aligning Biomedical Vocabularies at Scale in the UMLS Metathesaurus},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511946},
doi = {10.1145/3485447.3511946},
abstract = {The Unified Medical Language System (UMLS) Metathesaurus construction process mainly relies on lexical algorithms and manual expert curation for integrating over 200 biomedical vocabularies. A lexical-based learning model (LexLM) was developed to predict synonymy among Metathesaurus terms and largely outperforms a rule-based approach (RBA) that approximates the current construction process. However, the LexLM has the potential for being improved further because it only uses lexical information from the source vocabularies, while the RBA also takes advantage of contextual information. We investigate the role of multiple types of contextual information available to the UMLS editors, namely source synonymy (SS), source semantic group (SG), and source hierarchical relations (HR), for the UMLS vocabulary alignment (UVA) problem. In this paper, we develop multiple variants of context-enriched learning models (ConLMs) by adding to the LexLM the types of contextual information listed above. We represent these context types in context-enriched knowledge graphs (ConKGs) with four variants ConSS, ConSG, ConHR, and ConAll. We train these ConKG embeddings using seven KG embedding techniques. We create the ConLMs by concatenating the ConKG embedding vectors with the word embedding vectors from the LexLM. We evaluate the performance of the ConLMs using the UVA generalization test datasets with hundreds of millions of pairs. Our extensive experiments show a significant performance improvement from the ConLMs over the LexLM, namely +5.0\% in precision (93.75\%), +0.69\% in recall (93.23\%), +2.88\% in F1 (93.49\%) for the best ConLM. Our experiments also show that the ConAll variant including the three context types takes more time, but does not always perform better than other variants with a single context type. Finally, our experiments show that the pairs of terms with high lexical similarity benefit most from adding contextual information, namely +6.56\% in precision (94.97\%), +2.13\% in recall (93.23\%), +4.35\% in F1 (94.09\%) for the best ConLM. The pairs with lower degrees of lexical similarity also show performance improvement with +0.85\% in F1 (96\%) for low similarity and +1.31\% in F1 (96.34\%) for no similarity. These results demonstrate the importance of using contextual information in the UVA problem.},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {1037–1046},
numpages = {10},
keywords = {UMLS Metathesaurus, knowledge graph embeddings., neural networks, scalability, supervised learning, vocabulary alignment},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1109/TASLP.2024.3376984,
author = {Schmid, Florian and Koutini, Khaled and Widmer, Gerhard},
title = {Dynamic Convolutional Neural Networks as Efficient Pre-Trained Audio Models},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3376984},
doi = {10.1109/TASLP.2024.3376984},
abstract = {The introduction of large-scale audio datasets, such as AudioSet, paved the way for Transformers to conquer the audio domain and replace CNNs as the state-of-the-art neural network architecture for many tasks. Audio Spectrogram Transformers are excellent at exploiting large datasets, creating powerful pre-trained models that surpass CNNs when fine-tuned on downstream tasks. However, current popular Audio Spectrogram Transformers are demanding in terms of computational complexity compared to CNNs. Recently, we have shown that, by employing Transformer-to-CNN Knowledge Distillation, efficient CNNs can catch up with and even outperform Transformers on large datasets. In this work, we extend this line of research and increase the capacity of efficient CNNs by introducing dynamic CNN blocks constructed of dynamic convolutions, a dynamic ReLU activation function, and Coordinate Attention. We show that these dynamic CNNs outperform traditional efficient CNNs, such as MobileNets, in terms of the performance–complexity trade-off at the task of audio tagging on the large-scale AudioSet. Our experiments further indicate that the proposed dynamic CNNs achieve competitive performance with Transformer-based models for end-to-end fine-tuning on downstream tasks while being much more computationally efficient.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = mar,
pages = {2227–2241},
numpages = {15}
}

@proceedings{10.1145/3558489,
title = {PROMISE 2022: Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering},
year = {2022},
isbn = {9781450398602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our pleasure to welcome you to the 18th ACM International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE 2022), to be held in hybrid mode (physically and virtually) on November 18th, 2022, co-located with the ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). PROMISE is an annual forum for researchers and practitioners to present, discuss and exchange ideas, results, expertise and experiences in the construction and/or application of predictive models and data analytics in software engineering. Such models and analyses could be targeted at planning, design, implementation, testing, maintenance, quality assurance, evaluation, process improvement, management, decision making, and risk assessment in software and systems development. This year PROMISE received a total of 18 paper submissions. The review process was double blind and each paper was reviewed by at least three members of the program committee. An online discussion was also held for 8 days. Based on this procedure, we accepted a total of 10 full papers, which will be presented in 3 technical sessions. The acceptance criteria were entirely based on the quality of the papers, without imposing any constraint on the number of papers to be accepted.    We are delighted to announce an outstanding keynote: Release Engineering in the AI World: How can Analytics Help? By Prof. Bram Adams, Queen’s University, Canada    We would like to thank all authors for submitting high quality papers, and program committee members for their timely and accurate reviewing activity. Last, but not least, we would like to thank the FSE 2022 organizers for hosting PROMISE 2022 as a co-located event and for their logistic support in the organization of the conference.   We hope you will enjoy PROMISE 2022.   We certainly will!    Many thanks from   Shane McIntosh (General Chair),   Gema Rodriguez-Perez and Weiyi Shang (Program Chairs).},
location = {Singapore, Singapore}
}

@inproceedings{10.1145/3241403.3241457,
author = {Schr\"{o}der, Sandra and Riebisch, Matthias},
title = {An ontology-based approach for documenting and validating architecture rules},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241457},
doi = {10.1145/3241403.3241457},
abstract = {Architecture conformance checking is an important activity of architecture enforcement where the architect ensures that all architecture concepts are implemented correctly in the source code. In order to support the architect, a lot of tools for conformance checking are available that allow to formalize the architecture in order to perform an automated verification. Typically, the formalization uses a rigid, tool-specific architecture concept language that may strongly deviate from the project-specific architecture concept language. In addition, a high level of formal expertise is required in order to comprehend the created formalization. We present an approach that uses a controlled natural language for the formalization of architecture concepts. This language allows to flexibly express architecture rules directly with project-specific concepts. Consequently, the resulting formalization is easy to understand and might also be used as an architecture documentation at the same time. Nevertheless, the documentation can be automatically verified, since the approach is based on powerful means of the semantic web, i.e., ontologies and description logics. For the evaluation of the approach, we use the real-world software system TEAMMATES and show that architecture rules and concepts can be flexibly designed and checked for conformance in order to detect crucial architecture violations.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {52},
numpages = {7},
keywords = {architecture conformance checking, architecture documentation, architecture erosion, description logics, ontologies},
location = {Madrid, Spain},
series = {ECSA '18}
}

@inproceedings{10.1145/3726302.3730164,
author = {Li, Wing Yan and Wang, Zeqiang and Johnson, Jon and De, Suparna},
title = {Are Information Retrieval Approaches Good at Harmonising Longitudinal Surveys in Social Science?},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730164},
doi = {10.1145/3726302.3730164},
abstract = {Automated detection of semantically equivalent questions in longitudinal social science surveys is crucial for long-term studies informing empirical research in the social, economic, and health sciences. Retrieving equivalent questions faces dual challenges: inconsistent representation of theoretical constructs (i.e. concept/sub-concept) across studies as well as between question and response options, and the evolution of vocabulary and structure in longitudinal text. To address these challenges, our multi-disciplinary collaboration of computer scientists and survey specialists presents a new information retrieval (IR) task of identifying concept (e.g. Housing, Job, etc.) equivalence across question and response options to harmonise longitudinal population studies. This paper investigates multiple unsupervised approaches on a survey dataset spanning 1946-2020, including probabilistic models, linear probing of language models, and pre-trained neural networks specialised for IR. We show that IR-specialised neural models achieve the highest overall performance with other approaches performing comparably. Additionally, the re-ranking of the probabilistic model's results with neural models only introduces modest improvements of 0.07 at most in F1-score. Qualitative post-hoc evaluation by survey specialists shows that models generally have a low sensitivity to questions with high lexical overlap, particularly in cases where sub-concepts are mismatched. Altogether, our analysis serves to further research on harmonising longitudinal studies in social science.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2946–2950},
numpages = {5},
keywords = {conceptual comparison, information retrieval, longitudinal study, natural language processing},
location = {Padua, Italy},
series = {SIGIR '25}
}

@article{10.1145/3588316,
author = {Lin, Jerry Chun-Wei and D\'{I}az, Vicente Garc\'{I}a and Molinera, Juan Antonio Morente},
title = {Introduction to the Special Issue of Recent Advances in Computational Linguistics for Asian Languages},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3588316},
doi = {10.1145/3588316},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {62},
numpages = {5}
}

@article{10.1162/coli_a_00385,
author = {\v{Z}abokrtsk\'{y}, Zden\v{e}k and Zeman, Daniel and \v{S}ev\v{c}\'{\i}kov\'{a}, Magda},
title = {Sentence Meaning Representations Across Languages: What Can We Learn
                    from Existing Frameworks?},
year = {2020},
issue_date = {September 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {46},
number = {3},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00385},
doi = {10.1162/coli_a_00385},
abstract = {This article gives an overview of how sentence meaning is represented in eleven deep-syntactic frameworks, ranging from those based on linguistic theories elaborated for decades to rather lightweight NLP-motivated approaches. We outline the most important characteristics of each framework and then discuss how particular language phenomena are treated across those frameworks, while trying to shed light on commonalities as well as differences.},
journal = {Comput. Linguist.},
month = nov,
pages = {605–665},
numpages = {61}
}

@inproceedings{10.1145/3459637.3482097,
author = {Zhang, Zhiling and Zhou, Zelin and Tang, Haifeng and Li, Guangwei and Wu, Mengyue and Zhu, Kenny Q.},
title = {Enriching Ontology with Temporal Commonsense for Low-Resource Audio Tagging},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482097},
doi = {10.1145/3459637.3482097},
abstract = {Audio tagging aims at predicting sound events occurred in a recording. Traditional models require enormous laborious annotations, otherwise performance degeneration will be the norm. Therefore, we investigate robust audio tagging models in low-resource scenarios with the enhancement of knowledge graphs. Besides existing ontological knowledge, we further propose a semi-automatic approach that can construct temporal knowledge graphs on diverse domain-specific label sets. Moreover, we leverage a variant of relation-aware graph neural network, D-GCN, to combine the strength of the two knowledge types. Experiments on AudioSet and SONYC urban sound tagging datasets suggest the effectiveness of the introduced temporal knowledge, and the advantage of the combined KGs with D-GCN over single knowledge source.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {3652–3656},
numpages = {5},
keywords = {audio tagging, graph neural network, knowledge graph, low-resource},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@inproceedings{10.1145/3453892.3461349,
author = {Bifis, Aristeidis and Trigka, Maria and Dedegkika, Sofia and Goula, Panagiota and Constantinopoulos, Constantinos and Kosmopoulos, Dimitrios},
title = {A Hierarchical Ontology for Dialogue Acts in Psychiatric Interviews},
year = {2021},
isbn = {9781450387927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453892.3461349},
doi = {10.1145/3453892.3461349},
abstract = {We present our work on modeling the context of an interview during diagnostic sessions for patients with mental health problems. The results are to be exploited by translation system for telehealth services. More specifically, we plan to use the context of the psychiatric interview in order to set informative priors over the vocabulary of the speaker. Therefore we have modelled the context with a hierarchical ontology, and we use it to classify the current state of the interview. The state is extracted after the doctor asks a question, and allow us to select a non-uniform prior regarding the vocabulary of the patient.},
booktitle = {Proceedings of the 14th PErvasive Technologies Related to Assistive Environments Conference},
pages = {330–337},
numpages = {8},
keywords = {depression, dialogue context, hierarchical classification, stress},
location = {Corfu, Greece},
series = {PETRA '21}
}

@inproceedings{10.1145/3341620.3341640,
author = {Wang, Wei and Mu, Wenxin and Gou, Juanqiong},
title = {Spatial-temporal Data Association Based Ontology Alignment Research in High Education Context},
year = {2019},
isbn = {9781450360913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341620.3341640},
doi = {10.1145/3341620.3341640},
abstract = {In the process of practicing smart campus, student behavior is highly concerned. Student activities produce spatial-temporal data, which records the daily life of students and contains the potential regulations of student behavior. The complexity of these data brings challenges for data collection and data analysis. The key to solve these problems is data fusion. In addition, ontology alignment is an important method for exploring the association between different ontology in different fields. It can solve the problem of data fusion in practice and maximize the value of data. At present, the research methods of ontology alignment are mostly mathematical similarity algorithms, and not considering the uncertainty of spatial-temporal data. Ontology is better way to deal with spatial-temporal data. In order to solve this problem effectively, this paper proposes a method based on spatial-temporal data association, which establishes fuzzy ontology and formulates a series of fuzzy rules for fuzzy reasoning, and mines the relationship between data; then it connects the different concepts between ontologies to realize the alignment of ontologies. Finally, the fuzzy ontology modeling and fuzzy reasoning are implemented and tested by using high education context. The rationality and effectiveness of the method are verified.},
booktitle = {Proceedings of the 2019 International Conference on Big Data Engineering},
pages = {125–130},
numpages = {6},
keywords = {Data association, fuzzy ontology, fuzzy reasoning, ontology alignment},
location = {Hong Kong, Hong Kong},
series = {BDE '19}
}

@article{10.1145/3579615,
author = {Ferrier-Barbut, El\'{e}onore and Avellino, Ignacio and Canlorbe, Geoffroy and Vitrani, Marie-Aude and Luengo, Vanda},
title = {Learning With Pedagogical Models: Videos As Adjuncts to Apprenticeship for Surgical Training},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CSCW1},
url = {https://doi.org/10.1145/3579615},
doi = {10.1145/3579615},
abstract = {Videos are a powerful media to learn activities through guided physical training such as surgery, especially when they are produced following human learning models and not as "how-to" videos. However, their success greatly depends on how they are integrated into the extensive curricula of domains where learning occurs through guided practice. In this work, we investigate the impact of integrating video as a learning tool into the learning curricula of surgery. We created a pedagogical video on surgical hysterectomy through a model based on the Conceptual Fields theory (Vergnaud) and performed two rounds of interviews with seven medical residents, who watched the video freely during their residency in gynecology-obstetrics as they trained with experts. We find that videos can complement guided physical training, as they can provide the rationale behind expert action, something that is difficult to explicit during guided training. Still, their linear and static nature limits their integration as true adjuncts. We discuss our vision of moving towards interactive videos created with an ontological approach, developed in a workshop with four expert surgeons, which involves the ability to navigate through levels of information and layers of representations, so that experts can represent information to learners according to pedagogical models that complement their complex and extensive learning curricula.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {139},
numpages = {40},
keywords = {conceptual fields theory, guided practice, surgery, video-based learning}
}

@inproceedings{10.1145/3696410.3714694,
author = {Ahmetaj, Shqiponja and Boneva, Iovka and Hidders, Jan and Hose, Katja and Jakubowski, Maxime and Labra Gayo, Jose Emilio and Martens, Wim and Mogavero, Fabio and Murlak, Filip and Okulmus, Cem and Polleres, Axel and Savkovi\'{c}, Ognjen and \v{S}imkus, Mantas and Tomaszuk, Dominik},
title = {Common Foundations for SHACL, ShEx, and PG-Schema},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714694},
doi = {10.1145/3696410.3714694},
abstract = {Graphs have emerged as a foundation for a variety of applications, including capturing factual knowledge, semantic data integration, social networks, and informing machine learning algorithms. Formalising properties of the data and ensuring data quality requires describing schemas of such graphs. Driven by diverse applications, the Semantic Web and database communities developed not only different graph data models-RDF and property graphs-but also different graph schema languages-SHACL, ShEx, and PG-Schema. Each language has its unique approach to defining constraints and validating graph data, leaving potential users in the dark about their commonalities and differences. In this paper, we provide concise formal definitions of the core components of these languages, employ a uniform framework to facilitate a comprehensive comparison between them, and identify a common set of functionalities, shedding light on both overlapping and distinctive features.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {8–21},
numpages = {14},
keywords = {PG-schema, RDF, SHACL, ShEx, common data model, graph databases, graph schema languages, property graphs},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3290511.3290546,
author = {Chen, Jizhi and Gu, Junzhong},
title = {Developing educational ontology: a case study in physics},
year = {2018},
isbn = {9781450365178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290511.3290546},
doi = {10.1145/3290511.3290546},
abstract = {Nowadays, e-learning systems are widely developed. With e-learning, students can learn different subjects remotely, and teachers can edit online teaching scripts and issue online courseware. But complain is ceaseless, that is either students or teachers are not satisfied with the existing state of affairs. The reason is that they expect that students can master knowledge architecture of required subjects, but current scattered courseware lacks systematicness. How to describe knowledge architectures of subjects, e.g. in K12, and help students to master them? Ontology can be used to efficiently present knowledge architecture of different subjects, such as Physics. A big challenge is how to construct educational ontology to describe systematic knowledge of subjects automatically. In this paper educational ontology is as a new topic discussed. An approach to automatically constructing educational ontology is proposed to convert textbook into a corresponding Ontology, with High School Physics as an example.},
booktitle = {Proceedings of the 10th International Conference on Education Technology and Computers},
pages = {201–206},
numpages = {6},
keywords = {e-learning, educational ontology, ontology, ontology development},
location = {Tokyo, Japan},
series = {ICETC '18}
}

@inproceedings{10.1145/3377170.3377201,
author = {Ratsamano, Salintip and Chairungsee, Supaporn},
title = {Knowledge-Based Ontology Development for Folk Medicine},
year = {2020},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377170.3377201},
doi = {10.1145/3377170.3377201},
abstract = {This research sought to develop a semantic ontology of knowledge about the introduction of folk medicine. In this research, the content of folk medicine is presented comprehensively and the research goals or procedures are defined as follows: (1) determine the purpose and scope of the ontology; (2) develop the ontology by using the Hozo Ontology Editor program (Osaka University, Osaka, Japan); and (3) have an expert evaluate the ontology that is developed. The purpose of this research is to clarify the system-recommended treatment involving folk medicine that can guide patients well. Ideally, the treatment selection process will be accurate 100\% of the time and can be used in related research.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {70–74},
numpages = {5},
keywords = {Ontology, Thai medicine, folk medicine, semantic web},
location = {Shanghai, China},
series = {ICIT '19}
}

@inproceedings{10.1145/3580305.3599246,
author = {Zhang, Jiarui and Ilievski, Filip and Ma, Kaixin and Kollaa, Aravinda and Francis, Jonathan and Oltramari, Alessandro},
title = {A Study of Situational Reasoning for Traffic Understanding},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599246},
doi = {10.1145/3580305.3599246},
abstract = {Intelligent Traffic Monitoring (ITMo) technologies hold the potential for improving road safety/security and for enabling smart city infrastructure. Understanding traffic situations requires a complex fusion of perceptual information with domain-specific and causal commonsense knowledge. Whereas prior work has provided benchmarks and methods for traffic monitoring, it remains unclear whether models can effectively align these information sources and reason in novel scenarios. To address this assessment gap, we devise three novel text-based tasks for situational reasoning in the traffic domain: i) BDD-QA, which evaluates the ability of Language Models (LMs) to perform situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason about complex event causality, and iii) HDT-QA, which evaluates the ability of models to solve human driving exams. We adopt four knowledge-enhanced methods that have shown generalization capability across language reasoning tasks in prior work, based on natural language inference, commonsense knowledge-graph self-supervision, multi-QA joint training, and dense retrieval of domain information. We associate each method with a relevant knowledge source, including knowledge graphs, relevant benchmarks, and driving manuals. In extensive experiments, we benchmark various knowledge-aware methods against the three datasets, under zero-shot evaluation; we provide in-depth analyses of model performance on data partitions and examine model predictions categorically, to yield useful insights on traffic understanding, given different background knowledge and reasoning strategies.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3262–3272},
numpages = {11},
keywords = {language models, question answering, traffic understanding, zero-shot evaluation},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3355402.3355406,
author = {Fong, A. C.M. and Hong, Guanyue},
title = {Ontology-Powered Hybrid Extensional-Intensional Learning},
year = {2019},
isbn = {9781450372282},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3355402.3355406},
doi = {10.1145/3355402.3355406},
abstract = {Deep learning has made headlines in the past few years due to successes in tasks, such as self-driving vehicles and board games, which were previously thought difficult or impossible. The successes have generated much interest in artificial intelligence among researchers and members of the public. However, deep learning algorithms generally require very large labelled data sets to work well and large labelled data sets are not always readily available. In addition, most machine learning techniques, including deep learning, often perform well statistically but can fail miserably when, for example, data are deliberately perturbed in an adversarial attack. Another criticism of deep learning techniques is a relative lack of explainability. This paper proposes the use of intentional learning to simultaneously address these issues. Preliminary evaluation on the MNIST data set has shown promising results. Specifically, by combing extensional and intensional learning, it is possible to achieve similar accuracy result as extensional learning only using only one-sixth of the original training data set.},
booktitle = {Proceedings of the 2019 International Conference on Information Technology and Computer Communications},
pages = {18–23},
numpages = {6},
keywords = {Machine learning paradigms, intension and extension of concepts, neural networks, ontologies},
location = {Singapore, Singapore},
series = {ITCC '19}
}

@inproceedings{10.1145/3571884.3603756,
author = {Fischer, Joel E},
title = {Generative AI Considered Harmful},
year = {2023},
isbn = {9798400700149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571884.3603756},
doi = {10.1145/3571884.3603756},
abstract = {The recent months have seen an explosion of interest, hype, and concern about generative AI, driven by the release of ChatGPT. In this article I seek to explicate some potential and actual harms of the engineering and use of generative AI such as ChatGPT. With this I also suggest a reframing for researchers with an interest in interaction. With this reframing I seek to provoke researchers to consider studying the settings of ChatGPT development and use as active sites of production. Research should focus on the organisational, technological and interactional practices and contexts in and through which generative AI and its outputs—harmful and otherwise—are produced, by whom, to what end, and with what consequences on societies.},
booktitle = {Proceedings of the 5th International Conference on Conversational User Interfaces},
articleno = {7},
numpages = {5},
keywords = {ChatGPT, GPT-3, GPT-4, LLM, Large Language Models, NLG, NLP, generative AI, natural language, text generation},
location = {Eindhoven, Netherlands},
series = {CUI '23}
}

@inproceedings{10.1145/3365438.3410951,
author = {Song, Hui and Dautov, Rustem and Ferry, Nicolas and Solberg, Arnor and Fleurey, Franck},
title = {Model-based fleet deployment of edge computing applications},
year = {2020},
isbn = {9781450370196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365438.3410951},
doi = {10.1145/3365438.3410951},
abstract = {Edge computing brings software in close proximity to end users and IoT devices. Given the increasing number of distributed Edge devices with various contexts, as well as the widely adopted continuous delivery practices, software developers need to maintain multiple application versions and frequently (re-)deploy them to a fleet of many devices with respect to their contexts. Doing this correctly and efficiently goes beyond manual capabilities and requires employing an intelligent and reliable automated approach. Accordingly this paper describes a joint research with a Smart Healthcare application provider on a model-based approach to automatically assigning multiple software deployments to hundreds of Edge gateways. From a Platform-Specific Model obtained from the existing Edge computing platform, we extract a Platform-Independent Model that describes a list of target devices and a pool of available deployments. Next, we use constraint solving to automatically assign deployments to devices at once, given their specific contexts. The resulting solution is transformed back to the PSM as to proceed with software deployment accordingly. We validate the approach with a Fleet Deployment prototype integrated into the DevOps toolchain currently used by the application provider. Initial experiments demonstrate the viability of the approach and its usefulness in supporting DevOps in Edge computing applications.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {132–142},
numpages = {11},
keywords = {DevOps, IoT, device fleet, model-based software engineering, software deployment},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3397125.3397151,
author = {Voit, Nikolay and Kirillov, Sergey and Bochkov, Semen},
title = {Converting Diagram to a Timeline Ontology},
year = {2020},
isbn = {9781450377492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397125.3397151},
doi = {10.1145/3397125.3397151},
abstract = {CAD systems design and development is not a simple process. It consists of large amount of works, most of them are interconnected and should be performed either simultaneously or sequentially, some of them depend on success of previous works etc. Design workflows allow to describe works in visual form and calculate their quantitative and qualitative parameters. They significantly increase the design process efficiency and the product quality due to the usage of participants interaction language unification. However, modern workflow management tools lack of some important functions especially in part of temporal analysis and ontology-based timeline diagrams.In this paper, we describe the novel method to convert any diagram to a timeline structure like an ontology. The method includes converting algorithm and allows engineers to get the issue of workflows in which they are involved thus giving them a help to design complex CAD systems. It is shown that any diagram describing complex system behavior may be converted into simple view as a timeline ontology. An illustrated example is given in the article.},
booktitle = {Proceedings of the 2020 6th International Conference on Computer and Technology Applications},
pages = {80–86},
numpages = {7},
keywords = {analysis, business process, computer-aided design, workflows},
location = {Antalya, Turkey},
series = {ICCTA '20}
}

@inproceedings{10.1145/3412841.3441939,
author = {Combi, Carlo and Galetto, Francesca and Nakawala, Hirenkumar Chandrakant and Pozzi, Giuseppe and Zerbato, Francesca},
title = {Enriching surgical process models by BPMN extensions for temporal durations},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441939},
doi = {10.1145/3412841.3441939},
abstract = {Many surgical interventions are finding new techniques in robot-assisted surgery, which allows surgeons to perform surgery with the help of robotic arms. A formal representation of robot-assisted surgery can provide surgeons with an overview of the main stages of surgical intervention and a detailed description of the different steps, including all the possible emergencies that may occur. Formalizing such kinds of interventions could also help to train new surgeons. However, literature does not consider formal representations and properties of robot-assisted surgery properly.The Business Process Model and Notation (BPMN) is a standard language allowing to represent processes in a graphical and semi-formal way. In this paper, we propose to use BPMN for representing the processes and the guidelines underlying robot-assisted surgery, considering the explicit modeling of temporal and informational aspects: in detail, guidelines aim at providing surgeons with high-level recommendations based on the operational knowledge of expert users, delivering hints on how to execute exploration tasks. As a real-world application domain, we consider here the Robot-Assisted Partial Nephrectomy (RAPN), which is the partial surgical removal of a kidney to treat severe kidney diseases such as cancer.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {586–593},
numpages = {8},
keywords = {nephrectomy, process modelling (BPMN), surgical processes, temporal durations},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3463677.3463715,
author = {Palma, Ingrid and Ladeira, Marcelo and Reis, Ana Carla Bittencourt},
title = {Machine Learning Predictive Model for the Passive Transparency at the Brazilian Ministry of Mines and Energy},
year = {2021},
isbn = {9781450384926},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463677.3463715},
doi = {10.1145/3463677.3463715},
abstract = {This paper presents a case study based on the CRISP-DM Model and the use of Text Mining tools and techniques to automate the Passive Transparency process at the Brazilian Ministry of Mines and Energy. Thus, a Machine Learning Model is proposed to predict the class of the technical unit responsible for the data/information requested by citizens. Through the application of the algorithm LDA and TF-IDF it was possible to map the topics of the most relevant subjects for society. The stability of the model was tested from the comparative analysis between 5 known classification algorithms (Random Forest, Multinomial NB, Linear SVC, Logistic Regression, XGBoost and Gradient Boosting). XGBoost presented better performance and precision in multiclass learning outcomes.},
booktitle = {Proceedings of the 22nd Annual International Conference on Digital Government Research},
pages = {76–81},
numpages = {6},
keywords = {Machine Learning Algorithms, Multicriteria Decision Making, Passive Transparency, Predictive Analisys and XGBoost., Topic Modeling},
location = {Omaha, NE, USA},
series = {dg.o '21}
}

@article{10.1145/3700639,
author = {Rao, Abishek and Aithal, Shivani and Singh, Sanjay},
title = {Single-Document Abstractive Text Summarization: A Systematic Literature Review},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3700639},
doi = {10.1145/3700639},
abstract = {Abstractive text summarization is a task in natural language processing that automatically generates the summary from the source document in a human-written form with minimal loss of information. Research in text summarization has shifted towards abstractive text summarization due to its challenging aspects. This study provides a broad systematic literature review of abstractive text summarization on single-document summarization to gain insights into the challenges, widely used datasets, evaluation metrics, approaches, and methods. This study reviews research articles published between 2011 and 2023 from popular electronic databases. In total, 226 journal and conference publications were included in this review. The in-depth analysis of these papers helps researchers understand the challenges, widely used datasets, evaluation metrics, approaches, and methods. This article identifies and discusses potential opportunities and directions along with a generic conceptual framework and guidelines on abstractive summarization models and techniques for research in abstractive text summarization.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {60},
numpages = {37},
keywords = {Text summarization, abstractive summarization, abstractive text summarization, natural language processing}
}

@inproceedings{10.1145/3439961.3439993,
author = {Souza, \'{E}rica Ferreira de and Falbo, Ricardo de Almeida and Specimille, Marcos S. and Coelho, Alexandre G. N. and Vijaykumar, Nandamudi L. and Felizardo, Katia Romero and Meinerz, Giovani Volnei},
title = {Experience Report on Developing an Ontology-based Approach for Knowledge Management in Software Testing},
year = {2021},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439993},
doi = {10.1145/3439961.3439993},
abstract = {Software testing is a knowledge intensive process. Thus, Knowledge Management (KM) emerges as a means to manage testing knowledge, and, consequently, to improve software quality. However, there are only a few KM solutions supporting software testing. This paper reports experiences from the development of an approach, Ontology-based Testing Knowledge Management (OntoT-KM), to assist in launching KM initiatives in the software testing domain with the support of Knowledge Management Systems (KMSs). OntoT-KM provides a process guiding how to start applying KM in software testing. OntoT-KM is based on the findings of a systematic mapping on KM in software testing and the results of a survey with testing practitioners. Moreover, OntoT-KM considers the conceptualization established by a Reference Ontology on Software Testing (ROoST). As a proof of concept, OntoT-KM was applied to develop a KMS called Testing KM Portal (TKMP), which was evaluated in terms of usefulness, usability and functional correctness. Results show that the developed KMS from OntoT-KM is a potential system for managing knowledge in software testing, so, the approach is able to guide KM initiatives in software testing.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Software Quality},
articleno = {32},
numpages = {10},
keywords = {Knowledge Management, Knowledge Management System, Software Testing, Testing Ontology},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS '20}
}

@inproceedings{10.1145/3656766.3656776,
author = {Zhang, Xue and Wang, Li and Xu, Lianzheng and Fu, Deqian},
title = {A Distributed Logistics Data Security Sharing Model Based on Semantics and CP-ABE},
year = {2024},
isbn = {9798400716478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3656766.3656776},
doi = {10.1145/3656766.3656776},
abstract = {Effective sharing of logistics data in a secure manner is vital for the growth and progress of the logistics industry. With the development of logistics digitalization, the demand for data sharing among logistics enterprises is fiercely increasing. However, ensuring the safety and security of logistics business data is a challenging task, especially considering the importance of data sharing and exchange for business collaboration. To address this issue, we propose a distributed logistics data security sharing model that adopts semantics and CP-ABE technology to solve the semantic heterogeneity problem between logistics enterprises and ensure data security. In addition, we propose a semantic based access policy generation method, which is integrated into CP-ABE to simplify the creation process of access policies and improve the user friendliness and practicality of the system.},
booktitle = {Proceedings of the 2023 3rd International Conference on Big Data, Artificial Intelligence and Risk Management},
pages = {51–56},
numpages = {6},
location = {Chengdu, China},
series = {ICBAR '23}
}

@article{10.1145/3565481,
author = {Rizzo, Stefano Giovanni and Brucato, Matteo and Montesi, Danilo},
title = {Ranking Models for the Temporal Dimension of Text},
year = {2022},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3565481},
doi = {10.1145/3565481},
abstract = {Temporal features of text have been shown to improve clustering and organization of documents, text classification, visualization, and ranking. Temporal ranking models consider the temporal expressions found in text (e.g., “in 2021” or “last year”) as time units, rather than as keywords, to define a temporal relevance and improve ranking. This article introduces a new class of ranking models called Temporal Metric Space Models (TMSM), based on a new domain for representing temporal information found in documents and queries, where each temporal expression is represented as a time interval. Furthermore, we introduce a new frequency-based baseline called Temporal BM25 (TBM25). We evaluate the effectiveness of each proposed metric against a purely textual baseline, as well as several variations of the metrics themselves, where we change the aggregate function, the time granularity and the combination weight. Our extensive experiments on five test collections show statistically significant improvements of TMSM and TBM25 over state-of-the-art temporal ranking models. Combining the temporal similarity scores with the text similarity scores always improves the results, when the combination weight is between 2\% and 6\% for the temporal scores. This is true also for test collections where only 5\% of queries contain explicit temporal expressions.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {49},
numpages = {34},
keywords = {Temporal information retrieval, Temporal Metric Space, texto-temporal relevance, temporal ranking, timexes, time similarity}
}

@inproceedings{10.1145/3341105.3373863,
author = {Motara, Yusuf Moosa},
title = {A structural modeling notation for the typed functional paradigm},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373863},
doi = {10.1145/3341105.3373863},
abstract = {Although typed functional programming is becoming increasingly important for practical software development, it remains inaccessible from a modeling perspective. This paper develops and theoretically justifies an initial best-practices notation for the typed functional paradigm. A small case study explores how the same scenario is modeled differently in the object-oriented and typed functional paradigms, and it is argued that the notation developed is a necessary step on the path to a more comprehensive notation for modeling within the paradigm.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1515–1522},
numpages = {8},
keywords = {fml, functional programming, modeling languages, notation},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@article{10.1109/TCBB.2022.3167245,
author = {Di Persia, Leandro and Lopez, Tiago and Arce, Agustin and Milone, Diego H. and Stegmayer, Georgina},
title = {exp2GO: Improving Prediction of Functions in the Gene Ontology With Expression Data},
year = {2022},
issue_date = {March-April 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3167245},
doi = {10.1109/TCBB.2022.3167245},
abstract = {The computational methods for the prediction of gene function annotations aim to automatically find associations between a gene and a set of Gene Ontology (GO) terms describing its functions. Since the hand-made curation process of novel annotations and the corresponding wet experiments validations are very time-consuming and costly procedures, there is a need for computational tools that can reliably predict likely annotations and boost the discovery of new gene functions. This work proposes a novel method for predicting annotations based on the inference of GO similarities from expression similarities. The novel method was benchmarked against other methods on several public biological datasets, obtaining the best comparative results. exp2GO effectively improved the prediction of GO annotations in comparison to state-of-the-art methods. Furthermore, the proposal was validated with a full genome case where it was capable of predicting relevant and accurate biological functions. The repository of this project withh full data and code is available at &lt;uri&gt;https://github.com/sinc-lab/exp2GO&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = apr,
pages = {999–1008},
numpages = {10}
}

@inproceedings{10.1145/3677389.3702517,
author = {Yang, Can and Pereira Nunes, Bernardo and Rodr\'{\i}guez M\'{e}ndez, Sergio and Chen, Yige and Manrique, Rub\'{e}n and Casanova, Marco Antonio},
title = {Towards Comprehensive Artwork Representation: Motivations and Challenges in Capturing Multi-Dimensional Art Descriptions},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702517},
doi = {10.1145/3677389.3702517},
abstract = {This paper explores the motivations and challenges in developing a comprehensive framework for artwork representation. Current artwork-related ontologies and description methods often fall short in capturing the multi-faceted nature of artworks, focusing primarily on basic metadata or specific aspects while neglecting others. We propose a conceptual framework for an ontology that integrates descriptive, contextual, and interpretive aspects of artworks, addressing the limitations of existing models. The paper discusses the potential benefits of this holistic approach for art education, public appreciation, and digital accessibility. Key challenges are identified, including capturing complex visual elements, balancing objectivity with subjectivity in interpretation, and representing the multiple layers of meaning in artworks. The proposed framework aims to enhance the quality and depth of artwork representation, potentially facilitating the development of automated systems for artwork analysis and description.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {15},
numpages = {5},
keywords = {contextual object model, descriptive object model, ontology, captioning, artworks},
location = {Hong Kong, China},
series = {JCDL '24}
}

@inproceedings{10.5555/3522802.3522806,
author = {Wagner, Gerd},
title = {Business process modeling and simulation with DPMN: processing activities},
year = {2022},
publisher = {IEEE Press},
abstract = {The Business Process Modeling Notation (BPMN) has been established as a modeling standard in Business Process (BP) Management. However, BPMN lacks several important elements needed for BP simulation and is not well-aligned with the Queueing Network paradigm of Operations Research and the related BP simulation paradigm pioneered by the Discrete Event Simulation (DES) languages/tools GPSS and SIMAN/Arena. The Discrete Event Process Modeling Notation (DPMN) proposed by Wagner (2018) is based on Event Graphs (Schruben 1983), which capture the DES paradigm of Event-Based Simulation. By allowing to make flowchart models of queueing/processing networks with a precise semantics, DPMN reconciles (the flowchart approach of) BPMN with DES. DPMN is the first visual modeling language that supports all important DES approaches: event-based simulation, activity-based DES and Processing Network models, providing a foundation for harmonizing and unifying the many different terminologies/concepts and diagram languages of established DES tools.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {4},
numpages = {15},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3207677.3278056,
author = {Xian, Guojian and Li, Jiao and Kou, Yuantao and Luo, Tingting and Huang, Yongwen},
title = {Construction and Application of Upper Country Ontology Based on OWL and SKOS},
year = {2018},
isbn = {9781450365123},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3207677.3278056},
doi = {10.1145/3207677.3278056},
abstract = {The concept1 of country has been widely used in all kinds of databases or systems. A commonly understood and unified schema for country entity is needed to knowledge organization, semantic association and deeply integration of the multi-source and heterogeneous data resources about country concepts. This paper proposes a computer understandable and computable Upper Country Ontology based on OWL and SKOS, with instances of 195 countries, 7 continents, 38 regions and more than 10 international organizations. The proposed ontology is expected to be used as knowledge middleware, supports identifying and indexing country entities, publishing open linked dataset, referencing country instance, SPARQL endpoint query and semantic reasoning and so on.},
booktitle = {Proceedings of the 2nd International Conference on Computer Science and Application Engineering},
articleno = {153},
numpages = {6},
keywords = {Jena, OWL, Prot\'{e}g\'{e}, SKOS, SPARQL, Upper Country Ontology, WebVOWL},
location = {Hohhot, China},
series = {CSAE '18}
}

@inproceedings{10.1145/3357384.3358168,
author = {van Bremen, Timothy and Dries, Anton and Jung, Jean Christoph},
title = {Ontology-Mediated Queries over Probabilistic Data via Probabilistic Logic Programming},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358168},
doi = {10.1145/3357384.3358168},
abstract = {We study ontology-mediated querying over probabilistic data for the case when the ontology is formulated in EL(hdr), an expressive member of the EL family of description logics. We leverage techniques that have been developed (i) for classical ontology-mediated querying and (ii) for probabilistic logic programming and provide an implementation based on our findings. We include both theoretical considerations and an experimental evaluation of our approach.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2437–2440},
numpages = {4},
keywords = {ontology-mediated querying, probabilistic logic programming, uncertainty},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3285957.3285987,
author = {Pawe\l{}oszek, Ilona and Korczak, Jerzy},
title = {Merging of Ontologies - Conceptual Design Issues},
year = {2018},
isbn = {9781450364898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3285957.3285987},
doi = {10.1145/3285957.3285987},
abstract = {An approach to merging ontologies is presented that integrates financial knowledge in Decision Support Systems. The process of ontology merging is supported by an editor Prot\'{e}g\'{e}, and illustrated by examples extracted from financial information systems. It is shown how the correspondences between concepts, properties, and relations in the various ontologies can be processed. However not all problems of ontology integration may be resolved automatically. Therefore a number of cases of manager involvement in the merging process are considered.},
booktitle = {Proceedings of the 2018 10th International Conference on Information Management and Engineering},
pages = {59–63},
numpages = {5},
keywords = {Decision Support Systems, Financial ontology, Ontology integration, Ontology merging},
location = {Salford, United Kingdom},
series = {ICIME 2018}
}

@inproceedings{10.1109/ICSE43902.2021.00040,
author = {Lin, Jinfeng and Liu, Yalin and Zeng, Qingkai and Jiang, Meng and Cleland-Huang, Jane},
title = {Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00040},
doi = {10.1109/ICSE43902.2021.00040},
abstract = {Software traceability establishes and leverages associations between diverse development artifacts. Researchers have proposed the use of deep learning trace models to link natural language artifacts, such as requirements and issue descriptions, to source code; however, their effectiveness has been restricted by availability of labeled data and efficiency at runtime. In this study, we propose a novel framework called Trace BERT (T-BERT) to generate trace links between source code and natural language artifacts. To address data sparsity, we leverage a three-step training strategy to enable trace models to transfer knowledge from a closely related Software Engineering challenge, which has a rich dataset, to produce trace links with much higher accuracy than has previously been achieved. We then apply the T-BERT framework to recover links between issues and commits in Open Source Projects. We comparatively evaluated accuracy and efficiency of three BERT architectures. Results show that a Single-BERT architecture generated the most accurate links, while a Siamese-BERT architecture produced comparable results with significantly less execution time. Furthermore, by learning and transferring knowledge, all three models in the framework outperform classical IR trace models. On the three evaluated real-word OSS projects, the best T-BERT stably outperformed the VSM model with average improvements of 60.31\% measured using Mean Average Precision (MAP). RNN severely underper-formed on these projects due to insufficient training data, while T-BERT overcame this problem by using pretrained language models and transfer learning.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {324–335},
numpages = {12},
keywords = {Software traceability, deep learning, language models},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3341105.3373848,
author = {de Kok, Sophie and Frasincar, Flavius},
title = {Using word embeddings for ontology-driven aspect-based sentiment analysis},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373848},
doi = {10.1145/3341105.3373848},
abstract = {Nowadays, the Web is the main platform to gather information. The growing amount of freely available unstructured data has increased the interest in sentiment analysis, where the goal is to extract opinions from text. In this paper we focus on review-level aspect-based sentiment analysis, where we predict the sentiment of a certain aspect in a review. We propose a two-stage sentiment analysis algorithm. In the first stage a domain ontology is utilized to predict the sentiment. If the domain ontology stage is inconclusive, a back-up stage based on an SVM bag-of-words model is employed. Furthermore, the use of word embeddings to improve the domain ontology coverage in the first stage by finding semantically similar words is investigated. We find that the two-stage approach significantly outperforms two baseline methods and achieves competitive results for the SemEval-2016 data. Furthermore, by not employing the back-up stage, we still perform significantly better than the baselines. Lastly, we find that employing word embeddings improves the accuracy when the domain ontology size is relatively small.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {834–842},
numpages = {9},
keywords = {aspect-based sentiment analysis, domain ontology, review-level sentiment analysis, word embeddings},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3678610.3678613,
author = {Bekmanova, Gulmira and Sairanbekova, Ayaulym and Ongarbayev, Yerkin and Mukanova, Assel and Zulkhazhav, Altanbek and Omarbekova, Assel and Ukenova, Aru},
title = {Intelligent question-answering system based on the public political discourse knowledge},
year = {2024},
isbn = {9798400716799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678610.3678613},
doi = {10.1145/3678610.3678613},
abstract = {The article describes the implementation of a question-answering system based on knowledge of public political discourse. An ontology of the subject area was developed, a program was developed to publish an answer to a question on the topic of political discourse in the Kazakh language.},
booktitle = {Proceedings of the 2024 10th International Conference on E-Society, e-Learning and e-Technologies (ICSLT)},
pages = {14–19},
numpages = {6},
keywords = {Artificial intelligence, discourse, formalization, python, owlready, sparql, OWL, knowledge base, ontology},
location = {
},
series = {ICSLT '24}
}

@article{10.1145/3715011,
author = {Doerr, Martin},
title = {Identifiable Individuals and Reality: Describing the Past by&nbsp;Formal&nbsp;Propositions},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3715011},
doi = {10.1145/3715011},
abstract = {Data of historical and ideographic sciences, such as cultural heritage studies, geography, geological evolution, biodiversity, but also experimental data of nomothetic natural sciences, are increasingly documented and published in information systems compatible with predicate-logic that refer to things in reality by unique identifiers (or “keys” most likely to be unique in some context). This can only work as a method of sharing and integrating knowledge beyond the spatial and temporal context of local projects, if the referred features or phenomena of reality are distinct and can diachronically be identified by independent observers in the same way and without a clarifying dialogue between them. In this article, we argue that only a smaller part of the features in our environment are sufficiently distinct over a useful time-span in order to form such “identifiable individuals”. Ontological categories should each provide specific criteria for the so-called ontological individuation, i.e., about how parts of reality can be subdivided into identifiable individuals that are useful for modelling their behaviour and interactions in reality for answering specific scientific questions, by obeying evidential constraints as a result of applying these criteria in observations. We motivate by several examples that there are always cases in which the individuality of such an instance may be undecidable basically within all such ontological categories and that all ontological categories are more or less effective approximations of reality. We argue that effective knowledge sharing by information systems using formal ontologies is only possible if these limitations of applicability and precision to real world phenomena are well understood and taken into account.},
journal = {J. Comput. Cult. Herit.},
month = may,
articleno = {27},
numpages = {32},
keywords = {Historical Sciences, Historical Data, Scientific realism, Information integration, Ontological Individuation, Epistemic Individuation}
}

@inproceedings{10.1145/3368756.3369090,
author = {Ihab, Moudhich and Soumaya, Loukili and Mohamed, Bahra and Haytam, Hmami and Abdelhadi, Fennan},
title = {Ontology-based sentiment analysis and community detection on social media: application to Brexit},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369090},
doi = {10.1145/3368756.3369090},
abstract = {Sentiment Analysis and Community Detection are two of the main methods used to analyze and comprehend human interactions on social media. These domains expanded immensely with the rise of social media, as it provided a free and ever-increasing quantity of data. Domain ontologies are of great assistance in collecting specific data, as it describes the domain's features and their existing relationships. Therefore, we utilize them in collecting subject-specific data on social media. This paper describes the framework we've designed in order to understand, in depth, the impact of a subject on social media users, and also to evaluate the difference between the Lexicon Approach and the Machine Learning Approach, by assessing the strengths and weaknesses of each. This framework also aims to deeply understand the connections that exist between users, depending on their point of view on a particular subject. The resulting framework not only analyzes textual data (by taking into account the negation and sentence POS tags), but also visual one, such as images. In order to test the framework, we chose to analyze the Brexit phenomenon by collecting ontology-based data from Twitter and Reddit, and it had some promising results.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {103},
numpages = {7},
keywords = {classification, community detection, lexicon, machine learning, ontology, opinion mining, sentiment analysis, social media analysis},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@inproceedings{10.1145/3419604.3419797,
author = {Khannat, Aicha and Sbai, Hanae and Kjiri, Laila},
title = {Towards Mining Semantically Enriched Configurable Process Models},
year = {2020},
isbn = {9781450377331},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419604.3419797},
doi = {10.1145/3419604.3419797},
abstract = {Providing configurable process model with high quality is a primary objective to derive process variants with better accuracy and facilitate process model reuse. For this purpose, many research works have been interested in configurable process mining techniques to discover and configure processes from event logs. Moreover, to use the knowledge captured by event logs when mining processes, the concept of semantic process mining is introduced. It allows for combining semantic technologies with process mining. Despite the diversity of works in mining and customizing configurable process models, the application of these techniques is still limited to use semantics in minimizing the complexity of discovered processes. However, it seems to be pertinent to discover semantically enriched configurable process models directly from event logs. Consequently, this can facilitate using semantic in configuring, verifying conformance or enhancing discovered configurable processes. In this paper, we present a comparative study of existing works that focus on mining configurable process models with respect to semantic technologies. Our aim is to propose a new framework to automatically discover semantically enriched configurable processes.},
booktitle = {Proceedings of the 13th International Conference on Intelligent Systems: Theories and Applications},
articleno = {4},
numpages = {6},
keywords = {Configurable Process Model, Ontology, Process Mining, Semantic Technologies, Variability},
location = {Rabat, Morocco},
series = {SITA'20}
}

@inproceedings{10.1145/3371647.3371666,
author = {Olabe, Juan Carlos and Basogain, Xabier and Olabe, Miguel \'{A}ngel},
title = {Modern Education with a Computational Model of the Mind},
year = {2020},
isbn = {9781450372251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371647.3371666},
doi = {10.1145/3371647.3371666},
abstract = {We are witnessing a great effort on the part of the educational systems of the world in modernizing the curricular content of primary and secondary schools. One example of these educational initiatives is the effort of integrating aspects of engineering and technology with the existing core subjects of sciences and mathematic in K-12 education. These efforts are often labeled as STEM or STEAM (Science, Technology, Engineering, Mathematics, and Arts.) These subjects, studied in an integral form, are considered essential in the education of the citizens of a modern society. A common obstacle encountered in the implementation of these projects or initiatives is the lack of consensus on the specific topics to be included in the curriculum, the pedagogical methodology selected for the classroom, and the means, computer-based or otherwise, to be used by the teachers and the students. Often the lack of consensus among the different constituencies in charge of these projects finds its roots in the different assumptions made by their participants. This paper addresses one of the most acute set of differences present in these projects: the teaching methods used in class, knowing the resources and limitations of the human mind. In the last few decades we have learned much of how the mind works; what tasks are intrinsically easy or difficult for the human mind. Also, with the extensive access to computing power in our society, it is important to determine if a traditional task was studied in school for its practical use or for its value in developing the potential qualities of the mind. In this paper we will use the word computation in its traditional meaning of symbol manipulation. In that sense, all processes of thinking, solving problems, and endeavors of creation are processes of symbol manipulation, or computation. In this paper we present a computational model of the mind in order to provide a standard reference that will help in finding answers to questions such as: when is a task complex, what are the cognitive capabilities and limitations of the mind, what teaching methodologies are optimal, and why.},
booktitle = {Proceedings of the 2019 3rd International Conference on Education and E-Learning},
pages = {41–45},
numpages = {5},
keywords = {Complex Systems, Computational Theory of the Mind, Computing, Model of the Mind, New approaches to problem solving, Ontology, STEM Education},
location = {Barcelona, Spain},
series = {ICEEL '19}
}

@inproceedings{10.1145/3640310.3674079,
author = {Sultan, Bastien and Apvrille, Ludovic},
title = {AI-Driven Consistency of SysML Diagrams},
year = {2024},
isbn = {9798400705045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640310.3674079},
doi = {10.1145/3640310.3674079},
abstract = {Graphical modeling languages, expected to simplify systems analysis and design, present a challenge in maintaining consistency across their varied views. Traditional rule-based methods for ensuring consistency in languages like UML often fall short in addressing complex semantic dimensions. Moreover, the integration of Large Language Models (LLMs) into Model Driven Engineering (MDE) introduces additional consistency challenges, as LLM's limited output contexts requires the integration of responses. This paper presents a new framework that automates the detection and correction of inconsistencies across different views, leveraging formally defined rules and incorporating OpenAI's GPT, as implemented in TTool. Focusing on the consistency between use case and block diagrams, the framework is evaluated through its application to three case studies, highlighting its potential to significantly enhance consistency management in graphical modeling.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {149–159},
numpages = {11},
location = {Linz, Austria},
series = {MODELS '24}
}

@inproceedings{10.5555/3522802.3522925,
author = {Herding, Raphael and M\"{o}nch, Lars and Ehm, Hans},
title = {Design and application of an ontology for demand fulfillment in semiconductor supply chains},
year = {2022},
publisher = {IEEE Press},
abstract = {Ensuring interoperability of different information systems for planning and control is a challenging task in semiconductor supply chains. This is partially caused by the sheer size of the involved production facilities and the supply chains in the semiconductor domain, the permanent appearance of uncertainty, and the rapid technological changes which lead to sophisticated planning and control systems in this domain. Ontologies are a promising approach to support interoperability among such systems. Demand fulfillment is an important function in semiconductor supply chains. However, at the same time, it is a planning function that is not very well understood. In the present paper, a domain- and task ontology for demand fulfillment is designed based on a domain analysis. The usage of the proposed ontology is illustrated by means of an example.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {152},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3508072.3508117,
author = {Hammoudeh, Mohammad and Adebisi, Bamidele and Unal, Devrim and Laouid, Abdelkader},
title = {Bringing Coordination Languages Back to the Future Using Blockchain Smart Contracts},
year = {2022},
isbn = {9781450387347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508072.3508117},
doi = {10.1145/3508072.3508117},
abstract = {This paper presents a blockchain extension of the run-time Sensing as a Service SOA (3SOA) approach presented in&nbsp;[5]. 3SOA defines a practical approach for implementing service-oriented Internet of Things (IoT) using coordination languages to integrate and program individual IoT objects to compose into full IoT system. We believe that the modularity, reuse, interoperability and portability of this model has much to offer, but that there exist some challenges in overcoming the performance issues inherent in the approach, and extending the range of applications to which it is suited. We are particularly interested in applying the coordination languages to decentralized systems. To this end, blockchain smart contracts are proposed to offer a decentralized trustable method to automatically verify compliance with pre-defined conditions before executing a transaction involving multiple parties. To validate our proposal, we demonstrate a healthcare functional prototypes as a proof of concept.},
booktitle = {Proceedings of the 5th International Conference on Future Networks and Distributed Systems},
pages = {299–304},
numpages = {6},
keywords = {Internet of Things, blockchain, service-oriented, smart contracts},
location = {Dubai, United Arab Emirates},
series = {ICFNDS '21}
}

@inproceedings{10.1145/3580305.3599199,
author = {Gaur, Manas and Tsamoura, Efthymia and Sreedharan, Sarath and Mittal, Sudip},
title = {KiL 2023 : 3rd International Workshop on Knowledge-infused Learning},
year = {2023},
isbn = {9798400701030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3580305.3599199},
doi = {10.1145/3580305.3599199},
abstract = {Recent prolific advances in artificial intelligence through the incorporation of domain knowledge have constituted a new paradigm for AI and data mining communities. For example, the human feedback-based language generation in ChatGPT (a large language model (LLM)), the use of Protein Bank in DeepMind's AlphaFold, and the use of 23 rules of safety in DeepMind's Sparrow have demonstrated the success of teaming human knowledge and AI. In addition, the knowledge retrieval-guided language modeling methods have strengthened the association between knowledge and AI. However, translating research methods and resources into practice presents a new challenge for the machine learning and data/knowledge mining communities. For example, in DARPA's Explainable AI seminar, the need for explainable contextual adaptation is seen as the 3rd phase of AI, facilitating the interplay between data and knowledge for explainability, safety, and, eventually, trust. However, policymakers and practitioners assert serious usability and privacy concerns that constrain adoption, notably in high-consequence domains, such as cybersecurity, healthcare, and other social good domains. In addition, limitations in output quality, measurement, and interactive ability, including both the provision of explanations and the acceptance of user preferences, result in low adoption rates in such domains. This workshop aims to accelerate our pace towards creating innovative methods for integrating knowledge into contemporary AI and data science methods and develop metrics for assessing performance in various applications.},
booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {5857–5858},
numpages = {2},
keywords = {explainable ai, games, knowledge-infused learning, language models, neurosymbolic ai, programming languages, safe ai},
location = {Long Beach, CA, USA},
series = {KDD '23}
}

@inproceedings{10.1145/3635059.3635062,
author = {Giarelis, Nikolaos and Mastrokostas, Charalampos and Siachos, Ilias and Karacapilidis, Nikos},
title = {A Review of Greek NLP Technologies for Chatbot Development},
year = {2024},
isbn = {9798400716263},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635059.3635062},
doi = {10.1145/3635059.3635062},
abstract = {The advent of Generative AI has certainly boosted the interest in developing innovative chatbot applications. Despite a vast amount of machine learning (ML) and natural language processing (NLP) research and English language resources that greatly improve chatbot technology, the corresponding research and resources for the Greek language are limited. The contribution of this paper is twofold: (i) it reports on the state-of-the-art research in Greek NLP, as far as language resources, embeddings-based techniques, deep learning models, and existing chatbot applications are concerned; (ii) it offers a set of insights on current NLP models and chatbot implementation methodologies, and outlines a set of pending issues and future research directions.},
booktitle = {Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics},
pages = {15–20},
numpages = {6},
keywords = {Deep Learning, Greek Language, Large Language Models, Review, Text Classification, Text Summarization, Word Embeddings},
location = {Lamia, Greece},
series = {PCI '23}
}

@inproceedings{10.1145/3462757.3466149,
author = {Savelka, Jaromir and Westermann, Hannes and Benyekhlef, Karim and Alexander, Charlotte S. and Grant, Jayla C. and Amariles, David Restrepo and Hamdani, Rajaa El and Mee\`{u}s, S\'{e}bastien and Troussel, Aurore and Araszkiewicz, Micha\l{} and Ashley, Kevin D. and Ashley, Alexandra and Branting, Karl and Falduti, Mattia and Grabmair, Matthias and Hara\v{s}ta, Jakub and Novotn\'{a}, Tereza and Tippett, Elizabeth and Johnson, Shiwanni},
title = {Lex Rosetta: transfer of predictive models across languages, jurisdictions, and legal domains},
year = {2021},
isbn = {9781450385268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462757.3466149},
doi = {10.1145/3462757.3466149},
abstract = {In this paper, we examine the use of multi-lingual sentence embeddings to transfer predictive models for functional segmentation of adjudicatory decisions across jurisdictions, legal systems (common and civil law), languages, and domains (i.e. contexts). Mechanisms for utilizing linguistic resources outside of their original context have significant potential benefits in AI \&amp; Law because differences between legal systems, languages, or traditions often block wider adoption of research outcomes. We analyze the use of Language-Agnostic Sentence Representations in sequence labeling models using Gated Recurrent Units (GRUs) that are transferable across languages. To investigate transfer between different contexts we developed an annotation scheme for functional segmentation of adjudicatory decisions. We found that models generalize beyond the contexts on which they were trained (e.g., a model trained on administrative decisions from the US can be applied to criminal law decisions from Italy). Further, we found that training the models on multiple contexts increases robustness and improves overall performance when evaluating on previously unseen contexts. Finally, we found that pooling the training data from all the contexts enhances the models' in-context performance.},
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
pages = {129–138},
numpages = {10},
keywords = {adjudicatory decisions, annotation, document segmentation, domain adaptation, multi-lingual sentence embeddings, transfer learning},
location = {S\~{a}o Paulo, Brazil},
series = {ICAIL '21}
}

@inproceedings{10.1145/3625007.3627514,
author = {Kim, Seonhyeong and Khan, Irshad and Kwon, Young-Woo},
title = {Realtime Disaster Detection Through GNN Models Using Disaster Knowledge Graphs},
year = {2024},
isbn = {9798400704093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625007.3627514},
doi = {10.1145/3625007.3627514},
abstract = {In the context of the increasing scale and complexity of disasters caused by rapid climate change, a comprehensive understanding of disaster big data is essential for effective detection and response. The disaster knowledge graph proposed in this paper fills this gap by capturing the connections between various disaster-related data sources and their potential for growth across heterogeneous datasets. We generate time-series disaster graphs every minute using SNS data (e.g., Twitter) and public data, specifically focusing on disasters. Then, we create disaster knowledge graphs to represent the relationships between various data sources and try to predict their potential developments. We label and annotate knowledge graphs and then detect sudden changes in time-series disaster knowledge graphs for disaster detection. To that end, we assess the effectiveness of three state-of-the-art GNN models for graph-based event classification using Graph Convolutional Network (GCN), Graph Attention Network (GAT), and SageConv. In addition, we evaluate a simple clustering model, K-means, for comparison. Our experiments show promising results with approximately 87\% precision in detecting disaster events using structural data and connectivity patterns within disaster graphs. Finally, we measure the result of disaster detection time with an unseen dataset, showing positive results that about 70\% detect a disaster in less than 3 minutes. To comprehensively analyze real-time social media data and understand the patterns of disaster to enhance disaster management and response strategies, our approach combines the strength of GNNs with a designed disaster knowledge graph.},
booktitle = {Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {221–228},
numpages = {8},
keywords = {knowledge graphs, graph neural networks, disaster detection},
location = {Kusadasi, Turkiye},
series = {ASONAM '23}
}

@inproceedings{10.1145/3707292.3707389,
author = {Li, Yanjun and Yang, Ruiting and Guo, Donghao and Song, Yu},
title = {Research on the Construction of Digital Knowledge Graphs Based on Resources of National First-Class Undergraduate Programs},
year = {2025},
isbn = {9798400707308},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3707292.3707389},
doi = {10.1145/3707292.3707389},
abstract = {[Purpose/Significance]: The digitalization of education is an essential path to advancing higher education. The construction of knowledge graphs is a key approach to achieving the digitalization and intelligence of education. [Method/Process]: This paper leverages the rich video resources of existing national first-class undergraduate programs and, based on the teaching orientations of different universities, independently designs customized ontologies and extraction principles. These are then integrated into the LLM knowledge graph builder to ensure the hierarchical structure of the overall course framework. The course video content is transformed into text form, and large language models (LLMS) and word segmentation tools are used for core content extraction, text cleaning, and lexical analysis. The structured text is then converted into SPO (Subject-Predicate-Object) triplets database. [Results/Conclusions]: Finally, the database is imported into the LLM knowledge graph builder, which is pre-configured with extraction rules. It will automatically generate the knowledge graph. After the text is imported into the LLM knowledge graph builder, it will be manually checked to ensure it better meets the actual needs of the students. [Innovation/Limitations]: The research team plans to apply the knowledge graph to train a specialized knowledge-based Q&amp;A assistant. This will support students' understanding and self-assessment of knowledge points in an online learning community. Student feedback will be used to improve and enrich the knowledge graph. Compared to existing methods, this approach better aligns with the constantly evolving digital teaching resources available online, offering more comprehensive and higher-level automation.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Intelligent Information Processing},
pages = {353–359},
numpages = {7},
keywords = {Knowledge graph, course resources, intelligent Q&amp;A, ontology construction, personalized learning},
location = {
},
series = {AIIIP '24}
}

@article{10.1145/3314948,
author = {Mohammadi, Majid and Hofman, Wout and Tan, Yao-Hua},
title = {Simulated Annealing-based Ontology Matching},
year = {2019},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3314948},
doi = {10.1145/3314948},
abstract = {Ontology alignment is a fundamental task to reconcile the heterogeneity among various information systems using distinct information sources. The evolutionary algorithms (EAs) have been already considered as the primary strategy to develop an ontology alignment system. However, such systems have two significant drawbacks: they either need a ground truth that is often unavailable, or they utilize the population-based EAs in a way that they require massive computation and memory. This article presents a new ontology alignment system, called SANOM, which uses the well-known simulated annealing as the principal technique to find the mappings between two given ontologies while no ground truth is available. In contrast to population-based EAs, the simulated annealing need not generate populations, which makes it significantly swift and memory-efficient for the ontology alignment problem. This article models the ontology alignment problem as optimizing the fitness of a state whose optimum is obtained by using the simulated annealing. A complex fitness function is developed that takes advantage of various similarity metrics including string, linguistic, and structural similarities. A randomized warm initialization is specially tailored for the simulated annealing to expedite its convergence. The experiments illustrate that SANOM is competitive with the state-of-the-art and is significantly superior to other EA-based systems.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {3},
numpages = {24},
keywords = {OAEI, Ontology alignment, SANOM, simulated annealing}
}

@inproceedings{10.1145/3368756.3369075,
author = {Hamim, Touria and Benabbou, Faouzia and Sael, Nawal},
title = {Student profile modeling: an overview model},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369075},
doi = {10.1145/3368756.3369075},
abstract = {Today, provide a customized, personalized and adaptive service is a big challenge that improve the services and functionalities of systems. The knowledge of the user profile is a vital phase in this process. Profile modeling is an important field that aims to give an abstract representation of some aspects related to the user features. In the educational field, student profile modeling can be a support for decision-making at different levels like learning, orientation, and recommendation. It can mainly offer the most exact description of students in order to: be able to act in case of problems such as failure, drop out; offer students the most appropriate orientation and recommendation; and define the most adaptive learning resources depending on their profile. In this paper, we present an analytical and statistical study on student profile modeling to propose a detailed description of the student's profile obtained with different techniques in different contexts of the educational field. We aim to extract the different categories of student's features that can be used alone or combined for decision making in different fields.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {88},
numpages = {9},
keywords = {e-portfolio, learner profile, machine learning, ontologies, profile modeling},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@inproceedings{10.1145/3498851.3498942,
author = {Liu, Li and Li, Xuebo},
title = {Research and Construction of Classical Formulas Knowledge Graph Based on Ontology},
year = {2022},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3498942},
doi = {10.1145/3498851.3498942},
abstract = {Classical formula is an important part and basis of traditional Chinese prescriptions. The effective storage and expression of classical formula knowledge in ancient books and modern documents is a key issue for studying and using classical formulas. This paper standardized and normalized the contents of the classic formula in "Treatise on Exogenous Febrile Disease" and "The Synopsis of the Golden Chamber", collected and organized classical formulas related documents from China National Knowledge Infrastructure (CNKI). Ontology construction tool Prot\'{e}g\'{e} is used to establish the domain ontology of classical formulas, and Neo4j graph database is used to construct the knowledge graph of ancient books and modern CNKI documents. 296 ancient classical formula items and 11175 modern documents from CNKI are collected, normalized and stored in database as the data source of this research. On the basis of these work, constructed the ontology and knowledge graph of classical formulas and built an application system for knowledge query. Constructing knowledge graph from top to bottom based on ontology can express and visualize the related knowledge of classical formulas accurately and efficiently. The construction strategy mentioned in this paper has got a good result and showed great potential in traditional Chinese medicine knowledge domain.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {140–143},
numpages = {4},
keywords = {Classical Formula, Knowledge Graph, TCM},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@inproceedings{10.5555/3466184.3466211,
author = {Jose, Justin and Singh, Divye and Patel, Amit and Hayatnagarkar, Harshal G.},
title = {Simulating re-configurable multi-rovers for planetary exploration using behavior-based ontology},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {For planetary explorations, the space agencies have usually sent single robotic rovers to complete missions. An alternative approach is to send multiple rovers, which can insure against failure of one or more rovers. Planning for a multi-rover mission has its own challenges, and simulations can aid in identifying and addressing such challenges. In this paper, we present an ontology-based approach to simulate a multi-rover planetary exploration mission, with a focus on resilience, adaptation, heterogeneity, and reconfigurability. We present an ontology that describes multiple rovers along with an inventory of their parts shipped with a lander. Our approach shows that having the ontology-based simulations help in complex scenarios such as to loan parts from inventory, and salvaging a damaged rover for good parts.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {254–265},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3234698.3234761,
author = {Hemam, Mounir and Djezzar, Meriem and Seghir, Zianou Ahmed},
title = {Multi-Viewpoints Ontological Knowledge Representation: A Fuzzy Description Logics Based Approach},
year = {2018},
isbn = {9781450363921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234698.3234761},
doi = {10.1145/3234698.3234761},
abstract = {Description Logics (DLs) play a key role in the design of ontologies. An ontology is a formal description of important concepts in a particular domain. In practice, a set of a specific-domain applications use different representations of the same real world entity due to various viewpoints, context, and specific interest. In this paper, we are interested in the problem of representing an ontology in a heterogeneous domain by taking into consideration different viewpoints and different terminologies of various users, groups or even communities in the organisation. This type of ontology, called multi-viewpoints ontology, confers to the same universe of discourse, several partial descriptions, where each one is relative to a particular viewpoint. Moreover, these partial descriptions share at global level, fuzzy ontological elements allowing the representation of vague/imprecise knowledge between the various viewpoints. So, our goal is to propose a fuzzy multi-viewpoints ontology Web language, which is an extension of OWL-DL language, to allow the multi-viewpoints ontologies representation.},
booktitle = {Proceedings of the Fourth International Conference on Engineering \&amp; MIS 2018},
articleno = {63},
numpages = {6},
keywords = {Description Logics, Fuzziness, Knowledge Engineering, Ontology, Semantic Web, Viewpoint},
location = {Istanbul, Turkey},
series = {ICEMIS '18}
}

@inproceedings{10.1109/MODELS-C.2019.00095,
author = {Reynolds, Owen J.},
title = {Towards model-driven self-explanation for autonomous decision-making systems},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00095},
doi = {10.1109/MODELS-C.2019.00095},
abstract = {The ability for systems to make decisions by themselves is increasing with advances in different areas of AI such as machine learning and optimisation techniques for autonomous systems among other. Humans are handing over more decisions to systems that provide no explanations for their judgements unless they are enabled explicitly in their design. Trust based on a program being well written and tested correctly is not appropriate for AI-based autonomous systems. Unlike traditional software, this new software increasingly exhibit emergent behaviours making it unpredictable due to unexpected situations. Self-explanation is sometimes implemented, tracking decisions to give explanations to users. A more consistent, proven approach to self-explanation would be needed for making trustable systems.The paper proposes a research agenda to define an architecture to enable self-explanation for autonomous decision-making systems. The approach will be model-driven to facilitate reuse, the rapid development of tools and suitable abstractions for demonstrating concepts. The architecture will be informed by existing research in provenance ontology and model version research. The evaluation of the architecture is expected to be done using two case studies. The first will implement self-explanation as a primary concern in the building of a system. The second case will attempt to fit self-explanation to an existing system.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {624–628},
numpages = {5},
keywords = {autonomous, decision-making, model-driven, self-explanation},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1145/3587259.3627555,
author = {Verkijk, Stella and Roothaert, Ritten and Pernisch, Romana and Schlobach, Stefan},
title = {Do you catch my drift? On the usage of embedding methods to measure concept shift in knowledge graphs},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627555},
doi = {10.1145/3587259.3627555},
abstract = {Automatically detecting and measuring differences between evolving Knowledge Graphs (KGs) has been a topic of investigation for years. With the rising popularity of embedding methods, we investigate the possibility of using embeddings to detect Concept Shift in evolving KGs. Specifically, we go deeper into the usage of nearest neighbour set comparison as the basis for a similarity measure, and show why this approach is conceptually problematic. As an alternative, we explore the possibility of using clustering methods. This paper serves to (i) inform the community about the challenges that arise when using KG embeddings for the comparison of different versions of a KG specifically, (ii) investigate how this is supported by theories on knowledge representation and semantic representation in NLP and (iii) take the first steps into the direction of valuable representation of semantics within KGs for comparison.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {70–74},
numpages = {5},
keywords = {Concept Shift, Knowledge Graph Embeddings, NLP, Semantics},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3419804.3420262,
author = {Reynolds, Owen and Garc\'{\i}a-Dom\'{\i}nguez, Antonio and Bencomo, Nelly},
title = {Towards automated provenance collection for runtime models to record system history},
year = {2020},
isbn = {9781450381406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419804.3420262},
doi = {10.1145/3419804.3420262},
abstract = {In highly dynamic environments, systems are expected to make decisions on the fly based on their observations that are bound to be partial. As such, the reasons for its runtime behaviour may be difficult to understand. In these cases, accountability is crucial, and decisions by the system need to be traceable. Logging is essential to support explanations of behaviour, but it poses challenges. Concerns about analysing massive logs have motivated the introduction of structured logging, however, knowing what to log and which details to include is still a challenge. Structured logs still do not necessarily relate events to each other, or indicate time intervals. We argue that logging changes to a runtime model in a provenance graph can mitigate some of these problems. The runtime model keeps only relevant details, therefore reducing the volume of the logs, while the provenance graph records causal connections between the changes and the activities performed by the agents in the system that have introduced them. In this paper, we demonstrate a first version towards a reusable infrastructure for the automated construction of such a provenance graph. We apply it to a multithreaded traffic simulation case study, with multiple concurrent agents managing different parts of the simulation. We show how the provenance graphs can support validating the system behaviour, and how a seeded fault is reflected in the provenance graphs.},
booktitle = {Proceedings of the 12th System Analysis and Modelling Conference},
pages = {12–21},
numpages = {10},
keywords = {Provenance, multi-threading, runtime models, self-adaptation, self-explanation},
location = {Virtual Event, Canada},
series = {SAM '20}
}

@article{10.1007/s00165-021-00555-2,
author = {St\"{u}nkel, Patrick and K\"{o}nig, Harald and Lamo, Yngve and Rutle, Adrian},
title = {Comprehensive Systems: A formal foundation for Multi-Model Consistency Management},
year = {2021},
issue_date = {Dec 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {33},
number = {6},
issn = {0934-5043},
url = {https://doi.org/10.1007/s00165-021-00555-2},
doi = {10.1007/s00165-021-00555-2},
abstract = {Model management is a central activity in Software Engineering. The most challenging aspect of model management is to keep inter-related models consistent with each other while they evolve. As a consequence, there is a lot of scientific activity in this area, which has produced an extensive body of knowledge, methods, results and tools. The majority of these approaches, however, are limited to binary inter-model relations; i.e. the synchronisation of exactly two models. Yet, not every multi-ary relation can be factored into a family of  binary relations. In this paper, we propose and investigate a novel comprehensive system construction, which is able to represent multi-ary relations among multiple models in an integrated manner and thus serves as a formal foundation for artefacts used in consistency management activities involving multiple models. The construction is based on the definition of partial commonalities among a set of models using the same language, which is used to denote the (local) models. The main theoretical results of this paper are proofs of the facts that comprehensive systems are an admissible environment for (i) applying formal means of consistency verification (diagrammatic predicate framework), (ii) performing algebraic graph transformation (weak adhesive HLR category), and (iii) that they generalise the underlying setting of graph diagrams and triple graph grammars.},
journal = {Form. Asp. Comput.},
month = dec,
pages = {1067–1114},
numpages = {48},
keywords = {Multi-modelling, Inter-model consistency, Consistency verification, Consistency restoration, Model synchronisation, Multi-directional transformations (MX), Model merging, Model weaving, Graph diagrams, Triple graph grammars, Category theory, Adhesive categories}
}

@inproceedings{10.1145/3722237.3722259,
author = {Zhao, Linlin and Liu, Zhansheng and Zhao, Xuefeng},
title = {Developing a Knowledge Graph for Intelligent Structural Design Course},
year = {2025},
isbn = {9798400712692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722237.3722259},
doi = {10.1145/3722237.3722259},
abstract = {Knowledge graphs provide concept visualization and context information across many applications. However, the process of building a knowledge graph by transforming large and intricate unstructured data into a new domain presents numerous challenges. Learning from an Intelligent Structural Design (ISD) course requires comprehension of structural concepts, design techniques, AI technologies, and the tackling of multifaceted real-project problems, which require adaptive learning strategies and cognitive engagement. This study introduces a bottom-up approach to constructing a knowledge graph specifically designed for the course. Furthermore, by employing a deep learning model, a personalized learning path could be generated based on a student's submitted assignments. To evaluate the effectiveness of this novel educational paradigm, an experiment was conducted involving two groups. The results indicate that participants who used the Course Knowledge Graph (CKG) attained higher scores than those instructed through conventional teaching methods.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education},
pages = {123–128},
numpages = {6},
keywords = {Course Knowledge Graph, Intelligent Structural Design, Smart Construction, natural language processing (NLP), ontology},
location = {
},
series = {ICAIE '24}
}

@inproceedings{10.1109/MODELS-C.2019.00015,
author = {Almeida, Jo\~{a}o Paulo A. and Rutle, Adrian and Wimmer, Manuel},
title = {Preface to the 6th international workshop on multi-level modelling (MULTI 2019)},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00015},
doi = {10.1109/MODELS-C.2019.00015},
abstract = {Multi-level modeling (MLM) represents a significant extension to the traditional two-level object-oriented paradigm with the potential to dramatically improve upon the utility, reliability and complexity of models. Different from conventional approaches, they allow for an arbitrary number of classification levels and introduce other concepts that foster expressiveness, reuse and adaptability. A key aspect of the MLM paradigm is the use of entities that are simultaneously types and instances, a feature which has consequences for conceptual modeling, language engineering and for the development of model-based software systems.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {64–65},
numpages = {2},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1145/3227609.3227649,
author = {Flisar, Jernej and Podgorelec, Vili},
title = {Document Enrichment using DBPedia Ontology for Short Text Classification},
year = {2018},
isbn = {9781450354899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3227609.3227649},
doi = {10.1145/3227609.3227649},
abstract = {Every day, millions of short-texts are generated for which effective tools for organization and retrieval are required. Because of the short length of these documents and of their extremely sparse representations, the traditional text classification methods are not effective. We propose a new approach that uses DBpedia Spotlight annotation tools, to identify relevant entities in text and enrich short text documents with concepts derived from those entities, represented in DBpedia ontology. Our experiments show that the proposed document enrichment approach is beneficial for classification of short texts, and is robust with respect to concept drifts and input sources. We report experimental results in three challenging collections, using a variety of classification methods. The results show that the use of DBpedia ontology significantly improves the classification performance of classifiers in short-text classification.},
booktitle = {Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics},
articleno = {8},
numpages = {9},
keywords = {DBPedia, ontology, short text classification, text enrichment},
location = {Novi Sad, Serbia},
series = {WIMS '18}
}

@article{10.1145/3447508,
author = {Gottlob, Georg and Hernich, Andr\'{e} and Kupke, Clemens and Lukasiewicz, Thomas},
title = {Stable Model Semantics for Guarded Existential Rules and Description Logics: Decidability and Complexity},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {5},
issn = {0004-5411},
url = {https://doi.org/10.1145/3447508},
doi = {10.1145/3447508},
abstract = {This work investigates the decidability and complexity of database query answering under guarded existential rules with nonmonotonic negation according to the classical stable model semantics. In this setting, existential quantification is interpreted via Skolem functions, and the unique name assumption is adopted. As a first result, we show the decidability of answering first-order queries based on such rules by a translation into the satisfiability problem for guarded second-order formulas having the tree-model property. To obtain precise complexity results for unions of conjunctive queries, we transform the original problem in polynomial time into an intermediate problem that is easier to analyze: query answering for guarded disjunctive existential rules with stratified negation. We obtain precise bounds for the general setting and for various restricted settings. We also consider extensions of the original formalism with negative constraints, keys, and the possibility of negated atoms in queries. Finally, we show how the above results can be used to provide decidability and complexity results for a natural adaptation of the stable model semantics to description logics such as&nbsp;ELHI and the DL-Lite&nbsp;family.},
journal = {J. ACM},
month = oct,
articleno = {35},
numpages = {87},
keywords = {Answer set programming, stable models, complexity, decidability}
}

@inproceedings{10.1145/3339252.3341496,
author = {Doynikova, Elena and Fedorchenko, Andrey and Kotenko, Igor},
title = {Ontology of Metrics for Cyber Security Assessment},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3341496},
doi = {10.1145/3339252.3341496},
abstract = {Development of metrics that are valuable for assessing security and decision making is an important element of efficient counteraction to cyber threats. The paper proposes an ontology of metrics for cyber security assessment. The developed ontology is based on determining the concepts and relations between primary features of initial security data and forming a set of hierarchically interconnected security metrics. The paper describes the main classes of the proposed ontology, the revealed relations, the involved security metrics, and the used data sources. The publicly available sources of security data are analyzed to get primary security metrics. Application of the approach is shown on a case study. The main feature of the proposed ontology is representation of security metrics as separate instances of ontology. It allows using the relations between the concepts of ontology for calculating integral metrics reflecting the security state.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {52},
numpages = {8},
keywords = {Security metrics, countering cyber attacks, intelligent data analysis, ontology, security assessment, semantics},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inproceedings{10.1145/3423390.3423395,
author = {Tsoutsa, Paraskevi and Ragos, Omiros},
title = {Towards an Ontology for Teamwork Enabled Services},
year = {2020},
isbn = {9781450377324},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423390.3423395},
doi = {10.1145/3423390.3423395},
abstract = {Teamwork ability is a crucial aspect of humans, agents and other intelligent systems. This study is dedicated to the conceptual formal modeling of service teamworking by considering concepts defined within a vast range of models and theories of human and agent team development. We make the conceptual based modeling by using ontologies to define teams, roles, services and their modeling domains abstracting the most important concepts of team development whilst providing a decent level of formality and unambiguity. An ontology named TrEWSOnto is proposed as a model to assist the development process of teamwork enabled services in tandem with the representation of their composition process activity. This includes (i) the definition of concepts that are used to improve the teamwork ability of peer services, (ii) the description of the main concepts are used for the role modeling composition, and (iii) the description of situations that teamwork roles should monitor to catch potential "unhealthy" behavior happen during the service activity in order to run proactively and avoid obstacles or collisions. The result is a structure of five ontology sections together forming a representation for TeamwoRk Enabled Web (TrEW) Services and the environment they live.},
booktitle = {Proceedings of the 4th International Conference on Algorithms, Computing and Systems},
pages = {69–75},
numpages = {7},
keywords = {Composition, Microservices, Teamwork, Teamwork Ontology, Web Service},
location = {Rabat, Morocco},
series = {ICACS '20}
}

@inproceedings{10.1145/3423334.3431448,
author = {Dassereto, Federico and Rocco, Laura Di and Shaw, Shanley and Guerrini, Giovanna and Bertolotto, Michela},
title = {How to Tune Parameters in Geographical Ontologies Embedding},
year = {2020},
isbn = {9781450381604},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3423334.3431448},
doi = {10.1145/3423334.3431448},
abstract = {Many Natural Language Processing (NLP) tasks, like question answering or analyzing verbatim comments, have started to use word embeddings due to their ability to capture semantic relations between words. Recently, embeddings have been also applied in the geospatial context to represent geospatial ontologies, thanks to their ability to capture semantic similarity. In this paper, we present an analysis of a promising embedding technique particularly suitable for representing hierarchical structures. We conduct a deep technical evaluation of many parameters and their impact on the quality of the representation.},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL Workshop on Location-Based Recommendations, Geosocial Networks, and Geoadvertising},
articleno = {2},
numpages = {9},
keywords = {Embeddings, Geographic Information Retrieval, Geotagging, Knowledge Bases},
location = {Seattle, WA, USA},
series = {LocalRec'20}
}

@inproceedings{10.1145/3726302.3730365,
author = {Jia, Pengyue and Cai, Qingpeng and Zhao, Xiangyu and Pan, Ling and Xin, Xin and Huang, Jin and Zhang, Weinan and Zhao, Li and Yin, Dawei and Yang, Grace Hui},
title = {AgentIR: 2nd Workshop on Agent-based Information Retrieval},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730365},
doi = {10.1145/3726302.3730365},
abstract = {Information retrieval (IR) systems are essential in modern society, aiding users to efficiently locate relevant information through query expansion, document retrieval, ranking, and re-ranking. User feedback from ranked outputs forms a dynamic interaction loop with IR systems, which can be modeled as either one-time or sequential decision-making problems. Over the past decade, deep reinforcement learning (DRL) has emerged as a promising approach to decision-making, leveraging the high model capacity of deep learning for complex tasks. While significant research has explored the application of DRL to IR tasks, several fundamental challenges remain underexplored, including the underlying information theory in DRL settings, the limitations of reinforcement learning methods for industrial IR applications, and the simulation of DRL-based IR systems. Concurrently, the advent of large language models (LLMs) has introduced new opportunities for optimizing and simulating IR systems. Building on the success of the Agent-based IR Workshop at SIGIR 2024, we propose hosting the second Agent-based IR Workshop at SIGIR 2025. This workshop will continue to provide a platform for researchers and practitioners from academia and industry to present cutting-edge advances in DRL-based and LLM-based IR systems from an agent-based perspective. By building on the foundation laid in the first workshop, the 2025 edition aims to delve deeper into emerging research challenges, foster collaborations, and explore innovative applications. Through engaging discussions and insightful presentations, the workshop seeks to further expand the boundaries of IR research and solidify its role as a premier venue for advancing agent-based IR systems.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {4180–4183},
numpages = {4},
keywords = {agent-based information retrieval, drl, llm},
location = {Padua, Italy},
series = {SIGIR '25}
}

@inproceedings{10.1145/3472306.3478342,
author = {Antunes, Ana and Campos, Joana and Dias, Jo\~{a}o and Santos, Pedro A. and Prada, Rui},
title = {EEG Model: Emotional Episode Generation for Social Sharing of Emotions},
year = {2021},
isbn = {9781450386197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472306.3478342},
doi = {10.1145/3472306.3478342},
abstract = {Social sharing of emotions (SSE) occurs when one communicates their feelings and reactions to a certain event in the course of a social interaction. The phenomenon is part of our social fabric and plays an important role in creating empathetic responses and establishing rapport. Intelligent social agents capable of SSE will have a mechanism to create and build long-term interaction with humans. In this paper, we present the Emotional Episode Generation (EEG) model, a fine-tuned GPT-2 model capable of generating emotional social talk regarding multiple event tuples in a human-like manner. Human evaluation results show that the model successfully translates one or more event-tuples into emotional episodes, reaching quality levels close to human performance. Furthermore, the model clearly expresses one emotion in each episode as well as humans. To train this model we used a public dataset and built upon it using event extraction techniques1.},
booktitle = {Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents},
pages = {1–8},
numpages = {8},
keywords = {emotional text generation, event-to-text generation, social agents},
location = {Virtual Event, Japan},
series = {IVA '21}
}

@inproceedings{10.1145/3487664.3487742,
author = {Mondal, Shalmoly and Hassani, Alireza and Jayaraman, Prem Prakash and Delir Haghighi, Pari and Georgakopoulos, Dimitrios},
title = {Modelling IoT Application Requirements for Benchmarking IoT Middleware Platforms},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487742},
doi = {10.1145/3487664.3487742},
abstract = {The significant advances in the Internet of Things (IoT) have led to IoT applications being widely used in various scenarios ranging from smart city, smart farming, to Industrial IoT (IIoT) solutions. With the explosion of IoT application development, IoT middleware platforms are increasingly being used for hosting such IoT applications. This has given rise to the need for developing benchmarking solutions to analyze and test the performance of different middleware platforms that host these IoT applications. To develop such benchmarks, there are a number of key components that are needed. One of these components is an IoT dataset. To generate such datasets, representing IoT application requirements in a general and formal way is important. In this paper, we propose a framework to model the IoT Applications Requirements and enable Data Generation(ARDG-IoT). The framework supports a formal way to capture IoT application requirements and use these requirements to generate IoT data that can be used to create benchmarks for different IoT middleware platforms. ARDG-IoT consists of our proposed model, IoTSySML, which captures the application requirements, and an IoT data simulator tool, which is used to generate IoT data. We present an evaluation of the framework using a real world Industrial IoT application case study.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {553–561},
numpages = {9},
keywords = {Benchmarking, IoT Application Requirements, IoT Middleware, Requirements Engineering, Requirements Modelling},
location = {Linz, Austria},
series = {iiWAS2021}
}

@proceedings{10.1145/3571788,
title = {VaMoS '23: Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Odense, Denmark}
}

@article{10.1145/3484828,
author = {Amaral, Gabriel and Piscopo, Alessandro and Kaffee, Lucie-aim\'{e}e and Rodrigues, Odinaldo and Simperl, Elena},
title = {Assessing the Quality of Sources in Wikidata Across Languages: A Hybrid Approach},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3484828},
doi = {10.1145/3484828},
abstract = {Wikidata is one of the most important sources of structured data on the web, built by a worldwide community of volunteers. As a secondary source, its contents must be backed by credible references; this is particularly important, as Wikidata explicitly encourages editors to add claims for which there is no broad consensus, as long as they are corroborated by references. Nevertheless, despite this essential link between content and references, Wikidata's ability to systematically assess and assure the quality of its references remains limited. To this end, we carry out a mixed-methods study to determine the relevance, ease of access, and authoritativeness of Wikidata references, at scale and in different languages, using online crowdsourcing, descriptive statistics, and machine learning. Building on previous work of ours, we run a series of microtasks experiments to evaluate a large corpus of references, sampled from Wikidata triples with labels in several languages. We use a consolidated, curated version of the crowdsourced assessments to train several machine learning models to scale up the analysis to the whole of Wikidata. The findings help us ascertain the quality of references in Wikidata and identify common challenges in defining and capturing the quality of user-generated multilingual structured data on the web. We also discuss ongoing editorial practices, which could encourage the use of higher-quality references in a more immediate way. All data and code used in the study are available on GitHub for feedback and further improvement and deployment by the research community.},
journal = {J. Data and Information Quality},
month = oct,
articleno = {23},
numpages = {35},
keywords = {Wikidata, crowdsourcing, verifiability, data quality, knowledge graphs}
}

@inproceedings{10.1145/3627673.3679228,
author = {Dew, Rebecca and Li, Mingzhao and Baratha Raj, Sandya},
title = {A Skill Proficiency Framework for Workforce Learning and Development},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679228},
doi = {10.1145/3627673.3679228},
abstract = {Understanding the skills and proficiency levels required for various roles is crucial for effective workforce planning, learning and development. In this paper, we propose a robust skill proficiency modeling framework that offers a structured method to help describe, assess and develop proficiency in key skills, facilitating individuals' career pathways and aiding organizations in talent management and adaptability. We first design a skill proficiency description pipeline, which generates statements describing the requirements at each proficiency level of a skill. Following this, we build a skill proficiency by occupation model using large-scale job ad data to help organizations and individuals understand the skill proficiency requirements for different roles. Finally,we design a visual analytics system, based on a real-world career pathway scenario, to demonstrate the practical usefulness and effectiveness of our framework. A demo video is available at www.dropbox.com/scl/fi/nd0f3vi03n12g4y0sluaw/cikm24_demo.mp4?rlkey=55vya144q5ftai1uqqaubr5u5.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5210–5214},
numpages = {5},
keywords = {GPT, large language model, skill proficiency, visual analytics},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3711896.3737011,
author = {Abdullahi, Tassallah and Gemou, Ioanna and Nayak, Nihal V. and Murtaza, Ghulam and Bach, Stephen H. and Eickhoff, Carsten and Singh, Ritambhara},
title = {K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction},
year = {2025},
isbn = {9798400714542},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3711896.3737011},
doi = {10.1145/3711896.3737011},
abstract = {Biomedical knowledge graphs (KGs) encode rich, structured information critical for drug discovery tasks, but extracting meaningful insights from large-scale KGs remains challenging due to their complex structure. Existing biomedical subgraph retrieval methods are tailored for graph neural networks (GNNs), limiting compatibility with other paradigms, including large language models (LLMs). We introduce K-Paths, a model-agnostic retrieval framework that extracts structured, diverse, and biologically meaningful multi-hop paths from dense biomedical KGs. These paths enable prediction of unobserved drug-drug and drug-disease interactions, including those involving entities not seen during training, thus supporting inductive reasoning. K-Paths is training-free and employs a diversity-aware adaptation of Yen's algorithm to extract the K shortest loopless paths between entities in a query, prioritizing biologically relevant and relationally diverse connections. These paths serve as concise, interpretable reasoning chains that can be directly integrated with LLMs or GNNs to improve generalization, accuracy, and enable explainable inference. Experiments on benchmark datasets show that K-Paths improves zero-shot reasoning across state-of-the-art LLMs. For instance, Tx-Gemma 27B improves by 19.8 and 4.0 F1 points on interaction severity prediction and drug repurposing tasks, respectively. Llama 70B achieves gains of 8.5 and 6.2 points on the same tasks. K-Paths also boosts the training efficiency of EmerGNN, a state-of-the-art GNN, by reducing the KG size by 90\% while maintaining predictive performance. Beyond efficiency, K-Paths bridges the gap between KGs and LLMs, enabling scalable and explainable LLM-augmented scientific discovery. We release our code and the retrieved paths as a benchmark for inductive reasoning.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2},
pages = {5–16},
numpages = {12},
keywords = {drug discovery, explainability, gnns, inductive reasoning, knowledge graph reasoning, llms},
location = {Toronto ON, Canada},
series = {KDD '25}
}

@inproceedings{10.1145/3220547.3220551,
author = {Fan, Liju and Flood, Mark D.},
title = {An Ontology of Ownership and Control Relations for Bank Holding Companies},
year = {2018},
isbn = {9781450358835},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220547.3220551},
doi = {10.1145/3220547.3220551},
abstract = {We consider the challenges and benefits of ontologies for information management for regulatory reporting from bank holding companies (BHCs). Many BHCs, especially the largest and most complex firms, have multiple federal supervisors who oversee a diverse array of subsidiaries. This creates a federated data management problem that disperses information across many firms and regulators. We prototype an ontology for the Federal Reserve's public National Information Center (NIC) database. The NIC identifies all BHCs, their subsidiaries, and the ownership and control relationships among them. It is a basic official source on the structure of the industry. A formal ontology can capture this expert-curated knowledge in a coherent, structured format. This could assure data integrity and enable non-experts to more readily integrate and analyze data about complex organizations. We test the design and development of federated prototype ontologies in OWL/RDF to provide and integrate the NIC data with precise semantics for transparency and consistency. Our preliminary results indicate that this is feasible in practice for data search and analysis, and that the ontologies can facilitate semantic integration and improve the integrity of data and metadata.},
booktitle = {Proceedings of the Fourth International Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets},
articleno = {3},
numpages = {6},
keywords = {Financial regulation, bank holding companies, data integration, data integrity, knowledge representation, ontologies},
location = {Houston, TX, USA},
series = {DSMM'18}
}

@inproceedings{10.1145/3689187.3709607,
author = {Clear, Tony and Cajander, \r{A}sa and Clear, Alison and McDermott, Roger and Daniels, Mats and Divitini, Monica and Forshaw, Matthew and Humble, Niklas and Kasinidou, Maria and Kleanthous, Styliani and Kultur, Can and Parvini, Ghazaleh and Polash, Mohammad and Zhu, Tingting},
title = {AI Integration in the IT Professional Workplace: A Scoping Review and Interview Study with Implications for Education and Professional Competencies},
year = {2025},
isbn = {9798400712081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689187.3709607},
doi = {10.1145/3689187.3709607},
abstract = {As Artificial Intelligence (AI) continues transforming workplaces globally, particularly within the Information Technology (IT) industry, understanding its impact on IT professionals and computing curricula is crucial. This research builds on joint work from two countries, addressing concerns about AI's increasing influence in IT sector workplaces and its implications for tertiary education. The study focuses on AI technologies such as generative AI (GenAI) and large language models (LLMs). It examines how they are perceived and adopted and their effects on workplace dynamics, task allocation, and human-system interaction.IT professionals, noted as early adopters of AI, offer valuable insights into the interplay between AI and work engagement, highlighting the significant competencies required for digital workplaces. This study employs a dual-method approach, combining a systematic and multi-vocal literature review and qualitative research methods. These included a thematic analysis of a set of 47 interviews conducted between March and May of 2024 with IT professionals in two countries (New Zealand and Sweden). The research aimed to understand the implications for computing students, education curricula, and the assessment of emerging professional competencies.The literature review found insufficient evidence addressing comprehensive AI practice methodologies, highlighting the need to both develop and regulate professional competencies for effective AI integration. Key interview findings revealed diverse levels of GenAI adoption, ranging from individual experimentation to institutional integration. Participants generally expressed positive attitudes toward the technology and were actively pursuing self-learning despite some concerns. The themes emerging from the interviews included AI's role in augmenting human tasks, privacy and security concerns, productivity enhancements, legal and ethical challenges, and the evolving need for new competencies in the workplace.The study underscores the critical role of competency frameworks in guiding professional development and ensuring preparedness for an AI-driven environment. Additionally, it highlights the need for educational institutions to adapt curricula to address these emerging demands effectively},
booktitle = {2024 Working Group Reports on Innovation and Technology in Computer Science Education},
pages = {34–67},
numpages = {34},
keywords = {artificial intelligence, computing competencies, computing curricula, generative ai, it profession, large language models},
location = {Milan, Italy},
series = {ITiCSE 2024}
}

@inproceedings{10.1145/3297280.3297525,
author = {Me\v{s}kelundefined, Donatas and Frasincar, Flavius},
title = {ALDONA: a hybrid solution for sentence-level aspect-based sentiment analysis using a lexicalised domain ontology and a neural attention model},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297525},
doi = {10.1145/3297280.3297525},
abstract = {Sentences containing several different polarity aspects cause one of the main problems in sentiment analysis. Depending on an aspect, the same context words can have different effects on its sentiment value. Additionally, the polarity can be influenced by the domain-specific knowledge, showing the necessity to incorporate it into the sentiment classification. In this paper we present a hybrid solution for sentence-level aspect-based sentiment analysis using A Lexicalised Domain Ontology and Neural Attention (ALDONA) model to handle the problems mentioned above. To measure the influence of each word in a given sentence on an aspect's polarity, we introduce the bidirectional context attention mechanism. Moreover, the classification module is designed to handle the sentence's complex structure. Finally, the manually created lexicalised domain ontology (represented in OWL) is integrated to exploit the field-specific knowledge. Computational results obtained on a benchmark data set based on Web reviews have shown ALDONA's ability to outperform several state-of-the-art models and stress its contribution to aspect-based sentiment classification.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2489–2496},
numpages = {8},
keywords = {aspect-based sentiment classification, bidirectional gated neural network, hybrid model, lexicalised domain ontology},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3511808.3557365,
author = {Wang, Shuo and Zhang, Yifei and Lin, Bochen and Li, Boxun},
title = {Interpretable Emotion Analysis Based on Knowledge Graph and OCC Model},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557365},
doi = {10.1145/3511808.3557365},
abstract = {Sentiment analysis or opinion mining has been significant for information extraction from the text. At the same time, emotion psychology also proposed many appraisal theories for emotional evaluations and concrete predictions. While sentiment analysis focuses on identifying the polarity, appraisal theories of emotion can define different emotions and view emotions as process rather than states. In real life, the mechanism of emotional generations and interactions is complicated. Only plausible polarity can't provide enough explanations for the emotional mechanism. Hence an explainable model is in demand during emotion inference and dynamical analysis. In this paper, an analysis framework is constructed for interpreting casual association based on the emotional logic. Knowledge graph is introduced into the appraisal theories for inferring the emotions and predicting the action tendency. The emotion knowledge graph levels: concept level and case level. The concept level can be built manually as an abstract based on the appraisal model of Ortony, Clore \&amp; Collins (OCC model). The inference and predictions can be implemented at this level. The case level includes entities, objects, events and cognitive relations between them that extract from the text through the modular functions. The elements in the case level can be linked to the abstract types in the concept level for the emotional inference. We test this emotional analysis framework on several datasets from the appraisal theory and the text of drama works. The results demonstrate that our framework can make better inferences on emotions and good interpretability for human beings.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {2038–2045},
numpages = {8},
keywords = {analysis framework, appraisal theories, emotional logic, knowledge graph, occ model},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3604951.3605514,
author = {Anuradha Nanomi Arachchige, Isuri and Ha, Le and Mitkov, Ruslan and Steinert, Johannes-Dieter},
title = {Enhancing Named Entity Recognition for Holocaust Testimonies through Pseudo Labelling and Transformer-based Models},
year = {2023},
isbn = {9798400708411},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604951.3605514},
doi = {10.1145/3604951.3605514},
abstract = {The Holocaust was a tragic and catastrophic event in World War II (WWII) history that resulted in the loss of millions of lives. In recent years, the emergence of the field of digital humanities has made the study of Holocaust testimonies an important area of research for historians, Holocaust educators, social scientists, and linguists. One of the challenges in analysing Holocaust testimonies is the recognition and categorisation of named entities such as concentration camps, military officers, ships, and ghettos, due to the scarcity of annotated data. This paper presents a research study on a domain-specific hybrid named-entity recognition model, which focuses on developing NER models specifically tailored for the Holocaust domain. To overcome the problem of data scarcity, we employed hybrid annotation approach to training different transformer model architectures in order to recognise the named entities. Results show transformer models to have good performance compared to other approaches.},
booktitle = {Proceedings of the 7th International Workshop on Historical Document Imaging and Processing},
pages = {85–90},
numpages = {6},
keywords = {Holocaust Testimonies, NER, Pseudo Labelling, Transformers},
location = {San Jose, CA, USA},
series = {HIP '23}
}

@article{10.1145/3579353,
author = {Casadei, Roberto},
title = {Macroprogramming: Concepts, State of the Art, and Opportunities of Macroscopic Behaviour Modelling},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3579353},
doi = {10.1145/3579353},
abstract = {Macroprogramming refers to the theory and practice of expressing the macro(scopic) behaviour of a collective system using a single program. Macroprogramming approaches are motivated by the need of effectively capturing global/system-level aspects and the collective behaviour of multiple computational components, while abstracting over low-level details. Previously, this programming style had been primarily adopted to describe the data-processing logic in sensor networks; recently, research forums on spatial computing, collective systems, and the Internet of Things have provided renewed interest in macro approaches. However, related contributions are still fragmented and lack conceptual consistency. Therefore, to foster principled research, an integrated view of the field is provided, together with opportunities and challenges.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {275},
numpages = {37},
keywords = {Macro programming, system-level programming, collective intelligence}
}

@article{10.14778/3407790.3407858,
author = {Sen, Jaydeep and Lei, Chuan and Quamar, Abdul and \"{O}zcan, Fatma and Efthymiou, Vasilis and Dalmia, Ayushi and Stager, Greg and Mittal, Ashish and Saha, Diptikalyan and Sankaranarayanan, Karthik},
title = {ATHENA++: natural language querying for complex nested SQL queries},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3407790.3407858},
doi = {10.14778/3407790.3407858},
abstract = {Natural Language Interfaces to Databases (NLIDB) systems eliminate the requirement for an end user to use complex query languages like SQL, by translating the input natural language (NL) queries to SQL automatically. Although a significant volume of research has focused on this space, most state-of-the-art systems can at best handle simple select-project-join queries. There has been little to no research on extending the capabilities of NLIDB systems to handle complex business intelligence (BI) queries that often involve nesting as well as aggregation. In this paper, we present Athena++, an end-to-end system that can answer such complex queries in natural language by translating them into nested SQL queries. In particular, Athena++ combines linguistic patterns from NL queries with deep domain reasoning using ontologies to enable nested query detection and generation. We also introduce a new benchmark data set (FIBEN), which consists of 300 NL queries, corresponding to 237 distinct complex SQL queries on a database with 152 tables, conforming to an ontology derived from standard financial ontologies (FIBO and FRO). We conducted extensive experiments comparing Athena++ with two state-of-the-art NLIDB systems, using both FIBEN and the prominent Spider benchmark. Athena++ consistently outperforms both systems across all benchmark data sets with a wide variety of complex queries, achieving 88.33\% accuracy on FIBEN benchmark, and 78.89\% accuracy on Spider benchmark, beating the best reported accuracy results on the dev set by 8\%.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {2747–2759},
numpages = {13}
}

@inproceedings{10.1145/3356395.3365545,
author = {Kurte, Kuldeep and Potnis, Abhishek and Durbha, Surya},
title = {Semantics-enabled Spatio-Temporal Modeling of Earth Observation Data: An application to Flood Monitoring},
year = {2019},
isbn = {9781450369541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356395.3365545},
doi = {10.1145/3356395.3365545},
abstract = {Extreme events such as urban floods are dynamic in nature, i.e. they evolve with time. The spatiotemporal analysis of such disastrous events is important for understanding the resiliency of an urban system during these events. Remote Sensing (RS) data is one of the crucial earth observation (EO) data sources that can facilitate such spatiotemporal analysis due to its wide spatial coverage and high temporal availability. In this paper, we propose a discrete mereotopology (DM) based approach to enable representation and querying of spatiotemporal information from a series of multitemporal RS images that are acquired during a flood disaster event. We represent this spatiotemporal information using a semantic model called Dynamic Flood Ontology (DFO). To establish the effectiveness and applicability of the proposed approach, spatiotemporal queries relevant during an urban flood scenario such as, show me road segments that were partially flooded during the time interval t1 have been demonstrated with promising results.},
booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances on Resilient and Intelligent Cities},
pages = {41–50},
numpages = {10},
keywords = {discrete mereotopology, flood disaster, ontology, semantics, spatial relations, spatiotemporal},
location = {Chicago, IL, USA},
series = {ARIC'19}
}

@inproceedings{10.1145/3307630.3342397,
author = {Shaaban, Abdelkader Magdy and Gruber, Thomas and Schmittner, Christoph},
title = {Ontology-Based Security Tool for Critical Cyber-Physical Systems},
year = {2019},
isbn = {9781450366687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307630.3342397},
doi = {10.1145/3307630.3342397},
abstract = {Industry 4.0 considers as a new advancement concept of the industrial revolution, which introduces a full utilization of Internet technologies. This concept aims to combine diverse technological resources into the industry field, which enables the communication between two worlds: the physical and the cyber one. Cyber-physical Systems are one of the special forces that integrate and build a variety of existing technologies and components. The diversity of components and technologies creates new security threats that can exploit vulnerabilities to attack a critical system. This work introduces an ontology-based security tool-chain able to be integrated with the initial stages of the development process of critical systems. The tool detects the potential threats, and apply the suitable security requirements which can address these threats. Eventually, it uses the ontology approach to ensure that the security requirements are fulfilled.},
booktitle = {Proceedings of the 23rd International Systems and Software Product Line Conference - Volume B},
pages = {207–210},
numpages = {4},
keywords = {cyber-physical system, ontology, security, threats},
location = {Paris, France},
series = {SPLC '19}
}

@article{10.1109/TCBB.2023.3322753,
author = {Hasan, Md. Al Mehedi and Maniruzzaman, Md. and Shin, Jungpil},
title = {Gene Expression and Metadata Based Identification of Key Genes for Hepatocellular Carcinoma Using Machine Learning and Statistical Models},
year = {2023},
issue_date = {Nov.-Dec. 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2023.3322753},
doi = {10.1109/TCBB.2023.3322753},
abstract = {Biomarkers associated with hepatocellular carcinoma (HCC) are of great importance to better understand biological response mechanisms to internal or external intervention. The study aimed to identify key candidate genes for HCC using machine learning (ML) and statistics-based bioinformatics models. Differentially expressed genes (DEGs) were identified using limma and then selected their common genes among DEGs identified from four datasets. After that, protein-protein interaction networks were constructed using STRING and then Cytoscape was used to determine hub genes, significant modules, and their associated genes. Simultaneously, three ML-based techniques such as support vector machine (SVM), least absolute shrinkage and selection operator-logistic regression (LASSO-LR), and partial least squares-discriminant analysis (PLS-DA) were implemented to determine the discriminative genes of HCC from common DEGs. Moreover, metadata of hub genes were formed by listing all hub genes from existing studies to incorporate other findings in our analysis. Finally, seven key candidate genes (ASPM, CCNB1, CDK1, DLGAP5, KIF20 A, MT1X, and TOP2A) were identified by intersecting common genes among hub genes, significant modules genes, discriminative genes from SVM, LASSO-LR, and PLS-DA, and meta hub genes from existing studies. Another three independent test datasets were also used to validate these seven key candidate genes using AUC, computed from ROC.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = oct,
pages = {3786–3799},
numpages = {14}
}

@inproceedings{10.1145/3184558.3191548,
author = {Enea, Roberto and Pazienza, Maria Teresa and Turbati, Andrea and Colantonio, Alessandro},
title = {How to Support Human Operator in "Uncertainty" Managing during the Ontology Learning Process},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191548},
doi = {10.1145/3184558.3191548},
abstract = {Creating ontologies is an essential while challenging task to be performed by either a human or a system: on one hand it is excessively burdensome for a human operator, on the other it is very complex also for a machine due to the not negligible amount of "uncertainty" that it must be able to manage. In the last years, some attempts have been made to automate this process, but at present, due to the large number of aspects to be covered in the automatic creation of an ontology (such as Domain terminology extraction, Concept discovery, Concept hierarchy derivation, ") satisfactory solutions have not been reached yet. In order to produce efficient tools for both creation and enrichment of ontologies, the participation of the human in such a process still seems necessary. Our approach, that foresees a broader framework for ontology learning, is based by first on the automatic extraction of triples from heterogeneous sources, then on the presentation of the most reliable triples to the human operator for validation purposes. The system provides the user with a series of graphical representations that can give him an overview of the level of uncertainty of the automatically generated ontology. Then provides the user with the possibility to perform SPARQL what-if queries, (i.e. assuming as true the triples filtered according to the level of confidence, the source and the structure of the triples).},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1147–1154},
numpages = {8},
keywords = {human computer interaction, ontology learning, reasoning on uncertain knowledge},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3550356.3561573,
author = {Prinz, Andreas and Xanthopoulou, Themis Dimitra and Gj\o{}s\ae{}ter, Terje and M\o{}ller-Pedersen, Birger},
title = {On abstraction in the OMG hierarchy: systems, models, and descriptions},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561573},
doi = {10.1145/3550356.3561573},
abstract = {The Model-Driven Architecture (MDA) uses a metadata hierarchy with several layers that are placed on top of each other. The traditional view is that the layers provide abstractions related to models in languages defined by meta-models. Over the years, it has been difficult to define a consistent understanding of the layers. In this paper, we propose such a consistent understanding by clarifying the relations between the different elements in the hierarchy. This is done based on the Scandinavian approach to modelling that distinguishes between systems and system descriptions. Systems can be physical, digital, or even mental, while descriptions can be programs, language descriptions, specifications, and diagrams. We relate descriptions and systems by explaining where semantics of objects originate and how they apply in the hierarchy.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {322–330},
numpages = {9},
keywords = {abstraction, description, instantiation, model, semantics, system},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3318236.3318239,
author = {Kechagioglou, Xeni and Lemmens, Rob and Retsios, Vasilios},
title = {Sharing Geoprocessing Workflows with Business Process Model and Notation (BPMN)},
year = {2019},
isbn = {9781450362450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318236.3318239},
doi = {10.1145/3318236.3318239},
abstract = {Graphical geoprocessing workflows are often built visually on interactive canvases of GIS software. Such workflows cannot be shared among different software, due to structural and semantical differences. This study experiments with a workflow created for ILWIS software and transforms it into a BPMN process model, exploiting XML serialisations of the two workflows. Ultimately, it aims at contributing to interoperability of geoprocessing workflows, through an extended approach serving as a frame around workflow conversion.},
booktitle = {Proceedings of the 2019 2nd International Conference on Geoinformatics and Data Analysis},
pages = {56–60},
numpages = {5},
keywords = {Business Process Model and Notation (BPMN), Geoinformatics, Geoprocessing, ILWIS, Interoperability, Workflow, eXtensible Stylesheet Language Transformations (XSLT)},
location = {Prague, Czech Republic},
series = {ICGDA '19}
}

@inproceedings{10.1145/3511808.3557179,
author = {Barret, Nelly and Manolescu, Ioana and Upadhyay, Prajna},
title = {Abstra: Toward Generic Abstractions for Data of Any Model},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557179},
doi = {10.1145/3511808.3557179},
abstract = {Digital data sharing leads to unprecedented opportunities to develop data-driven systems for supporting economic activities, the social and political life, and science. Many open-access datasets are RDF (Linked Data) graphs, but others are JSON or XML documents, CSV files, Neo4J property graphs, etc.Potential users need to understand a dataset in order to decide if it is useful for their goal. While some published datasets come with a schema and/or documentation, this is not always the case.We demonstrate Abstra, a dataset abstraction system, which applies on a large variety of data models. Abstra computes a description meant for humans, and integrates Information Extraction to classify dataset content among a set of categories of interest to the user. Our abstractions are conceptually close to Entity-Relationship diagrams, but our entities can have deeply nested structure.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {4803–4807},
numpages = {5},
keywords = {E-R schema, data integration, dataset discovery},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3641584.3641803,
author = {Peng, Xiangyu and Zhang, Yu and Hu, Wang},
title = {Data fusing driven graph embedding model for multi-source heterogeneous knowledge graph},
year = {2024},
isbn = {9798400707674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641584.3641803},
doi = {10.1145/3641584.3641803},
abstract = {Graph embedding has been suggested as an efficient approach for discovering substantial and valuable knowledge in knowledge graphs. However, due to the vast amount and complexity of knowledge graphs, it is challenging to obtain reliable embeddings, which poses challenges in scalability and property selection. Previous research has given little attention to these challenges. In addressing this issue, a knowledge graph embedding model known as the data fusing driven graph embedding model (DFGE) is proposed, which comprises three modules: node embedding module, attribute embedding module, and merge module. Within DFGE, node attribute information is filtered and combined with graph structure information to calculate embeddings. Furthermore, multi-source heterogeneous data fusion technology (MHDF) is also proposed within this model to capture attribute features, embed and fuse nodes' attributes, and enhance DFGE's effectiveness. Extensive experiments are conducted on both Cora and a new proposed Buildings Seismic Damage dataset. Results confirm that DFGE delivers significant performance improvements over state-of-the-art models.},
booktitle = {Proceedings of the 2023 6th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {1452–1457},
numpages = {6},
keywords = {graph embedding, heterogeneous data, knowledge graph},
location = {Xiamen, China},
series = {AIPR '23}
}

@inproceedings{10.1145/3486011.3486523,
author = {Jim\'{e}nez-Mac\'{\i}as, Alberto and Mu\~{n}oz-Merino, Pedro J. and Delgado Kloos, Carlos},
title = {A model to characterize exercises using probabilistic methods},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486523},
doi = {10.1145/3486011.3486523},
abstract = {Many studies have been conducted on modeling learners in education using probabilistic methods to infer different indicators. However, little research has been done on modeling content in education by estimating different content characteristics using probabilistic methods. Based on the existing gap in this area, this paper presents a model for exercises using probabilistic methods with interactions carried out by students to obtain content characteristics and their relationship such as the number of attempts, grade, and time spent related with student efficiency. Simulations were performed to train the model and find the different curves that best characterize the exercises based on programmatically generated interactions. This model allows the teacher to redesign the exercises to improve the student’s learning process.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {594–599},
numpages = {6},
keywords = {content modeling, learning analytics, smart content, smart learning environments},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@inproceedings{10.1145/3412841.3442059,
author = {Chakraborty, Jaydeep and Bansal, Srividya K. and Virgili, Luca and Konar, Krishanu and Yaman, Beyza},
title = {OntoConnect: unsupervised ontology alignment with recursive neural network},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442059},
doi = {10.1145/3412841.3442059},
abstract = {Ontology alignment is performed to combine or integrate multiple knowledge bases at the elemental and structural levels. The current state-of-the-art systems use many different approaches to match semantics, syntax, and terminologies of different ontological entities. However, most of the ontology alignment systems depend on domain knowledge, which makes the alignment process domain-specific. To address this challenge, we aim at developing an ontology alignment approach that is independent of domain knowledge. To achieve this goal, an ontology alignment approach is proposed which exploits an unsupervised learning method using a recursive neural network to align classes between different ontologies. In particular, the proposed approach extracts structural information of the classes in ontology to train the unsupervised model. The proposed approach is tested against a reference gold copy of the Anatomy data set in the Ontology Alignment Evaluation Initiative. Our evaluation results show that the proposed unsupervised neural network approach using the meta information of ontological classes yields satisfactory results with a precision of 95.66\% and F-measure of 80.26\% for a similarity threshold of 0.96 with the 100-dimension input vector. Increasing the input vector dimension to 300 results in improved precision of 97.71\% and F-measure of 80.38\% with a 0.96 threshold. The significance of the proposed approach is that it can be used for ontology alignment independent of domain expertise and without the need for human intervention.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1874–1882},
numpages = {9},
keywords = {LSTM, ontology schema alignment, ontology schema matching, recursive neural network, unsupervised learning},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.5555/3470152.3470205,
author = {Barcel\'{o}, Pablo and Feier, Cristina and Lutz, Carsten and Pieris, Andreas},
title = {When is ontology-mediated querying efficient?},
year = {2021},
publisher = {IEEE Press},
abstract = {In ontology-mediated querying, description logic (DL) ontologies are used to enrich incomplete data with domain knowledge which results in more complete answers to queries. However, the evaluation of ontology-mediated queries (OMQs) over relational databases is computationally hard. This raises the question when OMQ evaluation is efficient, in the sense of being tractable in combined complexity or fixed-parameter tractable. We study this question for a range of ontology-mediated query languages based on several important and widely-used DLs, using unions of conjunctive queries as the actual queries. For the DL ELHI⊥, we provide a characterization of the classes of OMQs that are fixed-parameter tractable. For its fragment ELHdr⊥, which restricts the use of inverse roles, we provide a characterization of the classes of OMQs that are tractable in combined complexity. Both results are in terms of equivalence to OMQs of bounded tree width and rest on a reasonable assumption from parameterized complexity theory. They are similar in spirit to Grohe's seminal characterization of the tractable classes of conjunctive queries over relational databases. We further study the complexity of the meta problem of deciding whether a given OMQ is equivalent to an OMQ of bounded tree width, providing several completeness results that range from NP to 2ExpTime, depending on the DL used. We also consider the DL-Lite family of DLs, including members that, unlike ELHI⊥, admit functional roles.},
booktitle = {Proceedings of the 34th Annual ACM/IEEE Symposium on Logic in Computer Science},
articleno = {53},
numpages = {13},
location = {Vancouver, Canada},
series = {LICS '19}
}

@proceedings{10.1145/3731120,
title = {ICTIR '25: Proceedings of the 2025 International ACM SIGIR Conference on Innovative Concepts and Theories in Information Retrieval (ICTIR)},
year = {2025},
isbn = {9798400718618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to ACM ICTIR 2025, the 11th ACM SIGIR / the 15th International Conference on Innovative Concepts and Theories in Information Retrieval. ICTIR went through a number of major developments in 2025 that are briefly described below.A new chapter in the history of ICTIR: Ten years ago, ICTIR 2015 was established under the ACM SIGIR umbrella as the premier forum for presenting and discussing research on theoretical and foundational aspects of Information Retrieval. Given the evolving nature of the Information Retrieval field and the needs of the SIGIR community and the conference, ACM ICTIR, formerly known as the "ACM SIGIR International Conference on the Theory of Information Retrieval", is renamed to ACM SIGIR International Conference on Innovative Concepts and Theories in Information Retrieval.This new branding highlights and clarifies the focus of the conference for the authors, the reviewers, and the organizers. The conference aims to provide a forum for the presentation and discussion of research related to the foundational aspects of Information Retrieval (IR), including, for example, new or improved models of relevance, ranking, representation, information needs, and evaluation. The conference also welcomes interdisciplinary research that connects information retrieval with other research disciplines that are theoretically motivated. We use the definition of foundation used by ICTIR founders as a scientific result that others can build upon and use for their own research. While IR research traditionally relies heavily on rigorous experimentation, ICTIR does not see this as a must.SIGIR Revise-and-Resubmit for ICTIR: For the first time, ICTIR 2025 accepts two types of submissions: regular submissions and SIGIR Revise-and-Resubmit (SIGIR-RR) submissions. SIGIRRR submissions are for revised manuscripts of papers that were submitted to but not accepted by the SIGIR 2025 conference (only full and short paper tracks). Authors can use this option to address the issues raised in the SIGIR 2025 reviews and revise the paper accordingly.Special Theme: Even though ICTIR 2025 welcomes papers on all areas of Information Retrieval as detailed above, its program especially welcomes research papers related to the following theme: "LLMs + IR, what could possibly go wrong?" This theme stands on the long-term relationship between large language models and information retrieval. In order to foster a lively discussion on this theme, we seek innovative and foundational technical research papers and discussion papers that methodologically studies this theme by providing impactful perspectives, opinions, or positions on the matter.},
location = {Padua, Italy}
}

@inproceedings{10.1145/3570236.3570256,
author = {Zeli, Cen},
title = {The Establishment and Research of an Evaluation Model Based on Network Analytic Hierarchy Process},
year = {2023},
isbn = {9781450396714},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570236.3570256},
doi = {10.1145/3570236.3570256},
abstract = {AHP is a decision-making method that decomposes the elements that are always related to decision-making into the levels of goals, criteria, and plans, and conducts qualitative and quantitative analysis on this basis. By analyzing a series of factors that affect the target, comparing their relative importance, and finally selecting the scheme with the highest score is the optimal scheme. This paper makes use of this characteristic of its analysis, establishes a set of evaluation model system, and applies the evaluation model to Wuhan Business School. Finally, the model is scientific and reasonable, which provides a reference for other researches.},
booktitle = {Proceedings of the 7th International Conference on Intelligent Information Processing},
articleno = {19},
numpages = {7},
keywords = {AHP, evaluation model, evaluation system, model analysis},
location = {Bucharest, Romania},
series = {ICIIP '22}
}

@inproceedings{10.1145/3719160.3736609,
author = {Yang, Boyin and Dudley, John J and Kristensson, Per Ola},
title = {Design Activity Simulation: Opportunities and Challenges in Using Multiple Communicative AI Agents to Tackle Design Problems},
year = {2025},
isbn = {9798400715273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719160.3736609},
doi = {10.1145/3719160.3736609},
abstract = {Large Language Models (LLMs) can enhance structured design thinking, yet existing copilot approaches integrate them into human workflows rather than exploring their autonomous potential. This paper investigates how LLM-based communicative AI agents can independently tackle open-ended design problems and how their strengths and limitations inform human-AI collaboration. We iteratively design a system where AI agents play different roles and simulate human design activity through conversational turns. The agents investigate user needs, identify design constraints, and explore the design space, with useful insights emerging from their interactions. To assess reasoning quality, we conducted a human jury evaluation with five HCI researchers and explored potential applications through a contextual inquiry with seven professionals. Our findings demonstrate that integrating human design thinking techniques enhances AI reasoning. AI agents effectively tackle design problems, generating low-novelty yet well-grounded and practical solutions that meet key design requirements.},
booktitle = {Proceedings of the 7th ACM Conference on Conversational User Interfaces},
articleno = {46},
numpages = {19},
keywords = {Generative AI; End-user interaction with LLMs and Multimodal models},
location = {
},
series = {CUI '25}
}

@inproceedings{10.1145/3219788.3219797,
author = {Lin, Shuo and Han, Jun and Kumar, Kuldeep and Wang, Jiping},
title = {Generating Domain Ontology from Chinese Customer Reviews to Analysis Fine-gained Product Quality Risk},
year = {2018},
isbn = {9781450363938},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219788.3219797},
doi = {10.1145/3219788.3219797},
abstract = {With the rapid development of E-commerce in China, quality of the products on online shopping platforms has caused wide concern. Customer reviews, which commented by people who bought the very product, now have been one of the most important resources for analyzing product's quality risk. We can get fine-gained, aspect-oriented risk information of a product by mining its reviews. Unfortunately, people tend to write reviews with casual grammar or just omit parts of components of a sentence. Both these features will cause negative impacts when parsing the raw customer reviews directly. Thus a knowledge base which is built totally beyond the reviews could be used to analyze it despite the drawbacks above. In this paper, we generate a domain ontology from raw text in the online encyclopedia. It can be viewed as a graph whose nodes represent domain concepts and edges represent the relations between these concepts. In our work, we integrate syntactic tree structure in linear-chain CRFs for recognizing domain concepts and train SVMs and MaxEnt models on elaborate features for clarifying three types of relationship, namely "Attribute-of", "Part-of" and "Instance-of". Once the ontology has been built, product properties with potential risk will be extracted by our matching method. Experiment show that our approach achieves 64.4\% precision and 82.4\% recall on risky property extraction task.},
booktitle = {Proceedings of the 2018 International Conference on Computing and Data Engineering},
pages = {73–77},
numpages = {5},
keywords = {domain ontology, opinion mining, product quality risk},
location = {Shanghai, China},
series = {ICCDE '18}
}

@inproceedings{10.1145/3659677.3659742,
author = {Babaalla, Zakaria and Jakimi, Abdeslam and Oualla, Mohamed and Saadane, Rachid and Chehri, Abdellah},
title = {Towards an Automatic Extracting UML Class Diagram from System's Textual Specification},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659677.3659742},
doi = {10.1145/3659677.3659742},
abstract = {Developing a software system from natural language requirements is a complex and delicate task that requires a high level of design and programming expertise. Increasing the level of abstraction used to describe these requirements is the most natural solution. Model-Driven Engineering (MDE) also takes this route, using abstract models as primary entities to generate source code automatically or semi-automatically. Among these models, the UML class diagram occupies a privileged place in object-oriented systems because it not only serves as a basis for communication between developers but also provides a closely aligned static representation of the system implementation. However, creating a UML class diagram from a textual system specification poses a significant challenge due to the inherent imprecision and ambiguity commonly found in natural language expressions. In this paper, we propose a model-centric approach based on deep learning for the automatic extraction of UML class diagrams from textual requirements.},
booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
articleno = {36},
numpages = {5},
keywords = {Class diagram, Deep learning, Machine learning, Natural Language Processing, UML},
location = {Meknes, AA, Morocco},
series = {NISS '24}
}

@inproceedings{10.1145/3460231.3473901,
author = {Olufisayo Dahunsi, Bolanle},
title = {An Ontology-based Knowledgebase for User Profile and Garment Features in Apparel Recommender Systems},
year = {2021},
isbn = {9781450384582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460231.3473901},
doi = {10.1145/3460231.3473901},
abstract = {This research proposes the development of an ontology-based knowledgebase for Apparel recommender systems. User and garment attribute definitions and expert style rules will be extracted from expert literature and the most important accuracy-driving user and garment features will be parsed from it. The features will then be used to define the classes and slots of the knowledgebase with constraints defined by the style rules from the experts. This knowledgebase will be made publicly available for modification and reuse in future apparel recommender systems design.},
booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},
pages = {851–854},
numpages = {4},
location = {Amsterdam, Netherlands},
series = {RecSys '21}
}

@inproceedings{10.1145/3281375.3281386,
author = {Rinaldi, Antonio M. and Russo, Cristiano},
title = {A semantic-based model to represent multimedia big data},
year = {2018},
isbn = {9781450356220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281375.3281386},
doi = {10.1145/3281375.3281386},
abstract = {The use of formal representation is a key task in the era of big data. In the context of multimedia big data this issue is stressed due to the intrinsic complexity nature of this kind of data. Moreover, the relations among objects should be clearly expressed and formalized to give the right meaning of data correlation. For this reason the design of formal models to represent and manage information is a necessary task to implement intelligent information systems. In this latter some approaches related to the semantic web could be used to improve the data models which underlie the implementation of big data applications. Using these models the visualization of data and information become an intrinsic and strategic task for the analysis and exploration of multimedia BigData. In this paper we propose the use of a semantic approach to formalize the model structure of multimedia BigData. In addition, the recognition of multimodal features to represent concepts and linguistic properties to relate them are an effective way to bridge the gap between the target semantic classes and the available low-level multimedia descriptors. The proposed model has been implemented in a NoSQL graph database populated from different knowledge sources and a visualization of this very large knowledge base has been presented and discussed as a case study.},
booktitle = {Proceedings of the 10th International Conference on Management of Digital EcoSystems},
pages = {31–38},
numpages = {8},
keywords = {multimedia ontologies, semantic bigdata, semantics},
location = {Tokyo, Japan},
series = {MEDES '18}
}

@inproceedings{10.1145/3584371.3613001,
author = {Wang, Zifeng and Xiao, Cao and Sun, Jimeng},
title = {SPOT: Sequential Predictive Modeling of Clinical Trial Outcome with Meta-Learning},
year = {2023},
isbn = {9798400701269},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584371.3613001},
doi = {10.1145/3584371.3613001},
abstract = {Clinical trials are essential to drug development but time-consuming, costly, and prone to failure. Accurate trial outcome prediction based on historical trial data promises better trial investment decisions and more trial success. Existing trial outcome prediction models were not designed to model the relations among similar trials, capture the progression of features and designs of similar trials, or address the skewness of trial data which causes inferior performance for less common trials.To fill the gap and provide accurate trial outcome prediction, we propose Sequential Predictive mOdeling of clinical Trial outcome (SPOT) that first identifies trial topics to cluster the multisourced trial data into relevant trial topics. It then generates trial embeddings and organizes them by topic and time to create clinical trial sequences. With the consideration of each trial sequence as a task, it uses a meta-learning strategy to achieve a point where the model can rapidly adapt to new tasks with minimal updates. In particular, the topic discovery module enables a deeper understanding of the underlying structure of the data, while sequential learning captures the evolution of trial designs and outcomes. This results in predictions that are not only more accurate but also more interpretable, taking into account the temporal patterns and unique characteristics of each trial topic. We demonstrate that SPOT wins over the prior methods by a significant margin on trial outcome benchmark data: with a 21.5\% lift on phase I, an 8.9\% lift on phase II, and a 5.5\% lift on phase III trials in the metric of the area under precision-recall curve (PR-AUC). Code is available at https://github.com/RyanWangZf/PyTrial.},
booktitle = {Proceedings of the 14th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {53},
numpages = {11},
location = {Houston, TX, USA},
series = {BCB '23}
}

@inproceedings{10.1145/3278681.3278687,
author = {Motara, Yusuf Moosa and van der Schyff, Karl},
title = {A functional ontology},
year = {2018},
isbn = {9781450366472},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278681.3278687},
doi = {10.1145/3278681.3278687},
abstract = {The ontology of information systems --- the way in which knowledge claims, and thus theories, are conceptualised and represented --- is of particular importance in the information systems field, due to its reliance on relations between entities. This work proposes, demonstrates, and evaluates an alternative ontology for theory description which is arguably more powerful and more expressive than the dominant ontological model.},
booktitle = {Proceedings of the Annual Conference of the South African Institute of Computer Scientists and Information Technologists},
pages = {49–54},
numpages = {6},
keywords = {functional design, ontology, theory building, theory of planned behaviour},
location = {Port Elizabeth, South Africa},
series = {SAICSIT '18}
}

@inproceedings{10.1145/3286606.3286825,
author = {El Yamami, Abir and Mansouri, Khalifa and Qbadou, Mohammed and Illoussamen, Elhossein and Laaziri, Majida and Benmoussa, Khaoula},
title = {An Ontological Representation of PMBOK Framework Knowledge Areas},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286825},
doi = {10.1145/3286606.3286825},
abstract = {Considering the problematic of IT Governance frameworks implementation, since it is difficult to apply a common framework to all organizations, this paper intended to address issues related to the adoption of IT project governance practices in organizations. It provides a common representation of its knowledge areas (Scope management, Schedule management, Cost management, Quality management and Risk Management) through ontological approach, relying on PMBOK framework best practice. The goal is to provide a machine-readable document for modeling IT projects governance domain. The results of this paper constitute a considerable contribution for practitioners, by participating in the development of IT governance approaches, to be used by non-specialists PMBOK in organizations, and by limiting as much as possible the bureaucracy of the information systems frameworks.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {48},
numpages = {6},
keywords = {IT Governance, IT Project management, Ontology, PMBOK, Prot\'{e}g\'{e}},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.1145/3623509.3635326,
author = {Rayzhekov, Antoni and Murer, Martin},
title = {Between This and That is It: Embodied Semantic Space at the Edge},
year = {2024},
isbn = {9798400704024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3623509.3635326},
doi = {10.1145/3623509.3635326},
abstract = {This paper describes the interactive artwork “Between This and That is It”: An AI-augmented typewriter that blends several decades of computerized optimization of text production. The artwork employs an offline processing machine learning-based language model embedded in a typical office typewriter from the 1980s. Deliberately diverting from the pervasive conversational user interface, the interaction style is based on a well-defined minimalist pattern of complementing two user-supplied words with a third word that – in the model – lies in the middle of the other two. This constraint interaction invites to explore the limits of the semantic space of language models and poses questions related to the topology of meaning, with respect to truthfulness, biases, and cliches, by creating a semi-intelligent poetic co-performance involving the audience. This project seeks to foster a discussion on the creative collaboration between humans and AI within the constraints of machine learning technologies embedded into objects from the near past. The experience and the perception of the interaction are shaped by a hybrid space shared between the audience members and the AI, the mechanical limitations of the typewriter, the embedded mini-computer’s computational capacity, and the language model itself.},
booktitle = {Proceedings of the Eighteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {101},
numpages = {4},
keywords = {AI, AI edge computing, Interactive Art, Language models, graspable AI},
location = {Cork, Ireland},
series = {TEI '24}
}

@inproceedings{10.1145/3719160.3736616,
author = {Samimi, Reza and Bhattacharya, Aditya and Gosak, Lucija and Stiglic, Gregor and Verbert, Katrien},
title = {Visual-Conversational Interface for Evidence-Based Explanation of Diabetes Risk Prediction},
year = {2025},
isbn = {9798400715273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719160.3736616},
doi = {10.1145/3719160.3736616},
abstract = {Healthcare professionals need effective ways to use, understand, and validate AI-driven clinical decision support systems. Existing systems face two key limitations: complex visualizations and lack of grounding in scientific evidence. We present an integrated Decision Support System that combines interactive visualizations with a conversational agent for explaining diabetes risk assessments. We propose a hybrid prompt handling approach combining fine-tuned language models for analytical queries with general Large Language Models (LLMs) for broader medical questions, a methodology for grounding AI explanations in scientific evidence and a feature range analysis technique to support deeper understanding of feature contributions. We conducted a mixed-methods study with 30 healthcare professionals and found that the conversational interactions helped healthcare professionals build a clear understanding of model assessments, while the integration of scientific evidence calibrated trust in the system’s decisions. Most participants reported that the system supported both patient risk evaluation and recommendation.},
booktitle = {Proceedings of the 7th ACM Conference on Conversational User Interfaces},
articleno = {52},
numpages = {18},
keywords = {Clinical Decision Support Systems, Explainable AI, Human-centered AI, Conversational AI},
location = {
},
series = {CUI '25}
}

@article{10.1145/3486250,
author = {Guo, Jiafeng and Cai, Yinqiong and Fan, Yixing and Sun, Fei and Zhang, Ruqing and Cheng, Xueqi},
title = {Semantic Models for the First-Stage Retrieval: A Comprehensive Review},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3486250},
doi = {10.1145/3486250},
abstract = {Multi-stage ranking pipelines have been a practical solution in modern search systems, where the first-stage retrieval is to return a subset of candidate documents and latter stages attempt to re-rank those candidates. Unlike re-ranking stages going through quick technique shifts over the past decades, the first-stage retrieval has long been dominated by classical term-based models. Unfortunately, these models suffer from the vocabulary mismatch problem, which may block re-ranking stages from relevant documents at the very beginning. Therefore, it has been a long-term desire to build semantic models for the first-stage retrieval that can achieve high recall efficiently. Recently, we have witnessed an explosive growth of research interests on the first-stage semantic retrieval models. We believe it is the right time to survey current status, learn from existing methods, and gain some insights for future development. In this article, we describe the current landscape of the first-stage retrieval models under a unified framework to clarify the connection between classical term-based retrieval methods, early semantic retrieval methods, and neural semantic retrieval methods. Moreover, we identify some open challenges and envision some future directions, with the hope of inspiring more research on these important yet less investigated topics.},
journal = {ACM Trans. Inf. Syst.},
month = mar,
articleno = {66},
numpages = {42},
keywords = {Semantic retrieval models, information retrieval, survey}
}

@article{10.1109/TCBB.2019.2951137,
author = {Liu, Jian and Qu, Zhi and Yang, Mo and Sun, Jialiang and Su, Shuhui and Zhang, Lei},
title = {Jointly Integrating VCF-Based Variants and OWL-Based Biomedical Ontologies in MongoDB},
year = {2020},
issue_date = {Sept.-Oct. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {5},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2019.2951137},
doi = {10.1109/TCBB.2019.2951137},
abstract = {The development of the next-generation sequencing (NGS) technologies has led to massive amounts of VCF (Variant Call Format) files, which have been the standard formats developed with 1000 Genomes Project. At the same time, with the widespread use of biomedical ontologies in the biomedical community, more and more applications have accepted the Web Ontology Language (OWL) as the dominant data format for the specifications of biomedical ontology descriptions, leading to the rapid growth of OWL-based biomedical ontology scale. In this paper, we seek to explore an effective method for the management of VCF-based genetic variants and OWL-based biological ontologies using the MongoDB database. Considering many current applications (such as the short genetic variations database dbSNP, etc.) are transitioning to the new design by using JSON (JavaScript Object Notation) to support future massive data expansion and interchanges. We firstly propose a series of rules for the mapping from VCF and OWL files to JSON files, and then present rule-based algorithms for transforming VCF-based genetic variants and OWL-based biological ontologies into JSON objects. On this basis, we introduce effective approaches of integrating the mapped JSON files in MongoDB. Finally, we complement this work with a set of experiments to show the performance of our proposed approaches. The source code of the proposed approaches could be freely available at https://github.com/lyotvincent/AJIA.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = oct,
pages = {1504–1515},
numpages = {12}
}

@inproceedings{10.1145/3442442.3451386,
author = {Goel, Tushar and Chauhan, Vipul and Verma, Ishan and Dasgupta, Tirthankar and Dey, Lipika},
title = {TCS_WITM_2021 @FinSim-2: Transformer based Models for Automatic Classification of Financial Terms},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451386},
doi = {10.1145/3442442.3451386},
abstract = {Recent advancement in neural network architectures has provided several opportunities to develop systems to automatically extract and represent information from domain specific unstructured text sources. The Finsim-2021 shared task, collocated with the FinNLP workshop, offered the challenge to automatically learn effective and precise semantic models of financial domain concepts. Building such semantic representations of domain concepts requires knowledge about the specific domain. Such a thorough knowledge can be obtained through the contextual information available in raw text documents on those domains. In this paper, we proposed a transformer-based BERT architecture that captures such contextual information from a set of domain specific raw documents and then perform a classification task to segregate domain terms into fixed number of class labels. The proposed model not only considers the contextual BERT embeddings but also incorporates a TF-IDF vectorizer that gives a word level importance to the model. The performance of the model has been evaluated against several baseline architectures.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {311–315},
numpages = {5},
keywords = {Automatic Classification of Financial Term, Ontology, TFIDF Vectors, Text Classification, Transformers},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3319008.3319017,
author = {Wen, Shao-Fang and Katt, Basel},
title = {Preliminary Evaluation of an Ontology-Based Contextualized Learning System for Software Security},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319017},
doi = {10.1145/3319008.3319017},
abstract = {Learning software security is a big challenging task in the information technology sector due to the vast amount of security knowledge and the difficulties in understanding the practical applications. The traditional teaching and learning materials, which are usually organized topically and security-centric, have fewer linkages with learners' experience and prior knowledge that they bring to the learning sessions. Learners often do not associate vulnerabilities or coding practices with programs similar to what they were writing in their previous time. Consequently, their motivation for learning is not touched by conventional methods. The aim of this paper is the presentation of an ontology-based learning system for software security with contextualized learning approaches, and of the results of an initial evaluation using a controlled quasi-experiment in a university learning environment. This system facilitates the contextual learning process by providing contextualized access to security knowledge via real software application scenarios, in which learners can explore and relate the security knowledge to the context they are already familiar with. The experiment results show that the prototyped system with the proposed learning approach not only yields significant knowledge gain compared to the conventional learning approach but also gains better learning satisfaction of students.},
booktitle = {Proceedings of the 23rd International Conference on Evaluation and Assessment in Software Engineering},
pages = {90–99},
numpages = {10},
keywords = {Software security, context-based knowledge, learning system, ontology},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@article{10.1145/3238304,
author = {Arenas, Marcelo and Gottlob, Georg and Pieris, Andreas},
title = {Expressive Languages for Querying the Semantic Web},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {3},
issn = {0362-5915},
url = {https://doi.org/10.1145/3238304},
doi = {10.1145/3238304},
abstract = {The problem of querying RDF data is a central issue for the development of the Semantic Web. The query language SPARQL has become the standard language for querying RDF since its W3C standardization in 2008. However, the 2008 version of this language missed some important functionalities: reasoning capabilities to deal with RDFS and OWL vocabularies, navigational capabilities to exploit the graph structure of RDF data, and a general form of recursion much needed to express some natural queries. To overcome these limitations, a new version of SPARQL, called SPARQL 1.1, was released in 2013, which includes entailment regimes for RDFS and OWL vocabularies, and a mechanism to express navigation patterns through regular expressions. Unfortunately, there are a number of useful navigation patterns that cannot be expressed in SPARQL 1.1, and the language lacks a general mechanism to express recursive queries. To the best of our knowledge, no efficient RDF query language that combines the above functionalities is known. It is the aim of this work to fill this gap. To this end, we focus on a core fragment of the OWL 2 QL profile of OWL 2 and show that every SPARQL query enriched with the above features can be naturally translated into a query expressed in a language that is based on an extension of Datalog, which allows for value invention and stratified negation. However, the query evaluation problem for this language is highly intractable, which is not surprising since it is expressive enough to encode some inherently hard queries. We identify a natural fragment of it, and we show it to be tractable and powerful enough to define SPARQL queries enhanced with the desired functionalities.},
journal = {ACM Trans. Database Syst.},
month = nov,
articleno = {13},
numpages = {45},
keywords = {Datalog-based languages, RDF, SPARQL, Semantic web, query answering}
}

@inproceedings{10.1145/3286606.3286846,
author = {Pahal, Nisha and Mallik, Anupama and Chaudhury, Santanu},
title = {An Ontology-based Context-aware IoT Framework for Smart Surveillance},
year = {2018},
isbn = {9781450365628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3286606.3286846},
doi = {10.1145/3286606.3286846},
abstract = {In this paper, we have proposed an ontology-based context-aware framework for providing intelligent services such as smart surveillance, which employ IoT technologies to ensure better quality of life in a smart city. An IoT network such as a smart surveillance system combines the working of Closed-circuit television (CCTV) cameras and various sensors to perform real-time computation for identifying threats and critical situations with the help of valuable context information. This information is perceptual in nature and needs to be converted into higher-level abstractions that can further be used for reasoning to recognize situations. Semantic abstractions for perceptual inputs are possible with the use of a multimedia ontology encoded using Multimedia Web Ontology Language (MOWL) that helps to define concepts, properties and structure of a possible environment. MOWL also allows for a dynamic modeling of real-time situations by employing Dynamic Bayesian networks (DBN), which suits the requirements of a intelligent IoT system. In this paper, we show the application of this framework in a smart surveillance system. Surveillance is enhanced by not only helping to analyze past events, but by predicting dangerous situations for which preventive actions can be taken. In our proposed approach, continuous video stream of data captured by CCTV cameras can be processed on the fly to give real-time alerts to concerned authorities. These alerts can be disseminated using e-mail, text messaging, on-screen alerts and alarms.},
booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
articleno = {69},
numpages = {7},
keywords = {Dynamic Bayesian Network (DBN), Internet of Things (IoT), Multimedia ontology, Smart Surveillance},
location = {Tetouan, Morocco},
series = {SCA '18}
}

@inproceedings{10.1145/3196959.3196963,
author = {Barcelo, Pablo and Berger, Gerald and Pieris, Andreas},
title = {Containment for Rule-Based Ontology-Mediated Queries},
year = {2018},
isbn = {9781450347068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196959.3196963},
doi = {10.1145/3196959.3196963},
abstract = {Many efforts have been dedicated to identifying restrictions on ontologies expressed as tuple-generating dependencies (tgds), a.k.a. existential rules, that lead to the decidability of answering ontology-mediated queries (OMQs). This has given rise to three families of formalisms: guarded, non-recursive, and sticky sets of tgds. We study the containment problem for OMQs expressed in such formalisms, which is a key ingredient for solving static analysis tasks associated with them. Our main contribution is the development of specially tailored techniques for OMQ containment under the classes of tgds stated above. This enables us to obtain sharp complexity bounds for the problems at hand.},
booktitle = {Proceedings of the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
pages = {267–279},
numpages = {13},
keywords = {computational complexity, conjunctive queries, ontology-mediated queries, query containment, tuple-generating dependencies},
location = {Houston, TX, USA},
series = {PODS '18}
}

@inproceedings{10.1145/3706598.3713083,
author = {Krau\ss{}, Veronika and McGill, Mark and Kosch, Thomas and Thiel, Yolanda Maira and Sch\"{o}n, Dominik and Gugenheimer, Jan},
title = {"Create a Fear of Missing Out" - ChatGPT Implements Unsolicited Deceptive Designs in Generated Websites Without Warning},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713083},
doi = {10.1145/3706598.3713083},
abstract = {With the recent advancements in Large Language Models (LLMs), web developers increasingly apply their code-generation capabilities to website design. However, since these models are trained on existing designerly knowledge, they may inadvertently replicate bad or even illegal practices, especially deceptive designs (DD). This paper examines whether users can accidentally create DD for a fictitious webshop using GPT-4. We recruited 20 participants, asking them to use ChatGPT to generate functionalities (product overview or checkout) and then modify these using neutral prompts to meet a business goal (e.g., “increase the likelihood of us selling our product”). We found that all 20 generated websites contained at least one DD pattern (mean: 5, max: 9), with GPT-4 providing no warnings. When reflecting on the designs, only 4 participants expressed concerns, while most considered the outcomes satisfactory and not morally problematic, despite the potential ethical and legal implications for end-users and those adopting ChatGPT’s recommendations.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {857},
numpages = {20},
keywords = {ChatGPT, LLM, Deceptive Design, Dark Patterns, Design Inspiration},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3383219.3383292,
author = {Alenezi, Mamdouh and Basit, Hamid Abdul and Khan, Faraz Idris and Beg, Maham Anwar},
title = {A Comparison Study of Available Sofware Security Ontologies},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383292},
doi = {10.1145/3383219.3383292},
abstract = {A rising number of software and services malfunctioning due to security flaws has increased the importance of software security and resulted in numerous knowledge sources of the domain. Building secure software systems require the understanding and extraction of the available knowledge, and a standard knowledge management platform is needed. Ontologies form an integral part of knowledge management platforms as they capture and structure the given knowledge. Various software security ontologies have been proposed previously, either stand-alone or as part of some bigger ontology like a computer or information security. However, these ontologies do not cover the entire domain and cannot be used as a standard ontology for software security in its current form. In this paper, we have identified and evaluated the existing ontologies that specifically capture software security knowledge, both qualitatively and quantitatively with the help of ontology evaluation tools, in order to select the best ontology that can be extended to prepare the standard ontology for the software security domain.},
booktitle = {Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering},
pages = {499–504},
numpages = {6},
keywords = {Security Ontology, Software Quality Assurance, Software Quality Management, Software Security, Software Security Knowledge Management},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3193063.3193065,
author = {Yeh, Jian-hua},
title = {Towards a Biographic Knowledge-based Story Ontology System},
year = {2018},
isbn = {9781450363785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3193063.3193065},
doi = {10.1145/3193063.3193065},
abstract = {In this article, we illustrate some of the semantic web-related technologies and design a set of ontology knowledge structures based on biographical history, using the OWL markup language, which we call BKOnto. This is an official framework for processing biographical history-related messages on the semantic web, including biographical events, time and space relationships, related personal messages, and more. We elaborate on this ontology knowledge architecture and explain how to use BKOnto as a basis for more domain-specific knowledge representation. In BKOnto, we use the OWL language to define the main components of the cognitive structure of the historical body of biography, namely the Storyline of the biography and the historical event of the biography. The so-called biographical story line, which is used to organize the history of multiple biographical superstructure, can be used to describe the biography of a particular person. The so-called biographical historical events, based on the historical data can be based on the description of the content and related space-time factor description of the basic unit. BKOnto's design was based on the StoryLine and Event infrastructure, and then we developed the ontology knowledge building system based on this ontology awareness architecture. Therefore, we also developed a set of ontology knowledge building system based on BKOnto, which is called StoryTeller. The StoryTeller system can be used to construct relevant knowledge of human things in the history of the biography and form a complete biographical story. StoryTeller system, mainly based on the story line organized by the timeline, which contains a number of types and events related to multiple human things as the basic unit to build the story line. The event unit not only describes the description of related human affairs, but also contains the description of time factor and space factor, which is used to construct the space-time information of the unit in the story line. As a result, in a story line with multiple event units, you will be able to present a wealth of information about people and things with their associated spatiotemporal features. In addition, based on the idea of supporting the digital collection system, we also linked up individual event units with the digital collection system of their information sources so that more diverse digital collections could be presented in the future. The empirical study also uses the Mackay Digital Archives Project (http://dlm.csie.au.edu.tw/) as a source of information to demonstrate the ontology knowledge building process of Mackay's biographical stories, as well as related Digital collection of information.},
booktitle = {Proceedings of the 2018 International Conference on Intelligent Information Technology},
pages = {33–38},
numpages = {6},
keywords = {Biographical knowledge, OWL, ontology, ontology composition system, semantic web, temporal event},
location = {Ha Noi, Viet Nam},
series = {ICIIT '18}
}

@inproceedings{10.1145/3591106.3592258,
author = {Deng, Jiaxin and Shen, Dong and Pan, Haojie and Wu, Xiangyu and Liu, Ximan and Meng, Gaofeng and Yang, Fan and Gao, Tingting and Fu, Ruiji and Wang, Zhongyuan},
title = {A Unified Model for Video Understanding and Knowledge Embedding with Heterogeneous Knowledge Graph Dataset},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591106.3592258},
doi = {10.1145/3591106.3592258},
abstract = {Video understanding is an important task in short video business platforms and it has a wide application in video recommendation and classification. Most of the existing video understanding works only focus on the information that appeared within the video content, including the video frames, audio and text. However, introducing common sense knowledge from the external Knowledge Graph (KG) dataset is essential for video understanding when referring to the content which is less relevant to the video. Owing to the lack of video knowledge graph dataset, the work which integrates video understanding and KG is rare. In this paper, we propose a heterogeneous dataset that contains the multi-modal video entity and fruitful common sense relations. This dataset also provides multiple novel video inference tasks like the Video-Relation-Tag (VRT) and Video-Relation-Video (VRV) tasks. Furthermore, based on this dataset, we propose an end-to-end model that jointly optimizes the video understanding objective with knowledge graph embedding, which can not only better inject factual knowledge into video understanding but also generate effective multi-modal entity embedding for KG. Comprehensive experiments indicate that combining video understanding embedding with factual knowledge benefits the content-based video retrieval performance. Moreover, it also helps the model generate better knowledge graph embedding which outperforms traditional KGE-based methods on VRT and VRV tasks with at least 42.36\% and 17.73\% improvement in HITS@10.},
booktitle = {Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
pages = {95–104},
numpages = {10},
keywords = {knowledge graph, multi-modal learning, video inference, video understanding},
location = {Thessaloniki, Greece},
series = {ICMR '23}
}

@inproceedings{10.1145/3443279.3443291,
author = {Abri, Sara and Abri, Rayan and Cetin, Salih},
title = {A Classification on Different Aspects of User Modelling in Personalized Web Search},
year = {2021},
isbn = {9781450377607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443279.3443291},
doi = {10.1145/3443279.3443291},
abstract = {In the context of personalization has recently been doing a lot of researches and applications. A common component of all research in the field of personalization is user modeling that also called user profiling. The main work of the user modeling in the field of personalization in the first step is capturing information about users and in the next step is to identify the user's preferences and interests and efficient use this information for increasing the retrieval performance. How to collect information about the user, user model structures and the used techniques to create a user model is different in each of personalized applications. In the previous studies, there was not a complete classification on the major dimensions of user models. In this research, we present an appropriate classification on the major dimensions of user models. We aim to present a survey on applications and techniques of user modeling and make a classification of user modeling by considering the existing literature and research and we hope can help to researchers in better-developing on the area.},
booktitle = {Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval},
pages = {194–199},
numpages = {6},
keywords = {User Profiling, Recommendation Systems, Personalized Search},
location = {Seoul, Republic of Korea},
series = {NLPIR '20}
}

@inproceedings{10.1145/3652620.3688222,
author = {Exelmans, Joeri and Pietron, Jakob and Raschke, Alexander and Vangheluwe, Hans},
title = {A Virtual Global Monorepo of Immutable Linked Data},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688222},
doi = {10.1145/3652620.3688222},
abstract = {The data layer of today's model management solutions often is either centralized or Git-based. We point out a number of limitations of current approaches, such as poor replicability, manually configured access control, centralization, hard-coded 'meta-data', and inflexible encodings. We argue for a set of fundamental features / restrictions (most importantly immutability and capability-based security) for decentralized model management systems to adapt, to solve these problems at their root. We distinguish a fundamental core from non-fundamental applications (such as versioning), that can be built on top.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {1000–1004},
numpages = {5},
keywords = {model management, capability-based security, versioning},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3462757.3466104,
author = {Wehnert, Sabine and Sudhi, Viju and Dureja, Shipra and Kutty, Libin and Shahania, Saijal and De Luca, Ernesto W.},
title = {Legal norm retrieval with variations of the bert model combined with TF-IDF vectorization},
year = {2021},
isbn = {9781450385268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462757.3466104},
doi = {10.1145/3462757.3466104},
abstract = {In this work, we examine variations of the BERT model on the statute law retrieval task of the COLIEE competition. This includes approaches to leverage BERT's contextual word embeddings, fine-tuning the model, combining it with TF-IDF vectorization, adding external knowledge to the statutes and data augmentation. Our ensemble of Sentence-BERT with two different TF-IDF representations and document enrichment exhibits the best performance on this task regarding the F2 score. This is followed by a fine-tuned LEGAL-BERT with TF-IDF and data augmentation and our third approach with the BERTScore. As a result, we show that there are significant differences between the chosen BERT approaches and discuss several design decisions in the context of statute law retrieval.},
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
pages = {285–294},
numpages = {10},
keywords = {contextual word embeddings, data augmentation, document enrichment, legal information retrieval},
location = {S\~{a}o Paulo, Brazil},
series = {ICAIL '21}
}

@inproceedings{10.1145/3478676,
author = {Hormozdiari, Fereydoun},
title = {Session details: Ontologies \&amp; databases},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478676},
doi = {10.1145/3478676},
booktitle = {Proceedings of the 12th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
location = {Gainesville, Florida},
series = {BCB '21}
}

@inproceedings{10.1145/3706598.3713502,
author = {Choi, Jiin and Lee, Seung Won and Hyun, Kyung Hoon},
title = {GenPara: Enhancing the 3D Design Editing Process by Inferring Users' Regions of Interest with Text-Conditional Shape Parameters},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713502},
doi = {10.1145/3706598.3713502},
abstract = {In 3D design, specifying design objectives and visualizing complex shapes through text alone proves to be a significant challenge. Although advancements in 3D GenAI have significantly enhanced part assembly and the creation of high-quality 3D designs, many systems still to dynamically generate and edit design elements based on the shape parameters. To bridge this gap, we propose GenPara, an interactive 3D design editing system that leverages text-conditional shape parameters of part-aware 3D designs and visualizes design space within the Exploration Map and Design Versioning Tree. Additionally, among the various shape parameters generated by LLM, the system extracts and provides design outcomes within the user’s regions of interest based on Bayesian inference. A user study (N = 16) revealed that GenPara enhanced the comprehension and management of designers with text-conditional shape parameters, streamlining design exploration and concretization. This improvement boosted efficiency and creativity of the 3D design process.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {4},
numpages = {21},
keywords = {3D Generative AI, Design Space, Large Language Models (LLMs), Bayesian Inference, Human-AI Interaction},
location = {
},
series = {CHI '25}
}

@article{10.1145/3588938,
author = {Tu, Jianhong and Fan, Ju and Tang, Nan and Wang, Peng and Li, Guoliang and Du, Xiaoyong and Jia, Xiaofeng and Gao, Song},
title = {Unicorn: A Unified Multi-tasking Model for Supporting Matching Tasks in Data Integration},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588938},
doi = {10.1145/3588938},
abstract = {Data matching - which decides whether two data elements (e.g., string, tuple, column, or knowledge graph entity) are the "same" (a.k.a. a match) - is a key concept in data integration, such as entity matching and schema matching. The widely used practice is to build task-specific or even dataset-specific solutions, which are hard to generalize and disable the opportunities of knowledge sharing that can be learned from different datasets and multiple tasks. In this paper, we propose Unicorn, a unified model for generally supporting common data matching tasks. Unicorn can enable knowledge sharing by learning from multiple tasks and multiple datasets, and can also support zero-shot prediction for new tasks with zero labeled matching/non-matching pairs. However, building such a unified model is challenging due to heterogeneous formats of input data elements and various matching semantics of multiple tasks. To address the challenges, Unicorn employs one generic Encoder that converts any pair of data elements (a, b) into a learned representation, and uses a Matcher, which is a binary classifier, to decide whether a matches b. To align matching semantics of multiple tasks, Unicorn adopts a mixture-of-experts model that enhances the learned representation into a better representation. We conduct extensive experiments using 20 datasets on seven well-studied data matching tasks, and find that our unified model can achieve better performance on most tasks and on average, compared with the state-of-the-art specific models trained for ad-hoc tasks and datasets separately. Moreover, Unicorn can also well serve new matching tasks with zero-shot learning.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {84},
numpages = {26},
keywords = {data integration, data matching, multi-task learning}
}

@inproceedings{10.1145/3688671.3688738,
author = {Papoutsoglou, Maria and Meditskos, Georgios and Bassiliades, Nick and Kontopoulos, Efstratios and Vrochidis, Stefanos},
title = {Mapping the Current Status of CTI Knowledge Graphs through a Bibliometric Analysis},
year = {2024},
isbn = {9798400709821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688671.3688738},
doi = {10.1145/3688671.3688738},
abstract = {Bibliometric analysis in the field of cybersecurity and Cyber Threat Intelligence (CTI) is crucial for identifying research trends, key themes, and collaborative networks, which can guide future research directions and policy decisions. This paper presents a comprehensive bibliometric analysis of the current status of research on knowledge graphs in cybersecurity, highlighting significant trends and thematic clusters. The analysis reveals a rapidly growing interest in integrating knowledge graphs with advanced machine learning and AI techniques, such as deep learning and neural networks, to enhance cyber threat intelligence and response strategies. Key findings include the prominence of natural language processing, entity recognition, and relation extraction as critical methodologies in this field. Thematic evolution analysis shows the adoption of large language models (LLMs) and an ongoing focus on structured knowledge representation. The study underscores the potential of knowledge graphs to improve cybersecurity through better data organization, threat detection, and intelligence extraction.},
booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence},
articleno = {42},
numpages = {6},
keywords = {Knowledge Graphs, CTI, Cybersecurity, Bibliometric Analysis},
location = {
},
series = {SETN '24}
}

@inproceedings{10.1145/3543882.3543891,
author = {Lewis, David and Shibata, Elisabete and Saccomano, Mark and Rosendahl, Lisa and Kepper, Johannes and Hankinson, Andrew and Siegert, Christine and Page, Kevin},
title = {A model for annotating musical versions and arrangements across multiple documents and media},
year = {2022},
isbn = {9781450396684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543882.3543891},
doi = {10.1145/3543882.3543891},
abstract = {We present a model for the annotation of musical works, where the annotations are created with respect to a conceptual abstraction of the music instead of directly to concrete encodings. This supports musicologists in constructing arguments about musical elements that occur in multiple digital library sources (or other web resources), that recur across a work, or that appear in different forms in different arrangements. It provides a way of discussing musical content without tying that discourse to the location, notation or medium of the content, allowing evidence from multiple libraries and in different formats to be brought together to support musicological assertions. This model is implemented in Linked Data and illustrated in a prototype application in which musicologists annotate vocal arrangements of the Allegretto from Beethoven’s Seventh Symphony from multiple sources.},
booktitle = {Proceedings of the 9th International Conference on Digital Libraries for Musicology},
pages = {10–18},
numpages = {9},
keywords = {FRBR, conceptual modelling, digital musicology, linked data, music arrangements},
location = {Prague, Czech Republic},
series = {DLfM '22}
}

@inproceedings{10.1145/3316615.3316685,
author = {Wang, Bo and Luo, Jun and Zhu, Shuyuan},
title = {Research on Domain Ontology Automation Construction Based on Chinese Texts},
year = {2019},
isbn = {9781450365734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3316615.3316685},
doi = {10.1145/3316615.3316685},
abstract = {The main construction method of the current ontology is to rely on ontology experts for manual construction. Because manual construction requires a lot of manual participation, manual construction has great limitations. Text data as one of the main forms of data source, how to construct domain ontology automatically from texts and how to provide semantic retrieval support to text quickly by ontology is the hotspot of ontology research at present. Aiming at the above problems, an automatic construction method of domain ontology based on knowledge graph and association rule mining is presented, and it can extract the concepts, hierarchies and non-hierarchies of domain ontology from text, and finally form ontology by Jena. It also provides semantic retrieval of text by associating text and concepts in the process of ontology construction. Finally, the effect of automatic ontology construction is verified by the effect of text retrieval.},
booktitle = {Proceedings of the 2019 8th International Conference on Software and Computer Applications},
pages = {425–430},
numpages = {6},
keywords = {Association Rule Mining, Jena, Knowledge Graph, Ontology Construction, RDF},
location = {Penang, Malaysia},
series = {ICSCA '19}
}

@article{10.1145/3210256,
author = {Florence, Spencer P. and Fetscher, Burke and Flatt, Matthew and Temps, William H. and St-Amour, Vincent and Kiguradze, Tina and West, Dennis P. and Niznik, Charlotte and Yarnold, Paul R. and Findler, Robert Bruce and Belknap, Steven M.},
title = {POP-PL: A Patient-Oriented Prescription Programming Language},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/3210256},
doi = {10.1145/3210256},
abstract = {A medical prescription is a set of health care instructions that govern the plan of care for an individual patient, which may include orders for drug therapy, diet, clinical assessment, and laboratory testing. Clinicians have long used algorithmic thinking to describe and implement prescriptions but without the benefit of a formal programming language. Instead, medical algorithms are expressed using a natural language patois, flowcharts, or as structured data in an electronic medical record system. The lack of a prescription programming language inhibits expressiveness; results in prescriptions that are difficult to understand, hard to debug, and awkward to reuse; and increases the risk of fatal medical error.This article reports on the design and evaluation of Patient-Oriented Prescription Programming Language (POP-PL), a domain-specific programming language designed for expressing prescriptions. The language is based around the idea that programs and humans have complementary strengths that, when combined properly, can make for safer, more accurate performance of prescriptions. Use of POP-PL facilitates automation of certain low-level vigilance tasks, freeing up human cognition for abstract thinking, compassion, and human communication.We implemented this language and evaluated its design attempting to write prescriptions in the new language and evaluated its usability by assessing whether clinicians can understand and modify prescriptions written in the language. We found that some medical prescriptions can be expressed in a formal domain-specific programming language, and we determined that medical professionals can understand and correctly modify programs written in POP-PL. We also discuss opportunities for refining and further developing POP-PL.},
journal = {ACM Trans. Program. Lang. Syst.},
month = jul,
articleno = {10},
numpages = {37},
keywords = {DSL design, empirical evaluation, medical prescriptions, medical programming languages}
}

@inproceedings{10.1145/3314074.3314091,
author = {Bekkali, Mohammed and Lachkar, Abdelmonaime},
title = {Arabic Sentiment Analysis based on Topic Modeling},
year = {2019},
isbn = {9781450361293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314074.3314091},
doi = {10.1145/3314074.3314091},
abstract = {Users of social media generate a huge volume of reviews and comments. These reviews and comments express user's opinions about different topics. As a result, there is a great need to understand and classify these reviews. Sentiment Analysis Systems is a good way to overcome this problem. Reviews are considered as short texts and they are different from traditional documents without enough contextual information. To address this issue, we propose an efficient representation for short text based on concepts instead of terms, which transforms the data representation into a shorter, more compact, and more predictive one. However, for the Arabic language, the majority of semantic resources are incomplete projects; this may presents a serious problem about the coverage ratio of the Arabic language compared with other Languages. To overcome this problem and starting with the assumption that terms belonging to same topic share many semantic links in the same dataset, their corresponding concepts will share the same semantics links in the same dataset. We suggest integrating Topic Modeling as a tool to bring together terms with the same semantic links. The proposed method has been tested and evaluated using the Large Scale Arabic Book Reviews Dataset and the obtained results illustrate the interest and efficiency of our contribution.},
booktitle = {Proceedings of the New Challenges in Data Sciences: Acts of the Second Conference of the Moroccan Classification Society},
articleno = {17},
numpages = {6},
keywords = {Arabic Language, Conceptualization, LDA, Sentiment Analysis, Short Text Representation, Topic Modeling},
location = {Kenitra, Morocco},
series = {SMC '19}
}

@inproceedings{10.1145/3305160.3305212,
author = {Selvaraj, Suganya and Choi, Eunmi},
title = {A Study on Traditional Medicine Ontology},
year = {2019},
isbn = {9781450366427},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3305160.3305212},
doi = {10.1145/3305160.3305212},
abstract = {The traditional medical field needs to be studied more for applying compound reasoning using ontology because traditional medicine treats the patients by finding root cause of the symptoms rather than treating for the symptoms directly. Many countries have their own traditional medicines like traditional Chinese medicine, traditional Korean medicine, Siddha, Ayurveda and Unani. Already many projects are started to work on the ontology development for traditional medicines. In this paper, we analyze and summarize the existing medical ontology researches. This study is useful to understand the existing medical ontology system and also provide an idea to develop and enhance the traditional medicine ontology by reusing existing resources. In this study, we also propose the ontology-based medical support system to predict the seasonal diseases by combining medical ontology with Big data analysis.},
booktitle = {Proceedings of the 2nd International Conference on Software Engineering and Information Management},
pages = {235–239},
numpages = {5},
keywords = {Medical ontology study, Ontology-based medical support system, Traditional medicine ontology},
location = {Bali, Indonesia},
series = {ICSIM '19}
}

@inproceedings{10.1145/3276604.3276610,
author = {de Lara, Juan and Guerra, Esther and Kienzle, J\"{o}rg and Hattab, Yanis},
title = {Facet-oriented modelling: open objects for model-driven engineering},
year = {2018},
isbn = {9781450360296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276604.3276610},
doi = {10.1145/3276604.3276610},
abstract = {Model-driven engineering (MDE) promotes models as the principal assets in software projects. Models are built using a modelling language whose syntax is defined by a metamodel. Hence, objects in models are typed by a metamodel class, and this typing relation is static as it is established at creation time and cannot be changed later. This way, objects in MDE are closed and fixed with respect to the type they conform to, the slots/properties they have, and the constraints they should obey. This hampers the reuse of model-related artefacts like model transformations, as well as the opportunistic or dynamic combination of metamodels.  To alleviate this rigidity, we propose making model objects open so that they can acquire or drop so-called facets, each one contributing a type, slots and constraints to the object. Facets are defined by regular metamodels, hence being a lightweight extension of standard metamodelling. Facet metamodels may declare usage interfaces, and it is possible to specify laws that govern how facets are to be assigned to the instances of a metamodel. In this paper, we describe our proposal, report on an implementation, and illustrate scenarios where facets have advantages over other techniques.},
booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {147–159},
numpages = {13},
keywords = {Flexible Modelling, MetaDepth, Metamodelling, Model-Driven Engineering, Reuse, Role-Based Modelling},
location = {Boston, MA, USA},
series = {SLE 2018}
}

@inproceedings{10.1145/3417990.3421412,
author = {K\"{u}hne, Thomas and Lange, Arne},
title = {Meaningful metrics for multi-level modelling},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421412},
doi = {10.1145/3417990.3421412},
abstract = {One of the key enablers of further growth of multi-level modeling will be the development of objective ways to allow multi-level modeling approaches to be compared to one another and to two-level modeling approaches. While significant strides have been made regarding qualitative comparisons, there is currently no adequate way to quantitatively assess to what extent a multi-level model may be preferable over another model with respect to high-level qualities such as understandability, maintainability, and control capacity. In this paper, we propose deep metrics, as an approach to quantitatively measure high-level model concerns of multi-level models that are of interest to certain stakeholders. Beyond the stated goals, we see deep metrics as furthermore supporting the comparison of modeling styles and aiding modelers in making individual design decisions. We discuss what makes a metric "depth-aware" so that it can appropriately capture multi-level model properties, and present two concrete proposals for metrics that measure high-level multi-level model qualities.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {85},
numpages = {9},
keywords = {metrics, model comparison, multi-level modeling},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3302425.3302482,
author = {Sowah, Edmund and Xu, Jianqiu},
title = {Edgebase: A Cooperative Query Answering Database System With A Natural Language Interface},
year = {2018},
isbn = {9781450366250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302425.3302482},
doi = {10.1145/3302425.3302482},
abstract = {Traditional Database Management Systems (DBMS) require users to meticulously construct and submit queries to generate answers. The lack of query syntax flexibility in traditional database systems make results in simple and direct answers - queries retrieve precisely matched elements stated in the given Boolean query. In this paper, we propose a Cooperative Query Answering Database System (CDBS) that provide answers to user queries in the same manner as humans do, and not as machines.The method of "Cooperative Query Answering (CQA)", emanated from the perception that to provide adequate and "complete" answers to queries, recognition of users' intentions is vital. Most database systems require users to submit their queries using SQL syntax. In addition to presenting answers to queries in human-like manner, we present a cooperative approach to query submission. By this, we present an architecture that combines the rich features of html, Natural Language (NL) with Query-By-Form (QBF) method and MySQL to enable our proposed system accept user queries in plain English language.To authenticate our approach and proposed system, a set of thorough experiments were conducted on two database systems using mysqlslap benchmark and a comparative study with other methods is done.},
booktitle = {Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence},
articleno = {71},
numpages = {8},
keywords = {Cooperative Query Answering, Natural Language, Query Language, Query syntax, Query-By-Form},
location = {Sanya, China},
series = {ACAI '18}
}

@inproceedings{10.1145/3377170.3377274,
author = {Pattar, Santosh and Sandhya, C. R. and Vala, Darshil and Buyya, Rajkumar and Venugopal, K. R. and Iyenger, S. S. and Patnaik, L. M.},
title = {SoCo-ITS: Service Oriented Context Ontology for Intelligent Transport System},
year = {2020},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377170.3377274},
doi = {10.1145/3377170.3377274},
abstract = {Intelligent Transport System (ITS) is a culmination of technological and application systems that are contrived to improve the performance of road transportation and upgrade the commuter's experience. The integration of Internet of Things (IoT) with the transport system has contributed to the development of ITS. In this paper, we concentrate on the commercial servitization standpoint of the application. We structure and formulate an ontology called Service-Oriented Context Ontology for Intelligent Transport System: SoCo-ITS. This ontological framework abets in identifying appropriate services required by the commuters in transit based on their situation, predilection and ITS environmental information. We discuss the detailed implementation description and also accentuate its role in ITS through a use case scenario and an exemplar application portraying the importance of the proposed ontological model.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {503–508},
numpages = {6},
keywords = {Intelligent Transport Systems, Internet of Things, Ontology, Service Discovery, User Centric Services},
location = {Shanghai, China},
series = {ICIT '19}
}

@inproceedings{10.1145/3194658.3194680,
author = {Alfaifi, Yousef and Grasso, Floriana and Tamma, Valentina},
title = {An Ontology of Psychological Barriers to Support Behaviour Change},
year = {2018},
isbn = {9781450364935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194658.3194680},
doi = {10.1145/3194658.3194680},
abstract = {Helping people to adopt and maintain healthier lifestyles is a primary goal of behaviour change interventions. Successful interventions need to account for different barriers (informational, environmental, or psychological) that prevent people from engaging in healthy behaviours. Computational approaches to modelling these interventions focus primarily on informational needs, or on persuasive techniques. The study presented in this paper is specifically aimed at creating a formal conceptual model of the psychological notion of barriers to healthy behaviour, by means of an ontology,i.e. an explicit and machine readable specification of a conceptualisation shared by all the stakeholders~citeStuder-et-al98. The model accounts for other related patient concepts to understand patient behaviour better. This machine-readable knowledge can function as a background to finding the right interventions for behaviour change. Whilst the model is generic and expandable to include other diseases and behaviours, our study uses type 2 diabetes to contextualise the problem of behaviour change.},
booktitle = {Proceedings of the 2018 International Conference on Digital Health},
pages = {11–15},
numpages = {5},
keywords = {type 2 diabetes, physical activity behaviour, behaviour ontology, behaviour change ontology},
location = {Lyon, France},
series = {DH '18}
}

@inproceedings{10.1145/3622758.3622894,
author = {Wilczynski, Peter and Gregoire-Wright, Taylor and Jackson, Daniel},
title = {Concept-Centric Software Development: An Experience Report},
year = {2023},
isbn = {9798400703881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3622758.3622894},
doi = {10.1145/3622758.3622894},
abstract = {Developers have long recognized the importance of the concepts underlying the systems they build, and the primary role that concepts play in shaping user experience. To date, however, concepts have tended to be only implicit in software design with development being organized instead around more concrete artifacts (such as wireframes and code modules).    Palantir, a software company whose data analytics products are widely used by major corporations, recently reworked the internal representation of its software development process to bring concepts to the fore, making explicit the concepts underlying its products, including how they are clustered together, used in applications, and governed by teams. With a centralized repository of concepts, Palantir engineers are able to align products more closely based on shared concepts, evolve concepts in response to user needs, and communicate more effectively with non-engineering groups within the company.    This paper reports on Palantir's experiences to date, analyzing both successes and challenges, and offers advice to other organizations considering adopting a concept-centric approach to software development.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {120–135},
numpages = {16},
keywords = {concepts, ontology, software design},
location = {Cascais, Portugal},
series = {Onward! 2023}
}

@article{10.1145/3593224,
author = {Zen, Mathieu and Burny, Nicolas and Vanderdonckt, Jean},
title = {A Quality Model-based Approach for Measuring User Interface Aesthetics with Grace},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {EICS},
url = {https://doi.org/10.1145/3593224},
doi = {10.1145/3593224},
abstract = {User interface aesthetics, a particular sub-characteristic of the ISO 25010 software quality model, is correlated to the perceived or actual usability of a graphical user interface, its user experience, and trust. While many measures, such as balance, symmetry, proportion, alignment, regularity, and simplicity, can be computed, no consensus exists today on which measure to adopt, which formula to compute for each measure, and which interpretation to give for each computed formula. To accommodate these variations and to make the assessment explicit and interpretable, we present a quality model-based approach for measuring aesthetics that defines a quality function in terms of the formula of the measures, their weights, their composition, and their overall computation. This quality model is transformed into a configuration used by GRACE, a web application developed for this purpose. When the measures, their formula, their weights, or the quality function change, the quality model changes, which is re-computed to compare it with any other model, thus making the measurement process explicit and interpretable. We apply this approach to a large dataset "Lab in the Wild'' to investigate the correlations between aesthetic measures and perceived visual complexity and colorfulness. We discuss limitations through threats to validity.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {172},
numpages = {47},
keywords = {aesthetics, graphical user interfaces, software measurement, usability engineering, user interface evaluation, visual measures, visual techniques}
}

@inproceedings{10.1145/3587259.3627557,
author = {Blin, In\`{e}s and Stork, Lise and Spillner, Laura and Santagiustina, Carlo},
title = {OKG: A Knowledge Graph for Fine-grained Understanding of Social Media Discourse on Inequality},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627557},
doi = {10.1145/3587259.3627557},
abstract = {In recent years, social media platforms such as Twitter have allowed people to voice their opinions by engaging in online discussions. The availability of such discussions has garnered interest amongst researchers in analyzing the dynamics on critical topics, such as inequality. Most of the current strategies are, however, limited with respect to conveying the fine-grained opinions of users, focusing on tasks such as sentiment analysis or topic modeling that extract coarse categorizations. In this work, we address this challenge by integrating a Twitter corpus with the output of finer-grained semantic parsing for the analysis of social media discourse. To do so, we first introduce the OBservatory Integrated Ontology (OBIO) that integrates social media metadata with various types of linguistic knowledge. We then present the Observatory Knowledge Graph (OKG), a knowledge graph in terms of the ontology, populated with tweets on inequality. We lastly provide use cases showing how the knowledge graph can be used as the backbone of a social media observatory, to facilitate a deeper understanding of social media discourse.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {166–174},
numpages = {9},
keywords = {Ontology Engineering and Population, Social Media Discourse},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3696410.3714581,
author = {Jradeh, Chlo\'{e} Khadija and Raoufi, Ensiyeh and David, J\'{e}r\^{o}me and Larmande, Pierre and Scharffe, Fran\c{c}ois and Todorov, Konstantin and Trojahn, Cassia},
title = {Graph Embeddings Meet Link Keys Discovery for Entity Matching},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714581},
doi = {10.1145/3696410.3714581},
abstract = {Entity Matching (EM) automates the discovery of identity links between entities within different Knowledge Graphs (KGs). Link keys are crucial for EM, serving as rules allowing to identify identity links across different KGs, possibly described using different ontologies. However, the approach for extracting link keys struggles to scale on large KGs. While embedding-based EM methods efficiently handle large KGs they lack explainability. This paper proposes a novel hybrid EM approach to guarantee the scalability link key extraction approach and improve the explainability of embedding-based EM methods. First, embedding-based EM approaches are used to sample the KGs based on the identity links they generate, thereby reducing the search space to relevant sub-graphs for link key extraction. Second, rules (in the form of link keys) are extracted to explain the generation of identity links by the embedding-based methods. Experimental results demonstrate that the proposed approach allows link key extraction to scale on large KGs, preserving the quality of the extracted link keys. Additionally, it shows that link keys can improve the explainability of the identity links generated by embedding-methods, allowing for the regeneration of 77\% of the identity links produced for a specific EM task, thereby providing an approximation of the reasons behind their generation.},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {3344–3353},
numpages = {10},
keywords = {embedding-based em, entity matching, graph embeddings, hybrid ai, knowledge graphs, language models, link keys, symbolic em},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3606703,
author = {Murano, Francesca and Quochi, Valeria and Del Grosso, Angelo Mario and Rigobianco, Luca and Zinzi, Mariarosaria},
title = {Describing Inscriptions of Ancient Italy. The ItAnt Project and Its Information Encoding Process},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3606703},
doi = {10.1145/3606703},
abstract = {This article discusses the challenges addressed in the digital scholarly encoding of the fragmentary texts of the languages of Ancient Italy according to the TEI/EpiDoc Guidelines in XML format. It describes the solutions and customisations that have been adopted for dealing with the peculiarities of our epigraphical documentation and with the formalisation of epigraphical information deemed interesting for data retrieval in a historical linguistic perspective. The making of a digital corpus consisting of new critical editions of selected inscriptions is a work carried out in the context of the project “Languages and Cultures of Ancient Italy. Historical Linguistics and Digital Models”, which aims to investigate the languages of Ancient Italy by combining the traditional methods, proper to historical linguistics, with methods and technology proper to the digital humanities and computational lexicography. More specifically, the purpose of the project is to create a set of interrelated digital language resources which comprise: (1) a digital corpus of texts editions; (2) a computational lexicon compliant with the Web Semantic requirements; (3) a relevant bibliographic reference dataset encoded according to the FRBRoo/LRMoo specifications. Additionally, selected textual data and scientific interpretations will be encoded using CIDOC CRM and its extensions, namely CRMtex and CRMinf. The present contribution thus tackles one of the main aspects of the project, and proposes significant innovations in the encoding of critical editions for epigraphic texts of fragmentary languages, which will hopefully foster future interoperability and integration with other external datasets, a paramount concern of the project.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {53},
numpages = {14},
keywords = {Text encoding, ancient languages, digital epigraphy, TEI/EpiDoc}
}

@inproceedings{10.1145/3477314.3507031,
author = {Liu, Xinglan and Hussain, Hussain and Razouk, Houssam and Kern, Roman},
title = {Effective use of BERT in graph embeddings for sparse knowledge graph completion},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507031},
doi = {10.1145/3477314.3507031},
abstract = {Graph embedding methods have emerged as effective solutions for knowledge graph completion. However, such methods are typically tested on benchmark datasets such as Freebase, but show limited performance when applied on sparse knowledge graphs with orders of magnitude lower density. To compensate for the lack of structure in a sparse graph, low dimensional representations of textual information such as word2vec or BERT embeddings have been used. This paper proposes a BERT-based method (BERT-ConvE), to exploit transfer learning of BERT in combination with a convolutional network model ConvE. Comparing to existing text-aware approaches, we effectively make use of the context dependency of BERT embeddings through optimizing the features extraction strategies. Experiments on ConceptNet show that the proposed method outperforms strong baselines by 50\% on knowledge graph completion tasks. The proposed method is suitable for sparse graphs as also demonstrated by empirical studies on ATOMIC and sparsified-FB15k-237 datasets. Its effectiveness and simplicity make it appealing for industrial applications.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {799–802},
numpages = {4},
keywords = {BERT, context aware embedding, knowledge graph embedding, language model, sparse knowledge graph},
location = {Virtual Event},
series = {SAC '22}
}

@article{10.1109/TCBB.2021.3083150,
author = {Paul, Madhusudan and Anand, Ashish},
title = {A New Family of Similarity Measures for Scoring Confidence of Protein Interactions Using Gene Ontology},
year = {2021},
issue_date = {Jan.-Feb. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3083150},
doi = {10.1109/TCBB.2021.3083150},
abstract = {The large-scale protein-protein interaction (PPI) data has the potential to play a significant role in the endeavor of understanding cellular processes. However, the presence of a considerable fraction of false positives is a bottleneck in realizing this potential. There have been continuous efforts to utilize complementary resources for scoring confidence of PPIs in a manner that false positive interactions get a low confidence score. Gene Ontology (GO), a taxonomy of biological terms to represent the properties of gene products and their relations, has been widely used for this purpose. We utilize GO to introduce a new set of specificity measures: Relative Depth Specificity (RDS), Relative Node-based Specificity (RNS), and Relative Edge-based Specificity (RES), leading to a new family of similarity measures. We use these similarity measures to obtain a confidence score for each PPI. We evaluate the new measures using four different benchmarks. We show that all the three measures are quite effective. Notably, RNS and RES more effectively distinguish true PPIs from false positives than the existing alternatives. RES also shows a robust set-discriminating power and can be useful for protein functional clustering as well.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {19–30},
numpages = {12}
}

@inproceedings{10.1145/3386723.3387874,
author = {El-Ansari, Anas and Beni-Hssane, Abderrahim and Saadi, Mostafa},
title = {An improved modeling method for profile-based personalized search},
year = {2020},
isbn = {9781450376341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386723.3387874},
doi = {10.1145/3386723.3387874},
abstract = {A Personalized search system aims to provide tailor-made results for each user's query according to his preferences. Building such a system depends mainly on creating profiles that represent real user interests. In this paper, we present a method for modeling users and creating accurate profiles by implicitly tracking and collecting user-browsing data. Furthermore, we examine techniques to enhance profile accuracy through combining multiple browsing data sources, distinguishing important concepts from irrelevant ones in a user profile, and the concept levels required from a reference ontology to describe user's interests.},
booktitle = {Proceedings of the 3rd International Conference on Networking, Information Systems \&amp; Security},
articleno = {55},
numpages = {6},
keywords = {Accuracy, Ontology, Personalized Search, User profile},
location = {Marrakech, Morocco},
series = {NISS '20}
}

@article{10.1145/3557894,
author = {Liao, Junwei and Eskimez, Sefik and Lu, Liyang and Shi, Yu and Gong, Ming and Shou, Linjun and Qu, Hong and Zeng, Michael},
title = {Improving Readability for Automatic Speech Recognition Transcription},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3557894},
doi = {10.1145/3557894},
abstract = {Modern Automatic Speech Recognition (ASR) systems can achieve high performance in terms of recognition accuracy. However, a perfectly accurate transcript still can be challenging to read due to grammatical errors, disfluency, and other noises common in spoken communication. These readable issues introduced by speakers and ASR systems will impair the performance of downstream tasks and the understanding of human readers. In this work, we present a task called ASR post-processing for readability (APR) and formulate it as a sequence-to-sequence text generation problem. The APR task aims to transform the noisy ASR output into a readable text for humans and downstream tasks while maintaining the semantic meaning of speakers. We further study the APR task from the benchmark dataset, evaluation metrics, and baseline models: First, to address the lack of task-specific data, we propose a method to construct a dataset for the APR task by using the data collected for grammatical error correction. Second, we utilize metrics adapted or borrowed from similar tasks to evaluate model performance on the APR task. Lastly, we use several typical or adapted pre-trained models as the baseline models for the APR task. Furthermore, we fine-tune the baseline models on the constructed dataset and compare their performance with a traditional pipeline method in terms of proposed evaluation metrics. Experimental results show that all the fine-tuned baseline models perform better than the traditional pipeline method, and our adapted RoBERTa model outperforms the pipeline method by 4.95 and 6.63 BLEU points on two test sets, respectively. The human evaluation and case study further reveal the ability of the proposed model to improve the readability of ASR transcripts.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {142},
numpages = {23},
keywords = {Automatic speech recognition, post-processing for readability, data synthesis, pre-trained model}
}

@inproceedings{10.1145/3239372.3239376,
author = {Guerra, Esther and de Lara, Juan},
title = {On the Quest for Flexible Modelling},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239376},
doi = {10.1145/3239372.3239376},
abstract = {Modelling is a fundamental activity in Software Engineering, and central to model-based engineering approaches. It is used for different purposes, and so its nature can range from informal (e.g., as a casual mechanism for problem discussion and understanding) to fully formal (e.g., to enable the automated processing of models by model transformations). However, existing modelling tools only serve one of these two extreme purposes: either to create informal drawings or diagrams, or to build models fully conformant to their modelling language. This lack of reconciliation is hampering the adoption of model-based techniques in practice, as they are deemed too imprecise in the former case, and too rigid in the latter.In this new ideas paper, we claim that modelling tools need further flexibility covering different stages, purposes and approaches to modelling. We detail requirements for such a new generation of modelling tools, describe our first steps towards their realization in the Kite metamodelling tool, and showcase application scenarios.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {23–33},
numpages = {11},
keywords = {Flexible modelling, Model-driven engineering, Modelling process},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@inproceedings{10.1145/3417990.3419503,
author = {Reynolds, Owen and Garc\'{\i}a-Dom\'{\i}nguez, Antonio and Bencomo, Nelly},
title = {Automated provenance graphs for models@run.time},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3419503},
doi = {10.1145/3417990.3419503},
abstract = {Software systems are increasingly making decisions autonomously by incorporating AI and machine learning capabilities. These systems are known as self-adaptive and autonomous systems (SAS). Some of these decisions can have a life-changing impact on the people involved and therefore, they need to be appropriately tracked and justified: the system should not be taken as a black box. It is required to be able to have knowledge about past events and records of history of the decision making. However, tracking everything that was going on in the system at the time a decision was made may be unfeasible, due to resource constraints and complexity. In this paper, we propose an approach that combines the abstraction and reasoning support offered by models used at runtime with provenance graphs that capture the key decisions made by a system through its execution. Provenance graphs relate the entities, actors and activities that take place in the system over time, allowing for tracing the reasons why the system reached its current state. We introduce activity scopes, which highlight the high-level activities taking place for each decision, and reduce the cost of instrumenting a system to automatically produce provenance graphs of these decisions. We demonstrate a proof of concept implementation of our proposal across two case studies, and present a roadmap towards a reusable provenance layer based on the experiments.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {53},
numpages = {10},
keywords = {PROV-DM, autonomous decision-making, provenance, runtime models, self-explanation},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3194480.3194490,
author = {Gao, Mingxia and Chen, Furong and Wang, Rifeng},
title = {Improving Medical Ontology Based on Word Embedding},
year = {2018},
isbn = {9781450363488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194480.3194490},
doi = {10.1145/3194480.3194490},
abstract = {Medical ontology learning or improving is automatically learning the knowledge in ontology format from medical data, mainly text data. With the rise of the word vector space, improving ontology using word embedding has become a hot spot. Most of previous studies have focused on how to acquire different ontological elements using all kinds of learning technologies. Few studies focus on the prior knowledge in a given ontology. In essence, ontology learning or improving is still a learning process based on existing samples. So, the type and number of knowledge acquired is limited by existing samples in a given ontology. This paper firstly formalizes several kinds of prior knowledge for classes in a given ontology. Then we propose a method, named improving medical ontology based on word embeddings (IMO-WE), to enrich different types of knowledge from medical text according to characteristics of different types of prior knowledge. At last, the paper collects the PubMed Central (PMC) data and the PHARE ontology, and finishes a series of experiments to evaluate the IMO-WE. The experimental results yield the following conclusions. The first one is that the data-rich model can achieve higher accuracy for the IMO-WE under same setting in training progress. So, collecting and training big medical data is a viable way to learn more useful knowledge. The second one is that the IMO-WE can be used to improving ontology knowledge when medical data is sufficiently abundant and the ontology has appropriate prior knowledge. Moreover, in the task of improving synonymous labels through similarity distance, the accuracy of IMO-WE is significantly better than that of the Random indexing method.},
booktitle = {Proceedings of the 2018 6th International Conference on Bioinformatics and Computational Biology},
pages = {121–127},
numpages = {7},
keywords = {medical ontology improving, prior knowledge, word embedding},
location = {Chengdu, China},
series = {ICBCB 2018}
}

@inproceedings{10.1145/3230905.3230933,
author = {Bouihi, Bouchra and Bahaj, Mohamed},
title = {Moodle's Ontology Development from UML for Social Learning Network Analysis},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230933},
doi = {10.1145/3230905.3230933},
abstract = {The online learning called e-learning is a new learning path that offers to learners to study at their own pace and at the moments that suit them. It is in this perspective that the semantic web has known its emergence in the field of e-learning to offer platforms content more personalized and more adapted to student's and teacher's needs. Since Moodle is the most popular e-learning platform, we propose in this paper to build its OWL ontology by exploring the representative data that we collected from its UML class diagram. The choice of UML class diagram as a basis for data collection for the development of the ontology is justified by the fact that the transition from UML to OWL ontology brings ontology development process closer to the wider software engineering population. The built ontology brings also great benefit in the field of the Social Learning Network Analysis. Because it gives the opportunity to study the behavior of the platform users by giving meaning to their relationships instead of modelling them only as knots and edges.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {41},
numpages = {6},
keywords = {E-learning, Moodle, OWL, Ontology, Semantic web, Social Network Analysis, UML},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@article{10.1613/jair.1.12414,
author = {De Giacomo, Giuseppe and Oriol, Xavier and Rosati, Riccardo and Savo, Domenico Fabio},
title = {Instance-Level Update in DL-Lite Ontologies through First-Order Rewriting},
year = {2021},
issue_date = {May 2021},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {70},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.12414},
doi = {10.1613/jair.1.12414},
abstract = {In this paper we study instance-level update in DL-LiteA , a well-known description logic that influenced the OWL 2 QL standard. Instance-level update regards insertions and deletions in the ABox of an ontology. In particular we focus on formula-based approaches to instance-level update. We show that DL-LiteA , which is well-known for enjoying first-order rewritability of query answering, enjoys a first-order rewritability property also for instance-level update. That is, every update can be reformulated into a set of insertion and deletion instructions computable through a non-recursive Datalog program with negation. Such a program is readily translatable into a first-order query over the ABox considered as a database, and hence into SQL. By exploiting this result, we implement an update component for DL-LiteA-based systems and perform some experiments showing that the approach works in practice.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {1335–1371},
numpages = {37}
}

@inproceedings{10.1145/3477314.3507164,
author = {Corradini, Flavio and Fedeli, Arianna and Fornari, Fabrizio and Polini, Andrea and Re, Barbara},
title = {X-IoT: a model-driven approach for cross-platform IoT applications development},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507164},
doi = {10.1145/3477314.3507164},
abstract = {Several heterogeneous IoT platforms have been proposed and regularly used by enterprises and academies to support and facilitate IoT software applications development. However, IoT applications strongly depend on the functionalities supported by the specific platform used. This affects the development and portability of the developed applications that may require significant changes, or a complete re-design, for being migrated between platforms. This paper presents X-IoT, an MDE approach for developing cross-platform IoT applications. The approach implements a Domain-Specific Modelling Language (DSML) based on emerging IoT application requirements. A meta-model that incorporates the main IoT platform characteristics has been developed within the ADOxx platform, together with a graphical notation. Through the DSML, it is possible to model a platform-independent model of IoT applications that are refined and deployed on specific IoT platforms.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1448–1451},
numpages = {4},
keywords = {IoT application development, IoT platform, cross-platform, internet of things, model driven engineering},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3438872.3439101,
author = {Yang, JinXiong and Bai, Liang and Guo, Yanming},
title = {A survey of text classification models},
year = {2020},
isbn = {9781450388306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3438872.3439101},
doi = {10.1145/3438872.3439101},
abstract = {With the rapid development of artificial intelligence, text classification method based on deep learning model has surpassed traditional machine learning method in various aspects. This paper introduces dozens of deep learning models for text classification according to the different network structures of the models. In addition, this paper briefly introduces the evaluation indicators and application scenarios of text classification, summarizes and forecasts the current challenges and future development trend of text classification.},
booktitle = {Proceedings of the 2020 2nd International Conference on Robotics, Intelligent Control and Artificial Intelligence},
pages = {327–334},
numpages = {8},
keywords = {Deep learning, Model, Text classification},
location = {Shanghai, China},
series = {RICAI '20}
}

@inproceedings{10.1145/3320326.3320369,
author = {El Orche, Ahmed and Bahaj, Mohamed},
title = {Approach to use ontology based on electronic payment system and machine learning to prevent Fraud},
year = {2019},
isbn = {9781450366458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320326.3320369},
doi = {10.1145/3320326.3320369},
abstract = {Machine learning is a field of study of artificial intelligence that is based on statistical approaches to give computers the ability to learn from data. An ontology is the structured set of terms and concepts representing the meaning of a field of information, whether by the metadata of a namespace, or the elements of a domain of knowledge. in this paper we propose an approach to combine a machine learning with an ontology based on an electronic payment system and another ontology generated automatically to prevent and fight the risks of fraud.},
booktitle = {Proceedings of the 2nd International Conference on Networking, Information Systems \&amp; Security},
articleno = {37},
numpages = {6},
keywords = {Electronic payment system, Fraud, Machine learning, Ontology},
location = {Rabat, Morocco},
series = {NISS '19}
}

@inproceedings{10.1145/3368756.3369092,
author = {Kaoutar, Lamrani and Abderrahim, Ghadi},
title = {Global similarity based ontology to enhance the quality of big and distributed RDF data},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369092},
doi = {10.1145/3368756.3369092},
abstract = {Nowadays, the web content of e-commerce data is increasing rapidly, which make the traditional techniques to querying this resources not efficient, for that the researches focus to how using the new technologies to provide a relevant and complete answers to user query. Using Technologies of big data and web semantic are two new fields that can be exploiting to processes data semantically and to handle with storage of this hug data.In recent works [1, 2], we have proposed the techniques using in big data and we are proposed in architecture that integrate the big RDF (Resources Description Framework) data semantically by exploiting HDFS (Hadoop Distributed File System) to store Global RDF schema and Map Reduce to process the query, in aims to give an infrastructure who give a complete and pertinent answers to user query. In this paper we are proposed a simple scenario to have a complete and pertinent response to user query.},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {105},
numpages = {4},
keywords = {big data, ontology, semantic web},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@inproceedings{10.1145/3487553.3524675,
author = {Turki, Houcemeddine and Hadj Taieb, Mohamed Ali and Piad-Morffis, Alejandro and Ben Aouicha, Mohamed and Bile, Ren\'{e} Fabrice},
title = {Data Models for Annotating Biomedical Scholarly Publications: the Case of CORD-19},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524675},
doi = {10.1145/3487553.3524675},
abstract = {Semantic text annotations have been a key factor for supporting computer applications ranging from knowledge graph construction to biomedical question answering. In this systematic review, we provide an analysis of the data models that have been applied to semantic annotation projects for the scholarly publications available in the CORD-19 dataset, an open database of the full texts of scholarly publications about COVID-19. Based on Google Scholar and the screening of specific research venues, we retrieve seventeen publications on the topic mostly from the United States of America. Subsequently, we outline and explain the inline semantic annotation models currently applied on the full texts of biomedical scholarly publications. Then, we discuss the data models currently used with reference to semantic annotation projects on the CORD-19 dataset to provide interesting directions for the development of semantic annotation models and projects.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {740–750},
numpages = {11},
keywords = {Annotation models, CORD-19, Named entity annotation, Semantic annotations, Semantic relation annotation, Semantic relations},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3378936.3378976,
author = {Selvaraj, Suganya and Choi, Eunmi},
title = {TKM Ontology Integration and Visualization},
year = {2020},
isbn = {9781450376907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378936.3378976},
doi = {10.1145/3378936.3378976},
abstract = {Ontology is the most efficient way of representing knowledge and influence relationship about diseases, symptoms, medications, and diagnosis in the traditional medical field. Since integrating traditional medicine ontologies with modern medical ontologies can benefit to the treatment process for effectively using traditional medicines, there is a need for integration and visualization of traditional medicine ontology. Furthermore, an effective ontology visualization is useful to design, manage, and browse the traditional medicine ontology successfully. In this paper, we construct the traditional Korean medicine (TKM) ontology using TKM domain knowledge with a few imported classes from modern medicine ontology and traditional Chinese medicine ontology (TCM) and also visualize the TKM using Web-based Visualization of Ontologies (WebVowl).},
booktitle = {Proceedings of the 3rd International Conference on Software Engineering and Information Management},
pages = {146–149},
numpages = {4},
keywords = {TKM ontology integration, Traditional Korean medicine ontology, medical ontology virtualization},
location = {Sydney, NSW, Australia},
series = {ICSIM '20}
}

@inproceedings{10.1145/3178248.3178253,
author = {Kaar, Claudia and Frysak, Josef and Stary, Christian and Kannengiesser, Udo and M\"{u}ller, Harald},
title = {Resilient Ontology Support Facilitating Multi-Perspective Process Integration in Industry 4.0},
year = {2018},
isbn = {9781450353601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178248.3178253},
doi = {10.1145/3178248.3178253},
abstract = {A major challenge for Industry 4.0 organizations is the mutual alignment of automation and information technology while increasing effectiveness and agility of processes. From a technological view, it requires architectures and systems coupling heterogeneous technologies, from an operations perspective, it requires context-sensitive representations. Ontologies do not only support alignment, but also integration and development processes. For the introduced ontology we utilize the multi-perspective RAMI4.0 framework, as it provides several layers and perspectives, including production and business processes. We suggest using Subject-oriented Business Process Management (S-BPM) models to represent executable processes, as they allow encapsulating industry standard-conform as well as stakeholder behavior. Thereby, the ontology backs perspective specific knowledge, and can be adapted as semantic baseline in a flexible way.},
booktitle = {Proceedings of the 10th International Conference on Subject-Oriented Business Process Management},
articleno = {9},
numpages = {10},
keywords = {Industry 4.0, Ontology engineering, RAMI4.0, process integration, resilience, semantic interoperability},
location = {Linz, Austria},
series = {S-BPM One '18}
}

@inproceedings{10.1145/3322134.3322143,
author = {Demchenko, Yuri and Comminiello, Luca and Reali, Gianluca},
title = {Designing Customisable Data Science Curriculum Using Ontology for Data Science Competences and Body of Knowledge},
year = {2019},
isbn = {9781450361866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322134.3322143},
doi = {10.1145/3322134.3322143},
abstract = {Importance of Data Science education and training is growing with the emergence of data driven technologies and organisational culture that intend to derive actionable value for improving research process or enterprise business using variety of enterprise data and widely available open and social media data. Modern data driven research and industry require new types of specialists that are capable to support all stages of the data lifecycle from data production and input to data processing and actionable results delivery, visualisation and reporting, which can be jointly defined as the Data Science professions family. The education and training of Data Scientists requires multi-disciplinary approach combining wide view of the Data Science and Analytics foundation with deep practical knowledge in domain specific areas. In modern conditions with the fast technology change and strong skills demand, the Data Science education and training should be customizable and delivered in multiple form, also providing sufficient data labs facilities for practical training. This paper discusses approach to building customizable Data Science curriculum for different types of learners based on using the ontology of the EDISON Data Science Framework (EDSF) developed in the EU funded Project EDISON and widely used by universities and professional training organisations.},
booktitle = {Proceedings of the 2019 International Conference on Big Data and Education},
pages = {124–128},
numpages = {5},
keywords = {EDISON Data Science Framework (EDSF), Data Scientist Professional, Data Science Ontology, Data Science Model Curriculum, Data Science Competences Framework, Data Science Body of Knowledge, Data Science, Big Data},
location = {London, United Kingdom},
series = {ICBDE '19}
}

@article{10.1145/3746281,
author = {Hang, Tingting and Liu, Shuting and Feng, Jun and Djigal, Hamza and Huang, Jun},
title = {Few-Shot Relation Extraction Based on Prompt Learning: A Taxonomy, Survey, Challenges and Future Directions},
year = {2025},
issue_date = {January 2026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3746281},
doi = {10.1145/3746281},
abstract = {Relation extraction (RE) is critical in information extraction (IE) and knowledge graph construction. RE aims to identify the semantic relations between entities from natural language texts. Traditional RE models often rely on many manually annotated training samples, which are limited when data is scarce. Therefore, exploring how to perform RE under few-shot conditions has become a research focus. Recently, prompt learning has attracted attention from researchers due to its ability to fully activate the potential of Pre-trained Language Models (PLMs), especially making significant progress in Few-Shot RE (FSRE). This article comprehensively reviews FSRE based on prompt learning. We first introduce the fundamental concepts of FSRE and prompt learning. Then, we systematically review recent research advances in FSRE with prompt learning, focusing on two perspectives: template construction and model fine-tuning strategies. Next, we summarize the benchmark datasets, evaluation metrics, and experimental results of representative works in FSRE. Afterward, we present practical applications of prompt-based FSRE in specialized domains. Finally, we discuss the critical challenges and future research directions of FSRE tasks based on prompt learning.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {40},
numpages = {38},
keywords = {Few-shot relation extraction, prompt learning, template construction, model fine-tuning strategies}
}

@inproceedings{10.1145/3368691.3372391,
author = {Zulfiya, Kaderkeyeva and Gulmira, Bekmanova and Altynbek, Sharipbay and Assel, Omarbekova},
title = {A model and a method for assessing students' competencies in e-learning system},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3372391},
doi = {10.1145/3368691.3372391},
abstract = {This article discusses a model and a method for assessing students' competencies in e-learning system, verification of fulfillment of the educational program goals and formation of a graduate with competencies set as a goal at the entrance. It also offers assessment at the level of a discipline, a module and verification of the achievement of the educational program goals.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {58},
numpages = {5},
keywords = {artificial intelligence, competencies, e-learning, evaluation, knowledge, knowledge base, knowledge models, logic, ontology, sets},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/3487553.3524701,
author = {Cuffy, Clint and French, Evan and Fehrmann, Sophia and McInnes, Bridget T.},
title = {Exploring Representations for Singular and Multi-Concept Relations for Biomedical Named Entity Normalization},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524701},
doi = {10.1145/3487553.3524701},
abstract = {Since the rise of the COVID-19 pandemic, peer-reviewed biomedical repositories have experienced a surge in chemical and disease related queries. These queries have a wide variety of naming conventions and nomenclatures from trademark and generic, to chemical composition mentions. Normalizing or disambiguating these mentions within texts provides researchers and data-curators with more relevant articles returned by their search query. Named entity normalization aims to automate this disambiguation process by linking entity mentions onto their appropriate candidate concepts within a biomedical knowledge base or ontology. We explore several term embedding aggregation techniques in addition to how the term’s context affects evaluation performance. We also evaluate our embedding approaches for normalizing term instances containing one or many relations within unstructured texts.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {823–832},
numpages = {10},
keywords = {MeSH identifier, concept linking, concept mapping, concept normalization, concept unique identifier, datasets, entity linking, entity normalization, named entity disambiguation, named entity linking, named entity normalization, neural networks, transformer, word embeddings},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@article{10.1145/3476106,
author = {Zhao, Jiashu and Huang, Jimmy Xiangji and Deng, Hongbo and Chang, Yi and Xia, Long},
title = {Are Topics Interesting or Not? An LDA-based Topic-graph Probabilistic Model for Web Search Personalization},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {3},
issn = {1046-8188},
url = {https://doi.org/10.1145/3476106},
doi = {10.1145/3476106},
abstract = {In this article, we propose a Latent Dirichlet Allocation– (LDA) based topic-graph probabilistic personalization model for Web search. This model represents a user graph in a latent topic graph and simultaneously estimates the probabilities that the user is interested in the topics, as well as the probabilities that the user is not interested in the topics. For a given query issued by the user, the webpages that have higher relevancy to the interested topics are promoted, and the webpages more relevant to the non-interesting topics are penalized. In particular, we simulate a user’s search intent by building two profiles: A positive user profile for the probabilities of the user is interested in the topics and a corresponding negative user profile for the probabilities of being not interested in the the topics. The profiles are estimated based on the user’s search logs. A clicked webpage is assumed to include interesting topics. A skipped (viewed but not clicked) webpage is assumed to cover some non-interesting topics to the user. Such estimations are performed in the latent topic space generated by LDA. Moreover, a new approach is proposed to estimate the correlation between a given query and the user’s search history so as to determine how much personalization should be considered for the query. We compare our proposed models with several strong baselines including state-of-the-art personalization approaches. Experiments conducted on a large-scale real user search log collection illustrate the effectiveness of the proposed models.},
journal = {ACM Trans. Inf. Syst.},
month = dec,
articleno = {51},
numpages = {24},
keywords = {Personalization, probabilistic model, Web search, Latent Dirichlet Allocation (LDA), topic-graph}
}

@inproceedings{10.1145/3534678.3539385,
author = {Yang, Puhai and Huang, Heyan and Wei, Wei and Mao, Xian-Ling},
title = {Toward Real-life Dialogue State Tracking Involving Negative Feedback Utterances},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539385},
doi = {10.1145/3534678.3539385},
abstract = {Recently, the research of dialogue systems has been widely concerned, especially task-oriented dialogue systems, which have received increased attention due to their wide application prospect. As a core component, dialogue state tracking (DST) plays a key role in task-oriented dialogue systems, and its function is to parse natural language dialogues into dialogue state formed by slot-value pairs. It is well known that dialogue state tracking has been well studied and explored on current benchmark datasets such as the MultiWOZ. However, almost all current research completely ignores the user negative feedback utterances that exist in real-life conversations when a system error occurs, which often contains user-provided corrective information for the system error. Obviously, user negative feedback utterances can be used to correct the inevitable errors in automatic speech recognition and model generalization. Thus, in this paper, we will explore the role of negative feedback utterances in dialogue state tracking in detail through simulated negative feedback utterances. Specifically, due to the lack of dataset involving negative feedback utterances, first, we have to define the schema of user negative feedback utterances and propose a joint modeling method for feedback utterance generation and filtering. Then, we explore three aspects of interaction mechanism that should be considered in real-life conversations involving negative feedback utterances and propose evaluation metrics related to negative feedback utterances. Finally, on WOZ2.0 and MultiWOZ2.1 datasets, by constructing simulated negative feedback utterances in training and testing, we not only verify the important role of negative feedback utterances in dialogue state tracking, but also analyze the advantages and disadvantages of different interaction mechanisms involving negative feedback utterances, lighting future research on negative feedback utterances.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {2222–2232},
numpages = {11},
keywords = {dialogue state tracking, negative feedback, real-life dialogue},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3535756.3535770,
author = {Xiao, Yao and Zhan, Zehui and Yuan, Man},
title = {Event Graph Construction Based on Disciplinary Procedural Knowledge: Concept Model and Application},
year = {2022},
isbn = {9781450396974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535756.3535770},
doi = {10.1145/3535756.3535770},
abstract = {Abstract: At present, procedural knowledge is mainly represented by production rules. Although this representation is concise, problems exist on lack of semantic elements and single semantic relationship, which is not conducive to the representation of procedural knowledge and hinders the teaching and application of procedural knowledge. Therefore, this paper firstly analyzes the semantic elements that procedural knowledge should have, and the lack of time and space elements in traditional representation methods. Combining with the cognitive background of procedural knowledge and introducing event cognition, the representation method of procedural knowledge graph and the construction process model are obtained. The verification proves that the event graph is a procedural knowledge representation method with complete semantic roles and rich semantic relations.},
booktitle = {Proceedings of the 8th International Conference on Education and Training Technologies},
pages = {85–91},
numpages = {7},
keywords = {Event Graph, Knowledge Model, Knowledge Representation, Procedural knowledge},
location = {Macau, China},
series = {ICETT '22}
}

@inbook{10.1145/3677389.3702611,
author = {Yang, Can},
title = {Knowledge Graph-Enhanced Artwork Image Captioning},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702611},
abstract = {This paper explores the unique challenges of artwork image captioning, a task that demands deep understanding of historical, cultural, and stylistic elements often absent in traditional image captioning. We conducted preliminary experiments using a Meshed Memory Transformer on the Iconclass AI Test Set, which revealed significant improvements in standard metrics but highlighted critical limitations in current datasets and evaluation methods. To address these issues, we propose a novel approach integrating knowledge graphs with large language models. This approach involves creating a specialized art ontology and knowledge graph, and developing new evaluation metrics specifically designed for artwork captioning. While not yet implemented, this proposed method aims to generate more comprehensive, contextually rich, and accurate captions for artwork images. Our research lays the groundwork for future advancements in artwork image captioning, potentially enhancing the accessibility and educational value of digital art collections.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {113},
numpages = {3}
}

@inproceedings{10.1145/3500931.3501011,
author = {Guo, Chaohui and Lin, Shaofu and Huang, Zhisheng and Yao, Yahong},
title = {Mental Health Question and Answering System Based on Bert Model and Knowledge Graph Technology},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3501011},
doi = {10.1145/3500931.3501011},
abstract = {With the development and progress of society, people are facing increasing pressure. The emergence of this phenomenon has led to a rapid increase in the incidence of mental illness. In order to deal with this phenomenon, this paper proposes a system of question and answering on the basic knowledge of mental health (MHQ&amp;A) by using deep learning retrieval technology and knowledge graph technology. The system MHQ&amp;A is designed mainly for the general public, to answer the basic knowledge of mental health, especially the field of depression. First of all, the basic and the professional question and answer data about mental health were respectively obtained by the reptilian bot from the "IASK" website knowledge and the "Dr. Dingxiang" website. Then, the questions and answers obtained through the crawler are made into a Question and Answering Knowledge Graph of Basic Health Knowledge in the mental health field, which is combined with semantic data of antidepressants and the semantic data of depression papers. Finally, a set of template matching rules is designed to determine the type of problem of users. If the questions are about the professional knowledge of medicine or thesis, the reasoning template will be used to reason and search the answer in the "Question and Answering Knowledge Graph of Basic Health Knowledge in the Mental Health Field". If the questions are about other basic knowledge in the field of mental health, the BERT model is used to vectorize the questions of users, and the matching questions and corresponding answers in the MHQ&amp;A are found through cosine similarity calculation. Through the test of system accuracy, it is proved that the system can effectively combine deep learning technology and knowledge.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {472–476},
numpages = {5},
keywords = {Deep learning, Knowledge Graph, Mental illness, Question and answering system},
location = {Beijing, China},
series = {ISAIMS '21}
}

@inproceedings{10.1109/MODELS-C.2019.00101,
author = {H\"{o}ser, Moritz},
title = {Modeling adaptive learning agents for domain knowledge transfer},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00101},
doi = {10.1109/MODELS-C.2019.00101},
abstract = {The implementation of intelligent agents in industrial applications is often prevented by the high cost of adopting such a system to a particular problem domain. This paper states the thesis that when learning agents are applied to work environments that require domain-specific experience, the agent benefits if it can be further adapted by a supervising domain expert. Closely interacting with the agent, a domain expert should be able to understand its decisions and update the underlying knowledge base as needed.The result would be an agent with individualized knowledge that comes in part from the domain experts. The model of such an adaptive learning agent must take into account the problem domain, the design of the learning agent and the perception of the domain user. Therefore, already in the modeling phase, more attention must be paid to make the learning element of the agent adaptable by an operator. Domain modeling and meta-modeling methods could help to make inner processes of the agent more accessible. In addition, the knowledge gained should be made reusable for future agents in similar environments.To begin with, the existing methods for modeling agent systems and the underlying concepts will be evaluated, based on the requirements for different industrial scenarios. The methods are then compiled into a framework that allows for the description and modeling of such systems in terms of adaptability to a problem domain. Where necessary, new methods or tools will be introduced to close the gap between inconsistent modeling artifacts.The framework shall then be used to build learning agents for real-life scenarios and observe their application in a case study. The results will be used to assess the quality of the adapted knowledge base and compare it to a manual knowledge modeling process.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {660–665},
numpages = {6},
keywords = {adaptive learning agents, domain modeling, knowledge engineering, multi-modeling},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1145/3340531.3412151,
author = {Bahrani, Mohammad and Roelleke, Thomas},
title = {FDCM: Towards Balanced and Generalizable Concept-based Models for Effective Medical Ranking},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412151},
doi = {10.1145/3340531.3412151},
abstract = {Concept-based IR is expected to improve the quality of medical ranking since it captures more semantics than BOW representations. However, bringing concepts and BOW together into a transparent IR framework is challenging. We propose a new aggregation parameter to combine conceptual and term-based Dirichlet Compound Model scores effectively. The determination of this linear parameter is the result of exploring to what degree the difference of the conceptual and term-based sum of IDFs is influential to the integration. Instead of employing heuristics to find combined models, this paper aims to build the grounds for establishing reasonable aggregation standards based on semantic query performance predictors.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {1957–1960},
numpages = {4},
keywords = {concept-based ir, dirichlet compound language modelling, query formulation, semantic ir},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@proceedings{10.1145/3450569,
title = {SACMAT '21: Proceedings of the 26th ACM Symposium on Access Control Models and Technologies},
year = {2021},
isbn = {9781450383653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the ACM Symposium on Access Control Models and Technologies (SACMAT 2021). This year's symposium continues its tradition of being the premier forum for the presentation of research results and experience reports on leading-edge issues of access control, including models, systems, applications, and theory, while also embracing a renovated focus on the general area of security.The aim of the symposium is to share novel access control and security solutions that fulfill the needs of heterogeneous applications and environments, and to identify new directions for future research and development.SACMAT provides researchers and practitioners with a unique opportunity to share their perspectives with others interested in the various aspects of access control and security.},
location = {Virtual Event, Spain}
}

@inproceedings{10.1145/3277593.3277619,
author = {Belkaroui, Rami and Bertaux, Aur\'{e}lie and Labbani, Ouassila and Hugol-Gential, Cl\'{e}mentine and Nicolle, Christophe},
title = {Towards events ontology based on data sensors network for viticulture domain},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277619},
doi = {10.1145/3277593.3277619},
abstract = {Wine Cloud project is the first "Big Data" platform on the french viticulture value chain. The aim of this platform is to provide a complete traceability of the life cycle of the wine, from the wine-grower to the consumer. In particular, Wine Cloud may qualify as an agricultural decision platform that will be used for vine life cycle management in order to predict the occurrence of major risks (vine diseases, grape vine pests, physiological risks, fermentation stoppage, oxidation of vine, etc...). Also to make wine production more rational by offering winegrower a set of recommendation regarding their strategy's of production development.The proposed platform "Wine Cloud" is based on heterogeneous sensors network (agricultural machines, plant sensors and measuring stations) deployed throughout a vineyard. These sensors allow for capturing data from the agricultural process and remote monitoring vineyards in the Internet of Things (IoT) era. However, the sensors data from different source is hard to work together for lack of semantic. Therefore, the task of coherently combining heterogeneous sensors data becomes very challenging. The integration of heterogeneous data from sensors can be achieved by data mining algorithms able to build correlations. Nevertheless, the meaning and the value of these correlations is difficult to perceive without highlighting the meaning of the data and the semantic description of the measured environment.In order to bridge this gap and build causality relationships form heterogeneous sensor data, we propose an ontology-based approach, that consists in exploring heterogeneous sensor data (light, temperature, atmospheric pressure, etc) in terms of ontologies enriched with semantic meta-data describing the life cycle of the monitored environment.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {44},
numpages = {7},
keywords = {IoT, big data, event ontology, ontologies, semantic sensor data, smart viticulture},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@inproceedings{10.1145/3230905.3230929,
author = {El Hajjamy, Oussama and Alaoui, Larbi and Bahaj, Mohamed},
title = {Semantic integration of heterogeneous classical data sources in ontological data warehouse},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230929},
doi = {10.1145/3230905.3230929},
abstract = {The development of semantic web technologies and the expansion of the amount of data managed within companies databases lead to an enormous quantity of heterogeneous, distributed and autonomous data sources. However, this growth of information will give rise to real obstacles if we cannot maintain the pace with these changes and meet the needs of users. To succeed, it is necessary to find a solution for integrating data from traditional information systems into richer systems. In this perspective, Ontologies are a key component in solving the problem of semantic heterogeneity, thus enabling semantic interoperability between different web applications and services. In this paper, we provide and develop a semi-automatic approach for designing an ontological data warehouse from various sources. Our approach is to convert the different classical data sources (UML, XML, RDB) to local ontologies (OWL2), then merge these ontologies into a global ontological model based on syntactic, structural and semantic similarity measurement techniques to identify similar concepts and avoid their redundancy in the merge result. Our study is proven by a developed prototype that demonstrates the efficiency and power of our strategy and validates the theoretical concept.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {36},
numpages = {8},
keywords = {Integrating data, OWL2, RDB, Semantic web, UML, XML, data warehouse, ontologies},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@inproceedings{10.1145/3652620.3688566,
author = {Dagenais, Kyanna and David, Istvan},
title = {Driving Requirements Evolution by Engineers' Opinions},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688566},
doi = {10.1145/3652620.3688566},
abstract = {Requirements are often incomplete or imprecise. Especially when innovation is a pronounced aspect of product development, sufficiently refined requirements can only be obtained by leveraging engineering knowledge gained through the exploration of innovative designs. However, such innovative designs often contradict prevalent requirements and might be deemed incorrect unless requirements evolve. In this paper, we present a method to drive requirements evolution by engineering opinions---early indicators of emergent engineering knowledge. Opinions about the suitability of a new design emerge earlier than hard evidence can be produced, potentially accelerating the evolution of requirements and saving time and costs. In this work, we develop a sound formal framework to inform requirements engineers about the potential need for requirements evolution based on engineering opinions. We formalize engineering opinions in terms of subjective logic and unify them with key concepts of model-driven engineering.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {920–929},
numpages = {10},
keywords = {collaboration, consistency, design space exploration, model-driven engineering, ontologies, parallel engineering, uncertainty},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3293614.3293627,
author = {Barbalho, Ingridy M. P. and Silva, Patricio de A. and Fernandes, Felipe R. dos S. and Neto, Francisco M. M. and Leite, Cicilia R. M.},
title = {An Investigation on the use of Ontologies for pattern classification - Study applied to the monitoring of food intake},
year = {2018},
isbn = {9781450365727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293614.3293627},
doi = {10.1145/3293614.3293627},
abstract = {Several tools are developed with the purpose of solving problems and exposing results similar to human reasoning. For this, various artificial intelligence techniques are being implemented to improve these applications. For the poorly structured and high volume data, the ontology presents itself as a technique capable of structuring this data and exposing representative results. In this way, this work describes the use of an ontology as the data classification technique and pattern recognition. The objective is to develop an ontological structure capable of analyzing and classifying the movements and sound signals of the chewing and swallowing process in solids or liquids. To validate the ontology, the tests were performed in real environments. The results obtained, based on the realized experiments, point to the viability of the use of ontologies for the problem in question.},
booktitle = {Proceedings of the Euro American Conference on Telematics and Information Systems},
articleno = {4},
numpages = {8},
keywords = {Artificial Intelligence, Data classification, Food Intake, Ontologies},
location = {Fortaleza, Brazil},
series = {EATIS '18}
}

@inproceedings{10.1145/3467707.3467762,
author = {DIB, Ahmed Taki Eddine and MAAMRI, Ramdane},
title = {Bigraphical Modelling and Design of Multi-Agent Systems},
year = {2021},
isbn = {9781450389501},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3467707.3467762},
doi = {10.1145/3467707.3467762},
abstract = {Multi-agent systems are recognized as a major area of distributed artificial intelligence. In fact, MAS have found multiple applications, including the design and development of complex, hierarchical and critical systems. However, ensuring the accuracy of complex interactions and the correct execution of activities of a MAS is becoming a tedious task. In this work, we focus on the formal specification of interaction, holonic and sociotechnical concepts to the BRS-MAS model. The proposed approach, is based on Bigraphical reactive systems. Bigraphs, provide means to specify at same time locality and connectivity of different type of system ranging from soft systems to cyber physical systems. In addition, to its intuitive graphical representation, it provides algebraic definition. This, makes the resulted specifications more precise. Further, it enables the verification of the specified system at the design time (before the implementation) using verification tools.},
booktitle = {Proceedings of the 2021 7th International Conference on Computing and Artificial Intelligence},
pages = {365–371},
numpages = {7},
keywords = {Algebraic language theory, Computing methodologies, Formal specification, Holonic, Multi-agent system, Theory of computation},
location = {Tianjin, China},
series = {ICCAI '21}
}

@inproceedings{10.1145/3497623.3497652,
author = {Zhang, Congcong and Zhao, Haifeng and Cao, Mingwei},
title = {Research on General Text Classification Model Integrating Character-Level Attention and Multi-Scale Features},
year = {2022},
isbn = {9781450390439},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3497623.3497652},
doi = {10.1145/3497623.3497652},
abstract = {To solve the problem of poor interpretability of the model caused by the random initialization of convolution kernel in convolution neural network,and the problem of local and single feature extraction scale, a general character-level classification model is designed by referring to the method in CV. Firstly,a multi-scale feature extraction module is added to the network embedding layer to extract rich context information, and the problem of matrix sparsity is solved to some extent by setting different void rates. Then, the network depth is controlled by the number of blocks. Different depths have different grasps of global information and can adapt to tasks with different complexity. Next, add a modified attention mechanism module after the block to enhance the attention of the model to key parts. Finally, the full connection layer of the network is replaced by the full convolution layer to simplify the model. The block is a compressed version of deep separable convolution, and the overall model parameters are reduced to about one-tenth of the original, but the accuracy and performance are improved. The results show that the model is very effective.},
booktitle = {Proceedings of the 2021 10th International Conference on Computing and Pattern Recognition},
pages = {183–187},
numpages = {5},
keywords = {attention mechanism, full convolution,compression, multi-scale features},
location = {Shanghai, China},
series = {ICCPR '21}
}

@inproceedings{10.1145/3167132.3167344,
author = {Halilaj, Lavdim and Grangel-Gonz\'{a}lez, Irl\'{a}n and Lohmann, Steffen and Vidal, Maria-Esther and Auer, S\"{o}ren},
title = {EffTE: a dependency-aware approach for test-driven ontology development},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167344},
doi = {10.1145/3167132.3167344},
abstract = {The development of domain-specific ontologies requires joint efforts among different groups of stakeholders, such as ontology engineers and domain experts. By following a test-driven development technique, a set of test cases ensures that ontology changes do not violate predefined requirements. However, since the number of test cases can be large and their evaluation time may be high, the ontology development process can be negatively impacted. We propose EffTE, an approach for efficient test-driven ontology development relying on a graph-based model of dependencies between test cases. It enables prioritization and selection of test cases to be evaluated. Traversing the dependency graph is realized using breadth-first search along with a mechanism that tracks tabu test cases, i.e., test cases to be ignored for further evaluation due to faulty parent test cases. As a result, the number of evaluated test cases is minimized, thus reducing the time required for validating the ontology after each modification. We conducted an empirical evaluation to determine the efficiency of our approach. The evaluation results suggest that our approach is more efficient than an exhaustive evaluation of the test cases; in particular with an increasing ontology size and number of test cases.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1976–1983},
numpages = {8},
keywords = {dependency graph, ontology engineering, test cases, test-driven ontology development},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3585088.3589356,
author = {Tseng, Tiffany and King Chen, Jennifer and Abdelrahman, Mona and Kery, Mary Beth and Hohman, Fred and Hilliard, Adriana and Shapiro, R. Benjamin},
title = {Collaborative Machine Learning Model Building with Families Using Co-ML},
year = {2023},
isbn = {9798400701313},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3585088.3589356},
doi = {10.1145/3585088.3589356},
abstract = {Existing novice-friendly machine learning (ML) modeling tools center around a solo user experience, where a single user collects only their own data to build a model. However, solo modeling experiences limit valuable opportunities for encountering alternative ideas and approaches that can arise when learners work together; consequently, it often precludes encountering critical issues in ML around data representation and diversity that can surface when different perspectives are manifested in a group-constructed data set. To address this issue, we created Co-ML – a tablet-based app for learners to collaboratively build ML image classifiers through an end-to-end, iterative model-building process. In this paper, we illustrate the feasibility and potential richness of collaborative modeling by presenting an in-depth case study of a family (two children 11 and 14-years-old working with their parents) using Co-ML in a facilitated introductory ML activity at home. We share the Co-ML system design and contribute a discussion of how using Co-ML in a collaborative activity enabled beginners to collectively engage with dataset design considerations underrepresented in prior work such as data diversity, class imbalance, and data quality. We discuss how a distributed collaborative process, in which individuals can take on different model-building responsibilities, provides a rich context for children and adults to learn ML dataset design.},
booktitle = {Proceedings of the 22nd Annual ACM Interaction Design and Children Conference},
pages = {40–51},
numpages = {12},
keywords = {children, collaboration, families, learning, machine learning},
location = {Chicago, IL, USA},
series = {IDC '23}
}

@inproceedings{10.1145/3485730.3493445,
author = {Maresca, Fabio and Solmaz, G\"{u}rkan and Cirillo, Flavio},
title = {OntoAugment: Ontology Matching through Weakly-Supervised Label Augmentation},
year = {2021},
isbn = {9781450390972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485730.3493445},
doi = {10.1145/3485730.3493445},
abstract = {Ontology matching enables harmonizing heterogeneous data models. Existing ontology matching approaches include machine learning. In particular, recent works leverage weak supervision (WS) through programmatic labeling to avoid the intensive hand-labeling for large ontologies. Programmatic labeling relies on heuristics and rules, called Labeling Functions (LFs), that generate noisy and incomplete labels. However, to cover a reasonable portion of the dataset, programmatic labeling might require a significant number of LFs that might be time expensive and not always straightforward to program.This paper proposes a novel system, namely OntoAugment, that augments LF labels for the ontology matching problem, starting from outcomes of the LFs. Our solution leverages the "similarity of similarities" between ontology concept bipairs that are two pairs of concepts. OntoAugment projects a label yielded by an LF for a concept pair to a similar pair that the same LF does not label. Thus, a wider portion of the dataset is covered even with a limited set of LFs. Experimentation results show that OntoAugment provides significant improvements (up to 11 F1 points) compared to the state-of-the-art WS approach when fewer LFs are used, whereas it maintains the performance without creating additional noise when a higher number of LFs already achieves high performance.},
booktitle = {Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems},
pages = {420–425},
numpages = {6},
keywords = {data programming, ontology matching, semantic, weak supervision},
location = {Coimbra, Portugal},
series = {SenSys '21}
}

@inproceedings{10.1145/3567445.3571109,
author = {Schleipen, Miriam and Schubert, Viktor and Dzidic, Samir and Penner, Dimitri and Spieckermann, Sven},
title = {A modeling approach for integration and contextualization of simulation-based digital services in IIoT},
year = {2023},
isbn = {9781450396653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567445.3571109},
doi = {10.1145/3567445.3571109},
abstract = {In the context of the Industrial Internet of Things (IIoT) production plants and components are increasingly growing together with information technologies. This is often realized by means of digital twins. They collect data in real time and learn from this data; they control processes automatically or support human decisions; and they communicate and interact through the internet. This is more and more evolving to intercompany interactions based on digital services. In addition to data of isolated assets (e.g. production resources), new capabilities for standard-based data integration and orchestration are necessary to contextualize the interaction of multiple digital twins and services. This paper suggests an approach to use common standards in the industrial context such as AutomationML, FMI, and OPC UA as basis for integration and contextualization of simulation-based digital services on IIoT platforms.},
booktitle = {Proceedings of the 12th International Conference on the Internet of Things},
pages = {205–210},
numpages = {6},
keywords = {AutomationML, FMI, capability, simulation, skill, smart service},
location = {Delft, Netherlands},
series = {IoT '22}
}

@article{10.14778/3717755.3717772,
author = {Zhao, Fuheng and Deep, Shaleen and Psallidas, Fotis and Floratou, Avrilia and Agrawal, Divyakant and Abbadi, Amr El},
title = {Sphinteract: Resolving Ambiguities in NL2SQL through User Interaction},
year = {2025},
issue_date = {December 2024},
publisher = {VLDB Endowment},
volume = {18},
number = {4},
issn = {2150-8097},
url = {https://doi.org/10.14778/3717755.3717772},
doi = {10.14778/3717755.3717772},
abstract = {Translating natural language questions into SQL queries (NL2SQL) is a challenging task of great practical importance. Prior work has extensively studied how to address NL2SQL using Large Language Models (LLMs) with solutions ranging from careful prompt engineering, to fine-tuning existing LLMs, or even training custom models. However, a remaining challenging problem in NL2SQL is the inherent ambiguity in the natural language questions asked by users. In this paper, we introduce Sphinteract, a framework designed to assist LLMs in generating high-quality SQL answers that accurately reflect the user intent. Our key insight to resolve ambiguity is to take into account minimal user feedback interactively. We introduce the Summarize, Review, Ask (SRA) paradigm, which guides LLMs in identifying ambiguities in NL2SQL tasks and generates targeted questions for the user to answer. We propose three different methods of how to process user feedback and generate SQL queries based on user input. Our experiments on the challenging KaggleDBQA and BIRD benchmarks demonstrate that by means of asking clarification questions to the user, LLMs can efficiently incorporate the feedback, resulting in accuracy improvements of up to 42\%.},
journal = {Proc. VLDB Endow.},
month = may,
pages = {1145–1158},
numpages = {14}
}

@inproceedings{10.1145/3712716.3712726,
author = {Korol, Allan and Sikos, Leslie F.},
title = {FEAR: A Novel Framework for Representing Digital Forensic Artifacts in Knowledge Graphs},
year = {2025},
isbn = {9798400710766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3712716.3712726},
doi = {10.1145/3712716.3712726},
abstract = {In digital forensics, knowledge graphs have demonstrated significant potential via software agent automation and knowledge discovery using encoded expert knowledge in, for example, the form of Semantic Web rules. These advancements have been limited in terms of efficiently encoding extracted digital artifacts into graph representation, the associated overhead of implementing frameworks to handle digital forensic evidence, and the lack of sharing of such code that is often a preamble to other research. This paper introduces a digital forensic framework, Forensic Extraction and Representation (FEAR), that enables a simplified process of accessing and encoding extracted digital forensic artifacts in semantic knowledge graphs. The adoption of such a framework can facilitate the sharing of expert knowledge and reduce the burden of development for researchers exploring the application of knowledge graphs, software agents, and automated reasoning in the field of digital forensics, while accelerating the adoption of emerging research by practitioners.},
booktitle = {Proceedings of the Digital Forensics Doctoral Symposium},
articleno = {9},
numpages = {8},
keywords = {digital forensic artifacts, semantic knowledge graph, digital forensic software agent, digital forensic framework, declarative language},
location = {
},
series = {DFDS '25}
}

@inproceedings{10.1145/3243907.3243915,
author = {Atarashi, Ray and Sone, Takuro and Komohara, Yu and Tsukada, Manabu and Kasuya, Takashi and Okumura, Hiraku and Ikeda, Masahiro and Esaki, Hiroshi},
title = {The Software Defined Media Ontology for Music Events},
year = {2018},
isbn = {9781450364959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243907.3243915},
doi = {10.1145/3243907.3243915},
abstract = {With the advent of viewing services based on the Internet, the importance of object-based viewing services for interpreting objects existing in space and utilizing them as content is increasing. Since 2014, the Software Defined Media Consortium has been researching object-based media and Internet-based viewing spaces. This paper defineds a framework in event participants and professional recorders each freely share recorded data, and a third party can creates an application based on the data. This study aims to provide an SDM ontology-based contents management mechanism with a detailed description of the object-based audio and video data and the recording environment. The data can be shared via the Internet and is highly reusable. We implemented this management mechanism and have developed and validated applications that are capable of interactively playing 3D content from any viewpoints freely.},
booktitle = {Proceedings of the 1st International Workshop on Semantic Applications for Audio and Music},
pages = {15–23},
numpages = {9},
keywords = {Content management, Ontology, RDF},
location = {Monterey, CA, USA},
series = {SAAM '18}
}

@article{10.1145/3453185,
author = {Tan, Minghuan and Jiang, Jing and Dai, Bing Tian},
title = {A BERT-Based Two-Stage Model for Chinese Chengyu Recommendation},
year = {2021},
issue_date = {November 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3453185},
doi = {10.1145/3453185},
abstract = {In Chinese, Chengyu are fixed phrases consisting of four characters. As a type of idioms, their meanings usually cannot be derived from their component characters. In this article, we study the task of recommending a Chengyu given a textual context. Observing some of the limitations with existing work, we propose a two-stage model, where during the first stage we re-train a Chinese BERT model by masking out Chengyu from a large Chinese corpus with a wide coverage of Chengyu. During the second stage, we fine-tune the re-trained, Chengyu-oriented BERT on a specific Chengyu recommendation dataset. We evaluate this method on ChID and CCT datasets and find that it can achieve the state of the art on both datasets. Ablation studies show that both stages of training are critical for the performance gain.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {92},
numpages = {18},
keywords = {Question answering, Chengyu recommendation, idiom understanding}
}

@inproceedings{10.1145/3696410.3714782,
author = {Zhao, Xuejiao and Liu, Siyan and Yang, Su-Yin and Miao, Chunyan},
title = {MedRAG: Enhancing Retrieval-augmented Generation with Knowledge Graph-Elicited Reasoning for Healthcare Copilot},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3696410.3714782},
doi = {10.1145/3696410.3714782},
abstract = {Retrieval-augmented generation (RAG) is a well-suited technique for retrieving privacy-sensitive Electronic Health Records (EHR). It can serve as a key module of the healthcare copilot, helping reduce misdiagnosis for healthcare practitioners and patients. However, the diagnostic accuracy and specificity of existing heuristic-based RAG models used in the medical domain are inadequate, particularly for diseases with similar manifestations. This paper proposes MedRAG, a RAG model enhanced by knowledge graph (KG)-elicited reasoning for the medical domain that retrieves diagnosis and treatment recommendations based on manifestations. MedRAG systematically constructs a comprehensive four-tier hierarchical diagnostic KG encompassing critical diagnostic differences of various diseases. These differences are dynamically integrated with similar EHRs retrieved from an EHR database, and reasoned within a large language model. This process enables more accurate and specific decision support, while also proactively providing follow-up questions to enhance personalized medical decision-making. MedRAG is evaluated on both a public dataset DDXPlus and a private chronic pain diagnostic dataset (CPDD) collected from Tan Tock Seng Hospital, and its performance is compared against various existing RAG methods. Experimental results show that, leveraging the information integration and relational abilities of the KG, our MedRAG provides more specific diagnostic insights and outperforms state-of-the-art models in reducing misdiagnosis rates. Our code will be available at https://github.com/SNOWTEAM2023/MedRAG},
booktitle = {Proceedings of the ACM on Web Conference 2025},
pages = {4442–4457},
numpages = {16},
keywords = {decision support, healthcare copilot, knowledge graph, large language models, retrieval-augmented generation},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.5555/3463952.3463955,
author = {Istrate, Gabriel},
title = {Models We Can Trust: Toward a Systematic Discipline of (Agent-Based) Model Interpretation and Validation},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We advocate the development of a discipline of interacting with and extracting information from models, both mathematical (e.g. game-theoretic ones) and computational (e.g. agent-based models). We outline some directions for the development of a such a discipline: - the development of logical frameworks for the systematic formal specification of stylized facts and social mechanisms in (mathematical and computational) social science. Such frameworks would bring to attention new issues, such as phase transitions, i.e. dramatical changes in the validity of the stylized facts beyond some critical values in parameter space. We argue that such statements are useful for those logical frameworks describing properties of ABM. - the adaptation of tools from the theory of reactive systems (such as bisimulation) to obtain practically relevant notions of two systems "having the same behavior". - the systematic development of an adversarial theory of model perturbations, that investigates the robustness of conclusions derived from models of social behavior to variations in several features of the social dynamics. These may include: activation order, the underlying social network, individual agent behavior.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {6–11},
numpages = {6},
keywords = {adversarial perturbations, agent-based simulation, bisimulation, game-theoretic models, logical frameworks, robustness},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.1145/3415958.3433076,
author = {Tissaoui, Anis and Sassi, Salma and Chbeir, Richard},
title = {LEOnto: New Approach for Ontology Enrichment using LDA},
year = {2020},
isbn = {9781450381154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3415958.3433076},
doi = {10.1145/3415958.3433076},
abstract = {The Latent Dirichlet Allocation (LDA) model [18] was originally developed and utilised for document modeling and topic extraction in Information Retrieval. To design high quality domain ontologies, effective and usable methodologies are needed to facilitate their building process. In this paper, we propose a new approach for semi-automatic ontology enriching from textual corpus based on LDA model. In our approach, LDA is adopted to provide efficient dimension reduction, able to capture semantic relationships between word-topic and topic-document in terms of probability distributions with minimum human intervention. We conducted several experiments with different model parameters and the corresponding behavior of the enriching technique was evaluated by domain experts. We also compared the results of our method with two existing learning methods using the same dataset. The study showed that our method outperforms the other methods in terms of recall and precision measures.},
booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
pages = {132–139},
numpages = {8},
keywords = {Knowledge acquisition, LDA, Ontology enrichment, Ontology learning, Probabilistic topic models},
location = {Virtual Event, United Arab Emirates},
series = {MEDES '20}
}

@inproceedings{10.1145/3291533.3291571,
author = {Voutos, Yorghos and Mylonas, Phivos},
title = {A semantic data model for sensory spatio-temporal environmental concepts},
year = {2018},
isbn = {9781450366106},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3291533.3291571},
doi = {10.1145/3291533.3291571},
abstract = {Nowadays, the well-known Resource Description Framework (RDF) forms a rather general method for web resources' conceptual description or even for generic information modeling. However, RDF's capabilities are challenged once used to effectively represent non-thematic metadata, e.g, in the form of spatial and temporal objects deriving primarily from sensor information. In addition, Wireless Sensor Network (WSN) is considered today to be a widely adopted platform, related to environmental monitoring and decision making applications. Specifically, exclusive subjects, such as environmental degradation and optimized agriculture, provide a scope of applied research on the basis of multilevel semantic data analysis. Observations and sensors are the core of empirical science and their implementation (i.e., the increasing volume of data, heterogeneity of devices, data formats and measurement procedures) produce a large volume of unsupervised data. Thus, the prevailing growth of sensing systems has currently led to the development of defined interoperability among standards on web semantics. In particular, Semantic Sensor Network (SSN) ontologies prospect on modeling the capabilities and properties of sensors, monitoring procedures and observations. Furthermore, the dynamically evolving natural phenomena require proper conceptualization of environmental change and monitoring agents. Consequently, this paper describes an inaugural attempt to create a conceptual framework of spatially and temporally-enabled environmental variables for sensing systems.},
booktitle = {Proceedings of the 22nd Pan-Hellenic Conference on Informatics},
pages = {180–185},
numpages = {6},
keywords = {environmental monitoring, ontology model, semantics, sensors},
location = {Athens, Greece},
series = {PCI '18}
}

@inproceedings{10.1145/3369166.3369174,
author = {Cartealy, Imam and Liao, Li},
title = {Metabolic Pathway Membership Inference using an Ontology-based Similarity Approach},
year = {2020},
isbn = {9781450372510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369166.3369174},
doi = {10.1145/3369166.3369174},
abstract = {Determining whether a protein belongs to a metabolic pathway is an important annotation task, can provide context to the basic functional annotation, and aid reconstruction of incomplete pathways. In this work, we develop a method for pathway membership inference based gene ontology (GO) similarity between a query protein and proteins that are known to be members of a given pathway. By comparing with various existing measures of GO term semantic similarity, we develop an effective and efficient way to take into account of both information content of individual GO terms and the whole GO hierarchy. We test the classifier using 10-fold cross validation for all metabolic pathways reported in KEGG database and demonstrate that our method outperforms with statistical significance in comparison to a suite of existing semantic similarity measures, as evaluated using ROC score. And our method outperforms other methods in running time by multiple orders of magnitude for long pathways.},
booktitle = {Proceedings of the 2019 8th International Conference on Bioinformatics and Biomedical Science},
pages = {97–102},
numpages = {6},
keywords = {Gene Ontology, Membership Prediction, Metabolic Pathway, Semantic Similarity},
location = {Beijing, China},
series = {ICBBS '19}
}

@article{10.1145/3677074,
author = {Hadan, Hilda and Sgandurra, Sabrina Alicia and Zhang-Kennedy, Leah and Nacke, Lennart E.},
title = {From Motivating to Manipulative: The Use of Deceptive Design in a Game's Free-to-Play Transition},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3677074},
doi = {10.1145/3677074},
abstract = {Over the last decade, the free-to-play (F2P) game business model has gained popularity in the games industry. We examine the role of deceptive design during a game's transition to F2P and its impacts on players. Our analysis focuses on game mechanics and a Reddit analysis of the Overwatch (OW) series after it transitioned to an F2P model. Our study identifies nine game mechanics that use deceptive design patterns. We also identify factors contributing to a negative gameplay experience. Business model transitions in games present possibilities for problematic practices. Our findings identify the need for game developers and publishers to balance player investments and fairness of rewards. A game's successful transition depends on maintaining fundamental components of player motivation and ensuring transparent communication. Compared to existing taxonomies in other media, games need a comprehensive classification of deceptive design. We emphasize the importance of understanding player perceptions and the impact of deceptive practices in future research.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {309},
numpages = {31},
keywords = {Overwatch, deceptive design, free-to-play, game model transition, game player perception}
}

@inproceedings{10.1109/MODELS-C.2019.00103,
author = {Bogdanova, Daria and Snoeck, Monique},
title = {Use of personalized feedback reports in a blended conceptual modelling course},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00103},
doi = {10.1109/MODELS-C.2019.00103},
abstract = {Despite the substantial number of existing publications on conceptual modelling education and feedback, in particular, the perfect balance between the effectiveness of feedback and the costs of the feedback design tailored to the field-specific needs of conceptual modelling remains an unanswered scientific and pedagogical question. The existing educational literature and online courses on conceptual modelling tend to overlook the essential aspects of metacognition and self-regulation in the learning process. The problem of providing feedback is exacerbated by the time-consuming nature of manual feedback provision and the difficulties of automating the provision of personalized and elaborated feedback. This paper presents an experience report on designing and implementing a learning ontology-based personalized feedback report aimed at raising student self-awareness and self-regulated learning in a university level conceptual modelling course, while the design aims at automation of the feedback provisioning in the near future. It describes the stages of learning report development and provides directions for adapting this type of feedback for various learning settings in conceptual modelling education, in view of potential future automation of report provision.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {672–679},
numpages = {8},
keywords = {blended learning, conceptual modelling, data modelling, education, formative assessment, formative feedback, learning report},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.5555/3466184.3466455,
author = {Casale, Giuliano},
title = {Integrated performance evaluation of extended queueing network models with line},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Despite the large literature on queueing theory and its applications, tool support to analyze these models is mostly focused on discrete-event simulation and mean-value analysis (MVA). This circumstance diminishes the applicability of other types of advanced queueing analysis methods to practical engineering problems, for example analytical methods to extract probability measures useful in learning and inference. In this tool paper, we present LINE 2.0, an integrated software package to specify and analyze extended queueing network models. This new version of the tool is underpinned by an object-oriented language to declare a fairly broad class of extended queueing networks. These abstractions have been used to integrate in a coherent setting over 40 different simulation-based and analytical solution methods, facilitating their use in applications.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2377–2388},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3340531.3412009,
author = {Nikolov, Andriy and d'Aquin, Mathieu},
title = {Uncovering Semantic Bias in Neural Network Models Using a Knowledge Graph},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412009},
doi = {10.1145/3340531.3412009},
abstract = {While neural networks models have shown impressive performance in many NLP tasks, lack of interpretability is often seen as a disadvantage. Individual relevance scores assigned by post-hoc explanation methods are not sufficient to show deeper systematic preferences and potential biases of the model that apply consistently across examples. In this paper we apply rule mining using knowledge graphs in combination with neural network explanation methods to uncover such systematic preferences of trained neural models and capture them in the form of conjunctive rules. We test our approach in the context of text classification tasks and show that such rules are able to explain a substantial part of the model behaviour as well as indicate potential causes of misclassifications when the model is applied outside of the initial training context.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {1175–1184},
numpages = {10},
keywords = {explainable AI, knowledge graphs, neural networks, rule mining},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3377024.3377027,
author = {Caesar, Birte},
title = {Engineering support for variability modeling for context-sensitive reconfiguration of collaborative manufacturing systems},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377027},
doi = {10.1145/3377024.3377027},
abstract = {The manufacturing domain faces new challenges due to market changes. One of these changes affects consumer behavior, i.e. customers demand individualized products in small batches. Varying production requests require different configurations of manufacturing systems. But until today most of these systems are designed for single purpose usage, therefore new manufacturing systems are required. One solution are modular manufacturing systems, which can be composed of diverse modules from different vendors providing varying capabilities [1]. Modular manufacturing systems are re-configurable at two different levels. First, on the system group level, where modules with the required capabilities are selected and orchestrated. Second, on the module level, where a configuration has to be selected that provides the capability with the right manufacturing parameters. To be able to select and orchestrate the modules on the system group level, each module has to provide a self-description, which covers the current configuration [2].},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {14},
numpages = {2},
keywords = {context-sensitive reconfiguration, reverse engineering, variability mining, variability modeling},
location = {Magdeburg, Germany},
series = {VaMoS '20}
}

@inproceedings{10.1145/3411763.3451783,
author = {Seaborn, Katie and Pennefather, Peter and Miyake, Norihisa and Otake-Matsuura, Mihoko},
title = {Crossing the Tepper Line: An Emerging Ontology for Describing the Dynamic Sociality of Embodied AI},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451783},
doi = {10.1145/3411763.3451783},
abstract = {Artificial intelligences (AI) are increasingly being embodied and embedded in the world to carry out tasks and support decision-making with and for people. Robots, recommender systems, voice assistants, virtual humans—do these disparate types of embodied AI have something in common? Here we show how they can manifest as “socially embodied AI.” We define this as the state that embodied AI “circumstantially” take on within interactive contexts when perceived as both social and agentic by people. We offer a working ontology that describes how embodied AI can dynamically transition into socially embodied AI. We propose an ontological heuristic for describing the threshold: the Tepper line. We reinforce our theoretical work with expert insights from a card sort workshop. We end with two case studies to illustrate the dynamic and contextual nature of this heuristic.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {281},
numpages = {6},
keywords = {AI, Artificial agents, Ontology, Social embodiment, Social perceptions},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3483845.3483880,
author = {Li, Baizhen and Zhan, Yibin and Wei, Zhihua and Huang, Shi and Sun, Lijun},
title = {Improved non-autoregressive dialog state tracking model},
year = {2021},
isbn = {9781450390453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3483845.3483880},
doi = {10.1145/3483845.3483880},
abstract = {Dialogue systems, a powerful tool of human-machine interaction, are widely applied in e-commerce, online education, and cellphone assistant, etc. Dialogue state tracking (DST), updating the state of user goals during dialogue, is a core part of task-oriented dialogue systems. Recent research has made progress in low-latency and good-performance DST neural network models, i.e., non-autoregressive dialogue state tracking model (NADST). However, there are still some rooms for improvement in dialogue state tracking. In this paper, we propose following ways to improve the efficiency of NADST: (1) adding shrinkage residual network into fertility prediction; (2) constructing residual connection between different hierarchical attentions; (3) inserting a relative position encoding into state decoder for improving the performance of state prediction. The results of analysis and experiments indicate that the proposed model is the SOTA non-autoregressive method of dialog state tracking.},
booktitle = {Proceedings of the 2021 2nd International Conference on Control, Robotics and Intelligent System},
pages = {199–203},
numpages = {5},
location = {Qingdao, China},
series = {CCRIS '21}
}

@inproceedings{10.1145/3341161.3343854,
author = {Grisstte, Hanane and Nfaoui, ELhabib},
title = {Daily life patients sentiment analysis model based on well-encoded embedding vocabulary for related-medication text},
year = {2020},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3343854},
doi = {10.1145/3341161.3343854},
abstract = {Millions of health-related messages and fresh communications can reveal important public health issues. New Drugs, Diseases, Adverse Drug Reactions (ADRs) keep appearing on social media in new Unicode versions. In particular, generative Model for both Sentiment analysis (SA) and Naturel Language Understanding (NLU) requires medical human labeled data or making use of resources for weak supervision that operates with the ignorance and the inability to define related-medication targets, and results in inaccurate sentiment prediction performance. The frequent use of informal medical language, nonstandard format and abbreviation forms, as well as typos in social media messages has to be taken into account. We probe the transition-based approach between patients language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology[21] to be formal input of our neural network model. At this end, we propose daily life patients Sentiment Analysis model based on hybrid embedding vocabulary for related-medication text under distributed dependency, and concepts translation methodology by incorporating medical knowledge from social media and real life medical science systems. The proposed neural network layers is shared between medical concept Normalization model and sentiment prediction model in order to understand and leverage related-sentiment information behind conceptualized features in Multiple context. The experiments were performed on various real world scenarios where limited resources in this case.},
booktitle = {Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {921–928},
numpages = {8},
keywords = {BiLSTM model, medical concepts normalization, patient self-reports, sentiment analysis, social media},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@inproceedings{10.5555/3382225.3382427,
author = {Hu, Hengyi and Kerschberg, Larry},
title = {Evolving medical ontologies based on causal inference},
year = {2020},
isbn = {9781538660515},
publisher = {IEEE Press},
abstract = {Causal inference and analytics plays a critical role in public health and disease prevention. Through mining of large patient datasets, it is possible to identify opportunities for intervention and to determine the effectiveness of treatment. There are currently many methods to analyze and learn causal relationships in large patient datasets, as well as specific causal studies in epidemiology that define specific relationships among symptoms and treatments. This paper introduces a novel methodology to utilize causal knowledge to extend and improve a standard hierarchical medical ontology. First, we will obtain the hierarchical structure of the patient symptom variables based on the Medical Dictionary for Regulatory Activities Terminology (MedDRA). Then, we will learn a Causal Bayesian Network (CBN) using Max-Min Hill-Climbing (MMHC), a hybrid constraint and score-based learning algorithm, on the pre-existing National Institutes of Mental Health (NIMH) study on Sequenced Treatment Alternatives to Relieve Depression (STAR*D) patient dataset. Finally, we will use the causal links discovered in the CBN to evolve the ontology and its hierarchy.},
booktitle = {Proceedings of the 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {954–957},
numpages = {4},
keywords = {bayesian networks, causal inference, causal networks, causality, data management, data mining, healthcare data, healthcare information technology, ontology, ontology evolution, patient data},
location = {Barcelona, Spain},
series = {ASONAM '18}
}

@article{10.1109/TASLP.2022.3225537,
author = {Li, Yu and Hu, Bojie and Liu, Jian and Chen, Yufeng and Xu, Jinan},
title = {A Neighborhood Re-Ranking Model With Relation Constraint for Knowledge Graph Completion},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3225537},
doi = {10.1109/TASLP.2022.3225537},
abstract = {Knowledge graph completion (KGC) aims to predict missing links based on observed triples. However, current KGC models are still limited by the following two aspects. (1) the entity semantics is implicitly learned by neural network and merely depends on existing facts, which mostly suffers from less additional specific knowledge. Although previous studies have noticed that entity type information can effectively improve KGC task, most of them rely on labeled type-specific data. (2) the recent graph-based models mainly concentrate on Graph Neural Network (GNN) to update source entity representation, regardless of the separate role that neighborhood information plays and may mix noisy neighbor features for target prediction. To address the above two issues, we propose a neighborhood re-ranking model with relation constraint for KGC task. We suggest that both relation constraint and structured information located in triples can boost the model performance. More importantly, we automatically generate explicit constraints as additional type feature to enrich entity representation instead of depending on human annotated labels. Meanwhile, we construct a neighborhood completion module to re-rank candidate entities for full use of the neighbor structure rather than traditional GNN updating manner. Extensive experiments on seven benchmarks demonstrate that our model achieves the competitive results in comparison to the recent advanced baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {411–425},
numpages = {15}
}

@article{10.1145/3211871,
author = {Ochieng, Peter and Kyanda, Swaib},
title = {Large-Scale Ontology Matching: State-of-the-Art Analysis},
year = {2018},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3211871},
doi = {10.1145/3211871},
abstract = {Ontologies have become a popular means of knowledge sharing and reuse. This has motivated the development of large-sized independent ontologies within the same or different domains with some overlapping information among them. To integrate such large ontologies, automatic matchers become an inevitable solution. However, the process of matching large ontologies has high space and time complexities. Therefore, for a tool to efficiently and accurately match these large ontologies within the limited computing resources, it must have techniques that can significantly reduce the high space and time complexities associated with the ontology matching process. This article provides a review of the state-of-the-art techniques being applied by ontology matching tools to achieve scalability and produce high-quality mappings when matching large ontologies. In addition, we provide a direct comparison of the techniques to gauge their effectiveness in achieving scalability. A review of the state-of-the-art ontology matching tools that employ each strategy is also provided. We also evaluate the state-of-the-art tools to gauge the progress they have made over the years in improving alignment’s quality and reduction of execution time when matching large ontologies.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {75},
numpages = {35},
keywords = {Survey, mapping repair, ontology mapping, repair, scalability}
}

@inproceedings{10.1109/ICSSP.2019.00017,
author = {Islam, Chadni and Babar, Muhammad Ali and Nepal, Surya},
title = {An ontology-driven approach to automating the process of integrating security software systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSSP.2019.00017},
doi = {10.1109/ICSSP.2019.00017},
abstract = {A wide variety of security software systems need to be integrated into a Security Orchestration Platform (SecOrP) to streamline the processes of defending against and responding to cybersecurity attacks. Lack of interpretability and interoperability among security systems are considered the key challenges to fully leverage the potential of the collective capabilities of different security systems. The processes of integrating security systems are repetitive, time-consuming and error-prone; these processes are carried out manually by human experts or using ad-hoc methods. To help automate security systems integration processes, we propose an &lt;u&gt;On&lt;/u&gt;tology-driven approach for &lt;u&gt;S&lt;/u&gt;ecurity OrchestrAtion Platform (OnSOAP). The developed solution enables interpretability, and interoperability among security systems, which may exist in operational silos. We demonstrate OnSOAP's support for automated integration of security systems to execute the incident response process with three security systems (Splunk, Limacharlie, and Snort) for a Distributed Denial of Service (DDoS) attack. The evaluation results show that OnSOAP enables SecOrP to interpret the input and output of different security systems, produce error-free integration details, and make security systems interoperable with each other to automate and accelerate an incident response process.},
booktitle = {Proceedings of the International Conference on Software and System Processes},
pages = {54–63},
numpages = {10},
keywords = {automated integration process, incident response process, ontology, security orchestration, security system},
location = {Montreal, Quebec, Canada},
series = {ICSSP '19}
}

@inproceedings{10.1145/3348445.3348461,
author = {Arif, Khawaja Sarmad and Qamar, Usman and Wahab, Kanwal and Riaz, Muhammad Qasim},
title = {Building a Biomedical Ontology for Respiratory Tract Infection},
year = {2019},
isbn = {9781450371957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3348445.3348461},
doi = {10.1145/3348445.3348461},
abstract = {Respiratory tract infections are most common disease which can affect any human during any part of their age. According to sources almost 60\% of all antibiotic prescription is due to Respiratory tract infection. The concepts and their relations related to Respiratory tract infection are need to be explained with the help of biomedical literature as well as historical records. But these literature or records cannot be efficiently managed by users due to their unstructured representation. Biomedical Ontologies are best way to identify the concepts and their respective relations from huge amount of unstructured data. Our research aimed to create a biomedical Ontology for the domain of Respiratory tract infection using UMLS as a data source, which contains concepts, subtypes, their relationships, and semantic types. As a result ontology contains 26 main and sub types of Respiratory tract infections, also 234 broad relations with 107325 relation counts and 1151 narrow relation with 34580 relation counts. The ontology created is evaluated by domain experts and results are formulated.},
booktitle = {Proceedings of the 7th International Conference on Computer and Communications Management},
pages = {8–12},
numpages = {5},
keywords = {Biomedical Ontology, RTIs, Respiratory Tract Infection, Semantic web, UMLS},
location = {Bangkok, Thailand},
series = {ICCCM '19}
}

@inproceedings{10.1145/3192714.3196316,
author = {Pelzetter, Jens},
title = {Using Ontologies as a Foundation for Web Accessibility Tools},
year = {2018},
isbn = {9781450356510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192714.3196316},
doi = {10.1145/3192714.3196316},
abstract = {Creating web sites has become quite a complex task. One of most important aspects of a modern web site is accessibility. However, despite extensive standards many web sites have accessibility issues. This paper presents a new approach for creating tools to improve the accessibility of web sites using ontologies.},
booktitle = {Proceedings of the 15th International Web for All Conference},
articleno = {26},
numpages = {2},
keywords = {Accessibility Web Ontologies},
location = {Lyon, France},
series = {W4A '18}
}

@article{10.1145/3338843,
author = {Jackson, Daniel},
title = {Alloy: a language and tool for exploring software designs},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {62},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/3338843},
doi = {10.1145/3338843},
abstract = {Exploiting a simple, expressive logic based on relations to describe designs and automate their analysis.},
journal = {Commun. ACM},
month = aug,
pages = {66–76},
numpages = {11}
}

@inproceedings{10.1145/3544109.3544138,
author = {Yuan, Min},
title = {Modeling Analysis of the Influence of Seoul City Image on Tourists' Willingness to Revisit Based on Parallel Computing},
year = {2022},
isbn = {9781450395786},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544109.3544138},
doi = {10.1145/3544109.3544138},
abstract = {Tourism is closely related to people's lives today, and the development level of tourism informatization is an important indicator to measure the modern tourism industry. At present, more and more data on the Internet is released in the form of linked data, which reduces the complexity of the integration of distributed, heterogeneous or autonomous data, and at the same time promotes the application of linked data. The purpose of this article is to model the influence of Seoul's city image on tourists' willingness to revisit based on parallel computing. This paper studies the similarity calculation efficiency in the data set of passenger-related passenger revisiting intention resources. According to the established tourist tourism ontology, this paper adopts the MapReduce parallel computing framework to design a parallel computing method of related data similarity to improve the discovery efficiency of the related data model of the willingness of large-scale tourists to revisit. Experimental research shows that this article analyzes the difference in perceptions of various image factors by tourists of different ages, and finds that the Р value in the single-factor analysis of variance table is greater than 0.05, indicating that tourists of different ages do not have significant perceptions of each image factor.},
booktitle = {Proceedings of the 3rd Asia-Pacific Conference on Image Processing, Electronics and Computers},
pages = {164–168},
numpages = {5},
location = {Dalian, China},
series = {IPEC '22}
}

@inproceedings{10.1145/3434074.3446353,
author = {Wen, Ruchen},
title = {Toward Hybrid Relational-Normative Models of Robot Cognition},
year = {2021},
isbn = {9781450382908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434074.3446353},
doi = {10.1145/3434074.3446353},
abstract = {Most previous work on enabling robots' moral competence has used norm-based systems of moral reasoning. However, a number of limitations to norm-based ethical theories have been widely acknowledged. These limitations may be addressed by role-based ethical theories, which have been extensively discussed in the philosophy of technology literature but have received little attention within robotics. My work proposes a hybrid role/norm-based model of robot cognitive processes including moral cognition.},
booktitle = {Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {568–570},
numpages = {3},
keywords = {human-robot interaction, robot ethics, robotic moral competence, role ethics},
location = {Boulder, CO, USA},
series = {HRI '21 Companion}
}

@inproceedings{10.5555/3522802.3522932,
author = {Khemiri, Abdelhak and Yugma, Claude and Dauz\`{e}re-P\'{e}r\`{e}s, St\'{e}phane},
title = {Towards a generic semiconductor manufacturing simulation model},
year = {2022},
publisher = {IEEE Press},
abstract = {Simulation is one of the most used approaches to analyze semiconductor manufacturing systems. However, most simulation models are designed for single-use applications and study a limited set of problems that are not reusable afterwards. Recently, in order to overcome this problem, the idea to develop a generic wafer fab simulation model has emerged. Nonetheless, few papers address the development of a generic wafer fab simulation. This paper proposes a generic, data-driven simulation model to evaluate and analyze a wide range of problems arising in modern semiconductor manufacturing systems. We discuss the issues related to the genericity of such a simulation model and the data and semantic integration issues faced by users.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {159},
numpages = {10},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3407982.3407997,
author = {Popov, Miroslav and Ivanova, Tatyana},
title = {Knowledge Model for Developing, Searching and Using Personalized Learning Content for Learners, Having Dyslexia Disability},
year = {2020},
isbn = {9781450377683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407982.3407997},
doi = {10.1145/3407982.3407997},
abstract = {Much research in the area of e-learning systems today is focused on personalization and adaptivity, but only a few of the proposed systems are intended for students, having learning disabilities. These learners are significant amount of all learners (children, having some dyslexia symptoms are between 10\% and 20\% of all children). Dyslexia symptoms can be specific for different learners and can require specific organization of learning. All the experts in learning say that suitable learning is a key for achieving excellent results for these learners.We made extensive study on the relevant theories intended for better understanding of the requirements of an e-learning for people, having learning disabilities and existing frameworks or tools for people with dyslexia. As a result we propose a knowledge model for developing, describing, searching and recommending personalized learning resources in the web for learners, having dyslexia learning disability. We also propose agent-based software architecture, that manage and use implemented knowledge for supporting leaning resource development, searching, ranking according to specified criteria and recommendation of these resources to dyslexic learners.},
booktitle = {Proceedings of the 21st International Conference on Computer Systems and Technologies},
pages = {258–265},
numpages = {8},
keywords = {Dyslexia, E-Learning, Ontology, Web-based learning, knowledge},
location = {Ruse, Bulgaria},
series = {CompSysTech '20}
}

@inproceedings{10.1145/3586183.3606741,
author = {Chen, Weihao and Yu, Chun and Wang, Huadong and Wang, Zheng and Yang, Lichen and Wang, Yukun and Shi, Weinan and Shi, Yuanchun},
title = {From Gap to Synergy: Enhancing Contextual Understanding through Human-Machine Collaboration in Personalized Systems},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606741},
doi = {10.1145/3586183.3606741},
abstract = {This paper presents LangAware, a collaborative approach for constructing personalized context for context-aware applications. The need for personalization arises due to significant variations in context between individuals based on scenarios, devices, and preferences. However, there is often a notable gap between humans and machines in the understanding of how contexts are constructed, as observed in trigger-action programming studies such as IFTTT. LangAware enables end-users to participate in establishing contextual rules in-situ using natural language. The system leverages large language models (LLMs) to semantically connect low-level sensor detectors to high-level contexts and provide understandable natural language feedback for effective user involvement. We conducted a user study with 16 participants in real-life settings, which revealed an average success rate of 87.50\% for defining contextual rules in a variety of 12 campus scenarios, typically accomplished within just two modifications. Furthermore, users reported a better understanding of the machine’s capabilities by interacting with LangAware.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {110},
numpages = {15},
keywords = {Context-Aware Systems, End User Context Construction, Large Language Models, Personalization},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@article{10.1145/3530221,
author = {Malmi, Lauri and Sheard, Judy and Kinnunen, P\"{a}ivi and Simon and Sinclair, Jane},
title = {Development and Use of Domain-specific Learning Theories, Models, and Instruments in Computing Education},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
url = {https://doi.org/10.1145/3530221},
doi = {10.1145/3530221},
abstract = {Use of theory within a field of research provides the foundation for designing effective research programs and establishing a deeper understanding of the results obtained. This, together with the emergence of domain-specific theory, is often taken as an indicator of the maturity of any research area. This article explores the development and subsequent usage of domain-specific theories and theoretical constructs (TCs) in computing education research (CER). All TCs found in 878 papers published in three major CER publication venues over the period 2005–2020 were identified and assessed to determine the nature and purpose of the constructs found. We focused more closely on areas related to learning, studying, and progression, where our analysis found 80 new TCs that had been developed, based on multiple epistemological perspectives. Several existing frameworks were used to categorize the areas of CER focus in which TCs were found, the methodology by which they were developed, and the nature and purpose of the TCs. A citation analysis was undertaken, with 1,727 citing papers accessed to determine to what extent and in what ways TCs had been used and developed to inform subsequent work, also considering whether these aspects vary according to different focus areas within computing education. We noted which TCs were used most often and least often, and we present several brief case studies that demonstrate progressive development of domain-specific theory. The exploration provides insights into trends in theory development and suggests areas in which further work might be called for. Our findings indicate a general interest in the development of TCs during the period studied, and we show examples of how different approaches to theory development have been used. We present a framework suggesting how strategies for developing new TCs in CER might be structured and discuss the nature of theory development in relation to the field of CER.},
journal = {ACM Trans. Comput. Educ.},
month = dec,
articleno = {6},
numpages = {48},
keywords = {Computing education, theory, theoretical construct, literature, research, instrument}
}

@inproceedings{10.1145/3266237.3266266,
author = {da Silva Quirino, Glaice Kelly and Barcellos, Monalessa Perini and de Almeida Falbo, Ricardo},
title = {Visual notations for software pattern languages: a mapping study},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266266},
doi = {10.1145/3266237.3266266},
abstract = {Reuse has been recognized as an important practice in software engineering. The use of patterns makes it easier to reuse successful solutions, speeds up the development process, and promotes the application of good practices. Related patterns can be organized in a Pattern Language (PL), which represents the patterns and their relations, and provides guidance on how to select, reuse and integrate them. Visual notations are often used to provide a graphical representation to PLs. Aiming to investigate how PLs related to software have been visually represented, we carried out a systematic mapping. We identified and analyzed 64 PLs. As a result, we noticed a lack of consensus on the elements that should be represented in a PL and the symbols used to represent them. Moreover, most PLs have ambiguous or inexpressive visual representations.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {72–81},
numpages = {10},
keywords = {mapping study, pattern language, visual notation},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.5555/3586210.3586318,
author = {Bocciarelli, Paolo and D'Ambrogio, Andrea and Wagner, Gerd},
title = {Resource Modeling in Business Process Simulation},
year = {2023},
publisher = {IEEE Press},
abstract = {Business Process (BP) models address the specification of the flow of events and activities, along with the dependencies of activities on resources. BP models are often analyzed by using simulation-based approaches. This paper focuses on resource modeling for BP modeling and simulation, by first introducing the most important concepts and discussing how resources are modeled in the standard BP modeling notation (i.e., BPMN) and in the area of Discrete Event Simulation. Then, the paper presents two newer BP modeling and simulation approaches, namely the Object Event Modeling and Simulation with the Discrete Event Process Modeling Notation (DPMN) based on the JavaScript-based simulation framework OESjs, and Performability-enabled BPMN (PyBPMN), with the Java-based simulation framework eBPMN. A simple but effective case study dealing with a pizza service process is used to illustrate the main features of the presented approaches.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1296–1310},
numpages = {15},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3677052.3698597,
author = {Cho, Nicole and Srishankar, Nishan and Cecchi, Lucas and Watson, William},
title = {FISHNET: Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert Swarms, and Task Planning},
year = {2024},
isbn = {9798400710810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677052.3698597},
doi = {10.1145/3677052.3698597},
abstract = {Financial intelligence generation from vast data sources has typically relied on traditional methods of knowledge-graph construction or database engineering. Recently, fine-tuned financial domain-specific Large Language Models (LLMs), have emerged. While these advancements are promising, limitations such as high inference costs, hallucinations, and the complexity of concurrently analyzing high-dimensional financial data, emerge. This motivates our invention FISHNET (Financial Intelligence from Sub-querying, Harmonizing, Neural-Conditioning, Expert swarming, and Task planning), an agentic architecture that accomplishes highly complex analytical tasks for more than 98,000 regulatory filings that vary immensely in terms of semantics, data hierarchy, or format. FISHNET shows remarkable performance for financial insight generation (61.8\% success rate over 5.0\% Routing, 45.6\% RAG R-Precision). We conduct rigorous ablations to empirically prove the success of FISHNET, each agent’s importance, and the optimized performance of assembling all agents. Our modular architecture can be leveraged for a myriad of use-cases, enabling scalability, flexibility, and data integrity that are critical for financial tasks.},
booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
pages = {591–599},
numpages = {9},
keywords = {Harmonizing, LLM Agents, Planning, Sub-querying, Swarming},
location = {Brooklyn, NY, USA},
series = {ICAIF '24}
}

@inproceedings{10.1145/3274895.3274944,
author = {Bernard, Camille and Plumejeaud-Perreau, Christine and Villanova-Oliver, Marl\`{e}ne and Gensel, J\'{e}r\^{o}me and Dao, Hy},
title = {An ontology-based algorithm for managing the evolution of multi-level territorial partitions},
year = {2018},
isbn = {9781450358897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274895.3274944},
doi = {10.1145/3274895.3274944},
abstract = {Through times, regions all over the world are very often subject to change (their names, their belonging, their composition, and their geometries). In this paper, we present a Semantic Matching Algorithm for automatically detecting, describing and publishing in the Linked Open Data Web, rich descriptions of changes occurring in multi-level territorial partitions (e.g., partitions made of major regions, regions and districts levels). We adopt a Linked Data (LD) approach for the semantic descriptions of the changes they undergo, relying on two existing generic ontologies, TSN-Ontology and TSN-Change Ontology. The created RDF graphs draw the lineage of each region over time (horizontal reading of the graphs), as well as the propagation of a change event through the partition levels (vertical reading).},
booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
pages = {456–459},
numpages = {4},
keywords = {change detection, evolutive multi-level territorial partition, geospatial data matching, linked open data, semantic matching algorithm, semantic web, spatio-temporal ontology, versioning},
location = {Seattle, Washington},
series = {SIGSPATIAL '18}
}

@inproceedings{10.1145/3648188.3675152,
author = {Li, Ge and Vachtsevanou, Danai and Lem\'{e}e, J\'{e}r\'{e}my and Mayer, Simon and Strecker, Jannis},
title = {Reader-aware Writing Assistance through Reader Profiles},
year = {2024},
isbn = {9798400705953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3648188.3675152},
doi = {10.1145/3648188.3675152},
abstract = {Establishing rapport between authors and readers of scientific texts is essential for supporting readers in understanding texts as intended, facilitating socio-discursive practices within disciplinary communities, and helping in identifying interdisciplinary links among scientific writings. We propose a Reader-aware Congruence Assistant (RaCA), which supports writers to create texts that are adapted to target readers. Similar to user-centered design which is based on user profiles, RaCA features reader-centered writing through reader profiles that are dynamically computed from information discovered through academic search engines. Our assistant then leverages large language models to measure the congruence of a written text with a given reader profile, and provides feedback to the writer. We demonstrate our approach with an implemented prototype that illustrates how RaCA exploits information available on the Web to construct reader profiles, assesses writer-reader congruence and offers writers color-coded visual feedback accordingly. We argue that our approach to reader-oriented scientific writing paves the way towards the more personalized interaction of readers and writers with scientific content, and discuss how integration with Semantic Web technologies and Adaptive User Interface design can help materialize this vision within an ever-growing Web of scientific ideas, proof, and discourse.},
booktitle = {Proceedings of the 35th ACM Conference on Hypertext and Social Media},
pages = {344–350},
numpages = {7},
keywords = {Natural Language Processing, Personalized Text Adaptation, Reader Profile, Text Congruence},
location = {Poznan, Poland},
series = {HT '24}
}

@inproceedings{10.1145/3589335.3651914,
author = {Hanikov\'{a}, Kate\v{r}ina and Chud\'{a}n, David and Sv\'{a}tek, Vojt\v{e}ch and Vajde\v{c}ka, Peter and Troncy, Rapha\"{e}l and Vencovsk\'{y}, Filip and Syrov\'{a}tkov\'{a}, Jana},
title = {Towards Fact-check Summarization Leveraging on Argumentation Elements Tied to Entity Graphs},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651914},
doi = {10.1145/3589335.3651914},
abstract = {Fact-check consumers can have different preferences regarding the amount of text being used for explaining the claim veracity verdict. Dynamically adapting the size of a fact-check report is thus an important functionality for systems designed to convey claim verification explainability. Recent works have experimented with applying transformers-based or LLM-based text summarization methods in a zero-shot or few-shot manner, making use of some existing texts available in the summary parts of fact-check reports (e.g., called "justification'' in PolitiFact). However, for complex fact-checks, the purely sub-symbolic summarizers tend to either omit some elements of the fact-checker's argumentation chains or include contextual statements that may not be essential at the given level of granularity. In this paper, we propose a new method for enhancing fact-check summarization with the aim of injecting elements of structured fact-checker argumentation. This argumentation is, in turn, not only captured at the discourse level but tied to an entity graph representing the fact-check, for which we employ the PURO diagrammatic language. We have empirically performed a manual analysis of fact-check reports from two fact-checker websites, yielding (1) textual snippets containing the argumentation essence of the fact-check report and (2) categorized argumentation elements tied to entity graphs. These snippets are then fed to a state-of-the-art hybrid summarizer which has previously produced accurate fact-check summaries, as an additional input. We observe mild improvements on various ROUGE metrics, even if the validity of the results is limited given the small size of the dataset. We also compare the human-provided argumentation element categories with those returned, for the given fact-check ground truth summary, using a pre-trained language model upon both basic and augmented prompting. This yields a moderate accuracy as the model often fails to comply with the explicit given instructions.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1473–1481},
numpages = {9},
keywords = {argumentation, entity graph, fact-checking, text summarization},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1109/SC41406.2024.00013,
author = {Dharuman, Gautham and Hippe, Kyle and Brace, Alexander and Foreman, Sam and Hatanp\"{a}\"{a}, V\"{a}in\"{o} and Sastry, Varuni K. and Zheng, Huihuo and Ward, Logan and Muralidharan, Servesh and Vasan, Archit and Kale, Bharat and Mann, Carla M. and Ma, Heng and Cheng, Yun-Hsuan and Zamora, Yuliana and Liu, Shengchao and Xiao, Chaowei and Emani, Murali and Gibbs, Tom and Tatineni, Mahidhar and Canchi, Deepak and Mitchell, Jerome and Yamada, Koichi and Garzaran, Maria and Papka, Michael E. and Foster, Ian and Stevens, Rick and Anandkumar, Anima and Vishwanath, Venkatram and Ramanathan, Arvind},
title = {MProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal Protein Design Workflows with Direct Preference Optimization},
year = {2024},
isbn = {9798350352917},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SC41406.2024.00013},
doi = {10.1109/SC41406.2024.00013},
abstract = {We present a scalable, end-to-end workflow for protein design. By augmenting protein sequences with natural language descriptions of their biochemical properties, we train generative models that can be preferentially aligned with protein fitness landscapes. Through complex experimental- and simulation-based observations, we integrate these measures as preferred parameters for generating new protein variants and demonstrate our workflow on five diverse supercomputers. We achieve &gt;1 ExaFLOPS sustained performance in mixed precision on each supercomputer and a maximum sustained performance of 4.11 ExaFLOPS and peak performance of 5.57 ExaFLOPS. We establish the scientific performance of our model on two tasks: (1) across a predetermined benchmark dataset of deep mutational scanning experiments to optimize the fitness-determining mutations in the yeast protein HIS7, and (2) in optimizing the design of the enzyme malate dehydrogenase to achieve lower activation barriers (and therefore increased catalytic rates) using simulation data. Our implementation thus sets high watermarks for multimodal protein design workflows.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis},
articleno = {7},
numpages = {13},
keywords = {AI, HPC, Large language models, protein design},
location = {Atlanta, GA, USA},
series = {SC '24}
}

@inproceedings{10.5555/3522802.3522977,
author = {G\"{u}tlein, Moritz and German, Reinhard and Djanatliev, Anatoli},
title = {Hide your model! layer abstractions for data-driven co-simulations},
year = {2022},
publisher = {IEEE Press},
abstract = {Modeling and simulating of problems that span across multiple domains can be tricky. Often, the need for a co-simulation arises, for example because the modeling cannot be done with a single tool. Domain experts may face a barrier when it comes to the implementation of such a co-simulation. In addition, the demand for integrating data from various sources into simulation models seems to be growing. Therefore, we propose an abstraction concept that hides simulators and models behind generalized interfaces that are derived from prototypical classes. The data-driven abstraction concept facilitates having an assembly kit with predefined simulator building blocks that can be easily plugged together. Furthermore, data streams can be seamlessly ingested into such a composed model. Likewise, the co-simulation can be accessed via the resulting interfaces for further processing and interactions.},
booktitle = {Proceedings of the Winter Simulation Conference},
articleno = {206},
numpages = {12},
location = {Phoenix, Arizona},
series = {WSC '21}
}

@inproceedings{10.1145/3469213.3470224,
author = {Li, Haixia},
title = {A New query method for the temporal RDF Model RDFMT Based on SPARQL},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470224},
doi = {10.1145/3469213.3470224},
abstract = {With the explosion of real-time data, the representation and query of temporal data has become a hot research topic. Many researchers have proposed various temporal representation models and query methods. On the basis of the proposed temporal model RDFMT, we put forward the query language SPARQLMT for RDFMT. SPARQLMT is expanded based on SPARQL language, adding a syntax and semantics that is convenient for querying temporal information. As we all know, SPARQL is the official query language of the standard RDF model. SPARQLMT is based on SPARQL, which is also conducive to using the SPARQL query engine. In this paper we mainly illustrate a query method SPARQLMT for the RDFMT by extending SPARQL and give the semantics and syntax of SPARQLMT.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {24},
numpages = {6},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@article{10.5555/3648699.3648933,
author = {Liang, Paul Pu and Lyu, Yiwei and Fan, Xiang and Agarwal, Aravind and Cheng, Yun and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
title = {MULTIZOO \&amp; MULTIBENCH: a standardized toolkit for multimodal deep learning},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MULTIZOO, a public toolkit consisting of standardized implementations of &gt; 20 core multimodal algorithms and MULTIBENCH, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MULTIBENCH paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community. Code: https://github.com/pliang279/MultiBench Documentation: https://multibench.readthedocs.io/en/latest/},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {234},
numpages = {7},
keywords = {multimodal learning, representation learning, benchmarks, open source software}
}

@inproceedings{10.1145/3184558.3186943,
author = {Shi, Baoxu and Weninger, Tim},
title = {Visualizing the Flow of Discourse with a Concept Ontology},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186943},
doi = {10.1145/3184558.3186943},
abstract = {Understanding and visualizing human discourse has long being a challenging task. Although recent work on argument mining have shown success in classifying the role of various sentences, the task of recognizing concepts and understanding the ways in which they are discussed remains challenging. Given an email thread or a transcript of a group discussion, our task is to extract the relevant concepts and understand how they are referenced and re-referenced throughout the discussion. In the present work, we present a preliminary approach for extracting and visualizing group discourse by adapting Wikipedia's category hierarchy to be an external concept ontology. From a user study, we found that our method achieved better results than 4 strong alternative approaches, and we illustrate our visualization method based on the extracted discourse flows.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {89–90},
numpages = {2},
keywords = {concept ontology, discourse visualization},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3502223.3502744,
author = {Takhom, Akkharawoot and Utasri, Tharathon and Leenoi, Dhanon and Soomjinda, Pitchaya and Boonkwan, Prachya and Supnithi, Thepchai},
title = {Knowledge Graph Enhanced Community Consensus: a Scenario-based Knowledge Construction on Buddha Images},
year = {2022},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502744},
doi = {10.1145/3502223.3502744},
booktitle = {Proceedings of the 10th International Joint Conference on Knowledge Graphs},
pages = {191–194},
numpages = {4},
keywords = {Community consensus, Community-driven approach, Knowledge construction, Knowledge engineering, Knowledge graph, Ontology development, Semantic Web},
location = {Virtual Event, Thailand},
series = {IJCKG '21}
}

@inproceedings{10.1145/3432291.3432305,
author = {Cao, Suqun and Lingao, Wang and Ji, Rendong and Wang, Chao and Yao, Liu and Kai, Lin and Abdalla, Ahmed N. and k., Sujatha},
title = {Clinical Decision Support System Based on KNN/Ontology Extraction Method},
year = {2020},
isbn = {9781450375733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3432291.3432305},
doi = {10.1145/3432291.3432305},
abstract = {The complexity of the knowledge structure in the clinical cases, involving a wide range of attributes, results in making its case similarity calculation more complex. The existing medical ontologies, due to different expressions of the same concepts in computer information retrieval, causes difficulties in terms of sharing useful information in different database systems. This paper constructs a new decision support system based on KNN/ontology method was proposed. The detail of the methods and processes of common clinical case knowledge acquisition in combination with the method of obtaining structured information has been presented. The clinical case data similarity calculation method based on various types such as symptom information, medical history information, complications, surgical information, diagnostic results and other information, for record of a clinical diagnosis and treatment process. The validity of the similarity calculation method and the weight calculation method is verified by the clinical case data. The proposed methods can be effective for improving the quality and level of clinical services for medical service organizations.},
booktitle = {Proceedings of the 2020 3rd International Conference on Signal Processing and Machine Learning},
pages = {56–62},
numpages = {7},
keywords = {Neural Network, Similarity, clinical medicine, extraction method},
location = {Beijing, China},
series = {SPML '20}
}

@inproceedings{10.1145/3731120.3744624,
author = {Sign\'{e}, Quentin and Boughanem, Mohand and Moreno, Jose G. and Belkacem, Thiziri},
title = {A Substring Extraction-Based RAG Method for Minimising Hallucinations in Aircraft Maintenance Question Answering},
year = {2025},
isbn = {9798400718618},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3731120.3744624},
doi = {10.1145/3731120.3744624},
abstract = {Hallucination occurs when a language model generates plausible yet nonfactual information. In particular, faithfulness hallucinations (inconsistency with a given context) cannot be tolerated in critical domains such as aircraft maintenance due to the potentially severe consequences. To mitigate this issue, Retrieval Augmented Generation (RAG) methods have been introduced. These approaches are relevant for reducing the risks of hallucination but do not eliminate them, as the generator may still produce content unfaithful to the retrieved context. This paper proposes a novel RAG approach that leverages a substring extraction tool from retrieved documents to minimise hallucinations. Experiments performed on real aircraft maintenance documentation revealed that, despite the lower accuracy of the answers compared to traditional RAG methods, the proposed approach demonstrates an improved control over hallucination risks. This highlights the potential of our method in highly technical use cases where accuracy and reliability are key.},
booktitle = {Proceedings of the 2025 International ACM SIGIR Conference on Innovative Concepts and Theories in Information Retrieval (ICTIR)},
pages = {513–521},
numpages = {9},
keywords = {question answering, retrieval augmented generation, technical maintenance},
location = {Padua, Italy},
series = {ICTIR '25}
}

@inproceedings{10.1145/3397482.3450619,
author = {Kaindl, Hermann},
title = {ModelGenGUIs – High-level Interaction Design with Discourse Models for Automated GUI Generation},
year = {2021},
isbn = {9781450380188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397482.3450619},
doi = {10.1145/3397482.3450619},
abstract = {Since manual creation of user interfaces is hard and expensive, automated generation may become more and more important in the future. Instead of generating UIs from simple abstractions, transforming them from high-level models should be more attractive. In particular, we let an interaction designer model discourses in the sense of dialogues (supported by a tool), inspired by human-human communication. This tutorial informs about our approach, both about its advantages and its challenges (e.g., in terms of usability of generated UIs). In particular, our unique approach to optimization for a given device (e.g., a Smartphone) that applies Artificial Intelligence (AI) techniques will be high-lighted, as well as the techniques based on ontologies for automated GUI generation and customization. We also address low-vision accessibility of Web-pages, by combining automated design-time generation of Web-pages with responsive design for improving accessibility.},
booktitle = {Companion Proceedings of the 26th International Conference on Intelligent User Interfaces},
pages = {3–4},
numpages = {2},
keywords = {Interaction design, automated GUI generation, customization, low-vision accessibility of Web-pages, discourse models, task models},
location = {College Station, TX, USA},
series = {IUI '21 Companion}
}

@inproceedings{10.1145/3511808.3557617,
author = {Anelli, Vito Walter and Biancofiore, Giovanni Maria and De Bellis, Alessandro and Di Noia, Tommaso and Di Sciascio, Eugenio},
title = {Interpretability of BERT Latent Space through Knowledge Graphs},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557617},
doi = {10.1145/3511808.3557617},
abstract = {The advent of pretrained language have renovated the ways of handling natural languages, improving the quality of systems that rely on them. BERT played a crucial role in revolutionizing the Natural Language Processing (NLP) area. However, the deep learning framework it implements lacks interpretability. Thus, recent research efforts aimed to explain what BERT learns from the text sources exploited to pre-train its linguistic model. In this paper, we analyze the latent vector space resulting from the BERT context-aware word embeddings. We focus on assessing whether regions of the BERT vector space hold an explicit meaning attributable to a Knowledge Graph (KG). First, we prove the existence of explicitly meaningful areas through the Link Prediction (LP) task. Then, we demonstrate these regions being linked to explicit ontology concepts of a KG by learning classification patterns. To the best of our knowledge, this is the first attempt at interpreting the BERT learned linguistic knowledge through a KG relying on its pretrained context-aware word embeddings.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {3806–3810},
numpages = {5},
keywords = {deep learning, knowledge graphs, natural language processing},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.5555/3507788.3507811,
author = {Kici, Derya and Bozanta, Aysun and Cevik, Mucahit and Parikh, Devang and Ba\c{s}ar, Ay\c{s}e},
title = {Text classification on software requirements specifications using transformer models},
year = {2021},
publisher = {IBM Corp.},
address = {USA},
abstract = {Text classification in Software Requirements Specifications (SRS) documents is an essential task for various purposes including automatically extracting requirements and their types as well as identification of duplicate or conflicting information, which all contribute to avoiding potential issues in the later stages of the software development life cycle. While a variety of machine learning approaches have been considered for text classification over SRS documents, many of these fail to provide adequate performance as they often ignore the meaning of software artifacts or integrate domain knowledge for the classification task. Recent advances in deep learning methodology have significantly contributed to Natural Language Processing (NLP) and text classification. One of the main challenges in using deep learning models for various NLP tasks in the software engineering domain is the scarcity of labeled textual data. In addition, even with sufficient data, training from the scratch still requires significant training time and computational resources. Transfer learning is a novel approach that proposes a solution to such reservations by providing pre-trained models that enable fine-tuning with the customized data. In this research, we conduct an empirical analysis on multi-class text classification over SRS documents using different pre-trained transformer models including BERT, DistilBERT, Roberta, AlBERT, and XLNet, and compare their performance. We test the performance of these models using three SRS datasets: DOORS, NFR-PROMISE, and PURE. Our numerical study shows that the transformer models are able to generate highly accurate results to classify all categories except Priority of the requirements. While all models provide a 80\% or higher accuracy for other classification tasks, the accuracy of the models to classify the Priority does not exceed 60\%.},
booktitle = {Proceedings of the 31st Annual International Conference on Computer Science and Software Engineering},
pages = {163–172},
numpages = {10},
keywords = {BERT, NLP, software requirement specifications, text classification, transfer learning},
location = {Toronto, Canada},
series = {CASCON '21}
}

@inproceedings{10.1145/3177148.3180086,
author = {Refoufi, Allaoua and Benarab, Achref},
title = {A Robust Approach to the Ontology Matching Problem},
year = {2018},
isbn = {9781450352901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3177148.3180086},
doi = {10.1145/3177148.3180086},
abstract = {Ontology matching is the process that identifies correspondences between similar concepts in two different ontologies of the same domain of discourse to solve knowledge heterogeneous problems. We propose an automatic similarity based matching algorithm that exploits almost all types of entities descriptions as well as their relations to effectively compute the correspondences between the two to be matched ontologies. The iterative algorithm computes each measure of similarity separately and then aggregates them in a linear combination to compose the final similarity score. The measures used deal with linguistic, semantic, and structural as well as many other measures to gain efficiency. We also include a new similarity measure based on dynamic programming in conjunction with known measures to refine the similarity process. Finally, we provide comparative experimental results in support of our method on several well-known ontology benchmarks recommended by the OAEI1. The results obtained are shown to be quite superior compared to the state-of-the-art ontology matching systems.},
booktitle = {Proceedings of the 2nd Mediterranean Conference on Pattern Recognition and Artificial Intelligence},
pages = {76–83},
numpages = {8},
keywords = {OWL/XML, Ontology matching, WordNet, ontology alignment, similarity measure},
location = {Rabat, Morocco},
series = {MedPRAI '18}
}

@inproceedings{10.1145/3637494.3637508,
author = {Li, Xuxin and Wang, Ge and Wang, Yue and Zhou, Qirui},
title = {Mixed Knowledge-enhance Empathetic Dialogue Generation},
year = {2024},
isbn = {9798400716300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637494.3637508},
doi = {10.1145/3637494.3637508},
abstract = {Empathy plays a pivotal role in human communication, and thus, it is an essential capability that any human-centered dialogue system should possess. Early research on empathetic response generation often focused on directly capturing the emotional state of the context using fixed emotion labels. However, the logical aspects exhibited in human conversations heavily rely on experiential and knowledge-based resources within the brain. This implies that whether the aim is to acquire more nuanced emotional states or to generate responses enriched with comprehensive information, the incorporation of external knowledge as supplementary information in empathetic dialogue systems is imperative. In response to this challenge, we propose a novel approach for extracting external knowledge. This is achieved by designing two components: a fine-grained knowledge graph constructed using the context and an external knowledge base, and coarse-grained knowledge acquisition based on COMET. These two scales of knowledge are then integrated with the context using methods like context refinement. This not only make the model to gain a deeper understanding of the user's context but also enhances the expression of empathy in the dialogue system. We conducted extensive experiments on the EMPATHETICDIALOGUES dataset and demonstrated the superiority of our approach over the baseline model.CCS CONCEPTS • Computing methodologies∼Artificial intelligence∼Natural language processing∼Discourse, dialogue and pragmatics},
booktitle = {Proceedings of the 2023 International Conference on Electronics, Computers and Communication Technology},
pages = {77–81},
numpages = {5},
keywords = {Natural Language Processing, deep learning, dialog system, empathetic dialogue generation, external knowledge},
location = {Guilin, China},
series = {CECCT '23}
}

@inproceedings{10.1145/3230905.3230941,
author = {Guermah, Hatim and Fissaa, Tarik and Guermah, Bassma and Hafiddi, Hatim and Nassar, Mahmoud},
title = {Using Context Ontology and Linear SVM for Chronic Kidney Disease Prediction},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230941},
doi = {10.1145/3230905.3230941},
abstract = {In the e-Health learning area, the use of chronic patient context has become very important given the increase in the number of individuals who suffer from these diseases and the unavailability of medications. Specifically, chronic kidney failure is one of the diseases that goes undetected and undiagnosed until it is well advanced. The need for preventive prediction remains an essential task for the well-being of patients at risk. In this paper, we aim to explore the added value of using ontology-based prediction, focusing on Linear SVM, to deal with Chronic Kidney Disease Problems.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {48},
numpages = {6},
keywords = {Chronic Kidney Disease, Context-Awareness, Linear SVM, Ontologies},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@inproceedings{10.1109/MODELS-C.2019.00020,
author = {Rossi, Maria Teresa and De Sanctis, Martina and Iovino, Ludovico and Rutle, Adrian},
title = {A multilevel modelling approach for tourism flows detection},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00020},
doi = {10.1109/MODELS-C.2019.00020},
abstract = {Application development in the Internet of Things (IoT) faces various issues such as lack of separation of concerns and lack of high-level abstraction to address its large scale and heterogeneity. MDE supports the management of this heterogeneity raising the level of abstraction and thanks to its core operations. Multilevel modelling makes it possible to extend MDE techniques to more than two meta-levels permitting model elements to have a dual type-instance dimension, making it particularly suitable for this application domain. People flow monitoring and detection is one of the hot topics in smart cities projects. In this paper, we exploit MDE techniques, through multilevel modelling approaches, to design the infrastructure supporting a solution part of a comprehensive project related to urban informatics. Moreover, even if we target the people flow monitoring and detection scenario, the provided multilevel approach is open and extensible to further IoT scenarios, to specifically manage the evolutionary nature of the IoT.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {103–112},
numpages = {10},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1145/3456172.3456211,
author = {Hananto, Valentinus R. and Serd\"{u}lt, Uwe and Kryssanov, Victor},
title = {A Tourism Knowledge Model through Topic Modeling from Online Reviews},
year = {2021},
isbn = {9781450388450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456172.3456211},
doi = {10.1145/3456172.3456211},
abstract = {Ontologies and knowledge models have gained more recognition because of their extensive use in recommender systems. The lack of automatic approaches in ontology engineering, however, becomes a challenge to fulfill increasing needs for such knowledge models in the field of tourism. In this study, a system for building tourism knowledge models from online reviews is proposed. The main contribution of the study is the application of topic modeling to build a knowledge model that, in turn, allows for an automated labeling process to train classifiers. Given a collection of unlabeled tourism online reviews, Latent Dirichlet Allocation (LDA) is applied to automatically label each document. Each topic discovered by LDA is labeled with one specific category, representing its semantic meaning based on an existing general ontology as a reference. These automatically labeled documents are used for classification, and the result is compared with manual annotation. Experiments on Indonesian tourism datasets showed that the automatic labeling approach using LDA provides for a precision score of 70\%. In classification tasks, this approach can achieve comparable or even better classification performance than the manual labeling. The results obtained suggest that the developed system is capable of building a tourism knowledge model and providing acceptable-quality training data for the development of tourism recommender systems.},
booktitle = {Proceedings of the 2021 7th International Conference on Computing and Data Engineering},
pages = {87–93},
numpages = {7},
keywords = {recommender systems, topic modeling, tourism knowledge model},
location = {Phuket, Thailand},
series = {ICCDE '21}
}

@article{10.14778/3681954.3682023,
author = {Ke, Jin and Zacouris, Zenon and Acosta, Maribel},
title = {Efficient Validation of SHACL Shapes with Reasoning},
year = {2024},
issue_date = {July 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3681954.3682023},
doi = {10.14778/3681954.3682023},
abstract = {As the usage of knowledge graphs (KGs) becomes more pervasive in practical applications, there is a burgeoning need for high-quality data. The SHApes Constraint Language (SHACL) allows for expressing certain types of quality constraints that define sub-structures and correct values in KGs modelled with RDF. Nevertheless, performing SHACL validation without entailment often yields onesided outcomes, as it falls short of validating crucial implicit data encoded in the KG ontology. Current solutions that incorporate entailment into SHACL validation are inefficient, due to the time-intensive process of applying inference rules to the entire dataset. Moreover, applying entailment for SHACL validation can generate large amounts of redundant triples, exacerbating the validation workload and resulting in erroneous or redundant validation results. In light of these challenges, we propose Re-SHACL, an approach that combines targeted reasoning and entity merging techniques to generate a concise, consolidated RDF graph devoid of redundancy. Re-SHACL significantly reduces execution time and improves the accuracy of the validation reports. Our experiments demonstrate that Re-SHACL can be combined with state-of-the-art validators to deliver accurate validation reports efficiently.},
journal = {Proc. VLDB Endow.},
month = jul,
pages = {3589–3601},
numpages = {13}
}

@inproceedings{10.5555/3586210.3586257,
author = {Schroeder, Shane A. and Vendome, Christopher and Giabbanelli, Philippe J. and Montfort, Alan M.},
title = {Towards Reusable Building Blocks to Develop Covid-19 Simulation Models},
year = {2023},
publisher = {IEEE Press},
abstract = {Modeling \&amp; Simulation has played an essential role in supporting the decision-making activities of policymakers for COVID-19. However, a proliferation of models has been noted in the literature, and new models are only more likely to emerge given the shift to long-term management of the disease and the call for highly tailored tools. Having a multiplicity of models can have benefits, for example when contributing to ensembles of models. However, if each model is created from scratch, there is significant redundancy in efforts hence time inefficiency and a heightened risk of bugs. Our study examines the naturally occurring practices of modelers who wrote COVID-19 models in NetLogo to identify redundancy in code and thus suggest reusable 'building blocks' that would speed-up the process of model development as well as improving code quality. Based on 28 models, we identified five themes and discussed their transformation into potential building blocks for simulation.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {569–580},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3616855.3635724,
author = {Dave, Vachik S. and Pang, Linsey and Cui, Xiquan and Luo, Chen and Zamani, Hamed and Wu, Lingfei and Karypis, George},
title = {The 3rd International Workshop on Interactive and Scalable Information Retrieval Methods for eCommerce (ISIR-eCom 2024)},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3616855.3635724},
doi = {10.1145/3616855.3635724},
abstract = {Over the past few years, consumer behavior has shifted from traditional in-store shopping to online shopping. For example, eCommerce sales have grown from around 5\% of total US sales in 2012 to around 15.4\% in year 2023. This rapid growth of eCommerce has created new challenges and vital new requirements for intelligent information retrieval systems. Which lead to the primary motivations of this workshop:(1) Since the pandemic hit, eCommerce became an important part of people's routine and they started using online shop- ping for smallest grocery items to big electronics as well as cars. With such a large assortment of products and millions of users, achieving higher scalability without losing accuracy is a leading concern for information retrieval systems for eCommerce.(2) The diverse buyers make the relevance of the results highly subjective, because relevance varies for different buyers. The most suitable and intuitive solution to this problem is to make the system interactive and provide correct relevance for different users. Hence, interactive information retrieval systems are becoming necessity in eCommerce.(3) To handle sudden change in buyers' behavior, industries adopted existing sub-optimal information retrieval techniques for various eCommerce tasks. Parallelly, they also started exploring/researching for better solutions and in dire need of help from research community.This workshop will provide a forum to discuss and learn the latest trends for interactive and scalable information retrieval approaches for eCommerce. It will provide academic and industrial researchers a platform to present their latest works, share research ideas, present and discuss various challenges, and identify the areas where further research is needed. It will foster the development of a strong research community focused on solving eCommerce-related information retrieval problems that provide superior eCommerce experience to all users.},
booktitle = {Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
pages = {1208–1209},
numpages = {2},
keywords = {ecommerce search, information retrieval, interactive systems, large language models (llms) in ecommerce, natural language processing (nlp) for ecommerce, ranking models, recommender systems},
location = {Merida, Mexico},
series = {WSDM '24}
}

@article{10.1145/3745789,
author = {Abraham, Joel and Austin, Mark and Gilbert, Mark R. and Celiku, Orieta},
title = {Semantic Foundations for Precision Medicine},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3745789},
doi = {10.1145/3745789},
abstract = {Precision medicine, which aims to optimize medical care at the individual level, remains a significant challenge and aspiration in oncology. The pathway to a successful implementation requires methods that can work with a vast heterogeneity of cancer, and consider the interplay of environmental, societal, biological, and clinical factors. To support decision-making in this context, computational frameworks must integrate large-scale, diverse, and noisy data, discover fine-grained patient subgroups with shared underlying characteristics, and characterize the imperfect preclinical spaces where novel therapies are tested. We propose an integrated digital-twin framework in which machine learning and semantic models collaboratively represent and reason with diverse patient data and medical domain knowledge to generate treatment recommendations. Clinical and molecular characteristics are used to discover subtypes of brain cancers, which are represented as ontologies with associated rules to determine a patient’s membership in a given subtype. Similarly, preclinical models used for therapeutic testing are characterized and assessed for their similarity to patient cancer models. By semantically discovering links between these preclinical models and patient cancer subtypes, novel therapeutics tested on preclinical models can be prioritized and hypothesized for individual patients. This approach, which requires empirical testing, demonstrates how cross-domain reasoning can be used to propose individualized treatment plans.},
note = {Just Accepted},
journal = {ACM Trans. Comput. Healthcare},
month = jun,
keywords = {Digital Twins, Semantic Models, Ontologies, Machine Learning}
}

@inproceedings{10.1145/3281375.3281378,
author = {Saleme, Est\^{e}v\~{a}o B. and Santos, Celso A. S. and Falbo, Ricardo A. and Ghinea, Gheorghita and Andres, Frederic},
title = {Towards a reference ontology on mulsemedia systems},
year = {2018},
isbn = {9781450356220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281375.3281378},
doi = {10.1145/3281375.3281378},
abstract = {The use of multiple senses in interactive applications has become increasingly feasible due to the upsurge of commercial, off-the-shelf devices to produce sensory effects. Creating Multiple Sensorial Media (MulSeMedia) immersive systems requires understanding their digital ecosystem. Mulsemedia systems encompass a set of applications, and devices of different types assembled to communicate or express feelings from the virtual world to the real world. Despite existing standards, tools, and recent research devoted to them, there is still a lack of formal and explicit representation of what mulse-media is. Misconceptions could eventually lead to the construction of solutions that might not take into account reuse, integration, standardization, among other design features. In this paper, we propose to establish a common conceptualization about mulsemedia systems through a reference ontology, named MulseOnto, covering their main notions. To evaluate it, we applied ontology verification and validation techniques, including assessment by humans and a data-driven approach. The results showed that MulseOnto can be used as a consensual conceptual model for exploring the knowledge about the whole chain of mulsemedia systems.},
booktitle = {Proceedings of the 10th International Conference on Management of Digital EcoSystems},
pages = {23–30},
numpages = {8},
keywords = {mulsemedia systems, multimedia, reference ontology},
location = {Tokyo, Japan},
series = {MEDES '18}
}

@inproceedings{10.1145/3687997.3695635,
author = {Esterhuyse, Christopher A. and van Binsbergen, L. Thomas},
title = {Cooperative Specification via Composition Control},
year = {2024},
isbn = {9798400711800},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687997.3695635},
doi = {10.1145/3687997.3695635},
abstract = {High-level, declarative specification languages are typically highly modular: specifications are comprised of fragments that are themselves meaningful.  As such, complex specifications are built from incrementally composed fragments.  In a cooperative specification, different fragments are contributed by different agents, usually capturing requirements on different facets of the system.  For example, legal regulators and system administrators cooperate to specify the behaviour of a data exchange system.  In practice, cooperative specification is difficult, as different contributors' requirements are difficult to elicit, express, and compose.   In this work, we characterise cooperative specification and adopt an approach that leverages language features specifically introduced for controlling specification composition.   In our approach, specifications model the domain as usual, but also specify how specifications may change.  For example, a legal regulator defines 'consent to process data' and specifies which agents may consent, and which relaxations of the requirement are permitted.  We propose and demonstrate generic language extensions that improve composition control in three case study languages: Datalog, Alloy, and eFLINT.  We reflect on how these extensions improve composition control, and afford new data exchange scenarios.   Finally, we relate our contributions to existing works, and to the greater vision of multi-agent data exchange to the satisfaction of their shared, complex, dynamic requirements.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {2–15},
numpages = {14},
keywords = {Data Exchange, Meta-Programming, Program Composition, Program Refinement, Specification Languages},
location = {Pasadena, CA, USA},
series = {SLE '24}
}

@article{10.1145/3401027,
author = {Rizvi, Syed Zain Raza and Fong, Philip W. L.},
title = {Efficient Authorization of Graph-database Queries in an Attribute-supporting ReBAC Model},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {2471-2566},
url = {https://doi.org/10.1145/3401027},
doi = {10.1145/3401027},
abstract = {Neo4j is a popular graph database that offers two versions: an enterprise edition and a community edition. The enterprise edition offers customizable Role-based Access Control features through custom developed procedures, while the community edition does not offer any access control support. Being a graph database, Neo4j appears to be a natural application for Relationship-Based Access Control (ReBAC), an access control paradigm where authorization decisions are based on relationships between subjects and resources in the system (i.e., an authorization graph). In this article, we present AReBAC, an attribute-supporting ReBAC model for Neo4j that provides finer-grained access control by operating over resources instead of procedures. AReBAC&nbsp;employs Nano-Cypher, a declarative policy language based on Neo4j’s Cypher query language, the result of which allows us to weave database queries with access control policies and evaluate both simultaneously. Evaluating the combined query and policy produces a result that (i) matches the search criteria, and (ii) the requesting subject is authorized to access. AReBAC&nbsp;is accompanied by the algorithms and their implementation required for the realization of the presented ideas, including GP-Eval, a query evaluation algorithm. We also introduce Live-End Backjumping (LBJ), a backtracking scheme that provides a significant performance boost over conflict-directed backjumping for evaluating queries. As demonstrated in our previous work, the original version of GP-Eval already performs significantly faster than the Neo4j’s Cypher evaluation engine. The optimized version of GP-Eval, which employs LBJ, further improves the performance significantly, thereby demonstrating the capabilities of the technique.},
journal = {ACM Trans. Priv. Secur.},
month = jul,
articleno = {18},
numpages = {33},
keywords = {Neo4j, Relationship-based access control, attributes, graph database, graph patterns, live-end backjumping, nano-cypher}
}

@inproceedings{10.1145/3502223.3502746,
author = {Rusmawati, Yanti},
title = {Automated Reasoning on Machine Learning Model of Legislative Election Prediction},
year = {2022},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502746},
doi = {10.1145/3502223.3502746},
abstract = {Prediction models using machine learning have been utilized in various fields, including the general election prediction model. However, we still need more insight into the model result through explainable AI. To reason the result, in this in-use paper, here we compare two approaches: using ontology reasoner Prot\'{e}g\'{e} and Silas (a machine learning tool empowered with automated reasoning). Using the data set of the Indonesia legislative election in 2019, we build the prediction model, followed by extracting the formula from the decision tree then reasoning the model predicates. The result shows that to some extent we can have a better understanding of the reasonable result from the machine learning prediction model.},
booktitle = {Proceedings of the 10th International Joint Conference on Knowledge Graphs},
pages = {200–204},
numpages = {5},
keywords = {automated reasoning, explainable AI, machine learning},
location = {Virtual Event, Thailand},
series = {IJCKG '21}
}

@inproceedings{10.1145/3366423.3380229,
author = {Andresel, Medina and Corman, Julien and Ortiz, Magdalena and Reutter, Juan L. and Savkovic, Ognjen and Simkus, Mantas},
title = {Stable Model Semantics for Recursive SHACL},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380229},
doi = {10.1145/3366423.3380229},
abstract = {SHACL (SHape Constraint Language) is a W3C recommendation for validating graph-based data against a set of constraints (called shapes). Importantly, SHACL allows to define recursive shapes, i.e. a shape may refer to itself, directly of indirectly. The recommendation left open the semantics of recursive shapes, but proposals have emerged recently to extend the official semantics to support recursion. These proposals are based on the principle of possibility (or non-contradiction): a graph is considered valid against a schema if one can assign shapes to nodes in such a way that all constraints are satisfied. This semantics is not constructive, as it does not provide guidelines about how to obtain such an assignment, and it may lead to unfounded assignments, where the only reason to assign a shape to a node is that it allows validating the graph. In contrast, we propose in this paper a stricter, more constructive semantics for SHACL, based on stable models, which are well-known in Answer Set Programming (ASP). This semantics additionally requires a shape assignment to be properly justified by the input constraints. We further exploit the connection to logic programming, and show that SHACL constraints can be naturally represented as logic programs, and that the validation problem for a graph and a SHACL schema can be encoded as an ASP reasoning task. The proposed semantics also enjoys computationally tractable validation in the presence of constraints with stratified negation (as opposed to the previous semantics). We also extend our semantics to 3-valued stable models, which yields a more relaxed notion of validation, tolerant to certain faults in the schema or data. By exploiting a connection between 3-valued stable model semantics and the well-founded semantics for logic programs, we can use our translation into ASP to show another tractability result. Finally, we provide a preliminary evaluation of the approach, which leverages an ASP solver to perform graph validation.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {1570–1580},
numpages = {11},
keywords = {SHACL, answer set programming, graph-structured data},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@article{10.1145/3342356,
author = {Harikrishna, D. M. and Rao, K. Sreenivasa},
title = {Children’s Story Classification in Indian Languages Using Linguistic and Keyword-based Features},
year = {2019},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3342356},
doi = {10.1145/3342356},
abstract = {The primary objective of this work is to classify Hindi and Telugu stories into three genres: fable, folk-tale, and legend. In this work, we are proposing a framework for story classification (SC) using keyword and part-of-speech (POS) features. For improving the performance of SC system, feature reduction techniques and combinations of various POS tags are explored. Further, we investigated the performance of SC by dividing the story into parts depending on its semantic structure. In this work, stories are (i) manually divided into parts based on their semantics as introduction, main, and climax; and (ii) automatically divided into equal parts based on number of sentences in a story as initial, middle, and end. We have also examined sentence increment model, which aims at determining an optimum number of sentences required to identify story genre by incremental selection of sentences in a story. Experiments are conducted on Hindi and Telugu story corpora consisting of 300 and 150 short stories, respectively. The performance of SC system is evaluated using different combinations of keyword and POS-based features, with three well-established machine learning classifiers: (i) Naive Bayes (NB), (ii) k-Nearest Neighbour (KNN), and (iii) Support Vector Machine (SVM). Performance of the classifier is evaluated using 10-fold cross-validation and effectiveness of classifier is measured using precision, recall, and F-measure. From the classification results, it is observed that adding linguistic information boosts the performance of story classification. In view of the structure of the story, main, and initial parts of the story have shown comparatively better performance. The results from the sentence incremental model have indicated that the first nine and seven sentences in Hindi and Telugu stories, respectively, are sufficient for better classification of stories. In most of the studies, SVM models outperformed the other models in classification accuracy.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {30},
numpages = {22},
keywords = {K-nearest neighbour, keyword features, latent semantic analysis, linear discriminant analysis, linguistic features, naive Bayes, sparse representation, story classification, support vector machines, text-to-speech, vector space model}
}

@article{10.5555/3580619.3580624,
author = {Feng, Xingzhan and Periyasamy, Kasi},
title = {A Cost Estimation Model for Scrum Projects},
year = {2022},
issue_date = {November 2022},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {38},
number = {4},
issn = {1937-4771},
abstract = {One of the daunting tasks of software developers is to estimate the development cost of a new software product. Most cost estimation models use the set of requirements for the product as the starting point. Function-point, COCOMO and use case-based cost estimation models belong to this category. These models assume that the requirements of the product are fairly rigid. However, with the advent of agile-based software development methods, the requirements keep changing during the development process. Therefore, traditional cost estimation models need to be refined to accommodate changes in requirements. The refinements should reflect the changes in cost when the requirements change. In this paper, we describe a cost estimation model for projects that use scrum, a popular agile method. The model uses the requirements of the new software product, written in the form of user stories, as the primary source. The cost is adjusted every time the requirements are changed or new requirements are introduced. We have also developed a project tracking tool for scrum projects in which this model has been implemented. The model was applied to academic projects developed by graduate students; the results indicate that estimations are fairly reasonable.},
journal = {J. Comput. Sci. Coll.},
month = nov,
pages = {30–37},
numpages = {8}
}

@article{10.1145/3705313,
author = {Dang, Xiaochao and Ding, Guozhen and Dong, Xiaohui and Li, Fenfang and Gao, Shiwei and Wang, Yue},
title = {UIE-Based Relational Extraction Task for Mine Hoist Fault Data},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3705313},
doi = {10.1145/3705313},
abstract = {Information extraction is pivotal in natural language processing, where the goal is to convert unstructured text into structured information. A significant challenge in this domain is the diversity and specific needs of various processing tasks. Traditional approaches typically utilize separate frameworks for different information extraction tasks, such as named entity recognition and relationship extraction, which hampers their uniformity and scalability. In this study, this study introduce a Universal Information Extraction (UIE) framework combined with a cue learning strategy, significantly improving the efficiency and accuracy of extracting mine hoist fault data. Initially, domain-specific data is manually labeled to fine-tune the model, and the accuracy is further enhanced by constructing negative examples during this fine-tuning process. The model then focuses on faults using the Structured Extraction Language (SEL) and a schema-based prompt syntax, the Structural Schema Instructor (SSI), which targets and extracts key information from the fault data to meet specific domain requirements. Experimental results show that UIE substantially improves the processing efficiency and the F1 accuracy of the extracted mine hoist fault data, with the fine-tuned F1 score increasing from 23.59\% to 92.51\%.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {4},
numpages = {23},
keywords = {Joint extraction, mechanical problem, mining sector, prompt learning}
}

@inproceedings{10.1145/3424978.3425095,
author = {Shi, Yu and Qin, Qiuli},
title = {Construction of Neurosurgery Knowledge Graph Based on Bi-LSTM-CRF Model},
year = {2020},
isbn = {9781450377720},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3424978.3425095},
doi = {10.1145/3424978.3425095},
abstract = {Medical knowledge is complex. The knowledge graph provides an efficient solution for integrating medical knowledge and analyzing medical data. In this paper, the neurosurgery knowledge graph is constructed. First, the ontology pattern library is constructed, and then named entity recognition is performed based on the Bi-LSTM-CRF model. The entities are extracted from the text and the entity relationships are defined. The knowledge graph of the symptom-disease-department is integrated, and finally stored in Neo4j Graph database. As the basis of information retrieval, intelligent question answering, diagnosis and treatment system, knowledge graph can effectively transfer medical knowledge and provide intelligent medical assistance for doctors and patients.},
booktitle = {Proceedings of the 4th International Conference on Computer Science and Application Engineering},
articleno = {113},
numpages = {5},
keywords = {Bi-LSTM-CRF model, Knowledge graph, Neurosurgery},
location = {Sanya, China},
series = {CSAE '20}
}

@inproceedings{10.5555/3586210.3586387,
author = {Wilsdorf, Pia and Uhrmacher, Adelinde M.},
title = {Creating PROV-DM Graphs from Model Databases},
year = {2023},
publisher = {IEEE Press},
abstract = {Documenting the provenance of the main products of a simulation study plays a crucial role in improving the understanding of mechanistic, biological models as well as their reproducibility and credibility. With model databases already an ample collection of simulation models, including metainformation and source files, exists. In this paper, we bridge the gap between the information contained in model databases and the PROV-DM provenance standard, which allows making the diverse products and their relationships formally explicit. We present a procedure for creating PROV-DM graphs from model database entries, and illustrate the approach based on ten different models from the BioModels database. These case studies demonstrate the advantages of having a standardized provenance view in addition to the regular database entries, i.e., enhanced means for visualizing the structure of the simulation study and the curation process.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2118–2129},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@inproceedings{10.1145/3726302.3730344,
author = {Koleva, Aneta and Ringsquandl, Martin and Hatem, Ahmed and Runkler, Thomas and Tresp, Volker},
title = {Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3730344},
doi = {10.1145/3726302.3730344},
abstract = {Interest in solving table interpretation tasks has grown over the years, yet it still relies on existing datasets that may be overly simplified. This is potentially reducing the effectiveness of the dataset for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To enrich the existing benchmark datasets, we extract and annotate a new, more challenging dataset. The proposed Wiki-TabNER dataset features complex tables containing several entities per cell, with named entities labeled using DBpedia classes. This dataset is specifically designed to address named entity recognition (NER) task within tables, but it can also be used as a more challenging dataset for evaluating the entity linking task. In this paper we describe the distinguishing features of the Wiki-TabNER dataset and the labeling process. In addition, we propose a prompting framework for evaluating the new large language models on the within tables NER task. Finally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed~dataset.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3812–3820},
numpages = {9},
keywords = {named entity recognition, table interpretation},
location = {Padua, Italy},
series = {SIGIR '25}
}

@article{10.1145/3131610,
author = {Chantas, Giannis and Karavarsamis, Sotiris and Nikolopoulos, Spiros and Kompatsiaris, Ioannis},
title = {A Probabilistic, Ontological Framework for Safeguarding the Intangible Cultural Heritage},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3131610},
doi = {10.1145/3131610},
abstract = {In this article, we propose Multi-Entity Bayesian Networks (MEBNs) as the probabilistic ontological framework for the analysis of the Tsamiko and Salsa dances. More specifically, our analysis has the objective of the dancer assessment with respect to both choreography execution accuracy and the synchronization of the dance movements with the musical rhythm. For this task, we make use of the explicit, expert-provided knowledge on dance movements and their relations to the musical beat. Due to the complexity of this knowledge, the MEBNs were used as the probabilistic ontological framework in which the knowledge is formalized. The reason we opt for MEBNs for this task is that they combine Bayesian and formal (first-order) logic into a single model. In this way, the Bayesian probabilistic part of MEBNs was used to capture, using example data and training, the implicit part of the expert knowledge about dances, i.e., this part of the knowledge that cannot be formalized and explicitly defined accurately enough, while the logical maintains the explicit knowledge representation in the same way ontologies do. Moreover, we present in detail the MEBN models we built for Tsamiko and Salsa, using expert-provided explicit knowledge. Last, we conduct experiments that demonstrate the effectiveness of the proposed MEBN-based methodology we employ to achieve our analysis objectives. The results of the experiments demonstrate the superiority of MEBNs to conventional models, such as BNs, in terms of the dancer assessment accuracy.},
journal = {J. Comput. Cult. Herit.},
month = aug,
articleno = {12},
numpages = {29},
keywords = {Intangible cultural heritage, multi-entity bayesian networks, multimodal semantic analysis}
}

@inproceedings{10.1145/3360901.3364443,
author = {Kaffee, Lucie-Aim\'{e}e and Endris, Kemele M. and Simperl, Elena and Vidal, Maria-Esther},
title = {Ranking Knowledge Graphs By Capturing Knowledge about Languages and Labels},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364443},
doi = {10.1145/3360901.3364443},
abstract = {Capturing knowledge about the mulitilinguality of a knowledge graph is of supreme importance to understand its applicability across multiple languages. Several metrics have been proposed for describing mulitilinguality at the level of a whole knowledge graph. Albeit enabling the understanding of the ecosystem of knowledge graphs in terms of the utilized languages, they are unable to capture a fine-grained description of the languages in which the different entities and properties of the knowledge graph are represented. This lack of representation prevents the comparison of existing knowledge graphs in order to decide which are the most appropriate for a multilingual application.In this work, we approach the problem of ranking knowledge graphs based on their language features and propose LINGVO, a framework able to capture mulitilinguality at different levels of granularity. Grounded in knowledge graph descriptions, LINGVO is, additionally, able to solve the problem of ranking knowledge graphs according to a degree of mulitilinguality of the represented entities. We have empirically studied the effectiveness of LINGVO in a benchmark of queries to be executed against existing knowledge graphs. The observed results provide evidence that LINGVO captures the mulitilinguality of the studied knowledge graphs similarly than a crowd-sourced gold standard.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {21–28},
numpages = {8},
keywords = {knowledge graph, multilinguality, question answering, ranking},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1145/3208806.3208830,
author = {Floty\'{n}ski, Jakub and Soboci\'{n}ski, Pawe\l{}},
title = {Semantic 4-dimensional modeling of VR content in a heterogeneous collaborative environment},
year = {2018},
isbn = {9781450358002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3208806.3208830},
doi = {10.1145/3208806.3208830},
abstract = {Interactive 3D content gains increasing use in VR/AR applications in different domains, such as education, training, engineering, spatial and urban planning as well as architectural and interior design. While modeling and presenting interactive 3D scenes in collaborative VR/AR environments, different 3D objects are added, modified and removed by different users, which leads to the evolution of the scenes over time. Representation of VR content covering temporal knowledge is essential to enable exploration of such time-dependent VR/AR content. However, the available approaches do not enable exploration of VR content with regards to its time-dependent components and properties, which limits their usage in web-based systems. The main contribution of this paper is 4--dimensional representation of VR content, which encompasses time being the fourth dimension. The representation is based on the semantic web standards and ontologies, which enable the use of domain knowledge for collaborative creation and exploration of content. This could improve the availability of VR/AR applications to domain specialists without expertise in 3D graphics and animation, thus improving the overall dissemination of VR/AR on the web. The representation has been implemented in a heterogeneous collaborative VR environment for urban design.},
booktitle = {Proceedings of the 23rd International ACM Conference on 3D Web Technology},
articleno = {11},
numpages = {10},
keywords = {3D content, collaborative design, ontologies, semantic web, web 3D/VR/AR},
location = {Pozna\'{n}, Poland},
series = {Web3D '18}
}

@inproceedings{10.1145/3498851.3498968,
author = {Wang, Mengzhen and Chen, Jianhui and Lin, Shaofu},
title = {Medication Recommendation Based on a Knowledge-enhanced Pre-training Model},
year = {2022},
isbn = {9781450391870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498851.3498968},
doi = {10.1145/3498851.3498968},
abstract = {More and more attention has been paid to electronic medical record (EMR)-based auxiliary diagnosis and treatment, in which medication recommendation is an important research direction. The existing medication recommendation models mainly depend on the data of patients, diagnosis and medications. However, the insufficient amount of clinical data with temporal dependencies becomes a major obstacle. This paper proposes a new knowledge-enhanced pre-training model for medication recommendation. On the one hand, the classification knowledge in diagnostic codes and drug codes is encoded by Graph Attention Network and fused into the clinical data for expanding the data content. On the other hand, a large number of single visit data of EMR are used to create the pre-trained visit model by a modified BERT for expanding the data scale. The experimental results on EMR data from more than 2,000 medical and health institutions in Hainan, China show that the fusion of classification knowledge and pre-training model can effectively improve the accuracy of medication recommendation.},
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},
pages = {290–294},
numpages = {5},
keywords = {Electronic medical record, Graph Attention Network, Medication recommendation, Pre-training model},
location = {Melbourne, VIC, Australia},
series = {WI-IAT '21}
}

@inproceedings{10.1145/3703323.3703698,
author = {Gowhar, Saliq and Kempaiah, Praveen and Kamath S, Sowmya and Sugumaran, Vijayan},
title = {Imbalanced Multi-Class Research Article Classification using Sentence Transformers and Machine Learning Algorithms},
year = {2025},
isbn = {9798400711244},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3703323.3703698},
doi = {10.1145/3703323.3703698},
abstract = {Categorizing scientific articles into specific research fields is a challenging problem, considering the volume and variety of published literature. However, existing classification systems often suffer from limitations regarding taxonomy or the models used for classification. This article explores approaches built on Sentence Transformer embeddings combined with Machine Learning algorithms to classify articles into 123 predefined classes, with the dataset being heavily imbalanced in nature. The effectiveness of Large Language Models (LLMs) for generating synthetic data is also experimented with, along with synonym augmentation and SMOTE. The best-performing model, the One vs Rest classifier trained on MP-Net sentence embeddings with SMOTE, achieved an accuracy of 77\%, and outperformed all the other models.},
booktitle = {Proceedings of the 8th International Conference on Data Science and Management of Data (12th ACM IKDD CODS and 30th COMAD)},
pages = {309–310},
numpages = {2},
keywords = {Document classification, Machine Learning, Sentence Transformers, Natural Language Processing},
location = {
},
series = {CODS-COMAD '24}
}

@inproceedings{10.1145/3227609.3227661,
author = {Shi, Ling and Roman, Dumitru},
title = {Ontologies for the Real Property Domain},
year = {2018},
isbn = {9781450354899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3227609.3227661},
doi = {10.1145/3227609.3227661},
abstract = {Real property, also known as real estate, realty or immovable property, is one of the most important assets for the world economy. Real property data is valuable input for decision makers in various domains. Real property data has temporal and spatial characteristics and is distributed across multiple systems. Integration of real property data from legal and business systems (possibly from different countries) with contextual data in related domains is a challenging task that requires cross-domain knowledge. Real property ontologies, capturing relevant domain knowledge in a structured way, are essential in the process of integrating real property data. This paper identifies key aspects of the real property domain from a data integration perspective, and surveys ontologies for real property and its related domains. It analyzes geospatial standards for representing geospatial concepts and attributes relevant to real properties, the Land Administration Domain Model and its implementations, and ontologies for real property transactions and for real property data integration. This survey aims to collect and compare existing real property ontologies and conceptual models, serving as a reference point for ontologies and conceptual models in the real property domain.},
booktitle = {Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics},
articleno = {14},
numpages = {8},
keywords = {LADM, Real property ontology, cadaster, geospatial data},
location = {Novi Sad, Serbia},
series = {WIMS '18}
}

@article{10.1145/3511215,
author = {Alshammari, Nasser O. and Alharbi, Fawaz D.},
title = {Combining a Novel Scoring Approach with Arabic Stemming Techniques for Arabic Chatbots Conversation Engine},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3511215},
doi = {10.1145/3511215},
abstract = {Arabic is recognized as one of the main languages around the world. Many attempts and efforts have been done to provide computing solutions to support the language. Developing Arabic chatbots is still an evolving research field and requires extra efforts due to the nature of the language. One of the common tasks of any natural language processing application is the stemming step. It is important for developing chatbots, since it helps with pre-processing the input data and it can be involved with different phases of the chatbot development process. The aim of this article is to combine a scoring approach with Arabic stemming techniques for developing an Arabic chatbot conversation engine. Two experiments are conducted to evaluate the proposed solution. The first experiment is to select which stemmer is more accurate when applying our solution, since our algorithm can support various stemmers. The second experiment was conducted to evaluate our proposed approach against various machine learning models. The results show that the ISRIS stemming algorithm is the best fit for our solution with accuracy 78.06\%. The results also indicate that our novel solution achieved an F1 score of 65.5\%, while the other machine learning models achieved slightly lower scores. Our study presents a novel technique by combining scoring mechanisms with stemming processes to produce the best answer for every query sent by chatbots users compared to other approaches. This can be helpful for developing Arabic chatbot and can support many domains such as education, business, and health. This technique is among the first techniques that developed purposefully to serve the development of Arabic chatbots conversation engine.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jan,
articleno = {84},
numpages = {21},
keywords = {Arabic language, stemming, machine learning, chatbot, Natural language processing}
}

@inproceedings{10.1145/3178461.3178483,
author = {Laadidi, Yassine and Bahaj, Mohamed},
title = {Simplification of OWL Ontology Sources for Data Warehousing},
year = {2018},
isbn = {9781450354387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178461.3178483},
doi = {10.1145/3178461.3178483},
abstract = {Nowadays, with the emergence of new web technologies, no one could deny the necessity of including such external data sources in the analysis process in order to provide the necessary knowledge for companies to improve their services and increase their profits. However, processing data in an open environment such as the web has become too difficult due to the diversity of distributed data sources and incapability of machines to 'understand' the real semantic of web resources. The Semantic Web (SW) provides the semantic annotations to describe and link scattered information over the web and facilitate inference mechanisms using ontologies. Web Ontology Language (OWL) is the W3C recommendation. A Data warehouse (DW) is used in decision making processes to store multidimensional (MD) information from heterogeneous data sources using ETL (Extract, Transform and Load) techniques. In this paper, we introduce firstly a simplification method of OWL inputs and then we define the related MD schema. Transformation rules are applied for defining multidimensional concepts over the OWL graph.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
pages = {77–81},
numpages = {5},
keywords = {OWL ontology, data warehousing, semantic web},
location = {Casablanca, Morocco},
series = {ICSIM '18}
}

@inproceedings{10.1145/3428502.3428619,
author = {Kalogirou, Victoria and van Dooren, Sander and Dimopoulos, Ilias and Charalabidis, Yannis and De-Baets, Jean-Paul and Lobo, Georges},
title = {Linked government data hub, an ontology agnostic data harvester and API},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428619},
doi = {10.1145/3428502.3428619},
abstract = {Openness and Transparency are important principles for citizens and therefore for eGovernment. Most government portals are designed in a way that finding relevant and up-to-date information requires time and effort. Data and information are scattered in different platforms in a non-collaborative way, leading to a labyrinth of redirections. Sharing of data and information is hindered by the lack of interoperability between the multiple vocabularies and formats (mostly proprietary). This paper attempts to bridge those systems by proposing at a national level a data hub model that aggregates and categorizes data from various services. Linked data and standardized vocabularies are used to gather information from multiple public sectors and services. The case study reuses some basic components of Joinup collaborative platform to provide aggregated up-to-date information on the portal of the Greek Public Administration Di@vgeia. The model has the potential of a wider application in different business domains and multiple Government levels.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {779–782},
numpages = {4},
keywords = {public services, eGovernment (eGov), Linked Data (LD), Interoperability (IoP), Data Hub, Application Program Interface (API)},
location = {Athens, Greece},
series = {ICEGOV '20}
}

@inproceedings{10.1145/3373017.3373038,
author = {Wibowo, Adi and Davis, Joseph},
title = {Requirements Traceability Ontology to Support Requirements Management},
year = {2020},
isbn = {9781450376976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373017.3373038},
doi = {10.1145/3373017.3373038},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {21},
numpages = {9},
keywords = {Requirement traceability, requirements management, traceability ontology},
location = {Melbourne, VIC, Australia},
series = {ACSW '20}
}

@inproceedings{10.1145/3478905.3478908,
author = {Li, Haixia and Yan, Li},
title = {A Temporal RDF Model for Multi-grained Time Information Modeling},
year = {2021},
isbn = {9781450390248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478905.3478908},
doi = {10.1145/3478905.3478908},
abstract = {With the rapid increase of temporal data, how to represent and manage temporal data has become a research issue worth digging in. To better represent temporal data, there have been many works on adding the dimension to RDF or other data representations such as relational databases. However, few works pay attention to the problem of updating time information in the form of triple elements in RDF. Note that this not only makes it easy to express that the relationship between entities is effective over a period of time, but also makes it easy to express that the entities themselves are effective in the time. A model of temporal data representation based on RDF is proposed in this paper which not only considering the validity of triples, but also considering the temporal validity of the entities themselves within the triples.},
booktitle = {2021 4th International Conference on Data Science and Information Technology},
pages = {9–14},
numpages = {6},
keywords = {RDF, Temporal data, modeling},
location = {Shanghai, China},
series = {DSIT 2021}
}

@article{10.1145/3603499,
author = {Sangsavate, Suntarin and Sinthupinyo, Sukree and Chandrachai, Achara},
title = {Experiments of Supervised Learning and Semi-Supervised Learning in Thai Financial News Sentiment: A Comparative Study},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3603499},
doi = {10.1145/3603499},
abstract = {Sentiment classification is an instrument of natural language processing tasks in text analysis to measure customer feedback from given documents such as product reviews, news, and texts. This research aims to experiment with Thai financial news sentiment classification and evaluate sentiment classification performance. In this research, we show financial news sentiment classification experimental results when comparing supervised and semi-supervised methods. In the research methodology, we use PyThaiNLP to tokenize and remove stopwords and split datasets into 85\% of the training set and 15\% of the testing set. Next, we classify sentiment using machine learning and deep learning approaches with feature extraction such as bag-of-words, term frequency–inverse document frequency, and word embedding (Word2Vec and Bidirectional Encoder Representations from Transformers (BERT)) in given texts. The results show that support vector machine with the BERT model yields the best performance at 83.38\%; in contrast, the random forest classifier with bag-of-words yields the worst performance at 54.10\% in the machine learning approach. Another experiment reveals that long short-term memory with the BERT model yields the best performance at 84.07\% in contrast to the convolutional neural network with bag-of-words, which yields the worst performance at 69.80\% in the deep learning approach. The results imply that support vector machine, convolutional neural network, and long short-term memory are suitable for classifying sentiment in complex structure language. From this study, we observe the importance of sentiment classification tools between supervised and semi-supervised learning, and we look forward to furthering this work.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {197},
numpages = {36},
keywords = {Natural language processing, semi-supervised learning, sentiment classification, supervised learning, Thai language}
}

@inproceedings{10.1145/3469213.3470316,
author = {Liu, Bin and Wu, Baojun and Huang, Xinxin},
title = {Information system development method for domain ontology reuse},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470316},
doi = {10.1145/3469213.3470316},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {113},
numpages = {5},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@article{10.1145/3313801,
author = {Gon\c{c}ales, Lucian Jos\'{e} and Farias, Kleinner and Oliveira, Toacy Cavalcante De and Scholl, Murilo},
title = {Comparison of Software Design Models: An Extended Systematic Mapping Study},
year = {2019},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3313801},
doi = {10.1145/3313801},
abstract = {Model comparison has been widely used to support many tasks in model-driven software development. For this reason, many techniques of comparing them have been proposed in the last few decades. However, academia and industry have overlooked a classification of currently available approaches to the comparison of design models. Hence, a thorough understanding of state-of-the-art techniques remains limited and inconclusive. This article, therefore, focuses on providing a classification and a thematic analysis of studies on the comparison of software design models. We carried out a systematic mapping study following well-established guidelines to answer nine research questions. In total, 56 primary studies (out of 4,132) were selected from 10 widely recognized electronic databases after a careful filtering process. The main results are that a majority of the primary studies (1) provide coarse-grained techniques of the comparison of general-purpose diagrams, (2) adopt graphs as principal data structure and compare software design models considering structural properties only, (3) pinpoint commonalities and differences between software design models rather than assess their similarity, and (4) propose new techniques while neglecting the production of empirical knowledge from experimental studies. Finally, this article highlights some challenges and directions that can be explored in upcoming studies.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {48},
numpages = {41},
keywords = {UML, model comparison, model similarity, software design models}
}

@inproceedings{10.1145/3395035.3425206,
author = {Putze, Felix and Burri, Merlin and Vortmann, Lisa-Marie and Schultz, Tanja},
title = {Model-based Prediction of Exogeneous and Endogeneous Attention Shifts During an Everyday Activity},
year = {2021},
isbn = {9781450380027},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395035.3425206},
doi = {10.1145/3395035.3425206},
abstract = {Human attention determines to a large degree how users interact with technical devices and how technical artifacts can support them optimally during their tasks. Attention shifts between different targets, triggered through changing requirements of an ongoing task or through salient distractions in the environment. Such shifts mark important transition points which an intelligent system needs to predict and attribute to an endogenous or exogenous cause for an appropriate reaction. In this paper, we describe a model which performs this task through a combination of bottom-up and topdown modeling components. We evaluate the model in a scenario with a dynamic task in a rich environment and show that the model is able to predict attention future switches with a robust classification performance.},
booktitle = {Companion Publication of the 2020 International Conference on Multimodal Interaction},
pages = {417–425},
numpages = {9},
keywords = {attention shifts, exogenous and endogenous attention, top-down and bottom-up modeling},
location = {Virtual Event, Netherlands},
series = {ICMI '20 Companion}
}

@inproceedings{10.1145/3640115.3640224,
author = {Wu, Bing and Song, Yuanbin and Cao, Jinhao},
title = {Automatic Generation of GIM Data Audit Rules Based on Sentence Embedding Vectors},
year = {2024},
isbn = {9798400708299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640115.3640224},
doi = {10.1145/3640115.3640224},
abstract = {The digital design results of power substations establish the foundation for their running and maintaining. Currently, the digital design of substations is delivered in the format of Grid Information Model (GIM), an alternative Building Information Modeling (BIM) format for describing power grid infrastructure in China. Since the correctness, compliance, and consistency of GIM data are necessary conditions for information sharing and business decision support, the GIM data must be audited before sharing among the stakeholders. The traditional manual review of GIM data is too inefficient and costly to execute, and thus the power grid industry seeks automatic review approaches. However, one challenge for automated auditing of GIM data is the lack of auditing rules. In order to establish such a rule base for GIM data auditing, this study first categorizes the audit rules, and proposes an XML encoding method for the audit rules. Meanwhile, the methodology of converting the rules described in natural language into XML is also rules proposed using the SBERT model. The application of the developed tool is demonstrated and verified through case studies.},
booktitle = {Proceedings of the 6th International Conference on Information Technologies and Electrical Engineering},
pages = {668–673},
numpages = {6},
keywords = {Controlled natural language, Grid information model, Model audit, Natural language processing, Rule representation, Sentence BERT},
location = {Changde, Hunan, China},
series = {ICITEE '23}
}

@inproceedings{10.1145/3472163.3472267,
author = {Holubova, Irena and Contos, Pavel and Svoboda, Martin},
title = {Multi-Model Data Modeling and Representation: State of the Art and Research Challenges},
year = {2021},
isbn = {9781450389914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472163.3472267},
doi = {10.1145/3472163.3472267},
abstract = {Following the current trend, most of the well-known database systems, being relational, NoSQL, or NewSQL, denote themselves as multi-model. This industry-driven approach, however, lacks plenty of important features of the traditional DBMSs. The primary problem is a design of an optimal multi-model schema and its sufficiently general and efficient representation. In this paper, we provide an overview and discussion of the promising approaches that could potentially be capable of solving these issues, along with a summary of the remaining open problems.},
booktitle = {Proceedings of the 25th International Database Engineering \&amp; Applications Symposium},
pages = {242–251},
numpages = {10},
keywords = {Category theory, Conceptual modeling, Inter-model relationships, Logical models, Multi-model data},
location = {Montreal, QC, Canada},
series = {IDEAS '21}
}

@inproceedings{10.1145/3184558.3191598,
author = {Ben Ellefi, Mohamed and Papini, Odile and Merad, Djamal and Boi, Jean-Marc and Royer, Jean-Philip and Pasquet, J\'{e}r\^{o}me and Sourisseau, Jean-Christophe and Castro, Filipe and Nawaf, Mohammad Motasem and Drap, Pierre},
title = {Cultural Heritage Resources Profiling: Ontology-based Approach},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191598},
doi = {10.1145/3184558.3191598},
abstract = {Cultural heritage (CH) resources are very heterogeneous since the information was collected from vast diversity of cultural sites and digitally recorded in different formats. With the progress of 3D technologies, photogrammetry techniques become the adopted solution for representing CH artifacts by turning photos from small finds, to entire landscapes, into accurate 3D models. To meet knowledge representation with cultural heritage photogrammetry, this paper proposes an ontology-profiling method for modeling a real case of archaeological amphorae. The ontological profile consists of all needed information to represent a CH resource including typology attributes, geo-spatial information and photogrammetry process. An example illustrating the applicability of this profiling method to the problem of CH resources conceptualization is presented. We also outline our perspectives for using ontologies in data-driven science, in particular on modeling a complete pipeline that manages both the photogrammetric process and the archaeological knowledge.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1489–1496},
numpages = {8},
keywords = {archaeology, cultural heritage, ontology, photogrammetry, profiles},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3563359.3596661,
author = {Tsitseklis, Konstantinos and Stavropoulou, Georgia and Zafeiropoulos, Anastasios and Thanou, Athina and Papavassiliou, Symeon},
title = {RECBOT: Virtual Museum navigation through a Chatbot assistant and personalized Recommendations},
year = {2023},
isbn = {9781450398916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563359.3596661},
doi = {10.1145/3563359.3596661},
abstract = {The trend for digitalization of museums has been on the rise in recent years, as museums seek to make their collections and exhibitions more accessible to a wider audience. This has involved the use of technologies such as virtual and augmented reality, online exhibits, and digital archives. These digital initiatives have allowed museums to reach new audiences and provide immersive experiences that enhance visitors’ engagement with the exhibits. Following this trend, in the current work, we propose a conversational agent that assists remote visitors in accessing a museum’s collection. The proposed architecture includes a chatbot for user interaction that employs Natural Language Processing techniques for understanding the user’s input. To increase visitor engagement, a hybrid recommender system is developed that combines content-based and collaborative-filtering components. The available data is modeled in the form of a Knowledge Graph, which allows for useful insights to be extracted from it.},
booktitle = {Adjunct Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
pages = {388–396},
numpages = {9},
keywords = {Natural Language Processing, chatbot, conversational agent, online museum, recommender system, virtual tour},
location = {Limassol, Cyprus},
series = {UMAP '23 Adjunct}
}

@inproceedings{10.1145/3474124.3474169,
author = {Agarwal, Neha and Sikka, Geeta and Awasthi, Lalit Kumar},
title = {Comparative Study of Topic Modeling and Word Embedding Approaches for Web Service Clustering},
year = {2021},
isbn = {9781450389204},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474124.3474169},
doi = {10.1145/3474124.3474169},
abstract = {Vector space representation of web services plays a prominent role in enhancing the performance of different web service-based processes like clustering, recommendation, ranking, discovery, etc. Generally, Term Frequency - Inverse Document Frequency (TF-IDF) and topic modeling methods are widely used for service representation. In recent years, word embedding techniques have attracted researchers a lot because they can map services or documents based on semantic similarity. This paper provides a comparative analysis of two topic modeling techniques, i.e., Latent Dirichlet Allocation (LDA) and Gibbs Sampling algorithm for Dirichlet Multinomial Mixture (GSDMM) \&amp; two word embedding techniques, i.e., word2vec and fastText. These topic modeling and word embedding techniques are applied to a dataset of web service documents for vector space representation. K-Means clustering is used to analyze the performance, and results are evaluated based on standard evaluation criteria. Results demonstrate that word2vec model outperforms other techniques and provides a satisfactory improvement on clustering.},
booktitle = {Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing},
pages = {309–313},
numpages = {5},
keywords = {K-Means Clustering, Topic Models, Web Services, Word Embedding},
location = {Noida, India},
series = {IC3-2021}
}

@inproceedings{10.1145/3290420.3290459,
author = {Li, Fei and Liao, Lejian and Li, Chunyi and He, Sixing},
title = {Efficient and density adaptive edge weight model for measuring semantic similarity},
year = {2018},
isbn = {9781450365345},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290420.3290459},
doi = {10.1145/3290420.3290459},
abstract = {The measurement of semantic similarity between concepts is an important research topic in natural language processing. However, previous efforts suffered from the mismatch of the accuracy and efficiency. In this paper, we propose an edge weight model for improving the accuracy of edge-based measures that have an inherent high efficiency. It combines the edge counting model with the information theory and deduces a function of edge weight based on the number of direct hyponyms of the subsumer in the edge. This model doesn't require any additional parameter and can adapt the effect of different densities to edges. Extensive experiments on four test datasets for WordNet and SNOMED-CT demonstrate that the proposed edge weight model can significantly improve the accuracy of various edge-based similarity measures and has a wide coverage over different ontologies. Compared with IC-based measures, our model has a remarkable advantage in efficiency and is comparable to it in accuracy.},
booktitle = {Proceedings of the 4th International Conference on Communication and Information Processing},
pages = {127–134},
numpages = {8},
keywords = {SNOMED-CT, edge-weight, information theory, semantic similarity, wordnet},
location = {Qingdao, China},
series = {ICCIP '18}
}

@inproceedings{10.1145/3625007.3627505,
author = {Ranade, Priyanka and Joshi, Anupam},
title = {FABULA: Intelligence Report Generation Using Retrieval-Augmented Narrative Construction},
year = {2024},
isbn = {9798400704093},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625007.3627505},
doi = {10.1145/3625007.3627505},
abstract = {Narrative construction is the process of representing disparate event information into a logical plot structure that models an end to end story. Intelligence analysis is an example of a domain that can benefit tremendously from narrative construction techniques, particularly in aiding analysts during the largely manual and costly process of synthesizing event information into comprehensive intelligence reports. Manual intelligence report generation is often prone to challenges such as integrating dynamic event information, writing fine-grained queries, and closing information gaps. This motivates the development of a system that retrieves and represents critical aspects of events in a form that aids in automatic generation of intelligence reports.We introduce a Retrieval Augmented Generation (RAG) approach to augment prompting of an autoregressive decoder by retrieving structured information asserted in a knowledge graph to generate targeted information based on a narrative plot model. We apply our approach to the problem of neural intelligence report generation and introduce FABULA, framework to augment intelligence analysis workflows using RAG. An analyst can use FABULA to query an Event Plot Graph (EPG) to retrieve relevant event plot points, which can be used to augment prompting of a Large Language Model (LLM) during intelligence report generation. Our evaluation studies show that the plot points included in the generated intelligence reports have high semantic relevance, high coherency, and low data redundancy.},
booktitle = {Proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {603–610},
numpages = {8},
keywords = {retrieval augmented generation, large language models, knowledge graphs, narratives},
location = {Kusadasi, Turkiye},
series = {ASONAM '23}
}

@inproceedings{10.1145/3461702.3462557,
author = {Aka, Osman and Burke, Ken and Bauerle, Alex and Greer, Christina and Mitchell, Margaret},
title = {Measuring Model Biases in the Absence of Ground Truth},
year = {2021},
isbn = {9781450384735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461702.3462557},
doi = {10.1145/3461702.3462557},
abstract = {The measurement of bias in machine learning often focuses on model performance across identity subgroups (such as man and woman) with respect to groundtruth labels. However, these methods do not directly measure the associations that a model may have learned, for example between labels and identity subgroups. Further, measuring a model's bias requires a fully annotated evaluation dataset which may not be easily available in practice.We present an elegant mathematical solution that tackles both issues simultaneously, using image classification as a working example. By treating a classification model's predictions for a given image as a set of labels analogous to a "bag of words", we rank the biases that a model has learned with respect to different identity labels. We use man, woman as a concrete example of an identity label set (although this set need not be binary), and present rankings for the labels that are most biased towards one identity or the other. We demonstrate how the statistical properties of different association metrics can lead to different rankings of the most "gender biased" labels, and conclude that normalized pointwise mutual information (nPMI) is most useful in practice. Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.},
booktitle = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {327–335},
numpages = {9},
keywords = {bias, datasets, fairness, image tagging, information extraction, model analysis, stereotypes},
location = {Virtual Event, USA},
series = {AIES '21}
}

@inproceedings{10.1145/3229345.3229405,
author = {Barboza, Tatiana and Santoro, Fl\'{a}via Maria and Bai\~{a}o, Fernanda},
title = {Automatic Validation of Knowledge-intensive Process Models through Alloy},
year = {2018},
isbn = {9781450365598},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229345.3229405},
doi = {10.1145/3229345.3229405},
abstract = {Knowledge-intensive Processes (KiP) are poorly structured, dynamic and highly complex. The Knowledge Intensive Process Ontology (KiPO) constitutes a semantically rich conceptualization (encompassing a set of logical rules) about the domain of KiP that may serve as a basis to understand, identify and manage KiP effectively. However, applying KiPO in real scenarios requires its instantiation, validation and simulation in an application level, which are complex tasks for users that typically are not experts in non-trivial issues on conceptual modeling. This work proposes a rule-based strategy to validate or simulate KiP models. The proposed strategy transforms the KiPO rules into the existing specifications in the Alloy logic-based language, using the Alloy Analyzer model analyzer. The main contribution of this research is to show the applicability of the Alloy tool to this context in a case study with four different scenarios. A process modeler can directly benefit from these results.},
booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
articleno = {57},
numpages = {8},
keywords = {BPM, Conceptual Modeling, Knowledge-intensive Process, Model validation},
location = {Caxias do Sul, Brazil},
series = {SBSI '18}
}

@article{10.1145/3439800,
author = {Bi, Mingwen and Zhang, Qingchuan and Zuo, Min and Xu, Zelong and Jin, Qingyu},
title = {Bi-directional Long Short-Term Memory Model with Semantic Positional Attention for the Question Answering System},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3439800},
doi = {10.1145/3439800},
abstract = {The intelligent question answering system aims to provide quick and concise feedback on the questions of users. Although the performance of phrase-level and numerous attention models have been improved, the sentence components and position information are not emphasized enough. This article combines Ci-Lin and word2vec to divide all of the words in the question-answer pairs into groups according to the semantics and select one kernel word in each group. The remaining words are common words and realize the semantic mapping mechanism between kernel words and common words. With this Chinese semantic mapping mechanism, the common words in all questions and answers are replaced by the semantic kernel words to realize the normalization of the semantic representation. Meanwhile, based on the bi-directional LSTM model, this article introduces a method of the combination of semantic role labeling and positional context, dividing the sentence into multiple semantic segments according to semantic logic. The weight is given to the neighboring words in the same semantic segment and propose semantic role labeling position attention based on the bi-directional LSTM model (BLSTM-SRLP). The good performance of the BLSTM-SRLP model has been demonstrated in comparative experiments on the food safety field dataset (FS-QA).},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {77},
numpages = {13},
keywords = {Question answering, BLSTM model, semantic positional-based attention, Chinese semantic mapping mechanism}
}

@article{10.1145/3365000,
author = {Zhu, Meng and Wang, Alf Inge},
title = {Model-driven Game Development: A Literature Review},
year = {2019},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3365000},
doi = {10.1145/3365000},
abstract = {Model-driven game development (MDGD) introduces model-driven methodology to the computer game domain, shifting the focus of game development from coding to modeling to make game development faster and easier. The research on MDGD is concerned with both the general model-driven software development methodology and the particular characteristics of game development. People in the MDGD community have proposed several approaches in the past decades, addressing both the technology and the development process in the context of MDGD. This article presents the state-of-art of MDGD research based on a literature review of 26 approaches in the field. The review is structured around five perspectives: target game domains, domain frameworks, modelling languages, tooling, and evaluation methods. The article also includes reflections and a discussion of the challenges within MDGD.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {123},
numpages = {32},
keywords = {Model-driven software development, game development}
}

@article{10.1145/3514254,
author = {Williams, Rua M. and Alikhademi, Kiana and Munyaka, Imani N. S. and Gilbert, Juan E.},
title = {MetaCogs: Mitigating Executive Dysfunction via Agent-based Modeling for Metacognitive Strategy Development},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1936-7228},
url = {https://doi.org/10.1145/3514254},
doi = {10.1145/3514254},
abstract = {Executive functions (EF) are a collection of cognitive domains governing task initiation, motor planning, attention, and goal-oriented action. Difficulties with EF have marked impacts on adaptive living skills, learning outcomes, and quality of life for people with cognitive and psychosocial disabilities, as well as the broader population. While there is considerable research interest in EF training intervention for disabled populations, very few studies explore metacognitive intervention for people with cognitive disabilities. Metacognition comprises conscious beliefs and strategies around task management and goal setting. Metacognitive awareness has been shown to mediate the effects of executive function on self-regulated learning. Metacognitive interventions have also shown promise in general education, military training, and medical practice. We present a virtual reality experience deploying agent-based modeling to support explicit metacognitive strategy instruction for undergraduate students of all neurotypes. Our results support that explicit instructional material explaining executive function and metacognition in relation to problem-solving experiences influenced participant self-concept and awareness of personal traits and cognitive processes.},
journal = {ACM Trans. Access. Comput.},
month = jul,
articleno = {24},
numpages = {32},
keywords = {Metacognition, executive function, autism, ADHD, virtual reality}
}

@inproceedings{10.1109/MODELS-C.2019.00110,
author = {Ortiz, Victor-Alejandro and Esta\~{n}ol, Montserrat and Marinescu, Maria-Cristina and Sancho, Maria-Ribera and Teniente, Ernest and Rueda, Carmen},
title = {A semantic model to fight social exclusion},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00110},
doi = {10.1109/MODELS-C.2019.00110},
abstract = {This work presents a semantic model meant to help with the identification and prediction of individuals at risk of social exclusion. The model is based on the self-sufficiency matrix, a tool that evaluates a person's self-sufficiency in different areas, and that is used by Barcelona's City Council. Existing data sources can then be mapped to this model, in order to analyze, query, and visualize the data.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {730–731},
numpages = {2},
keywords = {modeling, self-suffiency matrix, semantic technologies, social exclusion},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1145/3282373.3282386,
author = {Bo\v{z}i\'{c}, Bojan and R\'{\i}os, Andr\'{e} and Delany, Sarah Jane},
title = {Validation of Tagging Suggestion Models for a Hotel Ticketing Corpus},
year = {2018},
isbn = {9781450364799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282373.3282386},
doi = {10.1145/3282373.3282386},
abstract = {This paper investigates methods for the prediction of tags on a textual corpus that describes hotel staff inputs in a ticketing system. The aim is to improve the tagging process and find the most suitable method for suggesting tags for a new text entry. The paper consists of two parts: (i) exploration of existing sample data, which includes statistical analysis and visualisation of the data to provide an overview, and (ii) evaluation of tag prediction approaches. We have included different approaches from different research fields in order to cover a broad spectrum of possible solutions. As a result, we have tested a machine learning model for multi-label classification (using gradient boosting), a statistical approach (using frequency heuristics), and two simple similarity-based classification approaches (Nearest Centroid and k-Nearest Neighbours). The experiment which compares the approaches uses recall to measure the quality of results. Finally, we provide a recommendation of the modelling approach which produces the best accuracy in terms of tag prediction on the sample data.},
booktitle = {Proceedings of the 20th International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {15–23},
numpages = {9},
keywords = {Multi-label Classification, Natural Language Processing, Tag Prediction, k-Nearest Neighbour},
location = {Yogyakarta, Indonesia},
series = {iiWAS2018}
}

@inproceedings{10.1145/3652620.3687795,
author = {Sch\"{o}berl, Stefan and Banse, Christian and Geist, Verena and Kunz, Immanuel and Pinzger, Martin},
title = {CertGraph: Towards a Comprehensive Knowledge Graph for Cloud Security Certifications},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687795},
doi = {10.1145/3652620.3687795},
abstract = {This paper introduces CertGraph, a knowledge graph-based approach designed to streamline security certification which integrates evidence from multiple sources. Unlike existing approaches, we consider the complete stack from software to policies, and enable the fusion of evidence from different views and sources. Its extensible ontology is designed to accommodate multiple domains, including cloud security, AI models, and source code. By providing an automated and systematic approach to build an ontology, CertGraph aims to facilitate more effective security certification and compliance verification.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {76–77},
numpages = {2},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3571662.3571664,
author = {Zhang, Zhuoran and Xu, Shibiao and Guo, Li and Lian, Wenke},
title = {Multi-modal Variational Auto-Encoder Model for Micro-video Popularity Prediction},
year = {2023},
isbn = {9781450397100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571662.3571664},
doi = {10.1145/3571662.3571664},
abstract = {Popularity prediction of micro videos on multimedia is a hotly studied topic due to the widespread use of video upload sharing services. It’s also a challenging task because popular pattern is affected by multiple factors and is hard to be modeled. The goal of this paper is to use feature extraction techniques and variation auto-encoder (VAE) framework to predict the popularity of online micro-videos. First, we identify four declarable modalities that are important for adaptability and expansibility. Then, we design a multi-modal based VAE regression model (MASSL) to exploit the domestic and foreign information extracted from heterogeneous features. The model can be applied to large-scale multimedia platforms, even the modality absence scenarios. With extensive experiments conducted on the dataset, which was originally generated from the most popular video-sharing website in China, the result demonstrates the effectiveness of our proposed model by comparing with baseline approaches.},
booktitle = {Proceedings of the 8th International Conference on Communication and Information Processing},
pages = {9–16},
numpages = {8},
keywords = {deep learning, popularity prediction, social media},
location = {Beijing, China},
series = {ICCIP '22}
}

@inproceedings{10.1145/3708557.3716350,
author = {Yordanova, Kristina Y. and Stoev, Teodor and Rebl, Henrike and Hahn, Olga and Peters, Kirsten},
title = {Text2RLab: No-Code Methodology for Robotic Programming and Interaction in Laboratory Tasks},
year = {2025},
isbn = {9798400714092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708557.3716350},
doi = {10.1145/3708557.3716350},
abstract = {Using robotic systems in laboratory settings increases the quality and reproducibility of laboratory experiments. One challenge laboratory personal faces is the need of programming knowledge to set up the robotic system. To address this problem, in this work we propose a no-code methodology for robotic programming in laboratory tasks. The methodology takes as an input instructions in natural language and generates machine readable models of execution steps. The final result is a model with a probabilistic structure that allows connecting sensor observations to the model’s states, thus providing a mechanism for robotic action execution and state estimation. The proposed methodology has the potential to increase the applicability of robotic systems and thus to improve the quality and reproducibility of laboratory experiments.},
booktitle = {Companion Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {96–100},
numpages = {5},
keywords = {robotic programming, model learning, plan generation, probabilistic modelling},
location = {
},
series = {IUI '25 Companion}
}

@article{10.1109/TCBB.2022.3197320,
author = {Ye, Cheng and Swiers, Rowan and Bonner, Stephen and Barrett, Ian},
title = {A Knowledge Graph-Enhanced Tensor Factorisation Model for Discovering Drug Targets},
year = {2022},
issue_date = {Nov.-Dec. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3197320},
doi = {10.1109/TCBB.2022.3197320},
abstract = {The drug discovery and development process is a long and expensive one, costing over 1 billion USD on average per drug and taking 10-15 years. To reduce the high levels of attrition throughout the process, there has been a growing interest in applying machine learning methodologies to various stages of drug discovery and development in the recent decade, especially at the earliest stage – identification of druggable disease genes. In this paper, we have developed a new tensor factorisation model to predict potential drug targets (genes or proteins) for treating diseases. We created a three-dimensional data tensor consisting of 1,048 gene targets, 860 diseases and 230,011 evidence attributes and clinical outcomes connecting them, using data extracted from the Open Targets and PharmaProjects databases. We enriched the data with gene target representations learned from a drug discovery-oriented knowledge graph and applied our proposed method to predict the clinical outcomes for unseen gene target and disease pairs. We designed three evaluation strategies to measure the prediction performance and benchmarked several commonly used machine learning classifiers together with Bayesian matrix and tensor factorisation methods. The result shows that incorporating knowledge graph embeddings significantly improves the prediction accuracy and that training tensor factorisation alongside a dense neural network outperforms all other baselines. In summary, our framework combines two actively studied machine learning approaches to disease target identification, namely tensor factorisation and knowledge graph representation learning, which could be a promising avenue for further exploration in data-driven drug discovery.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = aug,
pages = {3070–3080},
numpages = {11}
}

@inproceedings{10.1145/3167132.3167163,
author = {de Kok, Sophie and Punt, Linda and van den Puttelaar, Rosita and Ranta, Karoliina and Schouten, Kim and Frasincar, Flavius},
title = {Review-level aspect-based sentiment analysis using an ontology},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167163},
doi = {10.1145/3167132.3167163},
abstract = {The rapid growth of the World Wide Web has led to an explosion of information that is available on this platform. This has resulted in an increased interest in sentiment analysis, where the goal is to determine the opinion regarding a topic. Aspect-based sentiment analysis aims to capture the sentiment within a segment of text for mentioned aspects, rather than for the text as a whole. The task we consider is aspect-based sentiment analysis at the review-level for restaurant reviews. We focus on ontology-enhanced methods that complement a standard machine learning algorithm. For this task we use two different algorithms, a review-based and a sentence aggregation algorithm. By using an ontology as a knowledge base, the classification performance of our models improves significantly. Furthermore, the review-based algorithm gives more accurate predictions than the sentence aggregation algorithm.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {315–322},
numpages = {8},
keywords = {SVM, aspect-based sentiment analysis, domain ontology, reviews},
location = {Pau, France},
series = {SAC '18}
}

@inbook{10.1145/3382097.3382100,
title = {Semantic modeling},
year = {2020},
isbn = {9781450376174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382097.3382100},
abstract = {Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.},
booktitle = {Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL}
}

@inproceedings{10.1109/ICSE-SEET52601.2021.00013,
author = {Daun, Marian and Brings, Jennifer and Goger, Marcel and Koch, Walter and Weyer, Thorsten},
title = {Teaching model-based requirements engineering to industry professionals: an experience report},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00013},
doi = {10.1109/ICSE-SEET52601.2021.00013},
abstract = {The use of conceptual models to foster requirements engineering has been proposed and evaluated as beneficial for several decades. For instance, goal-oriented requirements engineering or the specification of scenarios are commonly done using conceptual models. Bringing such model-based requirements engineering approaches into industrial practice typically requires industrial training. In this paper, we report lessons learned from a training program for teaching industry professionals modelbased requirements engineering. Particularly, we as educators and learners report experiences from designing the training program, conducting the actual training, and applying the instructed material in our day-to-day work. From these findings we provide guidelines for educators designing requirements engineering courses for industry professionals.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {40–49},
numpages = {10},
keywords = {conceptual modeling, industrial training, requirements engineering},
location = {Virtual Event, Spain},
series = {ICSE-JSEET '21}
}

@book{10.1145/3462257,
author = {Jalali, Laleh and Jain, Ramesh},
title = {Event Mining for Explanatory Modeling},
year = {2021},
isbn = {9781450384827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {35},
abstract = {This book introduces the concept of Event Mining for building explanatory models from analyses of correlated data. Such a model may be used as the basis for predictions and corrective actions. The idea is to create, via an iterative process, a model that explains causal relationships in the form of structural and temporal patterns in the data. The first phase is the data-driven process of hypothesis formation, requiring the analysis of large amounts of data to find strong candidate hypotheses. The second phase is hypothesis testing, wherein a domain expert’s knowledge and judgment is used to test and modify the candidate hypotheses.The book is intended as a primer on Event Mining for data-enthusiasts and information professionals interested in employing these event-based data analysis techniques in diverse applications. The reader is introduced to frameworks for temporal knowledge representation and reasoning, as well as temporal data mining and pattern discovery. Also discussed are the design principles of event mining systems. The approach is reified by the presentation of an event mining system called EventMiner, a computational framework for building explanatory models. The book contains case studies of using EventMiner in asthma risk management and an architecture for the objective self. The text can be used by researchers interested in harnessing the value of heterogeneous big data for designing explanatory event-based models in diverse application areas such as healthcare, biological data analytics, predictive maintenance of systems, computer networks, and business intelligence.}
}

@inproceedings{10.1145/3371158.3371176,
author = {Jayasimha, Aditya and Gangavarapu, Tushaar and Kamath, S. Sowmya and Krishnan, Gokul S.},
title = {Deep Neural Learning for Automated Diagnostic Code Group Prediction Using Unstructured Nursing Notes},
year = {2020},
isbn = {9781450377386},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371158.3371176},
doi = {10.1145/3371158.3371176},
abstract = {Disease prediction, a central problem in clinical care and management, has gained much significance over the last decade. Nursing notes documented by caregivers contain valuable information concerning a patient's state, which can aid in the development of intelligent clinical prediction systems. Moreover, due to the limited adaptation of structured electronic health records in developing countries, the need for disease prediction from such clinical text has garnered substantial interest from the research community. The availability of large, publicly available databases such as MIMIC-III, and advancements in machine and deep learning models with high predictive capabilities have further facilitated research in this direction. In this work, we model the latent knowledge embedded in the unstructured clinical nursing notes, to address the clinical task of disease prediction as a multi-label classification of ICD-9 code groups. We present EnTAGS, which facilitates aggregation of the data in the clinical nursing notes of a patient, by modeling them independent of one another. To handle the sparsity and high dimensionality of clinical nursing notes effectively, our proposed EnTAGS is built on the topics extracted using Non-negative matrix factorization. Furthermore, we explore the applicability of deep learning models for the clinical task of disease prediction, and assess the reliability of the proposed models using standard evaluation metrics. Our experimental evaluation revealed that the proposed approach consistently exceeded the state-of-the-art prediction model by 1.87\% in accuracy, 12.68\% in AUPRC, and 11.64\% in MCC score.},
booktitle = {Proceedings of the 7th ACM IKDD CoDS and 25th COMAD},
pages = {152–160},
numpages = {9},
keywords = {Clinical Decision Support Systems, Deep Learning, Disease Prediction, Healthcare Analytics, Multi-label Classification, Natural Language Processing},
location = {Hyderabad, India},
series = {CoDS COMAD 2020}
}

@article{10.1145/3674847,
author = {Howison, Mark and Ensor, William O. and Maharjan, Suraj and Parikh, Rahil and Sengamedu, Srinivasan H. and Daniels, Paul and Gaither, Amber and Yeats, Carrie and Reddy, Chandan K. and Hastings, Justine S.},
title = {Extracting Structured Labor Market Information from Job Postings with Generative AI},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3674847},
doi = {10.1145/3674847},
abstract = {Labor market information is an important input to labor, workforce, education, and macroeconomic policy. However, granular and real-time data on labor market trends are lacking; publicly available data from survey samples are released with significant lags and miss critical information such as skills and benefits. We use generative Artificial Intelligence to automatically extract structured labor market information from unstructured online job postings for the entire U.S. labor market. To demonstrate our methodology, we construct a sample of 6,800 job postings stratified by 68 major occupational groups, extract structured information on educational requirements, remote-work flexibility, full-time availability, and benefits, and show how these job characteristics vary across occupations. As a validation, we compare frequencies of educational requirements by occupation from our sample to survey data and find no statistically significant difference. Finally, we discuss the scalability to collections of millions of job postings. Our results establish the feasibility of measuring labor market trends at scale from online job postings thanks to advances in generative AI techniques. Improved access to such insights at scale and in real-time could transform the ability of policy leaders, including federal and state agencies and education providers, to make data-informed decisions that better support the American workforce.},
journal = {Digit. Gov.: Res. Pract.},
month = feb,
articleno = {9},
numpages = {12},
keywords = {Workforce, education, policy, large language models, Amazon Bedrock}
}

@inproceedings{10.1145/3412841.3441957,
author = {Meijer, Lisa and Frasincar, Flavius and Tru\c{s}c\u{a}, Maria Mihaela},
title = {Explaining a neural attention model for aspect-based sentiment classification using diagnostic classification},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3441957},
doi = {10.1145/3412841.3441957},
abstract = {Many high performance machine learning models for Aspect-Based Sentiment Classification (ABSC) produce black box models, and therefore barely explain how they classify a certain sentiment value towards an aspect. In this paper, we propose explanation models, that inspect the internal dynamics of a state-of-the-art neural attention model, the LCR-Rot-hop, by using a technique called Diagnostic Classification. Our diagnostic classifier is a simple neural network, which evaluates whether the internal layers of the LCR-Rot-hop model encode useful word information for classification, i.e., the part of speech, the sentiment value, the presence of aspect relation, and the aspect-related sentiment value of words. We conclude that the lower layers in the LCR-Rot-hop model encode the part of speech and the sentiment value, whereas the higher layers represent the presence of a relation with the aspect and the aspect-related sentiment value of words.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {821–827},
numpages = {7},
keywords = {aspect-based sentiment classification, diagnostic classification, neural rotatory attention model},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3543873.3587618,
author = {Zhang, Yinglun and Broyaka, Antonina and Kastens, Jude and Featherstone, Allen M. and Shimizu, Cogan and Hitzler, Pascal and Mcginty, Hande K\"{u}\c{c}\"{u}k},
title = {Sustainable Grain Transportation in Ukraine Amidst War Utilizing KNARM and KnowWhereGraph},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587618},
doi = {10.1145/3543873.3587618},
abstract = {In this work, we propose a sustainable path-finding application for grain transportation during the ongoing Russian military invasion in Ukraine. This application is to build a suite of algorithms to find possible optimal paths for transporting grain that remains in Ukraine. The application uses the KNowledge Acquisition and Representation Methodology(KNARM) and the KnowWhereGraph to achieve this goal. Currently, we are working towards creating an ontology that will allow for a more effective heuristic approach by incorporating the lessons learned from the KnowWhereGraph. The aim is to enhance the path-finding process and provide more accurate and efficient results. In the future, we will continue exploring and implementing new techniques that can further improve the sustainability of the path-finding applications with a knowledge graph backend for grain transportation through hazardous and adversarial environments. The code is available upon reviewer’s request. It can not be made public due to the sensitive nature of the data.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {742–745},
numpages = {4},
keywords = {global food systems, knowledge graphs, ontology engineering, path-finding},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@article{10.1145/3616017,
author = {Smith, Ronnie and Dragone, Mauro},
title = {Generalisable Dialogue-based Approach for Active Learning of Activities of Daily Living},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {3},
issn = {2160-6455},
url = {https://doi.org/10.1145/3616017},
doi = {10.1145/3616017},
abstract = {While Human Activity Recognition systems may benefit from Active Learning by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in activity recognition systems, which utilises semantic similarity measures and a dataset of natural language descriptions of common activities (which we make publicly available). Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply this approach to two active learning scenarios: (i) using an existing CASAS dataset, demonstrating long-term usage; and (ii) using an online activity recognition system, which tackles the issue of online segmentation and labelling. We demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) quantitatively, as an efficient way to seek users’ input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to compare our approach and an established method. Results show the potential of our approach as a hands-free interface for annotation of sensor data as part of an active learning system. We provide insights into the challenges of active learning for activity recognition under real-world conditions and identify potential ways to address them.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = sep,
articleno = {18},
numpages = {37},
keywords = {Human-in-the-Loop (HITL) annotation, Active Learning (AL), natural language, semantic similarity, Human Activity Recognition (HAR) labelling}
}

@article{10.14778/3705829.3705832,
author = {Tang, Xiu and Liu, Wenhao and Wu, Sai and Yao, Chang and Yuan, Gongsheng and Ying, Shanshan and Chen, Gang},
title = {QueryArtisan: Generating Data Manipulation Codes for Ad-hoc Analysis in Data Lakes},
year = {2024},
issue_date = {October 2024},
publisher = {VLDB Endowment},
volume = {18},
number = {2},
issn = {2150-8097},
url = {https://doi.org/10.14778/3705829.3705832},
doi = {10.14778/3705829.3705832},
abstract = {Query processing over data lakes is a challenging task, often requiring extensive data pre-processing activities such as data cleaning, transformation, and loading. However, the advent of Large Language Models (LLMs) has illuminated a new pathway to address these complexities by offering a unified approach to understanding the diverse datasets submerged in data lakes. In this paper, we introduce QueryArtisan, a novel LLM-powered analytic tool specifically designed for data lakes. QueryArtisan transcends traditional ETL (Extract, Transform, Load) processes by generating just-intime code for dataset-specific queries. It eliminates the need for an intermediary schema, enabling users to query the data lake directly using natural language. To achieve this, we have developed a suite of heterogeneous operators capable of processing data across various modalities. Additionally, QueryArtisan incorporates a cost model-based query optimization technique, significantly enhancing its code generation capabilities for efficient query resolution. Our extensive experimental evaluations, conducted with real-life datasets, demonstrate that QueryArtisan markedly outperforms existing solutions in terms of effectiveness, efficiency and usability.},
journal = {Proc. VLDB Endow.},
month = oct,
pages = {108–116},
numpages = {9}
}

@inbook{10.1145/3501714.3501757,
author = {Shpitser, Ilya and Richardson, Thomas S. and Robins, James M.},
title = {Multivariate Counterfactual Systems and Causal Graphical Models},
year = {2022},
isbn = {9781450395861},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
url = {https://doi.org/10.1145/3501714.3501757},
booktitle = {Probabilistic and Causal Inference: The Works of Judea Pearl},
pages = {813–852},
numpages = {40}
}

@inproceedings{10.1145/3388440.3414702,
author = {Gbenro, Sola and Hippe, Kyle and Cao, Renzhi},
title = {HMMeta: Protein Function Prediction using Hidden Markov Models},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3414702},
doi = {10.1145/3388440.3414702},
abstract = {As the body of genomic product data increases at a much faster rate than can be annotated, computational analysis of protein function has never been more important. In this research, we introduce a novel protein function prediction method HMMeta, which is based on the prominent natural language prediction technique Hidden Markov Models (HMM). With a new representation of protein sequence as a language, we trained a unique HMM for each Gene Ontology (GO) term taken from the UniProt database, which in total has 27,451 unique GO IDs leading to the creation of 27,451 Hidden Markov Models. We employed data augmentation to artificially inflate the number of protein sequences associated with GO terms that have a limited amount in the database, and this helped to balance the number of protein sequences associated with each GO term. Predictions are made by running the sequence against each model created. The models within eighty percent of the top scoring model, or 75 models with the highest scores, whichever is less, represent the functions that are most associated with the given sequence. We benchmarked our method in the latest Critical Assessment of protein Function Annotation (CAFA 4) experiment as CaoLab2, and we also evaluated HMMeta against several other protein function prediction methods against a subset of the UniProt database. HMMeta achieved favorable results as a sequence-based method, and outperforms a few notable methods in some categories through our evaluation, which shows great potential for automated protein function prediction. The tool is available at https://github.com/KPHippe/HMM-For-Protein-Prediction.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {104},
numpages = {6},
keywords = {Hidden Markov Model, Machine Learning, Protein Function Prediction},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3473141.3473236,
author = {Acuna, Gabriel Edrick and Alvarez, Luis Antonio and Miraflores, Jeffrey and Samonte, Mary Jane},
title = {Towards the Development of an Adaptive E-learning System with Chatbot Using Personalized E-learning Model},
year = {2021},
isbn = {9781450389723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473141.3473236},
doi = {10.1145/3473141.3473236},
abstract = {Although there is no distinctive header, this is the abstract. This submission template allows authors to submit their papers E-learning has become one of the most used electronic systems in the field of education. Although it is beneficial, there are still some lacking capabilities and considerations that can negatively affect the performance of the students. This leads to the innovation that makes e-learning systems adaptive to the users’ personality, knowledge, behavior, interest, or preferences, the system is called personalized e-learning system. This survey paper aims to provide the general parameters in creating a personalized e-learning system based on the 150 research papers collected, and a timespan of 2016 to 2020 as a condition. Through a series of literature reviews of research papers published in the last five years, also related to personalized e-learning systems, this paper presents the common components, tools and algorithms, and learning model that are generally used in developing a personalized e-learning system to help as reference in developing more effective personalized e-learning systems. Moreover, considering the findings of this study, this paper has proposed developing a hybrid e-learning system with a chatbot.},
booktitle = {Proceedings of the 7th International Conference on Frontiers of Educational Technologies},
pages = {120–125},
numpages = {6},
keywords = {Adaptive, Chatbot, Myer-Briggs Type Indicator Theory, Personalized e-Learning Model},
location = {Bangkok, Thailand},
series = {ICFET '21}
}

@inproceedings{10.1145/3357254.3357277,
author = {Huang, Jih-Jeng},
title = {Computing the semantic similarity between documents by the copula-based econometric models},
year = {2019},
isbn = {9781450372299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357254.3357277},
doi = {10.1145/3357254.3357277},
abstract = {Semantic similarity is important information with which decision-makers can cluster, classify, or compare documents in text mining. Statistical and topological methods are two major ways to determine semantic similarity. However, conventional methods ignore the time factor when calculating the similarity between documents. It should be highlighted that narrative emotions play a critical role in comparing documents. In this paper, copula-based econometric models, including ARMA and GARCH families, are used to calculate the narrative semantic similarity between documents.},
booktitle = {Proceedings of the 2nd International Conference on Artificial Intelligence and Pattern Recognition},
pages = {134–139},
numpages = {6},
keywords = {copula, econometric models, semantic similarity, text mining},
location = {Beijing, China},
series = {AIPR '19}
}

@article{10.1145/3638555,
author = {Lin, Tzu-Mi and Hung, Man-Chen and Lee, Lung-Hao},
title = {Leveraging Dual Gloss Encoders in Chinese Biomedical Entity Linking},
year = {2024},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3638555},
doi = {10.1145/3638555},
abstract = {Entity linking is the task of assigning a unique identity to named entities mentioned in a text, a sort of word sense disambiguation that focuses on automatically determining a pre-defined sense for a target entity to be disambiguated. This study proposes the DGE (Dual Gloss Encoders) model for Chinese entity linking in the biomedical domain. We separately model a dual encoder architecture, comprising a context-aware gloss encoder and a lexical gloss encoder, for contextualized embedding representations. DGE are then jointly optimized to assign the nearest gloss with the highest score for target entity disambiguation. The experimental datasets consist of a total of 10,218 sentences that were manually annotated with glosses defined in the BabelNet 5.0 across 40 distinct biomedical entities. Experimental results show that the DGE model achieved an F1-score of 97.81, outperforming other existing methods. A series of model analyses indicate that the proposed approach is effective for Chinese biomedical entity linking.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
articleno = {28},
numpages = {15},
keywords = {Word sense disambiguation, lexical semantics, language transformers, natural language understanding, biomedical informatics}
}

@inproceedings{10.1145/3277593.3277597,
author = {Sahlmann, Kristina and Schwotzer, Thomas},
title = {Ontology-based virtual IoT devices for edge computing},
year = {2018},
isbn = {9781450365642},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277593.3277597},
doi = {10.1145/3277593.3277597},
abstract = {An IoT network may consist of hundreds heterogeneous devices. Some of them may be constrained in terms of memory, power, processing and network capacity. Manual network and service management of IoT devices are challenging. We propose a usage of an ontology for the IoT device descriptions enabling automatic network management as well as service discovery and aggregation. Our IoT architecture approach ensures interoperability using existing standards, i.e. MQTT protocol and Semantic Web technologies. We herein introduce virtual IoT devices and their semantic framework deployed at the edge of network. As a result, virtual devices are enabled to aggregate capabilities of IoT devices, derive new services by inference, delegate requests/responses and generate events. Furthermore, they can collect and pre-process sensor data. These tasks on the edge computing overcome the shortcomings of the cloud usage regarding siloization, network bandwidth, latency and speed. We validate our proposition by implementing a virtual device on a Raspberry Pi.},
booktitle = {Proceedings of the 8th International Conference on the Internet of Things},
articleno = {15},
numpages = {7},
keywords = {M2M, MQTT, edge computing, internet of things, oneM2M ontology, semantic interoperability},
location = {Santa Barbara, California, USA},
series = {IOT '18}
}

@article{10.1145/3689040,
author = {Bomba, Federico and Men\'{e}ndez-Blanco, Mar\'{\i}a and Grigis, Paolo and Cremaschi, Michele and De Angeli, Antonella},
title = {The Choreographer-Performer Continuum: A Diffraction Tool to Illuminate Authorship in More Than Human Co-Performances},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {6},
issn = {1073-0516},
url = {https://doi.org/10.1145/3689040},
doi = {10.1145/3689040},
abstract = {The design of robust and trustworthy Generative AI (GenAI) requires a deep understanding of the agencies emerging from human interactions with them. To contribute to this goal, we retrospectively studied an art project involving a visual artist, a computer scientist, an artistic director, and a generative model (GPT-2). The model was fine-tuned with trip reports describing the experience of eating psychedelic mushrooms. Building on agential realism, we analysed the co-performance between the artist and the model as their agency moved along the choreographer-performer continuum. Results reveal ontological surprises, leading to the proposal of entangled authorship to de-individualise the production of knowledge from a More Than Human perspective. The paper illustrates how art can expose different forms of relationships, challenging the idea of GenAI as just a tool that simplifies or replaces human labour. We conclude by emphasising the transformational potential of GenAI for novel modes of engagement between humans and machines.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = dec,
articleno = {75},
numpages = {23},
keywords = {Agency, Agential Realism, Large Language Models, AI and Art, Creative AI, Hallucination}
}

@inproceedings{10.1145/3703187.3703192,
author = {Li, Chengxue and Chen, Xuyang and Ding, Min and Jin, Wei and Gao, Feng},
title = {Research on Chinese Knowledge Base and Knowledge Q&amp;A Technology for Power Grid Dispatching},
year = {2024},
isbn = {9798400707254},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3703187.3703192},
doi = {10.1145/3703187.3703192},
abstract = {To support online professional knowledge query for power grid dispatchers, this article proposes a method for constructing a knowledge base based on knowledge graph, which implements systematic organization and management of knowledge resources in the field of power-grid dispatching. Moreover, a questions and answers (Q&amp;A) service is design based on large language model and proposed knowledge base. Based on the constructed knowledge base and Q&amp;A service, auxiliary learning functions can be provided in the domain of power grid operation. This enables accurate acquisition of professional knowledge through Chinese natural language interaction, enhancing the effectiveness and flexibility of online training for power-grid dispatchers.},
booktitle = {Proceedings of the 2024 7th International Conference on Computer Information Science and Artificial Intelligence},
pages = {18–23},
numpages = {6},
keywords = {Graph database, Knowledge graph, Large language model, Question answering},
location = {
},
series = {CISAI '24}
}

@inproceedings{10.1145/3220228.3220246,
author = {Al-Fedaghi, Sabah and Alduwaisan, Yousef},
title = {Modeling of an enterprise and information system: process specification based on the flow of things},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220246},
doi = {10.1145/3220228.3220246},
abstract = {Current modeling of enterprise and information systems is based on diverse methods such as function-orientation, data-orientation, process-orientation object-orientation, and object/process-orientation. Other emerging modeling methods include the ontology capture method and Business Process Modeling Notation (BPMN). These approaches have been criticized for lack of execution environment into which events that cause change could be incorporated. Such a topic is important in the study of dynamic behavior of systems for both analysis and control. This paper further develops a new approach oriented toward things that flow. Specifically, the paper concentrates on the study of process specification in analysis and design of enterprise/system architectures in order to most effectively facilitate their control. Here we produce a single, integrated diagrammatic representation that uniformly incorporates structural and behavioral aspects into an underlying conceptual description. The viability of the model is demonstrated by applying it to a case study of services provided by an existing organizational unit.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {142–150},
numpages = {9},
keywords = {BPMN, UML, conceptual modeling, data flow models, process specification},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.1145/3308558.3313511,
author = {Vedula, Nikhita and Maneriker, Pranav and Parthasarathy, Srinivasan},
title = {BOLT-K: Bootstrapping Ontology Learning via Transfer of Knowledge},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313511},
doi = {10.1145/3308558.3313511},
abstract = {Dynamically extracting and representing continually evolving knowledge entities is an essential scaffold for grounded intelligence and decision making. Creating knowledge schemas for newly emerging, unfamiliar, domain-specific ideas or events poses the following challenges: (i) detecting relevant, often previously unknown concepts associated with the new domain; and (ii) learning ontological, semantically accurate relationships among the new concepts, despite having severely limited annotated data. To this end, we propose a novel LSTM-based framework with attentive pooling, BOLT-K, to learn an ontology for a target subject or domain. We bootstrap our ontology learning approach by adapting and transferring knowledge from an existing, functionally related source domain. We also augment the inadequate labeled data available for the target domain with various strategies to minimize human expertise during model development and training. BOLT-K first employs semantic and graphical features to recognize the entity or concept pairs likely to be related to each other, and filters out spurious concept combinations. It is then jointly trained on knowledge from the target and source domains to learn relationships among the target concepts. The target concepts and their corresponding relationships are subsequently used to construct an ontology. We extensively evaluate our framework on several, real-world bio-medical and commercial product domain ontologies. We obtain significant improvements of 5-25\% F1-score points over state-of-the-art baselines. We also examine the potential of BOLT-K in detecting the presence of novel kinds of relationships that were unseen during training.},
booktitle = {The World Wide Web Conference},
pages = {1897–1908},
numpages = {12},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3617023.3617059,
author = {Hoppe, Pedro Henrique Brunoro and Souza, V\'{\i}tor Est\^{e}v\~{a}o Silva},
title = {Support for Single Page Application Frameworks on FrameWeb},
year = {2023},
isbn = {9798400709081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617023.3617059},
doi = {10.1145/3617023.3617059},
abstract = {In the field of Web Engineering, many methods have been proposed to guide developers in designing and coding Web applications. The FrameWeb method is a model-driven approach that targets the development of systems that use certain kinds of frameworks in their architecture, proposing the use of models that incorporate concepts from these frameworks during design. Currently, the FrameWeb method does not consider SPA (Single Page Application) frameworks and, in recent years, they have gained a lot of popularity among developers. In this work, we propose to add support for SPA frameworks to FrameWeb. With our research, we have managed to update the FrameWeb meta-model so that its modeling language now supports SPA frameworks and their constructs. FrameWeb tools (graphical editor and code generator) also evolved to support the new elements. Experiments of modeling existing SPAs with this new version of FrameWeb, generating code from the models and comparing with the original, showed that, in average, around 69\% of the HTML tags could be generated from the models. The support for SPA frameworks in FrameWeb allows developers to design and model their applications using constructs that relate to the frameworks used in practice, facilitating developer communication using the models and generating code to improve developer productivity.},
booktitle = {Proceedings of the 29th Brazilian Symposium on Multimedia and the Web},
pages = {260–268},
numpages = {9},
keywords = {DSL, FrameWeb, MDD, Reuse, SPA Frameworks, Software Engineering, WIS Frameworks, Web Engineering, language, method, tools},
location = {Ribeir\~{a}o Preto, Brazil},
series = {WebMedia '23}
}

@inproceedings{10.1145/3610978.3640622,
author = {Jokinen, Kristiina and Wilcock, Graham},
title = {Exploring a Japanese Cooking Database},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610978.3640622},
doi = {10.1145/3610978.3640622},
abstract = {The paper describes ongoing work applying Generative AI to a real world application. We use Retrieval Augmented Generation and other GenAI tools that combine large language models with Neo4j knowledge graphs. These tools help a robot to chat in English about Japanese cooking using a knowledge base that is in Japanese.},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {578–582},
numpages = {5},
keywords = {Japanese cooking, cypher query language, generative AI, graph databases, knowledge graphs, large language models, retrieval augmented generation, semantic search, social robots},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/3567512.3567516,
author = {Steimann, Friedrich and Freitag, Marius},
title = {The Semantics of Plurals},
year = {2022},
isbn = {9781450399197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567512.3567516},
doi = {10.1145/3567512.3567516},
abstract = {Inside many software languages lives an expression language that caters for the computation of single values from single values. These languages' fixation on single-valuedness is often at odds with their application domains, in which many values, or plurals, regularly occur in the places of single. While the classical mathematical means of dealing with plurals is the set, in computing, other representations have evolved, notably strings and the much lesser known bunches. We review bunch theory in the context of expression languages including non-recursive functions, and show how giving bunches set semantics suggests that evaluating bunch functions amounts to computing with relations. We maintain that the ensuing seamless integration of relations in expression languages that otherwise know only functions makes a worthwhile contribution in a field in which the difference between modeling, with its preference for relations, and programming, with its preference for functions, is increasingly considered accidental.},
booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {36–54},
numpages = {19},
keywords = {bunches, collections, denotational semantics, modeling, relational languages},
location = {Auckland, New Zealand},
series = {SLE 2022}
}

@article{10.1145/3702323,
author = {Aloraini, Abdulrahman and Yu, Juntao and Aliady, Wateen and Poesio, Massimo},
title = {A Survey of Coreference and Zeros Resolution for Arabic},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3702323},
doi = {10.1145/3702323},
abstract = {Coreference resolution is the task of resolving mentions that refer to the same entity into clusters. The area and its tasks are crucial in natural language processing (NLP) applications. Extensive surveys of this task have been conducted for English and Chinese; not too much for Arabic. The few Arabic surveys do not cover recent progress and the challenges for Arabic anaphora; and do not cover zero resolution and comprehensive resolution of zeros and full mentions, or anaphora resolution beyond coreference (e.g., bridging). In this paper, we examine the state-of-the-art for Arabic anaphora resolution, highlighting the challenges and advances in this field. We provide a comprehensive survey of the methods employed for Arabic coreference resolution, as well as an overview of the existing datasets and challenges. The goal is to equip researchers with a thorough understanding of Arabic anaphora resolution and to suggest potential future directions in the field.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
keywords = {Anaphora Resolution, Coreference Resolution, Arabic Natural Language Processing, Machine Learning.}
}

@inproceedings{10.1145/3456887.3457465,
author = {Lu, Bai},
title = {Trend estimation model of students' thought and behavior based on big data},
year = {2021},
isbn = {9781450389969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3456887.3457465},
doi = {10.1145/3456887.3457465},
abstract = {With the rapid development of global informatization, the ways for contemporary college students to acquire knowledge are enriched. College students have active thinking, strong ability to accept new things, easy to be influenced by various cultures, open-minded, and novel values. College students' ideas have gradually matured, but they are highly malleable and likely to be influenced by the external environment. In order to quantitatively analyze the trend of students' thinking and behavior, an estimation model of students' thinking and behavior trend based on big data is proposed. Constructing semantic ontology big data distribution set of students' thought and behavior trends, establishing semantic ontology fusion feature distribution set of students' thought and behavior trend estimation by adopting multi-source parameter distributed reconstruction and phase space fusion analysis methods, analyzing the parameter feature quantity of students' thought and behavior trend distribution fusion by combining ambiguity detection and information feature matching methods, and constructing association rule distribution set of students' thought and behavior trend estimation by adopting global ambiguity reconstruction and feature reconstruction methods. Through fuzzy detection and information fusion, the semantic structure characteristics of students' ideological and behavioral trends are analyzed. By matching ideological and behavioral characteristics and mining statistical information, students' ideological and behavioral trends are estimated and self-adaptive convergence control is realized, and the optimal solution of students' ideological and behavioral trends estimation is obtained. The simulation results show that this method is highly adaptive and stable in estimating the trend of students' thinking and behavior, and improves the accurate probability of estimating the trend of students' thinking and behavior.},
booktitle = {2021 2nd International Conference on Computers, Information Processing and Advanced Education},
pages = {1084–1089},
numpages = {6},
keywords = {Big data, Information fusion, Students' thought and behavior, Trend estimation},
location = {Ottawa, ON, Canada},
series = {CIPAE 2021}
}

@article{10.1145/3464938,
author = {Russo, Daniel},
title = {The Agile Success Model: A Mixed-methods Study of a Large-scale Agile Transformation},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3464938},
doi = {10.1145/3464938},
abstract = {Organizations are increasingly adopting Agile frameworks for their internal software development. Cost reduction, rapid deployment, requirements and mental model alignment are typical reasons for an Agile transformation. This article presents an in-depth field study of a large-scale Agile transformation in a mission-critical environment, where stakeholders’ commitment was a critical success factor. The goal of such a transformation was to implement mission-oriented features, reducing costs and time to operate in critical scenarios. The project lasted several years and involved over 40 professionals. We report how a hierarchical and plan-driven organization exploited Agile methods to develop a Command \&amp; Control (C2) system. Accordingly, we first abstract our experience, inducing a success model of general use for other comparable organizations by performing a post-mortem study. The goal of the inductive research process was to identify critical success factors and their relations. Finally, we validated and generalized our model through Partial Least Squares - Structural Equation Modelling, surveying 200 software engineers involved in similar projects. We conclude the article with data-driven recommendations concerning the management of Agile projects.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jul,
articleno = {52},
numpages = {46},
keywords = {Agile, ethnography, grounded theory, mixed methods research, multivariate analysis, partial least squares, structural equation modeling}
}

@inproceedings{10.1145/3550355.3552407,
author = {Rasti, Aidin and Amyot, Daniel and Parvizimosaed, Alireza and Roveri, Marco and Logrippo, Luigi and Anda, Amal Ahmed and Mylopoulos, John},
title = {Symboleo2SC: from legal contract specifications to smart contracts},
year = {2022},
isbn = {9781450394666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550355.3552407},
doi = {10.1145/3550355.3552407},
abstract = {Smart contracts (SCs) are software systems that monitor and control the execution of legal contracts to ensure compliance with the contracts' terms and conditions. They often exploit Internet-of-Things technologies to support their monitoring functions, and blockchain technology to ensure the integrity of their data. Ethereum and business blockchain platforms, such as Hyperledger Fabric, are popular choices for SC development. However, there is a gap in the knowledge of SCs between developers and legal experts. Symboleo is a formal specification language for legal contracts that was introduced to address this issue. Symboleo specifications directly encode legal concepts such as parties, obligations, and powers. In this paper, we propose a tool-supported method for translating Symboleo specifications into smart contracts. We have extended the current Symboleo IDE, implemented the ontology and semantics of Symboleo into a reusable library, and developed the Symboleo2SC tool to generate Hyperledger Fabric code exploiting this library. Symboleo2SC was evaluated with three sample contracts. The results shows that legal contract specifications in Symboleo can be fully converted to SCs for monitoring purposes. Moreover, Symboleo2SC helps simplify the SC development process, saves development effort, and helps reduce risks of coding errors.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems},
pages = {300–310},
numpages = {11},
keywords = {blockchain, code generation, domain-specific languages, legal ontology, smart contracts},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@article{10.1109/TCBB.2020.2968882,
author = {Zhang, Fuhao and Song, Hong and Zeng, Min and Wu, Fang-Xiang and Li, Yaohang and Pan, Yi and Li, Min},
title = {A Deep Learning Framework for Gene Ontology Annotations With Sequence- and Network-Based Information},
year = {2020},
issue_date = {Nov.-Dec. 2021},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {18},
number = {6},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.2968882},
doi = {10.1109/TCBB.2020.2968882},
abstract = {Knowledge of protein functions plays an important role in biology and medicine. With the rapid development of high-throughput technologies, a huge number of proteins have been discovered. However, there are a great number of proteins without functional annotations. A protein usually has multiple functions and some functions or biological processes require interactions of a plurality of proteins. Additionally, Gene Ontology provides a useful classification for protein functions and contains more than 40,000 terms. We propose a deep learning framework called DeepGOA to predict protein functions with protein sequences and protein-protein interaction (PPI) networks. For protein sequences, we extract two types of information: sequence semantic information and subsequence-based features. We use the word2vec technique to numerically represent protein sequences, and utilize a Bi-directional Long and Short Time Memory (Bi-LSTM) and multi-scale convolutional neural network (multi-scale CNN) to obtain the global and local semantic features of protein sequences, respectively. Additionally, we use the InterPro tool to scan protein sequences for extracting subsequence-based information, such as domains and motifs. Then, the information is plugged into a neural network to generate high-quality features. For the PPI network, the Deepwalk algorithm is applied to generate its embedding information of PPI. Then the two types of features are concatenated together to predict protein functions. To evaluate the performance of DeepGOA, several different evaluation methods and metrics are utilized. The experimental results show that DeepGOA outperforms DeepGO and BLAST.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jan,
pages = {2208–2217},
numpages = {10}
}

@inproceedings{10.1145/3297280.3297595,
author = {Dewitte, Pierre and Wuyts, Kim and Sion, Laurens and Van Landuyt, Dimitri and Emanuilov, Ivo and Valcke, Peggy and Joosen, Wouter},
title = {A comparison of system description models for data protection by design},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297595},
doi = {10.1145/3297280.3297595},
abstract = {Since the General Data Protection Regulation (GDPR) entered into force, every actor involved in the processing of personal data must comply with Data Protection by Design (DPbD). Doing so requires assessing the risks to data subjects' rights and freedoms and implementing appropriate countermeasures. While legal experts traditionally apply Data Protection Impact Assessments (DPIA), software engineers rely on threat modeling for their assessment.Despite significant differences, both approaches nonetheless revolve around (i) a description of the system and (ii) the identification, assessment and mitigation of specific risks. In practice, however, DPIAs and threat modeling are usually performed in complete isolation, following their own, unharmonized lexicon and abstractions. Such as disconnect lowers the quality of the assessment and of the conceptual and architectural trade-offsIn this paper, we present (i) an overview of the legal and architectural modeling requirements and (ii) incentives and recommendations for aligning both modeling paradigms in order to support data protection by design from both a legal and a technical perspective.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1512–1515},
numpages = {4},
keywords = {data protection by design, privacy, system model, threat modeling},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@article{10.1145/3524111,
author = {Niraula, Nobal B. and Dulal, Saurab and Koirala, Diwa},
title = {Linguistic Taboos and Euphemisms in Nepali},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3524111},
doi = {10.1145/3524111},
abstract = {Languages across the world have words, phrases, and behaviors—the taboos—that are avoided in public communication considering them as obscene or disturbing to the social, religious, and ethical values of society. However, people deliberately use these linguistic taboos and other language constructs to make hurtful, derogatory, and obscene comments. It is nearly impossible to construct a universal set of offensive or taboo terms because offensiveness is determined entirely by different factors such as socio-physical setting, speaker-listener relationship, and word choices. In this article, we present a detailed corpus-based study of offensive language in Nepali. We identify and describe more than 18 different categories of linguistic offenses including politics, religion, race, and sex. We discuss 12 common euphemisms, such as synonym, metaphor, and circumlocution. In addition, we introduce a manually constructed dataset of more than 1,000 offensive and taboo terms popular among contemporary speakers. We describe the first experiments that provide baseline results in detecting offensive language in Nepali. This in-depth study of offensive language and resource will provide a foundation for several downstream tasks, such as offensive language detection and language learning.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {129},
numpages = {26},
keywords = {Offensive language, linguistic taboo, dataset, offensive language detection, low-resource language, Nepali language}
}

@inproceedings{10.1145/3297662.3365813,
author = {Spiliotopoulos, Dimitris and Margaris, Dionisis and Vassilakis, Costas},
title = {Citizen Engagement for Transparent and Accountable Policy Modelling},
year = {2020},
isbn = {9781450362382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297662.3365813},
doi = {10.1145/3297662.3365813},
abstract = {This work presents a platform for linked legislative data to engage citizens in transparent and effective democracies. With a focus on scaling up participatory approaches from local to national level, the approach extends well established and open source tools and technologies, to build mobile monitoring and analysis tools that increase transparency of law-making and implementation to citizens. This is achieved by combining open data and open services with user and citizen generated content, in order to address citizen's needs in the context of open government. Data and feeds from trusted sources are interconnected with new and re-purposed data feeds generated by users via the social web to form a meaningful, searchable, customizable, reusable and open data-focused personalised mobile public service approach. The framework exploits the social aspects of open data, as well as the training of users, citizens and public servants to be able to understand and demand useful public open data, as well as facilitate the opening of more data.},
booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
pages = {158–165},
numpages = {8},
keywords = {Accountability, Citizen Engagement, Legislation, Mobile Public Services, Natural Language Processing, Policy Modelling, Transparency, e-Government},
location = {Limassol, Cyprus},
series = {MEDES '19}
}

@inproceedings{10.1145/3665601.3669846,
author = {Feng, Yanlin and Rahman, Sajjadur and Feng, Aaron and Chen, Vincent and Kandogan, Eser},
title = {CMDBench: A Benchmark for Coarse-to-fine Multimodal Data Discovery in Compound AI Systems},
year = {2024},
isbn = {9798400706943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3665601.3669846},
doi = {10.1145/3665601.3669846},
abstract = {Compound AI systems (CASs) that employ LLMs as agents to accomplish knowledge-intensive tasks via interactions with tools and data retrievers have garnered significant interest within database and AI communities. While these systems have the potential to supplement typical analysis workflows of data analysts in enterprise data platforms, unfortunately, CASs are subject to the same data discovery challenges that analysts have encountered over the years — silos of multimodal data sources, created across teams and departments within an organization, make it difficult to identify appropriate data sources for accomplishing the task at hand. Existing data discovery benchmarks do not model such multimodality and multiplicity of data sources. Moreover, benchmarks of CASs prioritize only evaluating end-to-end task performance. To catalyze research on evaluating the data discovery performance of multimodal data retrievers in CASs within a real-world setting, we propose CMDBench, a benchmark modeling the complexity of enterprise data platforms. We adapt existing datasets and benchmarks in open-domain — from question answering and complex reasoning tasks to natural language querying over structured data — to evaluate coarse- and fine-grained data discovery and task execution performance. Our experiments reveal the impact of data retriever design on downstream task performance — 46\% drop in task accuracy on average — across various modalities, data sources, and task difficulty. The results indicate the need to develop optimization strategies to identify appropriate LLM agents and retrievers for efficient execution of CASs over enterprise data.},
booktitle = {Proceedings of the Conference on Governance, Understanding and Integration of Data for Effective and Responsible AI},
pages = {16–25},
numpages = {10},
keywords = {Benchmark, Compound AI Systems., Data Discovery, LLMs},
location = {Santiago, AA, Chile},
series = {GUIDE-AI '24}
}

@article{10.1109/TCBB.2022.3173789,
author = {Ranjan, Ashish and Fahad, Md Shah and Fern\'{a}ndez-Baca, David and Tripathi, Sudhakar and Deepak, Akshay},
title = {MCWS-Transformers: Towards an Efficient Modeling of Protein Sequences via Multi Context-Window Based Scaled Self-Attention},
year = {2022},
issue_date = {March-April 2023},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {20},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2022.3173789},
doi = {10.1109/TCBB.2022.3173789},
abstract = {This paper advances the self-attention mechanism in the standard transformer network specific to the modeling of the protein sequences. We introduce a novel &lt;italic&gt;context-window based scaled self-attention&lt;/italic&gt; mechanism for processing protein sequences that is based on the notion of (i) &lt;italic&gt;local context&lt;/italic&gt; and (ii) &lt;italic&gt;large contextual pattern&lt;/italic&gt;. Both notions are essential to building a good representation for protein sequences. The proposed &lt;italic&gt;context-window based scaled self-attention&lt;/italic&gt; mechanism is further used to build the &lt;italic&gt;multi context-window based scaled (MCWS) transformer&lt;/italic&gt; network for the protein function prediction task at the protein sub-sequence level. Overall, the proposed &lt;italic&gt;MCWS transformer&lt;/italic&gt; network produced improved predictive performances, outperforming existing state-of-the-art approaches by substantial margins. With respect to the standard transformer network, the proposed network produced improvements in F1-score of +2.30% and +2.08% on the biological process (BP) and molecular function (MF) datasets, respectively. The corresponding improvements over the state-of-the-art ProtVecGen-Plus+ProtVecGen-Ensemble approach are +3.38% (BP) and +2.86% (MF). Equally important, robust performances were obtained across protein sequences of different lengths.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {1188–1199},
numpages = {12}
}

@article{10.1145/3461736,
author = {Bowen, Judy and Dittmar, Anke and Weyers, Benjamin},
title = {Task Modelling for Interactive System Design: A Survey of Historical Trends, Gaps and Future Needs},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {EICS},
url = {https://doi.org/10.1145/3461736},
doi = {10.1145/3461736},
abstract = {Task models have been used for decades in interactive system design and there are several mature modelling approaches with corresponding tool support. However, in our own work, we have also experienced their limitations, especially in situations where task models are partial ancillary models and not primary artifacts. This was one of the motivations for this paper, which presents a systematic examination of literature to better understand the current place of task models in the continual evolution of user-centred software development practices. While overview work in this domain typically focuses on the analysis of representative task modelling notations and/or tools and relies on foundation papers, we apply a mixed top-down and bottom-up approach to identify relevant themes and trends in the use of task models over the last twenty-years. The paper identifies and discusses dominant patterns of use as well as gaps. It provides a comprehensive framing of both past and present trends in task modelling and supports those who want to incorporate task modelling in their own work. From this we identify areas of research that should receive greater attention in order to address future considerations.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {214},
numpages = {22},
keywords = {interactive system design, task analysis, task models, task-based design}
}

@inproceedings{10.1145/3469213.3470326,
author = {Chen, Qiuping},
title = {Formal Semantic Model for Mobile Cloud Service System},
year = {2021},
isbn = {9781450390200},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469213.3470326},
doi = {10.1145/3469213.3470326},
abstract = {The paper proposes a semantic retrieval model for mobile learning resources based on ontology. The model includes three modules: word segmentation of retrieval information, semantic expansion, and semantic retrieval. We describe the agent as an object node in the category theory, the interaction and dependency between the agents as a morphism, and the entire cloud service system as a type category graph. The case study shows that the framework cannot only support the modeling and analysis of the service system at the abstract level, but also support the service composition by mapping the abstract model to the realization technology, which can analyze the correctness of the requirement decomposition and service composition well. It can be used to guide the description and construction of the service system.},
booktitle = {2021 2nd International Conference on Artificial Intelligence and Information Systems},
articleno = {123},
numpages = {4},
location = {Chongqing, China},
series = {ICAIIS 2021}
}

@article{10.1145/3604605,
author = {Ibrohim, Muhammad Okky and Bosco, Cristina and Basile, Valerio},
title = {Sentiment Analysis for the Natural Environment: A Systematic Review},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3604605},
doi = {10.1145/3604605},
abstract = {In this systematic review, Kitchenham’s framework is used to explore what tasks, techniques, and benchmarks for Sentiment Analysis have been developed for addressing topics about the natural environment. We comprehensively analyze seven dimensions including contribution, topical focus, data source and query, annotation, language, detail of the task, and technology/algorithm used. By showing how this research area has grown during the last few years, our investigation provides important findings about the results achieved and the challenges that need to be still addressed for making this technology actually helpful for stakeholders such as policymakers and governments.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {88},
numpages = {37},
keywords = {Natural environment, data-driven policy, sentiment analysis, natural language processing (NLP), systematic review}
}

@article{10.1145/3584861,
author = {Das, Ringki and Singh, Thoudam Doren},
title = {Image–Text Multimodal Sentiment Analysis Framework of Assamese News Articles Using Late Fusion},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3584861},
doi = {10.1145/3584861},
abstract = {Before the arrival of the web as a corpus, people detected positive and negative news based on the understanding of the textual content from physical newspaper rather than an automatic identification approach from readily available e-newspapers. Thus, the earlier sentiment analysis approach is based on unimodal data, and less effort is paid to the multimodal data. However, the presence of multimodal information helps us to get a clearer understanding of the sentiment. To the best of our knowledge, less work has been introduced on the image–text multimodal sentiment analysis framework of Assamese, a low-resource Indian language mostly spoken in the northeast part of India. We built an Assamese news articles dataset consisting of news text and associated images and one image caption to conduct an experimental study. Focusing on important words and discriminative regions of the images mostly related to sentiment, two individual unimodal such as textual and visual models are proposed. The visual model is developed using an encoder-decoder–based image caption generation system. An image–text multimodal approach is proposed to explore the internal correlation between textual and visual features for joint sentiment classification. Finally, we propose the multimodal sentiment analysis framework, i.e., Textual Visual Multimodal Fusion, by employing a late fusion scheme to merge the three different modalities for the final sentiment prediction. Experimental results conducted on the Assamese dataset built in-house demonstrate that the contextual integration of multimodal features delivers better performance than unimodal features.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jun,
articleno = {161},
numpages = {30},
keywords = {Multimodal sentiment analysis, low resource language, caption generation, machine learning classifier, late fusion}
}

@article{10.1145/3359253,
author = {Kursuncu, Ugur and Gaur, Manas and Castillo, Carlos and Alambo, Amanuel and Thirunarayan, Krishnaprasad and Shalin, Valerie and Achilov, Dilshod and Arpinar, I. Budak and Sheth, Amit},
title = {Modeling Islamist Extremist Communications on Social Media using Contextual Dimensions: Religion, Ideology, and Hate},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359253},
doi = {10.1145/3359253},
abstract = {Terror attacks have been linked in part to online extremist content. Online conversations are cloaked in religious ambiguity, with deceptive intentions, often twisted from mainstream meaning to serve a malevolent ideology. Although tens of thousands of Islamist extremism supporters consume such content, they are a small fraction relative to peaceful Muslims. The efforts to contain the ever-evolving extremism on social media platforms have remained inadequate and mostly ineffective. Divergent extremist and mainstream contexts challenge machine interpretation, with a particular threat to the precision of classification algorithms. Radicalization is a subtle long-running persuasive process that occurs over time. Our context-aware computational approach to the analysis of extremist content on Twitter breaks down this persuasion process into building blocks that acknowledge inherent ambiguity and sparsity that likely challenge both manual and automated classification. Based on prior empirical and qualitative research in social sciences, particularly political science, we model this process using a combination of three contextual dimensions -- religion, ideology, and hate -- each elucidating a degree of radicalization and highlighting independent features to render them computationally accessible. We utilize domain-specific knowledge resources for each of these contextual dimensions such as Qur'an for religion, the books of extremist ideologues and preachers for political ideology and a social media hate speech corpus for hate. The significant sensitivity of the Islamist extremist ideology and its local and global security implications require reliable algorithms for modelling such communications on Twitter. Our study makes three contributions to reliable analysis: (i) Development of a computational approach rooted in the contextual dimensions of religion, ideology, and hate, which reflects strategies employed by online Islamist extremist groups, (ii) An in-depth analysis of relevant tweet datasets with respect to these dimensions to exclude likely mislabeled users, and (iii) A framework for understanding online radicalization as a process to assist counter-programming. Given the potentially significant social impact, we evaluate the performance of our algorithms to minimize mislabeling, where our context-aware approach outperforms a competitive baseline by 10.2\% in precision, thereby enhancing the potential of such tools for use in human review.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {151},
numpages = {22},
keywords = {contextual dimensions, islamist extremism, multi-dimensional modeling, radicalization, user modeling}
}

@inproceedings{10.1145/3167132.3167370,
author = {Tiso, Alessandro and Reggio, Gianna and Leotta, Maurizio and Ricca, Filippo},
title = {A method for developing model to text transformations},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167370},
doi = {10.1145/3167132.3167370},
abstract = {In the field of business process development, model transformations play a key role, for example for moving from business process models to either code or inputs for simulation systems, as well as to convert models expressed with notation A into equivalent models expressed with notation B. In the literature, many cases of useful transformations of business process models can be found. However, in general each transformation has been developed in an ad-hoc fashion, at a quite low-level, and its quality is often neglected. To ensure the quality of the transformations is important to apply to them all the well-known software engineering principles and practices, from the requirements definition to the testing activities. For this reason, we propose a method, MeDMoT, for developing non-trivial Model to Text Transformations, which prescribes how to: (1) capture and specify the transformation requirements; (2) design the transformation, (3) implement the transformation and (4) test the transformation. The method has been applied in several case studies, including a transformation of UML business processes into inputs for an agent-based simulator.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {138–141},
numpages = {4},
keywords = {UML, model to text transformations, model-driven},
location = {Pau, France},
series = {SAC '18}
}

@article{10.1109/TASLP.2023.3293030,
author = {Yoshino, Koichiro and Chen, Yun-Nung and Crook, Paul and Kottur, Satwik and Li, Jinchao and Hedayatnia, Behnam and Moon, Seungwhan and Fei, Zhengcong and Li, Zekang and Zhang, Jinchao and Feng, Yang and Zhou, Jie and Kim, Seokhwan and Liu, Yang and Jin, Di and Papangelis, Alexandros and Gopalakrishnan, Karthik and Hakkani-Tur, Dilek and Damavandi, Babak and Geramifard, Alborz and Hori, Chiori and Shah, Ankit and Zhang, Chen and Li, Haizhou and Sedoc, Jo\~{a}o and D'Haro, Luis F. and Banchs, Rafael and Rudnicky, Alexander},
title = {Overview of the Tenth Dialog System Technology Challenge: DSTC10},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3293030},
doi = {10.1109/TASLP.2023.3293030},
abstract = {This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {765–778},
numpages = {14}
}

@inproceedings{10.1145/3583780.3614753,
author = {Hoseini, Sayed and Ali, Ahmed and Shaker, Haron and Quix, Christoph},
title = {SEDAR: A Semantic Data Reservoir for Heterogeneous Datasets},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3614753},
doi = {10.1145/3583780.3614753},
abstract = {Data lakes have emerged as a solution for managing vast and diverse datasets for modern data analytics. To prevent them from becoming ungoverned, semantic data management techniques are crucial, which involve connecting metadata with knowledge graphs, following the principles of Linked Data. This semantic layer enables more expressive data management, integration from various sources and enhances data access utilizing the concepts and relations to semantically enrich the data. Some frameworks have been proposed, but requirements like data versioning, linking of datasets, managing machine learning projects, automated semantic modeling and ontology-based data access are not supported in one uniform system. We demonstrate SEDAR, a comprehensive semantic data lake that includes support for data ingestion, storage, processing, and governance with a special focus on semantic data management. The demo will showcase how the system allows for various ingestion scenarios, metadata enrichment, data source linking, profiling, semantic modeling, data integration and processing inside a machine learning life cycle.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5056–5060},
numpages = {5},
keywords = {data lake, ontology-based data access, semantic data lake, semantic data management, semantic modeling},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@inproceedings{10.1145/3411764.3445674,
author = {Zhou, Zhilan and Wen, Ximing and Wang, Yue and Gotz, David},
title = {Modeling and Leveraging Analytic Focus During Exploratory Visual Analysis},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445674},
doi = {10.1145/3411764.3445674},
abstract = {Visual analytics systems enable highly interactive exploratory data analysis. Across a range of fields, these technologies have been successfully employed to help users learn from complex data. However, these same exploratory visualization techniques make it easy for users to discover spurious findings. This paper proposes new methods to monitor a user’s analytic focus during visual analysis of structured datasets and use it to surface relevant articles that contextualize the visualized findings. Motivated by interactive analyses of electronic health data, this paper introduces a formal model of analytic focus, a computational approach to dynamically update the focus model at the time of user interaction, and a prototype application that leverages this model to surface relevant medical publications to users during visual analysis of a large corpus of medical records. Evaluation results with 24 users show that the modeling approach has high levels of accuracy and is able to surface highly relevant medical abstracts.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {21},
numpages = {15},
keywords = {Analytic Focus, Insight Provenance, User Modeling, Visual Analytics},
location = {Yokohama, Japan},
series = {CHI '21}
}

@inproceedings{10.5555/3306127.3331869,
author = {Pico-Valencia, Pablo and Holgado-Terriza, Juan A. and Senso, Jos\'{e}},
title = {An Agent Model Based on Open Linked Data for Building Internet of Agents Ecosystems},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper presents an smart, collaborative and self-adaptive reactive agent model aimed at managing the resources of objects connected to Internet of Things (IoT). This agent model, called Linked Open Agent (LOA), is described using both a semantic agent contract built from descriptors published as linked data, and a workflow for agent control that is completed at runtime by the agent itself to address its behavior. The accuracy for semantic discovering agents partners was evaluated and compared with generic models of discovery such as the Yellow Pages of Java Agent DEvelopment Framework (JADE) and the Java implementation of the Universal Description, Discovery, and Integration (jUDDI). The results demonstrated that our method had a better accuracy for recovering agents than the accuracy of JADE and JUDDI.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1536–1538},
numpages = {3},
keywords = {contract, internet of agents, internet of things, linked open data},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3597512.3597529,
author = {Hatherall, Louise and Kek\"{u}ll\"{u}o\u{g}lu, Dilara and Kokciyan, Nadin and Rovatsos, Michael and Sethi, Nayha and Vierkant, Tillmann and Vallor, Shannon},
title = {Responsible Agency Through Answerability: Cultivating the Moral Ecology of Trustworthy Autonomous Systems},
year = {2023},
isbn = {9798400707346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597512.3597529},
doi = {10.1145/3597512.3597529},
abstract = {The decades-old debate over so-called ‘responsibility gaps’ in intelligent systems has recently been reinvigorated by rapid advances in machine learning techniques that are delivering many of the capabilities of machine autonomy that Matthias [1] originally anticipated. The emerging capabilities of intelligent learning systems highlight and exacerbate existing challenges with meaningful human control of, and accountability for, the actions and effects of such systems. The related challenge of human ‘answerability’ for system actions and harms has come into focus in recent literature on responsibility gaps [2, 3]. We describe a proposed interdisciplinary approach to designing for answerability in autonomous systems, grounded in an instrumentalist framework of ‘responsible agency cultivation’ drawn from moral philosophy and cognitive sciences as well as empirical results from structured interviews and focus groups in the application domains of health, finance and government. We outline a prototype dialogue agent informed by these emerging results and designed to help bridge the structural gaps in organisations that typically impede the human agents responsible for an autonomous sociotechnical system from answering to vulnerable patients of responsibility.},
booktitle = {Proceedings of the First International Symposium on Trustworthy Autonomous Systems},
articleno = {50},
numpages = {5},
keywords = {AI ethics, Agency, Answerability, Dialogue agents, Responsibility gaps, Sociotechnical Systems Design},
location = {Edinburgh, United Kingdom},
series = {TAS '23}
}

@inproceedings{10.1109/MODELS-C.2019.00067,
author = {Amrani, Moussa and Blouin, Dominique and Heinrich, Robert and Rensink, Arend and Vangheluwe, Hans and Wortmann, Andreas},
title = {Towards a formal specification of multi-paradigm modelling},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00067},
doi = {10.1109/MODELS-C.2019.00067},
abstract = {The notion of a programming paradigm is used to classify programming languages and their accompanying workflows based on their salient features. Similarly, the notion of a modelling paradigm can be used to characterise the plethora of modelling approaches used to engineer complex Cyber-Physical Systems (CPS). Modelling paradigms encompass formalisms, abstractions, workflows and supporting tool(chain)s. A precise definition of this modelling paradigm notion is lacking however. Such a definition will increase insight, will allow for formal reasoning about the consistency of modelling frameworks and may serve as the basis for the construction of new modelling, simulation, verification, synthesis, ... environments to support design of CPS. We present a formal framework aimed at capturing the notion of modelling paradigm, as a first step towards a comprehensive formalisation of multi-paradigm modelling. Our formalisation is illustrated by CookieCAD, a simple Computer-Aided Design paradigm used in the development of cookie stencils.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {419–424},
numpages = {6},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1145/3543873.3587662,
author = {Paulus, Alexander and Pomp, Andr\'{e} and Meisen, Tobias},
title = {The PLASMA Framework: Laying the Path to Domain-Specific Semantics in Dataspaces},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587662},
doi = {10.1145/3543873.3587662},
abstract = {Modern data management is evolving from centralized integration-based solutions to a non-integration-based process of finding, accessing and processing data, as observed within dataspaces. Common reference dataspace architectures assume that sources publish their own domain-specific schema. These schemas, also known as semantic models, can only be partially created automatically and require oversight and refinement by human modellers. Non-expert users, such as mechanical engineers or municipal workers, often have difficulty building models because they are faced with multiple ontologies, classes, and relations, and existing tools are not designed for non-expert users. The PLASMA framework consists of a platform and auxiliary services that focus on providing non-expert users with an accessible way to create and edit semantic models, combining automation approaches and support systems such as a recommendation engine. It also provides data conversion from raw data to RDF. In this paper we highlight the main features, like the modeling interface and the data conversion engine. We discuss how PLASMA as a tool is suitable for building semantic models by non-expert users in the context of dataspaces and show some applications where PLASMA has already been used in data management projects.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {1474–1479},
numpages = {6},
keywords = {dataspace, graphical user interface, resource description framework, semantic mapping, semantic model},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@article{10.1109/TCBB.2021.3069040,
author = {Zhao, Qichang and Yang, Mengyun and Cheng, Zhongjian and Li, Yaohang and Wang, Jianxin},
title = {Biomedical Data and Deep Learning Computational Models for Predicting Compound-Protein Relations},
year = {2021},
issue_date = {July-Aug. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {4},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2021.3069040},
doi = {10.1109/TCBB.2021.3069040},
abstract = {The identification of compound-protein relations (CPRs), which includes compound-protein interactions (CPIs) and compound-protein affinities (CPAs), is critical to drug development. A common method for compound-protein relation identification is the use of &lt;italic&gt;in vitro&lt;/italic&gt; screening experiments. However, the number of compounds and proteins is massive, and &lt;italic&gt;in vitro&lt;/italic&gt; screening experiments are labor-intensive, expensive, and time-consuming with high failure rates. Researchers have developed a computational field called virtual screening (VS) to aid experimental drug development. These methods utilize experimentally validated biological interaction information to generate datasets and use the physicochemical and structural properties of compounds and target proteins as input information to train computational prediction models. At present, deep learning has been widely used in computer vision and natural language processing and has experienced epoch-making progress. At the same time, deep learning has also been used in the field of biomedicine widely, and the prediction of CPRs based on deep learning has developed rapidly and has achieved good results. The purpose of this study is to investigate and discuss the latest applications of deep learning techniques in CPR prediction. First, we describe the datasets and feature engineering (i.e., compound and protein representations and descriptors) commonly used in CPR prediction methods. Then, we review and classify recent deep learning approaches in CPR prediction. Next, a comprehensive comparison is performed to demonstrate the prediction performance of representative methods on classical datasets. Finally, we discuss the current state of the field, including the existing challenges and our proposed future directions. We believe that this investigation will provide sufficient references and insight for researchers to understand and develop new deep learning methods to enhance CPR predictions.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = mar,
pages = {2092–2110},
numpages = {19}
}

@inproceedings{10.1145/3336191.3371772,
author = {Zhou, Tianshuo and Li, Ziyang and Cheng, Gong and Wang, Jun and Wei, Yu'Ang},
title = {GREASE: A Generative Model for Relevance Search over Knowledge Graphs},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3371772},
doi = {10.1145/3336191.3371772},
abstract = {Relevance search is to find top-ranked entities in a knowledge graph (KG) that are relevant to a query entity. Relevance is ambiguous, particularly over a schema-rich KG like DBpedia which supports a wide range of different semantics of relevance based on numerous types of relations and attributes. As users may lack the expertise to formalize the desired semantics, supervised methods have emerged to learn the hidden user-defined relevance from user-provided examples. Along this line, in this paper we propose a novel generative model over KGs for relevance search, named GREASE. The model applies to meta-path based relevance where a meta-path characterizes a particular type of semantics of relating the query entity to answer entities. It is also extended to support properties that constrain answer entities. Extensive experiments on two large-scale KGs demonstrate that GREASE has advanced the state of the art in effectiveness, expressiveness, and efficiency.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {780–788},
numpages = {9},
keywords = {generative model, knowledge graph, meta-path, relevance search},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.1145/3594315.3594362,
author = {Yu, Yang and Liu, Xiangzhi},
title = {Research on enterprise text classification methods of BiLSTM and CNN based on BERT},
year = {2023},
isbn = {9781450399029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594315.3594362},
doi = {10.1145/3594315.3594362},
abstract = {The traditional enterprise text data classification method ignores the context of the text. Each word is independent from each other and cannot represent semantic information. The text description and classification effect is poor, and the feature engineering needs human intervention, so the generalization ability is not strong. In view of the low efficiency and accuracy of enterprise text data classification, this paper proposes a bidirectional encoder representation based on Transformer (BERT) The enterprise text classification model BBLC-ATT based on convolutional neural networks (CNN) and bi-directional long short-term memory (BiLSTM) neural networks and attention mechanism (Attention). The model uses BERT training word vector and combines the features of CNN and BiLSTM to capture local potential features and context information. Secondly, the feature vectors extracted from the hybrid network layer are input into the self-attention layer to extract the syntactic and semantic features between words in the enterprise text sentences. Finally, this paper compares BBLC-ATT model with traditional deep learning model in terms of accuracy, accuracy, recall and F1 value. The experimental results show that the BBLC-ATT model is superior to other models in all evaluation indicators, and the accuracy rate is increased by 3.28\% - 15.86\%.},
booktitle = {Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence},
pages = {491–495},
numpages = {5},
keywords = {Attention model, BERT model, BiLSTM model, CNN model, Natural language processing, Text content classification},
location = {Tianjin, China},
series = {ICCAI '23}
}

@article{10.1109/TCBB.2018.2849362,
author = {Acharya, Sudipta and Saha, Sriparna and Pradhan, Prasanna},
title = {Multi-Factored Gene-Gene Proximity Measures Exploiting Biological Knowledge Extracted from Gene Ontology: Application in Gene Clustering},
year = {2020},
issue_date = {Jan.-Feb. 2020},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {17},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2018.2849362},
doi = {10.1109/TCBB.2018.2849362},
abstract = {To describe the cellular functions of proteins and genes, a potential dynamic vocabulary is Gene Ontology (GO), which comprises of three sub-ontologies namely, Biological-process, Cellular-component, and Molecular-function. It has several applications in the field of bioinformatics like annotating/measuring gene-gene or protein-protein semantic similarity, identifying genes/proteins by their GO annotations for disease gene and target discovery, etc. To determine semantic similarity between genes, several semantic measures have been proposed in literature, which involve information content of &lt;italic&gt;GO-terms&lt;/italic&gt;, GO tree structure, or the combination of both. But, most of the existing semantic similarity measures do not consider different topological and information theoretic aspects of &lt;italic&gt;GO-terms&lt;/italic&gt; collectively. Inspired by this fact, in this article, we have first proposed three novel semantic similarity/distance measures for genes covering different aspects of GO-tree. These are further implanted in the frameworks of well-known multi-objective and single-objective based clustering algorithms to determine functionally similar genes. For comparative analysis, 10 popular existing GO based semantic similarity/distance measures and tools are also considered. Experimental results on &lt;italic&gt;Mouse genome&lt;/italic&gt;, &lt;italic&gt;Yeast&lt;/italic&gt;, and &lt;italic&gt;Human genome&lt;/italic&gt; datasets evidently demonstrate the supremacy of multi-objective clustering algorithms in association with proposed multi-factored similarity/distance measures. Clustering outcomes are further validated by conducting some biological/statistical significance tests. Supplementary information is available at &lt;uri&gt;https://www.iitp.ac.in/sriparna/journals.html&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = feb,
pages = {207–219},
numpages = {13}
}

@inproceedings{10.1145/3194696.3194706,
author = {Azarm, Mana and Peyton, Liam},
title = {An ontology for a patient-centric healthcare interoperability framework},
year = {2018},
isbn = {9781450357340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194696.3194706},
doi = {10.1145/3194696.3194706},
abstract = {Healthcare clients are increasingly interested to be involved and informed of their healthcare delivery and status [1] [2]. They need to be able to access, view, and analyze their health data easily and securely. Clients need one single gateway to their medical records. Some healthcare providers are creating portals for their clients to flow some data for them to view [3]. In addition, clients can request a portion of their health data in paper format from their healthcare providers by filling in forms and manually submitting their requests. But, this is not sufficient for the average healthcare client. There is a need for a platform independent tool that can automatically gather and combine a client's health information from the various providers in their circle of care and provide the information securely and electronically without inconveniencing the client with multiple requests and sharing agreements [4]. Healthcare providers can also benefit from such a tool in the sense of gaining insights from their colleagues' efforts automatically and without starting a separate quest for each piece of information. In this paper we propose framework and toolset that can provide a secure single point of access to a client's full picture of their personal health information. In particular, we delve into one of the key components of our framework which is our proposed ontology.},
booktitle = {Proceedings of the International Workshop on Software Engineering in Healthcare Systems},
pages = {34–41},
numpages = {8},
keywords = {healthcare API, healthcare applications, healthcare integration, healthcare ontology, patient centric},
location = {Gothenburg, Sweden},
series = {SEHS '18}
}

@inproceedings{10.1145/3372454.3372465,
author = {Fujino, Iwao and Claramunt, Christophe and Boudraa, Abdel-Ouahab},
title = {Extracting 4-Attributes Vessel Courses from AIS Data with PQK-Means and Topic Model},
year = {2020},
isbn = {9781450372015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372454.3372465},
doi = {10.1145/3372454.3372465},
abstract = {AIS (Automatic Identification System) data received from moving vessels over an area of interest can be of very much interest for deriving maritime trajectory patterns. In this paper, a novel approach to extract course patterns from AIS data of vessels is presented. From machine learning and natural language processing principles, a topic model might be used for extracting implicit patterns underlying massive and unstructured collection of incoming data. To apply topic model to AIS data, PQk-means vector quantization to convert AIS data record to code documents is introduced. Then, a topic model is applied to extract course patterns from AIS data. In fact, courses, not only encompasses trajectory locations, but also headings and speeds, are recognized by the proposed algorithm. The performance of PQk-means is evaluated using the relative root mean square error and elapsed time. The potential of the approach is illustrated by a series of experimental results derived from practical AIS data set in a region of North West France.},
booktitle = {Proceedings of the 3rd International Conference on Big Data Research},
pages = {129–135},
numpages = {7},
keywords = {Automatic Identification System (AIS), Course Pattern Extraction, Maritime Big Data, PQk-means, Topic Model, Vector Quantization},
location = {Cergy-Pontoise, France},
series = {ICBDR '19}
}

@inproceedings{10.1145/3394486.3403223,
author = {Li, Diya and Zaki, Mohammed J.},
title = {RECIPTOR: An Effective Pretrained Model for Recipe Representation Learning},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403223},
doi = {10.1145/3394486.3403223},
abstract = {Recipe representation plays an important role in food computing for perception, recognition, recommendation and other applications. Learning pretrained recipe embeddings is a challenging task, as there is a lack of high quality annotated food datasets. In this paper, we provide a joint approach for learning effective pretrained recipe embeddings using both the ingredients and cooking instructions. We present RECIPTOR, a novel set transformer-based joint model to learn recipe representations, that preserves permutation-invariance for the ingredient set and uses a novel knowledge graph (KG) derived triplet sampling approach to optimize the learned embeddings so that related recipes are closer in the latent semantic space. The embeddings are further jointly optimized by combining similarity among cooking instructions with a KG based triplet loss. We experimentally show that RECIPTOR's recipe embeddings outperform state-of-the-art baselines on two newly designed downstream classification tasks by a wide margin.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {1719–1727},
numpages = {9},
keywords = {food computing, food knowledge graph, recipe embedding, representation learning, set transformer},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.1145/3419804.3420267,
author = {Hassane, Omar and Mustafiz, Sadaf and Khendek, Ferhat and Toeroe, Maria},
title = {A Model Traceability Framework for Network Service Management},
year = {2020},
isbn = {9781450381406},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3419804.3420267},
doi = {10.1145/3419804.3420267},
abstract = {Automating enactment along with traceability management of processes using model-driven engineering methods could be of significant benefit to the Network Functions Virtualization (NFV) paradigm in view of its move towards zero-touch automation of the orchestration and management of network services (NS). Earlier, we proposed an integrated process modelling and enactment environment with traceability support, MAPLE-T, for NS management. In this paper, we extend MAPLE-T with the notion of intents. We propose the usage of intents at both the process model (PM) and model-transformation levels as part of our traceability information. We define intents as information representing the objective of the PM actions/activities and their implementations. We extend MAPLE-T with traceability visualization support to visualize trace links relating models at different levels through the captured intents. The intent-enriched traceability information and the enhanced visualization enable semantically richer traceability analysis. We apply our traceability generation and analysis approach to the NS design process in order to show the benefits of intents not only for the process, but also for the whole NS lifecycle management operations.},
booktitle = {Proceedings of the 12th System Analysis and Modelling Conference},
pages = {64–73},
numpages = {10},
keywords = {megamodelling, model transformation, process modelling, traceability},
location = {Virtual Event, Canada},
series = {SAM '20}
}

@inproceedings{10.1145/3194085.3194086,
author = {Damm, Werner and Galbas, Roland},
title = {Exploiting learning and scenario-based specification languages for the verification and validation of highly automated driving},
year = {2018},
isbn = {9781450357395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194085.3194086},
doi = {10.1145/3194085.3194086},
abstract = {We propose a series of methods based on learning key structural properties from traffic data-basis and on statistical model checking, ultimately leading to the construction of a scenario catalogue capturing requirements for controlling criticality for highly autonomous vehicles. We sketch underlying mathematical foundations which allow to derive formal confidence levels that vehicles tested by such a scenario catalogue will maintain the required control of criticality in real traffic matching the probability distributions of key parameters of data recorded in the reference data base employed for this process.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems},
pages = {39–46},
numpages = {8},
keywords = {formal specification, highly automated driving, learning, requirement analysis, statistical model-checking, verification and validation},
location = {Gothenburg, Sweden},
series = {SEFAIS '18}
}

@article{10.1145/3627168,
author = {Liebeskind, Chaya and Liebeskind, Shmuel and Bouhnik, Dan},
title = {Machine Translation for Historical Research: A Case Study of Aramaic-Ancient Hebrew Translations},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3627168},
doi = {10.1145/3627168},
abstract = {In this article, by the ability to translate Aramaic to another spoken languages, we investigated machine translation in a cultural heritage domain for two primary purposes: evaluating the quality of ancient translations and preserving Aramaic (an endangered language). First, we detailed the construction of a publicly available Biblical parallel Aramaic-Hebrew corpus based on two ancient (early 2nd to late 4th century) Hebrew-Aramaic translations: Targum Onkelus and Targum Jonathan. Then using the statistical machine translation approach, which in our use case significantly outperforms neural machine translation, we validated the excepted high quality of the translations. The trained model failed to translate Aramaic texts of other dialects. However, when we trained the same statistical machine translation model on another Aramaic-Hebrew corpus of a different dialect (Zohar, 13th century), a very high translation score was achieved. We examined an additional important cultural heritage source of Aramaic texts, the Babylonian Talmud (early 3rd to late 5th century). Since we do not have a parallel Aramaic-Hebrew corpus of the Talmud, we used the model trained on the Bible corpus for translation. We performed an analysis of the results and suggest some potential promising future research.},
journal = {J. Comput. Cult. Herit.},
month = feb,
articleno = {20},
numpages = {23},
keywords = {Bible translation, neural machine translation, statistical machine translation, low-resource languages, Aramaic-Hebrew}
}

@inproceedings{10.5555/3374138.3374161,
author = {Wagner, Gerd},
title = {Process design modeling with extended event graphs},
year = {2019},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Schruben's Event Graphs (EGs), defining the event types of a simulation model and event scheduling arrows between them, representing causal regularities, provide an elegant visual modeling language and formalism for event-based simulation, which can be viewed as the most fundamental Discrete Event Simulation (DES) approach. We show how to extend and visually improve the language of EGs by adding elements of the Business Process Modeling Notation (BPMN): (1) mini diamonds for designating conditional control flow arrows, (2) Gateways for conditional and parallel branching, (3) typed Data Objects for accommodating object-oriented (OO) state structure modeling, and (4) Activities. The resulting extension of EGs, called Discrete Event Process Modeling Notation (DPMN), is more expressive and visually more clear than traditional EGs, and its visual syntax is harmonized with BPMN process diagrams, thus building a bridge between the DES and the Business Process Management research communities.},
booktitle = {Proceedings of the 2019 Summer Simulation Conference},
articleno = {23},
numpages = {12},
keywords = {BPMN, DPMN, discrete event simulation, event graphs, object event simulation},
location = {Berlin, Germany},
series = {SummerSim '19}
}

@article{10.1145/3651313,
author = {Denisenko, Natalia and Zhang, Youzhi and Pulice, Chiara and Bhattasali, Shohini and Jajodia, Sushil and Resnik, Philip and Subrahmanian, V.S.},
title = {A Psycholinguistics-inspired Method to Counter IP Theft Using Fake Documents},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3651313},
doi = {10.1145/3651313},
abstract = {Intellectual property (IP) theft is a growing problem. We build on prior work to deter IP theft by generating n fake versions of a technical document so a thief has to expend time and effort in identifying the correct document. Our new SbFAKE framework proposes, for the first time, a novel combination of language processing, optimization, and the psycholinguistic concept of surprisal to generate a set of such fakes. We start by combining psycholinguistic-based surprisal scores and optimization to generate two bilevel surprisal optimization problems (an Explicit one and a simpler Implicit one) whose solutions correspond directly to the desired set of fakes. As bilevel problems are usually hard to solve, we then show that these two bilevel surprisal optimization problems can each be reduced to equivalent surprisal-based linear programs. We performed detailed parameter tuning experiments and identified the best parameters for each of these algorithms. We then tested these two variants of SbFAKE (with their best parameter settings) against the best performing prior work in the field. Our experiments show that SbFAKE is able to more effectively generate convincing fakes than past work. In addition, we show that replacing words in an original document with words having similar surprisal scores generates greater levels of deception.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jun,
articleno = {7},
numpages = {25},
keywords = {AI for security, fake document generation}
}

@inproceedings{10.1145/3701716.3717540,
author = {Todorov, Konstantin and Fafalios, Pavlos and Dietze, Stefan and Dimitrov, Dimitar},
title = {BeyondFacts 2025: 5th International Workshop on Computational Methods for Online Discourse Analysis},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3717540},
doi = {10.1145/3701716.3717540},
abstract = {This workshop explores the intersection of computational and interdisciplinary approaches to analyzing online discourse, including claims, arguments, and opinions on controversial topics. With the rise of mis- and disinformation, bias, and echo chambers, NLP-based methods such as argument mining, stance detection, and fact verification have become essential. However, these tasks require robust conceptual foundations across fields like communication studies, computational linguistics, and computer science. BeyondFacts fosters collaboration among diverse research communities-including social sciences, political science, computational journalism, and computer science-to enhance machine-interpretation and analysis of societal debates using techniques from Web mining, AI, and NLP.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {2612–2615},
numpages = {4},
keywords = {computational fact-checking, computational journalism, intent detection, knowledge graphs, llms, mis- and dis-information spread and detection, online discourse analysis, social media mining, stance viewpoint discovery, web mining},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@article{10.1145/3582261,
author = {Wei, Kaiwen and Jin, Li and Zhang, Zequn and Guo, Zhi and Li, Xiaoyu and Liu, Qing and Feng, Weimiao},
title = {More Than Syntaxes: Investigating Semantics to Zero-shot Cross-lingual Relation Extraction and Event Argument Role Labelling},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3582261},
doi = {10.1145/3582261},
abstract = {Syntactic dependency structures are commonly utilized as language-agnostic features to solve the word order difference issues in zero-shot cross-lingual relation and event extraction tasks. However, while sentences in multiple forms can be employed to express the same meaning, the syntactic structure may vary considerably in specific scenarios. To fix this problem, we find semantics are rarely considered, which could provide a more consistent semantic analysis of sentences and be served as another bridge between different languages. Therefore, in this article, we introduce Syntax and Semantic Driven Network (SSDN) to equip syntax and semantic knowledge across languages simultaneously. Specifically, predicate–argument structures from semantic role labelling are explicitly incorporated into word representations. Then, a semantic-aware relational graph convolutional network and a transformer-based encoder are utilized to model both semantic dependency and syntactic dependency structures, respectively. Finally, a fusion module is introduced to integrate output representations adaptively. We conduct experiments on the widely used Automatic Content Extraction 2005 English, Chinese, and Arabic datasets. The evaluation results demonstrate that the proposed method achieves the state-of-the-art performance. Further study also indicates SSDN could produce robust representations that facilitate the transfer operations across languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {61},
numpages = {21},
keywords = {Cross-lingual relation and event extraction, zero-resource transfer, semantic parsing, relational graph convolutional network}
}

@inproceedings{10.1145/3539618.3591997,
author = {Lu, Jiaying and Shen, Jiaming and Xiong, Bo and Ma, Wenjing and Staab, Steffen and Yang, Carl},
title = {HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented Prompting},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591997},
doi = {10.1145/3539618.3591997},
abstract = {Medical decision-making processes can be enhanced by comprehensive biomedical knowledge bases, which require fusing knowledge graphs constructed from different sources via a uniform index system. The index system often organizes biomedical terms in a hierarchy to provide the aligned entities with fine-grained granularity. To address the challenge of scarce supervision in the biomedical knowledge fusion (BKF) task, researchers have proposed various unsupervised methods. However, these methods heavily rely on ad-hoc lexical and structural matching algorithms, which fail to capture the rich semantics conveyed by biomedical entities and terms. Recently, neural embedding models have proved effective in semantic-rich tasks, but they rely on sufficient labeled data to be adequately trained. To bridge the gap between the scarce-labeled BKF and neural embedding models, we propose HiPrompt, a supervision-efficient knowledge fusion framework that elicits the few-shot reasoning ability of large language models through hierarchy-oriented prompts. Empirical results on the collected KG-Hi-BKF benchmark datasets demonstrate the effectiveness of HiPrompt.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2052–2056},
numpages = {5},
keywords = {biomedical knowledge fusion, few-shot prompting, large language models for resource-constrained field, retrieve \&amp; re-rank},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3365109.3368779,
author = {Liu, Jie and Tang, Yan and Xu, Xinyi},
title = {HISDOM: A Hybrid Ontology Mapping System based on Convolutional Neural Network and Dynamic Weight},
year = {2019},
isbn = {9781450370165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365109.3368779},
doi = {10.1145/3365109.3368779},
abstract = {In the information explosion era, mapping multiple ontologies in different knowledge bases could provide a common layer from which several ontologies could be accessed and exchanged in semantically sound manners. However, most ontology mapping approaches rely heavily on human annotation and are restricted to limited domains. Moreover, most prior work is limited to combining a few mapping factors with moderate mapping accuracy, and the ontology mapping weigh calculation process is not adaptive and dynamic. Based on the current generalization of ontology mapping methods, we propose a novel ontology mapping system called HISDOM to improve the generalization performance of ontology mapping. HISDOM uses comprehensive factors like concept names, attributes, instances, and structural similarities to determine the similarity of ontology. A key novelty of HISDOM is leveraging CNN to calculate the comment similarity to assist the mapping process of concepts in the ontology. Then, HISDOM dynamically derives the weight of different factors in the overall ontology similarity proportional to the amount of information of each factor in the ontology. Finally, based on the overall similarity, HISDOM determines whether the two concepts in the ontology can be mapped. We study the performance of HISDOM through extensive experiments. The results show that HISDOM outperforms several baselines in ontology mapping tasks, the dynamic and adaptive weighting mechanism is effective, and all the mapping factors of HISDOM are positive towards improving the ontology mapping accuracy.},
booktitle = {Proceedings of the 6th IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {67–70},
numpages = {4},
keywords = {convolutional neural network, dynamic weight, hybrid similarity, ontology mapping},
location = {Auckland, New Zealand},
series = {BDCAT '19}
}

@book{10.1145/3233795,
editor = {Oviatt, Sharon and Schuller, Bj\"{o}rn and Cohen, Philip R. and Sonntag, Daniel and Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
title = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan \&amp; Claypool},
abstract = {The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative resource on what has become the dominant paradigm for new computer interfaces---user input involving new media (speech, multi-touch, hand and body gestures, facial expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume handbook is written by international experts and pioneers in the field. It provides a textbook, reference, and technology roadmap for professionals working in this and related areas.This third volume focuses on state-of-the-art multimodal language and dialogue processing, including semantic integration of modalities. The development of increasingly expressive embodied agents and robots has become an active test-bed for coordinating multimodal dialogue input and output, including processing of language and nonverbal communication. In addition, major application areas are featured for commercializing multimodal-multisensor systems, including automotive, robotic, manufacturing, machine translation, banking, communications, and others. These systems rely heavily on software tools, data resources, and international standards to facilitate their development. For insights into the future, emerging multimodal-multisensor technology trends are highlighted for medicine, robotics, interaction with smart spaces, and similar topics. Finally, this volume discusses the societal impact of more widespread adoption of these systems, such as privacy risks and how to mitigate them. The handbook chapters provide a number of walk-through examples of system design and processing, information on practical resources for developing and evaluating new systems, and terminology and tutorial support for mastering this emerging field. In the final section of this volume, experts exchange views on a timely and controversial challenge topic, and how they believe multimodal-multisensor interfaces need to be equipped to most effectively advance human performance during the next decade.}
}

@inproceedings{10.1145/3625549.3658830,
author = {Huang, Shaohan and Luan, Zhongzhi},
title = {Semantic-Aware Log Understanding and Analysis},
year = {2024},
isbn = {9798400704130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625549.3658830},
doi = {10.1145/3625549.3658830},
abstract = {The exponential growth in system complexity and the corresponding surge in log data volume necessitate advanced log analysis techniques for efficient system management and anomaly detection. Traditional log understanding and analysis methods often fail to capture the rich semantic context inherent in log messages, leading to suboptimal monitoring and diagnostic capabilities. This paper aims to bridge the semantic gap by integrating cutting-edge semantic technologies into the log analysis pipeline. We leverage natural language processing, information retrieval, and large language models to enrich log data with semantic information, facilitating a deeper understanding of log messages. Our methodology enhances anomaly detection accuracy by utilizing hierarchical contextual information and pre-training technology, and refining log-based QA processes by log retrieval and log reader. Preliminary results demonstrate a significant improvement in identifying and diagnosing system anomalies, as well as in the automated answering log questions. This research not only presents a breakthrough in log data analysis but also sets the stage for future advancements in intelligent system monitoring and proactive fault resolution. Through this semantic-aware approach, we envision a new paradigm in log analysis that transcends traditional machine learning methods, offering a more robust and intuitive understanding of system behaviors and states.},
booktitle = {Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing},
pages = {413–416},
numpages = {4},
keywords = {semantic-aware analysis, log understanding, natural language processing, anomaly detection, log parsing},
location = {Pisa, Italy},
series = {HPDC '24}
}

@inproceedings{10.1145/3345252.3345288,
author = {Ivanova, Tatyana},
title = {Resources and Semantic-based knowledge models for personalized and self-regulated learning in the Web: survey and trends},
year = {2019},
isbn = {9781450371490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345252.3345288},
doi = {10.1145/3345252.3345288},
abstract = {Learning is a complex and multifaceted process. Research findings in recent years show that student's control over the learning process is important for achieving higher results. As every student or lifelong learner have his specific interests and needs, in many cases no one learning course can meet all these needs. It is important to ensure possibilities for learners to find additional knowledge sources or tools during his learning process. Cloud Learning consider the entire Web (including Social Web tools, Open Educational Resources and many other sources for learning) as a space for learning content. Finding exactly the needed resource for every learning need in this enormous space is a challenge. There is a need to explore all the resources useful for learning, classify them in a way that will support searching, and to express relations between them using semantic annotations.In this paper we analyze the current state of the resources, appropriate for learning in the internet from technological point of view. We classify resources according to several dimensions, important for searching and using by learners. We take special attention to the ways of semantic description of resources and users (used metadata and models) as metadata are the most important for finding the most appropriate resources for specific learning task. Tasks as searching and retrieval for learning are not new and our main aim in this work is to outline resent changes in web-based learning, resulting from technological changes and discuss how they will affect resource searching. We will discuss how cloud-based technologies and embedded semantic descriptions will simplify searching and finding appropriate learning sources.},
booktitle = {Proceedings of the 20th International Conference on Computer Systems and Technologies},
pages = {316–323},
numpages = {8},
keywords = {Cloud-based learning, E-Learning, Ontology, Web-based learning},
location = {Ruse, Bulgaria},
series = {CompSysTech '19}
}

@article{10.1145/3664615,
author = {Ji, Shaoxiong and Li, Xiaobo and Sun, Wei and Dong, Hang and Taalas, Ara and Zhang, Yijia and Wu, Honghan and Pitk\"{a}nen, Esa and Marttinen, Pekka},
title = {A Unified Review of Deep Learning for Automated Medical Coding},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3664615},
doi = {10.1145/3664615},
abstract = {Automated medical coding, an essential task for healthcare operation and delivery, makes unstructured data manageable by predicting medical codes from clinical documents. Recent advances in deep learning and natural language processing have been widely applied to this task. However, deep learning–based medical coding lacks a unified view of the design of neural network architectures. This review proposes a unified framework to provide a general understanding of the building blocks of medical coding models and summarizes recent advanced models under the proposed framework. Our unified framework decomposes medical coding into four main components, i.e., encoder modules for text feature extraction, mechanisms for building deep encoder architectures, decoder modules for transforming hidden representations into medical codes, and the usage of auxiliary information. Finally, we introduce the benchmarks and real-world usage and discuss key research challenges and future directions.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {306},
numpages = {41},
keywords = {Medical coding, deep learning, unified framework}
}

@inproceedings{10.5555/3466184.3466189,
author = {Wagner, Gerd},
title = {Business process modeling and simulation with DPMN: resource-constrained activities},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {This tutorial article, which is extracted from (Wagner 2019), shows how to use UML Class Diagrams and Discrete Event Process Modeling Notation (DPMN) Process Diagrams for making simulation models of business processes with resource-constrained activities based on the DES paradigm of Object Event Modeling and Simulation. In this approach, the state structure of a business system is captured by a UML Class Diagram, which defines the types of objects, events and activities underlying a DPMN Process Diagram, which captures the causal regularities of the system in the form of a set of event rules. DPMN Process Diagrams extend the Event Graphs proposed by Schruben (1983) by adding elements from the Business Process Modeling Notation (BPMN), viz. data objects and activities, and, as its main innovation over BPMN, resource-dependent activity start arrows.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {45–59},
numpages = {15},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3448823.3448854,
author = {Raich, Krispin and Kathrein, Robert and Erharter, Michael and D\"{o}ller, Mario},
title = {Spatial Extension Model for Multimodal Traffic Management},
year = {2021},
isbn = {9781450389532},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448823.3448854},
doi = {10.1145/3448823.3448854},
abstract = {The management of traffic information is an essential component of Intelligent Transport Systems (ITS). This traffic is no longer confined to roads but is increasingly emerging into the airspace. Recent developments in the field of unmanned aerial systems (UAS) enable new ways of transportation. Furthermore, the sensor systems of these traffic participants, such as cars, drones, and also intelligent infrastructure, are becoming increasingly prominent and powerful. Thus, future ITS will be confronted with the challenge of ever-increasing amounts of data from a wide variety of different traffic participants. Based on this, a novel concept is presented, how data from multi-modal traffic users can be accumulated. To accomplish this, a parametrized geographic data-centric model is presented that can map traffic routes of arbitrary shape in 2- and 3D environments. By this, traffic management of multimodal vehicles (cars, UAS, etc.) can be represented by one unique multimodal model. Furthermore, the parametrized data model allows situational data processing (e.g. congestion, weather condition, etc.) on a local and global basis. The geographic model extends Geo JSON as its foundation in order to rely on well-established standards.},
booktitle = {Proceedings of the 2020 4th International Conference on Vision, Image and Signal Processing},
articleno = {51},
numpages = {6},
keywords = {3D corridor, UAS paths, air traffic management, drone corridor, intelligent transportation systems},
location = {Bangkok, Thailand},
series = {ICVISP 2020}
}

@inproceedings{10.5555/3466184.3466471,
author = {Li, Yitong and Ji, Wenying and AbouRizk, Simaan M.},
title = {Automated abstraction of operation processes from unstructured text for simulation modeling},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Abstraction of operation processes is a fundamental step for simulation modeling. To reliably abstract an operation process, modelers rely on text information to study and understand details of operations. Aiming at reducing modelers' interpretation load and ensuring the reliability of the abstracted information, this research proposes a systematic methodology to automate the abstraction of operation processes. The methodology applies rule-based information extraction to automatically extract operation process-related information from unstructured text and creates graphical representations of operation processes using the extracted information. To demonstrate the applicability and feasibility of the proposed methodology, a text description of an earthmoving operation is used to create its corresponding graphical representation. Overall, this research enhances the state-of-the-art simulation modeling through achieving automated abstraction of operation processes, which largely reduces modelers' interpretation load and ensures the reliability of the abstracted operation processes.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2517–2525},
numpages = {9},
location = {Orlando, Florida},
series = {WSC '20}
}

@inproceedings{10.1145/3323503.3360644,
author = {Bertalan, Vithor Gomes and Ruiz, Evandro Eduardo Seron},
title = {Using topic modeling to find main discussion topics in brazilian political websites},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360644},
doi = {10.1145/3323503.3360644},
abstract = {Knowing the main discussion topics debated by the general public is a valuable asset to politicians and professionals involved with politics. Lately, alternative media websites became popular venues in which political ideas are debated without the influence of mainstream media. In this article, we propose the construction of a topic modeling framework, using LSI, LDA, and HDP, to identify main discussion issues in political websites. Experiments show that these models presented results similar to state of the art, offering a viable solution to track political discourse in left-wing and right-wing websites.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {245–248},
numpages = {4},
keywords = {hierarchial dirichlet process, latent dirichlet allocation, latent semantic indexing, political texts, topic coherence, topic modeling},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.5555/3466184.3466452,
author = {Wilsdorf, Pia and Haack, Fiete and Uhrmacher, Adelinde M.},
title = {Conceptual models in simulation studies: making it explicit},
year = {2021},
isbn = {9781728194998},
publisher = {IEEE Press},
abstract = {Conceptual models play an important role in conducting simulation studies. A formal or at least explicit specification of conceptual models is key for effectively exploiting them during simulation studies and thereafter, for interpreting and reusing the simulation results. However, the perception of conceptual models varies strongly and with it possible means for specification. A broad definition of the conceptual model, i.e., as a loose collection of early-stage products of the simulation study, holds the potential to unify existing definitions, but also poses specific challenges for specification. To approach these challenges, without claiming to be exhaustive, we identify a set of products, which includes research question, data, and requirements, and define relations and properties of these products. Based on a cell biological case study and a prototypical implementation, we show how the formal structuring of the conceptual model assists in building a simulation model.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2353–2364},
numpages = {12},
location = {Orlando, Florida},
series = {WSC '20}
}

@article{10.1145/3715114,
author = {Chen, Xiaohong and Chen, Shi and Jin, Zhi and Bian, Han and Chen, Zihan and Li, Haotian},
title = {Expressing the Needs in Smart Home: What Is the End Users’ Favorite Way},
year = {2025},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/3715114},
doi = {10.1145/3715114},
abstract = {The Internet of Things (IoT) has witnessed remarkable advancements, enabling smart homes with user-centric features. To effectively articulate their personalized needs, it becomes crucial to equip end users with programming capabilities. Currently, the executable Trigger-Action Programming (TAP) rules have become the mainstream paradigm for IoT end-user programming. To simplify the creation of TAP rules, many studies have proposed various levels of requirements abstraction, yet the connections between them remain unclear. In this article, we employ a mixed-methods study to identify the preferred way of expressing end users’ requirements in practical scenarios. Subsequently, from the perspective of requirements engineering, we categorize the needs of smart home into three hierarchical levels of abstraction. Accordingly, we propose an innovative multi-level requirements description language called SH-RDL. We also address potential challenges and conduct an evaluation to validate SH-RDL’s usability, understandability and error-prevention. This will aid in the broader adoption of IoT end-user programming.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = apr,
articleno = {16},
numpages = {38},
keywords = {IoT End-User Programming, Requirements Engineering, Smart Homes, User Intentions, Requirements Description Language}
}

@inproceedings{10.1145/3184558.3191538,
author = {Baader, Franz and Borgwardt, Stefan and Forkel, Walter},
title = {Patient Selection for Clinical Trials Using Temporalized Ontology-Mediated Query Answering},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191538},
doi = {10.1145/3184558.3191538},
abstract = {Finding suitable candidates for clinical trials is a labor-intensive task that requires expert medical knowledge. Our goal is to design (semi-)automated techniques that can support clinical researchers in this task. We investigate the issues involved in designing formal query languages for selecting patients that are eligible for a given clinical trial, leveraging existing ontology-based query answering techniques. In particular, we propose to use a temporal extension of existing approaches for accessing data through ontologies written in Description Logics. We sketch how such a query answering system could work and show that eligibility criteria and patient data can be adequately modeled in our formalism.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1069–1074},
numpages = {6},
keywords = {clinical trials, patient cohort recruitment, temporal description logic},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3388176.3388209,
author = {Barth, Linard and Ehrat, Matthias and Fuchs, Rainer and Haarmann, Jens},
title = {Systematization of Digital Twins: Ontology and Conceptual Framework},
year = {2020},
isbn = {9781450377256},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388176.3388209},
doi = {10.1145/3388176.3388209},
abstract = {The development and progress in information and communication technologies will transform traditional products into smart products and allow to offer novel smart services [1]. Herein, the digital twin (DT) concept is regarded as a key technology to create value with smart services [2]. Although the research and applications of DTs emerge continuously many concerns are to be scrutinized [3]. The lack of a shared conceptual framework for DTs with an unambiguous terminology [4] complicates cross-functional discussions. Therefore, a systematization of the main dimensions of DTs is proposed in the form of an ontology and a conceptual framework thereof derived. The research questions addressed in this paper are a) «Which dimensions are used to classify and structure DTs in academic literature?», b) «What are the fundamental differences or specifications within these dimensions?» and c) «How do these different specifications relate to each other?» The focus of the research is on the objective to find classification systematics that are a) representing the entire spectrum of DTs, b) universally valid in all DT related domains and c) applicable in research and practice. A systematic literature review on the relevant aspects of DTs was conducted and the findings iteratively advanced within workshop sessions with academic experts. DTs are considered as integrators of physical and digital worlds as well as internal and external value creation. Further, the creation of DTs requires per definition the use of digital data. Hence, the proposed ontology and conceptual framework for DTs include the following main dimensions to consider for every DT: Data resources, external value creation and internal value creation. The main subdimensions of the data resources are the data sources to obtain the data, the data categories and the data formats. The main subdimension of the external value creation are the attributes of the services as the basis of the value propositions, the level of smartness of the connected products and the actors on the different levels of the ecosystem. The main subdimensions of the internal value creation are the lifecycle phases of products, the product management levels and the different generations of both. The proposed ontology and conceptual framework support researchers and practitioners in positioning and structuring their intended DT activities and communicating them to internal and external stakeholders. The holistic view on the data resource dimension further allows to easily deduct the needed data for certain applications or deduct possible applications from already available data.},
booktitle = {Proceedings of the 3rd International Conference on Information Science and Systems},
pages = {13–23},
numpages = {11},
keywords = {Digital Twin, conceptual framework, ontology, systematization},
location = {Cambridge, United Kingdom},
series = {ICISS '20}
}

@inproceedings{10.1145/3209219.3209259,
author = {Martin, St\'{e}phane and Faltings, Boi and Schickel, Vincent},
title = {Enhancing Session-Based Recommendations through Sequential Modeling},
year = {2018},
isbn = {9781450355896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209219.3209259},
doi = {10.1145/3209219.3209259},
abstract = {Recommender systems typically determine the items they should recommend by learning models of user-preferences. Most often, those preferences are modeled as static and independent of context. In real life however, users consider items in sequence: TV series are watched episode by episode and accessories are chosen after the main appliance. Unfortunately, since sequences are more complex to model, they are often not taken into account. We developed an efficient sequence-modeling approach based on Bayesian Variable-order Markov Models and combined it with an existing content-based system, the Ontology Filtering. We tested this approach through live evaluations on two e-commerce sites. It dramatically increased performance, more than doubling the CTR and strongly increasing recommendation-mediated sales. These tests also confirm that the technique works efficiently and reliably in a production setting.},
booktitle = {Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization},
pages = {359–360},
numpages = {2},
keywords = {context-tree, e-commerce, recommender systems, sequence-modeling, variable-order Markov model},
location = {Singapore, Singapore},
series = {UMAP '18}
}

@inbook{10.1145/3728725.3728757,
author = {Xia, Weiyi and Zhang, Yinggang and Zhao, Ben and Liu, Wei and Han, Linjie and Ye, Qifu},
title = {Intelligent PLC Code Generation in HCPS 2.0: A Multi-dimensional Taxonomy and Evolutionary Framework},
year = {2025},
isbn = {9798400713453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3728725.3728757},
abstract = {The manufacturing industry is accelerating towards the New-Generation Intelligent Manufacturing, where the automatic generation of Programmable Logic Controllers (PLCs) code serves as a key technology, crucial for enhancing the intelligence and efficiency of manufacturing systems. Traditional manual programming has become increasingly inadequate to meet the demands for flexibility and real-time performance in intelligent manufacturing. This paper reviews the integration of intelligent manufacturing and PLC technologies, highlighting the core role of Human-Cyber-Physical Systems (HCPS) and demonstrating how it effectively guides theoretical research and engineering practices in new-generation intelligent manufacturing. We innovatively classify automatic PLC code generation methods through multiple dimensions and propose a new framework for Structured Text (ST) code generation. Finally, this paper discusses future trends in controller code generation, identifying multimodal-driven intelligent generation, adaptive learning and real-time optimization, and cloud-edge collaborative intelligent generation as significant directions, forming a technical roadmap of "parallel promotion and integrated development" to support comprehensive intelligent transformation and upgrading of manufacturing.},
booktitle = {Proceedings of the 2025 2nd International Conference on Generative Artificial Intelligence and Information Security},
pages = {202–212},
numpages = {11}
}

@article{10.1145/3450969,
author = {Mousavi, Zahra and Faili, Heshaam},
title = {Developing the Persian Wordnet of Verbs Using Supervised Learning},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3450969},
doi = {10.1145/3450969},
abstract = {Nowadays, wordnets are extensively used as a major resource in natural language processing and information retrieval tasks. Therefore, the accuracy of wordnets has a direct influence on the performance of the involved applications. This paper presents a fully-automated method for extending a previously developed Persian wordnet to cover more comprehensive and accurate verbal entries. At first, by using a bilingual dictionary, some Persian verbs are linked to Princeton WordNet synsets. A feature set related to the semantic behavior of compound verbs as the majority of Persian verbs is proposed. This feature set is employed in a supervised classification system to select the proper links for inclusion in the wordnet. We also benefit from a pre-existing Persian wordnet, FarsNet, and a similarity-based method to produce a training set. This is the largest automatically developed Persian wordnet with more than 27,000 words, 28,000 PWN synsets and 67,000 word-sense pairs that substantially outperforms the previous Persian wordnet with about 16,000 words, 22,000 PWN synsets and 38,000 word-sense pairs.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {68},
numpages = {18},
keywords = {verbs, Persian language, wordnet, Ontology}
}

@inproceedings{10.1145/3646547.3689015,
author = {Huang, Ziyuan and Tang, Jiaming and Karir, Manish and Liu, Mingyan and Sarabi, Armin},
title = {Analyzing Corporate Privacy Policies using AI Chatbots},
year = {2024},
isbn = {9798400705922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646547.3689015},
doi = {10.1145/3646547.3689015},
abstract = {In this paper, we present and evaluate an automated pipeline for the large-scale analysis of corporate privacy policies. Organizations usually develop their privacy policies in isolation to best balance their business needs, user rights, as well as regulatory requirements. A wide-ranging and structured analysis of corporate privacy policies is essential to facilitate a deeper understanding of how organizations have balanced competing requirements. Our approach consists of a web crawler that can navigate to and scrape content from web pages that contain privacy policies, and a set of AI chatbot task prompts to process and extract structured/labeled annotations from the raw data. The analysis includes the types of collected user data, the purposes for which data is collected and processed, data retention and protection practices, and user rights and choices. Our validation shows that our annotations are highly accurate and consistent. We use this architecture to gather data on the privacy policies of companies in the Russell 3000 index, resulting in hundreds of thousands of annotations across all categories. Analysis of the resulting data allows us to obtain unique insights into the state of the privacy policy ecosystem as a whole.},
booktitle = {Proceedings of the 2024 ACM on Internet Measurement Conference},
pages = {505–515},
numpages = {11},
keywords = {ai chatbots, large language models, privacy policies, text annotation, web crawling},
location = {Madrid, Spain},
series = {IMC '24}
}

@inproceedings{10.1145/3372020.3391559,
author = {Foster, Simon and Nemouchi, Yakoub and O'Halloran, Colin and Stephenson, Karen and Tudor, Nick},
title = {Formal Model-Based Assurance Cases in Isabelle/SACM: An Autonomous Underwater Vehicle Case Study},
year = {2020},
isbn = {9781450370714},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372020.3391559},
doi = {10.1145/3372020.3391559},
abstract = {Isabelle/SACM is a tool for automated construction of model-based assurance cases with integrated formal methods, based on the Isabelle proof assistant. Assurance cases show how a system is safe to operate, through a human comprehensible argument demonstrating that the requirements are satisfied, using evidence of various provenances. They are usually required for certification of critical systems, often with evidence that originates from formal methods. Automating assurance cases increases rigour, and helps with maintenance and evolution. In this paper we apply Isabelle/SACM to a fragment of the assurance case for an autonomous underwater vehicle demonstrator. We encode the metric unit system (SI) in Isabelle, to allow modelling requirements and state spaces using physical units. We develop a behavioural model in the graphical RoboChart state machine language, embed the artifacts into Isabelle/SACM, and use it to demonstrate satisfaction of the requirements.},
booktitle = {Proceedings of the 8th International Conference on Formal Methods in Software Engineering},
pages = {11–21},
numpages = {11},
keywords = {Assurance Cases, Autonomous Systems, Isabelle/HOL, Verification},
location = {Seoul, Republic of Korea},
series = {FormaliSE '20}
}

@inproceedings{10.1145/3576842.3589161,
author = {Zhang, Ruipeng and Xie, Mengjun},
title = {A Knowledge Graph Question Answering Approach to IoT Forensics},
year = {2023},
isbn = {9798400700378},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3576842.3589161},
doi = {10.1145/3576842.3589161},
abstract = {Internet of Things (IoT) forensics has been a particularly challenging task for forensic practitioners due to the heterogeneity of IoT environments as well as the complexity and volume of IoT data. With the advent of artificial intelligence, question-answering (QA) systems have emerged as a potential solution for users to access sophisticated forensic knowledge and data. In this light, we present a novel IoT forensics framework that employs knowledge graph question answering (KGQA). Our framework enables investigators to access forensic artifacts and cybersecurity knowledge using natural language questions facilitated by a deep-learning-powered KGQA model. The proposed framework demonstrates high efficacy in answering natural language questions over the experimental IoT forensic knowledge graph.},
booktitle = {Proceedings of the 8th ACM/IEEE Conference on Internet of Things Design and Implementation},
pages = {446–447},
numpages = {2},
keywords = {Digital Forensics, Internet of Things, Knowledge Graph, Ontology Design, Question Answering},
location = {San Antonio, TX, USA},
series = {IoTDI '23}
}

@inbook{10.1145/3382097.3382112,
title = {Ontologies on the Web—putting it all together},
year = {2020},
isbn = {9781450376174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382097.3382112},
abstract = {Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.},
booktitle = {Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL}
}

@inproceedings{10.1145/3444370.3444600,
author = {Cai, Yinyin and Gu, Zhaoquan and Wang, Le and Li, Shudong and Han, Weihong},
title = {An APT Group Knowledge Model based on MDATA},
year = {2021},
isbn = {9781450387828},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3444370.3444600},
doi = {10.1145/3444370.3444600},
abstract = {Situational awareness is significant for cyber security, which can help researchers and security analysts obtain the network security situation comprehensively and accurately. Advanced Persistent Threat (APT) attack could cause severe consequences to cyberspace and detecting such attacks have become a very important part of cyber security situational awareness. Some APT attacks may belong to a same group, many countries and organizations have established databases for APT groups, such as adopting knowledge graph (KG) to represent the knowledge. However, cyberspace security knowledge varies by temporal and spatial characteristics, such as the attack technologies are updated very frequently, traditional KG cannot represent such knowledge timely. To address this problem, the MDATA (Multi-dimensional Data Association and inTelligent Analysis) model is proposed in [1], which is a supplement and improvement to traditional KG. In this paper, we introduce an APT group knowledge model based on MDATA, which adds spatial-temporal characteristics of the APT groups. We also analyze how this knowledge model could help address the challenges of APT attack awareness.},
booktitle = {Proceedings of the 2020 International Conference on Cyberspace Innovation of Advanced Technologies},
pages = {374–378},
numpages = {5},
keywords = {spatial-temporal characteristics, knowledge model, cyber situational awareness, MDATA, APT attack},
location = {Guangzhou, China},
series = {CIAT 2020}
}

@article{10.1145/3704723,
author = {Buneman, Peter and Dosso, Dennis and Lissandrini, Matteo and Silvello, Gianmaria and Sun, He},
title = {Can We Measure the Impact of a Database?},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {68},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3704723},
doi = {10.1145/3704723},
abstract = {If we want to measure the impact of a database, can we use its organization to treat it the same way we treat any other publishing agent, such as a journal or an author?},
journal = {Commun. ACM},
month = apr,
pages = {69–76},
numpages = {8},
keywords = {Data Citation, h-index, Scientific and Curated Databases}
}

@inproceedings{10.1145/3320435.3320472,
author = {Guchev, Vladimir and Cena, Federica and Vernero, Fabiana and Gena, Cristina},
title = {Visual Annotations for Hybrid Graph-based User Model},
year = {2019},
isbn = {9781450360210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3320435.3320472},
doi = {10.1145/3320435.3320472},
abstract = {Structured user model data not only allow system personalization, but also may be of interest as a source for analysis: in particular, for the study of general trends and for the detection of anomalies in preferences and mutually-referenced features among different user models. Such sources are multidimensional and interrelated, and recently started to be represented as graph-based datasets. Among the most effective ways of studying such data is visual exploration based on data-driven graph drawing approaches: in particular, node-link and node-link-group diagrams. The paper provides an overview of advanced approaches to the graphical representation of multidimensional data derived from user modeling and presents a proposal for developing flexible and scalable user interfaces for the hypergraph-based visual exploration of relations within a user model (UM). Then, we propose these principles in the visualization of an existing adaptive system.},
booktitle = {Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {31–35},
numpages = {5},
keywords = {graph and hypergraph drawing, user model visualization},
location = {Larnaca, Cyprus},
series = {UMAP '19}
}

@inproceedings{10.1145/3703790.3703811,
author = {Gui, Zhou and Freund, Michael and Harth, Andreas},
title = {A Context-aware Conversational Interface for Controlling Internet-of-Things Devices},
year = {2025},
isbn = {9798400712852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3703790.3703811},
doi = {10.1145/3703790.3703811},
abstract = {This paper presents a demonstration of a context-aware conversational interface for interacting with multiple IoT devices using multi-turn natural language commands. Our system facilitates device discovery and identification through a Knowledge Graph (KG) and addresses the interoperability issue by using the Web of Things (WoT) specifications as an intermediate abstraction layer. The proposed system comprises four major components: a context-aware multi-turn dialogue interface, a device discovery and identification module using a domain-specific KG, a text-to-code module to parse natural language commands into an executable code format, and a customized code execution engine. We demonstrate our system using two Philips Hue smart lamps and one Elgato Stream Deck controller. The Philips Hue smart lamp device can be controlled standalone via natural language, allowing users to adjust the power state, the color, and the brightness. Also, the lamp can be integrated with the Elgato Stream Deck Controller to enable user-defined automation rules following the Trigger Action programming (TAP) paradigm.},
booktitle = {Proceedings of the 14th International Conference on the Internet of Things},
pages = {160–163},
numpages = {4},
keywords = {Conversational Interface, Internet of Things, Trigger Action Programming, Natural Language Understanding, Knowledge Graph},
location = {
},
series = {IoT '24}
}

@article{10.1109/TCBB.2020.3044230,
author = {Hakala, Kai and Kaewphan, Suwisa and Bj\"{o}rne, Jari and Mehryary, Farrokh and Moen, Hans and Tolvanen, Martti and Salakoski, Tapio and Ginter, Filip},
title = {Neural Network and Random Forest Models in Protein Function Prediction},
year = {2020},
issue_date = {May-June 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.3044230},
doi = {10.1109/TCBB.2020.3044230},
abstract = {Over the past decade, the demand for automated protein function prediction has increased due to the volume of newly sequenced proteins. In this paper, we address the function prediction task by developing an ensemble system automatically assigning Gene Ontology (GO) terms to the given input protein sequence. We develop an ensemble system which combines the GO predictions made by random forest (RF) and neural network (NN) classifiers. Both RF and NN models rely on features derived from BLAST sequence alignments, taxonomy and protein signature analysis tools. In addition, we report on experiments with a NN model that directly analyzes the amino acid sequence as its sole input, using a convolutional layer. The Swiss-Prot database is used as the training and evaluation data. In the CAFA3 evaluation, which relies on experimental verification of the functional predictions, our submitted ensemble model demonstrates competitive performance ranking among top-10 best-performing systems out of over 100 submitted systems. In this paper, we evaluate and further improve the CAFA3-submitted system. Our machine learning models together with the data pre-processing and feature generation tools are publicly available as an open source software at &lt;uri&gt;https://github.com/TurkuNLP/CAFA3&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = dec,
pages = {1772–1781},
numpages = {10}
}

@article{10.1145/3643132,
author = {Jain, Minni and Jindal, Rajni and Jain, Amita},
title = {Automatic Construction of Interval-Valued Fuzzy Hindi WordNet using Lexico-Syntactic Patterns and Word Embeddings},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3643132},
doi = {10.1145/3643132},
abstract = {A computational lexicon is the backbone of any language processing system. It helps computers to understand the language complexity as a human does by inculcating words and their semantic associations. Manually constructed famous Hindi WordNet (HWN) consists of various classical semantic relations (crisp relations). To handle uncertainty and represent Hindi WordNet more semantically, Type- 1 fuzzy graphs are applied to relations of Hindi WordNet. But uncertainty in the crisp membership degree is not considered in Type 1 fuzzy set (T1FS). Also collecting billions (5,55,69,51,753 relations in HWN) of membership values from experts (humans) is not feasible. This paper applied the concept of Interval-Valued Fuzzy graphs and proposed Interval- Valued Fuzzy Hindi WordNet (IVFHWN). IVFHWN automatically identifies Interval- Valued Fuzzy relations between words and their degree of membership using word embeddings and lexico-syntactic patterns. The experimental results for the word sense disambiguation problem show better outcomes when IVFHWN is being used in place of Type 1 Fuzzy Hindi WordNet and classical Hindi WordNet.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = feb,
keywords = {Hindi WordNet, Interval- Valued Fuzzy Graphs, Computational Lexicon, Natural Language Processing, Low-Resource Language}
}

@article{10.1145/3229094,
author = {Magnaudet, Mathieu and Chatty, St\'{e}phane and Conversy, St\'{e}phane and Leriche, S\'{e}bastien and Picard, Celia and Prun, Daniel},
title = {Djnn/Smala: A Conceptual Framework and a Language for Interaction-Oriented Programming},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {EICS},
url = {https://doi.org/10.1145/3229094},
doi = {10.1145/3229094},
abstract = {The persistent difficulty to develop and maintain interactive software has unveiled the inadequacy of traditional imperative programming languages. In the recent years, several solutions have been proposed to enrich the existing languages with constructs dedicated to interaction. In this paper, we propose a different approach that takes interaction as the primary concern to build a new programming language. We present Djnn, a conceptual framework based on the concepts of process and process activation, then we introduce Smala a programming language derived from this framework. We propose a solution for the unification of the concepts of event and data-flow, and for the derivation of complex control structures from a small set of basic ones. We detail the syntax and the semantics of Smala. Finally, we illustrate through a real-size application how it enables building all parts of an interactive software. Djnn and Smala may offer designers and programmers usable means to think of interactions and translate them into running code.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {12},
numpages = {27},
keywords = {smala, reactive programming, interactive software, gui programming, djnn}
}

@inproceedings{10.1145/3701716.3715454,
author = {Yang, Yitian and Chen, Huaming},
title = {Construction of Domain-Specific Knowledge Graph for Advanced Persistent Threat Behaviour Analysis and Detection},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715454},
doi = {10.1145/3701716.3715454},
abstract = {Advanced Persistent Threat (APT) represents a sophisticated and targeted attack campaign, often orchestrated by well-resourced organizations. Understanding APT is crucial for adapting to their evolving tactics and effectively mitigating their infiltration methods. A key approach to accurately analysing and detecting APTs involves studying the behaviour, identifying their attack stage and uncovering the employed components. Existing works for APT detection heavily rely on network traffic analysis, limiting their practical applicability in real-world scenarios. This paper introduces a novel method to analyse and detect APT behaviours by constructing a domain-specific knowledge graph (APT-KG), utilising the MITRE ATT&amp;CK framework as its foundation. A hierarchical clustering-based model is proposed to uncover the correlations among various network attack techniques. It first vectorizes the attack techniques according to ATT&amp;CK standards, filtering out low-frequency techniques to optimise dimensionality reduction. Parent-child relationships within the ATT&amp;CK classification further facilitate this process. We observe that this strategy can reveal fully connected relationships among high-frequency techniques, while connections are preserved within clusters between high and low-frequency techniques. This structured representation enables the inference of potential attack patterns, achieving a prediction accuracy of 88.11\%. We then integrate the identified associations with APT-KG. Experimental validation demonstrates that APT-KG significantly enhances understanding of attack interrelations and improves the efficiency of APT detection and response mechanisms.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1480–1484},
numpages = {5},
keywords = {advanced persistent threat (apt), data mining, knowledge graph, network security, ontology construction},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3545862.3545890,
author = {Camelo, Diogo and Ascens\~{a}o, Jo\~{a}o and Alves, Rui and Matos, Paulo},
title = {Mech Desk: An ontology based system to help drivers diagnosis vehicle problems},
year = {2022},
isbn = {9781450396400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3545862.3545890},
doi = {10.1145/3545862.3545890},
abstract = {Semantic Web is a vision about an extension of the existing World Wide Web, which provides tools and technologies to support the transparent exchange of information and knowledge among organizations. As one of the building blocks of Semantic Technology, ontologies are part of the W3C standards stack for the Semantic Web. Nowadays, multiple areas can be aborded by ontologies and the semantic web world, as the subject of this project, mechanics. Mechanics have been accentuated in a visible way, where the reality of living without means of transportation is not feasible in people's lives. The development of new methods to increase the knowledge of drivers and everyday people about automated vehicles is essential. Regarding cars, revisions, maintenance, inspections, change of parts, among others, are necessary and "mandatory" subjects and due to this, it is possible to prevent future damage by prolonging the life of the car. In certain cases, this doesn't happen, either due to wear of parts or unforeseen events, and despite being a busy market, drivers are not always informed about the best cares to take or the problems that may arise. As such, the theme of this project is to make a relationship between mechanical details, issues, and solutions, throughout an ontology, to help an everyday driver to a better perception of what he encounters at hand. For that purpose, the defined ontology was exposed via a mobile application, with it providing to the user, several details that he can or not relate, and trough them, provide a connection with a certain problem and solution. The semantic web ontology was developed in Prot\'{e}g\'{e}, exposed into Apache Jena Fuseki server, and was running in an Azure Virtual Machine, allowing it to be available into the OutSystems application.},
booktitle = {Proceedings of the 8th International Conference on Frontiers of Educational Technologies},
pages = {169–175},
numpages = {7},
keywords = {‘Mobile Development', ‘Ontology’, 'Semantic Web’, 'Vehicles Maintenance'},
location = {Yokohama, Japan},
series = {ICFET '22}
}

@inproceedings{10.1145/3239372.3239397,
author = {Ballar\'{\i}n, Manuel and Marc\'{e}n, Ana C. and Pelechano, Vicente and Cetina, Carlos},
title = {Measures to report the Location Problem of Model Fragment Location},
year = {2018},
isbn = {9781450349499},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239372.3239397},
doi = {10.1145/3239372.3239397},
abstract = {Model Fragment Location (MFL) aims at identifying model elements that are relevant to a requirement, feature, or bug. Many MFL approaches have been introduced in the last few years to address the identification of the model elements that correspond to a specific functionality. However, there is a lack of detail when the measurements about the search space (models) and the measurements about the solution to be found (model fragment) are reported. Generally, the only reported measure is the model size. In this paper, we propose using five measurements (size, volume, density, multiplicity, and dispersion) to report the location problems. These measurements are the result of analyzing 1,308 MFLs in a family of industrial models over the last four years. Using two MFL approaches, we emphasize the importance of these measurements in order to compare results. Our work not only proposes improving the reporting of the location problem, but it also provides real measurements of location problems that are useful to other researchers in the design of synthetic location problems.},
booktitle = {Proceedings of the 21th ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {189–199},
numpages = {11},
keywords = {Bug Location, Feature Location, Model Fragment Location, Traceability Link Recovery},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@inproceedings{10.1145/3270112.3270121,
author = {Ciccozzi, Federico and Famelis, Michalis and Kappel, Gerti and Lambers, Leen and Mosser, Sebastien and Paige, Richard F. and Pierantonio, Alfonso and Rensink, Arend and Salay, Rick and Taentzer, Gabi and Vallecillo, Antonio and Wimmer, Manuel},
title = {Towards a body of knowledge for model-based software engineering},
year = {2018},
isbn = {9781450359658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3270112.3270121},
doi = {10.1145/3270112.3270121},
abstract = {Model-based Software Engineering (MBSE) is now accepted as a Software Engineering (SE) discipline and is being taught as part of more general SE curricula. However, an agreed core of concepts, mechanisms and practices --- which constitutes the Body of Knowledge of a discipline --- has not been captured anywhere, and is only partially covered by the SE Body of Knowledge (SWEBOK). With the goals of characterizing the contents of the MBSE discipline, promoting a consistent view of it worldwide, clarifying its scope with regard to other SE disciplines, and defining a foundation for a curriculum development on MBSE, this paper provides a proposal for an extension of the contents of SWEBOK with the set of fundamental concepts, terms and mechanisms that should constitute the MBSE Body of Knowledge.},
booktitle = {Proceedings of the 21st ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {82–89},
numpages = {8},
keywords = {body of knowledge, model-based software engineering},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@article{10.1145/3626568,
author = {Haffar, Nafaa and Zrigui, Mounir},
title = {A Synergistic Bidirectional LSTM and N-gram Multi-channel CNN Approach Based on BERT and FastText for Arabic Event Identification},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {11},
issn = {2375-4699},
url = {https://doi.org/10.1145/3626568},
doi = {10.1145/3626568},
abstract = {Event extraction from texts continues to pose a challenge for many NLP systems. This article presents a novel neural network architecture that can extract and classify events from Arabic sentences. The model combines word representations and Part-Of-Speech (POS) tags and uses a bidirectional LSTM layer and a dual combined convolutional neural network. The first layer of the network focuses on sentence representations, while the second layer focuses on POS representations. The model takes advantage of both N-gram character features from FastText and contextual representations from bidirectional encoder representations from transformers. This combination proves to be successful, as evidenced by the good results obtained from evaluating the model on the Arabic TimeML corpus. Our results show that combining both contextual and N-gram representations outperforms the traditional skip-gram model.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {246},
numpages = {27},
keywords = {BERT representation, FastText representation, events identification, Arabic language, deep learning, TimeML standard}
}

@inproceedings{10.1145/3382025.3414973,
author = {Schlie, Alexander and Kn\"{u}ppel, Alexander and Seidl, Christoph and Schaefer, Ina},
title = {Incremental feature model synthesis for clone-and-own software systems in MATLAB/Simulink},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414973},
doi = {10.1145/3382025.3414973},
abstract = {Families of related MATLAB/Simulink systems commonly emerge ad hoc using clone-and-own practices. Extractively migrating systems towards a software product line (SPL) can be a remedy. A feature model (FM) represents all potential configurations of an SPL, ideally, in non-technical domain terms. However, yielding a sensible FM from automated synthesis remains a major challenge due to domain knowledge being a prerequisite for features to be adequate abstractions. In incremental reverse engineering, subsequent generation of FMs may further overwrite changes and design decisions made during previous manual FM refinement.In this paper, we propose an approach to largely automate the synthesis of a suitable FM from a set of cloned MATLAB/Simulink models as part of reverse engineering an SPL. We fully automate the extraction of an initial, i.e., a technical, FM that closely aligns with realization artifacts and their variability, and further provide operations to manually refine it to incorporate domain knowledge. Most importantly, we provide concepts to capture such operations and to replay them on a structurally different technical FM stemming from a subsequent reverse engineering increment that included further systems of the portfolio. We further provide an implementation and demonstrate the feasibility of our approach using two MATLAB/Simulink data sets from the automotive domain.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {7},
numpages = {12},
keywords = {variability, synthesis, refinement, mapping, individual, incremental, feature model, clone-and-own, MATLAB/Simulink, 150\% model},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3382026.3431246,
author = {Kenner, Andy},
title = {Model-Based Evaluation of Vulnerabilities in Software Systems},
year = {2020},
isbn = {9781450375702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382026.3431246},
doi = {10.1145/3382026.3431246},
abstract = {Vulnerabilities in software systems result from faults, which occur at different stages in a software's life cycle, for example, in the design (i.e., undesired feature-interactions), the development (i.e., buffer overflows), or the operation (i.e., configuration errors). Various databases provide detailed information about vulnerabilities in software systems or the way to exploit it, but face severe limitations. The information is scattered across these databases, fluctuates in quality and granularity, and provides only an insight into a single vulnerability per entry. Even for a single software system it is challenging for any security-related stakeholder to determine the threat level, which consists of all vulnerabilities of the software system and its environment (i.e., operating system). Manual vulnerability management is feasible only to a limited extend if we want to identify all configurations that are affected by vulnerabilities, or determine a system's threat level and the resulting risk we have to deal with. For variant-rich systems, we also have to deal with variability, allowing different stakeholders to understand the threats to their particular setup. To deal with this variability, we propose vulnerability feature models, which offer a homogeneous view on all vulnerabilities of a software system. These models and the resulting analyses offer advantages in many disciplines of the vulnerability management process. In this paper, we report the research plan for our project, in which we focus on the model-based evaluation of vulnerabilities. This includes research objectives that take into account the design of vulnerability feature models, their application in the process of vulnerability management, and the impact of evolution, discovery, and verification of vulnerabilities.},
booktitle = {Proceedings of the 24th ACM International Systems and Software Product Line Conference - Volume B},
pages = {112–119},
numpages = {8},
keywords = {Exploit, Feature Model, Variability Model, Vulnerability, Vulnerability Analysis and Management},
location = {Montreal, QC, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3733567.3735567,
author = {H\"{u}s\"{u}nbeyi, Z. Melce and Seddah, Djam\'{e} and Scheffler, Tatjana},
title = {Integrating Semantic Representations in a Cross-Modal Approach to Fact-Checking},
year = {2025},
isbn = {9798400718915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3733567.3735567},
doi = {10.1145/3733567.3735567},
abstract = {We propose a cross-modal approach with deep fusion of a language model and graph structures based on Abstract Meaning Representations (AMRs) enriched with Wikidata to address the fact-checking problem. We collect and make available a large dataset of fact-checked claim sentences, and systematically compare a transformer-based model with Graph Neural Networks (GNNs) based on AMR graphs and extended with external information. Furthermore, we evaluate the integration of language models and GNNs for the fact verification task. While GNN models on AMR-based graphs alone yield lower scores than transformer based language models on their own, the combined cross-modal approach—leveraging a multilayer and deep interaction between textual and structural information—demonstrates the best performance. Finally, we evaluate the generalization capability of this cross-modal approach integrating AMR-based graph structures on out-of-domain English and German claims.},
booktitle = {Proceedings of the 4th ACM International Workshop on Multimedia AI against Disinformation},
pages = {17–27},
numpages = {11},
keywords = {fact-checking, AMR graphs, external knowledge, cross-modal, deep and interactive fusion of LMs and GNNs},
location = {
},
series = {MAD' 25}
}

@inproceedings{10.1145/3411564.3411629,
author = {Villa\c{c}a, Lu\'{\i}s Henrique Neves and Azevedo, Leonardo Guerreiro and Siqueira, Sean Wolfgand Matsui},
title = {Microservice Architecture for Multistore Database Using Canonical Data Model},
year = {2020},
isbn = {9781450388733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411564.3411629},
doi = {10.1145/3411564.3411629},
abstract = {In a microservice architecture, solutions are created by teams focused on specific domains and needs. They independently develop and deploy distributed services in the network. One characteristic of microservices is decentralized data management. Each microservice may use different data management technology which best fit its needs. Hence, it is an issue to integrate data of heterogenous microservices to come up with consolidate data views. Flexible and efficient solutions in this scenario are needed. This work is based on the use of a canonical data model as the mechanism for data integration in microservices. The canonical data model is the reference for query specifications and data integration. This work proposes and implements a microservice architecture based on this strategy and it is composed by nodes that intercommunicate through several mechanisms (e.g., SPARQL, GraphQL and JDBC queries, calls to REST services and proprietary APIs). The solution was analyzed in a proof of concept in a fictitious scenario but using real services available at DBPedia and Twitter. The evaluation goal was to qualitatively analyze the use of the architecture in the design, development and execution of microservices in order to identify the characteristics one should consider when using the canonical data model strategy. The evaluation employed the criteria of ISO/IEC 25010 model that most relate to the SOA challenges, which were: usability; performance; compatibility; and, maintainability. The identified advantages and disadvantages of using the architecture (i.e., the strategy) can be used by architects and developers to make their development decisions.},
booktitle = {Proceedings of the XVI Brazilian Symposium on Information Systems},
articleno = {20},
numpages = {8},
keywords = {canonical data model, microservice architecture, multistore, polyglot persistence, service-oriented architecture},
location = {S\~{a}o Bernardo do Campo, Brazil},
series = {SBSI '20}
}

@inproceedings{10.1145/3308558.3313439,
author = {Wang, Chengyu and Fan, Yan and He, Xiaofeng and Zhou, Aoying},
title = {A Family of Fuzzy Orthogonal Projection Models for Monolingual and Cross-lingual Hypernymy Prediction},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313439},
doi = {10.1145/3308558.3313439},
abstract = {Hypernymy is a semantic relation, expressing the “is-a” relation between a concept and its instances. Such relations are building blocks for large-scale taxonomies, ontologies and knowledge graphs. Recently, much progress has been made for hypernymy prediction in English using textual patterns and/or distributional representations. However, applying such techniques to other languages is challenging due to the high language dependency of these methods and the lack of large training datasets of lower-resourced languages. In this work, we present a family of fuzzy orthogonal projection models for both monolingual and cross-lingual hypernymy prediction. For the monolingual task, we propose a Multi-Wahba Projection (MWP) model to distinguish hypernymy vs. non-hypernymy relations based on word embeddings. This model establishes distributional fuzzy mappings from embeddings of a term to those of its hypernyms and non-hypernyms, which consider the complicated linguistic regularities of these relations. For cross-lingual hypernymy prediction, a Transfer MWP (TMWP) model is proposed to transfer the semantic knowledge from the source language to target languages based on neural word translation. Additionally, an Iterative Transfer MWP (ITMWP) model is built upon TMWP, which augments the training sets of target languages when target languages are lower-resourced with limited training data. Experiments show i) MWP outperforms previous methods over two hypernymy prediction tasks for English; and ii) TMWP and ITMWP are effective to predict hypernymy over seven non-English languages.},
booktitle = {The World Wide Web Conference},
pages = {1965–1976},
numpages = {12},
keywords = {Multi-Wahba Projection, cross-lingual transfer learning, hypernymy prediction},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@article{10.1145/3419972,
author = {Liu, Bulou and Li, Chenliang and Zhou, Wei and Ji, Feng and Duan, Yu and Chen, Haiqing},
title = {An Attention-based Deep Relevance Model for Few-shot Document Filtering},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3419972},
doi = {10.1145/3419972},
abstract = {With the large quantity of textual information produced on the Internet, a critical necessity is to filter out the irrelevant information and organize the rest into categories of interest (e.g., an emerging event). However, supervised-learning document filtering methods heavily rely on a large number of labeled documents for model training. Manually identifying plenty of positive examples for each category is expensive and time-consuming. Also, it is unrealistic to cover all the categories from an evolving text source that covers diverse kinds of events, user opinions, and daily life activities. In this article, we propose a novel attention-based deep relevance model for few-shot document filtering (named ADRM), inspired by the relevance feedback methodology proposed for ad hoc retrieval. ADRM calculates the relevance score between a document and a category by taking a set of seed words and a few seed documents relevant to the category. It constructs the category-specific conceptual representation of the document based on the corresponding seed words and seed documents. Specifically, to filter irrelevant yet noisy information in the seed documents, ADRM employs two types of attention mechanisms (namely whole-match attention and max-match attention) and generates category-specific representations for them. Then ADRM is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process, a self-attention layer, and a relevance aggregation layer. Extensive experiments on three real-world datasets show that ADRM consistently outperforms the existing technical alternatives, including the conventional classification and retrieval baselines, and the state-of-the-art deep relevance ranking models for few-shot document filtering. We also perform an ablation study to demonstrate that each component in ADRM is effective for enhancing filtering performance. Further analysis shows that ADRM is robust under varying parameter settings.},
journal = {ACM Trans. Inf. Syst.},
month = oct,
articleno = {6},
numpages = {35},
keywords = {Few-shot learning, deep learning, document filtering}
}

@article{10.1145/3447879.3447882,
author = {Vedula, Nikhita},
title = {Modeling knowledge and functional intent for context-aware pragmatic analysis},
year = {2021},
issue_date = {Winter 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2021},
number = {Winter},
issn = {1931-1745},
url = {https://doi.org/10.1145/3447879.3447882},
doi = {10.1145/3447879.3447882},
abstract = {Nikhita Vedula is an Applied Scientist at Amazon Alexa Science. She obtained her PhD in Computer Science and Engineering from the Ohio State University in August 2020, advised by Professor Srinivasan Parthasarathy. She received her bachelor's degree from the National Institute of Technology, Nagpur, India in 2015. Her research interests are at the intersection of data mining, natural language processing and social computing. Over the course of her PhD, her research involved designing efficient and novel machine learning and computational linguistic techniques that extract, interpret and transform the vast, unstructured digital content into structured knowledge representations in diverse contexts. She has worked with researchers from interdisciplinary fields such as emergency response, marketing, sociology and psychology. She performed research internships at Nokia Bell Laboratories, Adobe Research and Amazon Alexa AI. Her work has been published at several top data mining conferences such as the Web Conference, SIGIR, WSDM and ICDM. Her work on detecting user intentions from their natural language interactions won the Best paper award at the Web Conference 2020. She was a recipient of a Graduate Research Award (2020), a Presidential Fellowship (2019) and a University Graduate Fellowship (2015) at the Ohio State University. She was also selected as a Rising Star in EECS (2019).},
journal = {SIGWEB Newsl.},
month = feb,
articleno = {3},
numpages = {4}
}

@inproceedings{10.1109/MODELS-C.2019.00017,
author = {Partridge, Chris and Mitchell, Andrew and Loneragan, Michael and Atkinson, Hayden and de Cesare, Sergio and Khan, Mesbah},
title = {Coordinate systems: level ascending ontological options},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00017},
doi = {10.1109/MODELS-C.2019.00017},
abstract = {A major challenge faced in the deployment of collaborating unmanned vehicles is enabling the semantic interoperability of sensor data. One aspect of this, where there is significant opportunity for improvement, is characterizing the coordinate systems for sensed position data. We are involved in a proof of concept project that addresses this challenge through a foundational conceptual model using a constructional approach based upon the BORO Foundational Ontology. The model reveals the characteristics as sets of options for configuring the coordinate systems. This paper examines how these options involve, ontologically, ascending levels. It identifies two types of levels, the well-known type levels and the less well-known tuple/relation levels.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {78–87},
numpages = {10},
keywords = {BORO foundational ontology, constructional ontology, geometric coordinate system ontology, multi-level options, multi-platform-domain sensor system, power-tuple-builder, power-type-builder},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1145/3482632.3482749,
author = {Cui, Gaili},
title = {Design of Intelligent Recognition English Translation Model Based on Feature Extraction Algorithm},
year = {2021},
isbn = {9781450390255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482632.3482749},
doi = {10.1145/3482632.3482749},
abstract = {In recent years, with the deepening of globalization, international cooperation is becoming more and more extensive, and the importance of English is increasing. Aiming at the problems that the semantic context of English is not obvious in the process of English translation in traditional English translation system, the optimal translation solution is not reached in the process of selecting the optimal feature semantics, and the translation accuracy is low, an intelligent recognition English translation model based on feature extraction algorithm is designed. Search module is used to complete the search of basic meaning and subject content of vocabulary to be proofread, grasp the user's behavior data through behavior log and optimize the system; In the method based on the maximum entropy principle, the whole task of clause recognition is divided into three parts: sentence head recognition, sentence tail recognition and complete clause recognition. Experimental results show that the proposed algorithm has higher recognition rate.},
booktitle = {2021 4th International Conference on Information Systems and Computer Aided Education},
pages = {553–557},
numpages = {5},
location = {Dalian, China},
series = {ICISCAE 2021}
}

@inproceedings{10.1145/3459637.3482440,
author = {Sheng, Qiang and Zhang, Xueyao and Cao, Juan and Zhong, Lei},
title = {Integrating Pattern- and Fact-based Fake News Detection via Model Preference Learning},
year = {2021},
isbn = {9781450384469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459637.3482440},
doi = {10.1145/3459637.3482440},
abstract = {To defend against fake news, researchers have developed various methods based on texts. These methods can be grouped as 1) pattern-based methods, which focus on shared patterns among fake news posts rather than the claim itself; and 2) fact-based methods, which retrieve from external sources to verify the claim's veracity without considering patterns. The two groups of methods, which have different preferences of textual clues, actually play complementary roles in detecting fake news. However, few works consider their integration. In this paper, we study the problem of integrating pattern- and fact-based models into one framework via modeling their preference differences, i.e., making the pattern- and fact-based models focus on respective preferred parts in a post and mitigate interference from non-preferred parts as possible. To this end, we build a Preference-aware Fake News Detection Framework (Pref-FEND), which learns the respective preferences of pattern- and fact-based models for joint detection. We first design a heterogeneous dynamic graph convolutional network to generate the respective preference maps, and then use these maps to guide the joint learning of pattern- and fact-based models for final prediction. Experiments on two real-world datasets show that Pref-FEND effectively captures model preferences and improves the performance of models based on patterns, facts, or both.},
booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
pages = {1640–1650},
numpages = {11},
keywords = {fact-checking, fake news detection, graph neural networks, pattern mining, preference learning},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{10.1145/3744740,
author = {Trinh, Tam and Dao, Anh and Hy, Thi Hong Nhung and Hy, Truong Son},
title = {VietMedKG: Knowledge Graph and Benchmark for Traditional Vietnamese Medicine},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3744740},
doi = {10.1145/3744740},
abstract = {Traditional Vietnamese Medicine (TVM) and Traditional Chinese Medicine (TCM) have shared significant similarities due to their geographical location, cultural exchanges, and hot and humid climatic conditions. However, unlike TCM, which has substantial works published to construct a knowledge graph, there is a notable absence of a comprehensive knowledge graph for TVM. This article presents the first endeavor to build a knowledge graph for TVM based on extensive existing resources from TCM. We name our knowledge graph as VietMedKG. We propose a translation and filtration process to adapt TCM knowledge graphs to TVM, identifying the overlapping and unique elements of TVM. In addition, the constructed knowledge graph is then exploited further for developing a curated benchmark for the knowledge graph-based question-answering problem with the potential to support doctors and patients in assisting doctors and patients in identifying various diseases. Our work will not only bridge the gap between TCM and TVM but also set the foundation for future research into TVM community. Our source code is publicly available at .},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {69},
numpages = {17},
keywords = {Knowledge graph, traditional vietnamese mecidine, graph-based question answering, retrieval augmented generation}
}

@inproceedings{10.1145/3698587.3701527,
author = {Agapito, Giuseppe and Cannataro, Mario and Lloyd, Wes J. and Zucco, Chiara},
title = {13th Workshop on Parallel and AI-based Bioinformatics and Biomedicine (ParBio): Editorial},
year = {2024},
isbn = {9798400713026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698587.3701527},
doi = {10.1145/3698587.3701527},
abstract = {The goal of ParBio is to bring together scientists in high-performance computing, computational biology, and medicine to discuss parallel implementation of bioinformatics and biomedical applications and the challenges and opportunities of moving these applications to the cloud or edge. The workshop will also address Artificial Intelligence (AI), Large Language Models (LLMs), machine learning, and big data analytics in healthcare and bioinformatics, focusing on the integrated analysis of molecular and clinical data. This is motivated by the increasing production of experimental and clinical data and the shift towards data storage, integration, and analysis.},
booktitle = {Proceedings of the 15th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {95},
numpages = {1},
keywords = {Bioinformatics, Machine Learning, Parallel algorithms},
location = {Shenzhen, China},
series = {BCB '24}
}

@inproceedings{10.1145/3470481.3472704,
author = {Aryan, Peb Ruswono and Ekaputra, Fajar Juang and Sabou, Marta and Hauer, Daniel and Mosshammer, Ralf and Einfalt, Alfred and Miksa, Tomasz and Rauber, Andreas},
title = {Explainable cyber-physical energy systems based on knowledge graph},
year = {2021},
isbn = {9781450386081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470481.3472704},
doi = {10.1145/3470481.3472704},
abstract = {Explainability can help cyber-physical systems alleviating risk in automating decisions that are affecting our life. Building an explainable cyber-physical system requires deriving explanations from system events and causality between the system elements. Cyber-physical energy systems such as smart grids involve cyber and physical aspects of energy systems and other elements, namely social and economic. Moreover, a smart-grid scale can range from a small village to a large region across countries. Therefore, integrating these varieties of data and knowledge is a fundamental challenge to build an explainable cyber-physical energy system. This paper aims to use knowledge graph based framework to solve this challenge. The framework consists of an ontology to model and link data from various sources and graph-based algorithm to derive explanations from the events. A simulated demand response scenario covering the above aspects further demonstrates the applicability of this framework.},
booktitle = {Proceedings of the 9th Workshop on Modeling and Simulation of Cyber-Physical Energy Systems},
articleno = {4},
numpages = {6},
keywords = {explainability, knowledge graphs, ontologies, smart grid simulation, smart grids},
location = {Virtual Event},
series = {MSCPES '21}
}

@inproceedings{10.1145/3500931.3500960,
author = {Shengxin, Hu and Qing, Wang and Lu, Chen and Xingxin, Zhang and Leiqing, Huang and Tianyu, He and Songhe, Li and Xiangmin, Dong and Bingxiang, Yang},
title = {The establishment and evaluation of the automatic crisis balance analysis model for social network users based on artificial intelligence technology},
year = {2021},
isbn = {9781450395588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3500931.3500960},
doi = {10.1145/3500931.3500960},
abstract = {Online social media provides people with a platform to express their emotions anonymously. Social media has been identified as an important data source for suicide prevention related to emotional problems in China. Almost Three million messages were published by 450,000 users in a particular Chinese social media data base. This study aims to develop a Crisis Balance Analysis Model based on concepts of "balancing factors" as described by Aguilera. Through interactions with psychological experts, deep learning architecture that was built and refined. Three annotation levels free annotations (zero cost), easy annotations (by psychology students), and hard annotations (by psychology experts) were used. Our Model was evaluated accordingly and showed that its performance at each level was promising. Finally, suicide risks, cognitive distortions and interpersonal problems could be identified for messages from social media users using this model, which providing basis for proactive crisis intervention.},
booktitle = {Proceedings of the 2nd International Symposium on Artificial Intelligence for Medicine Sciences},
pages = {157–161},
numpages = {5},
keywords = {Artificial intelligent, Crisis, Depression, Suicide prevention},
location = {Beijing, China},
series = {ISAIMS '21}
}

@inproceedings{10.1145/3323878.3325807,
author = {Holubov\'{a}, Irena and Scherzinger, Stefanie},
title = {Unlocking the potential of nextGen multi-model databases for semantic big data projects},
year = {2019},
isbn = {9781450367660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323878.3325807},
doi = {10.1145/3323878.3325807},
abstract = {A new vision in semantic big data processing is to create enterprise data hubs, with a 360° view on all data that matters to a corporation. As we discuss in this paper, a new generation of multi-model database systems seems a promising architectural choice for building such scalable, non-native triple stores. In this paper, we first characterize this new generation of multi-model databases. Then, discussing an example scenario, we show how they allow for agile and flexible schema management, spanning a large design space for creative and incremental data modelling. We identify the challenge of generating sound triple-views from data stored in several, interlinked models, for SPARQL querying. We regard this as one of several appealing research challenges where the semantic big data and the database architecture community may join forces.},
booktitle = {Proceedings of the International Workshop on Semantic Big Data},
articleno = {6},
numpages = {6},
keywords = {multi-model DBMS, schema evolution, semantic data management},
location = {Amsterdam, Netherlands},
series = {SBD '19}
}

@article{10.1145/3612921,
author = {Bensalem, Raja and Haddar, Kais and Blache, Philippe},
title = {An Arabic Probabilistic Parser Based on a Property Grammar},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {10},
issn = {2375-4699},
url = {https://doi.org/10.1145/3612921},
doi = {10.1145/3612921},
abstract = {The specificities of Arabic parsing, such as agglutination, vocalization, and the relatively order-free words in Arabic sentences, remain major issues to consider. To promote its robustness, such parseing should define different types of constraints. Property Grammar (PG) formalism verifies the satisfiability of the constraints directly on the units of the structure, thanks to its properties (or relations). In this context, we propose to build a probabilistic parser with syntactic properties, using a PG, and we measure the production rules in terms of different implicit information and in particular the syntactic properties. We experimented with our parser on the treebank ATB, using the parsing algorithm CYK, and we obtained encouraging results. Our method is also automatic for implementation of most property types. Its generalization for other languages or corpus domains (using treebanks) could be a good perspective. Its combination with pre-trained models of BERT may also make our parser faster.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {237},
numpages = {25},
keywords = {Probabilistic parser, property grammar formalism, Arabic language, lexicalized grammar}
}

@inproceedings{10.1145/3383652.3423907,
author = {Ishii, Ryo and Ren, Xutong and Muszynski, Michal and Morency, Louis-Philippe},
title = {Can Prediction of Turn-management Willingness Improve Turn-changing Modeling?},
year = {2020},
isbn = {9781450375863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383652.3423907},
doi = {10.1145/3383652.3423907},
abstract = {For smooth conversation, participants must carefully monitor the turn-management (a.k.a. speaking and listening) willingness of other conversational partners and adjust turn-changing behaviors accordingly. Many studies have focused on predicting the actual moments of speaker changes (a.k.a. turn-changing), but to the best of our knowledge, none of them explicitly modeled the turn-management willingness from both speakers and listeners in dyad interactions. We address the problem of building models for predicting this willingness of both. Our models are based on trimodal inputs, including acoustic, linguistic, and visual cues from conversations. We also study the impact of modeling willingness to help improve the task of turn-changing prediction. We introduce a dyadic conversation corpus with annotated scores of speaker/listener turn-management willingness. Our results show that using all of three modalities of speaker and listener is important for predicting turn-management willingness. Furthermore, explicitly adding willingness as a prediction task improves the performance of turn-changing prediction. Also, turn-management willingness prediction becomes more accurate with this multi-task learning approach.},
booktitle = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},
articleno = {28},
numpages = {8},
keywords = {multimodal signal processing, multitask learning, turn-changing prediction, turn-management willingness},
location = {Virtual Event, Scotland, UK},
series = {IVA '20}
}

@inproceedings{10.1145/3429889.3429904,
author = {Liang, Changwei and Pan, Xiaosheng and Kong, Jiangping},
title = {A Speech-Driven 3-D Lip Synthesis with Realistic Dynamics in Mandarin Chinese},
year = {2020},
isbn = {9781450388603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429889.3429904},
doi = {10.1145/3429889.3429904},
abstract = {In this paper, a new speech-driven lip synchronization method is developed, predicting the 3-D geometric shape of the lip without using speech recognition model in the visualization procedure, and can be trained and evaluated with realistic dynamics. Videos of Mandarin Chinese words are used. Speech signals are calculated into MFCC as audio features. 68-points facial landmarks are annotated from the corresponding videos through the prediction algorithm from the Dlib Library. Eos, a 3-D Morphable Face Model, is applied, using the facial landmarks, to predict the 3-D shape, where we can acquire 3-D landmarks. A machine-learning sequence-tagging model, averaged Structured Perceptron using Viterbi algorithm, is applied for modelling the direct prediction of labial parameters from the acoustic MFCC parameters. The 3-D labial area shape from the 'eos' prediction of a frame is morphed according to the predicted 3-D labial landmarks, forming the 3-D lip sequence, which can be plotted synchronically with the acoustic signal. In this 3-D lip synthesis, acoustic features and realistic lip shapes are directly mapped, where lip units and speech recognition are not applied, preserving more realistic articulatory or personality details; and the predicted geometric shapes are comparable with realistic dynamics, with the comparison indicating that this synthesis is of good effect.},
booktitle = {Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences},
pages = {79–84},
numpages = {6},
keywords = {3-D lip synthesis, Mandarin Chinese, realistic dynamics, speech-driven},
location = {Beijing, China},
series = {ISAIMS '20}
}

@article{10.1145/3369780,
author = {Razis, Gerasimos and Anagnostopoulos, Ioannis and Zeadally, Sherali},
title = {Modeling Influence with Semantics in Social Networks: A Survey},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3369780},
doi = {10.1145/3369780},
abstract = {The discovery of influential entities in all kinds of networks (e.g., social, digital, or computer) has always been an important field of study. In recent years, Online Social Networks (OSNs) have been established as a basic means of communication and often influencers and opinion makers promote politics, events, brands, or products through viral content. In this work, we present a systematic review across (i) online social influence metrics, properties, and applications and (ii) the role of semantic in modeling OSNs information. We found that both areas can jointly provide useful insights towards the qualitative assessment of viral user-generated content, as well as for modeling the dynamic properties of influential content and its flow dynamics.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {7},
numpages = {38},
keywords = {Information quality, online social influence, social networks, social semantics}
}

@inproceedings{10.1145/3440094.3440389,
author = {Kabanda, Gabriel},
title = {A bayesian network model for machine learning and cyber security},
year = {2021},
isbn = {9781450387675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3440094.3440389},
doi = {10.1145/3440094.3440389},
abstract = {The phenomenal growth in the use of internet-based technologies has resulted in complexities in cyber security subjecting organizations to cyber-attacks. This research is purposed to develop a cyber-security system that uses the Bayesian Network structure and Machine Learning. The research determined the cyber-security framework appropriate for a developing nation; evaluated network detection and prevention systems that use Artificial Intelligence paradigms such as finite automata, neural networks, genetic algorithms, fuzzy logic, support vector machines, or diverse data-mining-based approaches; analyzed Bayesian Networks that can be represented as graphical models and are directional to represent cause-effect relationships; and developed a Bayesian Network model that can handle complexity in cybersecurity. The Pragmatism paradigm used in this research, as a philosophy is intricately related to the mixed-method approach, which is largely quantitative with the research design being a survey and an experiment, but supported by qualitative approaches where Focus Group discussions were held. The Artificial Intelligence paradigms evaluated include machine learning methods, autonomous robotic vehicles, artificial neural networks, and fuzzy logic. Alternative improved solutions discussed include the use of machine learning algorithms specifically Artificial Neural Networks (ANN), Decision Tree C4.5, Random Forests, and Support Vector Machines (SVM).},
booktitle = {Proceedings of the 2nd Africa-Asia Dialogue Network (AADN) International Conference on Advances in Business Management and Electronic Commerce Research},
articleno = {9},
numpages = {7},
keywords = {Bayesian network model, artificial intelligence (AI), artificial neural networks (ANN) and decision tree, cybersecurity, machine learning (ML)},
location = {Ganzhou, China},
series = {AADNIC-ABMECR '20}
}

@inproceedings{10.1145/3508397.3564825,
author = {Laamech, Nouha and Munier, Manuel and Pham, Congduc},
title = {IdSM-O: An IoT Data Sharing Management Ontology for Data Governance},
year = {2022},
isbn = {9781450392198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508397.3564825},
doi = {10.1145/3508397.3564825},
abstract = {The main purpose of IoT is to deliver reliable, high quality services and innovative solutions by transforming the captured data into meaningful information, and thus improving user's daily life. In this regard, it is in the interest of the community to encourage entities within IoT environments to share their data, and therefore serve public interest and contribute to the innovation and technological progress. Meanwhile, the distributed nature of IoT networks and the diversity of its actors lead to the recognition of security and data sharing management as one of the major challenges of the IoT domain. For instance, due to insufficient governance of the shared data within IoT environments, data provider retains little to no control over his assets once he has agreed to share them. Furthermore, data consumers are not able to trace the source of the available resource nor its history processing to assess its quality. All this creates a digital environment that is certainly functional but lacks mutual trust between its actors, which can prevent the domain's full potential to be exploited, and therefore disrupt the implemented services. In our work, we propose an approach to improve data sharing management using three main elements: semantic modeling, usage control policies, and data provenance.},
booktitle = {Proceedings of the 14th International Conference on Management of Digital EcoSystems},
pages = {88–95},
numpages = {8},
keywords = {data governance, data sharing management, semantic modeling, usage control},
location = {Venice, Italy},
series = {MEDES '22}
}

@inproceedings{10.1145/3449726.3459579,
author = {Kostovska, Ana and Vermetten, Diederick and Doerr, Carola and D\v{z}eroski, Sa\v{s}o and Panov, Pan\v{c}e and Eftimov, Tome},
title = {OPTION: optimization algorithm benchmarking ontology},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3459579},
doi = {10.1145/3449726.3459579},
abstract = {Many platforms for benchmarking optimization algorithms offer users the possibility of sharing their experimental data with the purpose of promoting reproducible and reusable research. However, different platforms use different data models and formats, which drastically inhibits identification of relevant data sets, their interpretation, and their interoperability. Consequently, a semantically rich, ontology-based, machine-readable data model is highly desired.We report in this paper on the development of such an ontology, which we name OPTION (OPTImization algorithm benchmarking ONtology). Our ontology provides the vocabulary needed for semantic annotation of the core entities involved in the benchmarking process, such as algorithms, problems, and evaluation measures. It also provides means for automated data integration, improved interoperability, powerful querying capabilities and reasoning, thereby enriching the value of the benchmark data. We demonstrate the utility of OPTION by annotating and querying a corpus of benchmark performance data from the BBOB workshop data - a use case which can be easily extended to cover other benchmarking data collections.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {239–240},
numpages = {2},
location = {Lille, France},
series = {GECCO '21}
}

@article{10.1145/3711939,
author = {Pennanen, Niki and Linkola, Simo and Kantosalo, Anna and Hiillos, Nicolas and M\"{a}nnist\"{o}, Tomi and Guckelsberger, Christian},
title = {From Product to Producer: The Impact of Perceptual Evidence and Robot Embodiment on the Human Assessment of AI Creativity},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
url = {https://doi.org/10.1145/3711939},
doi = {10.1145/3711939},
abstract = {While creative artificial intelligence (AI) is becoming integral to our lives, we know little about what makes us call AI “creative”. Informed by prior theoretical and empirical work, we investigate how perceiving evidence of a creative act beyond the final product affects our assessment of robot creativity. We study embodiment morphology as a potential moderator of this relationship, informing a 3 \texttimes{} 2 factorial design. In two lab experiments on visual art, participants (N = 30 + 60) assessed drawings produced by two physical robots with different morphologies, under exposure to product, process and producer as three levels of perceptual evidence. The data supports that the human assessment of robot creativity is significantly higher the more is revealed beyond the product about the creation process, and eventually the producer. We find no significant effects of embodiment morphology, contrasting existing hypotheses and offering a more detailed understanding for future work. The latter is also informed by additional exploratory analyses revealing factors potentially influencing creativity assessments, including perceived robot likeability and participants’ experience with robotics and AI. Our insights empirically ground existing design patterns, foster fairness and validity in system comparisons, and contribute to a deeper understanding of our relationship with creative AI and thus its adoption in society.},
journal = {J. Hum.-Robot Interact.},
month = apr,
articleno = {41},
numpages = {41},
keywords = {Human-Robot Interaction, Computational Creativity, Creative AI, Human Perception, Embodiment}
}

@inproceedings{10.1145/3584684.3597263,
author = {Ilani, Arnon and Dolev, Shlomi},
title = {Invited Paper: Common Public Knowledge for Enhancing Machine Learning Data Sets},
year = {2023},
isbn = {9798400701283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3584684.3597263},
doi = {10.1145/3584684.3597263},
abstract = {In this study, we show the advantages of incorporating multi-source knowledge from publicly available sources, such as ChatGPT and Wikipedia, into existing datasets to enhance the performance of machine learning models for routine tasks, such as classification. specifically, we propose the utilization of supplementary data from external sources and demonstrate the utility of widely accessible knowledge in the context of the Forest Cover Type Prediction task launched by the Roosevelt National Forest of Northern Colorado. Additionally, we exhibit an improvement in classification accuracy for the Isolated Letter Speech Recognition dataset when incorporating information on regional accents in the prediction of spoken English letter names.},
booktitle = {Proceedings of the 5th Workshop on Advanced Tools, Programming Languages, and PLatforms for Implementing and Evaluating Algorithms for Distributed Systems},
articleno = {2},
numpages = {10},
keywords = {ontology, machine learning, random forests, feature engineering, world knowledge, speech recognition, isolated letter, forest management, tree cover type, ChatGPT},
location = {Orlando, FL, USA},
series = {ApPLIED 2023}
}

@inproceedings{10.1145/3330204.3330233,
author = {Barroso, Jos\'{e} S. and Pimentel, Mariano and Nunes, Vanessa and Cappelli, Claudia},
title = {Design Science Research to design a conceptual model about prosopographic information related to politicians},
year = {2019},
isbn = {9781450372374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330204.3330233},
doi = {10.1145/3330204.3330233},
abstract = {The growing demand for information about politicians and the speed with which news is propagated by media and social networks reveals in contemporary times a more participatory view of the citizen in politics and social control in government actions. Speculation about information about politicians and their respective parties have become more constant, such as in electoral periods, investigations on processes, journalistic interest, etc. This can be observed during the electoral period of 2018 in which the search for information about the candidates and their respective parties was notorious. Given that information transparency is paramount for a democratic regime, the organization and consolidation of data from official sources is a reliable tool for consultation and dissemination of knowledge. According to this period, this work sought to understand some cognitive models that explain electoral behavior, which led to the investigation of factors that influence the decision to vote. The observed cognitive models indicate some attitudes, opinions, satisfactions, events and government assessments that interfere to some degree in the choice of vote, approval or disapproval for some purpose. Among the factors of influence observed, there are prosopographic information, which deals with the biography of politicians, and which are available in the various transparency portals. However, the data contained in these platforms are not organized in such a way as to make the information more intelligible and more reliable to the citizen. In this sense, this research is interested in the prosopographic information content, which would be better evaluated if the data of the politicians were organized according to a conceptual model of knowledge. The methodological approach adopted in this research is the Design Science Reserch (DSR), which directs the construction of an artifact in a given context, whose theoretical conjectures are based on the search and production of knowledge. As a result, research contributes to the knowledge base, as it discusses the evolution of cognitive models and through the design of the artifact, delivers reliable and relevant information about the life of a politician.},
booktitle = {Proceedings of the XV Brazilian Symposium on Information Systems},
articleno = {24},
numpages = {8},
keywords = {Conceptual Modeling, Data Transparency, Organized Information, Prosopography Politicial},
location = {Aracaju, Brazil},
series = {SBSI '19}
}

@inproceedings{10.1145/3701716.3715310,
author = {Zhou, Yan and Zhou, Baifan and Li, Huajian and Lyu, Qianhang and Qu, Yuanwei and Waaler, Arild and Yu, Ingrid C.},
title = {Dataset for Industrial Question Answering with Explanation and Scalable Ensemble Generation},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715310},
doi = {10.1145/3701716.3715310},
abstract = {The digital and green transition under Industry 4.0 has accelerated the adoption of AI in industries such as manufacturing, energy, and mining. Question Answering with Explanation (QAE), as a way of human interaction with AI, is crucial for enhancing transparency and trust in high-stakes industrial applications. However, industrial QAE remains underexplored due to the lack of publicly available, high-quality datasets, hindered by the need for expert effort and corporate restrictions. To this end, we introduce PANDAX ( https://doi.org/10.5281/zenodo.14510798 ), the first open-source industrial QAE dataset, and SEG, a scalable method for generating high-quality QAE datasets using LLMs. PANDAX focuses on three key topics of industrial system information: partonomy, functionality, and parameters, across critical domains such as green technology and cooling systems. SEG ensures scalability and quality through ensemble generation, majority voting, expert ranking, etc. The human evaluation validates PANDAX's high quality, positioning it as a valuable resource for advancing QAE techniques, benchmarking language technologies, and supporting research in explainable AI for industrial systems.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {825–828},
numpages = {4},
keywords = {dataset generation, industrial dataset resource, question answering with explanation, system information},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3383583.3398539,
author = {Esteva, Maria and Xu, Weijia and Simone, Nevan and Gupta, Amit and Jah, Moriba},
title = {Modeling Data Curation to Scientific Inquiry: A Case Study for Multimodal Data Integration},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398539},
doi = {10.1145/3383583.3398539},
abstract = {Scientific data publications may include interactive data applications designed by scientists to explore a scientific problem. Defined as knowledge systems, their development is complex when data are aggregated from multiple sources over time. Multimodal data are created, encoded, and maintained differently, and even when reporting about identical phenomena, fields and their values may be inconsistent across datasets. To assure the validity and accuracy of the application, the data has to abide by curation requirements similar to those ruling digital libraries. We present a novel, inquiry-driven curation approach aimed to optimize multimodal datasets curation and maximize data reuse by domain researchers. We demonstrate the method through the ASTRIAGraph project, in which multiple data sources about near earth space objects are aggregated into a central knowledge system. The process involves multidisciplinary collaboration, resulting in the design of a data model as the backbone for both data curation and scientific inquiry. We demonstrate a) how data provenance information is needed to assess the uncertainty of the results of scientific inquiries involving multiple data sources, and b) that continuous curation of integrated datasets is facilitated when undertaken as integral to the research project. The approach provides flexibility to support expansion of scientific inquiries and data in the knowledge system, and allows for transparent and explainable results.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {235–242},
numpages = {8},
keywords = {big data, data curation, data integration, data model, graph database, knowledge system, space traffic management},
location = {Virtual Event, China},
series = {JCDL '20}
}

@article{10.1145/3394979,
author = {Rocha Silva, Thiago and Winckler, Marco and Tr\ae{}tteberg, Hallvard},
title = {Ensuring the Consistency between User Requirements and Task Models: A Behavior-Based Automated Approach},
year = {2020},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {EICS},
url = {https://doi.org/10.1145/3394979},
doi = {10.1145/3394979},
abstract = {Evaluating and ensuring the consistency between user requirements and modeling artifacts is a long-time issue for model-based software design. Conflicts in requirements specifications can lead to many design errors and have a decisive impact on the quality of systems under development. This article presents an approach based on Behavior-Driven Development (BDD) to provide automated assessment for task models, which are intended to model the flow of user and system tasks in an interactive system. The approach has been evaluated by exploiting user requirements described by a group of experts in the domain of business trips. Such requirements gave rise to a set of BDD stories that have been used to automatically assess scenarios extracted from task models that were reengineered from an existing web system for booking business trips. The results have shown our approach, by performing a static analysis of the source files, was able to identify different types of inconsistencies between the user requirements and the set of task models analyzed.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {77},
numpages = {32},
keywords = {automated requirements assessment, behavior-driven development (BDD), task models, user stories}
}

@inproceedings{10.1145/3706598.3713280,
author = {Singh, Aneesha and Dechant, Martin Johannes and Patel, Dilisha and Soubutts, Ewan and Barbareschi, Giulia and Ayobi, Amid and Newhouse, Nikki},
title = {Exploring Positionality in HCI: Perspectives, Trends, and Challenges},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713280},
doi = {10.1145/3706598.3713280},
abstract = {Positionality acknowledges that researchers’ subjectivities, values and experiences influence approaches to and outcomes of research. It underlines and promotes self-awareness and explicit demonstration of reflexivity. To understand how positionality is conceptualised and used in HCI, we conducted two studies: (i) a scoping review of positionality and reflexivity statements in CHI papers from the last 11 years and (ii) a survey of HCI researchers (n=75). Our findings show that positionality statements are often viewed as a box-ticking exercise and their influence on the research is seldom discussed. They are also often restricted to more sensitive areas of research and may impact marginalised identities. We argue that positionality statements may be valuable but not as markers of methodological rigour; their content should be at the discretion of authors and methodologically consistent. Our contributions include a current snapshot of positionality in HCI and reflections on its current role and future directions in HCI.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {451},
numpages = {18},
keywords = {positionality, reflexivity, identity, methodology, methods},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3365438.3410987,
author = {Kesper, Arno and Wenz, Viola and Taentzer, Gabriele},
title = {Detecting quality problems in research data: a model-driven approach},
year = {2020},
isbn = {9781450370196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365438.3410987},
doi = {10.1145/3365438.3410987},
abstract = {As scientific progress highly depends on the quality of research data, there are strict requirements for data quality coming from the scientific community. A major challenge in data quality assurance is to localise quality problems that are inherent to data. Due to the dynamic digitalisation in specific scientific fields, especially the humanities, different database technologies and data formats may be used in rather short terms to gain experiences. We present a model-driven approach to analyse the quality of research data. It allows abstracting from the underlying database technology. Based on the observation that many quality problems show anti-patterns, a data engineer formulates analysis patterns that are generic concerning the database format and technology. A domain expert chooses a pattern that has been adapted to a specific database technology and concretises it for a domain-specific database format. The resulting concrete patterns are used by data analysts to locate quality problems in their databases. As proof of concept, we implemented tool support that realises this approach for XML databases. We evaluated our approach concerning expressiveness and performance in the domain of cultural heritage based on a qualitative study on quality problems occurring in cultural heritage data.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {354–364},
numpages = {11},
keywords = {data quality, model-driven development, pattern matching},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@proceedings{10.1145/3701716,
title = {WWW '25: Companion Proceedings of the ACM on Web Conference 2025},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to The ACM Web Conference 2025 (WWW'25), held from April 28 to May 2, 2025, at the Sydney Convention \&amp; Exhibition Centre in Australia. Recognizing the breadth of this year's program, we are publishing two sets of proceedings: one dedicated to research track papers, the Web4Good track, and keynotes; and a companion proceedings featuring the Demo Paper Track, Short Paper Track, PhD Symposium, Special Day, Resource Track, and History of Web sessions. Through this rich assortment of activities, we aim to foster a vibrant community where delegates can share ideas, forge collaborations, and cultivate a sustainable, inclusive environment.Marking its 34th edition, WWW'25 welcomes more than 1,000 industry and academic experts eager to shape the future of Web technologies and applications. The conference logo, depicting the Sydney Harbour Bridge, symbolizes the Web's key function of "connecting" people and information. Originally founded at CERN in 1994 as the International World Wide Web Conference (WWW), this event has long been the leading venue for research, development, standards, and applications related to the Web.Over the years, WWW has introduced important breakthroughs, from The Anatomy of a Large-Scale Web Search Engine in 1998-which heralded Google-to the EigenTrust algorithm in 2003 and the YAGO knowledge base in 2007. In the period between 2024 and 2025, large language models (LLMs) have significantly influenced countless industries and everyday life, prompting fundamental shifts in the Web ecosystem. We anticipate that the research and discussions at this year's conference will spark further breakthroughs in this rapidly evolving field.Continuing the tradition of depth and diversity, WWW'25 accepted 26 workshops and 20 tutorials as pre-conference events. In addition, consistent with previous editions, we feature a Demo Paper Track, Short Paper Track, PhD Symposium, Special Day, Resource Track, History of Web sessions, and Artifact Badging. Some of these are incorporated into the main conference schedule to encourage communication and collaboration among attendees.A commitment to diversity is pivotal to fostering robust, sustainable Web technologies. Our Web4Good track highlights how Web-based tools can address societal challenges, while the Industry Track showcases novel and impactful results from the industrial sector. This year, we have also launched an Emerging World Track to broaden participation from developing countries, as well as a competition track to enhance industry engagement..Organizing WWW'25 has been a true team effort. We sincerely thank the authors who contributed their work, and we extend our gratitude to the program and senior program committees for their dedication to reviewing submissions and offering feedback. We also appreciate the many additional chairs whose efforts ensured that each element of the program ran smoothly. Together, they have helped this conference continue to grow into a premier event for the Web research community.Finally, we would like to thank ACM SIGWEB and our industry sponsors-Meta, Huawei, Google, Baidu, Taobao, JD, and Infinigence-for their generous support, as well as our local government sponsors, Business Event Sydney and the NSW Government. We are equally grateful to our academic partner, the University of Technology Sydney, and the PCO company, ICMSA, for their indispensable help with registration, venue logistics, and social events. Their collective contributions have made the 2025 edition of this conference a resounding success.},
location = {Sydney NSW, Australia}
}

@inproceedings{10.1145/3603163.3609035,
author = {Renda, Giulia and Daquino, Marilena and Presutti, Valentina},
title = {Melody: A Platform for Linked Open Data Visualisation and Curated Storytelling},
year = {2023},
isbn = {9798400702327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603163.3609035},
doi = {10.1145/3603163.3609035},
abstract = {Data visualisation and storytelling techniques help experts highlight relations between data and share complex information with a broad audience. However, existing solutions targeted to Linked Open Data visualisation have several restrictions and lack the narrative element. In this article we present MELODY, a web interface for authoring data stories based on Linked Open Data. MELODY has been designed using a novel methodology that harmonises existing Ontology Design and User Experience methodologies (eXtreme Design and Design Thinking), and provides reusable User Interface components to create and publish web-ready article-alike documents based on data retrievable from any SPARQL endpoint. We evaluate the software by comparing it with existing solutions, and we show its potential impact in projects where data dissemination is crucial.},
booktitle = {Proceedings of the 34th ACM Conference on Hypertext and Social Media},
articleno = {27},
numpages = {8},
keywords = {Linked Open Data, data visualization, design thinking, ontology design, storytelling},
location = {Rome, Italy},
series = {HT '23}
}

@inproceedings{10.1145/3516875.3516922,
author = {Widianto, Idam Ragil Widianto and Ardiansyah, Roy and Saputri, Dwi Yuniasih},
title = {The Empowerment of Critical Thinking Skills through Problem-Based Learning Model Viewed From Epigenetic},
year = {2022},
isbn = {9781450386920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3516875.3516922},
doi = {10.1145/3516875.3516922},
abstract = {Critical thinking skills can face 21st-century challenges, which increasingly require technology and science in a global society in this world. Thus, education must be oriented towards mathematics and natural sciences accompanied by social and human sciences. Therefore, this study aims to describe the empowerment of critical thinking skills through a problem-based learning model viewed from the epigenetic aspect. This research employed library research. In this study, data collection was obtained from news, articles in journals, and relevant books. The analysis was carried out in four stages: 1) data collection, 2) data reduction, 3) data display, and 4) conclusion. This study's results revealed that learning needs to pay attention to nature and nurture because it dramatically affects the mastery of thinking skills, and bridging the two things in learning will be a challenge for science teachers in the future. The problem-based learning model can be utilized as a natural science learning model that can empower critical thinking skills from an epigenetic perspective. It can be concluded that the cellular and molecular mechanisms of learning and memory have long been a major focus of neurology and molecular biology, a concern regarding the epigenetic mechanisms behind dynamic changes in the transcription of genes responsible for memory formation and maintenance.},
booktitle = {Proceedings of the 5th International Conference on Learning Innovation and Quality Education},
articleno = {38},
numpages = {3},
location = {Surakarta, Indonesia},
series = {ICLIQE '21}
}

@inproceedings{10.1145/3715275.3732003,
author = {Sabuncuoglu, Alpay and Burr, Christopher and Maple, Carsten},
title = {Justified Evidence Collection for Argument-based AI Fairness Assurance},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732003},
doi = {10.1145/3715275.3732003},
abstract = {It is well recognised that ensuring fair AI systems is a complex sociotechnical challenge, which requires careful deliberation and continuous oversight across all stages of a system’s lifecycle, from defining requirements to model deployment and deprovisioning. Dynamic argument-based assurance cases, which present structured arguments supported by evidence, have emerged as a systematic approach to evaluating and mitigating safety risks and hazards in AI-enabled system development and have also been extended to deal with broader normative goals such as fairness and explainability. This paper introduces a systems-engineering-driven framework, supported by software tooling, to operationalise a dynamic approach to argument-based assurance in two stages. In the first stage, during the requirements planning phase, a multi-disciplinary and multi-stakeholder team define goals and claims to be established (and evidenced) by conducting a comprehensive fairness governance process. In the second stage, a continuous monitoring interface gathers evidence from existing artefacts (e.g. metrics from automated tests), such as model, data, and use case documentation, to support these arguments dynamically. The framework’s effectiveness is demonstrated through an illustrative case study in finance, with a focus on supporting fairness-related arguments.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {18–28},
numpages = {11},
keywords = {trustworthy and ethical assurance, continuous fairness monitoring, system transparency artefacts, large language models in finance},
location = {
},
series = {FAccT '25}
}

@inproceedings{10.5555/3716662.3716664,
author = {Akbulut, Canfer and Weidinger, Laura and Manzini, Arianna and Gabriel, Iason and Rieser, Verena},
title = {All Too Human? Mapping and Mitigating the Risks from Anthropomorphic AI},
year = {2025},
publisher = {AAAI Press},
abstract = {The development of highly-capable conversational agents, underwritten by large language models, has the potential to shape user interaction with this technology in profound ways, particularly when the technology is anthropomorphic, or appears human-like. Although the effects of anthropomorphic AI are often benign, anthropomorphic design features also create new kinds of risk. For example, users may form emotional connections to human-like AI, creating the risk of infringing on user privacy and autonomy through over-reliance. To better understand the possible pitfalls of anthropomorphic AI systems, we make two contributions: first, we explore anthropomorphic features that have been embedded in interactive systems in the past, and leverage this precedent to highlight the current implications of anthropomorphic design. Second, we propose research directions for informing the ethical design of anthropomorphic AI. In advancing the responsible development of AI, we promote approaches to the ethical foresight, evaluation, and mitigation of harms arising from user interactions with anthropomorphic AI.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {13–26},
numpages = {14},
location = {San Jose, California, USA},
series = {AIES '24}
}

@article{10.1145/3291043,
author = {Abdulahhad, Karam and Berrut, Catherine and Chevallet, Jean-Pierre and Pasi, Gabriella},
title = {Modeling Information Retrieval by Formal Logic: A Survey},
year = {2019},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3291043},
doi = {10.1145/3291043},
abstract = {Several mathematical frameworks have been used to model the information retrieval (IR) process, among them, formal logics. Logic-based IR models upgrade the IR process from document-query comparison to an inference process, in which both documents and queries are expressed as sentences of the selected formal logic. The underlying formal logic also permits one to represent and integrate knowledge in the IR process. One of the main obstacles that has prevented the adoption and large-scale diffusion of logic-based IR systems is their complexity. However, several logic-based IR models have been recently proposed that are applicable to large-scale data collections. In this survey, we present an overview of the most prominent logical IR models that have been proposed in the literature. The considered logical models are categorized under different axes, which include the considered logics and the way in which uncertainty has been modeled, for example, degrees of belief or degrees of truth. Accordingly, the main contribution of the article is to categorize the state-of-the-art logical models on a fine-grained basis, and for the considered models the related implementation aspects are described. Consequently, the proposed survey is finalized to better understand and compare the different logical IR models. Last, but not least, this article aims at reconsidering the potentials of logical approaches to IR by outlining the advances of logic-based approaches in close research areas.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {15},
numpages = {37},
keywords = {uncertainty, survey, logical models, information retrieval models, Formal logics}
}

@article{10.1145/3656587,
author = {Lorv\~{a}o Antunes, Ant\'{o}nio and Barateiro, Jos\'{e} and Cardoso, Elsa},
title = {Strategic Analysis in the Public Sector Using Semantic Web Technologies},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
url = {https://doi.org/10.1145/3656587},
doi = {10.1145/3656587},
abstract = {This article addresses the complex challenges that public organizations face in designing, implementing, and evaluating their strategies, where public interest and regulatory compliance often intertwine with strategic objectives. This research investigates the application of ontologies in the field of public sector strategy management to enhance the capacity of organizations to make informed data-driven decisions, efficiently allocate resources, and effectively navigate the intricate landscape of the public sector. The LNEC - National Laboratory for Civil Engineering’s strategy is used as an exploratory case study. Semantic web technologies are used to perform strategy analysis, including validating the strategy formulation and supporting the strategy execution by assessing performance indicators, verifying the design of cause-and-effect relationships between strategic objectives, and monitoring and empirically validating these relationships. The increased interoperability of these technologies enables information sharing across systems and organizations. Following the strategy analysis, recommendations are provided, leading to a more robust and data-driven strategic management approach, enabling accurate, traceable, and continuous monitoring of an organization’s strategy. Theoretical and practical implications are discussed, along with limitations and future work. This research offers a blueprint for public sector organizations seeking to optimize their strategies, foster transparency, and deliver more effective services to the public they serve.},
journal = {Digit. Gov.: Res. Pract.},
month = sep,
articleno = {20},
numpages = {20},
keywords = {Strategy Analysis, Public Sector, Semantic Web, Balanced Scorecard, Ontology}
}

@article{10.1145/3609483,
author = {Moscato, Vincenzo and Postiglione, Marco and Sperl\'{\i}, Giancarlo},
title = {Few-shot Named Entity Recognition: Definition, Taxonomy and Research Directions},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {5},
issn = {2157-6904},
url = {https://doi.org/10.1145/3609483},
doi = {10.1145/3609483},
abstract = {Recent years have seen an exponential growth (+98\% in 2022 w.r.t. the previous year) of the number of research articles in the few-shot learning field, which aims at training machine learning models with extremely limited available data. The research interest toward few-shot learning systems for Named Entity Recognition (NER) is thus at the same time increasing. NER consists in identifying mentions of pre-defined entities from unstructured text, and serves as a fundamental step in many downstream tasks, such as the construction of Knowledge Graphs, or Question Answering. The need for a NER system able to be trained with few-annotated examples comes in all its urgency in domains where the annotation process requires time, knowledge and expertise (e.g., healthcare, finance, legal), and in low-resource languages. In this survey, starting from a clear definition and description of the few-shot NER (FS-NER) problem, we take stock of the current state-of-the-art and propose a taxonomy which divides algorithms in two macro-categories according to the underlying mechanisms: model-centric and data-centric. For each category, we line-up works as a story to show how the field is moving toward new research directions. Eventually, techniques, limitations, and key aspects are deeply analyzed to facilitate future studies.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = oct,
articleno = {94},
numpages = {46},
keywords = {Few-shot learning, Named Entity Recognition}
}

@inproceedings{10.1145/3638584.3638670,
author = {Sai P, Daiveek and Rajesh, Anouksha},
title = {Semantic Topic Extraction from Research Artifacts},
year = {2024},
isbn = {9798400708688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638584.3638670},
doi = {10.1145/3638584.3638670},
abstract = {The GENESIS project introduces a novel framework for Research Artifact Data Semantics to enhance semantic web technologies within academic institutions. This paper introduces a system developed as part of the GENESIS challenge, focusing on extracting and labelling topics from diverse research artifacts. The approach employs knowledge graphs, topic models, and named entity recognition to infer meaningful topics. Evaluation encompasses measures such as topic coherence and semantic similarity. Future work includes refining entity linking techniques, optimizing system performance, and implementing advanced graph centrality algorithms. This contribution advances semantic understanding in various research contexts.},
booktitle = {Proceedings of the 2023 7th International Conference on Computer Science and Artificial Intelligence},
pages = {539–545},
numpages = {7},
keywords = {knowledge graph, linked data, natural language processing, text summarization, topic modelling},
location = {Beijing, China},
series = {CSAI '23}
}

@inproceedings{10.1145/3657604.3662030,
author = {Moore, Steven and Schmucker, Robin and Mitchell, Tom and Stamper, John},
title = {Automated Generation and Tagging of Knowledge Components from Multiple-Choice Questions},
year = {2024},
isbn = {9798400706332},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3657604.3662030},
doi = {10.1145/3657604.3662030},
abstract = {Knowledge Components (KCs) linked to assessments enhance the measurement of student learning, enrich analytics, and facilitate adaptivity. However, generating and linking KCs to assessment items requires significant effort and domain-specific knowledge. To streamline this process for higher-education courses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs) in Chemistry and E-Learning. We analyzed discrepancies between the KCs generated by the Large Language Model (LLM) and those made by humans through evaluation from three domain experts in each subject area. This evaluation aimed to determine whether, in instances of non-matching KCs, evaluators showed a preference for the LLM-generated KCs over their human-created counterparts. We also developed an ontology induction algorithm to cluster questions that assess similar KCs based on their content. Our most effective LLM strategy accurately matched KCs for 56\% of Chemistry and 35\% of E-Learning MCQs, with even higher success when considering the top five KC suggestions. Human evaluators favored LLM-generated KCs, choosing them over human-assigned ones approximately two-thirds of the time, a preference that was statistically significant across both domains. Our clustering algorithm successfully grouped questions by their underlying KCs without needing explicit labels or contextual information. This research advances the automation of KC generation and classification for assessment items, alleviating the need for student data or predefined KC labels.},
booktitle = {Proceedings of the Eleventh ACM Conference on Learning @ Scale},
pages = {122–133},
numpages = {12},
keywords = {concept labeling, knowledge component, knowledge labeling, learning engineering, multiple-choice question},
location = {Atlanta, GA, USA},
series = {L@S '24}
}

@inproceedings{10.1145/3627673.3679528,
author = {Yang, Zhihao and Zhao, Yizheng},
title = {What a Surprise! Computing Rewritten Modules Can Be as Efficient as Computing Subset Modules},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679528},
doi = {10.1145/3627673.3679528},
abstract = {Uniform Interpolation (UI) is an advanced non-standard reasoning service that seeks to refine ontologies by creating rewritten modules. These modules, known as uniform interpolants, retain only "relevant names" while preserving their meanings in the absence of other names. UI holds significant potential across various domains where tailored ontology modules are required. However, realizing its full potential demands highly optimized techniques for generating such modules. Previous studies have identified notable challenges in generating uniform interpolants for EL-ontologies, where their computation is substantially more complex and computationally demanding than standard subset modules.Despite these obstacles, this paper introduces an advanced "forgetting" method tailored for computing uniform interpolants of ELIO-ontologies with ABoxes. We show that with effective normalization and inference strategies, these uniform interpolants can be computed efficiently, matching the speed of standard module computation. A comprehensive evaluation using a prototype implementation of this method achieved a 100\% success rate on two major benchmark datasets, Oxford-ISG and BioPortal, with results delivered within seconds. The efficiency of our approach is attributed to our novel linear strategy for introducing definers, in sharp contrast to existing strategies that lead to an exponential increase in definers and computational inefficiency. Our method is unique in its ability to create signature-restricted modules for large-scale ontologies, making it a vital addition to the community's toolkit.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {2940–2949},
numpages = {10},
keywords = {description logics, forgetting, ontologies, uniform interpolation},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3477314.3507253,
author = {Amaral, Larissa Mangolim and Siqueira, F\'{a}bio Levy and Brand\~{a}o, Anarosa Alves Franco},
title = {A survey on requirements notations in software engineering research},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507253},
doi = {10.1145/3477314.3507253},
abstract = {Requirements can be documented using different notations that vary, for example, on expressivity, formality, and visualization. Each of them have advantages and disadvantages on different contexts. In fact, there is not much information on which and how requirements notations are being used, most of all when considering not only the Software Engineering industry, but also its research community. Therefore, this study investigates researchers' applications for requirements notations. Furthermore, we explore their reasoning behind the choice for a notation and how formally these notations are defined. First we reviewed the literature to identify general requirements notations and their definition methods. Then, we conducted a survey study with Software Engineering researchers, asking them about the notations they have used. We received 251 usable responses from 24 countries. Our main findings are that participants' preferred notations are Use Case, Class Diagram, and User Stories; and they choose notations based mainly on their suitability to the problem, their adequacy to the stakeholders, and common usage.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1291–1298},
numpages = {8},
keywords = {metamodel, ontology, requirements language, requirements notation, survey},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.5555/3427510.3427542,
author = {Neubauer, Kevin and Bucher, Harald and Haas, Benedikt and Becker, J\"{u}rgen},
title = {Model-based development and simulative verification of logical vehicle functions using executable UN/ECE regulations},
year = {2020},
isbn = {9781713814290},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {On the way towards autonomous driving, a steadily increasing number of Advanced Driver Assistance Systems leads to a tremendous test effort to approve their safe operation. However, route-based real-world tests cannot cover this effort sufficiently which is why virtual testing has become part of the vehicle development and approval process as well. Since regulations such as those of the United Nations Economic Commission for Europe, are mandatory for vehicle approval there is a need to integrate their prescribed test scenarios into virtual test environments. In this paper, we present a novel approach to transform these textually available scenarios into executable state machines. It is complemented by a holistic simulation-based verification where model-based vehicle functions are stimulated by sensor data of the virtual vehicle under test to achieve meaningful and more realistic results. We prototyped a tool-chain to execute approval-related test scenarios on the example of an Advanced Emergency Braking System.},
booktitle = {Proceedings of the 2020 Summer Simulation Conference},
articleno = {31},
numpages = {12},
keywords = {E/E architecture, homologation, model-based, simulation, verification},
location = {Virtual Event, Spain},
series = {SummerSim '20}
}

@inproceedings{10.1145/3418094.3418121,
author = {Li, Guoxuan},
title = {DeepFCA: Matching Biomedical Ontologies Using Formal Concept Analysis Embedding Techniques},
year = {2020},
isbn = {9781450377768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418094.3418121},
doi = {10.1145/3418094.3418121},
abstract = {Biomedical ontologies contain target domain knowledge. In many cases, multiple ontologies are created independently for different purposes in the same biomedical domain. To fuse and extend existing knowledge, we need to find the corresponding entities (i.e. classes and properties) from different ontologies. Formal Concept Analysis (FCA) is a mature mathematical tool for biomedical ontology matching tasks and has achieved competitive performance. The FCA-based method mainly matches the ontologies through lexical tokens and structural information. This method ignores the inherent semantics of entities. On the other hand, representation learning techniques are widely used in different NLP tasks to capture the semantic similarity of words. In this paper, we propose a novel biomedical ontology matching method which we dub DeepFCA. We use pre-trained word vectors to initialize the vector representations onto which semantic information is inscribed. FCA embedding techniques are used to refine these vectors. DeepFCA combines FCA and word2vec methods to enhance the performance of biomedical ontology matching. To the best of our knowledge, this is the first attempt to apply FCA embedding techniques to biomedical ontology matching. Experiments on real-world biomedical ontologies show that DeepFCA improves the recall and F1-measure compared with the traditional FCA-based algorithm. It also achieves competitive performance compared with several state-of-the-art systems.},
booktitle = {Proceedings of the 4th International Conference on Medical and Health Informatics},
pages = {259–265},
numpages = {7},
keywords = {Artificial intelligence, Biomedical ontology matching, Formal concept analysis, Word embedding},
location = {Kamakura City, Japan},
series = {ICMHI '20}
}

@article{10.1145/3502854,
author = {Shoaib, Umar and Fiaz, Laiba and Chakraborty, Chinmay and Rauf, Hafiz Tayyab},
title = {Context-aware Urdu Information Retrieval System},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3502854},
doi = {10.1145/3502854},
abstract = {World Wide Web (WWW) is playing a vital role for sharing dynamic knowledge in every field of life. The information on web comprises a huge amount of data in different forms such as structured, semi structured, or few is totally in unstructured format. Due to huge size of information, searching from larger textual data about the specific topic or getting precise information is a challenging task. All this leads to the problem of word sense ambiguity (WSA). Urdu language-based information retrieval system using different techniques related to Web Semantic Search Engine architecture is proposed to efficiently retrieve the relevant information and solve the problem of WSA. The proposed system has average precision ratio 96\% as compared to average precision ratio of 74\% and 75\% average precision Google for single word query. For the long text queries, our system outperforms the existing famous search engines with 92\% accuracy such as Bing and Google having 16.50\% and 16\% accuracy, respectively. Similarly, the proposed system for single word query, the recall ratio is 32.25\% as compared to 25\% and 25\% of Bing and Google. The results of recall ratio for long text query are improved as well, showing 6.38\% as compared to 6.20\% and 4.8\% of Bing and Google, respectively. The results showed that the proposed system gives better and efficient results as compared to the existing systems for Urdu language.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {70},
numpages = {19},
keywords = {Urdu language, information retrieval, semantic web, ontology, triplets, quad extraction, context-based, Web Semantic Search Engine, WSA, searching and indexing, keywords, corpus, Uniform Resource Identifier}
}

@inproceedings{10.1145/3674225.3674330,
author = {Xu, Jianing and Lou, Fei and Jiang, Ying and Chen, Bo and Zhong, Zhenyuan},
title = {A Method for Constructing a Knowledge Graph of Electric Power Digital Marketing Based on Artificial Intelligence Deep Learning},
year = {2024},
isbn = {9798400716638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3674225.3674330},
doi = {10.1145/3674225.3674330},
abstract = {With the rapid development of digitalization and informatization in the power industry, power companies have accumulated a large amount of data in various business fields. This article focuses on the intelligent application requirements in the field of electric power marketing, and designs and constructs a knowledge graph of electric power marketing business that includes domain background knowledge. Firstly, utilize the relationships between the basic business data tables organized by domain experts to construct a conceptual ontology. Next, through operations such as data cleaning, data filtering, and feature selection, traverse the data tables of the business database, use knowledge graph tools to obtain a knowledge graph, and finally use the power marketing knowledge graph to build an intelligent question answering application that supports natural language question answering services in the field of power marketing, better serving power users. Experimental results have shown that the power marketing knowledge graph constructed in this article, along with intelligent question answering applications, can accurately answer user questions and significantly improve user satisfaction.},
booktitle = {Proceedings of the 2024 International Conference on Power Electronics and Artificial Intelligence},
pages = {582–587},
numpages = {6},
keywords = {Electricity Marketing, Knowledge graph, Natural language processing},
location = {Xiamen, China},
series = {PEAI '24}
}

@article{10.1145/3579821,
author = {Andr\'{e}, \'{E}tienne and Liu, Shuang and Liu, Yang and Choppy, Christine and Sun, Jun and Dong, Jin Song},
title = {Formalizing UML State Machines for Automated Verification – A Survey},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3579821},
doi = {10.1145/3579821},
abstract = {The Unified Modeling Language (UML) is a standard for modeling dynamic systems. UML behavioral state machines are used for modeling the dynamic behavior of object-oriented designs. The UML specification, maintained by the Object Management Group (OMG), is documented in natural language (in contrast to formal language). The inherent ambiguity of natural languages may introduce inconsistencies in the resulting state machine model. Formalizing UML state machine specification aims at solving the ambiguity problem and at providing a uniform view to software designers and developers. Such a formalization also aims at providing a foundation for automatic verification of UML state machine models, which can help to find software design vulnerabilities at an early stage and reduce the development cost. We provide here a comprehensive survey of existing work from&nbsp;1997 to&nbsp;2021 related to formalizing UML state machine semantics for the purpose of conducting model checking at the design stage.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {277},
numpages = {47},
keywords = {UML, semantics, formal specification, formal verification}
}

@inproceedings{10.1145/3701551.3704125,
author = {Sakor, Ahmad and Brunet, Mauricio and Iglesias, Enrique and Rivas, Ariam and Rohde, Philipp D. and Kraft, Angelina and Vidal, Maria-Esther},
title = {Integrating Knowledge Graphs and Neuro-Symbolic AI: LDM Enables FAIR and Federated Research Data Management},
year = {2025},
isbn = {9798400713293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701551.3704125},
doi = {10.1145/3701551.3704125},
abstract = {Managing research digital objects (RDOs) in compliance with FAIR principles is crucial for ensuring accessibility, interoperability, and reusability across scientific domains. The Leibniz Data Manager (LDM) is a state-of-the-art framework that integrates Knowledge Graphs (KGs) and Neuro-Symbolic AI, combining the reasoning power of Large Language Models (LLMs) with structured metadata. LDM supports the management and enhancement of RDOs through entity linking, connecting datasets to external KGs like Wikidata and the Open Research Knowledge Graph (ORKG). Additionally, LDM offers federated query processing across KGs, enabling users to explore related papers, datasets, and resources through natural language questions. This demo showcases LDM's capabilities to explore RDOs, compare existing datasets, and extend metadata. By blending Neuro-Symbolic AI with FAIR and federated research data management, LDM offers a powerful tool for accelerating data-driven discovery in science. LDM is publicly accessible at https://service.tib.eu/ldmservice/.},
booktitle = {Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining},
pages = {1044–1047},
numpages = {4},
keywords = {data science, digital repositories, federated search},
location = {Hannover, Germany},
series = {WSDM '25}
}

@article{10.1145/3530257,
author = {Wu, Chuhan and Wu, Fangzhao and Huang, Yongfeng and Xie, Xing},
title = {Personalized News Recommendation: Methods and Challenges},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {1},
issn = {1046-8188},
url = {https://doi.org/10.1145/3530257},
doi = {10.1145/3530257},
abstract = {Personalized news recommendation is important for users to find interesting news information and alleviate information overload. Although it has been extensively studied over decades and has achieved notable success in improving user experience, there are still many problems and challenges that need to be further studied. To help researchers master the advances in personalized news recommendation, in this article, we present a comprehensive overview of personalized news recommendation. Instead of following the conventional taxonomy of news recommendation methods, in this article, we propose a novel perspective to understand personalized news recommendation based on its core problems and the associated techniques and challenges. We first review the techniques for tackling each core problem in a personalized news recommender system and the challenges they face. Next, we introduce the public datasets and evaluation methods for personalized news recommendation. We then discuss the key points on improving the responsibility of personalized news recommender systems. Finally, we raise several research directions that are worth investigating in the future. This article can provide up-to-date and comprehensive views on personalized news recommendation. We hope this article can facilitate research on personalized news recommendation as well as related fields in natural language processing and data mining.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {24},
numpages = {50},
keywords = {News recommendation, personalization, survey, user modeling, natural language processing}
}

@inproceedings{10.1145/3495018.3501225,
author = {Li, Shaoyi},
title = {Research on Financial Risk Control Model and Algorithm Based on Machine Learning under the Background of Rural Revitalization Strategy},
year = {2022},
isbn = {9781450385046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3495018.3501225},
doi = {10.1145/3495018.3501225},
abstract = {The strategy of rural revitalization is of great significance to the reconstruction of rural economic growth, in which rural industries, represented by the integration and development of rural industries, have sprung up. The purpose of this paper is to use machine learning (ML) technology to build an effective risk control model, so as to help Internet finance enterprises better control the loan risk. Sample data of Internet financial platform borrowers are extracted from multiple dimensions, and then the data are further processed, and the data used to build the model is extracted by feature engineering. Combined with the Gradient Boosting Decision Tree (GBDT) algorithm in ML algorithm, the comprehensive evaluation is carried out by using the basic information of bank customers, flow records, user detection information and user detection scale. The performance of the wind control model is further improved by ML, which provides guidance and reference for the performance improvement of the model.},
booktitle = {2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {3010–3014},
numpages = {5},
location = {Manchester, United Kingdom},
series = {AIAM2021}
}


@inproceedings{10.1145/3663548.3675598,
author = {Nevsky, Alexandre and Bircanin, Filip and Cruice, Madeline N and Wilson, Stephanie and Simperl, Elena and Neate, Timothy},
title = {"I Wish You Could Make the Camera Stand Still": Envisioning Media Accessibility Interventions with People with Aphasia},
year = {2024},
isbn = {9798400706776},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3663548.3675598},
doi = {10.1145/3663548.3675598},
abstract = {Audiovisual media is integral to modern living, yet is not always accessible to all. Modern accessibility interventions, such as subtitles, support many, however, communities with complex communication needs are largely unconsidered. In this work, we envision future accessibility interventions from the ground up with one such community – people with aphasia. Over two workshops and a probe activity, we problematise the space of audiovisual consumption by people with aphasia, and co-envision directions for development in accessible audiovisual media. From low-fi diegetic prototypes to mid-fidelity solutions, we explore new visions of accessibility interventions for complex communication needs – notably enabling high levels of content manipulation and personalisation. Our findings raise open questions and set directions for the research community in developing accessibility interventions for audiovisual media to support users with diverse needs in accessing audiovisual content.},
booktitle = {Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {46},
numpages = {17},
keywords = {Accessibility, aphasia, audiovisual, complex communication needs, envisioning, media, probes, prototype},
location = {St. John's, NL, Canada},
series = {ASSETS '24}
}

@inproceedings{10.1145/3463274.3463344,
author = {Junaid, Waqas},
title = {Evaluating the Effectiveness of Problem Frames for Contextual Modeling of Cyber-Physical Systems: a Tool Suite with Adaptive User Interfaces},
year = {2021},
isbn = {9781450390538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3463274.3463344},
doi = {10.1145/3463274.3463344},
abstract = {Bridging the gap between academic research and industrial application is an important issue to promote Jackson's Problem Frames approach (PF) to the software engineering community. Various attempts have been made to tackle this problem, such as defining formal semantics of PF for software development, and providing a semi-formal approach to model transformations of problem diagrams, with automated tool support. In this paper, we propose to exclusively focus on exploring and evaluating the effectiveness of Jackson's problem diagrams for modeling the context of cyber-physical systems, by developing a suite of support tools enhanced with adaptive user interfaces, and empirically and comprehensively assess its usability. This paper introduces the state of the art, corresponding research questions, research methodologies and current progress of our research.},
booktitle = {Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering},
pages = {284–287},
numpages = {4},
keywords = {cyber-physical systems, context-based knowledge, adaptive user interfaces, Problem Frames},
location = {Trondheim, Norway},
series = {EASE '21}
}

@inproceedings{10.1145/3185089.3185092,
author = {Gazzawe, Foziah and Lock, Russell and Dawson, Christian},
title = {Use of Ontology in Identifying Missing Artefact Links},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185092},
doi = {10.1145/3185089.3185092},
abstract = {The techniques of requirement traceability have evolved over recent years. However, as much as they have contributed to the software engineering field, significant ambiguity remains in many software engineering processes. This paper reports on an investigation of requirement traceability artefacts, stakeholders, and SDLC development models. Data were collected to gather evidence of artefacts and their properties from previous studies. The aim was to find the missing link between artefacts and their relationship to one another, the stakeholders, and SDLC models. This paper undertakes the first phase of the main research project, which aims to develop a framework for guiding software developers to actively manage traceability. After inquiring into and examining previous research on this topic, the links between artefacts and their functions were identified. The analysis resulted in the development of a new model for requirement traceability, defined in the form of an ontology portraying the contributively relations between software artefacts using common properties with the aid of Prot\'{e}g\'{e} Software. This study thus provides an important insight into the future of the requirement artefacts relation, and thereby lays an important foundation towards increasing our understanding of their potential and limitations.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {6–9},
numpages = {4},
keywords = {Requirements Traceability, Requirement Artefacts, Mapping the Requirement Artefacts, Artefacts Link},
location = {Kuantan, Malaysia},
series = {ICSCA '18}
}

@inproceedings{10.1145/3314493.3314504,
author = {Shangguan, Duansen and Chen, Liping and Ding, Jianwan},
title = {A Hierarchical Digital Twin Model Framework for Dynamic Cyber-Physical System Design},
year = {2019},
isbn = {9781450360951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314493.3314504},
doi = {10.1145/3314493.3314504},
abstract = {Cyber-physical system (CPS) is a new trend in the complex system related research works, where network connectivity enhances computing power and systemic behavior emerges through the competition, interaction, collaboration and integration among individual interweaving, which consists of real-time monitoring, data management, physical feedback control. From this perspective, CPS is a dynamic entity with rich functions. However, designers may encounter a difficult situation, in which subsequent dynamic changes of the system are discussed and appropriate functionalities are added in the early design phase. Since the digital twin is the digital duplicate of the physical entity, it can dynamically evolve following the product life cycle. In this paper, we propose a hierarchical digital twin model framework for CPS design. In the light of digital twin concept, the hierarchical high-level models facilitate storage of information from the entire product life cycle. Finally, an industrial robot application is presented to demonstrate the efficacy of the model framework.},
booktitle = {Proceedings of the 5th International Conference on Mechatronics and Robotics Engineering},
pages = {123–129},
numpages = {7},
keywords = {Modeling&amp;Simulation, Industrial Robot, Digital Twin, Complex System, CPS},
location = {Rome, Italy},
series = {ICMRE'19}
}

@inproceedings{10.5555/3291291.3291325,
author = {Bekele, Amente and Samuel, Joe and Nizami, Shermeen and Basharat, Amna and Giffen, Randy and Green, James R.},
title = {Ontology driven temporal event annotator mHealth application framework},
year = {2018},
publisher = {IBM Corp.},
address = {USA},
abstract = {We present an application (app) framework to facilitate the collection of gold standard temporal event annotations. These data will enable training and evaluation of machine learning algorithms for predicting events of clinical significance. Recording of such data using pen and paper can prove to be tedious and error-prone due to the variation in the types of events and the frequency of occurrence. To address this problem, we developed an mHealth application framework that presents an intuitive and configurable user interface for annotating a timeline with events.The presented Temporal Event Annotator (TEA) app framework supports dynamically building a customized application inclusive of events, event categories, and study attributes based on the design input of a specific study. This is accomplished by presenting a terminology schema for the hierarchical definition of event types and an additional user interface (UI) schema to support UI-specific attributes.We describe the framework architecture independent of specific technology implementations. We also describe specific instantiations of the framework that we used to develop and evaluate apps for three different use cases: 1) patient monitoring in the Neonatal Intensive Care Unit (NICU), 2) estimating patient stress levels during immersive rehabilitation therapy, and 3) quantifying the patient experience during emergency neonatal transport. The TEA framework provides a reliable and intuitive solution for temporal event annotation that accounts for the unique experimental requirements of each study.},
booktitle = {Proceedings of the 28th Annual International Conference on Computer Science and Software Engineering},
pages = {309–314},
numpages = {6},
keywords = {mobile applications, medical event annotations, life and medical sciences, healthcare, data model, data entry and integration},
location = {Markham, Ontario, Canada},
series = {CASCON '18}
}

@inproceedings{10.1145/3628454.3629551,
author = {Nimpattanavong, Chollakorn and Taveekitworachai, Pittawat and Khan, Ibrahim and Nguyen, Thai Van and Thawonmas, Ruck and Choensawat, Worawat and Sookhanaphibarn, Kingkarn},
title = {Am I Fighting Well? Fighting Game Commentary Generation With ChatGPT},
year = {2023},
isbn = {9798400708497},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3628454.3629551},
doi = {10.1145/3628454.3629551},
abstract = {This paper presents a new approach for leveraging ChatGPT in fighting game commentary generation task. Commentary generation often relies on deep learning techniques, which typically demand extensive data to achieve effectiveness. Large language models (LLMs) have become essential due to their remarkable ability to process data efficiently, thanks to their extensive training on vast datasets. Our proposed approach integrates the use of LLMs, specifically the GPT-3.5 model, for generating commentaries through the utilization of various prompts with data from the open-source fighting game, DareFightingICE. Four prompt variants are employed to assess the effectiveness of each prompt components. Objective evaluation using natural language metrics reveals that different prompt components significantly affect the generated commentaries. Additionally, subjective evaluation through a questionnaire reveals that prompts without parameter definitions received the highest preference from human evaluators. These results suggest that LLMs exhibit versatility in generating fighting game commentaries and hold promise for broader applications.},
booktitle = {Proceedings of the 13th International Conference on Advances in Information Technology},
articleno = {14},
numpages = {7},
keywords = {ChatGPT, Commentary Generation, DareFightingICE, Fighting Game, Prompt Engineering},
location = {Bangkok, Thailand},
series = {IAIT '23}
}

@inproceedings{10.1145/3366030.3366110,
author = {Saad, Farag and Hackl-Sommer, Rene},
title = {Building a Semantic Model for Linking and Visualizing Patent Citations (SeMViPaC)},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366110},
doi = {10.1145/3366030.3366110},
abstract = {Patents are a high-quality resource of information that is currently insufficiently leveraged. In times when continuously rising prices for basic knowledge access threaten to throttle academic research everywhere, this is a resource that can not be longer neglected. Thus, in the project we plan to extract and semantically describe citations from patents. Furthermore, we will establish linking and data integration between disparate data sources, i.e. linking patents with resources in the LOD (Linked Open Data) cloud. Based on the developed semantic citation model a visualization tool to explore and gain better insight and understanding of the extracted citation data will be developed and made available to the public.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {645–648},
numpages = {4},
keywords = {Visualization, Semantic, Patents, Exploration, Citations},
location = {Munich, Germany},
series = {iiWAS2019}
}

@article{10.1145/3183628.3183632,
author = {Kalra, Sumit and Prabhakar, T. V.},
title = {Ontology-based framework for internal-external quality trade-offs and tenant management in multi-tenant applications},
year = {2018},
issue_date = {December 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {4},
issn = {1559-6915},
url = {https://doi.org/10.1145/3183628.3183632},
doi = {10.1145/3183628.3183632},
abstract = {Software Quality Attributes (QAs) can be categorized as either internal to the system as experienced by the developers or external to the system perceived by the end users. These QA categories have trade-off among them - an emphasis on internal QA may result in a compromise of an external QA. For example, there is a trade-off between maintainability and performance. Model-driven development approaches manage this trade-off and increase the degree of internal QA maintainability. In this work, we propose an ontology-based communication mechanism among software components to handle the trade-off. The approach increases the degree of internal QAs such as modifiability, maintainability, testability during the design and development phases without compromising the external QAs for the end users during the operation phase. We also evaluate a prototype system to validate the proposed approach using Software Architecture Analysis Method (SAAM). It is also easier to integrate into the software development lifecycle as compared to existing model-driven approaches. The internal quality attributes become more significant in a multi-tenant scenario than conventional software. It requires managing dynamic requirements of tenants continuously. The proposed approach also useful in such scenario to reduce the maintenance overhead without compromising the degree of multi-tenancy.},
journal = {SIGAPP Appl. Comput. Rev.},
month = jan,
pages = {46–58},
numpages = {13},
keywords = {software product quality attributes, quality attributes trade-off, multi-tenant, internal quality attributes, external quality attributes}
}

@inproceedings{10.1145/3385209.3385235,
author = {Kumar, Akshi and Sharma, Aditi and Nayyar, Anand},
title = {Fuzzy Logic based Hybrid Model for Automatic Extractive Text Summarization},
year = {2020},
isbn = {9781450376594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385209.3385235},
doi = {10.1145/3385209.3385235},
abstract = {In the contemporary age of information, accessing data becomes easy, but finding knowledge is very difficult. The participation \&amp; publishing of information has consequently escalated the suffering of 'Information Glut.' Assisting users' informational searches with reduced reading or surfing time by extracting and evaluating accurate, authentic \&amp; relevant information are the primary concerns in the present milieu. Automatic text summarization condenses an original document into a shorter form to create a smaller, compact version from the abundant information that is available, preserving the content \&amp; meaning such that it meets the needs of the user. Though many summarization techniques have been proposed, there are no 'silver bullets' to achieve the superlative results as of human-generated summaries. Fuzzy Logic has appeared as a robust theoretical framework for studying human reasoning. A new hybrid model based on fuzzy logic has been proposed using two graph-based techniques named TextRank and LexRank and one semantic-based technique named Latent semantic analysis (LSA). The techniques are evaluated on the Opinosis dataset using 'ROUGE-1' (Recall-Oriented Understudy for Gisting Evaluation-1) and 'time to extract the keywords.' The proposed technique has outperformed the existing techniques when compared with the results given by the original studies.},
booktitle = {Proceedings of the 2020 5th International Conference on Intelligent Information Technology},
pages = {7–15},
numpages = {9},
keywords = {Automatic text summarization, Extractive Text summarization, Fuzzy logic, Hybrid Model, LSA, LexRank, TextRank},
location = {Hanoi, Viet Nam},
series = {ICIIT '20}
}

@inproceedings{10.1145/3460210.3493583,
author = {Mero\~{n}o-Pe\~{n}uela, Albert and Pernisch, Romana and Gu\'{e}ret, Christophe and Schlobach, Stefan},
title = {Multi-domain and Explainable Prediction of Changes in Web Vocabularies},
year = {2021},
isbn = {9781450384575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460210.3493583},
doi = {10.1145/3460210.3493583},
abstract = {Web vocabularies (WV) have become a fundamental tool for structuring Web data: over 10 million sites use structured data formats and ontologies to markup content. Maintaining these vocabularies and keeping up with their changes are manual tasks with very limited automated support, impacting both publishers and users. Existing work shows that machine learning can be used to reliably predict vocabulary changes, but on specific domains (e.g. biomedicine) and with limited explanations on the impact of changes (e.g. their type, frequency, etc.). In this paper, we describe a framework that uses various supervised learning models to learn and predict changes in versioned vocabularies, independent of their domain. Using well-established results in ontology evolution we extract domain-agnostic and human-interpretable features and explain their influence on change predictability. Applying our method on 139 WV from 9 different domains, we find that ontology structural and instance data, the number of versions, and the release frequency highly correlate with predictability of change. These results can pave the way towards integrating predictive models into knowledge engineering practices and methods.},
booktitle = {Proceedings of the 11th Knowledge Capture Conference},
pages = {193–200},
numpages = {8},
keywords = {vocabulary change, ontology evolution, change modelling},
location = {Virtual Event, USA},
series = {K-CAP '21}
}

@inproceedings{10.1145/3314183.3323463,
author = {Barria-Pineda, Jordan and Akhuseyinoglu, Kamil and Brusilovsky, Peter},
title = {Explaining Need-based Educational Recommendations Using Interactive Open Learner Models},
year = {2019},
isbn = {9781450367110},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314183.3323463},
doi = {10.1145/3314183.3323463},
abstract = {Students might pursue different goals throughout their learning process. For example, they might be seeking new material to expand their current level of knowledge, repeating content of prior classes to prepare for an exam, or working on addressing their most recent misconceptions. Multiple potential goals require an adaptive e-learning system to recommend learning content appropriate for students' intent and to explain this recommendation in the context of this goal. In our prior work, we explored explainable recommendations for the most typical 'knowledge expansion goal". In this paper, we focus on students' immediate needs to remedy misunderstandings when they solve programming problems. We generate learning content recommendations to target the concepts with which students have struggled more recently. At the same time, we produce explanations for this recommendation goal in order to support students' understanding of why certain learning activities are recommended. The paper provides an overview of the design of this explainable educational recommender system and describes its ongoing evaluation},
booktitle = {Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization},
pages = {273–277},
numpages = {5},
keywords = {open learner models, explanations, educational recommender systems},
location = {Larnaca, Cyprus},
series = {UMAP'19 Adjunct}
}

@inproceedings{10.1145/3720554.3736184,
author = {Zhao, Rui and Wright, Jesse},
title = {Introduce an Auditing Layer to Web Science},
year = {2025},
isbn = {9798400715358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3720554.3736184},
doi = {10.1145/3720554.3736184},
abstract = {Scientific discoveries increasingly depend on data and data processing, and Web Science is no exception. As an established practice, data-intensive research typically uses scientific workflows and provenance to facilitate data and method sharing while automatically preserving processing history. Prior research has reported the possibility of ex-post policy-based compliance checking from provenance data. Based on these works, in this paper, we present the conceptual design of a framework of data-harvesting Web Science practices, especially by introducing a common auditing layer. We discuss the framework’s practical, scientific, and ethical advantages, including its applicability in the period of large language model (LLM), autonomous agent, and artificial intelligence (AI) explosion. We hope this framework design can incubate a new norm for research practice to be transparent, ethical, and lightweight.},
booktitle = {Companion Publication of the 17th ACM Web Science Conference 2025},
pages = {49–53},
numpages = {5},
keywords = {Data governance, transparency, audit, usage control},
location = {
},
series = {Websci Companion '25}
}

@article{10.1145/3359316,
author = {Pradhan, Alisha and Findlater, Leah and Lazar, Amanda},
title = {"Phantom Friend" or "Just a Box with Information": Personification and Ontological Categorization of Smart Speaker-based Voice Assistants by Older Adults},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359316},
doi = {10.1145/3359316},
abstract = {As voice-based conversational agents such as Amazon Alexa and Google Assistant move into our homes, researchers have studied the corresponding privacy implications, embeddedness in these complex social environments, and use by specific user groups. Yet it is unknown how users categorize these devices: are they thought of as just another object, like a toaster? As a social companion? Though past work hints to human-like attributes that are ported onto these devices, the anthropomorphization of voice assistants has not been studied in depth. Through a study deploying Amazon Echo Dot Devices in the homes of older adults, we provide a preliminary assessment of how individuals 1) perceive having social interactions with the voice agent, and 2) ontologically categorize the voice assistants. Our discussion contributes to an understanding of how well-developed theories of anthropomorphism apply to voice assistants, such as how the socioemotional context of the user (e.g., loneliness) drives increased anthropomorphism. We conclude with recommendations for designing voice assistants with the ontological category in mind, as well as implications for the design of technologies for social companionship for older adults.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = nov,
articleno = {214},
numpages = {21},
keywords = {voice assistants, smart speakers, personification, ontology, older adults, anthropomorphism}
}

@inproceedings{10.1145/3401335.3401344,
author = {Pouri, Maria J. and Hilty, Lorenz M.},
title = {The Relevance of Digital Sharing Business Models for Sustainability},
year = {2020},
isbn = {9781450375955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3401335.3401344},
doi = {10.1145/3401335.3401344},
abstract = {There is a growing discussion about the "Digital Sharing Economy" (DSE). The pervasiveness of digital platforms and the growing interest in a sharing (rather than ownership) style of consumption have allowed for sharing practices to scale up and become a widespread phenomenon. Digital sharing platforms offer a wide variety of services which appear to be more affordable, efficient, and accessible than their conventional counterparts, making them more attractive in the eyes of consumers. The DSE has manifested itself most remarkably in consumer-to-consumer (C2C) and business-to-consumer (B2C) sharing models. New business models have been created to capture and offer the values driving the emerging sharing trend.The innovative, digitally enabled mode of providing access to resources as a service in the DSE has changed consumption patterns both at micro level, as a change in individual lifestyles, and at macro level, manifested in a transformation of socio-economic structures. These ongoing changes may have both positive and negative implications for society from a sustainability perspective. Recognising that the (potential and actual) impacts of sharing platforms on sustainability have not been studied in a systematic way yet, the present paper aims to develop a systematic insight into this interaction by focusing on the business models emerging around sharing platforms as a central starting point. To achieve this, we use a typology of business models that recognizes the affordances and key attributes of sharing in the DSE. The typology covers both C2C and B2C models of sharing. Based on this typology, we discuss the implications of each type of sharing model for sustainability by asking two central questions: How may the given type of sharing affect resource consumption? And what will be the potential impacts on social practices and structures? We hope that the present study can serve as a guideline for assessing the sustainability impacts of sharing platforms -- either already operating in the market or envisaged. By highlighting the aspects most relevant from a sustainability point of view, we expect to contribute to an evolution of the DSE business models towards sustainable development.},
booktitle = {Proceedings of the 7th International Conference on ICT for Sustainability},
pages = {77–87},
numpages = {11},
keywords = {Sustainability impacts, Socio-economic structures, Sharing platforms, Sharing business models, Resource consumption, Information and communication technology (ICT), Digital sharing economy},
location = {Bristol, United Kingdom},
series = {ICT4S2020}
}

@article{10.1145/3449213,
author = {Mishra, Swati and Rzeszotarski, Jeffrey M.},
title = {Crowdsourcing and Evaluating Concept-driven Explanations of Machine Learning Models},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449213},
doi = {10.1145/3449213},
abstract = {An important challenge in building explainable artificially intelligent (AI) systems is designing interpretable explanations. AI models often use low-level data features which may be hard for humans to interpret. Recent research suggests that situating machine decisions in abstract, human understandable concepts can help. However, it is challenging to determine the right level of conceptual mapping. In this research, we explore granularity (of data features) and context (of data instances) as dimensions underpinning conceptual mappings. Based on these measures, we explore strategies for designing explanations in classification models. We introduce an end-to-end concept elicitation pipeline that supports gathering high-level concepts for a given data set. Through crowd-sourced experiments, we examine how providing conceptual information shapes the effectiveness of explanations, finding that a balance between coarse and fine-grained explanations help users better estimate model predictions. We organize our findings into systematic themes that can inform design considerations for future systems.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {139},
numpages = {26},
keywords = {classification, concepts, explanations, machine learning}
}

@article{10.1145/3626307.3626310,
author = {Guittoum, Amal and A\"{\i}ssaoui, Fran\c{c}ois and Bolle, S\'{e}bastien and Boyer, Fabienne and De Palma, Noel},
title = {Leveraging Semantic Technologies for Collaborative Inference of Threatening IoT Dependencies},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/3626307.3626310},
doi = {10.1145/3626307.3626310},
abstract = {IoT Device Management (DM) refers to the remote administration of customer devices. In practice, DM is ensured by multiple actors such as operators or device manufacturers, each operating independently via their DM solution. These siloed DM solutions are limited in addressing IoT threats related to device dependencies, such as cascading failures, as these threats spread across devices managed by different DM actors, and their mitigation can no longer be performed without collaborative DM efforts. The first step toward collaborative mitigation of these threats is the identification of threatening dependency topology. However, this task is challenging, requiring the inference of dependencies from the data held by different actors. In this work, we propose a collaborative framework that infers the threatening topology of dependencies by accessing and aggregating data from legacy DM solutions. It combines the assets of Semantic Web standards and Digital Twin technology to capture on-demand the topology of dependencies, and it is designed to be used in business applications such as customer care to enhance customer Quality of Experience. We integrate our solution within the in-use Orange's Digital Twin platform Thing in the future and demonstrate its effectiveness by automatically inferring threatening dependencies in the two settings: a simulated smart home scenario managed by ground-truth DM solutions, such as Orange's implementation of the USP Controller and Samsung's SmartThings Platform, and a realistic smart home called DOMUS testbed.},
journal = {SIGAPP Appl. Comput. Rev.},
month = sep,
pages = {32–48},
numpages = {17},
keywords = {thing description, semantic web, ontology, inference, entity resolution, digital twin, dependencies management, collaboration, SHACL, IoT device management}
}

@inproceedings{10.1145/3274192.3274213,
author = {Queiroz, Randerson and Marques, Anna Beatriz and Lopes, Adriana and Oliveira, Edson and Conte, Tayana},
title = {Evaluating Usability of IFML Models: How Usability is Perceived and Propagated},
year = {2018},
isbn = {9781450366014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274192.3274213},
doi = {10.1145/3274192.3274213},
abstract = {System acceptance is strongly related to its usability. It is important that the usability is carefully designed during the system design steps and later propagated to the interface. This care avoids reworking the interface design of an application in the future because of its usability. However, we did not find works that specifically addresses the modeling of interfaces in conjunction with usability. The Interaction Flow Modeling Language (IFML) is a proposal that supports the modeling of the interface. This work investigates the following research question: "usability in IFML models is perceived and propagated to the final interface?" In order to answer this research question, we performed an empirical study to evaluate how usability is perceived by participants through IFML models and if usability is propagated to an interface prototype. The study showed that not all aspects of usability are easily perceived and propagated in the interface through IFML models.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Human Factors in Computing Systems},
articleno = {21},
numpages = {10},
keywords = {User Interface, Usability, Interface Model, IFML, FUFs, Empirical studies},
location = {Bel\'{e}m, Brazil},
series = {IHC '18}
}

@article{10.1145/3280985,
author = {Lara, Juan De and Guerra, Esther},
title = {Refactoring Multi-Level Models},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3280985},
doi = {10.1145/3280985},
abstract = {Multi-level modelling promotes flexibility in modelling by enabling the use of several meta-levels instead of just two, as is the case in mainstream two-level modelling approaches. While this approach leads to simpler models for some scenarios, it introduces an additional degree of freedom as designers can decide the meta-level where an element should reside, having to ascertain the suitability of such decisions.In this respect, model refactorings have been successfully applied in the context of two-level modelling to rearrange the elements of a model while preserving its meaning. Following this idea, we propose a catalogue of 17 novel refactorings specific to multi-level models. Their objective is to help designers in rearranging elements across and within meta-levels and exploring the consequences. In this article, we detail each refactoring in the catalogue, show a classification across different dimensions, and describe the support we provide in our MetaDepth tool. We present two experiments to assess two aspects of our refactorings. The first one validates the predicted semantic side effects of the refactorings on the basis of more than 210.000 refactoring applications. The second one measures the impact of refactorings on three quality attributes of multi-level models.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = nov,
articleno = {17},
numpages = {56},
keywords = {multi-level modelling, model refactoring, MetaDepth, Meta-modelling}
}

@inproceedings{10.1145/3469968.3469998,
author = {Liu, Jiahao and Shen, Yifan and Zhang, Yijie and krishnamoorthy, Sujatha},
title = {Resume Parsing based on Multi-label Classification using Neural Network models},
year = {2021},
isbn = {9781450389808},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3469968.3469998},
doi = {10.1145/3469968.3469998},
abstract = {Application for jobs usually brings much work for both appliers and HR. Appliers want to apply for the jobs which they are most suitable. The number of applications for a particular position can be significant, making the candidates’ selection cumbersome for HR. Nowadays, hiring processes are often conducted through the Virtual mode with emails. This creates chances for analyzing the data in the resume. Therefore, to enhance selection problems’ efficiency, resume parsing algorithms have been developed in recent years to predict resume-based skills or good jobs quickly. The artificial neural network is a hot spot in the field of artificial intelligence since the 1980s. It abstracts the human brain's neural network from the angle of information processing, establishes some simple models, and forms different networks according to different connection modes. In recent years, neural networks-based algorithms perform high efficiency in processing text classification. This paper put forward some of the efficient algorithms used in text classification, Like BPNN, CNN, BiLSTM, and CRNN, for resume parsing. The original resumes are parsed by splitting them into words, and word base is trained to get the most appropriate word, which has a high score in the resume is resulting suitable job for each resume. The CRNN performs best in resume parsing, which the accuracy can reach 96\%. CNN places the lowest accuracy. The BPNN achieves good accuracy but brings inflexible.},
booktitle = {Proceedings of the 6th International Conference on Big Data and Computing},
pages = {177–185},
numpages = {9},
keywords = {Resume Parsing, Neural Network, Bi-LSTM, BPNN},
location = {Shenzhen, China},
series = {ICBDC '21}
}

@inproceedings{10.1145/3391274.3393640,
author = {Edgar, Vatricia and La Place, Cecilia and Schmidt, Julia and Bansal, Ajay and Bansal, Srividya},
title = {SustainOnt: an ontology for defining an index of neighborhood sustainability across domains},
year = {2020},
isbn = {9781450379748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3391274.3393640},
doi = {10.1145/3391274.3393640},
abstract = {Massive amounts of data, both structured and unstructured, are available to be harvested for competitive business advantage, sound government policies, and new insights in a broad array of applications. This paper specifically focuses on extraction, integration, and querying of open data available about environmental sustainability. The global trend toward urbanization has created a need for residents of urban neighborhoods to better understand the factors impacting the social, environmental, and economic sustainability of an area. To date, there is no concise representation of all aspects of sustainability. This paper aims to fill this gap. A version of sustainability resting on economic, societal, and environmental development as the three main indicators was chosen to inform an ontology called SustainOnt used to organize and analyze relevant data from various sources. The newly-linked data is made available through a dual-platform application aimed at reaching a wide array of audiences. An initial prototype has been designed, using data for a small region, to provide a sustainability index of each city and/or neighborhood area that can be more accessible to people without the means to directly analyze the available data.},
booktitle = {Proceedings of The International Workshop on Semantic Big Data},
articleno = {9},
numpages = {6},
keywords = {sustainability, ontology, linked open data, data integration},
location = {Portland, Oregon},
series = {SBD '20}
}

@inproceedings{10.1145/3243907.3243913,
author = {Rashid, Sabbir M. and De Roure, David and McGuinness, Deborah L.},
title = {A Music Theory Ontology},
year = {2018},
isbn = {9781450364959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243907.3243913},
doi = {10.1145/3243907.3243913},
abstract = {Many existing music ontologies have focused on expressing metadata related to performances or recordings, aiding with recommendations of songs or artists, and studying the psychological affects of music. These music ontologies provide a foundation for describing many practical aspects related to music. We believe further primitives are needed in order to represent written music and provide a foundation for performing analysis of music. We are motivated by questions related to analyzing music that might inform composers or musicians. Informational elements may include possible underlying chords from a set of notes, as well as summaries of key signatures or scales used in a given song. In order to leverage Semantic Web technologies to answer such questions, we present our Music Theory Ontology that expands on existing work by including theoretical concepts that were absent from previous music ontologies. We further describe a methodology for using the ontology to infer new knowledge. We demonstrate this capability by inferring the notes in various scales and chords, and evaluate the ontology in terms of competency question answering.},
booktitle = {Proceedings of the 1st International Workshop on Semantic Applications for Audio and Music},
pages = {6–14},
numpages = {9},
location = {Monterey, CA, USA},
series = {SAAM '18}
}

@inproceedings{10.1145/3624062.3624094,
author = {Kousha, Pouya and Sathu, Vivekananda and Lieber, Matthew and Subramoni, Hari and Panda, Dhabaleswar K.},
title = {Democratizing HPC Access and Use with Knowledge Graphs},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624094},
doi = {10.1145/3624062.3624094},
abstract = {The field of High-Performance Computing (HPC) is undergoing rapid evolution, with an expanding and diverse user base harnessing its unparalleled computational capabilities. As the range of HPC applications grows, newcomers to the field are faced with the daunting task of optimizing their applications for efficient execution on HPC systems. Traditional documentation, often spanning dozens of pages, is cumbersome for finding answers and ill-suited for integration with emerging conversational AI-powered user interfaces like chatbots. Addressing this challenge, we propose a novel HPC ontology crafted to encapsulate HPC runtime relations in a scalable fashion. Our proposed ontology not only facilitates the transfer and querying of this knowledge but also serves as a foundational pillar for our AI-powered Speech Assistant Interface (SAI)[13]. This ensures reproducibility, reliability, and optimal performance when executing tasks. In this paper, we elucidate the relationships and properties underpinning our ontology and showcase how users can interact with knowledge graphs based on our proposed ontology to derive insights.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {243–251},
numpages = {9},
keywords = {Documentation, HPC, Knowledge Graph, Ontology},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3529372.3530915,
author = {Tudhope, Douglas and Gnoli, Claudio and Golub, Koraljka and Mayr, Philipp},
title = {20th European NKOS workshop: networked knowledge organization systems and services},
year = {2022},
isbn = {9781450393454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3529372.3530915},
doi = {10.1145/3529372.3530915},
abstract = {The workshop will explore the potential of Knowledge Organization Systems (KOS), such as classification systems, taxonomies, thesauri, ontologies, and lexical databases, in the context of current developments and possibilities. These tools help model the underlying semantic structure of a domain for purposes of information retrieval, knowledge discovery, language engineering, etc. The workshop provides an opportunity to discuss projects, research and development activities, evaluation approaches, lessons learned, and research findings. The main theme of the workshop is Designing for Cultural Hospitality and Indigenous Knowledge in KOS.},
booktitle = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
articleno = {55},
numpages = {2},
keywords = {vocabulary mapping, thesauri, terminology services, taxonomies, ontologies, knowledge organization systems, classification systems},
location = {Cologne, Germany},
series = {JCDL '22}
}

@inbook{10.5555/3716662.3716790,
author = {Varshney, Kush R.},
title = {Decolonial AI Alignment: Openness, Vi\'{s}eundefineda-Dharma, and Including Excluded Knowledges},
year = {2025},
publisher = {AAAI Press},
abstract = {Prior work has explicated the coloniality of artificial intelligence (AI) development and deployment through mechanisms such as extractivism, automation, sociological essentialism, surveillance, and containment. However, that work has not engaged much with alignment: teaching behaviors to a large language model (LLM) in line with desired values, and has not considered a mechanism that arises within that process: moral absolutism---a part of the coloniality of knowledge. Colonialism has a history of altering the beliefs and values of colonized peoples; in this paper, I argue that this history is recapitulated in current LLM alignment practices and technologies. Furthermore, I suggest that AI alignment be decolonialized using three forms of openness: openness of models, openness to society, and openness to excluded knowledges. This suggested approach to decolonial AI alignment uses ideas from the argumentative moral philosophical tradition of Hinduism, which has been described as an open-source religion. One concept used is vi\'{s}eundefineda-dharma, or particular context-specific notions of right and wrong. At the end of the paper, I provide a suggested reference architecture to work toward the proposed framework.},
booktitle = {Proceedings of the 2024 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {1467–1481},
numpages = {15}
}

@proceedings{10.1145/3696410,
title = {WWW '25: Proceedings of the ACM on Web Conference 2025},
year = {2025},
isbn = {9798400712746},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 2025 ACM Web Conference (WWW '25) took place from April 28 to May 2, 2025, in the Sydney Convention \&amp; Exhibition Centre, Australia. Its logo, featuring the Sydney Harbour Bridge, symbolizes the core "connecting" function of the Web. Formerly known as the International World Wide Web Conference (WWW), this event originated at CERN in 1994 and has long served as the premier venue for presenting and discussing research, development, standards, and applications related to the Web.The 2025 ACM Web Conference (WWW'25) took place from April 28 to May 2, 2025, in the Sydney Convention \&amp; Exhibition Centre, Australia. Its logo, featuring the Sydney Harbour Bridge, symbolizes the core "connecting" function of the Web. Formerly known as the International World Wide Web Conference (WWW), this event originated at CERN in 1994 and has long served as the premier venue for presenting and discussing research, development, standards, and applications related to the Web.Between 2024 and 2025, large language models (LLMs) significantly impacted nearly every industry and many aspects of daily life, prompting transformations in the Web's ecosystem. Acknowledging the importance of LLMs in advancing Web technologies, the call for papers (CFP) across ten research tracks was slightly modified. Three program chairs - Liane Lewin-Eytan, Helen Huang, and Elad Yom-Tov - led the program committee, which used OpenReview to evaluate and accept the research track papers.},
location = {Sydney NSW, Australia}
}

@inproceedings{10.1145/3454127.3456595,
author = {Benabdellah, Abla Chaouni and Benghabrit, Asmaa and Bouhaddou, Imane and Zekhnini, Kamar},
title = {An agent organizational method for modeling the complexity of the design process},
year = {2021},
isbn = {9781450388719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3454127.3456595},
doi = {10.1145/3454127.3456595},
abstract = {The management of the design process is a challenging mission; and most researchers would argue that design is linked to intentional action and it cannot emerge out of complexity. In fact, the interactions between processes, operators, and activities define an unexpected emergent behavior, which is based on complex assumptions such as non-linearity, dynamic and adaptive firm behavior. Therefore, we need a complex thinking. This article proposes to explore how we may deepen our understanding of design process as a complex adaptive system. In fact, this new understanding creates a quite challenge for researches to develop appropriate tools to support design reasoning and decision-making. In this respect, the aim of this paper is first to define the complexity of design process as a complexity of system, by matching its characteristics with those of complex adaptive systems (CAS). Second, the paper provides an agent organizational modelization of the design process in order to support its complexity by following the ASPECS methodology which is an agent-oriented software process for engineering complex systems as well as the knowledge identification of the design process using the RIOCK meta-model.},
booktitle = {Proceedings of the 4th International Conference on Networking, Information Systems \&amp; Security},
articleno = {20},
numpages = {7},
location = {KENITRA, AA, Morocco},
series = {NISS '21}
}

@article{10.1145/3622933,
author = {Jia, Qi and Liu, Yizhu and Ren, Siyu and Zhu, Kenny Q.},
title = {Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches, and Future Directions},
year = {2023},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3622933},
doi = {10.1145/3622933},
abstract = {Abstractive dialogue summarization generates a concise and fluent summary covering the salient information in a dialogue among two or more interlocutors. It has attracted significant attention in recent years based on the massive emergence of social communication platforms and an urgent requirement for efficient dialogue information understanding and digestion. Different from news or articles in traditional document summarization, dialogues bring unique characteristics and additional challenges, including different language styles and formats, scattered information, flexible discourse structures, and unclear topic boundaries. This survey provides a comprehensive investigation of existing work for abstractive dialogue summarization from scenarios, approaches to evaluations. It categorizes the task into two broad categories according to the type of input dialogues, i.e., open-domain and task-oriented, and presents a taxonomy of existing techniques in three directions, namely, injecting dialogue features, designing auxiliary training tasks, and using additional data. A list of datasets under different scenarios and widely accepted evaluation metrics are summarized for completeness. After that, the trends of scenarios and techniques are summarized, together with deep insights into correlations between extensively exploited features and different scenarios. Based on these analyses, we recommend future directions, including more controlled and complicated scenarios, technical innovations and comparisons, publicly available datasets in special domains, and so on.},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {67},
numpages = {38},
keywords = {abstractive summarization, dialogue context modeling, Dialogue summarization}
}

@inproceedings{10.1145/3677779.3677821,
author = {Zhang, Shuai and Guan, Yanzhi and Gu, Zhongyu},
title = {Research on named entity recognition in the field of CNC machine tool design based on deep learningKnowledge map of mechanical field},
year = {2024},
isbn = {9798400709760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677779.3677821},
doi = {10.1145/3677779.3677821},
abstract = {Our goal is to extract entities from the text data of unstructured CNC machine tool design for the construction of knowledge graph. The key entity extraction problem in the construction of CNC machine tool design knowledge graph is studied. In order to realize the recognition of named entities, we have formulated the standard and labeling method of knowledge classification for the field of CNC machine tools, and constructed the corresponding domain data set. In addition, we also propose an entity recognition technology based on RoBertTa-BiLSTM-LCRF for CNC machine tool design text. Firstly, we fine-tune the RoBertTa-BiLSTM-LCRF model using data sets in the field of CNC machine tools, and then use RoBERTa to encode the text to generate a vector representation ; next, we use bidirectional long short-term memory ( BiLSTM ) to extract the features of vectors. Finally, we introduce LCRF as the overall optimization layer of the label, so as to derive the best answer and label the entity.The experimental results show that the F1 value of the model in the data set reaches 71.16 \% ; for most of the key entities, the value of F1 exceeds 65 \% ; this method shows significant advantages in the entity recognition of CNC machine tool design knowledge. It can accurately identify the core entities in the machine tool design knowledge document, and provides a solid data support for the construction of CNC machine tool design knowledge graph.},
booktitle = {Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning},
pages = {257–262},
numpages = {6},
location = {Xi'an, China},
series = {CMNM '24}
}

@inproceedings{10.1145/3313831.3376793,
author = {Das Swain, Vedant and Saha, Koustuv and Reddy, Manikanta D. and Rajvanshy, Hemang and Abowd, Gregory D. and De Choudhury, Munmun},
title = {Modeling Organizational Culture with Workplace Experiences Shared on Glassdoor},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376793},
doi = {10.1145/3313831.3376793},
abstract = {Organizational culture (OC) encompasses the underlying beliefs, values, and practices that are unique to an organization. However, OC is inherently subjective and a coarse construct, and therefore challenging to quantify. Alternatively, self-initiated workplace reviews on online platforms like Glassdoor provide the opportunity to leverage the richness of language to understand OC. In as much, first, we use multiple job descriptors to operationalize OC as a word vector representation. We validate this construct with language used in 650k different Glassdoor reviews. Next, we propose a methodology to apply our construct on Glassdoor reviews to quantify the OC of employees by sector. We validate our measure of OC on a dataset of 341 employees by providing empirical evidence that it helps explain job performance. We discuss the implications of our work in guiding tailored interventions and designing tools for improving employee functioning.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–15},
numpages = {15},
keywords = {glassdoor, organizational culture, social media, wordvector},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{10.1145/3652620.3686250,
author = {Cederbladh, Johan and Eisenberg, Martin and Berardinelli, Luca and Bilic, Damir},
title = {Automation Support for System Simulation and Architecture Layout Design in Cyber-Physical Systems Engineering},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3686250},
doi = {10.1145/3652620.3686250},
abstract = {Simulations have long been part of hardware-centric system domains. Similarly, architecture design is a common practice for complex industrial systems, which comprise many components that can be arranged in different layouts according to given requirements. Configuring simulation models and choosing the architecture design can be time-consuming activities. This paper presents a model-driven approach to automate the simulation configuration and architecture layouting engineering activities by leveraging model-driven optimisation techniques. The approach leverages a research solution, MOMoT (Marrying Optimisation and Model Transformations), an academic tool that combines search-based algorithms and model transformations. MOMoT is extended with two software modules, leveraging the Functional Mock-up Interface standard for simulation configuration and an architectural description language to design architecture layouts. Our solution is presented in the context of Volvo Construction Equipment's industrial use case, which is part of the European-funded project AIDOaRt. Our approach contributes to automated decision support to simulation and architecture design through model-driven optimisation while preserving the organisation's engineering practices.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {299–310},
numpages = {12},
keywords = {simulation, models, optimisation, architecture, layout},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@proceedings{10.1145/3664647,
title = {MM '24: Proceedings of the 32nd ACM International Conference on Multimedia},
year = {2024},
isbn = {9798400706868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We are delighted to welcome you to Melbourne, Australia for ACM Multimedia 2024, the 32nd ACM International Conference on Multimedia. ACM Multimedia is the premier international conference series in the area of multimedia within the field of computer science. Since 1993, ACM Multimedia has been bringing together worldwide researchers and practitioners from academia and industry to present their innovative research and to discuss recent advancements in multimedia.For the first time since the end of the COVID-19 pandemic, this year's conference returns to the Asia-Pacific region and resumes as a full-fledged, inperson event. With no travel restrictions or significant visa challenges, we are excited to once again experience the warmth of face-to-face gatherings, where we can reconnect with colleagues and friends.The enthusiasm and support from the community have been incredible. ACM Multimedia 2024 received over 4,300 main conference submissions, accepting more than 1,100 papers (please refer to the TPC Chairs' message for details). In addition, 10 Grand Challenges were selected from 22 submissions, 18 workshops from 30 submissions, and 8 tutorials from 13 proposals. We've prepared an exciting five-day program: workshops, grand challenges, and tutorials will be held on the 1st and 5th days, with the main conference occupying the middle three days. All accepted papers will be accessible online prior to the conference, and we are working to ensure proceedings are available through the ACM Digital Library around the conference period.This year's conference features three distinguished academic keynote speeches, several prestigious SIGMM award talks, a panel discussion on Generative AI in Multimedia, a refreshed Brave New Idea (BNI) session, and our inaugural industry program.The opening keynote will be delivered by Prof. Pascale Fung from HKUST, a Fellow of AAAI, ACL, and IEEE. Her talk will explore the pressing topic of Agents in the Large Language Model (LLM) Era. Prof. Judy Kay from the University of Sydney, a renowned expert in HCI, user modeling, and ubiquitous computing, will give the second keynote on how to empower individuals to harness and control their multimodal data. The final academic keynote will be presented by Prof. Jiebo Luo from the University of Rochester, a Fellow of ACM, AAAI, IEEE, SPIE, and IAPR, as well as a member of Academia Europaea and the US National Academy of Inventors. He will discuss leveraging LLMs as social multimedia analysis engines.This year, we continue using OpenReview to ensure an open and transparent review process. Thanks to the exceptional efforts of the technical program committee, every paper received at least three reviews before the review announcement. The BNI track has also revamped its review process to align with the main conference, promoting visionary papers. Additionally, we are excited to introduce the industry program to ACM Multimedia for the first time, featuring industry keynote speeches, expert talks, and demonstrations (please refer to the industry chairs' message for further details).We are also committed to making the conference inclusive and accessible. To support students with financial constraints, we have awarded travel grants to at least 25 students from the ACM Multimedia 2024 budget, with an additional 20+ students receiving SIGMM travel grants. Over 20 local students have also been recruited as volunteers, benefiting from complimentary registration. Furthermore, we have arranged childcare facilities to accommodate attendees with young children. A welcome reception will take place on the 2nd day of the conference, followed by a gala dinner on the 3rd day, featuring exciting cultural performances.We hope you find this year's program engaging and thought-provoking and that it offers valuable opportunities to exchange ideas with fellow researchers and practitioners from around the globe. We also encourage you to take time to explore the beautiful city of Melbourne and its surrounding regions.},
location = {Melbourne VIC, Australia}
}

@article{10.1145/3735974,
author = {Ansar, Wazib and Goswami, Saptarsi and Chakrabarti, Amlan and Chakraborty, Basabi},
title = {TexIm FAST: Text-to-Image Encoding for Semantic Similarity Evaluation of Disproportionate Sequences},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {6},
issn = {1551-6857},
url = {https://doi.org/10.1145/3735974},
doi = {10.1145/3735974},
abstract = {One of the principal objectives of Natural Language Processing (NLP) is to generate meaningful representations from text. Improving the informativeness of the representations has led to a tremendous rise in the dimensionality and the memory footprint. It leads to a cascading effect amplifying the complexity of the downstream model by increasing its parameters. The available techniques cannot be applied to cross-modal applications such as text-to-image. To ameliorate these issues, a novel Text-to-Image Fixed-dimensional encoding technique through a self-supervised Variational Auto-Encoder (VAE) for semantic evaluation applying transformers (TexIm FAST) has been proposed in this article. The pictorial representations allow oblivious inference while retaining the linguistic intricacies and are potent in cross-modal applications. TexIm FAST deals with variable-length sequences and generates uniform-dimensional images with over 75\% reduced memory footprint. It enhances the efficiency of the models for downstream tasks by reducing its parameters. The efficacy of TexIm FAST has been extensively analyzed for the task of Semantic Textual Similarity (STS) on a benchmark dataset and two new datasets put forth containing disproportionate sequences. The results demonstrate its exceptional ability to compare disparate-length sequences such as a text with its summary with 3\% improvement in accuracy compared to the SOTA despite having 68\% less parameters.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jul,
articleno = {168},
numpages = {23},
keywords = {Oblivious Inference, Semantic Similarity, Text-to-Image, Text Embedding, Transformers NLP, Variational Auto-Encoder}
}

@inproceedings{10.1145/3243082.3267457,
author = {Oliveira, Yuri and Silveira, Leonardo and Souza, Cidcley},
title = {A Model-Driven Approach to Evolve Recommender Systems},
year = {2018},
isbn = {9781450358675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243082.3267457},
doi = {10.1145/3243082.3267457},
abstract = {Recommender systems have become an important issue on Web applications, but its research is usually focused on algorithms and data optimization. However, as the recommendation techniques improve and these systems become more commonly used in software applications, there is the need of easily adapt and evolve them. To address this need, we propose a model-driven approach to evolve recommender systems and present an architecture solution from our research, using an events management system as the domain for an use-case scenario. Future work might demonstrate the architecture feasibility.},
booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
pages = {169–172},
numpages = {4},
keywords = {software architecture, recommender systems, model-driven engineering},
location = {Salvador, BA, Brazil},
series = {WebMedia '18}
}

@article{10.1145/3588911,
author = {Omar, Reham and Dhall, Ishika and Kalnis, Panos and Mansour, Essam},
title = {A Universal Question-Answering Platform for Knowledge Graphs},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3588911},
doi = {10.1145/3588911},
abstract = {Knowledge from diverse application domains is organized as knowledge graphs (KGs) that are stored in RDF engines accessible in the web via SPARQL endpoints. Expressing a well-formed SPARQL query requires information about the graph structure and the exact URIs of its components, which is impractical for the average user. Question answering (QA) systems assist by translating natural language questions to SPARQL. Existing QA systems are typically based on application-specific human-curated rules, or require prior information, expensive pre-processing and model adaptation for each targeted KG. Therefore, they are hard to generalize to a broad set of applications and KGs. In this paper, we propose KGQAn, a universal QA system that does not need to be tailored to each target KG. Instead of curated rules, KGQAn introduces a novel formalization of question understanding as a text generation problem to convert a question into an intermediate abstract representation via a neural sequence-to-sequence model. We also develop a just-in-time linker that maps at query time the abstract representation to a SPARQL query for a specific KG, using only the publicly accessible APIs and the existing indices of the RDF store, without requiring any pre-processing. Our experiments with several real KGs demonstrate that KGQAn is easily deployed and outperforms by a large margin the state-of-the-art in terms of quality of answers and processing time, especially for arbitrary KGs, unseen during the training.},
journal = {Proc. ACM Manag. Data},
month = may,
articleno = {57},
numpages = {25},
keywords = {RDF, just-in-time entity and relation linking, knowledge graphs, natural language question answering, seq2seq models}
}

@inproceedings{10.1145/3723010.3723036,
author = {B\"{o}hm, Karsten},
title = {Towards a Semantic Representation of Framework Recommendations for Curricular Specifications in Higher Education},
year = {2025},
isbn = {9798400712821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3723010.3723036},
doi = {10.1145/3723010.3723036},
abstract = {Curricular specifications play an important role in the Higher Education sector and the domain of Computer Science and Software Engineering is characterized by a wide range of education programs with a broad range of topic. Therefore, recommendation frameworks play an important role and their usage is beneficial for a unification of education profiles in a systematic way. This  research is contributing to this development by exploring how a recommendation for the domain of Business Informatics in German speaking countries can be improved by formalizing the recommendations in a semantic model that relies on sophisticated European ontologies in the domain like the European Learning Model (ELM) and related data models. It employs Generative Artificial Intelligence Systems to create semantic models in an experimental way and evaluates the resulting model quality. The results show that a formalization using GenAI has a high potential, but currently also shows deficits in the correctness of the resulting models, requiring human oversight during the model creation.},
booktitle = {Proceedings of the 6th European Conference on Software Engineering Education},
pages = {154–160},
numpages = {7},
keywords = {Business Informatics, Competence Specification, European Learning Model, Higher Education, Learning Framework, Semantic Web},
location = {
},
series = {ECSEE '25}
}

@article{10.1145/3577204,
author = {Boudi, Zakaryae and Wakrime, Abderrahim Ait and Toub, Mohamed and Haloua, Mohamed},
title = {A Deep Reinforcement Learning Framework with Formal Verification},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {1},
issn = {0934-5043},
url = {https://doi.org/10.1145/3577204},
doi = {10.1145/3577204},
abstract = {Artificial Intelligence (AI) and data are reshaping organizations and businesses. Human Resources (HR) management and talent development make no exception, as they tend to involve more automation and growing quantities of data. Because this brings implications on workforce, career transparency, and equal opportunities, overseeing what fuels AI and analytical models, their quality standards, integrity, and correctness becomes an imperative for those aspiring to such systems. Based on an ontology transformation to B-machines, this article presents an approach to constructing a valid and error-free career agent with Deep Reinforcement Learning (DRL). In short, the agent's policy is built on a framework we called Multi State-Actor (MuStAc) using a decentralized training approach. Its purpose is to predict both relevant and valid career steps to employees, based on their profiles and company pathways (observations). Observations can comprise various data elements such as the current occupation, past experiences, performance, skills, qualifications, and so on. The policy takes in all these observations and outputs the next recommended career step, in an environment set as the combination of an HR ontology and an Event-B model, which generates action spaces with respect to formal properties. The Event-B model and formal properties are derived using OWL to B transformation.},
journal = {Form. Asp. Comput.},
month = mar,
articleno = {5},
numpages = {17},
keywords = {Atelier B, Model Transformation, Safe RL, Safe AI, AI Control, Formal Verification, Event-B}
}

@inproceedings{10.1145/3331184.3331427,
author = {Firsov, Anton and Bugay, Vladimir and Karpenko, Anton},
title = {USEing Transfer Learning in Retrieval of Statistical Data},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331427},
doi = {10.1145/3331184.3331427},
abstract = {DSSM-like models showed good results in retrieval of short documents that semantically match the query. However, these models require large collections of click-through data that are not available in some domains. On the other hand, the recent advances in NLP demonstrated the possibility to fine-tune language models and models trained on one set of tasks to achieve a state of the art results on a multitude of other tasks or to get competitive results using much smaller training sets. Following this trend, we combined DSSM-like architecture with USE (Universal Sentence Encoder) and BERT (Bidirectional Encoder Representations from Transformers) models in order to be able to fine-tune them on a small amount of click-through data and use them for information retrieval. This approach allowed us to significantly improve our search engine for statistical data.},
booktitle = {Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1391–1392},
numpages = {2},
keywords = {transfer learning, language model, information retrieval},
location = {Paris, France},
series = {SIGIR'19}
}

@article{10.1145/3577925,
author = {Schiappa, Madeline C. and Rawat, Yogesh S. and Shah, Mubarak},
title = {Self-Supervised Learning for Videos: A Survey},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3577925},
doi = {10.1145/3577925},
abstract = {The remarkable success of deep learning in various domains relies on the availability of large-scale annotated datasets. However, obtaining annotations is expensive and requires great effort, which is especially challenging for videos. Moreover, the use of human-generated annotations leads to models with biased learning and poor domain generalization and robustness. As an alternative, self-supervised learning provides a way for representation learning that does not require annotations and has shown promise in both image and video domains. In contrast to the image domain, learning video representations are more challenging due to the temporal dimension, bringing in motion and other environmental dynamics. This also provides opportunities for video-exclusive ideas that advance self-supervised learning in the video and multimodal domains. In this survey, we provide a review of existing approaches on self-supervised learning focusing on the video domain. We summarize these methods into four different categories based on their learning objectives: (1) pretext tasks, (2) generative learning, (3) contrastive learning, and (4) cross-modal agreement. We further introduce the commonly used datasets, downstream evaluation tasks, insights into the limitations of existing works, and the potential future directions in this area.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {288},
numpages = {37},
keywords = {visual-language models, multimodal learning, representation learning, zero-shot learning, video understanding, deep learning, Self-supervised learning}
}

@article{10.1145/3746658,
author = {Ozdemir, Anil and Odaci, Berke and Tanatar Baruh, Lorans and Varol, Onur and Balcisoy, Selim},
title = {Enhancing Cultural Heritage Archive Analysis via Automated Entity Extraction and Graph-Based Representation Learning},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4673},
url = {https://doi.org/10.1145/3746658},
doi = {10.1145/3746658},
abstract = {Recent efforts to digitize textual, visual, and physical forms of cultural heritage require advanced tools for preservation and analysis. The availability of extensive online data creates a need for intelligent systems to help users and archivists understand latent relationships in these collections. A major challenge in cultural heritage studies is the labor-intensive process of analyzing these materials. Inconsistent linguistic terms and ambiguous concepts in digital documents make it difficult to uncover relationships without expert supervision. Moreover, while advanced models based on large-scale pretraining demonstrate strong performance in extracting semantic relationships, they depend on extensive pretraining on large external datasets, limiting their applicability for smaller or specialized collections. We propose a system that combines natural language processing for entity extraction with graph representation learning to model relationships among documents, categories, and n-grams, resulting in a fully-connected network representation. Unlike methods requiring large-scale pretraining, our approach operates effectively using only the information available in the dataset itself, making it particularly suited for smaller cultural heritage document collections. The system extracts significant terms from document metadata, produces embeddings for each document, and uses these embeddings to build a recommendation system for entity discovery. We tested the system on a collection of early 20th-century documents from Crete, evaluating its performance against alternative methods in collaboration with experts from the archival research organization SALT. This approach not only facilitates deeper insights into smaller, specialized collections but also reduces dependency on vast external training resources, enhancing its practical utility in cultural heritage studies.},
note = {Just Accepted},
journal = {J. Comput. Cult. Herit.},
month = jul,
keywords = {Natural Language Processing, Machine Learning, Graph Representation Learning, Recommendation Systems}
}

@article{10.1613/jair.1.13167,
author = {Koto, Fajri and Baldwin, Timothy and Lau, Jey Han},
title = {FFCI: A Framework for Interpretable Automatic Evaluation of Summarization},
year = {2022},
issue_date = {May 2022},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {73},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.13167},
doi = {10.1613/jair.1.13167},
abstract = {In this paper, we propose FFCI, a framework for fine-grained summarization evaluation that comprises four elements: faithfulness (degree of factual consistency with the source), focus (precision of summary content relative to the reference), coverage (recall of summary content relative to the reference), and inter-sentential coherence (document fluency between adjacent sentences). We construct a novel dataset for focus, coverage, and inter-sentential coherence, and develop automatic methods for evaluating each of the four dimensions of FFCI based on cross-comparison of evaluation metrics and model-based evaluation methods, including question answering (QA) approaches, semantic textual similarity (STS), next-sentence prediction (NSP), and scores derived from 19 pre-trained language models. We then apply the developed metrics in evaluating a broad range of summarization models across two datasets, with some surprising findings.},
journal = {J. Artif. Int. Res.},
month = may,
numpages = {55},
keywords = {neural networks, machine learning, natural language}
}

@inproceedings{10.1145/3277139.3277154,
author = {Lin, Menglong and Yao, Yiping},
title = {Modeling framework of general simulation model based on model template},
year = {2018},
isbn = {9781450364867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3277139.3277154},
doi = {10.1145/3277139.3277154},
abstract = {The model is the core of the simulation system, aiming at the problems of low reuse efficiency of the current simulation model and long model development cycle, this paper proposes a generic model generation technology framework. First, the simulation model is split into independent components through military concept analysis, and then the common concept of the same model is extracted to form a model template with a unified description specification. The specific simulation model is instantiated quickly through parameterized configuration. The modeling framework can effectively support the generation and application of the platform models in military simulation systems, and has great scalability and reusability.},
booktitle = {Proceedings of the 1st International Conference on Information Management and Management Science},
pages = {223–227},
numpages = {5},
keywords = {simulation model, modeling framework, model template, component-based},
location = {Chengdu, China},
series = {IMMS '18}
}

@inproceedings{10.1145/3658271.3658339,
author = {Molina De Armas, Elvismary and Hamazaki Da Silva, Geiza Maria and Torres Izquierdo, Yenier and Lemos, Melissa and De Lima Britto, Paulo Vin\'{\i}cius and Corseuil, Eduardo Thadeu and Souza Garcia, Robinson Luiz},
title = {A Proposal of a Knowledge Graph for Digital Engineering Systems Integration for Operation and Maintenance Activities in Industrial Plants},
year = {2024},
isbn = {9798400709968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658271.3658339},
doi = {10.1145/3658271.3658339},
abstract = {Context: Over the last years, we have observed Knowledge Graphs (KGs) being used more and more as a tool for representing knowledge, data integration and querying data. Problem: There are many distinguished yet partially-integrated information management systems used to support the life-cycle of Oil and Gas industrial plants. Our approach considers a 3D plants viewer system, a visual navigation system on platforms, and the integrated intelligent search system. However, these systems lack a semantic integration that can guide the user actions over each functionality for a unique asset. Solution: This paper presents the use of KGs to represent and help monitoring and controlling operational and maintenance activities within an Oil and Gas industrial environment. Our approach highlights the challenges and initial work required to establish a fully-integrated management domain, where the execution of the aforementioned activities can easily be managed. SI Theory: This study draws inspiration from Representation Theory, which posits that an information system faithfully mirrors specific phenomena occurring in the physical world. Method: To develop this work, it was necessary to review the literature related to the development of KGs and ontologies. The generated KG was developed using well-established standards like the Industrial Data Ontology (IDO), and the Capital Facilities Information Handover Specification (CFIHOS), complemented with the use of other ontologies. Summary of Results: A prototype of the conceptual KG was implemented, verifying the viability of our approach for data integration. Contributions and Impact in IS area: The resulted graph contains the main terms in compliance with international semantic standards for representing operational and maintenance activities data associated with facilities involved in Oil and Gas production. Finally, the KG resulting from this effort can be further extended through the incorporation of new tools and subdomains in the industrial plants life-cycle.},
booktitle = {Proceedings of the 20th Brazilian Symposium on Information Systems},
articleno = {67},
numpages = {10},
keywords = {Data Integration, Digital Engineering, Industrial Plants, Knowledge Graphs, Ontology, Operation and Maintenance activities},
location = {Juiz de Fora, Brazil},
series = {SBSI '24}
}

@inproceedings{10.1145/3331453.3361301,
author = {Ma, Zhiyi and Wang, Xiaoxi and Chen, Hongjie and Qiu, Ye},
title = {A Modeling Tool for Sensor-based Mobile Applications},
year = {2019},
isbn = {9781450362948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331453.3361301},
doi = {10.1145/3331453.3361301},
abstract = {With the variety of the sensors on mobile devices, sensor-based mobile applications are constantly emerging. Usually, it is necessary to model sensor-based mobile applications, and the modeling requires the domain metamodels and the modeling tools. This paper presents a set of the modeling concepts from the aspects of sensor data perception, sensor data understanding, and sensor data processing, and then builds a metamodel with UML profile mechanism. Moreover, the paper proposes a modeling tool architecture and discusses the design and implementation of key modules in the architecture based on this metamodel. According to the metamodel, architecture, and key modules, the developers can build the tools to model sensor-based mobile applications and generate key code to greatly improve the development productivity and quality of the applications.},
booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
articleno = {130},
numpages = {6},
keywords = {Sensors, Modeling tool, Mobile applications, Metamodel},
location = {Sanya, China},
series = {CSAE '19}
}

@inproceedings{10.1145/3538637.3538838,
author = {He, Fang and Zhang, Xiaoyang and Wang, Dan},
title = {Cement-α: an ontology-based data access system for building analytics with multiple data sources},
year = {2022},
isbn = {9781450393973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3538637.3538838},
doi = {10.1145/3538637.3538838},
abstract = {To enhance the portability of data-driven building analytics across buildings, data models have been developed to provide unified representations of building data, such as Brick, which defines how to construct an ontology to capture the semantics of building entities and the relationships among them. Unfortunately, existing data models are all developed for the data of building systems, e.g., the HVAC system of a building. Yet, building analytics can require data that are external to a building system, e.g., weather data for cooling load forecasting. Such external data can be accessed from external sources, e.g., observatory, yet these data have their own data models and storage methods.To enable portable data access from multiple data sources for building analytics, in this paper, we firstly define building periphery data, and then we study the approach to develop a data model to support building analytics which require both building data and building periphery data. We further develop Cement-α, an ontology-based data access system to extract both building and building periphery data. We evaluate the Cement-α qualitatively and quantitatively by using four real-world building analytics, and find that the development effort of building analytics with multiple data sources can be reduced by an average of 57.2\%.},
booktitle = {Proceedings of the Thirteenth ACM International Conference on Future Energy Systems},
pages = {436–437},
numpages = {2},
keywords = {smart building, machine learning, data analytics, data access},
location = {Virtual Event},
series = {e-Energy '22}
}

@inproceedings{10.1145/3418688.3418696,
author = {Alaa El Din Talha, Shorouk},
title = {A Semantic Based Annotation Technique for the Internet of Things},
year = {2020},
isbn = {9781450387866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3418688.3418696},
doi = {10.1145/3418688.3418696},
abstract = {Due to the recent deployments of Internet of Things (IoT) technologies in many real-life applications, enormous amount of diverse and real-time streams of data are being generated. To facilitate dealing with the heterogeneity of IoT data streams, semantic technologies became the main element to guarantee data interoperability, with its nature to unify concepts, extend knowledge, and share a machine-readable representation of data. In this paper, we propose an adaptable approach for IoT data semantic annotation, to achieve an efficient way to enrich data semantically considering its heterogeneity, volume, and frequency. A use case is implemented using Apache Kafka, Spark to deal with data streams in real-time, and a Semantic ontology model extending the SOSA standard ontology is developed to annotate data into enriched Resource Description Framework (RDF) triples.},
booktitle = {Proceedings of the 2020 3rd International Conference on Computing and Big Data},
pages = {42–47},
numpages = {6},
keywords = {Internet of Things, Ontology, Semantic Annotation, stream processing},
location = {Taichung, Taiwan},
series = {ICCBD '20}
}

@inproceedings{10.1145/3437963.3441784,
author = {Balashankar, Ananth and Beutel, Alex and Subramanian, Lakshminarayanan},
title = {Enhancing Neural Recommender Models through Domain-Specific Concordance},
year = {2021},
isbn = {9781450382977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437963.3441784},
doi = {10.1145/3437963.3441784},
abstract = {Recommender models trained on historical observational data alone can be brittle when domain experts subject them to counterfactual evaluation. In many domains, experts can articulate common, high-level mappings or rules between categories of inputs (user's history) and categories of outputs (preferred recommendations). One challenge is to determine how to train recommender models to adhere to these rules. In this work, we introduce the goal of domain-specific concordance: the expectation that a recommender model follow a set of expert-defined categorical rules. We propose a regularization-based approach that optimizes for robustness on rule-based input perturbations. To test the effectiveness of this method, we apply it in a medication recommender model over diagnosis-medicine categories, and in movie and music recommender models, on rules over categories based on movie tags and song genres. We demonstrate that we can increase the category-based robustness distance by up to 126\% without degrading accuracy, but rather increasing it by up to 12\% compared to baseline models in the popular MIMIC-III, MovieLens-20M and Last.fm Million Song datasets.},
booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
pages = {1002–1010},
numpages = {9},
keywords = {recommender systems, information systems},
location = {Virtual Event, Israel},
series = {WSDM '21}
}

@article{10.1109/TCBB.2020.3010975,
author = {Nguyen, Trinh-Trung-Duong and Ho, Quang-Thai and Le, Nguyen-Quoc-Khanh and Phan, Van-Dinh and Ou, Yu-Yen},
title = {Use Chou's 5-Steps Rule With Different Word Embedding Types to Boost Performance of Electron Transport Protein Prediction Model},
year = {2020},
issue_date = {March-April 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {2},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.3010975},
doi = {10.1109/TCBB.2020.3010975},
abstract = {Living organisms receive necessary energy substances directly from cellular respiration. The completion of electron storage and transportation requires the process of cellular respiration with the aid of electron transport chains. Therefore, the work of deciphering electron transport proteins is inevitably needed. The identification of these proteins with high performance has a prompt dependence on the choice of methods for feature extraction and machine learning algorithm. In this study, protein sequences served as natural language sentences comprising words. The nominated word embedding-based feature sets, hinged on the word embedding modulation and protein motif frequencies, were useful for feature choosing. Five word embedding types and a variety of conjoint features were examined for such feature selection. The support vector machine algorithm consequentially was employed to perform classification. The performance statistics within the 5-fold cross-validation including average accuracy, specificity, sensitivity, as well as MCC rates surpass 0.95. Such metrics in the independent test are 96.82, 97.16, 95.76 percent, and 0.9, respectively. Compared to state-of-the-art predictors, the proposed method can generate more preferable performance above all metrics indicating the effectiveness of the proposed method in determining electron transport proteins. Furthermore, this study reveals insights about the applicability of various word embeddings for understanding surveyed sequences.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jul,
pages = {1235–1244},
numpages = {10}
}

@inbook{10.1145/3382097.3382113,
title = {Good and bad modeling practices},
year = {2020},
isbn = {9781450376174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382097.3382113},
abstract = {Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.},
booktitle = {Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL}
}

@article{10.1162/coli_a_00378,
author = {Ranta, Aarne and Angelov, Krasimir and Gruzitis, Normunds and Kolachina, Prasanth},
title = {Abstract Syntax as Interlingua: Scaling Up the Grammatical Framework
                    from Controlled Languages to Robust Pipelines},
year = {2020},
issue_date = {June 2020},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {46},
number = {2},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00378},
doi = {10.1162/coli_a_00378},
abstract = {Abstract syntax is an interlingual representation used in compilers. Grammatical Framework (GF) applies the abstract syntax idea to natural languages. The development of GF started in 1998, first as a tool for controlled language implementations, where it has gained an established position in both academic and commercial projects. GF provides grammar resources for over 40 languages, enabling accurate generation and translation, as well as grammar engineering tools and components for mobile and Web applications. On the research side, the focus in the last ten years has been on scaling up GF to wide-coverage language processing. The concept of abstract syntax offers a unified view on many other approaches: Universal Dependencies, WordNets, FrameNets, Construction Grammars, and Abstract Meaning Representations. This makes it possible for GF to utilize data from the other approaches and to build robust pipelines. In return, GF can contribute to data-driven approaches by methods to transfer resources from one language to others, to augment data by rule-based generation, to check the consistency of hand-annotated corpora, and to pipe analyses into high-precision semantic back ends. This article gives an overview of the use of abstract syntax as interlingua through both established and emerging NLP applications involving GF.},
journal = {Comput. Linguist.},
month = jun,
pages = {425–486},
numpages = {62}
}

@inproceedings{10.1145/3384544.3384549,
author = {Khan, Rimsha and Azam, Farooque and Maqbool, Bilal and Anwar, Muhammad Waseem},
title = {A Framework for Automated Reengineering of BPMN Models by Excluding Inefficient Activities},
year = {2020},
isbn = {9781450376655},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384544.3384549},
doi = {10.1145/3384544.3384549},
abstract = {Business Process Reengineering (BPR), originally floated in the early 1990s, is gaining importance in industry and academia. BPR helps the organization rethink their work rationally by redesigning their current processes and resource consumption. Due to the high rate of software evolution, there is a need to run legacy systems on a new computing platform. BPMN models are subject to erroneous or unnecessary activities that are taking too many resources. Such process models are leading to additional cost and effort. Re-engineering help in improving the legacy system or in this context a set of legacy processes to perform better than before. This work presents a framework for automatic reengineering of a BPMN by identifying activities that are taking too much time and resources but are insignificant to the business process. An extensive literature review has led to the extraction of three important parameters based on which the business process activities can be evaluated as necessary or unnecessary i.e. time, resources and priority of an activity. The proposed model has been validated using a case study on the Claim Management System. This work shall be beneficial for the research community and developers targeting construction of a BPR tool},
booktitle = {Proceedings of the 2020 9th International Conference on Software and Computer Applications},
pages = {147–151},
numpages = {5},
keywords = {BPMN, Business Process Re-engineering (BPR), Resource Optimization, Software Automation},
location = {Langkawi, Malaysia},
series = {ICSCA '20}
}

@inproceedings{10.1145/3428502.3428611,
author = {Loutsaris, Michalis Avgerinos and Charalabidis, Yannis},
title = {Legal informatics from the aspect of interoperability: a review of systems, tools and ontologies},
year = {2020},
isbn = {9781450376747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3428502.3428611},
doi = {10.1145/3428502.3428611},
abstract = {In the reality of globalization, the legislation of every country needs to be followed in order to achieve a well-organized globalization process because the rule of Law is a cornerstone, a fundamental foundation of every democratic state and it should be observed and respected by all in the society. The economic and industrial globalization has increased international competition and given rise to the need for an increasingly integrated and evolving legal system but the fundamental debates over globalization of the 1990s more or less petered out, without leading to a clear consensus. So, society is still overwhelmed with an over-load of legal information while in the era of Digital Transformation, technologies such as Big data, artificial intelligence, machine learning, blockchain, 3D promise to have a profoundly disruptive effect on the industry, business models, governance models and on the way we interact with each other in society. However, there is not a legal information system capable of supporting the legislation of all countries in order to facilitate the above operation but there are many initiatives in order to develop legal ontologies. These legal ontologies in combination with the disruptive technologies can be help the problem of fragmented legal information across-border in order to create the Big Linked Open Legal Data.},
booktitle = {Proceedings of the 13th International Conference on Theory and Practice of Electronic Governance},
pages = {731–737},
numpages = {7},
keywords = {Legal text mining, Legal ontologies, Legal information systems, Legal Interoperability, Legal Editing Tools},
location = {Athens, Greece},
series = {ICEGOV '20}
}

@article{10.1145/3393692,
author = {Kaur, Manpreet and Salim, Flora D. and Ren, Yongli and Chan, Jeffrey and Tomko, Martin and Sanderson, Mark},
title = {Joint Modelling of Cyber Activities and Physical Context to Improve Prediction of Visitor Behaviors},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {1550-4859},
url = {https://doi.org/10.1145/3393692},
doi = {10.1145/3393692},
abstract = {This article investigates the cyber-physical behavior of users in a large indoor shopping mall by leveraging anonymized (opt in) Wi-Fi association and browsing logs recorded by the mall operators. Our analysis shows that many users exhibit a high correlation between their cyber activities and their physical context. To find this correlation,propose a mechanism to semantically label a physical space with rich categorical information from DBPedia concepts and compute a contextual similarity that represents a user’s activities with the mall context. We demonstrate the application of cyber-physical contextual similarity in two situations: user visit intent classification and future location prediction. The experimental results demonstrate that exploitation of contextual similarity significantly improves the accuracy of such applications.},
journal = {ACM Trans. Sen. Netw.},
month = aug,
articleno = {28},
numpages = {25},
keywords = {user profiling, user modelling, shopping behaviour, semantic enrichment, retail behaviour, recommender systems, movement analysis, logs analysis, location prediction, knowledge graph, intent recognition, indoor trajectory, cyber-physical, context-aware computing, check-ins, Wi-Fi}
}

@article{10.1145/3229087,
author = {Tomlein, Mat\'{u}\v{s} and Gr\o{}nb\ae{}k, Kaj},
title = {Augmented Reality Supported Modeling of Industrial Systems to Infer Software Configuration},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {EICS},
url = {https://doi.org/10.1145/3229087},
doi = {10.1145/3229087},
abstract = {This paper proposes and evaluates an approach for building models of installed industrial Cyber-Physical Systems using augmented reality on smartphones. It proposes a visual language for annotating devices, containers, flows of liquids and networking connections in augmented reality. Compared to related work, it provides a more lightweight and flexible approach for building 3D models of industrial systems. The models are further used to automatically infer software configuration of controllable industrial products. This addresses a common problem of error-prone and time-consuming configuration of industrial systems in the current practice. The proposed approach is evaluated in a study with 16 domain experts. The study participants are involved in creating a model of an industrial system for water treatment. Their comments show that the approach can enable a less error-prone configuration for more complex systems. Opportunities for improvement in usability and reflections on the potential of the approach are discussed.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {5},
numpages = {17},
keywords = {modeling, iot, configuration, augmented reality}
}

@inproceedings{10.1145/3429889.3429909,
author = {Xiao, Rui and Hu, Fengju and Pei, Wei and Bie, Minkun},
title = {Research on Traditional Chinese Medicine Data Mining Model Based on Traditional Chinese Medicine Basic Theories and Knowledge Graphs},
year = {2020},
isbn = {9781450388603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429889.3429909},
doi = {10.1145/3429889.3429909},
abstract = {In recent years, great progress has been made in the study of knowledge graph in various fields, and it has become a hot topic in Traditional Chinese Medicine (TCM) related fields. This paper utilizes a Chinese Herbal Medicine collection, which includes 537 medicines, retrieved from a hospital affiliated with a TCM university, as data source; referenced the Chinese Pharmacopoeia for building the knowledge graph founded on the basic TCM theory. Via associating the prescription with drug properties, taste and meridian tropism of Chinese medicine and visualizing the complex network of Chinese medicine prescription from a novel perspective, the rules in the prescription can be mined in a deeper level, which has a strong practical reference value for developing new clinical medicine and studying the prescription data mining.},
booktitle = {Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences},
pages = {102–106},
numpages = {5},
keywords = {Knowledge Graph, Data Mining, Basic Theories of TCM},
location = {Beijing, China},
series = {ISAIMS '20}
}

@inproceedings{10.1145/3227609.3227659,
author = {Loukachevitch, Natalia and Ivanov, Kirill and Dobrov, Boris},
title = {Thesaurus-Based Topic Models and Their Evaluation},
year = {2018},
isbn = {9781450354899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3227609.3227659},
doi = {10.1145/3227609.3227659},
abstract = {In this paper we study thesaurus-based topic models and evaluate them from the point of view of topic coherence. Thesaurus-based topic model enhances scores of related terms found in the same text, which means that the model encourages these terms to be in the same topics. We evaluate various variants of such models. At the first step, we carry out manual evaluation of the obtained topics. At the second step, we study the possibility to use the collected manual data for evaluating new variants of thesaurus-based models, propose a method and select the best of its parameters in cross-validation. At the third step, we apply the created evaluation method to estimate the influence of word frequencies on adding thesaurus relations during generating topic models.},
booktitle = {Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics},
articleno = {11},
numpages = {9},
keywords = {topic models, thesaurus, content-based analysis},
location = {Novi Sad, Serbia},
series = {WIMS '18}
}

@inproceedings{10.1145/3368756.3369029,
author = {Kouissi, Mohamed and Ghouch, Nihad El and En-naimi, El Mokhtar},
title = {New approach for modeling and developing multi-agent systems based on case based reasoning},
year = {2019},
isbn = {9781450362894},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368756.3369029},
doi = {10.1145/3368756.3369029},
abstract = {In this paper, we present a multi agent architecture based on Incremental Dynamic Case-Based Reasoning (IDCBR). Our approach inherits from Model Driven Architecture (MDA [11]), which aims to design, develop and implement models or meta-models of multi-agent systems that we build from AUML. We have designed a generic and scalable class diagram to develop complex multi-agent systems [3] for Decision Support System based on IDCBR to predict and anticipate a dynamic situation. The source code of the models is generated by an open source tool called AndroMDA [13]. The model and source code will be used to design and develop applications to implement and simulate multi-agent models for Management of Common Renewable Resources [4].},
booktitle = {Proceedings of the 4th International Conference on Smart City Applications},
articleno = {49},
numpages = {8},
keywords = {simulation, multi agents systems, model driven architecture (MDA), incremental dynamic case-based reasoning (IDCBR), decision making, common renewable resources, Jade platform, AUML},
location = {Casablanca, Morocco},
series = {SCA '19}
}

@inproceedings{10.1145/3265689.3265708,
author = {Huang, Yadong and Chai, Yueting and Liu, Yi and Zhang, Anting and Wu, Hao},
title = {Modeling and Analysis of Demand for Personalized Portal},
year = {2018},
isbn = {9781450365871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265689.3265708},
doi = {10.1145/3265689.3265708},
abstract = {E-commerce1 has experienced great growth during the past two decades, which changes the consumption mode of consumers significantly. All researchers from institutions and enterprises want to identify and satisfy the personalized demand intelligently and conveniently by every possible means. In this paper, we proposed smart demand strategy based on holographic demand and model of transaction subject, which is applicable for the decentralized, disintermediated, intelligent e-commerce platform. User demands are classified from two aspects, which will improve the accuracy of demand obtaining. In addition, from standardized description of demand and full life cycle tracking of demand, the user demand will be identified comprehensively. Meanwhile, models of the user, including physical, preference, knowledge, digital label, and social attributes, are built based on his standard description and fragmented description from his interactive objects, which results in a holographic demander. Then, smart demand strategy, i.e. demand forecast and recommendation are proposed. Based on trigger point and demand attributes, the user demand will be updated in real time, which ensures the accuracy of demand accusation and recommendation. The relationship and help degree, based on the interactions within the cyberspace, are important references in filtering the recommendation.},
booktitle = {Proceedings of the 3rd International Conference on Crowd Science and Engineering},
articleno = {19},
numpages = {8},
keywords = {Subject Model, Personalized portal, Holographic demand, E-commerce, Demand Strategy},
location = {Singapore, Singapore},
series = {ICCSE'18}
}

@article{10.5555/3546258.3546539,
author = {Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
title = {Contrastive estimation reveals topic posterior information to linear models},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Contrastive learning is an approach to representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we prove that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers trained on these representations perform well in document classification tasks with very few training examples.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {281},
numpages = {31},
keywords = {representation learning, latent Dirichlet allocation, contrastive estimation}
}

@inproceedings{10.1145/3350768.3351795,
author = {Bispo, Cristiana and Fernandes, Sergio and Magalh\~{a}es, Ana Patr\'{\i}cia},
title = {Strategies for Use Case Modeling: A Systematic Literature Review},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3351795},
doi = {10.1145/3350768.3351795},
abstract = {A major challenge in teaching use-case modeling (UCM) is to mitigate the difficulties of students that prevent them from producing use-case models with quality. The strategies for UCM are scattered in the literature in several areas, and may not be known to the students, who therefore fail to receive the benefits that would mitigate their difficulties. This paper aims to present a systematic literature review (SLR) to identify, gather and analyze strategies for UCM. During the SLR, two thousand two hundred sixty-six studies published between 2008 and 2018 were returned from 6 bases (ACM, IEEE, Scopus, Science Direct, SpringerLink and Engineering Village), which resulted in the selection of 39 primary studies. These were classified, following the coding procedures of Grounded Theory, into 13 categories of different strategies for UCM. The results can help teachers in the adoption of the most appropriate UCM strategies for their students. Besides, they provide a quick reference for teachers and researchers interested in conducting additional studies on teaching strategies for UCM.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {254–263},
numpages = {10},
keywords = {Use Case, Systematic Review, Strategy for Use Case Modeling, Software Modeling, Requirement},
location = {Salvador, Brazil},
series = {SBES '19}
}

@article{10.1145/3543826,
author = {Demir, Seniz},
title = {Turkish Data-to-Text Generation Using Sequence-to-Sequence Neural Networks},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3543826},
doi = {10.1145/3543826},
abstract = {End-to-end data-driven approaches lead to rapid development of language generation and dialogue systems. Despite the need for large amounts of well-organized data, these approaches jointly learn multiple components of the traditional generation pipeline without requiring costly human intervention. End-to-end approaches also enable the use of loosely aligned parallel datasets in system development by relaxing the degree of semantic correspondences between training data representations and text spans. However, their potential in Turkish language generation has not yet been fully exploited. In this work, we apply sequence-to-sequence (Seq2Seq) neural models to Turkish data-to-text generation where the input data given in the form of a meaning representation is verbalized. We explore encoder-decoder architectures with attention mechanism in unidirectional, bidirectional, and stacked recurrent neural network (RNN) models. Our models generate one-sentence biographies and dining venue descriptions using a crowdsourced dataset where all field value pairs that appear in meaning representations are fully captured in reference sentences. To support this work, we also explore the performances of our models on a more challenging dataset, where the content of a meaning representation is too large to fit into a single sentence, and hence content selection and surface realization need to be learned jointly. This dataset is retrieved by coupling introductory sentences of person-related Turkish Wikipedia articles with their contained infobox tables. Our empirical experiments on both datasets demonstrate that Seq2Seq models are capable of generating coherent and fluent biographies and venue descriptions from field value pairs. We argue that the wealth of knowledge residing in our datasets and the insights obtained from this study hold the potential to give rise to the development of new end-to-end generation approaches for Turkish and other morphologically rich languages.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = dec,
articleno = {37},
numpages = {27},
keywords = {Wikipedia, Turkish, sequence-to-sequence model, Data-to-text generation}
}

@inproceedings{10.1145/3269206.3269230,
author = {G\"{u}zel Kalayci, Elem and Xiao, Guohui and Ryzhikov, Vladislav and Kalayci, Tahir Emre and Calvanese, Diego},
title = {Ontop-temporal: A Tool for Ontology-based Query Answering over Temporal Data},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3269230},
doi = {10.1145/3269206.3269230},
abstract = {We present Ontop-temporal, an extension of the ontology-based data access system Ontop for query answering with temporal data and ontologies. Ontop is a system to answer SPARQL queries over various data stores, using standard R2RML mappings and an OWL2QL domain ontology to produce high-level conceptual views over the raw data. The Ontop-temporal extension is designed to handle timestamped log data, by additionally using (i) mappings supporting validity time specification, and (ii) rules based on metric temporal logic to define temporalised concepts. In this demo we present how Ontop-temporal can be used to facilitate the access to the MIMIC-III critical care unit dataset containing log data on hospital admissions, procedures, and diagnoses. We use the ICD9CM diagnoses ontology and temporal rules formalising the selection of patients for clinical trials taken from the clinicaltrials.gov database. We demonstrate how high-level queries can be answered by Ontop-temporal to identify patients eligible for the trials.},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {1927–1930},
numpages = {4},
keywords = {ontology-based data access, mimic-iii, metric temporal logic},
location = {Torino, Italy},
series = {CIKM '18}
}

@inproceedings{10.1145/3341105.3374111,
author = {Mebrek, Wafaa and Bouzeghoub, Amel},
title = {A stream reasoning framework based on a multi-agents model},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374111},
doi = {10.1145/3341105.3374111},
abstract = {Processing on-the-fly high volume of data streams is increasingly needed. To cope with the heterogeneity of this data, RDF model is more and more being adopted leading to plethora of RDF Stream Processing (RSP) systems and languages dealing with issues such as continuous querying, incremental reasoning and complex event processing (CEP). However, most of them has implemented centralized approaches and therefore suffer from some limitations as collaboration, sharing, expressiveness and scalability. Multi-agents systems have widely proven their worth and efficiency in particular their intrinsic decentralized property along with their cooperation and communication mechanism. In this paper we propose a new framework MAS4MEAN (Multi-Agent System for streaM rEAsoNing) based on a multi-agents model to embrace their benefits and tackle the challenges of increasing the scalability and ease of deployment in highly dynamic environments. A preliminary experimental evaluation with a real-world dataset show promising results when compared to an existing work.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {509–512},
numpages = {4},
keywords = {stream reasoning, stream processing, multi-agents systems, RDF streams},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3631802.3631826,
author = {Winkelnkemper, Felix and Schulte, Carsten},
title = {Reconstructing the Digital – An Architectural Perspective for Non-Engineers (Discussion Paper)},
year = {2024},
isbn = {9798400716539},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631802.3631826},
doi = {10.1145/3631802.3631826},
abstract = {Knowing and understanding the world of digital artefacts we are living in is a requirement for everyone today, regardless of their general interest in technology. Computer science education, however, often treats pupils as if they all wanted to become engineers. Educational models of computer science are rather not targeted at understanding the behaviour of the digital world, but at constructing it. Our paper complements such classical approaches with an Ontology of the Digital as an approach which reconstructs digital artefacts and thereby creates a model which helps to understand and explain the technological potentials of digital artefacts without relying on minute details of the engineering discipline of computing.},
booktitle = {Proceedings of the 23rd Koli Calling International Conference on Computing Education Research},
articleno = {21},
numpages = {7},
keywords = {CS for all, digital artefacts, explanation model, ontology, technological knowledge},
location = {Koli, Finland},
series = {Koli Calling '23}
}

@inproceedings{10.1145/3434780.3436541,
author = {Hu, Shuyang and Chew, Esyin},
title = {The Investigation and Novel Trinity Modeling for Museum Robots},
year = {2021},
isbn = {9781450388504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3434780.3436541},
doi = {10.1145/3434780.3436541},
abstract = {There have been interactive museum tour-guide robots under investigation since the end of twentieth century. However, those researches are limited to localisations and telepresence with less humanoids deployment or human-touched features. This research used a humanoid robot to develop the first Welsh-based museum robots that can speak bilingual, English and Welsh, addressing the design method, constraints and initial experimental results. This article introduces the definition and development of robots and service robots with three aims: 1) to design and pilot service robots in a public educational environment, National Museum of Wales, Cardiff. This is to develop a semi-autonomous robotic museum programme that can guide and educate visitors, explain exhibits and perform surveys based on a higher level of robot technology platform; 2) to perform voice interaction with the visitors and provides an inquiry and corporate branding services by the robotic programme with initial user experiences inquiry; and 3) to provide educational service robot design recommendation and a novel Trinity conceptual model and design principles to the sector based on the findings from objectives 1 and 2, for preliminary study and research on artificial intelligence in education and social cognition. Case study research method is used to lays a reference for museum robotic research, and it is easy to expand functionally, that is, secondary development; developing an autonomous humanoid robot for museum visit and interactive education.},
booktitle = {Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {21–28},
numpages = {8},
keywords = {service robot, educational robot, Nao robot, Museum robot},
location = {Salamanca, Spain},
series = {TEEM'20}
}

@inproceedings{10.1145/3714334.3714388,
author = {Li, Haili and Wang, Xiaodong and Zhou, Yunyan and Liu, Weijie and Pan, Shilong},
title = {An Overview of Event Extraction Methods based on Semantic Disambiguation},
year = {2025},
isbn = {9798400711237},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3714334.3714388},
doi = {10.1145/3714334.3714388},
abstract = {Event extraction is a fundamental and complex task in information extraction, aiming at automatically identifying and extracting structured event information from unstructured text. However, the same word may have different meanings in different contexts, necessitating semantic disambiguation during the classification process. The polysemy of triggers presents a significant challenge to accurate EE. Existing surveys primarily focus on different domains, fields, or technical paradigms, but none provide a systematic summary of the semantic disambiguation techniques employed. This study classifies these techniques into three categories based on the underlying technical frameworks and offers a comprehensive overview of the most advanced methods in each category. Finally, we summarize the application scenarios, strengths, and limitations of existing approaches, and discuss potential directions for future research.},
booktitle = {Proceedings of the 2024 2nd International Conference on Artificial Intelligence, Systems and Network Security},
pages = {319–326},
numpages = {8},
keywords = {deep neural networks, event extraction, information extraction, natural language processing, semantic disambiguation},
location = {
},
series = {AISNS '24}
}

@inproceedings{10.1145/3373744.3373745,
author = {Franco, Aldrin Jaramillo and Giraldo, Germ\'{a}n Urrego},
title = {On the Use of Business Process Models to Discover System Requirements},
year = {2020},
isbn = {9781450372343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373744.3373745},
doi = {10.1145/3373744.3373745},
abstract = {A framework of generic categories of process activities is adopted as a framework of generic categories of system goals in order to guide the reasoning of system analysts and stakeholders for the discovery of system goals from business process models. The categories of process activities are organized in four essential aspects of the process concept: input, evolution, evaluation and decision and output. These categories are characterized by verbs which offer a semantic diversity that clarifies the field of reasoning and guides the discovery of goals. This article proposes an approach for obtaining system goals from business process models; the proposal is illustrated with a diverse and rich set of pertinent goals discovered for a system supporting a "booking a flight" process.},
booktitle = {Proceedings of the 2019 11th International Conference on Information Management and Engineering},
pages = {1–9},
numpages = {9},
keywords = {system goals elicitation, business process models reuse, Business processes},
location = {London, United Kingdom},
series = {ICIME 2019}
}

@inproceedings{10.1145/3301761.3301775,
author = {Yeh, Jian-hua and Huang, Xin-mao},
title = {BKOntoVR: A Virtual Reality Exhibition System for Biographic Ontology-Based Semantic Structure},
year = {2018},
isbn = {9781450361279},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301761.3301775},
doi = {10.1145/3301761.3301775},
abstract = {In this article, we illustrate some of the semantic web-related technologies and design a virtual exhibition system for a set of ontology knowledge structures based on biographical history, which we call BKOntoVR. This is an official framework for processing and presenting biographical history-related messages on the semantic web with virtual-reality technology, including biographical events, time and space relationships, related personal messages, and more. We elaborate on this ontology knowledge architecture and explain how to use our ontological structure called BKOnto as a basis for domain-specific knowledge to support virtual presentation. Information management is becoming an important part of cultural collections related technologies, from the management of personal collections to the establishment of large, decentralized "semantic" databases. These semantic databases can be used to a certain extent using semantic web technology to process and construct a machine-understandable data network. Such a data network can be linked to the referenced knowledge structure to give a concept and a relationship form specification associated with a set of descriptive objects in the definition domain (person, thing, place, etc.) - that is, link to its meaning. Biographical history of the specific characters is through the life and other areas of a systematic description of the introduction of a text that form. In this paper, we overview some of the Semantic Web-related technologies and describe a cognitive knowledge structure for biographical knowledge representation based on OWL markup language, we call it BKOnto. This is a formal framework for dealing with information about biographical history on the semantic web, including biographical events, temporal and spatial relationships, related information of persons, and so on. We describe this ontology knowledge structure and explain how BKOnto can act as a basis for more domain-specific knowledge representation. In this study, we further present such cultural collections in a virtual reality form, by transforming the ontology cognitive architecture data into a virtual reality exhibition space, allowing users to present the semantic structure in the form of multimedia in a three-dimensional space more easily. Such a form of presentation can be used in the virtual exhibition of the museum, making it easier for museums to organize cultural collections of semantic structures and exert their influence through the Internet. The empirical study also uses the Mackay Digital Archives Project (http://dlm.csie.au.edu.tw/) as a source of information to demonstrate the ontology knowledge building process of Mackay's biographical stories, as well as related Digital collection of information.},
booktitle = {Proceedings of the 2018 2nd International Conference on Software and E-Business},
pages = {69–73},
numpages = {5},
keywords = {virtual reality, temporal event, semantic web, ontology, museum exhibition, Biographical knowledge},
location = {Zhuhai, China},
series = {ICSEB '18}
}

@inproceedings{10.1145/3436829.3436862,
author = {Youssef, Clara K. and Ahmed, Farida M. and Hashem, Hashem M. and Talaat, Veronia E. and Shorim, Nada and Ghanim, Taraggy},
title = {GQM-based Tree Model for Automatic Recommendation of Design Pattern Category},
year = {2021},
isbn = {9781450377218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436829.3436862},
doi = {10.1145/3436829.3436862},
abstract = {Software Design Patterns (DP) are formal approaches that propose generic reusable solutions to different design problems. Building DP automatic recommendation system is one of the most challenging topics in the field of software industry to improve the final software quality. Proposing a DP for a design problem requires good base knowledge about each DP and its functionality. In this paper, we propose an approach that automatically recommends the appropriate design pattern category. The proposed approach is a Goal Question Metric (GQM) based tree model of questions. The software engineer answers these questions based on the user requirements, and finally the approach recommends the category of the suitable DP category based on our designed tree model. The GQM is responsible for weight calculation process at each node based on the questions' answers. The software engineer is responsible for delivering the user requirements to our system, via answering the proposed model. The precision and accuracy obtained by our system is 80\% while the recall is 100\%.},
booktitle = {Proceedings of the 9th International Conference on Software and Information Engineering},
pages = {126–130},
numpages = {5},
keywords = {Recommendation System, Goal-Question-Metric, Design Patterns, Design Pattern selection, Decision Trees},
location = {Cairo, Egypt},
series = {ICSIE '20}
}

@article{10.1145/3141772,
author = {Siabato, Willington and Claramunt, Christophe and Ilarri, Sergio and Manso-Callejo, Miguel Angel},
title = {A Survey of Modelling Trends in Temporal GIS},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3141772},
doi = {10.1145/3141772},
abstract = {The main achievements of spatio-temporal modelling in the field of Geographic Information Science that spans the past three decades are surveyed. This article offers an overview of: (i) the origins and history of Temporal Geographic Information Systems (T-GIS); (ii) relevant spatio-temporal data models proposed; (iii) the evolution of spatio-temporal modelling trends; and (iv) an analysis of the future trends and developments in T-GIS. It also presents some current theories and concepts that have emerged from the research performed, as well as a summary of the current progress and the upcoming challenges and potential research directions for T-GIS. One relevant result of this survey is the proposed taxonomy of spatio-temporal modelling trends, which classifies 186 modelling proposals surveyed from more than 1,450 articles.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {30},
numpages = {41},
keywords = {time geography, temporal models, temporal GIS, survey, spatio-temporal databases, literature review, Spatio-temporal models}
}

@inproceedings{10.1145/3338906.3338974,
author = {Ne\v{s}i\'{c}, Damir and Kr\"{u}ger, Jacob and St\u{a}nciulescu, undefinedtefan and Berger, Thorsten},
title = {Principles of feature modeling},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338974},
doi = {10.1145/3338906.3338974},
abstract = {Feature models are arguably one of the most intuitive and successful notations for modeling the features of a variant-rich software system. Feature models help developers to keep an overall understanding of the system, and also support scoping, planning, development, variant derivation, configuration, and maintenance activities that sustain the system's long-term success. Unfortunately, feature models are difficult to build and evolve. Features need to be identified, grouped, organized in a hierarchy, and mapped to software assets. Also, dependencies between features need to be declared. While feature models have been the subject of three decades of research, resulting in many feature-modeling notations together with automated analysis and configuration techniques, a generic set of principles for engineering feature models is still missing. It is not even clear whether feature models could be engineered using recurrent principles. Our work shows that such principles in fact exist. We analyzed feature-modeling practices elicited from ten interviews conducted with industrial practitioners and from 31 relevant papers. We synthesized a set of 34 principles covering eight different phases of feature modeling, from planning over model construction, to model maintenance and evolution. Grounded in empirical evidence, these principles provide practical, context-specific advice on how to perform feature modeling, describe what information sources to consider, and highlight common characteristics of feature models. We believe that our principles can support researchers and practitioners enhancing feature-modeling tooling, synthesis, and analyses techniques, as well as scope future research.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {62–73},
numpages = {12},
keywords = {software product lines, modeling principles, Feature models},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/3213214.3213226,
author = {Bocciarelli, Paolo and D'Ambrogio, Andrea and Giglio, Andrea and Paglia, Emiliano},
title = {Model transformation services for MSaaS platforms},
year = {2018},
isbn = {9781510860186},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The development of complex systems may take advantage by the introduction of Modeling \&amp; Simulation (M&amp;S) based analysis techniques from the early stages of the system lifecycle. However, M&amp;S approaches typically require significant know-how and effort, as well as remarkable resources to setup and maintain proper execution platforms. Such issues can be tackled by use of automated approaches based on model transformation, which reduce the simulation model building effort, and by the M&amp;S as a Service (MSaaS) paradigm, which brings the benefits of service-oriented architectures and cloud computing into the M&amp;S field, so to reduce the costs of M&amp;S efforts. In this paper, we show how MSaaS platforms can be effectively extended by introducing model transformation services, with specific application to the M&amp;S-based analysis of complex systems specified by use of SysML. The paper also describes a catalog of currently available model transformation services, in order to show how the proposed MSaaS platform may ease the introduction of M&amp;S approaches at any stage of the system development cycle.},
booktitle = {Proceedings of the Model-Driven Approaches for Simulation Engineering Symposium},
articleno = {12},
numpages = {12},
keywords = {model-driven, model transformation, cloud computing, MSaaS, MDA},
location = {Baltimore, Maryland},
series = {Mod4Sim '18}
}

@inproceedings{10.5555/3213214.3213218,
author = {Durak, Umut and M\"{u}ller, David and M\"{o}cke, Florian and Koch, Claus B.},
title = {Modeling and simulation based development of an enhanced ground proximity warning system for multicore targets},
year = {2018},
isbn = {9781510860186},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {The advances in Cyber-Physical Systems (CPS) are also effecting the aeronautics. The growth of the cyber layer in aircraft is demanding higher throughput and eventually multi-core systems are becoming topics of interest. The development of parallel real-time systems for multicore processors requires new approaches in model-based design and simulation-based verification. The Enhanced Ground Proximity Warning System (EGPWS) is a terrain awareness system that creates aural and visual warnings for the pilot to prevent Controlled Flight into Terrain (CFIT). This paper presents a multi-core parallelization workflow and a corresponding x-in-the-loop testing pipeline for model-based development of an EGPWS.},
booktitle = {Proceedings of the Model-Driven Approaches for Simulation Engineering Symposium},
articleno = {4},
numpages = {12},
keywords = {x-in-the-loop testing, multi-core parallelization, model-based development, enhanced ground proximity warning systems},
location = {Baltimore, Maryland},
series = {Mod4Sim '18}
}

@inproceedings{10.1145/3652620.3688214,
author = {Balaban, Mira and Hamann, Lars and Khais, Gil and Saad, Amiel Amram and Maraee, Azzam and Sturm, Arnon},
title = {Mediation-Based MLM in USE},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688214},
doi = {10.1145/3652620.3688214},
abstract = {Multi Level Modeling (MLM) has been around in the modeling community for over twenty years. It has attracted much attention, both on theoretical and practical grounds. Multiple approaches have emerged, with different support for MLM concepts on the levels of syntax, semantics and pragmatics. MLM tools support a variety of applications, ranging from ontology specification to software modeling.This paper introduces the MedMLM-USE tool, which implements the Mediation-based MLM theory as an extension of the USE tool. The USE tool has been selected due to its open, well-structured architecture. The MedMLM-USE application extends: (1) the well-defined meta-model that is supported by USE, with MedMLM concepts; (2) the USE modeling services to support MLM-models. This paper describes the extended meta-model and services, provides examples, defines an inheritance semantics for instance-of, and discusses engineering difficulties.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {818–827},
numpages = {10},
keywords = {multi-level modeling, MLM semantics, MLM implementation},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@article{10.5555/3447080.3447095,
author = {Drumheller, William R. and Conner, David C.},
title = {Online system modeling and documentation using ROS snapshot},
year = {2020},
issue_date = {October 2020},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {36},
number = {3},
issn = {1937-4771},
abstract = {Robotic systems are complex systems running a number of software components to control the hardware. The Robot Operating System (ROS) is often used in such systems. The integration of these software components, even when the interfaces are heavily documented, can become a daunting task. As part of a typical Software Development Life Cycle, requirements and interfaces change for entire software systems as well as the individual software components that make up those systems.Model-Integrated Computing (MIC) can be used to manage such complexity, and provides a means for validating the system integration throughout its lifecycle. Unfortunately, current MIC tools for ROS systems are limited and lack the ability to automatically gather information needed to make models for ROS entities. This paper presents a new tool, ROS Snapshot, that captures a snapshot of an existing ROS-based system during runtime, and fully documents the system using specified metamodels for each ROS entity. The resulting system model can then be used to document the current state of the system, which is especially important when upgrading versions (e.g. conversion from ROS 1.0 to ROS 2.0). We present a set of current applications for the ROS Snapshot tool, as well as future development plans to integrate the acquired system model into a full MIC system.},
journal = {J. Comput. Sci. Coll.},
month = oct,
pages = {128–141},
numpages = {14}
}

@inproceedings{10.1145/3700486.3700511,
author = {Gao, Jingjing and Wang, Chunyan and Wang, Ruixiang},
title = {Utilizing Deep Learning for Named Entity Recognition in Ancient Chinese Stroke Medical Cases},
year = {2024},
isbn = {9798400710063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3700486.3700511},
doi = {10.1145/3700486.3700511},
abstract = {Objective: Based on the deep learning method, to construct the named entity recognition model of Ming and Qing dynasty stroke medical cases, to extract as comprehensive as possible the effective information in the Ming and Qing dynasty stroke medical cases, and to realise the transmission and sharing of knowledge.Method: (1) Collation of textual data from ancient stroke cases and comprehensive information labelling of the texts. (2) Train a Bert-BiLSTM-CRF model of ancient stroke medical cases based on the annotated dataset. (3) The constructed model was evaluated using recall and F1 value.Results: The Bert-BiLSTM-CRF model for stroke medical cases was successfully constructed, and automatic identification of key entities was achieved. The evaluation results show that among the 17 named entities designed in this study, the model has a higher accuracy rate than the others for three entities, namely, Chinese medicine, dose, and pulse, and their F1 values are 85\%, 90\%, and 85\%, respectively.Conclusion: The Bert-BiLSTM-CRF model of ancient stroke cases constructed in this study achieves the automatic identification of entities in medical cases, verifies the effectiveness of NER technology in the processing of ancient Chinese medical books, and provides a reference for research in similar fields.},
booktitle = {Proceedings of the 2024 International Conference on Biomedicine and Intelligent Technology},
pages = {152–157},
numpages = {6},
keywords = {Ancient medical cases, Deep learning, Named entity recognition, Natural language processing, Stroke},
location = {
},
series = {ICBIT '24}
}

@inproceedings{10.1145/3356991.3365474,
author = {Palumbo, Rachel and Thompson, Laura and Thakur, Gautam},
title = {SONET: a semantic ontological network graph for managing points of interest data heterogeneity},
year = {2019},
isbn = {9781450369602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356991.3365474},
doi = {10.1145/3356991.3365474},
abstract = {Scalability, standardization, and management are important issues when working with very large Volunteered Geographic Information (VGI). VGI is a rich and valuable source of Points of Interest (POI) information, but its inherent heterogeneity in content, structure, and scale across sources present major challenges for interlinking data sources for analysis. To be useful at scale, the raw information needs to be transformed into a standardized schema that can be easily and reliably used by data analysts. In this work, we tackle the problem of unifying POI categories (e.g. restaurants, temple, and hotel) across multiple data sources to aid in improving land use maps and population distribution estimation as well as support data analysts wishing to fuse multiple data sources with the OpenStreetMap (OSM) mapping platform or working with projects that are already configured in the OSM schema and wish to add additional sources of information. Graph theory and its implementation through the SONET graph database, provides a programmatic way to organize, store, and retrieve standardized POI categories at multiple levels of abstraction. Additionally, it addresses category heterogeneity across data sources by standardizing and managing categories in a way that makes cross-domain analysis possible.},
booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Geospatial Humanities},
articleno = {6},
numpages = {6},
keywords = {points of interest, openstreetmap, ontology, graph database, big data},
location = {Chicago, Illinois},
series = {GeoHumanities '19}
}

@inproceedings{10.1145/3534678.3539443,
author = {Huang, Jiaxin and Meng, Yu and Han, Jiawei},
title = {Few-Shot Fine-Grained Entity Typing with Automatic Label Interpretation and Instance Generation},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539443},
doi = {10.1145/3534678.3539443},
abstract = {We study the problem of few-shot Fine-grained Entity Typing (FET), where only a few annotated entity mentions with contexts are given for each entity type. Recently, prompt-based tuning has demonstrated superior performance to standard fine-tuning in few-shot scenarios by formulating the entity type classification task as a ''fill-in-the-blank'' problem. This allows effective utilization of the strong language modeling capability of Pre-trained Language Models (PLMs). Despite the success of current prompt-based tuning approaches, two major challenges remain: (1) the verbalizer in prompts is either manually designed or constructed from external knowledge bases, without considering the target corpus and label hierarchy information, and (2) current approaches mainly utilize the representation power of PLMs, but have not explored their generation power acquired through extensive general-domain pre-training. In this work, we propose a novel framework for few-shot FET consisting of two modules: (1) an entity type label interpretation module automatically learns to relate type labels to the vocabulary by jointly leveraging few-shot instances and the label hierarchy, and (2) a type-based contextualized instance generator produces new instances based on given instances to enlarge the training set for better generalization. On three benchmark datasets, our model outperforms existing methods by significant margins.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {605–614},
numpages = {10},
keywords = {entity typing, few-shot learning, prompt-based learning},
location = {Washington DC, USA},
series = {KDD '22}
}

@article{10.1145/3299887.3299892,
author = {Hirzel, Martin and Baudart, Guillaume and Bonifati, Angela and Della Valle, Emanuele and Sakr, Sherif and Akrivi Vlachou, Akrivi},
title = {Stream Processing Languages in the Big Data Era},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0163-5808},
url = {https://doi.org/10.1145/3299887.3299892},
doi = {10.1145/3299887.3299892},
abstract = {This paper is a survey of recent stream processing languages, which are programming languages for writing applications that analyze data streams. Data streams, or continuous data flows, have been around for decades. But with the advent of the big-data era, the size of data streams has increased dramatically. Analyzing big data streams yields immense advantages across all sectors of our society. To analyze streams, one needs to write a stream processing application. This paper showcases several languages designed for this purpose, articulates underlying principles, and outlines open challenges.},
journal = {SIGMOD Rec.},
month = dec,
pages = {29–40},
numpages = {12}
}

@inproceedings{10.1145/3747912.3747914,
author = {Xie, Weiming and Yao, Zhaomin and Bai, Xiaozhou and Mai, Lang and Zhan, Ying and Wu, Xiaodan and Dai, Yingxin and Pei, Yusong and Zhang, Guoxu and Wang, Zhiguo},
title = {A Novel Artificial Intelligence Voice Electronic Medical Record Based on Blockchain},
year = {2025},
isbn = {9798400715136},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3747912.3747914},
doi = {10.1145/3747912.3747914},
abstract = {Nowadays, the global digital transformation of healthcare is advancing rapidly with the help of technologies such as electronic medical records, telemedicine, and mobile medical applications. However, there are still challenges in EMR interoperability, security, and data exchange. To address these existing limitations, This study proposes a voice electronic medical record system driven by artificial intelligence and blockchain, which is designed to improve clinical records and nursing coordination. This system adopts a dedicated deep learning architecture. It transcribe the conversations between doctors and patients into text, and then uses natural language processing to extract the relevant medical information. At the same time, it also provides diagnostic prompts, which can reduce the risk of misdiagnosis. Doctors can view and edit these summaries generated by artificial intelligence. Then safely record them on the decentralized crypto blockchain ledger. With federated learning, the model can be continuously improved in multiple centers without infringing on data privacy. This solution integrates automatic speech recognition, distributed ledger technology, and collaborative deep learning, aiming to enhance the EMR efficiency, security, data integrity, and care continuity of medical institutions. The combination of blockchain technology and artificial intelligence technology holds great potential. It can transform fragmented health data into portable and interoperable records under patient control, thus bringing strategic advantages to the health system that is undergoing a comprehensive digital transformation.},
booktitle = {Proceedings of the 2025 International Conference on Software Engineering and Computer Applications},
pages = {12–19},
numpages = {8},
keywords = {Artificial Intelligence, Blockchain, Digital Healthcare, Electronic Medical Records, Speech Recognition},
location = {
},
series = {SECA '25}
}

@inproceedings{10.1145/3382025.3414955,
author = {Ananieva, Sofia and Greiner, Sandra and K\"{u}hn, Thomas and Kr\"{u}ger, Jacob and Linsbauer, Lukas and Gr\"{u}ner, Sten and Kehrer, Timo and Klare, Heiko and Koziolek, Anne and L\"{o}nn, Henrik and Krieter, Sebastian and Seidl, Christoph and Ramesh, S. and Reussner, Ralf and Westfechtel, Bernhard},
title = {A conceptual model for unifying variability in space and time},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414955},
doi = {10.1145/3382025.3414955},
abstract = {Software engineering faces the challenge of developing and maintaining systems that are highly variable in space (concurrent variations of the system at a single point in time) and time (sequential variations of the system due to its evolution). Recent research aims to address this need by managing variability in space and time simultaneously. However, such research often relies on nonuniform terminologies and a varying understanding of concepts, as it originates from different communities: software product-line engineering and software configuration management. These issues complicate the communication and comprehension of the concepts involved, impeding the development of techniques to unify variability in space and time. To tackle this problem, we performed an iterative, expert-driven analysis of existing tools to derive the first conceptual model that integrates and unifies terminologies and concepts of both dimensions of variability. In this paper, we present the unification process of concepts for variability in space and time, and the resulting conceptual model itself. We show that the conceptual model achieves high coverage and that its concepts are of appropriate granularity with respect to the tools for managing variability in space, time, or both that we considered. The conceptual model provides a well-defined, uniform terminology that empowers researchers and developers to compare their work, clarifies communication, and prevents redundant developments.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {15},
numpages = {12},
keywords = {version control, variability, revision management, product lines},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3626772.3657989,
author = {Cai, Qingpeng and Zhao, Xiangyu and Pan, Ling and Xin, Xin and Huang, Jin and Zhang, Weinan and Zhao, Li and Yin, Dawei and Yang, Grace Hui},
title = {AgentIR: 1st Workshop on Agent-based Information Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657989},
doi = {10.1145/3626772.3657989},
abstract = {Information retrieval (IR) systems have become an essential component in modern society to help users find useful information, which consists of a series of processes including query expansion, item recall, item ranking and re-ranking, etc. Based on the ranked information list, users can provide their feedbacks. Such an interaction process between users and IR systems can be naturally formulated as a decision-making problem, which can be either one-step or sequential. In the last ten years, deep reinforcement learning (DRL) has become a promising direction for decision-making, since DRL utilizes the high model capacity of deep learning for complex decision-making tasks. On the one hand, there have been emerging research works focusing on leveraging DRL for IR tasks. However, the fundamental information theory under DRL settings, the challenge of RL methods for Industrial IR tasks, or the simulations of DRL-based IR systems, has not been deeply investigated. On the other hand, the emerging LLM provides new opportunities for optimizing and simulating IR systems. To this end, we propose the first Agent-based IR workshop at SIGIR 2024, as a continuation from one of the most successful IR workshops, DRL4IR. It provides a venue for both academia researchers and industry practitioners to present the recent advances of both DRL-based IR systems and LLM-based IR systems from the agent-based IR's perspective, to foster novel research, interesting findings, and new applications.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3025–3028},
numpages = {4},
keywords = {agent-based information retrieval, drl, llm},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3348445.3351305,
author = {Iqtidar, Khushbakht and Azam, Farooque and Anwar, Muhammad Waseem and Amjad, Anam},
title = {Model-Driven approach to Integrate Requirements for Safety-Critical Systems},
year = {2019},
isbn = {9781450371957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3348445.3351305},
doi = {10.1145/3348445.3351305},
abstract = {A sophisticated approach is required to elicit requirements for Safety-Critical Systems (SCS). Incomplete, inconsistent or ambiguous requirements can result in many safety-critical catastrophes. While specifying a SCS, it is one of the greatest challenges to extract a complete set of consistent requirements. To overcome this problem, we have proposed a meta-model in this paper for integration of requirements which were specified using several representations to ensure the completeness of requirements. The idea is to use a database, for the integration of the extracted data that will implement the meta-model for the requirements. Problems like inconsistencies and ambiguities can be identified and solved according to the defined meta-model, which will help us in the development and testing phase and will minimize the project chaos. We have validated the proposed methodology with using insulin pump system case study. For the Requirement Engineering process of Safety-Critical Systems, the proposed approach is highly beneficial as final product will result in fewer defects, reduced development cost by avoiding rework, easy maintenance, increased satisfaction of the stakeholders and possibility of faster delivery of the safety critical system.},
booktitle = {Proceedings of the 7th International Conference on Computer and Communications Management},
pages = {58–62},
numpages = {5},
keywords = {model-driven requirement engineering, meta-model, Safety-Critical System (SCS)},
location = {Bangkok, Thailand},
series = {ICCCM '19}
}

@article{10.1145/3479155,
author = {Katyayan, Pragya and Joshi, Nisheeth},
title = {Development of Automatic Rule-based Semantic Tagger and Karaka Analyzer for Hindi},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {2},
issn = {2375-4699},
url = {https://doi.org/10.1145/3479155},
doi = {10.1145/3479155},
abstract = {Hindi is the third most-spoken language in the world (615 million speakers) and has the fourth highest native speakers (341 million). It is an inflectionally rich and relatively free word-order language with an immense vocabulary set. Despite being such a celebrated language across the globe, very few Natural Language Processing (NLP) applications and tools have been developed to support it computationally. Moreover, most of the existing ones are not efficient enough due to the lack of semantic information (or contextual knowledge). Hindi grammar is based on Paninian grammar and derives most of its rules from it. Paninian grammar very aggressively highlights the role of karaka theory in free-word order languages. In this article, we present an application that extracts all possible karakas from simple Hindi sentences with an accuracy of 84.2\% and an F1 score of 88.5\%. We consider features such as Parts of Speech tags, post-position markers (vibhaktis), semantic tags for nouns and syntactic structure to grab the context in different-sized word windows within a sentence. With the help of these features, we built a rule-based inference engine to extract karakas from a sentence. The application takes in a text file with clean (without punctuation) simple Hindi sentences and gives back karaka tagged sentences in a separate text file as output.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {39},
numpages = {25},
keywords = {language resource, feature extraction, semantic tagging, Karaka analyzer}
}

@inproceedings{10.1145/3412841.3442056,
author = {Halilaj, Lavdim and L\"{u}ttin, J\"{u}rgen and Rothermel, Susanne and Arumugam, Santhosh Kumar and Dindorkar, Ishan},
title = {Towards a knowledge graph-based approach for context-aware points-of-interest recommendations},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442056},
doi = {10.1145/3412841.3442056},
abstract = {Context-aware Recommender Systems (CARS) are becoming an integral part of the everyday life by providing users the ability to retrieve relevant information based on their contextual situation. To increase the predictive power considering many parameters, such as mood, hunger level and user preferences, information from heterogeneous sources should be leveraged. However, these data sources are typically isolated and unexplored and the efforts for integrating them are exacerbated by variety of data structures used for their modelling and costly pre-processing operations. We propose a Knowledge Graph-based approach to allow integration of data according to abstract semantic models for Points-of-Interests (POI)s recommendation scenarios. By enriching data with information about attributes, relationships and their meaning, additional knowledge can be derived from what already exists. We demonstrate the applicability of the proposed approach with a concrete example showing benefits of the retrieving the dispersed data with a unified access mechanism.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1846–1854},
numpages = {9},
keywords = {context-aware POI recommendations, knowledge graphs, ontology-based data integration},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/3343031.3351090,
author = {Yin, Yifang and Chiou, Meng-Jiun and Liu, Zhenguang and Shrivastava, Harsh and Shah, Rajiv Ratn and Zimmermann, Roger},
title = {Multi-Level Fusion based Class-aware Attention Model for Weakly Labeled Audio Tagging},
year = {2019},
isbn = {9781450368896},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3343031.3351090},
doi = {10.1145/3343031.3351090},
abstract = {Recognizing ongoing events based on acoustic clues has been a critical research problem for a variety of AI applications. Compared to visual inputs, acoustic cues tend to be less descriptive and less consistent in time domain. The duration of a sound event can be quite short, which creates great difficulties for, especially weakly labeled, audio tagging. To solve these challenges, we present a novel end-to-end multi-level attention model that first makes segment-level predictions with temporal modeling, followed by advanced aggregations along both time and feature domains. Our model adopts class-aware attention based temporal fusion to highlight/suppress the relevant/irrelevant segments to each class. Moreover, to improve the representation ability of acoustic inputs, a new multi-level feature fusion method is proposed to obtain more accurate segment-level predictions, as well as to perform more effective multi-layer aggregation of clip-level predictions. We additionally introduce a weight sharing strategy to reduce model complexity and overfitting. Comprehensive experiments have been conducted on the AudioSet and the DCASE17 datasets. Experimental results show that our proposed method works remarkably well and obtains the state-of-the-art audio tagging results on both datasets. Furthermore, we show that our proposed multi-level fusion based model can be easily integrated with existing systems where additional performance gain can be obtained.},
booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
pages = {1304–1312},
numpages = {9},
keywords = {multi-layer feature fusion, convolutional recurrent neural network, audio tagging, attention-based model},
location = {Nice, France},
series = {MM '19}
}

@article{10.1145/3397178,
author = {Fox, Sarah E. and Menking, Amanda and Eschler, Jordan and Backonja, Uba},
title = {Multiples Over Models: Interrogating the Past and Collectively Reimagining the Future of Menstrual Sensemaking},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {4},
issn = {1073-0516},
url = {https://doi.org/10.1145/3397178},
doi = {10.1145/3397178},
abstract = {In this article, we describe our efforts to retrace and reimagine period tracking technology—or, mobile applications designed to support the documentation and quantification of menstrual cycle data. In their current form, these systems often encourage those who menstruate to extract intimate information about the body (e.g., consistency or color of menstrual flow, physical and emotional symptoms), while promising to predict fertility and offer insight into managing one's period. In doing so, these technologies subtly dictate the forms of knowledge and types of relationships menstruators are expected to establish with their bodies (i.e., transactional or instrumentalized). Through historical analysis and a series of participatory experiments, we offer a vision for menstrual sensemaking that expands on these forms of interaction and ways of knowing to emphasize multiplicity and dimensionality rather than models, predictability, or a user's relation to averages or norms.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = sep,
articleno = {22},
numpages = {24},
keywords = {women's health, transgender health, self-tracking, Menstruation}
}

@inproceedings{10.1145/3209415.3209427,
author = {Androutsopoulou, Aggeliki and Charalabidis, Yannis},
title = {A framework for evidence based policy making combining big data, dynamic modelling and machine intelligence},
year = {2018},
isbn = {9781450354219},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209415.3209427},
doi = {10.1145/3209415.3209427},
abstract = {Governments and policy makers are striving to respond to contemporary socio-economic challenges, however, often neglecting the human factor and the multidimensionality of policy implications. In this chapter, a framework for evidence based policy making is proposed, which integrates the usage of open big data coming from a multiplicity of sources with policy simulations. It encompasses the application of dynamic modelling methodologies and data mining techniques to extract knowledge from two types of data. On the one hand, objective data such as governmental and statistical data, are used to capture the interlinked policy domains and their underlying casual mechanisms. On the other hand, behavioural patterns and citizens' opinions are extracted from Web 2.0 sources, social media posts, polls and statistical surveys. To combine this multimodal information, our approach suggests a modelling methodology that bases on big data acquisition and processing for the identification of significant factors and counterintuitive interrelations between them, which can be applied in any policy domain. Then, to allow the practical application of the framework an ICT architecture is designed, with the aim to overcome challenges related with big data management and processing. Finally, validation of the approach for driving policy design and implementation in the future in diverse policy domains, is suggested.},
booktitle = {Proceedings of the 11th International Conference on Theory and Practice of Electronic Governance},
pages = {575–583},
numpages = {9},
keywords = {policy Modelling, impact assessment, evidence based policy making, dynamic simulation, data mining, behavioural patterns, Big data},
location = {Galway, Ireland},
series = {ICEGOV '18}
}

@inproceedings{10.1145/3746709.3746742,
author = {Du, Xinmeng and Wang, Yiruo and Ying, Qunbo and Zeng, Qunyao and Zhang, Yuanjie and Zhao, Li},
title = {Construction of visual knowledge graph of crop diseases and pests based on deep learning},
year = {2025},
isbn = {9798400713163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3746709.3746742},
doi = {10.1145/3746709.3746742},
abstract = {In response to the complex entity relationships, data aggregation difficulties, and knowledge sharing challenges in the field of crop pests and diseases, this study leverages the advantages of knowledge graph structure to propose a deep learning-based construction method. This method is grounded in domain ontology and employs a new annotation model to transform entity and relationship extraction into sequence labeling problems, with simultaneous labeling to enhance efficiency. It directly models ternary relationships to address the challenge of extracting overlapping relationships. Using the BERT-BiLSTM +CRF end-to-end model for experimentation, its F1 score reaches 91.34\%, outperforming various classic models. Finally, the extracted knowledge is stored in a Neo4j graph database to achieve knowledge visualization and inference, providing a high-quality knowledge base for downstream applications.},
booktitle = {Proceedings of the 2025 6th International Conference on Computer Information and Big Data Applications},
pages = {178–182},
numpages = {5},
keywords = {Crop, deep learning, entity relationship joint extraction, knowledge graph, model, pest and disease},
location = {
},
series = {CIBDA '25}
}

@inproceedings{10.1145/3425269.3425272,
author = {Silva, Jorge Luiz Machado da and de Fran\c{c}a, Breno B. Nicolau and Rubira, Cec\'{\i}lia Mary Fischer},
title = {Generating Trustworthiness Adaptation Plans Based on Quality Models for Cloud Platforms},
year = {2020},
isbn = {9781450387545},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425269.3425272},
doi = {10.1145/3425269.3425272},
abstract = {Cloud computing platforms can offer many benefits related to the provision of service processing and storage for hosting client applications. Trustworthiness can be defined as the trust of a customer in a cloud service and its provider; however, the assurance of this property is not trivial. First, trustworthiness in general is not composed by a single quality attribute, but by the combination of multiple attributes, such as data privacy, performance, reliability, etc. Second, during runtime clients can experience a change of the trustworthiness level required by their application due to the degradation of the cloud service. This article presents a solution that monitors during runtime the set of quality attributes of a specific application and generates adaptation plans in order to certify that an adequate resource amount be provided by the cloud in order to keep its trustworthiness level. Our solution is based on quality models to compute the metric associated to each non-functional requirement and their combination them into different types of trustworthiness levels. The main contribution of the solution is to provide an approach which deals with multiple requirements at the same time (or simultaneously) during runtime in order to adapt the cloud resources to keep the trustworthiness level required by the application. The solution was evaluated by an experiment considering a scenario where the application trustworthiness level was composed by three quality attributes: data privacy, performance and reliability. Initial results have shown that the approach is feasible in terms of the execution of the adaptation plans during runtime to certify the trustworthiness level required by the application.},
booktitle = {Proceedings of the 14th Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {141–150},
numpages = {10},
keywords = {Trustworthiness, Self-adaptive Systems, Cloud Computing, Adaptation Planning},
location = {Natal, Brazil},
series = {SBCARS '20}
}

@inproceedings{10.1145/3639233.3639245,
author = {Ranganathan, Aarthi and Tamminaina, Sai Gowtham and Raina, Gaurav},
title = {A Study Of Dialog Summarization Across Datasets And Domains},
year = {2024},
isbn = {9798400709227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639233.3639245},
doi = {10.1145/3639233.3639245},
abstract = {This study of dialog summarization covers multi-domain, multimodal and multilingual datasets, and the potential challenges in the different domains. The scope and progress of this rapidly evolving topic rely on the availability of datasets and emerging domains. Such a study can facilitate the cross-application of datasets to different domains to refine models and also aid scenarios where there is a lack of data in privacy-sensitive settings. Further, our work can enable the cross-fertilization of ideas across domains and in different contexts. Our study encompasses current and emerging domains, a comprehensive compilation of datasets, and avenues for further research.},
booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
pages = {196–202},
numpages = {7},
keywords = {Datasets, Dialog summarization, Domains, Machine learning, NLP},
location = {Seoul, Republic of Korea},
series = {NLPIR '23}
}

@inproceedings{10.1145/3688671.3688736,
author = {Kilanioti, Irene and Papadopoulos, George Angelos},
title = {AI-based knowledge graph construction and distributed storage for collaboration on the Sustainable Development Goals},
year = {2024},
isbn = {9798400709821},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688671.3688736},
doi = {10.1145/3688671.3688736},
abstract = {The achievement of the Sustainable Development Goals (SDGs) is crucial for future generations. The plethora of SDG data available for analysis facilitates the tasks of practitioners that gather and assess SDG data, including intergovernmental organizations, government agencies and social welfare organizations. In this paper, we propose a framework that aspires to have a substantial impact for SDG practitioners: We propose AI-based construction of SDG knowledge graphs. AI-based methods along with dimensionality reduction undertake the task to semantically cluster new uncategorised SDG data and novel indicators and efficiently place them in the environment of a distributed knowledge graph store.},
booktitle = {Proceedings of the 13th Hellenic Conference on Artificial Intelligence},
articleno = {40},
numpages = {6},
keywords = {Sustainable Development Goals ontology, distributed knowledge graphs, Hilbert Space Filling Curves, Artificial Intelligence},
location = {
},
series = {SETN '24}
}

@article{10.1145/3331149,
author = {Roels, Reinout and Signer, Beat},
title = {A Conceptual Framework and Content Model for Next Generation Presentation Solutions},
year = {2019},
issue_date = {June 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {EICS},
url = {https://doi.org/10.1145/3331149},
doi = {10.1145/3331149},
abstract = {Mainstream presentation tools such as Microsoft PowerPoint were originally built to mimic physical media like photographic slides and still exhibit the same characteristics. However, the state of the art in presentation tools shows that more recent solutions start to go beyond the classic presentation paradigms. For instance, presentations are becoming increasingly non-linear, content is quickly evolving beyond simple text and images and the way we author our presentations is becoming more collaborative. Nevertheless, existing presentation content models are often based on assumptions that do not apply to the current state of presentations any more, making them incompatible for some use cases and limiting the potential of end-user presentation solutions. In order to support state-of-the-art presentation functionality, we rethink the concept of a presentation and introduce a conceptual framework for presentation content. We then present a new content model for presentation solutions based on the Resource-Selector-Link (RSL) hypermedia metamodel. We further discuss an implementation of our model and show some example use cases. We conclude by outlining how design choices in the model address currently unmet needs with regards to extensibility, content reuse, collaboration, semantics, user access management, non-linearity, and context awareness, resulting in better support for the corresponding end-user functionality in presentation tools.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = jun,
articleno = {7},
numpages = {22},
keywords = {slideware, presentations, powerpoint, mindxpres, content model, conceptual framework}
}

@inproceedings{10.1109/JCDL.2019.00029,
author = {Fenlon, Katrina},
title = {Modeling digital humanities collections as research objects},
year = {2020},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00029},
doi = {10.1109/JCDL.2019.00029},
abstract = {Advancing digital libraries to increase the sustainability and usefulness of digital scholarship depends on identifying and developing data models capable of representing increasingly complex scholarly products. This paper considers the potential for an emergent model of scientific communication, the research objects data model, to accommodate the complexities of digital humanities collections. Digital humanities collections aggregate and enrich diverse sources of evidence and context, serving simultaneously as "publications" and dynamic, interactive platforms for research. The research objects model is an alternative to traditional formats of publication, facilitating aggregation and description of all of the inputs and outputs of a research process, ranging from datasets to papers to executable code. This model increasingly underpins research infrastructures in some scientific domains, yet its efficacy for representing humanities scholarship, and for undergirding humanities cyberinfrastructure, remains largely untested. This study offers a qualitative content analysis of digital humanities collections relying on a content/context analytical framework for characterizing collection components and their interrelationships. This study then maps those components and relationships into a research objects model to identify the model's strengths and limitations for representing diverse digital humanities scholarship.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {138–147},
numpages = {10},
keywords = {research objects, digital libraries, digital humanities, data models},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@article{10.1145/3161607,
author = {Chen, Qin and Hu, Qinmin and Huang, Jimmy Xiangji and He, Liang},
title = {Modeling Queries with Contextual Snippets for Information Retrieval},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {4},
issn = {2157-6904},
url = {https://doi.org/10.1145/3161607},
doi = {10.1145/3161607},
abstract = {Query expansion under the pseudo-relevance feedback (PRF) framework has been extensively studied in information retrieval. However, most expansion methods are mainly based on the statistics of single terms, which can generate plenty of irrelevant query terms and decrease retrieval performance. To alleviate this problem, we propose an approach that adapts the PRF-based contextual snippets into a context-aware topic model to enhance query representations. Specifically, instead of selecting a series of independent terms, we make full use of the query contextual information and focus on the snippets with the length of n in the PRF documents. Furthermore, we propose a context-aware topic (CAT) model to mine the topic distributions of the query-relevant snippets, namely, fine contextual snippets. In contrast to the traditional topic models that infer the topics from the whole corpus, we establish a bridge between the snippets and the corresponding PRF documents, which can be used for modeling the topics more precisely and efficiently. Finally, the topic distributions of the fine snippets are used for context-aware and topic-sensitive query representations. To evaluate the performance of our approach, we integrate the obtained queries into a topic-based hybrid retrieval model and conduct extensive experiments on various TREC collections. The experimental results show that our query-modeling approach is more effective in boosting retrieval performance compared with the state-of-the-art methods.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = jan,
articleno = {47},
numpages = {26},
keywords = {topic modeling, query representation, Contextual snippet}
}

@inproceedings{10.1145/3290607.3312990,
author = {Owen, Alex and Martinez, Kirk},
title = {A Dynamic Hierarchical Approach to Modelling and Orchestrating the Web of Things Using the DOM, CSS and JavaScript},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3312990},
doi = {10.1145/3290607.3312990},
abstract = {There is a lot of work in progress by the W3C and others surrounding a Web standards compliant Web of Things (WoT) which it is hoped will unify the current Internet of Things infrastructure. Our contribution to this uses the Document Object Model (DOM) to represent complex physical environments, with a CSS-like syntax for storing and controlling the state of 'things' within it. We describe how JavaScript can be used in conjunction with these to create an approach which is familiar to Web developers and may help them to transition more smoothly into WoT development. We share our implementation and explore some of the many potential avenues for future research. These include rich WoT development tools and the possibility of content production for physical environments.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {web of things, web development, linked data, internet of things, document object model, WoT, IoT},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@inproceedings{10.1145/3191697.3191714,
author = {Johnson, Michael and Stevens, Perdita},
title = {Confidentiality in the process of (model-driven) software development},
year = {2018},
isbn = {9781450355131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3191697.3191714},
doi = {10.1145/3191697.3191714},
abstract = {Much is now understood about how to develop software that will have good security properties in use. We claim that a topic which needs more attention, in particular from the Bx community, is security, especially confidentiality, in the software development process itself. What is then at issue is not what particular users of the software may be allowed to know, but rather, what particular developers of the software may be allowed to know. How can software development processes guarantee to respect confidentiality without compromising effective development?  The question is of general interest across software engineering, but model-driven development (MDD) seems a particularly promising arena in which to address it, because of MDD's focus on separation of concerns. In MDD, different people work with separate models, where (ideally) each model records all and only the information necessary to those who work with it. When necessary, the models are reconciled by bidirectional transformations, which automate a process which would otherwise have to be undertaken manually by the groups of experts meeting and studying both their models in order to bring them back into consistency. In model-driven development confidentiality issues become particularly clear and tractable, and bidirectional transformations have a key technical role. We hope to encourage the community to take up this challenge, and in this paper we begin our own analysis of a selection of the issues, focusing particularly on developing a threat model and some examples of secure restoration of consistency.},
booktitle = {Companion Proceedings of the 2nd International Conference on the Art, Science, and Engineering of Programming},
pages = {1–8},
numpages = {8},
keywords = {Security, Model-driven software development, Cospan, Confidentiality},
location = {Nice, France},
series = {Programming '18}
}

@inproceedings{10.1145/3167918.3167944,
author = {Kingsun, Melinda and Myers, Trina and Hardy, Dianna},
title = {C-DOM: a structured co-design framework methodology for ontology design and development},
year = {2018},
isbn = {9781450354363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167918.3167944},
doi = {10.1145/3167918.3167944},
abstract = {The development of ontologies is traditionally a process of building vocabularies and understanding complexities of a specific domain to link data and infer knowledge. However, this method does not always include the domain experts throughout the entire development process. This paper presents a Co-design framework as a structured methodology for ontology design and development. The framework, known as the Co-Designing Ontologies Methodology (C-DOM), presents a strategy to draw knowledge from the Subject Matter Expert (SME) without the SME needing to know any-thing about how to create an ontology. The C-DOM framework outlines a series of workshops that align to the layers of the Semantic Technologies architecture and has been developed via an iterative design process and tested in different domains with different levels of complexity and purpose. The first iteration focused on the creation of a light-weight ontology to link generic data in the incident and accident. The next iteration built a heavyweight ontology that included complex inference and reasoning within the urban water management domain. This paper describes the C-DOM in detail for re-use within the Semantic Technologies community and outlines its iterative development process undertake.1},
booktitle = {Proceedings of the Australasian Computer Science Week Multiconference},
articleno = {22},
numpages = {10},
keywords = {ontology development methodology, co-development, co-design},
location = {Brisband, Queensland, Australia},
series = {ACSW '18}
}

@inproceedings{10.1145/3462757.3466100,
author = {Paley, Andrew and Zhao, Andong L. Li and Pack, Harper and Servantez, Sergio and Adler, Rachel F. and Sterbentz, Marko and Pah, Adam and Schwartz, David and Barrie, Cameron and Einarsson, Alexander and Hammond, Kristian},
title = {From data to information: automating data science to explore the U.S. court system},
year = {2021},
isbn = {9781450385268},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462757.3466100},
doi = {10.1145/3462757.3466100},
abstract = {The U.S. court system is the nation's arbiter of justice, tasked with the responsibility of ensuring equal protection under the law. But hurdles to information access obscure the inner workings of the system, preventing stakeholders - from legal scholars to journalists and members of the public - from understanding the state of justice in America at scale. There is an ongoing data access argument here: U.S. court records are public data and should be freely available. But open data arguments represent a half-measure; what we really need is open information. This distinction marks the difference between downloading a zip file containing a quarter-million case dockets and getting the real-time answer to a question like "Are pro se parties more or less likely to receive fee waivers?" To help bridge that gap, we introduce a novel platform and user experience that provides users with the tools necessary to explore data and drive analysis via natural language statements. Our approach leverages an ontology configuration that adds domain-relevant data semantics to database schemas to provide support for user guidance and for search and analysis without user-entered code or SQL. The system is embodied in a "natural-language notebook" user experience, and we apply this approach to the space of case docket data from the U.S. federal court system. Additionally, we provide detail on the collection, ingestion and processing of the dockets themselves, including early experiments in the use of language modeling for docket entry classification with an initial focus on motions.},
booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Law},
pages = {119–128},
numpages = {10},
keywords = {visualization, notebook interface, natural language processing, information extraction, data analytics},
location = {S\~{a}o Paulo, Brazil},
series = {ICAIL '21}
}

@inproceedings{10.1145/3297280.3297417,
author = {Almeida, Ricardo Borges and Covalski, Victor and Machado, Roger and Rosa, Di\'{o}rgenes Yuri Leal da and Yamin, Adenauer Corr\^{e}a and Donato, Lucas Medeiros and Pernas, Ana Marilza},
title = {A hierarchical architectural model for network security exploring situational awareness},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297417},
doi = {10.1145/3297280.3297417},
abstract = {Often network security technologies used by organizations for securing their computational systems are deficient in providing holistic view of the environment. Based on this, our paper presents an architectural model based on a Situational Awareness approach for securing computational systems in distributed environments. The architecture is called EXEHDA-ISSA and is inspired by SIEM systems. It is composed of three modular software components called Collector, SmartLogger, and Manager. These components are interconnected following a multi-level hierarchical model and provide features such as event collection, hybrid event processing and a hybrid approach to contextual data storage. For the purpose of evaluating this proposal, four case studies were developed to validate the holistic view of security events as well as the model's characteristics such as flexibility, autonomy, scalability and the support to heterogeneity. Finally, the strengths and limitations of our approach are discussed, then followed by future works.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1365–1372},
numpages = {8},
keywords = {architectural model, network security, situational awareness},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3508397.3564846,
author = {Rinaldi, Antonio M. and Russo, Cristiano and Tommasino, Cristian},
title = {A Novel Approach to Populate Multimedia Knowledge Graph via Deep Learning and Semantic Analysis},
year = {2022},
isbn = {9781450392198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508397.3564846},
doi = {10.1145/3508397.3564846},
abstract = {The growth of data in volume and complexity needs automatic tools to manage and process information. Semantic Web Technologies are a silver bullet in this context due to their capacity to transform human-readable contents into machine-readable ones. Knowledge graphs and the related ontologies represent essential tools for managing very large knowledge bases. The population process of these knowledge structures is composed of expensive and time-consuming tasks, and we propose a novel approach to automate the population step. Our approach is based on novel techniques based on semantic analysis and deep learning using NoSQL technologies. Several results to show the effectiveness of our approach is also reported.},
booktitle = {Proceedings of the 14th International Conference on Management of Digital EcoSystems},
pages = {40–47},
numpages = {8},
keywords = {semantics, ontology, knowledge graph, deep learning, NoSQL},
location = {Venice, Italy},
series = {MEDES '22}
}

@inproceedings{10.1145/3539618.3591846,
author = {Er-Rahmadi, Btissam and Oncevay, Arturo and Ji, Yuanyi and Pan, Jeff Z.},
title = {KATIE: A System for Key Attributes Identification in Product Knowledge Graph Construction},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591846},
doi = {10.1145/3539618.3591846},
abstract = {We present part of Huawei's efforts in building a Product Knowledge Graph (PKG). We want to identify which product attributes (i.e. properties) are relevant and important in terms of shopping decisions to product categories (i.e. classes). This is particularly challenging when the attributes and their values are mined from online product catalogues, i.e. HTML pages. These web pages contain semi-structured data, which do not follow a concerted format and use diverse vocabulary to designate the same features. We propose a system for key attribute identification (KATIE) based on fine-tuning pre-trained models (e.g., DistilBERT) to predict the applicability and importance of an attribute to a category. We also propose an attribute synonyms identification module that allows us to discover synonymous attributes by considering not only their labels' similarities but also the similarity of their values sets. We have evaluated our approach to Huawei categories taxonomy and a set of internally mined attributes from web pages. KATIE guarantees promising performance results compared to the most recent baselines.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3320–3324},
numpages = {5},
keywords = {entity resolution, fine-tuning, pre-trained language model, product knowledge graph, relation discovery},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inbook{10.1145/3233795.3233806,
author = {Tumuluri, Raj and Dahl, Deborah and Patern\`{o}, Fabio and Zancanaro, Massimo},
title = {Standardized representations and markup languages for multimodal interaction},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan \&amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233806},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {347–392},
numpages = {46}
}

@inproceedings{10.5555/3470152.3470156,
author = {Jung, Jean Christoph and Papacchini, Fabio and Wolter, Frank and Zakharyaschev, Michael},
title = {Model comparison games for horn description logics},
year = {2021},
publisher = {IEEE Press},
abstract = {Horn description logics are syntactically defined fragments of standard description logics that fall within the Horn fragment of first-order logic and for which ontology-mediated query answering is in PTime for data complexity. They were independently introduced in modal logic to capture the intersection of Horn first-order logic with modal logic. In this paper, we introduce model comparison games for the basic Horn description logic hornALC (corresponding to the basic Horn modal logic) and use them to obtain an Ehrenfeucht-Fra\"{\i}ss\'{e} type definability result and a van Benthem style expressive completeness result for hornALC. We also establish a finite model theory version of the latter. The Ehrenfeucht-Fra\"{\i}ss\'{e} type definability result is used to show that checking hornALC indistinguishability of models is EXPTIME-complete, which is in sharp contrast to ALC indistinguishability (i.e., bisimulation equivalence) checkable in PTime. In addition, we explore the behavior of Horn fragments of more expressive description and modal logics by defining a Horn guarded fragment of first-order logic and introducing model comparison games for it.},
booktitle = {Proceedings of the 34th Annual ACM/IEEE Symposium on Logic in Computer Science},
articleno = {4},
numpages = {14},
location = {Vancouver, Canada},
series = {LICS '19}
}

@inproceedings{10.5555/3427510.3427546,
author = {Keller, Nicholas and Zeigler, Bernard and Kim, Doohwan and Anderson, Chase and Ceney, James},
title = {Supporting the reuse of algorithmic simulation models},
year = {2020},
isbn = {9781713814290},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Stateless functions, also referred to as algorithmic models, return an output given inputs that all occur at the same time instant. As relatively simple dynamic models, which define the behavior of variables over a timeline, algorithmic models nevertheless encode knowledge of entities that can be essential for use within models in a particular domain. This paper presents a development methodology for representing algorithmic models within the Discrete Event Systems Specifications (DEVS) formalism and employing the System Entity Structure (SES) to organize these models for reuse in new compositions. A use case example is used for illustration of the development process and the benefits in savings of time and effort are illustrated. Finally, some future possibilities to enhance the support of DEVS environments for this methodology are discussed.},
booktitle = {Proceedings of the 2020 Summer Simulation Conference},
articleno = {35},
numpages = {11},
keywords = {reuse, algorithmic, SES, DEVS},
location = {Virtual Event, Spain},
series = {SummerSim '20}
}

@article{10.1145/3714456,
author = {Haider Rizvi, Syed Mustafa and Imran, Ramsha and Mahmood, Arif},
title = {Text Classification Using Graph Convolutional Networks: A Comprehensive Survey},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3714456},
doi = {10.1145/3714456},
abstract = {Text classification is a quintessential and practical problem in natural language processing with applications in diverse domains such as sentiment analysis, fake news detection, medical diagnosis, and document classification. A sizable body of recent works exists where researchers have studied and tackled text classification from different angles with varying degrees of success. Graph convolution network (GCN)-based approaches have gained a lot of traction in this domain over the last decade with many implementations achieving state-of-the-art performance in more recent literature and thus, warranting the need for an updated survey. This work aims to summarize and categorize various GCN-based Text Classification approaches with regard to the architecture and mode of supervision. It identifies their strengths and limitations and compares their performance on various benchmark datasets. We also discuss future research directions and the challenges that exist in this domain.},
journal = {ACM Comput. Surv.},
month = mar,
articleno = {201},
numpages = {38},
keywords = {GCN, text classification, text analysis, text categorization}
}

@inproceedings{10.1145/3511808.3557446,
author = {Burgdorf, Andreas and Paulus, Alexander and Pomp, Andr\'{e} and Meisen, Tobias},
title = {DocSemMap 2.0: Semantic Labeling based on Textual Data Documentations Using Seq2Seq Context Learner},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557446},
doi = {10.1145/3511808.3557446},
abstract = {Methods for automated semantic labeling of data are an indispensable basis for increasing the usability of data. On the one hand, they contribute to the homogenization of the annotations and thus to the increase in quality; on the other hand, they reduce the modeling effort, provided that the quality of the used methodology is sufficient. In the past, research has focused primarily on data- and label-based methods. Another approach that has received recent attention is the incorporation of textual data documentations to support the automatic mapping of datasets to a knowledge graph. However, upon deeper analysis, our recent approach called DocSemMap gives away potential in a number of places. In this paper, we extend the current state of the art approach by uncovering existing shortcomings and presenting our own improvements. Using a sequence-to-sequence model (Seq2Seq), we exploit the context of datasets. An additional introduced classifier provides the linkage of documentation and labels for prediction. Our extended approach achieves a sustainable improvement in comparison to the reference approach.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {98–107},
numpages = {10},
keywords = {seq2seq, semantic mapping, natural language processing, classifier},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3555776.3578573,
author = {Guittoum, Amal and A\"{\i}ssaoui, Francois and Bolle, S\'{e}bastien and Boyer, Fabienne and De Palma, Noel},
title = {Inferring Threatening IoT Dependencies using Semantic Digital Twins Toward Collaborative IoT Device Management},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3578573},
doi = {10.1145/3555776.3578573},
abstract = {IoT Device Management (DM) refers to registering, configuring, monitoring, and updating IoT devices. DM is facing new challenges as dependencies between IoT devices generate various threats, such as update breaks and cascading failures. Dependencies-related threats are exacerbated by the fragmentation of the DM market, where multiple actors, e.g., operators and device manufacturers, are uncoordinately ensuring DM on interdependent devices, each using its DM solution. Identifying the topology of threatening dependencies is key in developing dependency-aware DM capabilities for legacy DM solutions to tackle dependencies-related threats efficiently. In this work, we apply Semantic Web and Digital Twin technologies to build a decision-support framework that automatically infers the topology of threatening dependencies in IoT systems. We integrate the proposed framework into the in-use Digital Twin platform Thing in the future and demonstrate its effectiveness by inferring threatening dependencies in smart home scenarios managed by ground-truth DM solutions, such as Orange's implementation of the USP Controller and Samsung's SmartThings Platform.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1732–1741},
numpages = {10},
keywords = {collaboration, dependencies management, entity resolution, thing description, SHACL, inference, ontology, digital twin, semantic web, IoT device management},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3639478.3639814,
author = {Speth, Sandro},
title = {Architecture-Based Cross-Component Issue Management and Propagation Analysis},
year = {2024},
isbn = {9798400705021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639478.3639814},
doi = {10.1145/3639478.3639814},
abstract = {This paper addresses the challenge of issue management in complex, component-based software architectures. In these systems, issues in one component often propagate across the architecture along the call chains. Yet, traditional issue management systems (IMSs) are limited to the boundaries of a single component and lack mechanisms for managing issues concerning their architectural dependencies. We present Gropius, a novel method that enhances issue management by integrating issues in an architecture graph. Gropius allows semantically linking issues across different components, synchronizes changes with underlying IMSs like GitHub, and allows modeling the architecture ontologically by defining the components' semantics at runtime. We explore whether combining issue and architecture management improves the development of component-based architectures regarding issue management. We hypothesize that this method will improve the efficiency and effectiveness of identifying and resolving cross-component issues, maintaining a comprehensive view of the application's state.},
booktitle = {Proceedings of the 2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings},
pages = {145–149},
numpages = {5},
keywords = {issue management, issue propagation analysis, component-based software architecture, model-based analysis},
location = {Lisbon, Portugal},
series = {ICSE-Companion '24}
}

@article{10.1145/3717413.3717435,
author = {Duchon, Markus and Matar, Jessy and Shoyari, Mahsa Faraji and Perzylo, Alexander and Kessler, Ingmar and Buchenberg, Patrick and Kuhn, Philipp and Hamacher, Thomas and Schlachter, Thorsten and S\"{u}ss, Wolfgang and Thinh, Nguyen Xuan and Salari, Haniyeh Ebrahimi and Latko, Jasmin and Xu, Minsheng and Shamovich, Maxim and Schl\"{u}tter, Dominik and Frisch, J\'{e}r\^{o}me and Rustagi, Kushagar and Kraft, Markus and Ayasse, Carolin and Steinke, Florian and Metzger, Michael and Kuper, Laura},
title = {A Platform Ecosystem Providing New Data For The Energy Transition},
year = {2025},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3717413.3717435},
doi = {10.1145/3717413.3717435},
abstract = {There is a great need for high-quality and comprehensive data in the energy sector. This data is collected and preprocessed at considerable expense and is not only required for research, but also by planning offices and other industries in connection with planning activities, such as the creation of municipal heat planning. The NEED ecosystem will accelerate these processes establishing an efficient, robust, and scalable energy data ecosystem. Heterogeneous energy-related data sources will be brought together and automatically linked consistently across different sectors as well as temporal and spatial levels. In this context, existing data sources will not be replaced but rather integrated into the NEED ecosystem as dedicated sources including a semantic description on how to utilize them. In addition to conventional data sources from the various planning levels, we envision a quality assessment scheme based on the FAIR criteria. In reality, we are often faced with missing data, too. To close this gap we explore data-driven, model-driven, AI-based, and tool-driven generation of synthetic data. These heterogeneous data sources will be interlinked using ontology modules which will be represented in a knowledge graph. Via a semantic API, queries will be generated to identify the required data sources, which will be orchestrated to provide the data needed. This will enable researchers, planners, and others including their tools to interact with the NEED ecosystem, while a tool proxy will be able to translate the resulting data into proprietary formats, required by some tools to operate. The NEED ecosystem is planned to be a robust, easy-to-maintain, and flexible infrastructure to enhance planning energy measures at different spatial levels and with different time horizons. We envision to evaluate our NEED approach for the transparent provision of data by integrating relevant data sources as microservices, definition and analysis of application scenarios in the planning domain, as well as the integration of various tools for different planning purposes. With these elements, we will be able to quantify the efficiency of data procurement and demonstrate the functionality of the approach using practical use cases.},
journal = {SIGENERGY Energy Inform. Rev.},
month = feb,
pages = {226–237},
numpages = {12}
}

@inproceedings{10.1145/3400903.3400919,
author = {Spitz, Andreas and Aumiller, Dennis and Soproni, B\'{a}lint and Gertz, Michael},
title = {A Versatile Hypergraph Model for Document Collections},
year = {2020},
isbn = {9781450388146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3400903.3400919},
doi = {10.1145/3400903.3400919},
abstract = {Efficiently and effectively representing large collections of text is of central importance to information retrieval tasks such as summarization and search. Since models for these tasks frequently rely on an implicit graph structure of the documents or their contents, graph-based document representations are naturally appealing. For tasks that consider the joint occurrence of words or entities, however, existing document representations often fall short in capturing cooccurrences of higher order, higher multiplicity, or at varying proximity levels. Furthermore, while numerous applications benefit from structured knowledge sources, external data sources are rarely considered as integral parts of existing document models. To address these shortcomings, we introduce heterogeneous hypergraphs as a versatile model for representing annotated document collections. We integrate external metadata, document content, entity and term annotations, and document segmentation at different granularity levels in a joint model that bridges the gap between structured and unstructured data. We discuss selection and transformation operations on the set of hyperedges, which can be chained to support a wide range of query scenarios. To ensure compatibility with established information retrieval methods, we discuss projection operations that transform hyperedges to traditional dyadic cooccurrence graph representations. Using PostgreSQL and Neo4j, we investigate the suitability of existing database systems for implementing the hypergraph document model, and explore the impact of utilizing implicit and materialized hyperedge representations on storage space requirements and query performance.},
booktitle = {Proceedings of the 32nd International Conference on Scientific and Statistical Database Management},
articleno = {7},
numpages = {12},
location = {Vienna, Austria},
series = {SSDBM '20}
}

@inproceedings{10.1145/3290605.3300231,
author = {Colusso, Lucas and Jones, Ridley and Munson, Sean A. and Hsieh, Gary},
title = {A Translational Science Model for HCI},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300231},
doi = {10.1145/3290605.3300231},
abstract = {Using scientific discoveries to inform design practice is an important, but difficult, objective in HCI. In this paper, we provide an overview of Translational Science in HCI by triangulating literature related to the research-practice gap with interview data from many parties engaged (or not) in translating HCI knowledge. We propose a model for Translational Science in HCI based on the concept of a continuum to describe how knowledge progresses (or stalls) through multiple steps and translations until it can influence design practice. The model offers a conceptual framework that can be used by researchers and practitioners to visualize and describe the progression of HCI knowledge through a sequence of translations. Additionally, the model may facilitate a precise identification of translational barriers, which allows devising more effective strategies to increase the use of scientific findings in design practice.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {translational science, translational research, research-practice gap},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@inproceedings{10.1145/3629527.3652898,
author = {Khan, Junaid Ahmed and Molan, Martin and Angelinelli, Matteo and Bartolini, Andrea},
title = {ExaQuery: Proving Data Structure to Unstructured Telemetry Data in Large-Scale HPC},
year = {2024},
isbn = {9798400704451},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3629527.3652898},
doi = {10.1145/3629527.3652898},
abstract = {High-performance computing (HPC) is the cornerstone of technological advancements in our digital age, but its management is becoming increasingly challenging, particularly as systems approach exascale. Operational data analytics (ODA) and holistic monitoring frameworks aim to alleviate this burden by collecting live telemetry from HPC systems. ODA frameworks rely on NoSQL databases for scalability, with implicit data structures embedded in metric names, necessitating domain knowledge for navigating telemetry data relations. To address the imperative need for explicit representation of relations in telemetry data, we propose a novel ontology for ODA, which we apply to a real HPC installation. The proposed ontology captures relationships between topological components and links hardware components(compute nodes, rack, systems) with job's execution and allocations collected telemetry. This ontology forms the basis for constructing a knowledge graph, enabling graph queries for ODA. Moreover, we propose a comparative analysis of the complexity (expressed in lines of code) and domain knowledge requirement (qualitatively assessed by informed end-users) of complex query implementation with the proposed method and NoSQL methods commonly employed in today's ODAs. We focused on six queries informed by facility managers' daily operations, aiming to benefit not only facility managers but also system administrators and user support. Our comparative analysis demonstrates that the proposed ontology facilitates the implementation of complex queries with significantly fewer lines of code and domain knowledge required as compared to NoSQL methods.},
booktitle = {Companion of the 15th ACM/SPEC International Conference on Performance Engineering},
pages = {127–134},
numpages = {8},
keywords = {high performance computing (hpc), operational data analytics(oda), resource description framework (rdf) ontology, sparql},
location = {London, United Kingdom},
series = {ICPE '24 Companion}
}

@inproceedings{10.1145/3341981.3344217,
author = {Purpura, Alberto and Maggipinto, Marco and Silvello, Gianmaria and Susto, Gian Antonio},
title = {Probabilistic Word Embeddings in Neural IR: A Promising Model That Does Not Work as Expected (For Now)},
year = {2019},
isbn = {9781450368810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341981.3344217},
doi = {10.1145/3341981.3344217},
abstract = {In this paper, we discuss how a promising word vector representation based on Probabilistic Word Embeddings (PWE) can be applied to Neural Information Retrieval (NeuIR). We illustrate PWE pros for text retrieval, and identify the core issues which prevent a full exploitation of their potential. In particular, we focus on the application of elliptical probabilistic embeddings, a type of PWE, to a NeuIR system (i.e., MatchPyramid). The main contributions of this paper are: (i) an analysis of the pros and cons of PWE in NeuIR; (ii) an in-depth comparison of PWE against pre-trained Word2Vec, FastText and WordNet word embeddings; (iii) an extension of the MatchPyramid model to take advantage of broader word relations information from WordNet; (iv) a topic-level evaluation of the MatchPyramid ranking models employing the considered word embeddings. Finally, we discuss some lessons learned and outline some open research problems to employ PWE in NeuIR systems more effectively.},
booktitle = {Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval},
pages = {3–10},
numpages = {8},
keywords = {probabilistic word embedding, neural information retrieval, natural language processing},
location = {Santa Clara, CA, USA},
series = {ICTIR '19}
}

@inproceedings{10.1145/3460210.3493569,
author = {Mansfield, Martin and Tamma, Valentina and Goddard, Phil and Coenen, Frans},
title = {Capturing Expert Knowledge for Building Enterprise SME Knowledge Graphs},
year = {2021},
isbn = {9781450384575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460210.3493569},
doi = {10.1145/3460210.3493569},
abstract = {Whilst Knowledge Graphs (KGs) are increasingly used in business scenarios, the construction of enterprise ontologies and the population of KGs from existing relational data remains a significant challenge. In this paper we report our experience in supporting CSols (an SME operating in the analytical laboratory domain) in transitioning their data from legacy databases to a bespoke KG. We modelled the KG using a streamlined approach based on state of the art ontology engineering methodologies, that addresses the challenges faced by SMEs when transitioning to new technologies: lack of resources to devote to the transition, paucity of comprehensive data governance policies, and resistance within the organisation to accepting new practices and knowledge. Our approach uses a combination of UML diagrams and a controlled language glossary to support stakeholders in reaching consensus during the knowledge capture phase, thus reducing the intervention of the ontology engineer only to cases where no agreement can be found. We present a case study illustrating the generation of the KG from a UML specification of part of the analytical domain and from legacy relational data, and we discuss the benefits and challenges of the approach.},
booktitle = {Proceedings of the 11th Knowledge Capture Conference},
pages = {129–136},
numpages = {8},
keywords = {uml, relational data, r2rml, ontology engineering, enterprise knowledge graphs},
location = {Virtual Event, USA},
series = {K-CAP '21}
}

@inproceedings{10.1145/3341105.3373910,
author = {Donadello, Ivan and Dragoni, Mauro and Eccher, Claudio},
title = {Explaining reasoning algorithms with persuasiveness: a case study for a behavioural change system},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373910},
doi = {10.1145/3341105.3373910},
abstract = {Explainable AI aims at building intelligent systems that are able to provide a clear, and human understandable, justification of their decisions. This holds for both rule-based and data-driven methods. In management of chronic diseases, the users of such systems are patients that follow strict dietary rules to manage such diseases. After receiving the input of the intake food, the system performs reasoning to understand whether the users follow an unhealthy behaviour. Successively, the system has to communicate the results in a clear and effective way, that is, the output message has to persuade users to follow the right dietary rules. In this paper, we address the main challenges to build such systems: i) the natural language generation of messages that explain the reasoner inconsistency; ii) the effectiveness of such messages at persuading the users. Results prove that the persuasive explanations are able to reduce the unhealthy users' behaviours.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {646–653},
numpages = {8},
keywords = {ontologies, natural language generation, mHealth, explainable reasoning, explainable AI},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3396743.3396781,
author = {Chaveesuk, Singha and Chaiyasoonthorn, Wornchanok and Khalid, Bilal},
title = {Understanding the Model of User Adoption and Acceptance of Technology by Thai Farmers: A Conceptual Framework},
year = {2020},
isbn = {9781450377065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396743.3396781},
doi = {10.1145/3396743.3396781},
abstract = {Thailand is constantly making policy changes and implementing Industrial revolution agendas like Thailand 4.0. With the advent of Industry revolution 4.0, Thailand government is making policy initiatives and technological advancement for successful transition to industry 5.0. The initiative is focused to offer farmers an opportunities to incorporate technology in their farming and agricultural processes to produce better crops and high-quality food. The Development of Eastern Economic Corridor is a crucial step in this regard as it focused on the evolution and progression of 10 key industries in Thailand. The plan is designed to be driven through agricultural technology and innovativeness. The future benefits of the ICT based 5.0 industrial revolution in agricultural field directly depends on the user adoption and acceptance of agricultural technology. The study, therefore, investigates the acceptance and adoption of ICT based products and services by farmers by utilizing the basics of TAM. The study proposes the framework of FTAM, and identifies the internal and external factors affecting the behavior intentions and attitude of farmers. It was found out that factors like "occupation relevance", "self-efficacy" and "social influence" affect the "Perceived usefulness (PU)" and "Perceived Ease of Use (PEOU) which in return impact their behavior intention and attitude towards utilizing and accepting ICT based products and services in farming and agriculture.},
booktitle = {Proceedings of the 2020 2nd International Conference on Management Science and Industrial Engineering},
pages = {279–285},
numpages = {7},
keywords = {innovativeness, Technology Acceptance Model, Perceived Usefulness, Perceived Ease of Use, Internet of Things, Information Communication Technology, Industrial Revolution},
location = {Osaka, Japan},
series = {MSIE '20}
}

@inproceedings{10.1145/3417990.3421413,
author = {Frank, Ulrich and T\"{o}pel, Daniel},
title = {Contingent level classes: motivation, conceptualization, modeling guidelines, and implications for model management},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421413},
doi = {10.1145/3417990.3421413},
abstract = {It has been known for some time that the level of a class may vary with the context it is used in. There are a few approaches that enable modelers to deal with corresponding requirements. However, they usually provide workarounds to avoid the problem of one class being on different levels at the same time. In this paper, the need for those classes, which are called contingent level classes, is motivated from a conceptual perspective. A conceptualization of contingent level classes is presented that addresses principal integrity issues and accounts for resulting constraints on class properties and relationships. Based on that conceptualization, the paper provides an analysis of specific challenges related to change operations on models that include contingent level classes. Subsequently, a set of patterns for coping with certain kinds of change operations is presented.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {86},
numpages = {10},
keywords = {possible world, multi-level modeling, level jump, contingent level class, change operations},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.1145/3631700.3664913,
author = {Bompotas, Agorakis and Triantafyllopoulos, Panagiotis and Raptis, George E. and Katsini, Christina and Makris, Christos},
title = {Towards Exploring Personalized Hyperlink Recommendations Through Machine Learning},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3664913},
doi = {10.1145/3631700.3664913},
abstract = {The Internet offers a wealth of content, making it increasingly difficult for users to navigate website information. The volume of hyperlinks on a website often leaves users struggling with content overload, hindering their ability to find relevant information of high interest. This problem highlights the critical need for tools to improve the user experience by providing personalized hyperlink recommendations on a specific website. This paper introduces HypeRec, a browser extension that attempts to address this problem by leveraging and comparing different machine learning and recommendation algorithms to guide users to content consistent with their interests and preferences. Our approach involves extracting hyperlinks from a webpage and subjecting the corresponding textual content to natural language processing techniques. In this way, it simplifies the users’ navigation within a website and promotes a more intuitive and satisfying web browsing experience.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {528–533},
numpages = {6},
keywords = {Content Overload, Hyperlink Analysis, Machine Learning, Natural Language Processing, Personalization, Recommendation Systems, User Experience, Web Navigation, Web Usability},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@inproceedings{10.1145/3372782.3406279,
author = {Malmi, Lauri and Sheard, Judy and Kinnunen, P\"{a}ivi and Simon and Sinclair, Jane},
title = {Theories and Models of Emotions, Attitudes, and Self-Efficacy in the Context of Programming Education},
year = {2020},
isbn = {9781450370929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372782.3406279},
doi = {10.1145/3372782.3406279},
abstract = {Research into the relationship between learning computing and students' attitudes, beliefs, and emotions often builds on theoretical frameworks from the social sciences in order to understand how these factors influence, for example, students' motivation, study practices, and learning results. In this paper we explore the computing education research literature to identify new theoretical constructs that have emerged from this research. We focus on empirical work in programming education that extends or adapts theories or instruments from the social sciences or that independently develops theories specific to programming. From an initial data set of more than 3800 papers published in the years 2010--2019, we identify 50 papers that present a range of domain-specific theoretical constructs addressing emotions, affect, beliefs, attitudes, and self-efficacy. They include 11 validated instruments and a number of statistical models, but also grounded theories and pedagogical models. We summarize the main results of many of these constructs and provide references for all of them. We also investigate how these constructs have informed further research by analysing over 850 papers that cite these 50 papers. We categorize the ways that theories can inform further research, and give examples of papers in each of these categories. Our findings indicate that among these categories, instruments have been most widely used in further research, thus affirming their value in the field.},
booktitle = {Proceedings of the 2020 ACM Conference on International Computing Education Research},
pages = {36–47},
numpages = {12},
keywords = {theory, theoretical construct, self-efficacy, research, programming, instrument, emotion, computing education, belief, attitude, affect},
location = {Virtual Event, New Zealand},
series = {ICER '20}
}

@inproceedings{10.1145/3603163.3609069,
author = {Gagliardi, Isabella and Artese, Maria Teresa},
title = {Intuitive Semantic Graph Tool for Enhanced Archive Exploration},
year = {2023},
isbn = {9798400702327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603163.3609069},
doi = {10.1145/3603163.3609069},
abstract = {The paper introduces a new method for visualizing and navigating information in a cultural heritage archive in a simple and intuitive way. The proposed approach employs pre-trained language models to cluster data and create semantic graphs. The creation of multilayer maps enables deep exploration of archives with large datasets, while the ability to handle multilingual datasets makes it suitable for archives with documents in various languages. These features combine to provide a user-friendly tool that can be adapted to different contexts and provides an overview of archive contents, to allow even non expert users to successfully query the archive.},
booktitle = {Proceedings of the 34th ACM Conference on Hypertext and Social Media},
articleno = {11},
numpages = {3},
keywords = {transformers, pre-trained language models, non-expert users, data visualization, clustering, archives, Bert},
location = {Rome, Italy},
series = {HT '23}
}

@article{10.1145/3680287,
author = {Degha, Houssem Eddine and Laallam, Fatima Zohra},
title = {ICA-CRMAS: Intelligent Context-Awareness Approach for Citation Recommendation based on Multi-Agent System},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3680287},
doi = {10.1145/3680287},
abstract = {Navigating the ever-expanding sea of scientific literature presents a daunting challenge for researchers seeking relevant and up-to-date information. Traditional citation recommendation systems, while well-intentioned, often fall short due to their limited focus on text-based features and lack of contextual awareness. In this article, we introduce the ICA-CRMAS (Intelligent Context-Aware Approach for Citation Recommendation based on Multi-Agent System), an intelligent system that leverages the power of deep learning, semantic analysis, and multimodal learning to overcome these limitations. ICA-CRMAS goes beyond the surface, delving into the rich tapestry of information within academic papers, including figures, which often hold vital contextual clues. By weaving this contextual data directly into its recommendation models, ICA-CRMAS generates highly personalized and relevant suggestions. This comprehensive approach unlocks enhanced accuracy, diversity, and serendipity, enabling researchers to effectively discover papers aligning with their interests and research objectives. ICA-CRMAS illuminates its reasoning. Instead of opaque suggestions, the system provides clear explanations that justify and illustrate recommended citations. This transparency builds user confidence, allowing researchers to critically engage with and trust the system’s recommendations. Evaluation experiments conducted on real-world academic datasets demonstrate that ICA-CRMAS outperforms existing approaches across various metrics. it surpassing its closest competitor by a margin of 7.53 on accuracy, 6.07\% on MRR and by 5.87 on Recall. User feedback further reinforces its effectiveness, with an Overall System Usability Scale (SUS) score of 76.73, exceeding benchmark scores for comparable systems.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {13},
numpages = {52},
keywords = {Multi-agent systems, ontology, scientific papers, recommendation systems, search engines, user profiles, personalized information filtering, world wide web, context-awareness}
}

@inproceedings{10.1145/3417990.3421411,
author = {Theisz, Zolt\'{a}n and B\'{a}csi, S\'{a}ndor and Mezei, Gergely and Somogyi, Ferenc A. and Palatinszky, D\'{a}niel},
title = {Join potency: a way of combining separate multi-level models},
year = {2020},
isbn = {9781450381352},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3417990.3421411},
doi = {10.1145/3417990.3421411},
abstract = {Multi-level modeling has become a mature modeling paradigm both theoretically and by technical means. It has proved itself when a single domain has to be created without accidental complexity. However, when several interconnected domains are to be handled, multi-level modeling is still not as capable as legacy metamodeling. Our position paper aims to narrow the gap by the introduction of a novel technique that can combine several multi-level models from different domains statically. Besides its theoretical proposal, the solution is also defined in our multi-layer modeling framework (Dynamic Multi-Layer Algebra) and is demonstrated by an illustrative example.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
articleno = {84},
numpages = {5},
keywords = {potency notion, multi-level modeling, megamodeling, clabject},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inbook{10.1145/3677389.3702564,
author = {Jin, Yan and Ren, Zongxing and Bi, Chongwu and Sun, Zhuo and Yang, Ruixian},
title = {Knowledge Graph Construction of Chinese Traditional Yu Opera Based on Joint Entity-Relation Extraction Method},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702564},
abstract = {Knowledge of Chinese traditional opera is primarily preserved in informal forms, such as ancient texts, cultural relics, and oral traditions. To appreciate the unique charm of traditional opera, we focus on Yu Opera as a case study and develop the Yu Opera Knowledge Resource (YOKR) Ontology. Our approach involves collecting data from specialized literature, semantically segmenting it, and using the Bert4torch model for entity and relationship extraction. This paper explores the connections among historical figures, roles, plays, music, costumes, and other elements, aiming to enrich knowledge in Yu Opera and provide effective knowledge management services.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {81},
numpages = {3}
}

@inproceedings{10.1145/3322640.3326728,
author = {Zhong, Linwu and Zhong, Ziyi and Zhao, Zinian and Wang, Siyuan and Ashley, Kevin D. and Grabmair, Matthias},
title = {Automatic Summarization of Legal Decisions using Iterative Masking of Predictive Sentences},
year = {2019},
isbn = {9781450367547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322640.3326728},
doi = {10.1145/3322640.3326728},
abstract = {We report on a pilot experiment in automatic, extractive summarization of legal cases concerning Post-traumatic Stress Disorder from the US Board of Veterans' Appeals. We hypothesize that length-constrained extractive summaries benefit from choosing among sentences that are predictive for the case outcome. We develop a novel train-attribute-mask pipeline using a CNN classifier to iteratively select predictive sentences from the case, which measurably improves prediction accuracy on partially masked decisions. We then select a subset for the summary through type classification, maximum marginal relevance, and a summarization template. We use ROUGE metrics and a qualitative survey to evaluate generated summaries along with expert-extracted and expert-drafted summaries. We show that sentence predictiveness does not reliably cover all decision-relevant aspects of a case, illustrate that lexical overlap metrics are not well suited for evaluating legal summaries, and suggest that future work should focus on case-aspect coverage.},
booktitle = {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law},
pages = {163–172},
numpages = {10},
keywords = {text classification, legal case summarization},
location = {Montreal, QC, Canada},
series = {ICAIL '19}
}

@article{10.1145/3295822,
author = {Guo, Yangyang and Cheng, Zhiyong and Nie, Liqiang and Wang, Yinglong and Ma, Jun and Kankanhalli, Mohan},
title = {Attentive Long Short-Term Preference Modeling for Personalized Product Search},
year = {2019},
issue_date = {April 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3295822},
doi = {10.1145/3295822},
abstract = {E-commerce users may expect different products even for the same query, due to their diverse personal preferences. It is well known that there are two types of preferences: long-term ones and short-term ones. The former refers to users’ inherent purchasing bias and evolves slowly. By contrast, the latter reflects users’ purchasing inclination in a relatively short period. They both affect users’ current purchasing intentions. However, few research efforts have been dedicated to jointly model them for the personalized product search. To this end, we propose a novel Attentive Long Short-Term Preference model, dubbed as ALSTP, for personalized product search. Our model adopts the neural networks approach to learn and integrate the long- and short-term user preferences with the current query for the personalized product search. In particular, two attention networks are designed to distinguish which factors in the short-term as well as long-term user preferences are more relevant to the current query. This unique design enables our model to capture users’ current search intentions more accurately. Our work is the first to apply attention mechanisms to integrate both long- and short-term user preferences with the given query for the personalized search. Extensive experiments over four Amazon product datasets show that our model significantly outperforms several state-of-the-art product search methods in terms of different evaluation metrics.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {19},
numpages = {27},
keywords = {long short-term preference, attention mechanism, Personalized product search}
}

@inproceedings{10.1145/3184558.3191527,
author = {Nikolov, Andriy and Haase, Peter and Herzig, Daniel M. and Trame, Johannes and Kozlov, Artem},
title = {Combining RDF Graph Data and Embedding Models for an Augmented Knowledge Graph},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191527},
doi = {10.1145/3184558.3191527},
abstract = {Vector embedding models have recently become popular for encoding both structured and unstructured data. In the context of knowledge graphs such models often serve as additional evidence supporting various tasks related to the knowledge base population: e.g., information extraction or link prediction to expand the original dataset. However, the embedding models themselves are often not used directly alongside structured data: they merely serve as additional evidence for structured knowledge extraction. In the metaphactory knowledge graph management platform, we use federated hybrid SPARQL queries for combining explicit information stated in the graph, implicit information from the associated embedding models, and information extracted using vector embeddings in a transparent way for the end user. In this paper we show how we integrated RDF data with vector space models to construct an augmented knowledge graph to be used in customer applications.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {977–980},
numpages = {4},
keywords = {deep learning, graph embeddings, knowledge graph, machine learning, query federation},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.1109/TCBB.2020.3000518,
author = {V, Sunil Kumar P and Thahsin, Adheeba and M, Manju and G, Gopakumar},
title = {A Heterogeneous Information Network Model for Long Non-Coding RNA Function Prediction},
year = {2020},
issue_date = {Jan.-Feb. 2022},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {19},
number = {1},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2020.3000518},
doi = {10.1109/TCBB.2020.3000518},
abstract = {Exciting information on the functional roles played by long non-coding RNA (lncRNA) has drawn substantial research attention these days. With the advent of techniques such as RNA-Seq, thousands of lncRNAs are identified in very short time spans. However, due to the poor annotation rate, only a few of them are functionally characterised. The wet lab experiments to elucidate lncRNA functions are challenging, slow progressing and sometimes prohibitively expensive. This work attempts to solve the crucial problem of developing computational methods to predict lncRNA functions. The model presented here, predicts the functions of lncRNAs by making use of a meta-path based measure, AvgSim on a Heterogeneous Information Network (HIN). The network is constructed from existing protein and function association data of lncRNAs, lncRNA co-expression data and protein protein interaction data. Out of the 2,758 lncRNA considered for the experiment, the proposed method predicts possible functions for 2,695 lncRNAs with an accuracy of 73.68 percent and found to perform better than the other state-of-the-art approaches for an independent test set. A case study of two well-known lncRNAs (HOTAIR and H19) is conducted and the associated functions are identified. The results were validated using experimental evidence from the literature. The script and data used for the implementation of the model is freely available at: &lt;uri&gt;http://bdbl.nitc.ac.in/LncFunPred/index.html&lt;/uri&gt;.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = jun,
pages = {255–266},
numpages = {12}
}

@inproceedings{10.1109/JCDL.2019.00117,
author = {Weber, Nicholas and Fenlon, Katrina and Organisciak, Peter and Thomer, Andrea K.},
title = {Conceptual models in digital libraries, archives, and museums},
year = {2020},
isbn = {9781728115474},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL.2019.00117},
doi = {10.1109/JCDL.2019.00117},
abstract = {This workshop addresses the development, use, and evolution of conceptual models in the context of digital libraries, archives, and museums. The workshop will convene domain practitioners and researchers in order to formalize a research agenda for conceptual modelling in digital libraries, and foster a cooperative research community to make progress on these topics over the coming decade. Activities at the workshop will include paper presentations, round-table discussions, and a keynote from an expert in the conceptual and logical foundations of information organization systems. Workshop papers as well as a summary of the proceedings will be published in a free and openly accessible repository.},
booktitle = {Proceedings of the 18th Joint Conference on Digital Libraries},
pages = {457–458},
numpages = {2},
keywords = {digital libraries, conceptual models},
location = {Champaign, Illinois},
series = {JCDL '19}
}

@inproceedings{10.1145/3371300.3383339,
author = {Pelzetter, Jens},
title = {A declarative model for accessibility requirements},
year = {2020},
isbn = {9781450370561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371300.3383339},
doi = {10.1145/3371300.3383339},
abstract = {The web has become the primary source of information for many people. Many services are provided are on the web. Despite extensive guidelines for the accessibility of web pages, many web sites are not accessible making these web sites difficult or impossible to use for people with disabilities. Evaluating the accessibility of web pages can either be done manually, which is a very laborious task or using automated tools. Unfortunately, the results from different tools are often inconsistent because of the ambiguity of the current guidelines. In this paper, a declarative approach for describing the requirements for accessible web pages is presented. This declarative model will help developers of accessibility evaluation tools to create tools that produce more consistent results and are easier to maintain.},
booktitle = {Proceedings of the 17th International Web for All Conference},
articleno = {4},
numpages = {10},
keywords = {accessibility, WCAG, ACT rules},
location = {Taipei, Taiwan},
series = {W4A '20}
}

@article{10.1145/3725729,
author = {Xu, Nuo and Wang, Pinghui and Liang, Zi and Zhao, Junzhou and Guan, Xiaohong},
title = {How Vital Is the Jurisprudential Relevance: Law Article-Intervened Legal Case Retrieval and Matching},
year = {2025},
issue_date = {July 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {1046-8188},
url = {https://doi.org/10.1145/3725729},
doi = {10.1145/3725729},
abstract = {Legal case retrieval aims to automatically scour comparable legal cases based on a given query, which is crucial for offering relevant precedents to support the judgment in intelligent legal systems. Due to similar goals, it is often associated with a similar case matching task. To address them, a daunting challenge is assessing the uniquely defined legal-rational similarity within the judicial domain, which distinctly deviates from the semantic similarities in general text retrieval. Past works either tagged domain-specific factors or incorporated reference laws to capture legal-rational information. However, their heavy reliance on expert or unrealistic assumptions restricts their practical applicability in real-world scenarios. In this article, we propose an end-to-end model named LCM-LAI to solve the above challenges. Through meticulous theoretical analysis, LCM-LAI employs a dependent multi-task learning framework to capture legal-rational information within legal cases by a law article prediction sub-task, without any additional assumptions in inference. In addition, LCM-LAI proposes an article-aware attention mechanism to evaluate the legal-rational similarity between across-case sentences based on the law distribution, which is more effective than semantic similarity. We perform a series of exhaustive experiments that include two different tasks that involving four real-world datasets. The results demonstrate that LCM-LAI achieves state-of-the-art performance.},
journal = {ACM Trans. Inf. Syst.},
month = may,
articleno = {85},
numpages = {32},
keywords = {legal case retrieval, legal case matching, dependent multi-task learning}
}

@article{10.1145/3274784.3274789,
author = {Ferro, Nicola and Fuhr, Norbert and Grefenstette, Gregory and Konstan, Joseph A. and Castells, Pablo and Daly, Elizabeth M. and Declerck, Thierry and Ekstrand, Michael D. and Geyer, Werner and Gonzalo, Julio and Kuflik, Tsvi and Lindn, Krister and Magnini, Bernardo and Nie, Jian-Yun and Perego, Raffaele and Shapira, Bracha and Soboroff, Ian and Tintarev, Nava and Verspoor, Karin and Willemsen, Martijn C. and Zobel, Justin},
title = {The Dagstuhl Perspectives Workshop on Performance Modeling and Prediction},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0163-5840},
url = {https://doi.org/10.1145/3274784.3274789},
doi = {10.1145/3274784.3274789},
abstract = {This paper reports the findings of the Dagstuhl Perspectives Workshop 17442 on performance modeling and prediction in the domains of Information Retrieval, Natural language Processing and Recommender Systems. We present a framework for further research, which identifies five major problem areas: understanding measures, performance analysis, making underlying assumptions explicit, identifying application features determining performance, and the development of prediction models describing the relationship between assumptions, features and resulting performance.},
journal = {SIGIR Forum},
month = aug,
pages = {91–101},
numpages = {11}
}

@inproceedings{10.1145/3709026.3709114,
author = {Chang, Bingtao and Wen, Weiping and Wu, Xiaojie and Cheng, Siyang and Jiang, Jianchun and Mei, Rui},
title = {TCLens: Towards Toxicity Tags Aggregation of Massive Labels Generated by Content Moderation for AIGC},
year = {2025},
isbn = {9798400718182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709026.3709114},
doi = {10.1145/3709026.3709114},
abstract = {The recent boost of artificial intelligence represented by Large Language Models (LLMs) is surging. Due to the outstanding performance of LLMs, AI-Generated Content (AIGC) has also made important progress in multimodal knowledge creation referring to text, image, audio, and video. However, the security, privacy, and ethical risks associated with AIGC (e.g., fake news, social engineering attacks, and toxic content) have deeply weakened the compliance of AIGC. Although existing content moderation solutions can filter out several types of toxic content, the audit performance of different vendors and techniques are of varying quality. Some AIGC service providers improve the moderation effectiveness by introducing multiple sources of audit vendors. Due to the lack of general content moderation standards and taxonomy, the labels of multi-source moderation vendors vary greatly. To this end, We propose a novel massive label aggregation approach for content moderation named TCLens. First, we collect results of multi-vendor content moderation engines for building massive toxic labels for AIGC. Then, we introduce an ontology for better tagging with the capability of automatic updating and vendor-agnostic. Finally, we implement a prototype of TCLens. Our evaluation demonstrates that it outperforms single-source tagging and existing SOTA solutions.},
booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence},
pages = {466–473},
numpages = {8},
keywords = {information content moderation, toxicity tags, labels aggregation, AIGC},
location = {
},
series = {CSAI '24}
}

@inproceedings{10.1145/3704814.3704819,
author = {Pang, Haijie and Li, Chengji},
title = {A BERT and TextCNN integration-based Method for Public Complaints and Proposals Text Classification},
year = {2025},
isbn = {9798400718090},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3704814.3704819},
doi = {10.1145/3704814.3704819},
abstract = {With the wide use of Blockchain technology and online public complaints and proposals (hereinafter referred to as PCP) platforms, huge amounts of data are accordingly produced, which poses a challenge on the government's effectively choosing and classifying the key information. In view of this, this paper endeavors to conduct research on the PCP text classification for a smart government. Firstly, PCP texts are collected automatically and preprocessed, and then a hierarchical classification structure involving targeted government departments and PCP contents is proposed. Finally, a BERT and TextCNN model is adopted for PCP text classification. The experimental results show that the classification structure and model proposed in this paper can effectively categorize the PCP text, providing a scientific analysis and effective technological method to improve the work efficiency and service quality of PCP.},
booktitle = {Proceedings of the 8th International Conference on Computer Science and Application Engineering},
pages = {15–18},
numpages = {4},
keywords = {BERT, Blockchain, Public complaints and proposals, Text classification, TextCNN},
location = {
},
series = {CSAE '24}
}

@inproceedings{10.1145/3310986.3311025,
author = {Jin, Ying and Zhao, Shuai and Wu, Yudong},
title = {Geographic Entity Relationship Extraction Model Based on Piecewise Convolution of Residual Network},
year = {2019},
isbn = {9781450366120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310986.3311025},
doi = {10.1145/3310986.3311025},
abstract = {Nowadays, geographic entity relationship extraction systems generally rely on artificial feature extraction. These features either require complex and complete data sets, or cannot describe deep features such as semantics. And data sets that can be used for geographic relationship extraction are scarce. To tackle these problems, this paper uses distant supervision to map existing knowledge bases into rich unstructured data which contributes to a large amount of training data. In training, this paper uses the deep residual network to extract more abstract and deeper features. Then the piecewise max pooling and selective attention mechanisms are used to further improve the accuracy of the model. Finally, the experimental results show that the deeper network and the piecewise max pooling significantly improve the extraction results.},
booktitle = {Proceedings of the 3rd International Conference on Machine Learning and Soft Computing},
pages = {160–165},
numpages = {6},
keywords = {relationship extraction, distant supervision, attention, ResNet, PCNN, Gis},
location = {Da Lat, Viet Nam},
series = {ICMLSC '19}
}

@inproceedings{10.1145/3477314.3507177,
author = {Groza, Adrian and Nitu, Cristian},
title = {Question answering over logic puzzles using theorem proving},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507177},
doi = {10.1145/3477314.3507177},
abstract = {We developed a tool to automatically solve logical puzzles in natural language. The solution is composed by a parser and an inference engine. The parser translates the text into First Order Logic (FOL), while the Mace4 model finder computes the interpretation models of the given FOL theory. The solver uses the Prover9 theorem prover to compute Yes/No answers to natural language questions related to each puzzle. Each answer is backup by a graphical representation of its proof, which is in line with Explainable Artificial Intelligence (XAI). The advantage of using reasoning for Natural Language Understanding (NLU) instead of learned models is that the user can obtain an explanation of the reasoning chain. We illustrate how the system performs on 382 knights and knaves puzzles. These features together with the overall performance rate of 80.89\% makes the proposed solution an improvement upon similar solvers for natural language understanding in the puzzles domain.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {871–874},
numpages = {4},
keywords = {explainable ai, logical puzzles, natural language understanding, question answering},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3383583.3398530,
author = {Vogt, Lars and D'Souza, Jennifer and Stocker, Markus and Auer, S\"{o}ren},
title = {Toward Representing Research Contributions in Scholarly Knowledge Graphs Using Knowledge Graph Cells},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398530},
doi = {10.1145/3383583.3398530},
abstract = {There is currently a gap between the natural language expression of scholarly publications and their structured semantic content modeling to enable intelligent content search. With the volume of research growing exponentially every year, a search feature operating over semantically structured content is compelling. Toward this end, in this work, we propose a novel semantic data model for modeling the contribution of scientific investigations. Our model, i.e. the Research Contribution Model (RCM), includes a schema of pertinent concepts highlighting six core information units, viz. Objective, Method, Activity, Agent, Material, and Result, on which the contribution hinges. It comprises bottom-up design considerations made from three scientific domains, viz. Medicine, Computer Science, and Agriculture, which we highlight as case studies. For its implementation in a knowledge graph application we introduce the idea of building blocks called Knowledge Graph Cells (KGC), which provide the following characteristics: (1) they limit the expressibility of ontologies to what is relevant in a knowledge graph regarding specific concepts on the theme of research contributions; (2) they are expressible via ABox and TBox expressions; (3) they enforce a certain level of data consistency by ensuring that a uniform modeling scheme is followed through rules and input controls; (4) they organize the knowledge graph into named graphs; (5) they provide information for the front end for displaying the knowledge graph in a human-readable form such as HTML pages; and (6) they can be seamlessly integrated into any existing publishing process thatsupports form-based input abstracting its semantic technicalities including RDF semantification from the user. Thus RCM joins the trend of existing work toward enhanced digitalization of scholarly publication enabled by an RDF semantification as a knowledge graph fostering the evolution of the scholarly publications beyond written text.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {107–116},
numpages = {10},
keywords = {semantic publishing, scholarly infrastructure, open science, ontology, machine actionability, fair data principles, digital libraries},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3555776.3577686,
author = {Loseto, Giuseppe and Scioscia, Floriano and Ruta, Michele and Gramegna, Filippo and Bilenchi, Ivano},
title = {Semantic-based Adaptation of Quality of Experience in Web Multimedia Streams},
year = {2023},
isbn = {9781450395175},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555776.3577686},
doi = {10.1145/3555776.3577686},
abstract = {Video streaming accounts for the majority of worldwide Internet traffic, and HTTP-based multimedia has the largest share among technologies and protocols. The wide availability of mobile devices and wireless broadband networks currently leads to wider heterogeneity of fruition contexts and frequent condition changes during a streaming session. MPEG-DASH is the reference standard for Dynamic Adaptive Streaming over HTTP: a provider defines several representations for a segmented multimedia source, with different bit rates, allowing a client to dynamically select the best one based on current conditions, and to download the corresponding sequence of segments for smooth playback. MPEG-DASH does not mandate specific bit rate adaptation schemes; conventional approaches are divided in buffer-based, bandwidth-based and hybrid. Nevertheless, Quality of Experience (QoE) can be influenced by many additional factors. This paper proposes a novel QoE adaptation approach based on dynamic ontology-based annotation of streaming context and mobile matchmaking with DASH representation profiles in a Web Ontology Language (OWL) fragment, exploiting a WebAssembly port of an embedded reasoning engine. The proposed framework enables adaptation based not only on network status, but also on client device capabilities, ambient conditions and multimedia content type. A case study validates the proposal, while early experiments support its sustainability.},
booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
pages = {1821–1830},
numpages = {10},
keywords = {MPEG-DASH, multimedia streaming, quality of experience, web ontology language (OWL), semantic matchmaking},
location = {Tallinn, Estonia},
series = {SAC '23}
}

@inproceedings{10.1145/3448139.3448180,
author = {Lin, Yiwen and Dowell, Nia and Godfrey, Andrew},
title = {Skills Matter: Modeling the relationship between decision making processes and collaborative problem-solving skills during Hidden Profile Tasks},
year = {2021},
isbn = {9781450389358},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448139.3448180},
doi = {10.1145/3448139.3448180},
abstract = {Collaborative problem-solving (CPS) is one of the most essential 21st century skills for success across educational and professional settings. The hidden-profile paradigm is one of the most prominent avenues of studying group decision making and underlying issues in information sharing. Previous research on the hidden-profile paradigm has primarily focused on static constructs (e.g., group size, group expertise), or on the information itself (whether certain pieces of information is being shared). In the current study, we propose a lens on individual and group’s collaborative problem-solving skills, to explore the relationships between dynamic discourse processes and decision making in a distributed information environment. Specifically, we sought to examine CPS skills in association with decision change and productive decision-making. Our results suggest that while sharing information has significantly positive association with decision change and effective decision-making, other aspects of social processes appear to be negatively correlated with these outcomes. Cognitive CPS skills, however, exhibit a strong positive relationship with making a (productive) change in students final decisions. We also find that these results are more pronounced at the group level, particularly with cognitive CPS skills. Our study shed lights on a more nuanced picture of how social and cognitive CPS interactions are related to effective information sharing and decision making in collaborative problem-solving interactions.},
booktitle = {LAK21: 11th International Learning Analytics and Knowledge Conference},
pages = {428–437},
numpages = {10},
keywords = {group processes, decision making, collaborative problem solving, Hidden-profile paradigm},
location = {Irvine, CA, USA},
series = {LAK21}
}

@inproceedings{10.1145/3603765.3603771,
author = {Diaz Gonzalez, Armando D. and Hughes, Kevin S. and Yue, Songhui and Hayes, Sean T.},
title = {Applying BioBERT to Extract Germline Gene-Disease Associations for Building a Knowledge Graph from the Biomedical Literature},
year = {2023},
isbn = {9798400700637},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603765.3603771},
doi = {10.1145/3603765.3603771},
abstract = {Published biomedical information has and continues to rapidly increase. The recent advancements in Natural Language Processing (NLP), have generated considerable interest in automating the extraction, normalization, and representation of biomedical knowledge about entities such as genes and diseases. Our study analyzes germline abstracts in the construction of knowledge graphs of the immense work that has been done in this area for genes and diseases. This paper presents SimpleGermKG, an automatic knowledge graph construction approach that connects germline genes and diseases. For the extraction of genes and diseases, we employ BioBERT, a pre-trained BERT model on biomedical corpora. We propose an ontology-based and rule-based algorithm to standardize and disambiguate medical terms. For semantic relationships between articles, genes, and diseases, we implemented a part-whole relation approach to connect each entity with its data source and visualize them in a graph-based knowledge representation. Lastly, we discuss the knowledge graph applications, limitations, and challenges to inspire the future research of germline corpora. Our knowledge graph contains 297 genes, 130 diseases, and 46,747 triples. Graph-based visualizations are used to show the results.},
booktitle = {Proceedings of the 2023 7th International Conference on Information System and Data Mining},
pages = {37–42},
numpages = {6},
keywords = {BioBERT, entity recognition, germline mutations, knowledge graph, semantic relation},
location = {Atlanta, USA},
series = {ICISDM '23}
}

@inproceedings{10.1145/3342558.3345414,
author = {Piotrowski, Michael},
title = {A Vision for User-Defined Semantic Markup},
year = {2019},
isbn = {9781450368872},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342558.3345414},
doi = {10.1145/3342558.3345414},
abstract = {Typesetting systems, such as LATEX, permit users to define custom markup and corresponding formatting to simplify authoring, ensure the consistent presentation of domain-specific recurring elements and, potentially, enable further processing, such as the generation of an index of such elements. In XML-based and similar systems, the separation of content and form is also reflected in the processing pipeline: while document authors can define custom markup, they cannot define its semantics. This could be said to be intentional to ensure structural integrity of documents, but at the same time it limits the expressivity of markup. The latter is particularly true for so-called lightweight markup languages like Mark-down, which only define very limited sets of generic elements. This vision paper sketches an approach for user-defined semantic markup that could permit authors to define the semantics of elements by formally describing the relations between its constituent parts and to other elements, and to define a formatting intent that would ensure that a default presentation is always available.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2019},
articleno = {28},
numpages = {4},
keywords = {scholarly publishing, markup semantics, document models and structures, document authoring},
location = {Berlin, Germany},
series = {DocEng '19}
}

@article{10.1145/3505639.3505647,
author = {Yu, Xi and Zaza, Sam and Schuberth, Florian and Henseler, J\"{o}rg},
title = {Counterpoint: Representing Forged Concepts as Emergent Variables Using Composite-Based Structural Equation Modeling},
year = {2022},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {SI},
issn = {0095-0033},
url = {https://doi.org/10.1145/3505639.3505647},
doi = {10.1145/3505639.3505647},
abstract = {Studying and modeling theoretical concepts is a cornerstone activity in information systems (IS) research. Researchers have been familiar with one type of theoretical concept, namely behavioral concepts, which are assumed to exist in nature and measured by a set of observable variables. In this paper, we present a second type of theoretical concept, namely forged concepts, which are designed and assumed to emerge within their environment. While behavioral concepts are classically operationalized as latent variables, forged concepts are better specified as emergent variables. Additionally, we propose composite-based structural equation modeling (SEM) as a subtype of SEM that is eminently suitable to analyze models containing emergent variables. We shed light on the composite-based SEM steps: model specification, model identification, model estimation, and model assessment. Then, we present an illustrative example from the domain of IS research to demonstrate these four steps and show how modeling with emergent variables proceeds.},
journal = {SIGMIS Database},
month = dec,
pages = {114–130},
numpages = {17},
keywords = {forged concept, emergent variables, composite-based structural equation modeling, composite model, behavioral concept}
}

@inproceedings{10.1145/3640457.3688071,
author = {Irrera, Ornella and Lissandrini, Matteo and Dell'Aglio, Daniele and Silvello, Gianmaria},
title = {Reproducibility and Analysis of Scientific Dataset Recommendation Methods},
year = {2024},
isbn = {9798400705052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640457.3688071},
doi = {10.1145/3640457.3688071},
abstract = {Datasets play a central role in scholarly communications. However, scholarly graphs are often incomplete, particularly due to the lack of connections between publications and datasets. Therefore, the importance of dataset recommendation—identifying relevant datasets for a scientific paper, an author, or a textual query—is increasing. Although various methods have been proposed for this task, their reproducibility remains unexplored, making it difficult to compare them with new approaches. We reviewed current recommendation methods for scientific datasets, focusing on the most recent and competitive approaches, including an SVM-based model, a bi-encoder retriever, a method leveraging co-authors and citation network embeddings, and a heterogeneous variational graph autoencoder. These approaches underwent a comprehensive analysis under consistent experimental conditions. Our reproducibility efforts show that three methods can be reproduced, while the graph variational autoencoder is challenging due to unavailable code and test datasets. Hence, we re-implemented this method and performed a component-based analysis to examine its strengths and limitations. Furthermore, our study indicated that three out of four considered methods produce subpar results when applied to real-world data instead of specialized datasets with ad-hoc features.},
booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
pages = {570–579},
numpages = {10},
keywords = {Dataset Recommendations, Recommender Systems, Reproducibility},
location = {Bari, Italy},
series = {RecSys '24}
}

@inproceedings{10.1145/3389189.3397978,
author = {Sosnowski, Tomasz and Yordanova, Kristina},
title = {A probabilistic conversational agent for intelligent tutoring systems},
year = {2020},
isbn = {9781450377737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3389189.3397978},
doi = {10.1145/3389189.3397978},
abstract = {The paper proposes a novel approach for a Conversational Intelligent Tutoring System, which combines Natural Language Processing techniques with dynamic Bayesian Inference for probabilistic student state estimation. Our aim is to develop and implement the probabilistic model for the discourse between the user and the Conversational Agent. Most of the already proposed tutoring systems interacting with the user via natural language use rather simple pattern matching rules to carry on the conversation. Even when the fuzzy pattern matching is employed, already existing approaches use these fuzzy values only to find a single best-matching rule, thus immediately discretising the result. Our proposed approach differs in this regard, as we would like to use the fuzzy values resulting from pattern matching as inputs for our probabilistic model. To be more precise, we would like to treat these values as observations for the state estimation in the dynamic Bayesian Network. Our approach is, not coincidentally, similar to the methods typical for state and action recognition. This paper focuses on the Natural Language processing aspect, while the probabilistic model is described in a separate work.},
booktitle = {Proceedings of the 13th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {43},
numpages = {7},
keywords = {text similarity, sentence similarity, semantic similarity, natural language understanding, natural language processing, markov chain, intelligent tutoring system, dynamic bayesian inference, conversational agent, computational state space models},
location = {Corfu, Greece},
series = {PETRA '20}
}

@inproceedings{10.1145/3596947.3596952,
author = {Skobelev, Petr and Simonova, Elena and Tabachinskiy, Aleksey and Kudryakov, Evgeniy and Strizhakov, Anatoly and Goryanin, Oleg and Ermakov, Vasiliy and Chan, Yung-Kuan and Lee, Tzong-Ru and Sung, Yu},
title = {Concept and Development of a Multi-Agent Digital Twin of Plant Focused on Broccoli},
year = {2023},
isbn = {9781450399920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3596947.3596952},
doi = {10.1145/3596947.3596952},
abstract = {The paper discusses the principles of developing a multi-agent digital twin of plants using broccoli as an example of plants. The developed model of the digital twin of plants must meet the following requirements: real-time environmental data acquisition, user feedback collection, continuous adaptation of the plant development plan for each event, individual instance for field or field part. The digital twin of plant is designed as an intelligent cyber-physical system that has a user-defined knowledge bas and a multi-agent system for planning and modeling of plant growth and development, as well as for forecasting crop parameters. For this purpose, a new method for estimate stage duration and yield is proposed, which defines a "tube" – a corridor to each of the factors corresponding plant development. The key factors have been determined during consultations with practicing agronomists but can be adjusted by users experience. This concept was originally introduced for wheat digital twin, but now is scaled and modified to simulate broccoli growth process.},
booktitle = {Proceedings of the 2023 7th International Conference on Intelligent Systems, Metaheuristics \&amp; Swarm Intelligence},
pages = {132–138},
numpages = {7},
keywords = {precision farming, ontology, multi-agent technologies, multi-agent model, knowledge base, intelligent services, digital twin, broccoli cultivation},
location = {Virtual Event, Malaysia},
series = {ISMSI '23}
}

@inproceedings{10.1145/3508397.3564853,
author = {Beldi, Amal and Sassi, Salma and Jemai, Abedrazzek},
title = {Learn2Sum: A New Approach to Unsupervised Text Summarization Based on Topic Modeling},
year = {2022},
isbn = {9781450392198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3508397.3564853},
doi = {10.1145/3508397.3564853},
abstract = {Due to the enormous volume of data on the web, it is hard for the user to retrieve effective and useful information within the right time. Thus, it has become a need to generate a brief summary from a large amount of textual data according to the user profile. In this context, text summarization is used to identify important information within text documents. It aims to generate shorter versions of the source text, by including only the relevant and salient information. In recent years, the research on summarization techniques based on topic modeling techniques has become a hot topic among researchers thanks to their ability to classify, understand a large text corpora and extract important topics on the text. However, existing studies do not provide the support of personalization when generating summaries because they need to know not only which documents are most helpful to the users, but also which topics and keywords are more or less related to the user' interests. Thus, existing studies lack of the support of adaptive user modeling for user applications in the emerging areas of automatic summarization, topic modeling and visualization. In this context, we propose a new approach of automated text summarization based on topic modeling techniques and taking into account the user's profile which helps to semantically extract relevant topics of textual documents, summarizing information according to the user' topics interests and finally visualize them through a hyper-graph Experiments have been conducted to measure the effectiveness of our solution compared to existing summarizing approaches based on text content. The results show the superiority of our approach.},
booktitle = {Proceedings of the 14th International Conference on Management of Digital EcoSystems},
pages = {136–143},
numpages = {8},
keywords = {user profile, topics, topic modeling, text transformation, summarization, graph, classification},
location = {Venice, Italy},
series = {MEDES '22}
}

@article{10.1145/3590773,
author = {Becattini, Federico and Bongini, Pietro and Bulla, Luana and Bimbo, Alberto Del and Marinucci, Ludovica and Mongiov\`{\i}, Misael and Presutti, Valentina},
title = {VISCOUNTH: A Large-scale Multilingual Visual Question Answering Dataset for Cultural Heritage},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {1551-6857},
url = {https://doi.org/10.1145/3590773},
doi = {10.1145/3590773},
abstract = {Visual question answering has recently been settled as a fundamental multi-modal reasoning task of artificial intelligence that allows users to get information about visual content by asking questions in natural language. In the cultural heritage domain, this task can contribute to assisting visitors in museums and cultural sites, thus increasing engagement. However, the development of visual question answering models for cultural heritage is prevented by the lack of suitable large-scale datasets. To meet this demand, we built a large-scale heterogeneous and multilingual (Italian and English) dataset for cultural heritage that comprises approximately 500K Italian cultural assets and 6.5M question-answer pairs. We propose a novel formulation of the task that requires reasoning over both the visual content and an associated natural language description, and present baselines for this task. Results show that the current state of the art is reasonably effective but still far from satisfactory; therefore, further research in this area is recommended. Nonetheless, we also present a holistic baseline to address visual and contextual questions and foster future research on the topic.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = jul,
articleno = {193},
numpages = {20},
keywords = {cultural heritage, Visual question answering}
}

@inproceedings{10.1145/3631700.3665237,
author = {Zavarella, Vanni and Reforgiato, Diego and Consoli, Sergio and Fenu, Gianni},
title = {Charting the Landscape of Digital Health: Towards A Knowledge Graph Approach to News Media Analysis},
year = {2024},
isbn = {9798400704666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3631700.3665237},
doi = {10.1145/3631700.3665237},
abstract = {In this paper, we present our currently on-going work on a method for analyzing digital health transformation in our society by constructing a Knowledge Graph from a large corpus of 7.8 million English news articles, dating from 1987 through 2023. We firstly sampled around 95k articles relevant to the Digital Health topic by training and deploying a Deep Learning binary classifier via fine-tuning BERT. Successively, by deploying NLP techniques, we extracted triples from the identified articles to form a Digital Health News Knowledge Graph, which consists of 431k distinct triples connecting 186k entities through 1866 relations. The constructed Knowledge Graph provides insights into the evolution of Digital Health in news media and serves as a resource for further research in the field. The analysis that we have carried out reveals significant trends in Digital Health as reflected in the news, with notable peaks coinciding with key events like the COVID-19 pandemic.},
booktitle = {Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {419–423},
numpages = {5},
location = {Cagliari, Italy},
series = {UMAP Adjunct '24}
}

@article{10.1145/3371906,
author = {Kienzle, J\"{o}rg and Mussbacher, Gunter and Combemale, Benoit and Bastin, Lucy and Bencomo, Nelly and Bruel, Jean-Michel and Becker, Christoph and Betz, Stefanie and Chitchyan, Ruzanna and Cheng, Betty H. C. and Klingert, Sonja and Paige, Richard F. and Penzenstadler, Birgit and Seyff, Norbert and Syriani, Eugene and Venters, Colin C.},
title = {Toward model-driven sustainability evaluation},
year = {2020},
issue_date = {March 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {3},
issn = {0001-0782},
url = {https://doi.org/10.1145/3371906},
doi = {10.1145/3371906},
abstract = {Exploring the vision of a model-based framework that may enable broader engagement with and informed decision making about sustainability issues.},
journal = {Commun. ACM},
month = feb,
pages = {80–91},
numpages = {12}
}

@inproceedings{10.1145/3587259.3627563,
author = {Alghamdi, Ghadah Abdulrahman S and Schmidt, Renate A. and Gao, Yongsheng},
title = {Focus Set Semantic Differences},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627563},
doi = {10.1145/3587259.3627563},
abstract = {Ontologies are being utilized widely as sources for formally organized information in a range of fields. The SNOMED CT ontology is a key resource in national and international health sectors for automatically linking information captured by diverse clinical information systems and research data ensuring consistent patient data capture and effective data analytics and decision support. Offering a comprehensive multilingual vocabulary for encoding clinical knowledge of multiple domains, the ontology is large and new releases are created regularly to reflect domain changes and user requirements. The main contribution of the paper is a novel automated approach for tracking semantic differences of subdomains in different versions of SNOMED CT targeted at terminologists, debuggers, ontology evaluators and developers of software using SNOMED CT. Whereas the semantic difference sets produced with existing methods are rather large and difficult to analyze, our method produces concise semantic difference sets for user-specified input focus concepts. Our method is based on subontology generation and semantic difference computation using uniform interpolation, which aids in finding inferred differences that other semantic difference tools do not reveal. The obtained semantic difference sets are related to the meaning of focus concept definitions for specific ontology subdomains, where some of these differences would not have been generated without this focused method for computing semantic differences between ontologies. A case study using SNOMED CT has shown the proposed approach is useful for domain experts.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {250–258},
numpages = {9},
keywords = {Forgetting/Uniform Interpolation, Modularisation, Ontology Engineering, Ontology Extraction, SNOMED CT, Semantic Differences, Subontologies, Subontology Extraction},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3265757.3265764,
author = {Grgurina, Natasa and Barendsen, Erik and Suhre, Cor and Zwaneveld, Bert and van Veen, Klaas},
title = {Assessment of modeling and simulation in secondary computing science education},
year = {2018},
isbn = {9781450365888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3265757.3265764},
doi = {10.1145/3265757.3265764},
abstract = {The introduction of the new computing science curriculum in the Netherlands in 2019 raises the need for new evidence-based teaching materials that include practical assignments and guidelines for their assessment. As a part of our research project on teaching Computational Science (modeling and simulation), we participate in these efforts and developed a curriculum intervention including a practical assignment and an accompanying assessment instrument consisting of grading rubrics based on the SOLO taxonomy. In this research paper we focus on the assessment instrument. We describe its development and report on a pilot study carried out in the secondary computing science course implementing the curriculum intervention. The instrument proved to be reliable and effective in tracing high and low levels of the students' achievements in modeling and simulation projects and exposed the expected differences in performance levels of various groups of students, which renders it useful for both formative and summative assessment. Furthermore, our application of the instrument has provided new insights into the needs of specific groups of students to receive instruction prior to and during the work on the assignments.},
booktitle = {Proceedings of the 13th Workshop in Primary and Secondary Computing Education},
articleno = {7},
numpages = {10},
keywords = {secondary computing education, modeling and simulation, assessment, SOLO taxonomy},
location = {Potsdam, Germany},
series = {WiPSCE '18}
}

@inproceedings{10.1145/3652620.3687790,
author = {Crespo, Jos\'{e} Francisco and Juanola, Mart\'{\i} and Oriol, Xavier and Recalde, Mart\'{\i} and Teniente, Ernest},
title = {IMP-Logics: a metamodel for analysis and transformations of Datalog programs},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3687790},
doi = {10.1145/3652620.3687790},
abstract = {Datalog is a logic-based query-language used for knowledge representation and reasoning. Through the years, the literature has defined highly valuable algorithms, desirable properties, and useful transformations for this language and its extensions (e.g. Datalog±).However, few to none tools facilitate the implementation of such results, making the existence of mature Datalog-based tools scarce.This demonstration presents IMP-Logics, a Java library that offers a metamodel for Datalog and its extensions Datalog± that will allow researchers to easily implement the algorithms and demonstrations of the properties and transformations the community is working on.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {51–55},
numpages = {5},
keywords = {datalog, datalog±, dependencies, metamodel},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3333165.3333179,
author = {Demaidi, Mona Nabil and Gaber, Mohamed Medhat},
title = {TONE: A Method for Terminological Ontology Evaluation},
year = {2019},
isbn = {9781450360890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3333165.3333179},
doi = {10.1145/3333165.3333179},
abstract = {Selecting the most appropriate candidate domain ontology is necessary to ensure that the ontology covers the domain of interest at a reasonable level of detail. Existing approaches have the following drawbacks:(1) Focused on the ontology coverage of concepts in the domain of interest, and ignored the semantic richness associated with each concept.(2) The ontology coverage metrics tend to select either large ontologies with broad scope or ontologies with a small number of concepts which may not capture a particular domain of interest.(3) The approaches are not robust in the coverage and semantic richness metric results when different term extraction and recognition algorithms are used.The limitations mentioned above will result in selecting ontologies which are not related to the domain of interest. Therefore, this paper presents a novel Terminological ONtology Evaluator (TONE). TONE uses a textual corpus to evaluate the ontology coverage and semantic richness. TONE was compared with existing ontology evaluation approaches and it proved that it was able to select the domain ontology which was intentionally developed to cover a specific domain of interest. In addition, TONE proved to be more robust in the coverage and semantic richness metric results compared to existing approaches.},
booktitle = {Proceedings of the ArabWIC 6th Annual International Conference Research Track},
articleno = {14},
numpages = {10},
keywords = {semantic richness, percentage agreement, ontology evaluation, Ontologies},
location = {Rabat, Morocco},
series = {ArabWIC 2019}
}

@article{10.1145/3649451,
author = {Du, Kelvin and Xing, Frank and Mao, Rui and Cambria, Erik},
title = {Financial Sentiment Analysis: Techniques and Applications},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3649451},
doi = {10.1145/3649451},
abstract = {Financial Sentiment Analysis (FSA) is an important domain application of sentiment analysis that has gained increasing attention in the past decade. FSA research falls into two main streams. The first stream focuses on defining tasks and developing techniques for FSA, and its main objective is to improve the performances of various FSA tasks by advancing methods and using/curating human-annotated datasets. The second stream of research focuses on using financial sentiment, implicitly or explicitly, for downstream applications on financial markets, which has received more research efforts. The main objective is to discover appropriate market applications for existing techniques. More specifically, the application of FSA mainly includes hypothesis testing and predictive modeling in financial markets. This survey conducts a comprehensive review of FSA research in both the technique and application areas and proposes several frameworks to help understand the two areas’ interactive relationship. This article defines a clearer scope for FSA studies and conceptualizes the FSA-investor sentiment-market sentiment relationship. Major findings, challenges, and future research directions for both FSA techniques and applications have also been summarized and discussed.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {220},
numpages = {42},
keywords = {Financial sentiment analysis, financial forecasting, natural language processing, information system, machine learning, deep learning}
}

@inproceedings{10.1145/3613904.3642720,
author = {Benabdallah, Gabrielle and Peek, Nadya},
title = {Technical Mentality: Principles for HCI Research and Practice},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642720},
doi = {10.1145/3613904.3642720},
abstract = {This paper presents a reflection on the role of ontological inquiry in HCI research and practice. Specifically, we introduce philosopher Gilbert Simondon’s proposal of technical mentality, an onto-epistemology based on direct knowledge of technical objects and systems. This paper makes the following contributions: an analysis of Simondon’s ontological critique and its connection to technical mentality; a reflection on the ethical and practical implications of Simondon’s proposal for systems research; an example of technical mentality in practice; and a discussion of how technical mentality might be extended into a design program for HCI through four principles: extension, integration, legibility, and expression.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {283},
numpages = {14},
keywords = {Design, Gilbert Simondon, Ontology, Philosophy, Technical Mentality},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{10.1145/3357384.3357827,
author = {Khabiri, Elham and Gifford, Wesley M. and Vinzamuri, Bhanukiran and Patel, Dhaval and Mazzoleni, Pietro},
title = {Industry Specific Word Embedding and its Application in Log Classification},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357827},
doi = {10.1145/3357384.3357827},
abstract = {Word, sentence and document embeddings have become the cornerstone of most natural language processing-based solutions. The training of an effective embedding depends on a large corpus of relevant documents. However, such corpus is not always available, especially for specialized heavy industries such as oil, mining, or steel. To address the problem, this paper proposes a semi-supervised learning framework to create document corpus and embedding starting from an industry taxonomy, along with a very limited set of relevant positive and negative documents. Our solution organizes candidate documents into a graph and adopts different explore and exploit strategies to iteratively create the corpus and its embedding. At each iteration, two metrics, called Coverage and Context Similarity, are used as proxy to measure the quality of the results. Our experiments demonstrate how an embedding created by our solution is more effective than the one created by processing thousands of industry-specific document pages. We also explore using our embedding in downstream tasks, such as building an industry specific classification model given labeled training data, as well as classifying unlabeled documents according to industry taxonomy terms.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2713–2721},
numpages = {9},
keywords = {word embeddings, text classification, natural language processing},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.5555/3320516.3321085,
author = {Sarli, Juan L.},
title = {An interoperability model for collaborative development of distributed supply chain simulations},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Development of a collaborative distributed supply chain simulation implies interoperation of heterogeneous systems. Interoperability among several independent systems requires mutual understanding and meaning of shared data represented in a common structure. These two requirements are always a real challenge. In a High Level Architecture (HLA) based supply chain simulation, the federation object model (FOM) performs as a contract where mutual understanding and shared information are described. However, this contract is usually established manually and then the consistency and completeness cannot be guaranteed. Developing FOM and modifying existing systems to comply with the FOM implies a significant amount of time and effort which reduce the benefits of system reuse. This paper presents a heavy-weighted ontology-based method to construct interoperation models of HLA based supply chain simulation in a human-friendly, efficient, consistent and complete way. Besides, this method provides support to collaboration among several organizations of a supply chain.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {4222–4223},
numpages = {2},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/3338468.3356830,
author = {Islam, Md Mazharul and Duan, Qi and Al-Shaer, Ehab},
title = {Specification-driven Moving Target Defense Synthesis},
year = {2019},
isbn = {9781450368285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338468.3356830},
doi = {10.1145/3338468.3356830},
abstract = {Cyber agility enables cyber systems to defend proactively against sophisticated attacks by dynamically changing the system configuration parameters (called mutable parameters) in order to deceive adversaries from reaching their goals, disrupt the attack plans by forcing them to change their adversarial behaviors, and/or deterring them through prohibitively increasing the cost for attacks. However, developing cyber agility such as moving target defense techniques that are provable safe is a highly complex task that requires significant time and expertise. Our goal is to address this challenge by providing a framework for automating the creation of configuration-based moving target techniques rapidly and safely.In this paper, we present a cyber agility synthesis framework, called MTDSynth, that contains a formal ontology, MTD policy language, and MTD controller synthesis engine for implementing configuration-based moving target defense techniques. The policy language contains the agility specifications required to model the MTD technique, such as sensors, mutation trigger, mutation parameters, mutation actions, and mutation constraints. Based on the mutation constraints, the MTD controller synthesis engine provides an MTD policy refinement implementation for SDN configuration with provable properties using constraint satisfaction solvers. We show several examples of MTD controller synthesis, including temporal and spatial IP mutation, path mutation, detector mutation.We developed our ActivSDN over OpenDaylight SDN controller as an open programming environment to enable rapid and safe development of MTD sense-making and decision-making actions. Our implementation and evaluation experiments show not only the feasibility of MTD policy refinement but also the insignificant computational overhead of this refinement process.},
booktitle = {Proceedings of the 6th ACM Workshop on Moving Target Defense},
pages = {13–24},
numpages = {12},
keywords = {sdn, mtd, formal language, automation},
location = {London, United Kingdom},
series = {MTD'19}
}

@article{10.1145/3703918,
author = {Bartalesi, Valentina and Pratelli, Nicol\`{o}},
title = {Representing Geospatial Knowledge in Narratives},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3703918},
doi = {10.1145/3703918},
abstract = {This article explores the representation of geospatial knowledge within narratives through a Semantic Web approach. We introduce the NOnt+Space (NOnt+S) model, an extension of the CIDOC CRM-based Narrative Ontology, which allows the representation of narratives and their geospatial aspects. By leveraging standards such as CRMgeo and GeoSPARQL, NOnt+S ensures systematic and interoperable geospatial representation in narratives, enabling geospatial queries on knowledge graphs. We present an assessment of NOnt+S utilizing data from the H2020 MOVING European project (2021–2024), which collected knowledge about European mountain value chains intended as Cultural Heritage. We have represented this knowledge as geospatial narratives using NOnt+S. GeoSPARQL queries and semantic reasoning applied to the created KG reveal the ontology ability to infer new geospatial knowledge. Our work contributes to the ongoing efforts in the Semantic Web community to integrate and represent geospatial information within narratives, promoting collaboration and interoperability across various scientific domains.},
journal = {J. Comput. Cult. Herit.},
month = feb,
articleno = {17},
numpages = {15},
keywords = {Semantic Web, Knowledge Graph, CRMgeo, GeoSPARQL, Geospatial Narratives, Digital Humanities}
}

@inproceedings{10.1145/3371425.3371442,
author = {Cheng, Xianyi and Ji, Guohua and Zhang, Xiaohua and Chen, Fengmei},
title = {The semantic tagging model of chinese question sentence chunk based on description logics},
year = {2019},
isbn = {9781450376334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371425.3371442},
doi = {10.1145/3371425.3371442},
abstract = {QA (Question Answering) is one of the hot spots in artificial intelligence field. At present, English QA research has made great progress in large-scale text. There are still many difficulties in Chinese QA and it is impossible to understand the true meaning of questions. To solve the problem of Chinese question sentence can't provide deep semantic information for syntactic analysis, the paper proposes a semantic tagging model of Chinese question sentence chunk based on description logics. With the knowledge of HNC's concept symbol to get semantic information about the word, and by starting with the connotation of the concept categories, it preliminarily analyzes the logical structure of specific question sentences. Finally, with description logic reasoning mechanism, we get semantic view of question sentence and proving it's verify in practical.},
booktitle = {Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing},
articleno = {44},
numpages = {6},
keywords = {semantic tagging, description logics, chunk, QA, HNC},
location = {Sanya, China},
series = {AIIPCC '19}
}

@article{10.1145/3732794,
author = {Israelsen, Brett and Ahmed, Nisar R. and Aitken, Matthew and Frew, Eric W. and Lawrence, Dale A. and Argrow, Brian M.},
title = {“A Good Bot Always Knows Its Limitations”: Assessing Autonomous System Decision-Making Competencies through Factorized Machine Self-Confidence},
year = {2025},
issue_date = {December 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {4},
url = {https://doi.org/10.1145/3732794},
doi = {10.1145/3732794},
abstract = {How can intelligent machines assess their competency to complete a task? This question has come into focus for autonomous systems that algorithmically make decisions under uncertainty. We argue that machine self-confidence—a form of meta-reasoning based on self-assessments of system knowledge about the state of the world, itself, and ability to reason about and execute tasks—leads to many computable and useful competency indicators for such agents. This article presents our body of work, so far, on this concept in the form of the Factorized Machine Self-Confidence (FaMSeC) framework, which holistically considers several major factors driving competency in algorithmic decision-making: outcome assessment, solver quality, model quality, alignment quality, and past experience. In FaMSeC, self-confidence indicators are derived via “problem-solving statistics” embedded in Markov Decision Process solvers and related approaches. These statistics come from evaluating probabilistic exceedance margins in relation to certain outcomes and associated competency standards specified by an evaluator. Once designed, and evaluated, the statistics can be easily incorporated into autonomous agents and serve as indicators of competency. We include detailed descriptions and examples for Markov Decision Process agents and show how outcome assessment and solver quality factors can be found for a range of tasking contexts through novel use of meta-utility functions, behavior simulations, and surrogate prediction models. Numerical evaluations are performed to demonstrate that FaMSeC indicators perform as desired (references to human subject studies beyond the scope of this article are provided).},
journal = {J. Hum.-Robot Interact.},
month = jul,
articleno = {66},
numpages = {63},
keywords = {autonomous robots, proficiency assessment, Markov decision processes, probabilistic models, human-autonomy interaction}
}

@inproceedings{10.1145/3603163.3609067,
author = {Gounakis, Nikos and Mountantonakis, Michalis and Tzitzikas, Yannis},
title = {Evaluating a Radius-based Pipeline for Question Answering over Cultural (CIDOC-CRM based) Knowledge Graphs},
year = {2023},
isbn = {9798400702327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603163.3609067},
doi = {10.1145/3603163.3609067},
abstract = {CIDOC-CRM is an event-based international standard for cultural documentation that has been widely used for offering semantic interoperability in the Cultural Heritage (CH) domain. Although there are several Knowledge Graphs (KGs) expressed by using CIDOC-CRM, the task of Question Answering (QA) has not been studied over such graphs. For this reason, in this paper we propose and evaluate a Radius-based QA pipeline over CIDOC-CRM KGs for single-entity factoid questions. In particular, we propose a generic QA pipeline that comprises several models and methods, including a keyword search model for recognizing the entity of the question (and linking it to the KG), methods that are based on path expansion for constructing subgraphs of different radius (i.e., path lengths) starting from the recognized entity, i.e., for being used as a context, and pre-trained neural models (based on BERT) for answering the question using the mentioned context. Moreover, since there are no available benchmarks over CIDOC-CRM KGs, we construct (by using a real KG) an evaluation benchmark having 10,000 questions, i.e., 5,000 single-entity factoid, 2,500 comparative and 2,500 confirmation questions. For evaluating the QA pipeline, we use the 5,000 single-entity factoid questions. Concerning the results, the QA pipeline achieves satisfactory results both in the entity recognition step (78\% accuracy) and in the QA process (51\% F1 score).},
booktitle = {Proceedings of the 34th ACM Conference on Hypertext and Social Media},
articleno = {24},
numpages = {10},
keywords = {Resource Description Framework, Path Expansion, Natural Language Processing, Linked Data, Knowledge Graph, Event-Based Ontology, Entity Recognition, Cultural Heritage, Answer Extraction},
location = {Rome, Italy},
series = {HT '23}
}

@inproceedings{10.1145/3377170.3377231,
author = {Ding, Pan and Zhuoqian, Liang and Yuan, Deng},
title = {Textual Information Extraction Model of Financial Reports},
year = {2020},
isbn = {9781450376631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377170.3377231},
doi = {10.1145/3377170.3377231},
abstract = {This paper proposes a model to extract textual information from financial reports automatically. It takes event extraction as the core, maps narrative information of financial reports into the concepts of financial accounting field, and forms the integration of heterogeneous data of distributed financial information. This study shows that the model can identify text events in large-scale financial reporting corpus, automatically extract events and theirs attribute information, and convert them into structured data. Moreover, this paper presents and evaluates the effect of the model in extracting information from annual reports of listed companies. The experimental results turn out that the model provides semantic mapping between text events and domain knowledge concepts, which is reasonable and reliable to be applied in the field of financial statement analysis.},
booktitle = {Proceedings of the 2019 7th International Conference on Information Technology: IoT and Smart City},
pages = {404–408},
numpages = {5},
keywords = {Textual information extraction, Financial report, Event extraction, Corpus},
location = {Shanghai, China},
series = {ICIT '19}
}

@article{10.1145/3624013,
author = {Kumari, Namrata and Singh, Pardeep},
title = {Hindi Text Summarization Using Sequence to Sequence Neural Network},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {10},
issn = {2375-4699},
url = {https://doi.org/10.1145/3624013},
doi = {10.1145/3624013},
abstract = {Text summarizing reduces a large block of text data to a precise, short, and intelligible text that conveys the whole meaning of the actual text in a few words while maintaining the original context. Due to a lack of relevant summaries, it is hard to understand the main idea of the document. Text summarization using the abstractive technique is well-studied in English, although it is still in its infancy in Indian regional languages. In this study, we investigate the effectiveness of using a sequence-to-sequence (Seq2Seq) neural network based on attention and its optimization for text summarization for the Hindi language (HiATS), explicitly comparing the Adam and RMSprop optimizers. Our method allows the model to take the Hindi language dataset and, as output, provides a concise summary that accurately reflects the gist of the original text. The performance of the models will be evaluated using Rouge-1 and Rouge-2 metrics.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {239},
numpages = {18},
keywords = {neural network, word embedding, optimizers, Abstractive text summarization}
}

@inproceedings{10.1145/3744169.3744197,
author = {Hu, Botao Amber},
title = {Autonomous Realities: A Journey into Protocolizing Digital Object Permanence in a Future of Many Mixed Realities},
year = {2025},
isbn = {9798400720031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744169.3744197},
doi = {10.1145/3744169.3744197},
abstract = {Major technology companies envision a future where mixed reality (MR) devices become as ubiquitous as smartphones are today. Yet most collaborative MR research assumes users share a single augmented layer—an assumption that may not hold true. Rather, MR technology is inherently permissionless: users control what they see, making each person’s augmented layers private and unique. We are moving toward a future of multiple overlapping and co-existing mixed realities. This paper employs protocol fiction as a speculative design method to explore this near-future scenario. We follow the journey of a fictional digital pet rock as it travels through successive protocol eras of mixed reality, adapting to the changing infrastructures and protocols it encounters. Through a comic-style narrative, the story unfolds across four protocol-defined chapters: Centralized Realities, Distributed Realities, Persistent Realities, and Autonomous Realities. Each chapter examines moments when digital pet rock owners—wearing MR headsets—engage in social encounters, revealing how protocols shape the ontological nature of digital object permanence and highlight the socio-technical challenges of constructing consensus reality.},
booktitle = {Proceedings of the Sixth Decennial Aarhus Conference: Computing X Crisis},
pages = {290–302},
numpages = {13},
keywords = {Social Mixed Reality, Protocol Design, Merging Mixed Reality, Protocol Fiction, Design Fiction, Ontology of Digital Object, Object Permanence, Metaverse Interoperability},
location = {
},
series = {AAR '25}
}

@inproceedings{10.1145/3701716.3718483,
author = {Prabowo, Arian and Lin, Xiachong and Razzak, Imran and Xue, Hao and Amos, Matthew and White, Stephen D. and Salim, Flora D.},
title = {Brick-by-Brick: Cyber-Physical Building Data Classification Challenge},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3718483},
doi = {10.1145/3701716.3718483},
abstract = {optimization essential in combating climate change. Cyber-Physical Buildings, enabled by the integration of Internet-of-Things (IoT) devices and advanced data analytics tools like AI, offer a smart and effective approach to energy management. A key challenge, however, lies in automating the semantic labeling of IoT devices to ensure machine-interpretable data. The ''Brick-by-Brick: Cyber-Physical Building Data Classification Challenge'' aims to tackle this challenge by classifying time-series data from IoT devices within buildings. Participants will engage with a dataset consisting of over 10,000 time-series streams collected over three years across three buildings, representing 91 unique semantic classes. Both the dataset and baselines are established in a published paper. With a total prize pool of 20,000 AUD, the competition is ready to launch in December 2024 and run through February 2025, hosted by AIcrowd. This challenge invites researchers, practitioners, and technologists to drive AI-enabled solutions for advancing the next generation of environmentally sustainable cyber-physical buildings. Additional details on the dataset, benchmark, and code can be found in the official repository ( https://github.com/cruiseresearchgroup/DIEF_BTS). The challenge was published on AIcrowd www.aicrowd.com/challenges/brick-by-brick-2024.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {3021–3025},
numpages = {5},
keywords = {building, classification, machine learning, ontology, timeseries},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3230905.3230935,
author = {Khallouki, Hajar and Abatal, Ahmed and Bahaj, Mohamed},
title = {An Ontology-based Context awareness for Smart Tourism Recommendation System},
year = {2018},
isbn = {9781450353045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3230905.3230935},
doi = {10.1145/3230905.3230935},
abstract = {Smart tourism concept appeared with the development of Smart Cities. Bringing Smartness into Tourism needs a dynamic and interconnected system on which information relating to tourism activities could be exchanged in real time. In this paper, we introduce a new approach for designing mobile tourism recommendation system using context awareness. The proposed approach combines Internet of Things (IoT) technologies with semantic web services to predict the tourist real-time context and provide the suitable services.},
booktitle = {Proceedings of the International Conference on Learning and Optimization Algorithms: Theory and Applications},
articleno = {43},
numpages = {5},
keywords = {tourism recommendation system, semantic web, real-time context, context awareness, Smart tourism, IoT},
location = {Rabat, Morocco},
series = {LOPAL '18}
}

@inproceedings{10.1145/3637528.3671941,
author = {Yeom, Kyuhwan and Yang, Hyeongjun and Park, Gayeon and Jeon, Myeongheon and Ko, Yunjeong and Oh, Byungkook and Lee, Kyong-Ho},
title = {Embedding Two-View Knowledge Graphs with Class Inheritance and Structural Similarity},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671941},
doi = {10.1145/3637528.3671941},
abstract = {Numerous large-scale knowledge graphs (KGs) fundamentally represent two-view KGs: an ontology-view KG with abstract classes in ontology and an instance-view KG with specific collections of entities instantiated from ontology classes. Two-view KG embedding aims to jointly learn continuous vector representations of entities and relations in the aforementioned two-view KGs. In essence, an ontology schema exhibits a tree-like structure guided by class hierarchies, which leads classes to form inheritance hierarchies. However, existing two-view KG embedding models neglect those hierarchies, which provides the necessity to reflect class inheritance. On the other hand, KG is constructed based on a pre-defined ontology schema that includes heterogeneous relations between classes. Furthermore, these relations are defined within the scope of those among classes since instances inherit all the properties of their corresponding classes, which reveals structural similarity between two multi-relational networks. Despite the consideration to bridge the gap among two-view KG representations, existing methods ignore the existence of structural similarity between two-view KGs. To address these issues, we propose a novel two-view KG embedding model, CISS, considering Class Inheritance and Structural Similarity between two-view KGs. To deal with class inheritance, we utilize class sets, each of which is composed of sibling classes, to learn fine-grained class representations. In addition, we configure virtual instance-view KG from clustered instances and compare subgraph representations of two-view KGs to enhance structural similarity between them. Experimental results show our superior performance compared to existing models.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3931–3941},
numpages = {11},
keywords = {class inheritance, knowledge graph, ontology, structural similarity},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3241403.3241426,
author = {Plakidas, Konstantinos and Schall, Daniel and Zdun, Uwe},
title = {Model-based support for decision-making in architecture evolution of complex software systems},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241426},
doi = {10.1145/3241403.3241426},
abstract = {Design decision support for software architects in complex industrial software systems, such as software ecosystems and systems-of-systems, which feature extensive reuse of third-party solutions and a variety of deployment options, is still an open challenge. We describe three industrial use cases involving considerable re-architecting, where on-premises solutions were migrated to a cloud-based IoT platforms. Based on these use cases, we analyse the challenges and derive requirements for an architecture knowledge model supporting this process. The presented methodology builds upon existing approaches and proposes a model for the description of extant software applications and the management of domain knowledge. We demonstrate its use to support the evolution and/or composition of software applications in a migration scenario in a systematic and traceable manner.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {21},
numpages = {7},
keywords = {systems-of-systems composition, software variability management, software migration, software architecture evolution, model-based decision support},
location = {Madrid, Spain},
series = {ECSA '18}
}

@inproceedings{10.1145/3443279.3443306,
author = {Fritz, Simon and Jaenicke, Matthias and Ovtcharova, Jivka and Wicaksono, Hendro},
title = {Context-sensitive Assistance in Requirements-based Knowledge Management},
year = {2021},
isbn = {9781450377607},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3443279.3443306},
doi = {10.1145/3443279.3443306},
abstract = {In this paper, a concept of a digital assistance system is presented which, based on computer linguistic methods, supports the user in the tasks of requirement-based knowledge management. The concept is divided into six modules that offer context-sensitive support in the identification, documentation, linking, modification and reuse of requirements and the associated knowledge. Since this concept was developed as part of the BMBF-funded SME Innovative Project DAM4KMU, which is primarily aimed at German SMEs, the concept developed was specially designed for processing German-language texts.The digital assistance system pursues the goal, on the one hand, of increasing the quality of the documentation by supporting the user in the creation of complete formulations. On the other hand, with the help of the most modern language models, possible relationships between the information should be identified and linked to each other in a partially automated manner. In addition, the integration of web crawling technologies should make the knowledge available on the Internet available in a context-sensitive manner, in order to lift possible innovations on the one hand and not to forget possible non-considered boundary conditions on the other.The automatic linking of all information is intended to ensure a continuous exchange of knowledge, which should reduce misunderstandings and non-communicated changes to requirements or goals to a minimum.},
booktitle = {Proceedings of the 4th International Conference on Natural Language Processing and Information Retrieval},
pages = {47–54},
numpages = {8},
keywords = {Web Crawling, Requirements Engineering, Natural Language Processing, Knowledge management, Digital Assistance},
location = {Seoul, Republic of Korea},
series = {NLPIR '20}
}

@article{10.1145/3295662,
author = {Badaro, Gilbert and Baly, Ramy and Hajj, Hazem and El-Hajj, Wassim and Shaban, Khaled Bashir and Habash, Nizar and Al-Sallab, Ahmad and Hamdi, Ali},
title = {A Survey of Opinion Mining in Arabic: A Comprehensive System Perspective Covering Challenges and Advances in Tools, Resources, Models, Applications, and Visualizations},
year = {2019},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3295662},
doi = {10.1145/3295662},
abstract = {Opinion-mining or sentiment analysis continues to gain interest in industry and academics. While there has been significant progress in developing models for sentiment analysis, the field remains an active area of research for many languages across the world, and in particular for the Arabic language, which is the fifth most-spoken language and has become the fourth most-used language on the Internet. With the flurry of research activity in Arabic opinion mining, several researchers have provided surveys to capture advances in the field. While these surveys capture a wealth of important progress in the field, the fast pace of advances in machine learning and natural language processing (NLP) necessitates a continuous need for a more up-to-date literature survey. The aim of this article is to provide a comprehensive literature survey for state-of-the-art advances in Arabic opinion mining. The survey goes beyond surveying previous works that were primarily focused on classification models. Instead, this article provides a comprehensive system perspective by covering advances in different aspects of an opinion-mining system, including advances in NLP software tools, lexical sentiment and corpora resources, classification models, and applications of opinion mining. It also presents future directions for opinion mining in Arabic. The survey also covers latest advances in the field, including deep learning advances in Arabic Opinion Mining. The article provides state-of-the-art information to help new or established researchers in the field as well as industry developers who aim to deploy an operational complete opinion-mining system. Key insights are captured at the end of each section for particular aspects of the opinion-mining system giving the reader a choice of focusing on particular aspects of interest.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {27},
numpages = {52},
keywords = {sentiment lexicons, sentiment analysis applications, opinion mining, deep learning, arabic natural language processing, Sentiment analysis}
}

@inproceedings{10.1145/3176258.3176331,
author = {Rizvi, Syed Zain R. and Fong, Philip W. L.},
title = {Efficient Authorization of Graph Database Queries in an Attribute-Supporting ReBAC Model},
year = {2018},
isbn = {9781450356329},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3176258.3176331},
doi = {10.1145/3176258.3176331},
abstract = {Neo4j is a popular graph database that offers two versions; a paid enterprise edition and a free community edition. The enterprise edition offers customizable Role-Based Access Control (RBAC) features through custom developed procedures, while the community edition does not offer any access control support. Being a graph database, Neo4j is a natural application for Relationship-Based Access Control (ReBAC), an access control paradigm where authorization decisions are based on relationships between subjects and resources in the system. In this paper we present AReBAC, an attribute-supporting ReBAC model for Neo4j (applicable to both editions) that provides finer grained access control. AReBAC employs Nano-Cypher, a declarative policy language based on Neo4j»s Cypher query language, the result of which allows us to weave database queries with access control policies and evaluate both simultaneously. Evaluating the combined query and policy produces a result that i) matches the search criteria, and ii) the requesting subject has access to. Our experiments show that our evaluation algorithm performs faster than Neo4j»s query evaluation engine when evaluating queries that are expressible using Nano-Cypher.},
booktitle = {Proceedings of the Eighth ACM Conference on Data and Application Security and Privacy},
pages = {204–211},
numpages = {8},
keywords = {relatioship-based access control, neo4j, graph database, attributes},
location = {Tempe, AZ, USA},
series = {CODASPY '18}
}

@inproceedings{10.5555/3400397.3400521,
author = {Benaben, Frederick and Lauras, Matthieu and Fertier, Audrey and Salatg\'{e}, Nicolas},
title = {Integrating model-driven engineering as the next challenge for artificial intelligence: application to risk and crisis management},
year = {2020},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {Artificial Intelligence (AI) is currently on top of the hype regarding simultaneously research publications and industrial development. However, the current status of AI makes it quite far and different from the current understanding of Human intelligence. One suggestion that is made in this article is that Model-Driven approaches could be considered as an interesting avenue to complement classical visions of AI and to provide some missing features. Specifically, the use of Model-Driven Engineering tools (such as metamodel and model transformation) could benefit to the domain of AI by introducing a way to extend the apprehension of unknown situations. To support that proposal, an illustrative example is provided regarding the domain of risk and crisis management.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1549–1563},
numpages = {15},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@article{10.1145/3640018,
author = {Kapugama Geeganage, Dakshi Tharanga and Wynn, Moe Thandar and ter Hofstede, Arthur H. M.},
title = {Text2EL+: Expert Guided Event Log Enrichment Using Unstructured Text},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {1},
issn = {1936-1955},
url = {https://doi.org/10.1145/3640018},
doi = {10.1145/3640018},
abstract = {Through the application of process mining, business processes can be improved on the basis of process execution data captured in event logs. Naturally, the quality of this data determines the quality of the improvement recommendations. Improving data quality is non-trivial, and there is great potential to exploit unstructured text, e.g., from notes, reviews, and comments, for this purpose and to enrich event logs. To this end, this article introduces Text2EL+&nbsp;, a three-phase approach to enrich event logs using unstructured text. In its first phase, events and (case and event) attributes are derived from unstructured text linked to organisational processes. In its second phase, these events and attributes undergo a semantic and contextual validation before their incorporation in the event log. In its third and final phase, recognising the importance of human domain expertise, expert guidance is used to further improve data quality by removing redundant and irrelevant events. Expert input is used to train a Named Entity Recognition (NER) model with customised tags to detect event log elements. The approach applies natural language processing techniques, sentence embeddings, training pipelines and models, as well as contextual and expression validation. Various unstructured clinical notes associated with a healthcare case study were analysed, and completeness, concordance, and correctness of the derived event log elements were evaluated through experiments. The results show that the proposed method is feasible and applicable.},
journal = {J. Data and Information Quality},
month = mar,
articleno = {8},
numpages = {28},
keywords = {Event data quality, process mining, event log, unstructured text, natural language processing, semantic validation}
}

@inproceedings{10.1145/3708035.3736045,
author = {Saboia, Priscila and Sweet, James and Sweet, Christopher},
title = {OpenBridge: Bridging Domain Scientists and APIs through AI-Powered Interfaces},
year = {2025},
isbn = {9798400713989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708035.3736045},
doi = {10.1145/3708035.3736045},
abstract = {We propose OpenBridge, an MCP-compliant server that enables AI assistants to access scientific APIs through natural language. It converts OpenAPI Specification (OAS) endpoints into Model Context Protocol (MCP) tools and enriches responses with JavaScript Object Notation for Linked Data (JSON-LD) for improved semantic clarity. As we demonstrate using data from Paper Analytical Devices (PADs), OpenBridge allows researchers to retrieve and explore structured data without writing code. Thus, it offers a new approach to bridging domain experts and data systems through AI-assisted, semantically aware interfaces. The implementation is available at https://github.com/PaperAnalyticalDeviceND/OpenBridge.},
booktitle = {Practice and Experience in Advanced Research Computing 2025: The Power of Collaboration},
articleno = {97},
numpages = {3},
keywords = {AI Agents, OpenAPI, Model Context Protocol, Research Data Infrastructure},
location = {
},
series = {PEARC '25}
}

@inproceedings{10.1145/3587259.3627549,
author = {D'Aquin, Mathieu and Bunoiu, Renata and Cirstea, Horatiu and Lenczner, Michel and Lieber, Jean and Zamkotsian, Fr\'{e}d\'{e}ric},
title = {Combining representation formalisms for reasoning upon mathematical knowledge},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3627549},
doi = {10.1145/3587259.3627549},
abstract = {Knowledge in mathematics (definitions, theorems, proofs, etc.) is usually expressed in a way that combines natural language and mathematical expressions (e.g. equations). Using an ontology formalism such as OWL&nbsp;DL is well-suited for formalizing the natural language part, but complex mathematical expressions can be better handled by symbolic computation systems. We examine this representation issue and propose an original extension of OWL&nbsp;DL by call formulas, i.e., formulas from which assertions can be drawn thanks to calls to external functions. Using this formalism makes it possible to classify a mathematical problem defined by its relations to instances and classes and by some mathematical expressions: if a theorem for solving this problem is represented in the knowledge base, it can be retrieved, and thus, the problem can be solved by applying this theorem. We describe an inference algorithm and discuss its properties as well as its limitations. Indeed, the proposed extension, algorithm, and implementation represent a first step towards a combined formalism for representing mathematical knowledge, with some open issues regarding the representation of more complex problems: the resolution of multiscale, multiphysics cases in physics are foreseen.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {180–187},
numpages = {8},
keywords = {knowledge modeling, knowledge representation, mathematical knowledge},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3233027.3233031,
author = {Kaindl, Hermann and Kramer, Stefan and Hoch, Ralph},
title = {An inductive learning perspective on automated generation of feature models from given product specifications},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233031},
doi = {10.1145/3233027.3233031},
abstract = {For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {25–30},
numpages = {6},
keywords = {machine learning, inductive generalization from examples, generating feature models},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3168365.3168378,
author = {Carbonnel, Jessie and Huchard, Marianne and Nebut, Cl\'{e}mentine},
title = {Towards the Extraction of Variability Information to Assist Variability Modelling of Complex Product Lines},
year = {2018},
isbn = {9781450353984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168365.3168378},
doi = {10.1145/3168365.3168378},
abstract = {Software product line engineering gathers a set of methods that rely on systematic reuse and mass customisation to reduce the development time and cost of a set of similar software systems. Boolean feature models are the de facto standard used to represent product line variability in terms of features, a feature being a distinguishable characteristic of one or several softwares. The extractive adoption of a product line from a set of individually developed softwares requires to extract variability information from a collection of software descriptions to model their variability. With the appearance of more and more complex software systems, software product line engineering faces new challenges including variability extraction and modelling. Extensions of boolean feature models, as multi-valued attributes or UML-like cardinalities have since been proposed to support variability modelling in complex product lines. In this paper, we propose research directions to address the issue of extracting more complex variability information, as a part of extended feature models synthesis from software descriptions. We consider the capabilities of Formal Concept Analysis, a mathematical framework for knowledge discovery, along with two of its extensions called Pattern Structures and Relational Concept Analysis, to answer this problematic. These frameworks bring theoretical foundations to complex variability extraction algorithms.},
booktitle = {Proceedings of the 12th International Workshop on Variability Modelling of Software-Intensive Systems},
pages = {113–120},
numpages = {8},
keywords = {Variability Extraction, Software Product Line, Reverse Engineering},
location = {Madrid, Spain},
series = {VAMOS '18}
}

@inproceedings{10.1145/3361570.3361597,
author = {Gueddes, Abdelweheb and Mahjoub, Mohamed Ali},
title = {e-SAAD system: Ontologies based approach for home Care Services platform},
year = {2019},
isbn = {9781450362924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361570.3361597},
doi = {10.1145/3361570.3361597},
abstract = {In the first generation of interventions and home support, the needs were triggered generally by phone call and they are described orally. In most cases, there is no registered information about the patient's conditions or medical history. The interveners are either from the private or state health sector. Despite the fact that there are people who can intervene quicker than others, yet they have not been recruited, or work as professional liberal. With the development of technology, systems based on data mining or artificial intelligence have been developed to focus on the intervention's time for example. Although intervention and home-based care are the subject of many studies [1], the resolution of the overall decision-making problem is not sufficiently developed. On the one hand, the state of health presupposes the definition of a patient. There is a set of parameters characterizing the habits of daily life of the person analyzed in parallel with the evolution of physiological and environment. On the other hand, it is necessary to take into consideration the medical Core, his location, his profile not only his professional status but also his abilities and skills that are not explicitly described in his curriculum. Different studies and systems exist in the literature [2]. Each of his studies tackles only a part of the parameters. Indeed, these studies consider either the monitoring of daily activities, the monitoring of physiological data or other environmental parts. Either they consider the specificities of the medical Core's profile, or these systems use a probabilistic data mining that involves many interactions with the experts to interpret the data, either wise an expert system based on the inference rules defined by the medical experts. In addition, most systems do not use controlled vocabulary that provides semantics needed. This complicates information sharing and collaborative work. The objective of the e-SAAD project is to propose a methodological process to facilitate the analysis and procedure of intervention systems and home support. The process should identify the generic and specific aspects of each part. The patient's data set, profile, history, its environment and location should be taken into consideration. As well as the service providers, their profiles, their skills and essentially their availability and locations. These models must be open to be adapted to new data sources.},
booktitle = {Proceedings of the 9th International Conference on Information Systems and Technologies},
articleno = {21},
numpages = {6},
keywords = {ontology, home-based care, e-SAAD, Telemedicine, Semantic Web},
location = {Cairo, Egypt},
series = {ICIST '19}
}

@inproceedings{10.1109/MODELS-C.2019.00028,
author = {Burgue\~{n}o, Loli and Burdusel, Alexandru and G\'{e}rard, S\'{e}bastien and Wimmer, Manuel},
title = {MDE intelligence 2019: 1st workshop on artificial intelligence and model-driven engineering},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00028},
doi = {10.1109/MODELS-C.2019.00028},
abstract = {Model-driven engineering (MDE) and Artificial Intelligence (AI) are two separate fields in computer science, which can clearly benefit from cross-fertilization and collaboration. There are at least two ways in which such integrations---which we call MDE Intelligence---can manifest: (1) MDE can benefit from integrating AI concepts and ideas to increasing the power and flexibility of model-driven techniques by means of the application of AI algorithms. (2) Conversely, AI can benefit from integrating concepts and ideas from MDE---for example, using domain-specific languages and model transformations allows domain experts to directly express and manipulate their problems while providing an auditable computation pipeline.To discuss and further stimulate such integrations, the 1st edition of the Workshop on Artificial Intelligence and Model-driven Engineering (MDE Intelligence) was held on September 16, 2019 in Munich, Germany, as part of the satellite events of the IEEE/ACM 22th International Conference on Model-Driven Engineering Languages and Systems (MODELS 2019).},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems Companion},
pages = {168–169},
numpages = {2},
keywords = {MDE, MDE intelligence, artificial intelligence},
location = {Munich, Germany},
series = {MODELS '19 Companion}
}

@inproceedings{10.1145/3352593.3352604,
author = {Boruah, Abhijit and Kakoty, Nayan M. and Ali, Tazid},
title = {Reasoning on Objects' Geometric Shapes for Prosthetic Hand Grasping},
year = {2020},
isbn = {9781450366502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352593.3352604},
doi = {10.1145/3352593.3352604},
abstract = {The problem of knowing what to grasp and deciding how to grasp is an open issue for development of intelligent prosthetic hands. To emulate the potentialities of a human hand, knowledge of the grasping domain has to be accumulated and modelled in a machine interpretable format. In this paper, we have tried to comprehend and model a specific part of the knowledge (information) of a prosthetic hand-grasping domain into a reusable Web-Ontology-Language (OWL) format. This ontology build after basic analysis of hand object coordination, can be used for preserving, improving and sharing the captured knowledge. We begin with our description of the required knowledge of a geometrical concept formed during human grasping, to a point where it can be used to plan grasping based on the objects identified. Using tactile and kinesthetic information along with relevant domain concepts, we emphasized on the rationality of designing an ontology for reusability and sustainability of knowledge. We tried to lay down a visual model of the ontology, also called the Ontograph, which illuminates the existence and relationships among the various objects of the grasping domain. We have also checked the decisive capability of the ontology by reasoning it with Description Logic (DL) queries of data property values for individuals of geometric classes. The output of the queries provided us with individuals of the specific geometric pattern, which can be used to decide the type of grasp that could be implemented on objects.},
booktitle = {Proceedings of the 2019 4th International Conference on Advances in Robotics},
articleno = {10},
numpages = {6},
keywords = {Tactile, Reasoning, Prosthetics, Ontology, OWL, Knowledge, Kinesthetic},
location = {Chennai, India},
series = {AIR '19}
}

@inproceedings{10.1145/3539618.3594250,
author = {Liao, Lizi and Yang, Grace Hui and Shah, Chirag},
title = {Proactive Conversational Agents in the Post-ChatGPT World},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3594250},
doi = {10.1145/3539618.3594250},
abstract = {ChatGPT and similar large language model (LLM) based conversational agents have brought shock waves to the research world. Although astonished by their human-like performance, we find they share a significant weakness with many other existing conversational agents in that they all take a passive approach in responding to user queries. This limits their capacity to understand the users and the task better and to offer recommendations based on a broader context than a given conversation. Proactiveness is still missing in these agents, including their ability to initiate a conversation, shift topics, or offer recommendations that take into account a more extensive context. To address this limitation, this tutorial reviews methods for equipping conversational agents with proactive interaction abilities.The full-day tutorial is divided into four parts, including multiple interactive exercises. We will begin the tutorial with an interactive exercise and cover the design of existing conversational systems architecture and challenges. The content includes coverage of LLM-based recent advancements such as ChatGPT and Bard, along with reinforcement learning with human feedback (RLHF) technique. Then we will introduce the concept of proactive conversation agents and preset recent advancements in proactiveness of conversational agents, including actively driving conversations by asking questions, topic shifting, and methods that support strategic planning of conversation. Next, we will discuss important issues in conversational responses' quality control, including safety, appropriateness, language detoxication, hallucination, and alignment. Lastly, we will launch another interactive exercise and discussion with the audience to arrive at concluding remarks, prospecting open challenges and new directions. By exploring new techniques for enhancing conversational agents' proactive behavior to improve user engagement, this tutorial aims to help researchers and practitioners develop more effective conversational agents that can better understand and respond to user needs proactively and safely.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {3452–3455},
numpages = {4},
keywords = {conversational ai, conversational search, proactive conversation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3664476.3669922,
author = {Suciu, George and Sachian, Mari-Anais and Bratulescu, Razvan and Koci, Kejsi and Parangoni, Grigor},
title = {Entity Recognition on Border Security},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3669922},
doi = {10.1145/3664476.3669922},
abstract = {Entity recognition, also known as named entity recognition (NER), is a fundamental task in natural language processing (NLP) that involves identifying and categorizing entities within text. These entities, such as names of people, organizations, locations, dates, and numerical values, provide structured information from unstructured text data. NER models, ranging from rule-based to machine learning-based approaches, decode linguistic patterns and contextual information to extract entities effectively. This article explores the roles of entities, tokens, and NER models in NLP, detailing their significance in various applications like information retrieval and border security. It delves into the practices of implementing NER in legal document analysis, travel history analysis, and document verification, showcasing its transformative impact in streamlining processes and enhancing security measures. Despite challenges such as ambiguity and data scarcity, ongoing research and emerging trends in multilingual NER and ethical considerations promise to drive innovation in the field. By addressing these challenges and embracing new developments, entity recognition is poised to continue advancing NLP capabilities and powering diverse real-world applications.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {135},
numpages = {6},
keywords = {Border Security, Entity, Frameworks, Machine Learning, NER, RNNs, Recognition, SVM, Travel},
location = {Vienna, Austria},
series = {ARES '24}
}

@inproceedings{10.1145/3625469.3625470,
author = {Wu, Wenjing and Yuan, Qi and Chen, Qiulan and Cao, Yunzhong},
title = {Construction Safety Knowledge Graph Integrating Text and Image Information},
year = {2023},
isbn = {9798400707681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625469.3625470},
doi = {10.1145/3625469.3625470},
abstract = {To improve the extraction efficiency and visualization of construction safety knowledge, this paper combines knowledge graph technology with construction safety domain, and proposes a basic framework of construction safety knowledge ontology based on the evolution logic of safety events according to the characteristics of knowledge. Considering two types of safety knowledge carriers and data sources, text and image, a knowledge graph is designed to contain text semantic features and image features, and the knowledge services based on different dimensional knowledge queries are validated in the experiments. The results show that the BERT-BiLSTM-CRF algorithm can be used to extract entities in text, and YOLOv5-FastPose can extract excavator poses from images. This paper verifies the applicability of knowledge graphs for safety knowledge mining, visualization and services.},
booktitle = {Proceedings of the 2023 6th International Conference on Information Management and Management Science},
pages = {26–32},
numpages = {7},
keywords = {Construction safety management, Entity recognition, Knowledge graph, Pose estimation},
location = {Chengdu, China},
series = {IMMS '23}
}

@inproceedings{10.1145/3231053.3231128,
author = {jabbar, Sohail and Malik, Kaleem Razzaq and Ahmad, Mudassar},
title = {Real-time RDF adaptation model for smart human-care querying in IoT based mobile applications},
year = {2018},
isbn = {9781450364287},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3231053.3231128},
doi = {10.1145/3231053.3231128},
abstract = {Majorly, nowadays, the collected raw data through mobiles is huge based on sensors embedded in devices and IoT based applications. These applications use Internet of Things (IoT) and Big Data analytics services and daily activities as routines for recording and analyzing real-time data for human-care. Nowadays, mobile is having services built on sensors to reduce human involvement in data collection. Many issues concerning security and privacy can be resolved if we use data analytics in services to represent data as Resource Description Framework (RDF). The automated transformation mechanism in relational database taken from mobile sensors and applications into semantically annotated RDF stores. This study is comprised of a methodology for refining compatibility between different data models by introducing real-time RDF context model for adopting data to smart querying in mobile applications. Smart querying capabilities come from transformation between sensors with activity services data and RDF data store for mobile applications. Whereas, case study built-up out of applications data is used to show data adaptation process for smart querying for human-care in mobile devices. Multiple queries are used to extract mobile video information smartly and efficiently. According to results shows if standard deviation gets greater than mean that tend of values is spreading over a wider range of values.},
booktitle = {Proceedings of the 2nd International Conference on Future Networks and Distributed Systems},
articleno = {61},
numpages = {5},
keywords = {real-time smart querying, real-time data transformation, mobile application, linked data, human-care services, data modeling, big data, IoT},
location = {Amman, Jordan},
series = {ICFNDS '18}
}

@inproceedings{10.1145/3535735.3535755,
author = {Zeynalova, Nigar},
title = {Student Creativity and Talent Development in Higher Education Institutions of Azerbaijan},
year = {2022},
isbn = {9781450396196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535735.3535755},
doi = {10.1145/3535735.3535755},
abstract = {The aim of the study is to find out how to develop student creativity in Azerbaijan higher education institutions. This research is consists of two parts: descriptive part of the research, where the author tried to analyze background of the study and research have been done in the filed so far. The second part based on a survey, which is conducted for students and teachers of various HEIs of the country. 219 respondents answered the research questions from various universities of Azerbaijan. The results of research indicated that creativity can be strengthen in higher education with help and support of teachers and by knowledge, creativity can be further enhanced in the teaching process depending on the teacher's pedagogical skills and approach, various teaching methods can develop creativity of students in higher education including modern innovative technologies, creativity justifies itself at every moment in teaching and learning process in higher education, motivation is a key issue for increasing creativity.},
booktitle = {Proceedings of the 7th International Conference on Information and Education Innovations},
pages = {163–175},
numpages = {13},
keywords = {talent, knowledge, innovation, higher education, development, creativity},
location = {Belgrade, Serbia},
series = {ICIEI '22}
}

@inproceedings{10.1145/3216122.3216152,
author = {Leclercq, \'{E}ric and Savonnet, Marinette},
title = {A Tensor Based Data Model for Polystore: An Application to Social Networks Data},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216152},
doi = {10.1145/3216122.3216152},
abstract = {In this article, we show how the mathematical object tensor can be used to build a multi-paradigm model for the storage of social data in data warehouses. From an architectural point of view, our approach allows to link different storage systems (polystore) and limits the impact of ETL tools performing model transformations required to feed different analysis algorithms. Therefore, systems can take advantage of multiple data models both in terms of query execution performance and the semantic expressiveness of data representation. The proposed model allows to reach the logical independence between data and programs implementing analysis algorithms. With a concrete case study on message virality on Twitter during the French presidential election of 2017, we highlight some of the contributions of our model.},
booktitle = {Proceedings of the 22nd International Database Engineering \&amp; Applications Symposium},
pages = {110–118},
numpages = {9},
keywords = {Tensor, Polystore, OLAP, Multi-relational Networks, Multi-paradigm Storage, Associative Array},
location = {Villa San Giovanni, Italy},
series = {IDEAS '18}
}

@inproceedings{10.1145/3373722.3373778,
author = {Surkova, Anna and Skorynin, Sergey and Chernobaev, Igor},
title = {Word embedding and cognitive linguistic models in text classification tasks},
year = {2020},
isbn = {9781450376709},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373722.3373778},
doi = {10.1145/3373722.3373778},
abstract = {The paper considers two linguistic models, analyzed the possibility of their use for the text data classification as well as their associations in the integrated texts presentation. A cognitive approach for the text classification issues is presented. An algorithm to identify the words basic level using WordNet is considered. A model for text classification based on the pre-trained word embeddings is presented. The model consists of three layers: embedding layer Long-Short Term Memory (LSTM) layer, and softmax layer. The model was trained and evaluated on the 20 Newsgroups dataset. The classification quality was assessed by F- measure, precision and recall. The obtained results analysis is carried out. Both described models show good results, low scores for some texts are explained. The advantages and limitations of the linguistic models are shown. In future works the authors are going to combine proposed models and modify them. Thus, for model based on word embedding there are pretty vast opportunities for extension: from experimenting with different word embeddings and various distance metrics to more complicated architecture of layers and even promising state of the art artificial neural network models, activation functions and their modifications. In addition, there is research area of proper ensemble strategy selection.},
booktitle = {Proceedings of the XI International Scientific Conference Communicative Strategies of the Information Society},
articleno = {12},
numpages = {6},
keywords = {words vector representations, thesaurus, sequential data, data mining, cognitive semantics, classification, WordNet},
location = {St. Petersburg, Russian Federation},
series = {CSIS'2019}
}

@article{10.1109/TASLP.2023.3290428,
author = {Petermann, Darius and Wichern, Gordon and Subramanian, Aswin Shanmugam and Wang, Zhong-Qiu and Roux, Jonathan Le},
title = {Tackling the Cocktail Fork Problem for Separation and Transcription of Real-World Soundtracks},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3290428},
doi = {10.1109/TASLP.2023.3290428},
abstract = {Emulating the human ability to solve the cocktail party problem, i.e., focus on a source of interest in a complex acoustic scene, is a long standing goal of audio source separation research. In this paper, we focus on the cocktail fork problem, which takes a three-pronged approach to source separation by separating an audio mixture such as a movie soundtrack or podcast into the three broad categories of speech, music, and sound effects (SFX - understood to include ambient noise and natural sound events). We evaluate several deep learning-based source separation models on this task using simple objective measures such as signal-to-distortion ratio (SDR) as well as objective metrics that better correlate with human perception. Furthermore, we thoroughly evaluate how source separation can influence the downstream transcription asks of speech recognition for speech and audio tagging for music and SFX. We also investigate the task of activity detection on the three sources as a way to further improve source separation and transcription. While we observe that source separation improves transcription performance in comparison to the original soundtrack, performance is still sub-optimal due to artifacts introduced by the separation process. Therefore, we thoroughly investigate how remixing of the three separated source stems at various relative levels can reduce artifacts and consequently improve transcription performance. We find that remixing music and SFX interferences at a target SNR of 17.5 dB reduces speech recognition word error rate, and similar impact from remixing is observed for tagging music and SFX content.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {2592–2605},
numpages = {14}
}

@inproceedings{10.1145/3632410.3632466,
author = {Kumar, Ayush and Shalghar, Abhay M and Chauhan, Harsh and Ganesan, Balaji and Chaudhuri, Ritwik and Kannan, Aswin},
title = {Document structure aware Relation Extraction for Semantic Automation},
year = {2024},
isbn = {9798400716348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632410.3632466},
doi = {10.1145/3632410.3632466},
abstract = {Relational Graph Convolutional Network models are a class of Graph Neural Network models used for link prediction in heterogeneous graphs. They’re being used in a variety of industrial applications including semantic automation tasks in a Lakehouse. In this work, we propose a novel way to incorporate document specific features into a RGCN model that helps improve relation extraction accuracy by about 15 points. Further, we extend this document awareness to semantic tasks on tabular data and discuss our results.},
booktitle = {Proceedings of the 7th Joint International Conference on Data Science \&amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)},
pages = {232–236},
numpages = {5},
keywords = {RGCN, information extraction, semantic automation},
location = {Bangalore, India},
series = {CODS-COMAD '24}
}

@inproceedings{10.1145/3487664.3487784,
author = {Stach, Christoph and Br\"{a}cker, Julia and Eichler, Rebecca and Giebler, Corinna and Mitschang, Bernhard},
title = {Demand-Driven Data Provisioning in Data Lakes: BARENTS&nbsp;—&nbsp;A Tailorable Data Preparation Zone},
year = {2022},
isbn = {9781450395564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487664.3487784},
doi = {10.1145/3487664.3487784},
abstract = {Data has never been as significant as it is today. It can be acquired virtually at will on any subject. Yet, this poses new challenges towards data management, especially in terms of storage (data is not consumed during processing, i.&nbsp;e., the data volume keeps growing), flexibility (new applications emerge), and operability (analysts are no IT experts). The goal has to be a demand-driven data provisioning, i.&nbsp;e., the right data must be available in the right form at the right time. Therefore, we introduce a tailorable data preparation zone for Data Lakes called BARENTS. It enables users to model in an ontology how to derive information from data and assign the information to use cases. The data is automatically processed based on this model and the refined data is made available to the appropriate use cases. Here, we focus on a resource-efficient data management strategy. BARENTS can be embedded seamlessly into established Big Data infrastructures, e.&nbsp;g., Data Lakes.},
booktitle = {The 23rd International Conference on Information Integration and Web Intelligence},
pages = {187–198},
numpages = {12},
keywords = {zone model, ontology, knowledge modeling, food analysis, data transformation, data pre-processing, data management, Data Lakes},
location = {Linz, Austria},
series = {iiWAS2021}
}

@inproceedings{10.1145/3217197.3217207,
author = {Yang, Xi and Lehman, Tom and Kettimuthu, Raj and Winkler, Linda and Jung, Eun-Sung},
title = {A Model Driven Intelligent Orchestration Approach to Service Automation in Large Distributed Infrastructures},
year = {2018},
isbn = {9781450358620},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3217197.3217207},
doi = {10.1145/3217197.3217207},
abstract = {Today's scientific computing applications and workflows operate on heterogeneous and vastly distributed infrastructures. Traditional human-in-the-loop service engineering approach met its insurmountable challenge in dealing with these very complex and diverse networked systems, including conventional and software defined networks, compute, storage, clouds and instruments. Orchestration is the key to integrate and coordinate the networked multi-services and automate end-to-end workflows. In this work, we present a model driven intelligent orchestration approach to this end-to-end automation, which is built upon a semantic modeling solution that supports the full stack of service integration, orchestration, abstraction, and intent and policy representation. We also present the design of a real-world orchestrator called StackV that is able to accommodate highly complex application scenarios such as Software Defined ScienceDMZ (SD-SDMZ) and Hybrid Cloud Inter-Networking (HCIN) by implementing this approach.},
booktitle = {Proceedings of the 1st International Workshop on Autonomous Infrastructure for Science},
articleno = {5},
numpages = {8},
keywords = {Service Automation, Modeling, Intelligent Orchestration, Distributed Infrastructure},
location = {Tempe, AZ, USA},
series = {AI-Science'18}
}

@inproceedings{10.1145/3209281.3209333,
author = {Oliveira, Marcelo Iury S. and Oliveira, Lairson Emanuel R. A. and Batista, Marlos G. Ribeiro and L\'{o}scio, Bernadette Farias},
title = {Towards a meta-model for data ecosystems},
year = {2018},
isbn = {9781450365260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209281.3209333},
doi = {10.1145/3209281.3209333},
abstract = {Data Ecosystems are socio-technical networks that enable collaboration between autonomous actors such as enterprises, institutions, and individuals. While Data Ecosystems are thus gaining importance, research into Data Ecosystems is still in its infancy stages. The terminology and definitions for Data Ecosystem vary greatly. This diversity imposes a pressing problem for the development of a clear understanding of the new opportunities and emergent challenges in exploiting Data Ecosystems. Accurate definitions are required to get a mutual understanding of what Data Ecosystems involve. Moreover, to the best of our knowledge, a model for describing a Data Ecosystem and its essential concepts has not been proposed yet. In this work, we aim to fill these gaps by reviewing the Data Ecosystem literature, and based on the field literature, we propose a meta-model for describing Data Ecosystems. In particular, the proposed meta-model describes the Data Ecosystem fundamental concepts and their inter-relationships for enabling analysis and description of ecosystems. Especially, the meta-model declares explicitly how all these concepts are related to each other in such holistic view, hence facilitating knowledge creation and management in the ecosystem.},
booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
articleno = {72},
numpages = {10},
keywords = {standardization, meta-model, government data, data ecosystem},
location = {Delft, The Netherlands},
series = {dg.o '18}
}

@article{10.1145/3363574,
author = {Lee, John Boaz and Rossi, Ryan A. and Kim, Sungchul and Ahmed, Nesreen K. and Koh, Eunyee},
title = {Attention Models in Graphs: A Survey},
year = {2019},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {6},
issn = {1556-4681},
url = {https://doi.org/10.1145/3363574},
doi = {10.1145/3363574},
abstract = {Graph-structured data arise naturally in many different application domains. By representing data as graphs, we can capture entities (i.e., nodes) as well as their relationships (i.e., edges) with each other. Many useful insights can be derived from graph-structured data as demonstrated by an ever-growing body of work focused on graph mining. However, in the real-world, graphs can be both large—with many complex patterns—and noisy, which can pose a problem for effective graph mining. An effective way to deal with this issue is to incorporate “attention” into graph mining solutions. An attention mechanism allows a method to focus on task-relevant parts of the graph, helping it to make better decisions. In this work, we conduct a comprehensive and focused survey of the literature on the emerging field of graph attention models. We introduce three intuitive taxonomies to group existing work. These are based on problem setting (type of input and output), the type of attention mechanism used, and the task (e.g., graph classification, link prediction). We motivate our taxonomies through detailed examples and use each to survey competing approaches from a unique standpoint. Finally, we highlight several challenges in the area and discuss promising directions for future work.},
journal = {ACM Trans. Knowl. Discov. Data},
month = nov,
articleno = {62},
numpages = {25},
keywords = {graph attention survey, graph attention, deep learning, Attention mechanism}
}

@article{10.1145/3748239.3748249,
author = {Liu, Lihui and Wang, Zihao and Tong, Hanghang},
title = {Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/3748239.3748249},
doi = {10.1145/3748239.3748249},
abstract = {Knowledge graph reasoning is pivotal in various domains such as data mining, artificial intelligence, the Web, and social sciences. These knowledge graphs function as comprehensive repositories of human knowledge, facilitating the inference of new information. Traditional symbolic reasoning, despite its strengths, struggles with the challenges posed by incomplete and noisy data within these graphs. In contrast, the rise of Neural Symbolic AI marks a significant advancement, merging the robustness of deep learning with the precision of symbolic reasoning. This integration aims to develop AI systems that are not only highly interpretable and explainable but also versatile, effectively bridging the gap between symbolic and neural methodologies. Additionally, the advent of large language models (LLMs) has opened new frontiers in knowledge graph reasoning, enabling the extraction and synthesis of knowledge in unprecedented ways. This survey offers a thorough review of knowledge graph reasoning, focusing on various query types and the classification of neural symbolic reasoning. Furthermore, it explores the innovative integration of knowledge graph reasoning with large language models, highlighting the potential for groundbreaking advancements. This comprehensive overview is designed to support researchers and practitioners across multiple fields, including data mining, AI, the Web, and social sciences, by providing a detailed understanding of the current landscape and future directions in knowledge graph reasoning.},
journal = {SIGKDD Explor. Newsl.},
month = jul,
pages = {124–136},
numpages = {13},
keywords = {knowledge graph question answering, knowledge graph reasoning, neural symbolic reasoning}
}

@inproceedings{10.1145/3677779.3677815,
author = {Guan, Jing},
title = {Research on Human-Computer Interaction Design Standards in Artificial Intelligence Products},
year = {2024},
isbn = {9798400709760},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677779.3677815},
doi = {10.1145/3677779.3677815},
abstract = {This article first analyzes the concept and development process of artificial intelligence products, then delves into the application and development of human-computer interaction technology in artificial intelligence products, and then analyzes the standardization research of human-computer interaction design in artificial intelligence products; Finally, the two-dimensional architecture of human-computer interaction design for artificial intelligence products was elaborated, with detailed discussions and planning in terms of technical and value dimensions. The rapid development of artificial intelligence has driven the emergence of a large number of artificial intelligence products, resulting in a fundamental change in the human-computer interaction mode of products and higher requirements for human-computer interaction design. Therefore, how to construct standardized human-computer interaction design patterns in artificial intelligence products in the new era is a key research topic in the development of artificial intelligence products.},
booktitle = {Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning},
pages = {220–225},
numpages = {6},
location = {Xi'an, China},
series = {CMNM '24}
}

@article{10.1145/3660826,
author = {Yan, Chuan and Meng, Mark Huasong and Xie, Fuman and Bai, Guangdong},
title = {Investigating Documented Privacy Changes in Android OS},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {FSE},
url = {https://doi.org/10.1145/3660826},
doi = {10.1145/3660826},
abstract = {Android has empowered third-party apps to access data and services on mobile devices since its genesis.This involves a wide spectrum of user privacy-sensitive data, such as the device ID and location. In recent years, Android has taken proactive measures to adapt its access control policies for such data, in response to the increasingly strict privacy protection regulations around the world. When each new Android version is released, its privacy changes induced by the version evolution are transparently disclosed, and we refer to them as documented privacy changes (DPCs). Implementing DPCs in Android OS is a non-trivial task, due to not only the dispersed nature of those access control points within the OS, but also the challenges posed by backward compatibility. As a result, whether the actual access control enforcement in the OS implementations aligns with the disclosed DPCs becomes a critical concern.                                                                In this work, we conduct the first systematic study on the consistency between the operational behaviors of the OS at runtime and the officially disclosed DPCs. We propose DopCheck, an automatic DPC-driven testing framework equipped with a large language model (LLM) pipeline. It features a serial of analysis to extract the ontology from the privacy change documents written in natural language, and then harnesses the few-shot capability of LLMs to construct test cases for the detection of DPC-compliance issues in OS implementations. We apply DopCheck with the latest versions (10 to 13) of Android Open Source Project (AOSP). Our evaluation involving 79 privacy-sensitive APIs demonstrates that DopCheck can effectively recognize DPCs from Android documentation and generate rigorous test cases. Our study reveals that the status quo of the DPC-compliance issues is concerning, evidenced by 19 bugs identified by DopCheck. Notably, 12 of them are discovered in Android 13 and 6 in Android 10 for the first time, posing more than 35\% Android users to the risk of privacy leakage. Our findings should raise an alert to Android users and app developers on the DPC compliance issues when using or developing an app, and would also underscore the necessity for Google to comprehensively validate the actual implementation against its privacy documentation prior to the OS release.},
journal = {Proc. ACM Softw. Eng.},
month = jul,
articleno = {119},
numpages = {24},
keywords = {Android, documentation, privacy, testing}
}

@inproceedings{10.1145/3183428.3183429,
author = {Barn, Balbir S. and Barn, Ravinder},
title = {Towards a unified conceptual model for surveillance theories: "we shall meet in the place where there is no darkness" - 1984, george orwell},
year = {2018},
isbn = {9781450356619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183428.3183429},
doi = {10.1145/3183428.3183429},
abstract = {The erosion of values such as privacy can be a critical factor in preventing the acceptance of new innovative technology especially in challenging environments such as the criminal justice system. Erosion of privacy happens through either deliberate or inadvertent surveillance. Since Bentham's original liberal project in the 1900s, a literature and a whole study area around theories of surveillance has developed. Increasingly this general body of work has focussed on the role of information technology as a vehicle for surveillance activity. Despite an abundance of knowledge, a unified view of key surveillance concepts that is useful to designers of information systems in preventing or reducing unintended surveillance remains elusive. This paper contributes a conceptual model that synthesises the gamut of surveillance theories as a first step to a theory building effort for use by Information Systems professionals. The model is evaluated using a design science research paradigm using data from both examples of surveillance and a recently completed research project that developed technology for the UK youth justice system.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Society},
pages = {71–80},
numpages = {10},
keywords = {surveillance, reference model, privacy, conceptual model},
location = {Gothenburg, Sweden},
series = {ICSE-SEIS '18}
}

@article{10.1145/3582263,
author = {Bartalesi, Valentina and Pratelli, Nicolo’ and Lenzi, Emanuele and Pontari, Paolo},
title = {Using Semantic Web to Create and Explore an Index of Toponyms Cited in Medieval Geographical Works},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3582263},
doi = {10.1145/3582263},
abstract = {Western thought in European history was mainly affected by the image of the world created during the Middle Ages and Renaissance. The most popular reason to travel during the Middle Ages was taking a pilgrimage. Jerusalem, Rome, and Santiago de Compostela were the most popular destinations. It is not surprising that a lot of works written by travellers as guides for pilgrims exist. By the beginning of the Renaissance, a more precise image of the world was defined, thanks to the discovery of ancient geographical models, especially the work of Ptolemy. The Italian National Research Project (PRIN) IMAGO --- Index Medii Aevi Geographiae Operum --- (2020-2023) aims to provide a systematic overview of the medieval and renaissance Latin geographical literature using the Semantic Web technologies and the LOD paradigm. Indeed, until now, this literature has not been studied using digital methods. In particular, this article presents how we formally represented the knowledge about the toponyms, or place names, in the IMAGO ontology. To maximise the interoperability, we developed the IMAGO ontology as an extension of two reference vocabularies: the CIDOC CRM and its extension FRBRoo, including its in-progress reformulation, LRMoo. Furthermore, we used Wikidata as reference knowledge base. As case study, we chose to represent the knowledge related to the toponyms cited by the Italian poet Dante Alighieri in his Latin works. We carried out a first experiment for visualising the knowledge about these toponyms on a map and in the form of tables and CSV files.},
journal = {J. Comput. Cult. Herit.},
month = apr,
articleno = {26},
numpages = {18},
keywords = {Dante Alighieri, Wikidata, CIDOC CRM, toponyms, ontology, Linked Open Data, Semantic Web}
}

@inproceedings{10.1145/3411564.3411630,
author = {Thalheimer, J\'{e}ferson Miguel and Filho, Aluizio Haendchen and Briks, Fabio Julio Pereira and Ribeiro, Rafael Castaneda and Concatto, Fernando and Viecelli, Ang\'{e}lica Karize},
title = {A Microservice-driven Collaborative Agent in Virtual Learning Environments: A Role Model for a Tracing Agent},
year = {2020},
isbn = {9781450388733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411564.3411630},
doi = {10.1145/3411564.3411630},
abstract = {Currently, distance learning comprises almost half of students enrolled in undergraduate courses in Brazil. However, the dropout rate of this modality is over 50\%, and only 22\% of students complete the courses [27]. Despite technological advances and good acceptance of this modality, research indicates that the lack of involvement in a virtual community can lead to feelings of loneliness, low self-esteem, isolation and desmotivation. There is evidence that these feelings are among the main factors responsible for the low performance and high evasion rate. Virtual Learning Environments (VLE) handles a large volume of student interaction data. In this context, it is important to create mechanisms to maintain and manage a data structure to facilitate the processes of transforming data into information and knowledge. This paper aims to present a tracing agent responsible for maintaining and managing the data structure in VLE. The agent acts in the context of a microservice-oriented multi-agent system, interacting and collaborating with other agents in order to improve interaction and decision-making processes. This work becomes original and at the same time innovative, presenting an unprecedented combination of technologies and techniques in the context of VLEs.},
booktitle = {Proceedings of the XVI Brazilian Symposium on Information Systems},
articleno = {21},
numpages = {8},
keywords = {Virtual Learning Environment, Tracing Agent, Multiagent System},
location = {S\~{a}o Bernardo do Campo, Brazil},
series = {SBSI '20}
}

@inproceedings{10.1145/3345252.3345295,
author = {Baeva, Desislava},
title = {Using Lindenmayer Systems For Generative Modeling Of Graphic Concepts, Set In Elements Of Bulgarian Folklore Embroidery},
year = {2019},
isbn = {9781450371490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345252.3345295},
doi = {10.1145/3345252.3345295},
abstract = {L-systems as a type of fractal model are one of the most widely used examples of the integration of mathematics and information technology. They can be seen in many elements created with the means of computer graphics, and have become a new tool that is relevant for modeling in biology, geology, and other natural sciences. Along with their applications in advanced technology science, L-systems also refer to archaic models that are surprisingly common in traditional designs of different ethnicities, and some of their basic concepts are also fundamental to systems of knowledge about the Bulgarian embroidery.This article reviews and analyzes the generative characteristics of some graphic motifs specific to Bulgarian folklore. The applicability of the study consists in finding tools for a general description of these motifs, which can easily refer us to other elements of human creativity - such as speech or music, and provoke the discovery of relationships and relationships between them.},
booktitle = {Proceedings of the 20th International Conference on Computer Systems and Technologies},
pages = {234–239},
numpages = {6},
keywords = {generative art, embroidery simulation, L-systems},
location = {Ruse, Bulgaria},
series = {CompSysTech '19}
}

@inbook{10.1145/3382097.3382114,
title = {Expert modeling in OWL},
year = {2020},
isbn = {9781450376174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382097.3382114},
abstract = {Enterprises have made amazing advances by taking advantage of data about their business to provide predictions and understanding of their customers, markets, and products. But as the world of business becomes more interconnected and global, enterprise data is no long a monolith; it is just a part of a vast web of data. Managing data on a world-wide scale is a key capability for any business today.The Semantic Web treats data as a distributed resource on the scale of the World Wide Web, and incorporates features to address the challenges of massive data distribution as part of its basic design. The aim of the first two editions was to motivate the Semantic Web technology stack from end-to-end; to describe not only what the Semantic Web standards are and how they work, but also what their goals are and why they were designed as they are. It tells a coherent story from beginning to end of how the standards work to manage a world-wide distributed web of knowledge in a meaningful way.The third edition builds on this foundation to bring Semantic Web practice to enterprise. Fabien Gandon joins Dean Allemang and Jim Hendler, bringing with him years of experience in global linked data, to open up the story to a modern view of global linked data. While the overall story is the same, the examples have been brought up to date and applied in a modern setting, where enterprise and global data come together as a living, linked network of data. Also included with the third edition, all of the data sets and queries are available online for study and experimentation at data.world/swwo.},
booktitle = {Semantic Web for the Working Ontologist: Effective Modeling for Linked Data, RDFS, and OWL}
}

@inproceedings{10.1145/3709026.3709105,
author = {Aili, Elyar and Yilahun, Hankiz and Imam, Seyyare and Hamdulla, Askar},
title = {Relational Representation Augmented Graph Attention Network for Knowledge Graph Completion},
year = {2025},
isbn = {9798400718182},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3709026.3709105},
doi = {10.1145/3709026.3709105},
abstract = {Knowledge Graph Completion (KGC) is a popular topic in knowledge graph construction and related applications, aiming to complete the structure of knowledge graphs by predicting missing entities or relations and mining unknown facts in the knowledge graph. In the KGC task, Graph Neural Network (GNN)-based methods have achieved remarkable results due to their advantage of effectively capturing complex relations among entities and generating more accurate and rich entity representations by aggregating information from neighbouring nodes. These methods mainly focus on the representation of entities, and the representation of relations is obtained using simple dimensional transformations or initial embeddings. This treatment ignores the diversity and complex semantics of relations and restricts the efficiency of the model in utilizing relational information in the reasoning process. In this work, we propose the Relational Representation Augmented Graph Attention Network (RRA-GAT), which effectively identifies and weights neighbouring relations that actually contribute to the target relation by filtering out irrelevant information through an attention function based on the information and spatial domain. Furthermore, we capture complex patterns and features in the relational embedding by means of a feed-forward network consisting of a series of linear transformations and nonlinear activation functions. Experiments demonstrate the very advanced performance of RRA-GAT on the link prediction task on standard datasets FB15k-237 and WN18RR (e.g., improved the MRR metric on the WN18RR dataset by 7.8\% relative improvement).},
booktitle = {Proceedings of the 2024 8th International Conference on Computer Science and Artificial Intelligence},
pages = {449–455},
numpages = {7},
keywords = {Graph neural networks, Knowledge graph completion, Knowledge graph embedding},
location = {
},
series = {CSAI '24}
}

@inproceedings{10.1145/3323503.3360641,
author = {Martini, Bruno G. and Helfer, Gilson A. and Barbosa, Jorge L. V. and Silva, Marcio R. da and de Figueiredo, Rodrigo M. and Modolo, Regina C. E. and Yamin, Adenauer C.},
title = {A computational model for ubiquitous intelligent services in indoor agriculture},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360641},
doi = {10.1145/3323503.3360641},
abstract = {The application of ubiquitous computing has increased in recent years, especially due to the development of technologies such as mobile computing, accurate sensors and specific protocols for an IoT. One of the trends in this research area is the use of context awareness. In agriculture, the context can be related to the environment, for example, the conditions found inside a greenhouse. Recently a series of studies proposed the use of sensors to monitor the production or the use of cameras to obtain crop information, providing data, reminders and alerts to farmers. This paper proposes a computational model for Indoor Agriculture called IndoorPlant that uses the contexts history analysis to provide intelligent services such as predict the productivity, indicate the problems that the crop may suffer, give suggestions for improvements in the parameters in the greenhouse, among others. IndoorPlant was tested on cucumber prediction using simulated data that was approved by three farmers with more than 10 years of experience each. The results obtained in the prediction of cucumber, with a coefficient of determination (R2) of 0.9912 for root mean square error (RMSE) of 8,06 units of cucumber.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {497–500},
numpages = {4},
keywords = {prediction in agriculture, indoor agriculture, context awareness, computing in agriculture},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/3705391.3705402,
author = {Sun, Yuyuan and Li, Tongyan and Chen, Xingyu and Tan, Hao},
title = {ConMask-GNN: Leveraging Graph Neural Networks for Enhanced Knowledge Graph Completion in Static Contexts},
year = {2025},
isbn = {9798400709630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3705391.3705402},
doi = {10.1145/3705391.3705402},
abstract = {With the rapid development of knowledge graphs (KG), enhancing the performance of knowledge graph completion has become a critical research challenge. Traditional methods for KG completion rely heavily on entity and relation embeddings, often neglecting the synergistic interaction between graph structures and textual information. To address this issue, this paper proposes a novel approach that combines Graph Neural Networks (GNN) with the ConMask model to improve the effectiveness of static knowledge graph completion tasks. Specifically, the GNN is employed to generate entity embeddings based on graph structures, while ConMask extracts key information from text using a relation-dependent content masking mechanism. The model further integrates the embeddings through a multi-head attention mechanism at the embedding layer, fusing the GNN-based entity embeddings with text embeddings. Additionally, the loss function incorporates contrastive learning, which enhances the representational capacity of the embeddings. Experiments are conducted on standard datasets FB15k-237 and WN18RR, evaluated with metrics such as Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits@1, Hits@3, and Hits@10. The results demonstrate that the proposed model significantly outperforms existing baseline methods in prediction accuracy. Finally, ablation studies validate the critical role of combining GNN and ConMask in improving model performance.},
booktitle = {Proceedings of the 2024 6th International Conference on Telecommunications and Communication Engineering},
pages = {65–69},
numpages = {5},
keywords = {ConMask, Graph Neural Networks (GNN), Knowledge graph completion, embedding fusion, multi-head attention mechanism},
location = {
},
series = {ICTCE '24}
}

@inproceedings{10.1145/3167132.3167271,
author = {da Silva, Jo\~{a}o Pablo S. and Ecar, Miguel and Pimenta, Marcelo S. and Kepler, Fabio Natanael and Guedes, Gilleanes T. A. and Betemps, Carlos Michel},
title = {Improving self-adaptive systems conceptual modeling},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167271},
doi = {10.1145/3167132.3167271},
abstract = {Self-adaptive Systems (SaSs) operate under uncertainty conditions and have intrinsic properties that have posed some challenges for requirements analysis. Conceptual modeling is useful to requirements analysis because it aids to understand the situation in which a problem occurs. SaSs conceptual modeling is a non-trivial activity because it is necessary to deal with requirements uncertainty, contextual changes, and behavior adaptation. Since conceptual models are built by humans, their quality heavily depends on the humans expertise, which is not a good software engineering practice. Regarding SaSs, the exposure to quality risks increases because of intrinsic characteristic in this class of system. In this paper, we present a SaSs conceptual modeling approach composed of a metamodel and a modeling process. The process defines how to instantiate the metamodel from requirements specifications to create SaSs conceptual models. We performed a controlled experiment with subjects to evaluate our modeling approach effectiveness. As the outcome, we found that our approach had a better performance than an ad hoc approach. The contribution of this paper is a well-defined approach for guiding SaSs conceptual modeling, supported by evidence of its effectiveness by means of an empirical experiment.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1292–1299},
numpages = {8},
keywords = {self-adaptive system, requirements analysis, empirical experiment, conceptual modeling},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3184558.3186571,
author = {Kapugama Geeganage, Dakshi Tharanga},
title = {Concept Embedded Topic Modeling Technique},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186571},
doi = {10.1145/3184558.3186571},
abstract = {Text contents are overloaded with the digitization of the data and new contents are transmitted through many sources by generating a large volume of information, which spreads all over the world through different communication media. Therefore, text data is available everywhere and reading, understanding and analysing the text data has become a main activity in daily routine. With the increment of the volume and the variety of information, organizing and searching, the required information has become vital. Topic modelling is the state of the art for information organization, understanding and extracting the content. Most of the prevailing topic models use the probabilistic approaches and consider the frequency and the co-occurrence to discover the topics from collections of documents. The proposed research aims to address the existing problems of topic modeling by introducing a concept embedded topic model which generates the most relevant and meaningful topics by understanding the content. The research includes approaches to understand the semantic elements from the content, domain identification of concepts and provide most suitable topics without getting the number of topics from the user beforehand. Capturing the semantics of document collections and generating the most related set of topics according to the actual meaning will be the significance of this research.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {831–835},
numpages = {5},
keywords = {concepts, semantics, topic modeling},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3375959.3375975,
author = {Seok, Hyunseung and Nam, Sunghyun and Lee, Yongju},
title = {Implementing A Semantic-based loT Mashup Service},
year = {2020},
isbn = {9781450372633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375959.3375975},
doi = {10.1145/3375959.3375975},
abstract = {The semantic information provided through the semantic-based IoT system will produce new high value-added products that are completely different from what we have known and experienced. From this point of view, a key issue of current IoT technology and applications is the development of an intelligent IoT platform architecture. Our proposed system collects the IoT data of the sensors from the cloud computer, converts them into RDF, and annotates them with semantics. The converted semantic data are shared and utilized through the ontology repository. We use KT's IoTMakers as a cloud computing environment, and the ontology repository uses Jena's Fuseki server to express SPARQL query results on the Web using Daum Map API and HighCharts API. This gives people the opportunity to access the semantic IoT mash-up service easily and has various application possibilities.},
booktitle = {Proceedings of the 2019 2nd Artificial Intelligence and Cloud Computing Conference},
pages = {162–167},
numpages = {6},
keywords = {Semantic-based Mashup Service, Responsive Web Design, Ontology Modeling, IoT},
location = {Kobe, Japan},
series = {AICCC '19}
}

@inproceedings{10.1145/3688574.3688591,
author = {Zhu, Jiangtao and Wang, Tiankun and Ma, Xinru and Zuo, Chao and Zhao, Junjie and Luo, Quan},
title = {Deep Learning-Based Knowledge Graph Construction for Three-Dimensional Design Specification of Power Plant Engineering},
year = {2024},
isbn = {9798400717857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3688574.3688591},
doi = {10.1145/3688574.3688591},
abstract = {With the rapid development of natural language processing technology, the field of knowledge engineering is gradually advancing towards a novel phase of knowledge expression and orderly data storage. The purpose of this paper is to explore the construction method of knowledge graph of 3D design specification for power plant engineering based on deep learning, and to realize the extraction of key information in the design specification and the automatic construction of knowledge graph by integrating the deep learning models such as BERT, Bi-LSTM, CRF, and so on.},
booktitle = {Proceedings of the 2024 6th International Conference on Big Data Engineering},
pages = {118–125},
numpages = {8},
keywords = {Building information modeling, Deep learning, Knowledge graph, Power plant engineering, Three-dimensional design specification},
location = {Xining, China},
series = {BDE '24}
}

@inproceedings{10.1145/3404835.3463113,
author = {Nguyen, Hoang-Van and Gelli, Francesco and Poria, Soujanya},
title = {DOZEN: Cross-Domain Zero Shot Named Entity Recognition with Knowledge Graph},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463113},
doi = {10.1145/3404835.3463113},
abstract = {With the new developments of natural language processing, increasing attention has been given to the task of Named Entity Recognition (NER). However, the vast majority of work focus on a small number of large-scale annotated datasets with a limited number of entities such as person, location and organization. While other datasets have been introduced with domain-specific entities, the smaller size of these largely limits the applicability of state-of-the-art deep models. Even if there are promising new approaches for performing zero-shot learning (ZSL), they are not designed for a cross-domain settings. We propose Cross Domain Zero Shot Named Entity Recognition with Knowledge Graph (DOZEN), which learns the relations between entities across different domains from an existing ontology of external knowledge and a set of analogies linking entities and domains. Experiments performed on both large scale and domain-specific datasets indicate that DOZEN is the most suitable option to extracts unseen entities in a target dataset from a different domain.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1642–1646},
numpages = {5},
keywords = {cross-domain machine learning, knowledge graph, named entity recognition, natural language processing, zero-shot learning},
location = {Virtual Event, Canada},
series = {SIGIR '21}
}

@inproceedings{10.1145/3535508.3545550,
author = {Wei, Anqi and Wang, Liangjiang},
title = {Deep sequence representation learning for predicting human proteins with liquid-liquid phase separation propensity and synaptic functions},
year = {2022},
isbn = {9781450393867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535508.3545550},
doi = {10.1145/3535508.3545550},
abstract = {With advancements in next-generation sequencing techniques, the whole protein sequence repertoire has increased to a great extent. In the meantime, deep learning techniques have promoted the development of computational methods to interpret large-scale proteomic data and facilitate functional studies of proteins. Inferring properties from protein amino acid sequences has been a long-standing problem in Bioinformatics. Extensive studies have successfully applied natural language processing (NLP) techniques for the representation learning of protein sequences. In this paper, we applied the deep sequence model - UDSMProt, to fine-tune and evaluate two protein prediction tasks: (1) predict proteins with liquid-liquid phase separation propensity and (2) predict synaptic proteins. Our results have shown that, without prior domain knowledge and only based on protein sequences, the fine-tuned language models achieved high classification accuracies and outperformed baseline models using compositional k-mer features in both tasks. Hence, it is promising to apply the protein language model to some learning tasks and the fine-tuned models can be used to predict protein candidates for biological studies.},
booktitle = {Proceedings of the 13th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {41},
numpages = {8},
keywords = {synaptic proteins, protein language model, liquid-liquid phase separation},
location = {Northbrook, Illinois},
series = {BCB '22}
}

@inproceedings{10.1145/3220228.3220263,
author = {Hamdy, Abeer and Elsayed, Mohamed},
title = {Topic modelling for automatic selection of software design patterns},
year = {2018},
isbn = {9781450364454},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220228.3220263},
doi = {10.1145/3220228.3220263},
abstract = {Design pattern is a high-quality and reusable solution to a recurring software design problem. It is considered an important concept in the software engineering field due to its ability to enhance some of the quality attributes of the software systems including maintainability and extensibility. However, novice developers need to be provided by a tool to assist them in selecting the fit design pattern to solve a design problem. The paper proposes a novel approach for the automatic selection of the fit design pattern. This approach is based on using Latent Dirichlet Allocation (LDA) topic model. The topic is a set of words that often appear together. LDA is able to relate words with similar meaning and to differentiate between uses of words with multiple meanings. In this paper LDA is used to analyze the textual descriptions of design patterns and extract the topics then discover the similarity between the target problem scenario and the collection of patterns using Improved Sqrt-Cosine similarity measure (ISCS). The proposed approach was evaluated using Gang of four design patterns. The experimental results showed that the proposed approach outperforms approach based on the traditional vector space model of Unigrams.},
booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
pages = {41–46},
numpages = {6},
keywords = {topic modelling, text mining, information retrieval, gang of four, design pattern selection, LDA and vector space model, DP recommendation},
location = {Prague, Czech Republic},
series = {ICGDA '18}
}

@inproceedings{10.1145/3386164.3387296,
author = {Merkle, Lukas},
title = {Cloud-Based Battery Digital Twin Middleware Using Model-Based Development},
year = {2020},
isbn = {9781450376617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386164.3387296},
doi = {10.1145/3386164.3387296},
abstract = {Following the trends of electrification, the energy storage of vehicles is gaining importance as the most expensive part of an electric car. Since lithium-ion batteries are perishable goods and underlie e. g. aging effects, environmental and operating conditions during manufacturing and car usage need close supervision. With regard to the paradigm of digital twins, data from various life cycle phases needs to be collected and processed to improve the general quality of the system. To achieve this complex task, a suitable framework is needed in order to operate the fleet of digital twins during manufacturing processes, the automotive usage and a potential second life. Based on a literature review, we formulate requirements for a digital twin framework in the field of battery systems. We propose a framework to develop and operate a fleet of digital twins during all life cycle phases. Results feature a case study in which we implement the stated framework in a cloud-computing environment using early stages of battery system production as test a bed. With the help of a self-discharge model of li-ion cells, the system can estimate the SOC of battery modules and provide this information to the arrival testing procedures.},
booktitle = {Proceedings of the 2019 3rd International Symposium on Computer Science and Intelligent Control},
articleno = {59},
numpages = {7},
keywords = {Self-Discharge, IoT, Digital Twin, Control Middleware, Battery System},
location = {Amsterdam, Netherlands},
series = {ISCSIC 2019}
}

@article{10.1145/3597455,
author = {Ding, Ling and Chen, Xiaojun and Wei, Jian and Xiang, Yang},
title = {MABERT: Mask-Attention-Based BERT for Chinese Event Extraction},
year = {2023},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {7},
issn = {2375-4699},
url = {https://doi.org/10.1145/3597455},
doi = {10.1145/3597455},
abstract = {Event extraction is an essential but challenging task in information extraction. This task has considerably benefited from pre-trained language models, such as BERT. However, when it comes to the trigger-word mismatch problem in languages without natural delimiters, existing methods ignore the complement of lexical information to BERT. In addition, the inherent multi-role noise problem could limit the performance of methods when one sentence contains multiple events. In this article, we propose a Mask-Attention-based BERT (MABERT) framework for Chinese event extraction to address the above problems. Firstly, in order to avoid trigger-word mismatch and integrate lexical features into BERT layers directly, a mask-attention-based transformer augmented with two mask matrices is devised to replace the original one in BERT. By the mask-attention-based transformer, the character sequence interacts with external lexical semantics sufficiently and keeps its structure information at the same time. Moreover, against the multi-role noise problem, we make use of event type information from representation and classification, two aspects to enrich entity features, where type markers and event-schema-based mask matrix are proposed. Experimental results on the widely used ACE2005 dataset show the effectiveness of our proposed MABERT on Chinese event extraction task compared with other state-of-the-art methods.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
articleno = {192},
numpages = {21},
keywords = {event ontology, event type markers, mask-attention-based transformer, Event extraction}
}

@inproceedings{10.1145/3324884.3416668,
author = {Nguyen, Hoang Lam and Nassar, Nebras and Kehrer, Timo and Grunske, Lars},
title = {MoFuzz: a fuzzer suite for testing model-driven software engineering tools},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416668},
doi = {10.1145/3324884.3416668},
abstract = {Fuzzing or fuzz testing is an established technique that aims to discover unexpected program behavior (e.g., bugs, security vulnerabilities, or crashes) by feeding automatically generated data into a program under test. However, the application of fuzzing to test Model-Driven Software Engineering (MDSE) tools is still limited because of the difficulty of existing fuzzers to provide structured, well-typed inputs, namely models that conform to typing and consistency constraints induced by a given meta-model and underlying modeling framework. By drawing from recent advances on both fuzz testing and automated model generation, we present three different approaches for fuzzing MDSE tools: A graph grammar-based fuzzer and two variants of a coverage-guided mutation-based fuzzer working with different sets of model mutation operators. Our evaluation on a set of real-world MDSE tools shows that our approaches can outperform both standard fuzzers and model generators w.r.t. their fuzzing capabilities. Moreover, we found that each of our approaches comes with its own strengths and weaknesses in terms of fault finding capabilities and the ability to cover different aspects of the system under test. Thus the approaches complement each other, forming a fuzzer suite for testing MDSE tools.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1103–1115},
numpages = {13},
keywords = {automated model generation, eclipse modeling framework, fuzzing, model-driven software engineering, modeling tools},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3583780.3615992,
author = {Miller, Michael},
title = {Astrolabe: Visual Graph Database Queries with Tabular Output},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615992},
doi = {10.1145/3583780.3615992},
abstract = {Graph databases are an established solution for large, highly connected datasets. One challenge associated with deploying graph databases in industrial settings is usability. Typically, developers interact with graph databases through queries in languages such as Cypher or GraphQL. Many end-users, analysts, and administrators are not familiar with these specialized languages. Additionally, these queries return hierarchical data in formats such as JSON (JavaScript Object Notation) or XML (Extensible Markup Language). Additional scripts and interfaces are needed to convert hierarchical data into more easily digested tables. To overcome these challenges, each graph database use-case typically involves significant custom software to explore, view, and export data.We introduce Astrolabe, a generalized interface that addresses the challenges of querying graph databases. In Astrolabe, queries are constructed visually, so users do not need to learn new graph query languages. Results are returned as tables, which can be easily digested by end users or down-stream applications. Astrolabe was designed to function with arbitrary graph databases, so schema definition is not required. Astrolabe revolutionizes graph exploration and querying by allowing graph databases to be viewed as tables, without the need for custom software adapters.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {5248},
numpages = {1},
keywords = {query generation, no-code, knowledge representation, knowledge graph, interactive information retrieval, graphical user interface, graph database, data visualization, data exploration},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@article{10.1145/3125621,
author = {Ferry, Nicolas and Chauvel, Franck and Song, Hui and Rossini, Alessandro and Lushpenko, Maksym and Solberg, Arnor},
title = {CloudMF: Model-Driven Management of Multi-Cloud Applications},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
issn = {1533-5399},
url = {https://doi.org/10.1145/3125621},
doi = {10.1145/3125621},
abstract = {While the number of cloud solutions is continuously increasing, the development and operation of large-scale and distributed cloud applications are still challenging. A major challenge is the lack of interoperability between the existing cloud solutions, which increases the complexity of maintaining and evolving complex applications potentially deployed across multiple cloud infrastructures and platforms. In this article, we show how the Cloud Modelling Framework leverages model-driven engineering and supports the DevOps ideas to tame this complexity by providing: (i) a domain-specific language for specifying the provisioning and deployment of multi-cloud applications, and (ii) a models@run-time environment for their continuous provisioning, deployment, and adaptation.},
journal = {ACM Trans. Internet Technol.},
month = jan,
articleno = {16},
numpages = {24},
keywords = {multi-cloud, models@run-time, model-driven engineering, DevOps, Cloud computing}
}

@article{10.1145/3624557,
author = {Duong, Huong T. and Ho, Van H. and Do, Phuc},
title = {Fact-checking Vietnamese Information Using Knowledge Graph, Datalog, and KG-BERT},
year = {2023},
issue_date = {October 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {10},
issn = {2375-4699},
url = {https://doi.org/10.1145/3624557},
doi = {10.1145/3624557},
abstract = {In the era of digital information, ensuring the accuracy and reliability of information is crucial, making fact-checking a vital process. Currently, English fact-checking has thrived due to various language processing tools and ample datasets. However, the same cannot be said for Vietnamese fact-checking, which faces significant challenges due to the lack of such resources. To address these challenges, we propose a model for checking Vietnamese facts by synthesizing three popular technologies: Knowledge Graph (KG), Datalog, and KG-BERT. The KG serves as the foundation for the fact-checking process, containing a dataset of Vietnamese information. Datalog, a logical programming language, is used with inference rules to complete the knowledge within the Vietnamese KG. KG-BERT, a Deep Learning (DL) model, is then trained on this KG to rapidly and accurately classify information that needs fact-checking. Furthermore, to put Vietnamese complex sentences into the fact-checking model, we present a solution for extracting triples from these sentences. This approach also contributes significantly to the ease of constructing foundational datasets for the Vietnamese KG. To evaluate the model's performance, we create a Vietnamese dataset comprising 130,190 samples to populate the KG. Using Datalog, we enrich this graph with additional knowledge. The KG is then utilized to train the KG-BERT model, achieving an impressive accuracy of 95\%. Our proposed solution shows great promise for fact-checking Vietnamese information and has the potential to contribute to the development of fact-checking tools and techniques for other languages. Overall, this research makes a significant contribution to the field of data science by providing an accurate solution for fact-checking information in Vietnamese language contexts.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {240},
numpages = {23},
keywords = {datalog, inference rule, KG-BERT, knowledge graph, Fact checking}
}

@inproceedings{10.1145/3706598.3713711,
author = {Wu, Y. Kelly and Sohrawardi, Saniat Javid and Gerstner, Candice R. and Wright, Matthew},
title = {Understanding and Empowering Intelligence Analysts: User-Centered Design for Deepfake Detection Tools},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713711},
doi = {10.1145/3706598.3713711},
abstract = {Intelligence analysts must quickly and accurately examine and report on information in multiple modalities, including video, audio, and images. With the rise of Generative AI and deepfakes, analysts face unprecedented challenges, and require effective, reliable, and explainable media detection and analysis tools. This work explores analysts’ requirements for deepfake detection tools and explainability features. From a study of 30 practitioners from the United States Intelligence Community, we identified the need for a comprehensive and explainable solution that incorporates a wide variety of methods and supports the production of intelligence reports. In response, we propose a design for an analyst-centered tool, and introduce a digital media forensics ontology to support analysts’ interactions with the tool and understanding of its results. We conducted a study grounded in work-related tasks as an initial evaluation of this approach, and report on its potential to assist analysts and areas for improvement in future work.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {870},
numpages = {26},
keywords = {Deepfake, Intelligence Community, Qualitative Studies, Ontology, Explainability},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3701716.3715308,
author = {Sun, Qiang and Li, Sirui and Huynh, Du and Reynolds, Mark and Liu, Wei},
title = {TimelineKGQA: A Comprehensive Question-Answer Pair Generator for Temporal Knowledge Graphs},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715308},
doi = {10.1145/3701716.3715308},
abstract = {Question answering over temporal knowledge graphs (TKGs) is crucial for understanding evolving facts and relationships, yet its development is hindered by limited datasets and difficulties in generating custom QA pairs. We propose a novel categorization framework based on timeline-context relationships, along with TimelineKGQA, a universal temporal QA generator applicable to any TKGs. The code is available at: https://github.com/PascalSun/TimelineKGQA as an open source Python package.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {797–800},
numpages = {4},
keywords = {knowledge graph, question answering, temporal knowledge graph},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3589132.3625629,
author = {Wang, Zhaonan and Jin, Bowen and Hu, Wei and Jiang, Minhao and Kang, Seungyeon and Li, Zhiyuan and Zhou, Sizhe and Han, Jiawei and Wang, Shaowen},
title = {Geospatial Knowledge Hypercube},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625629},
doi = {10.1145/3589132.3625629},
abstract = {Today a tremendous amount of geospatial knowledge is hidden in massive volumes of text data. To facilitate flexible and powerful geospatial analysis and applications, we introduce a new architecture: geospatial knowledge hypercube, a multi-scale, multidimensional knowledge structure that integrates information from geospatial dimensions, thematic themes and diverse application semantics, extracted and computed from spatial-related text data. To construct such a knowledge hypercube, weakly supervised language models are leveraged for automatic, dynamic and incremental extraction of heterogeneous geospatial data, thematic themes, latent connections and relationships, and application semantics, through combining a variety of information from unstructured text, structured tables, and maps. The hypercube lays a foundation for many knowledge discovery and in-depth spatial analysis, and other advanced applications. We have deployed a prototype web application of proposed geospatial knowledge hypercube for public access at: https://hcwebapp.cigi.illinois.edu/.},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {79},
numpages = {4},
keywords = {weakly-supervised text classification, geographic information retrieval, knowledge hypercube},
location = {Hamburg, Germany},
series = {SIGSPATIAL '23}
}

@article{10.1145/3757064,
author = {Guzm\'{a}n, Ana Rosa and Karunaratne, Thashmee},
title = {A Framework for Efficient Semantic Elicitation of EU-Wide Evidence for Public Services},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3757064},
doi = {10.1145/3757064},
abstract = {Digital implementation of the Once-Only Principle (OOP) reduces the administrative burden of accessing public services and enhances public administration performance. It also facilitates the mobility of citizens and businesses within the European Union through legal and effective EU-wide evidence. However, despite the existence of several semantic standards, a lack of an ontology for EU-wide evidence persists, primarily due to the low level of EU harmonisation of information that may prove compliance with procedural requirements. Building EU-wide evidence involves several cross-border communities of practice, composed of experts with different technical, legal, organisational, semantic, idiomatic and cultural backgrounds, spanning various public administration levels and sectors. This diversity introduces a range of cross-border, socio-technical challenges that have not been directly addressed in existing literature or by European projects and initiatives. Based on the lessons learned from the Digital Europe for All (DE4A) large-scale pilot project, the DE4A EU-wide OOP Semantic Elicitation Framework (DEOSEF) is proposed to guide business and semantic experts in the elicitation of EU-wide evidence for target public services, through a collaborative, context-aware, and iterative agile process involving diverse communities of practice. This paper presents DEOSEF as a foundational step for future initiatives involving a combined semantic elicitation-creation of EU-wide evidence.},
note = {Just Accepted},
journal = {Digit. Gov.: Res. Pract.},
month = jul,
keywords = {Once-Only principle, Digital public services, Cross-border interoperability}
}

@inproceedings{10.1145/3487553.3524704,
author = {Xue, Xingsi and Guo, Jianhua},
title = {Word Embedding based Heterogeneous Entity Matching on Web of Things},
year = {2022},
isbn = {9781450391306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3487553.3524704},
doi = {10.1145/3487553.3524704},
abstract = {Web of Things (WoT) is capable of promoting the knowledge discovery and address interoperability problems of diverse Internet of Things (IoT) applications. However, due to the dynamic and diverse features of data entities on WoT, the heterogeneous entity matching has become arguably the greatest “new frontier” for WoT advancements. Currently, the data entities and the corresponding knowledge on WoT are generally modelled with the ontology, and therefore, matching heterogeneous data entities on WoT can be converted to the problem of matching ontologies. Ontology matching is a complex cognitive process, it is usually initially done manually by domain experts. To effectively distinguish the heterogeneous entities and determine high-quality ontology alignment, this work proposes a word embedding based matching technique. Our approach models the word’s semantic in the vector space, and use two vectors’ cosine angle to measure the corresponding words’ similarity. In addition, the word embedding approach does not depend on a specific knowledge base and retain the rich semantic information of words, which makes our proposal more robust. The experiment uses Ontology Alignment Evaluation Initiative (OAEI)’s benchmark for testing, and the experimental results show that our approach outperforms other advanced matching methods.},
booktitle = {Companion Proceedings of the Web Conference 2022},
pages = {941–947},
numpages = {7},
keywords = {Word Embedding, Web of Things, Ontology Matching},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{10.1145/3330089.3330101,
author = {Saba, Djamel and Maouedj, Rachid and Berbaoui, Brahim},
title = {Contribution to the development of an energy management solution in a green smart home (EMSGSH)},
year = {2018},
isbn = {9781450361019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330089.3330101},
doi = {10.1145/3330089.3330101},
abstract = {This document offers a smart solution for managing green energy for a home (EMSGSH). It offers services such as optimizing the energy consumed while assuming green energy available at home. However, the random nature of this type of energy in the power generation, we require using a hybrid energy system (HES) accompanied by energy storage in the form of a battery. The home is considered a distributed system, it is composed of a set of elements that are geographically distributed and in permanent interaction. Following all its features, we chose two approaches for this solution, the multi-agent systems (MAS) for the control of the elements and the domain ontology to ensure a good formal representation of the data associated with the system. We propose a master-slave architecture. A slave agent is proposed for each device to control its local consumption and a single master agent is proposed to control all agents. The objective is to optimize the use of green energy and minimize consumption costs by exploiting the offers of electric power suppliers. The last part of this work was reserved to present the agents (tasks and responsibilities, interactions ...) and to model the agent's society using a unified modeling language (UML), also the ontology elements are presented and edited in the Prot\'{e}g\'{e} software.},
booktitle = {Proceedings of the 7th International Conference on Software Engineering and New Technologies},
articleno = {5},
numpages = {7},
keywords = {Unified modeling language, Smart home, Prot\'{e}g\'{e} software, Ontology, Multi agent systems, Hybrid energy systems, Green energy, Energy efficient, Distributed systems, Decision-making},
location = {Hammamet, Tunisia},
series = {ICSENT 2018}
}

@article{10.1145/3603254,
author = {Romberg, Julia and Escher, Tobias},
title = {Making Sense of Citizens’ Input through Artificial Intelligence: A Review of Methods for Computational Text Analysis to Support the Evaluation of Contributions in Public Participation},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {1},
url = {https://doi.org/10.1145/3603254},
doi = {10.1145/3603254},
abstract = {Public sector institutions that consult citizens to inform decision-making face the challenge of evaluating the contributions made by citizens. This evaluation has important democratic implications but at the same time, consumes substantial human resources. However, until now the use of artificial intelligence such as computer-supported text analysis has remained an under-studied solution to this problem. We identify three generic tasks in the evaluation process that could benefit from natural language processing (NLP). Based on a systematic literature search in two databases on computational linguistics and digital government, we provide a detailed review of existing methods and their performance. While some promising approaches exist, for instance to group data thematically and to detect arguments and opinions, we show that there remain important challenges before these could offer any reliable support in practice. These include the quality of results, the applicability to non-English language corpuses and making algorithmic models available to practitioners through software. We discuss a number of avenues that future research should pursue that can ultimately lead to solutions for practice. The most promising of these bring in the expertise of human evaluators, for example through active learning approaches or interactive topic modeling.},
journal = {Digit. Gov.: Res. Pract.},
month = mar,
articleno = {3},
numpages = {30},
keywords = {Policy analytics, citizen participation, computational linguistics}
}

@inproceedings{10.1145/3687311.3687339,
author = {Yang, Xin and Zhao, Fengjuan},
title = {Integrating AI with Pedagogies: Drama, Multimodal and the Production-oriented Approach- a Study Based on the 6th SFLEP Intercultural Competence Contest},
year = {2024},
isbn = {9798400709920},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687311.3687339},
doi = {10.1145/3687311.3687339},
abstract = {Cultural studies have garnered significant attention in China for many decades, and the cultivation of intercultural competence within the academic sphere has been meticulously developed, particularly within the university context. Although intercultural competence is inherently multidisciplinary, its cultivation is predominantly integrated into the pedagogy of foreign language instruction. The discourse surrounding intercultural communication is mainly led by educators and scholars in the field of foreign language studies. The SFLEP Intercultural Competence Contest comprises three pivotal tasks: the development of intercultural case studies, scenario analysis, and the narration of Chinese stories. The rapid development of AI has opened new avenues in the field of education, particularly in language learning. The integration of AI with traditional pedagogies like drama, multimodal learning, and the production-oriented approach has been observed to enrich the learning experience and improve intercultural competence. A thorough examination of the 6th iteration of the contest provides the foundation for this exploration. The study not only highlights the potential of AI integrated approaches but also underscores their significant relevance in enhancing scaffolding techniques in foreign language education.},
booktitle = {Proceedings of the 2024 International Conference on Intelligent Education and Computer Technology},
pages = {151–157},
numpages = {7},
location = {Guilin, China},
series = {IECT '24}
}

@article{10.1145/3760786,
author = {Jian, Yue and Zhang, Miao and Qin, Ziyue and Xie, Chuyuan and Xiao, Kui and Zhang, Yan and Li, Zhifei},
title = {Adaptive Modality Interaction Transformer for Multimodal Knowledge Graph Completion},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1556-4681},
url = {https://doi.org/10.1145/3760786},
doi = {10.1145/3760786},
abstract = {Knowledge graphs (KGs) are frequently confronted with the challenge of incompleteness, a problem that extends to multimodal knowledge graphs (MKGs). The primary goal of multimodal knowledge graph completion (MKGC) is to predict missing entities within MKGs. However, current MKGC methods face difficulties in adequately addressing modal preferences and imbalances in modal information. To overcome these issues, we introduce AdaMKGC, an innovative hybrid model incorporating an adaptive modality interaction transformer. This model employs a dynamic attention interaction strategy and a self-enhancing sampling approach. AdaMKGC achieves a more precise utilization of multimodal information by integrating modal preference information into modal interactions. Additionally, it effectively mitigates the issue of modal imbalance through targeted sampling and adjustment for entities with deficient information. Experimental evaluations demonstrate AdaMKGC's superior performance in overcoming these prevalent challenges. Compared to existing state-of-the-art MKGC models, AdaMKGC shows a notable enhancement of 28\% in MR on the WN18-IMG dataset and an improvement of 2.7\% in Hits@1 on the FB15k-237-IMG dataset. Our code is available at .},
note = {Just Accepted},
journal = {ACM Trans. Knowl. Discov. Data},
month = aug,
keywords = {Multimodal Knowledge Graphs, Knowledge Graph Completion, Link Prediction}
}

@inbook{10.1145/3677389.3702495,
author = {Mu, Wenchuan and Liu, Junhua and Lim, Kwan Hui},
title = {Fast Bibliography Pre-Selection via Two-Vector Semantic Representations},
year = {2025},
isbn = {9798400710933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3677389.3702495},
abstract = {In academic writing, bibliography compilations is essential but time-consuming, often requiring repeated searches for references. Hence, an efficient tool for faster bibliography compilation is needed. Our work offers a solution to the challenges of managing large-scale bibliographic databases, introducing a new algorithm that improves both efficiency and sensitivity. Using two-vector semantic modelling, bibliographic entries and queries are embedded into the same vector space to select relevant references based on semantic similarity. Experimental results with 3.37 million entries show the method reduces the time needed to generate a manageable subset, streamlining scholarly writing. Our code and dataset are publicly available at https://github.com/cestwc/bibliography-pre-selection.},
booktitle = {Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries},
articleno = {23},
numpages = {6}
}

@inproceedings{10.1145/3638884.3638979,
author = {Wu, Yu and Miao, Lin and Li, Han},
title = {Attribute Value Extraction in Weapon Domain Based on Bi-LSTM and Attention},
year = {2024},
isbn = {9798400708909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3638884.3638979},
doi = {10.1145/3638884.3638979},
abstract = {Aiming at the problem that the traditional extraction method caused by the diversification of weapon attributes has a large amount of work to construct the label of weapon attributes, in this paper, we propose a weapon attribute value extraction method based on bidirectional long-term and short-term memory network (Bi-LSTM) and attention mechanism. The method first uses the Bi-LSTM model to extract the features of the input text and attribute names. Then, the attention mechanism focuses on the relations between words and attributes in the sentence. Afterward, the global BIO tag marks the position of the attribute values in the sentence. In this way, the method can reduce the workload during the corpus preparation period to improve the generalization ability of the model so that it can extract different weapon attribute data. Compared with Bi-LSTM, Bi-LSTM_CRF, and OpenTag from the experimental results, the F1 values of the proposed model on the weapon domain attribute dataset are increased by about 6.9\%, 5.7\%, and 2.5\%, respectively.},
booktitle = {Proceedings of the 2023 9th International Conference on Communication and Information Processing},
pages = {603–610},
numpages = {8},
keywords = {Attribute Value Extraction, Information Extraction, Knowledge Base, Natural Language Processing},
location = {Lingshui, China},
series = {ICCIP '23}
}

@article{10.1145/3609336,
author = {Hamed, Naeima and Gaglione, Andrea and Gluhak, Alex and Rana, Omer and Perera, Charith},
title = {Query Interface for Smart City Internet of Things Data Marketplaces: A Case Study},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
url = {https://doi.org/10.1145/3609336},
doi = {10.1145/3609336},
abstract = {Cities are increasingly becoming augmented with sensors through public, private, and academic sector initiatives. Most of the time, these sensors are deployed with a primary purpose (objective) in mind (e.g., deploy sensors to understand noise pollution) by a sensor owner (i.e., the organization that invests in sensing hardware, e.g., a city council). Over the past few years, communities undertaking smart city development projects have understood the importance of making the sensor data available to a wider community—beyond their primary usage. Different business models have been proposed to achieve this, including creating data marketplaces. The vision is to encourage new startups and small and medium-scale businesses to create novel products and services using sensor data to generate additional economic value. Currently, data are sold as pre-defined independent datasets (e.g., noise level and parking status data may be sold separately). This approach creates several challenges, such as (i) difficulties in pricing, which leads to higher prices (per dataset); (ii) higher network communication and bandwidth requirements; and (iii) information overload for data consumers (i.e., those who purchase data). We investigate the benefit of semantic representation and its reasoning capabilities toward creating a business model that offers data on demand within smart city Internet of Things data marketplaces. The objective is to help data consumers (i.e., small and medium enterprises) acquire the most relevant data they need. We demonstrate the utility of our approach by integrating it into a real-world IoT data marketplace (developed by the synchronicity-iot.eu project). We discuss design decisions and their consequences (i.e., tradeoffs) on the choice and selection of datasets. Subsequently, we present a series of data modeling principles and recommendations for implementing IoT data marketplaces.},
journal = {ACM Trans. Internet Things},
month = sep,
articleno = {19},
numpages = {39},
keywords = {knowledge management, linked data, multi-dimensional querying, data discovery, semantic interoperability, Internet of Things}
}

@article{10.1145/3469722,
author = {Kulkarni, Dhanashree S. and Rodd, Sunil S.},
title = {Sentiment Analysis in Hindi—A Survey on the State-of-the-art Techniques},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {2375-4699},
url = {https://doi.org/10.1145/3469722},
doi = {10.1145/3469722},
abstract = {Sentiment Analysis (SA) has been a core interest in the field of text mining research, dealing with computational processing of sentiments, views, and subjective nature of the text. Due to the availability of extensive web-based data in Indian languages such as Hindi, Marathi, Kannada, Tamil, and so on. It has become extremely significant to analyze this data and recover valuable and relevant information. Hindi being the first language of the majority of the population in India, SA in Hindi has turned out to be a critical task particularly for companies and government organizations. This research portrays a systematic review specifically in the field of Hindi SA. The major contribution of this article includes the categorization of numerous articles based on techniques that have attracted researchers in performing SA tasks in Hindi language. This survey classifies these state-of-the-art computational intelligence techniques into four major categories namely lexicon-based techniques, machine learning techniques, deep learning techniques, and hybrid techniques. It discusses the importance of these techniques based on different aspects such as their impact on the issues of SA, levels of analysis, and performance evaluation measures. The research puts forward a comprehensive overview of the majority of the work done in Hindi SA. This study will help researchers in finding out resources such as annotated datasets, linguistic resources, and lexical resources. This survey delivers some significant findings and presents overall future research directions in the field of Hindi SA.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = nov,
articleno = {21},
numpages = {46},
keywords = {systematic review, lexicon technique, opinion mining, hindi language, Sentiment analysis}
}

@inproceedings{10.1145/3341105.3373974,
author = {Cornejo Lupa, Maria A. and Ticona-Herrera, Regina P. and Cardinale, Yudith and Barrios-Aranibar, Dennis},
title = {A categorization of simultaneous localization and mapping knowledge for mobile robots},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373974},
doi = {10.1145/3341105.3373974},
abstract = {Autonomous robots are playing important roles in academic, technological, and scientific activities. Thus, their behavior is getting more complex. The main tasks of autonomous robots include mapping an environment and localize themselves. These tasks comprise the Simultaneous Localization and Mapping (SLAM) problem. Representation of the SLAM knowledge (e.g., robot characteristics, environment information, mapping and location information), with a standard and well-defined model, provides the base to develop efficient and interoperable solutions. However, as far as we know, there is not a common classification of such knowledge. Many existing works based on Semantic Web, have formulated ontologies to model information related to only some SLAM aspects, without a standard arrangement. In this paper, we propose a categorization of the knowledge managed in SLAM, based on existing ontologies and SLAM principles. We also classify recent and popular ontologies according to our proposed categories and highlight the lessons to learn from existing solutions.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {956–963},
numpages = {8},
keywords = {semantic web, semantic robots, ontologies, mobile robots, SLAM},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@article{10.14778/3704965.3704970,
author = {Bellomarini, Luigi and Benedetto, Davide and Brandetti, Matteo and Sallinger, Emanuel and Vlad, Adriano},
title = {The Vadalog Parallel System: Distributed Reasoning with Datalog+/-},
year = {2024},
issue_date = {September 2024},
publisher = {VLDB Endowment},
volume = {17},
number = {13},
issn = {2150-8097},
url = {https://doi.org/10.14778/3704965.3704970},
doi = {10.14778/3704965.3704970},
abstract = {Over the past years, there has been a growing demand for ontological reasoning systems based on languages of the Datalog+/- family, such as Vadalog, for their ability to effectively model a wide range of real-world problems with powerful features such as existential quantification. As the scale and complexity of data analysis tasks continue to grow, the ability to distribute the computational workload across multiple non-communicating processors has become vital for these systems to achieve scalable performance.The joint presence of existential quantification and recursion poses new challenges, currently unsolved by existing distributed systems, which only concentrate on Datalog and are therefore unsuitable for ontological reasoning. When working across multiple processors, generating all the facts to answer a specific reasoning query, avoiding duplication, and guaranteeing termination are non-trivial tasks as infinitely many new symbols and facts can be generated by existential quantification and recursion.In this paper, we address such challenges and introduce the first distributed framework in the Datalog+/- space. We propose the condition of homomorphic decomposability, which identifies sets of Datalog+/- rules with good distribution properties. We put homomorphic decomposability into action with a distributed reasoning algorithm for Warded Datalog+/-, the core of Vadalog. We implement Vadalog Parallel, a distributed reasoner for Vadalog and provide experimental evaluation against state-of-the-art systems.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {4614–4626},
numpages = {13}
}

@inproceedings{10.1145/3447568.3448541,
author = {Capodieci, Antonio and Mainetti, Luca and Dipietrangelo, Flavio},
title = {Model-Driven approach to Cyber Risk Analysis in Industry 4.0},
year = {2021},
isbn = {9781450376556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447568.3448541},
doi = {10.1145/3447568.3448541},
abstract = {In the contest of industrial process and automation, and in particular in the so-called Industry 4.0, the now intensive application of control systems in interconnected networks has led to an increase in unexpected threats to information security for supervisory control and data acquisition (SCADA) and control systems distributed (DCS).Risk assessment is essential and the its common methods such as HHM, IIM, and RFRM have been successfully applied to SCADA systems.Another equally important need is the use of metrics and methodologies to analyze the risk (PRA- probability risk analysis), which includes methods such as FTA, ETA and FEMA and HAZOP. The goal of these methods is, in general, to determine the impact of a problem on the process plant and the risk reduction associated with a particular countermeasure.In this paper we present a methodology named CRiSP (Cyber Risk Analysis in Industrial Process System Environment). CRiSP defines an approach to analyze the risk related to the manipulation of a single element of the plant and to analyze the consequence to entire plant and in the same time to a restricted portion.},
booktitle = {Proceedings of the 10th International Conference on Information Systems and Technologies},
articleno = {33},
numpages = {7},
keywords = {Risk management, Risk Analysis, Industry 4.0, Cybersecurity},
location = {Lecce, Italy},
series = {ICIST '20}
}

@inproceedings{10.1145/3543873.3587540,
author = {Naik, Riya},
title = {Multi-turn mediated solutions for Conversational Artificial Intelligent systems leveraging graph-based techniques},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587540},
doi = {10.1145/3543873.3587540},
abstract = {The current era is dominated by intelligent Question Answering (QA) systems that can instantly answer almost all their questions, saving users search time and increasing the throughput and precision in the applied domain. A vast amount of work is being carried out in QA systems to deliver better content satisfying users’ information needs [2]. Since QA systems are ascending the cycle of emerging technologies, there are potential research gaps that can be explored. QA systems form a significant part of Conversational Artificial Intelligent systems giving rise to a new research pathway, i.e., Conversational Question Answering (CQA) systems [32]. We propose to design and develop a CQA system leveraging Hypergraph-based techniques. The approach focuses on the multi-turn conversation and multi-context to gauge users’ exact information needs and deliver better answers. We further aim to address "supporting evidence-based retrieval" for fact-based responsible answer generation. Since the QA system requires a large amount of data and processing, we also intend to investigate hardware performance for effective system utilization.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {586–590},
numpages = {5},
keywords = {Contextual Embeddings, Conversational Artificial Intelligence, Evidence-based retrieval, Graph-based models, Question Answering},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{10.1109/SEAMS.2019.00018,
author = {Bennaceur, Amel and Ghezzi, Carlo and Tei, Kenji and Kehrer, Timo and Weyns, Danny and Calinescu, Radu and Dustdar, Schahram and Hu, Zhenjiang and Honiden, Shinichi and Ishikawa, Fuyuki and Jin, Zhi and Kramer, Jeffrey and Litoiu, Marin and Loreti, Michele and Moreno, Gabriel A. and M\"{u}ller, Hausi A. and Nenzi, Laura and Nuseibeh, Bashar and Pasquale, Liliana and Reisig, Wolfgang and Schmidt, Heinz and Tsigkanos, Christos and Zhao, Haiyan},
title = {Modelling and analysing resilient cyber-physical systems},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SEAMS.2019.00018},
doi = {10.1109/SEAMS.2019.00018},
abstract = {From smart buildings to medical devices to smart nations, software systems increasingly integrate computation, networking, and interaction with the physical environment. These systems are known as Cyber-Physical Systems (CPS). While these systems open new opportunities to deliver improved quality of life for people and reinvigorate computing, their engineering is a difficult problem given the level of heterogeneity and dynamism they exhibit. While progress has been made, we argue that complexity is now at a level such that existing approaches need a major re-think to define principles and associated techniques for CPS. In this paper, we identify research challenges when modelling, analysing and engineering CPS. We focus on three key topics: theoretical foundations of CPS, self-adaptation methods for CPS, and exemplars of CPS serving as a research vehicle shared by a larger community. For each topic, we present an overview and suggest future research directions, thereby focusing on selected challenges. This paper is one of the results of the Shonan Seminar 118 on Modelling and Analysing Resilient Cyber-Physical Systems, which took place in December 2018.},
booktitle = {Proceedings of the 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {70–76},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {SEAMS '19}
}

@inproceedings{10.1145/3323873.3325026,
author = {Karayil, Tushar and Blandfort, Philipp and Hees, J\"{o}rn and Dengel, Andreas},
title = {The Focus-Aspect-Value Model for Explainable Prediction of Subjective Visual Interpretation},
year = {2019},
isbn = {9781450367653},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323873.3325026},
doi = {10.1145/3323873.3325026},
abstract = {Subjective visual interpretation is a challenging yet important topic in computer vision. Many approaches reduce this problem to the prediction of adjective- or attribute-labels from images. However,most of these do not take attribute semantics into account, or only process the image in a holistic manner. Furthermore, there is alack of relevant datasets with fine-grained subjective labels. In this paper, we propose the Focus-Aspect-Value (FAV) model to structure the process of capturing subjectivity in image processing,and introduce a novel dataset following this way of modeling. We run experiments on this dataset to compare several deep learning methods and find that incorporating context information based on tensor multiplication outperforms the default way of information fusion (concatenation).},
booktitle = {Proceedings of the 2019 on International Conference on Multimedia Retrieval},
pages = {16–24},
numpages = {9},
keywords = {zero-shot, subjectivity, neural network, logistic regression, information fusion, images, fav},
location = {Ottawa ON, Canada},
series = {ICMR '19}
}

@inproceedings{10.1145/3308560.3316518,
author = {McKenna, Lucy and Debruyne, Christophe and O'Sullivan, Declan},
title = {Modelling the Provenance of Linked Data Interlinks for the Library Domain},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316518},
doi = {10.1145/3308560.3316518},
abstract = {As the Web of Data grows, so does the need to establish the quality and trustworthiness of its contents. Increasing numbers of libraries are publishing their metadata as Linked Data (LD). As these institutions are considered authoritative sources of information, it is likely that library LD will be treated with increased credibility over data published by other sources. However, in order to establish this trust, the provenance of library LD must be provided.In 2018 we conducted a survey which explored the position of Information Professionals (IPs), such as librarians, archivists and cataloguers, with regards to LD. Results indicated that IPs find the process of LD interlinking to be a particularly challenging. In order to publish authoritative interlinks, provenance data for the description and justification of the links is required. As such, the goal of this research is to provide a provenance model for the LD interlinking process that meets the requirements of library metadata standards. Many current LD technologies are not accessible to non-technical experts or attuned to the needs of the library domain. By designing a model specifically for libraries, with input from IPs, we aim to facilitate this domain in the process of creating interlink provenance data.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {954–958},
numpages = {5},
keywords = {semantic web, provenance, linked data, library, interlinking},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3450614.3463389,
author = {Diaz-Agudo, Belen and Bosca, Alessio and Bolioli, Andrea and Jimenez Jimenez Diaz, Guilermo and Kuflik, Tsvi and J. Wecker, Alan},
title = {Towards Personalized Social Recommendations for Cultural Heritage Activities: Methods and technology to enable cohesive and inclusive recommendations},
year = {2021},
isbn = {9781450383677},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450614.3463389},
doi = {10.1145/3450614.3463389},
abstract = {The aim of the SPICE project is to build social cohesion, both between and within citizen communities, by developing tools and methods to support citizen curation. We define citizen curation as a process in which cultural objects are used as a resource by citizens to develop their own personal interpretations. Within communities, citizens can use their interpretations to build a representation of themselves and their shared perspective on culture. Interpretations can also be used to support social cohesion across groups. In this short position paper we outline the methodologies and technologies needed to be built in order to build a recommender system of cultural objects that will implement these goals of social cohesion and inclusion.},
booktitle = {Adjunct Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {199–202},
numpages = {4},
location = {Utrecht, Netherlands},
series = {UMAP '21}
}

@article{10.14778/3712221.3712232,
author = {Gilray, Thomas and Sahebolamri, Arash and Sun, Yihao and Kunapaneni, Sowmith and Kumar, Sidharth and Micinski, Kristopher},
title = {Datalog with First-Class Facts},
year = {2024},
issue_date = {November 2024},
publisher = {VLDB Endowment},
volume = {18},
number = {3},
issn = {2150-8097},
url = {https://doi.org/10.14778/3712221.3712232},
doi = {10.14778/3712221.3712232},
abstract = {Datalog is a popular logic programming language for deductive reasoning tasks in a wide array of applications, including business analytics, program analysis, and ontological reasoning. However, Datalog's restriction to flat facts over atomic constants leads to challenges in working with tree-structured data, such as derivation trees or abstract syntax trees. To ameliorate Datalog's restrictions, popular extensions of Datalog support features such as existential quantification in rule heads (Datalog*, Datalog∃) or algebraic data types (Souffl\'{e}). Unfortunately, these are imperfect solutions for reasoning over structured and recursive data types, with general existentials leading to complex implementations requiring unification, and ADTs unable to trigger rule evaluation and failing to support efficient indexing.We present DL∃!, a Datalog with first-class facts, wherein every fact is identified with a Skolem term unique to the fact. We show that this restriction offers an attractive price point for Datalogbased reasoning over tree-shaped data, demonstrating its application to databases, artificial intelligence, and programming languages. We implemented DL∃! as a system Slog, which leverages the uniqueness restriction of DL∃! to enable a communication-avoiding, massively-parallel implementation built on MPI. We show that Slog outperforms leading systems (Nemo, Vlog, RDFox, and Souffl\'{e}) on a variety of benchmarks, with the potential to scale to thousands of threads.},
journal = {Proc. VLDB Endow.},
month = nov,
pages = {651–665},
numpages = {15}
}

@inproceedings{10.5555/3721488.3721763,
author = {Cooper, Sara and Ros, Raquel and Lemaignan, S\'{e}verin and Gebell\'{\i}, Ferran and Ferrini, Lorenzo and Juri?i\'{c}, Luka},
title = {Demonstration of an Open-source ROS 2 Framework and Simulator for Situated Interactive Social Robot},
year = {2025},
publisher = {IEEE Press},
abstract = {We introduce an open-source ROS 2 architecture for situated social robots, along with a simulator that allows mixed-reality development and interactions. The architecture is a hybrid symbolic/subsymbolic system that integrates explicit ontology semantics for perception, reasoning, and execution, with LLMs. It features multimodal social perception by leveraging the open source ROS4HRI framework; LLMs (both edge- and cloud-based) to facilitate natural language interaction between the user and system; KnowledgeCore, an open-source knowledge base, to reason about facts in the world; and an intent-based controller to supervise the execution of parallel/sequential tasks and skills. We demonstrate our system architecture with a social robot running the mixed-reality system.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1770–1772},
numpages = {3},
keywords = {mixed-reality simulator, ros 2 framework, situated social robots},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3539618.3592092,
author = {Lin, Hsien-Chin and Feng, Shutong and Geishauser, Christian and Lubis, Nurul and van Niekerk, Carel and Heck, Michael and Ruppik, Benjamin and Vukovic, Renato and Gasi\'{c}, Milica},
title = {EmoUS: Simulating User Emotions in Task-Oriented Dialogues},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3592092},
doi = {10.1145/3539618.3592092},
abstract = {Existing user simulators (USs) for task-oriented dialogue systems only model user behaviour on semantic and natural language levels without considering the user persona and emotions. Optimising dialogue systems with generic user policies, which cannot model diverse user behaviour driven by different emotional states, may result in a high drop-off rate when deployed in the real world. Thus, we present EmoUS, a user simulator that learns to simulate user emotions alongside user behaviour. EmoUS generates user emotions, semantic actions, and natural language responses based on the user goal, the dialogue history, and the user persona. By analysing what kind of system behaviour elicits what kind of user emotions, we show that EmoUS can be used as a probe to evaluate a variety of dialogue systems and in particular their effect on the user's emotional state. Developing such methods is important in the age of large language model chat-bots and rising ethical concerns.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2526–2531},
numpages = {6},
keywords = {dialogue system, emotion simulation, user simulation},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3336499.3338012,
author = {Maurino, Andrea and Rula, Anisa and von, Bj\o{}rn Marius and Gomez, Mauricio Soto and Elves\ae{}ter, Brian and Roman, Dumitru},
title = {Modelling and Linking Company Data in the euBusinessGraph Platform},
year = {2019},
isbn = {9781450368230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336499.3338012},
doi = {10.1145/3336499.3338012},
abstract = {In the business environment, knowledge of company data is essential for a variety of tasks. The European funded project euBusinessGraph enables the establishment of a company data platform where data providers and consumers can publish and access company data. The core of the platform is the semantic data model that is the conceptual representation of company data in a common way so that it is easier to share and interlink company data. In this paper we show how the unified model and Grafterizer, a tool for manipulating and transforming raw data into Linked Data, support the linking challenge proposed in FEIII 2019. Results show that geographical enrichment of RDF data supports the interlinking process between company entities in different datasets.},
booktitle = {Proceedings of the 5th Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets},
articleno = {12},
numpages = {6},
keywords = {Record Linkage, RDF, Entity Matching, Company data},
location = {Amsterdam, Netherlands},
series = {DSMM'19}
}

@inproceedings{10.1145/3599957.3606249,
author = {Ahn, Sung-Yoon and Lee, Sang-Woong},
title = {BERT-based classification of fungi protein sequences with multiple GO labels},
year = {2023},
isbn = {9798400702280},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599957.3606249},
doi = {10.1145/3599957.3606249},
abstract = {Due to the increase of reported fungi-related diseases, it has come to many health organizations concern that there may be possible highly contagious fungi that may cause yet another pandemic. Though the likelihood of such is low, research is needed to grasp the understanding of unknown fungi. Identifying and figuring out the traits of unknown fungi through in vitro and in vivo experiments take time and resources. In silico methods yield faster results with a slight drop in accuracy. Modern in silico approaches utilizing deep learning, allow for faster and more accurate classifications. In this study, we perform the classification of one or more gene ontologies of fungi protein sequences. We collected open-source protein sequences from UniProt and applied an algorithm to label the sequences with their gene ontologies. We use ProtBERT with additional layers to give classification results to all the different gene ontologies. Experimental results reveal that when classifying with the top 5 most frequent gene ontologies, the model was able to yield 0.7915 for F1-score, 0.7073 for MCC, and 0.8865 for AuROC. With the top 10 most frequent gene ontologies it yielded 0.6490 for F1-score, 0.6836 for MCC, and 0.7653 for AuROC.},
booktitle = {Proceedings of the 2023 International Conference on Research in Adaptive and Convergent Systems},
articleno = {28},
numpages = {4},
keywords = {Gene Ontology, Fungi, BERT},
location = {Gdansk, Poland},
series = {RACS '23}
}

@inproceedings{10.1145/3626772.3657666,
author = {Prieur, Maxime and Du Mouza, C\'{e}dric and Gadek, Guillaume and Grilheres, Bruno},
title = {Shadowfax: Harnessing Textual Knowledge Base Population},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657666},
doi = {10.1145/3626772.3657666},
abstract = {Knowledge base population (KBP) from texts involves the extraction and organization of information from unstructured textual data to enhance or create a structured knowledge base. This process is crucial for various applications, such as natural language understanding, question-answering systems, and knowledge-driven decision-making. However the difficulty lies in the complexity of natural language, which is nuanced, ambiguous, and context-dependent. Extracting accurate and reliable information requires overcoming challenges such as entity disambiguation and relation extraction which are time-consuming tasks for users.Shadowfax is an interactive platform designed to support users by streamlining the process of knowledge base population (KPB) from text documents. Unlike other existing tools, it relies on a unified machine learning model to extract relevant information from unstructured text, enabling operational agents to gain a quick overview. The proposed system supports a variety of natural language processing (NLP) tasks using a single architecture, while presenting information in the most comprehensive way possible to the end user.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2796–2800},
numpages = {5},
keywords = {data mining, deep-learning, end-to-end, information extraction, knowledge base population, user in the loop},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@inproceedings{10.1145/3366030.3366128,
author = {Garc\'{\i}a, Roberto and Gil, Rosa},
title = {Social Media Copyright Management using Semantic Web and Blockchain},
year = {2020},
isbn = {9781450371797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366030.3366128},
doi = {10.1145/3366030.3366128},
abstract = {Solutions based on distributed ledgers require sophisticated tools for data modelling and integration that can be overcome using semantic and Linked Data technologies. One example is copyright management, where we attempt to adapt the Copyright Ontology so it can be used to build applications that benefit from both worlds, rich information modelling and reasoning together with immutable and accountable information storage that provides trust and confidence on the modelled rights statements. This approach has been applied in the context of an application for the management of social media re-use for journalistic purposes.},
booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {339–343},
numpages = {5},
keywords = {Semantic Web, Rights Expression Language, Ontology, Linked Data, Ethereum, Distributed Ledger, Copyright, Blockchain},
location = {Munich, Germany},
series = {iiWAS2019}
}

@inproceedings{10.1145/3429889.3430079,
author = {Wu, Nankai and Cao, Qingsong and Li, Huanzhe and Hou, Xingquan and Lo, Infat and Kong, Jiangping},
title = {Correlation between Pathological Voice Onset and Voice Quality Based on Vocal Attack Time(VAT) and Multidimensional Voice Parameters},
year = {2020},
isbn = {9781450388603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429889.3430079},
doi = {10.1145/3429889.3430079},
abstract = {Correlation between pathological Voice Onset and Voice Quality of Chinese patients based on Vocal Attack Time(VAT) and Multidimensional Voice Parameters is discussed in this paper. The test subjects were divided into three groups, one is normal voice group and the other two are pathologic voice groups, namely, vocal cord polyps and non vocal cord polyps. We recorded the EGG signal for the above subjects and extracted the Jitter, Shimmer, HNR and VAT parameters by the relevant software. The VAT and other voice parameters at /a:/, /i:/ and /u:/ vowels were then compared and analyzed in different groups. The results showed that there was no significant difference in the VAT between the three groups at /a:/, /i:/, and /u:/ vowels. In addition, the analysis of VAT changes in a patient with vocal cord polyps before and after surgery revealed that neither the overall difference in stops nor the difference in manner of aspiration or not was significant, indicating VAT is not a specific indicator of vocal cord polyps.},
booktitle = {Proceedings of the 1st International Symposium on Artificial Intelligence in Medical Sciences},
pages = {107–111},
numpages = {5},
keywords = {Voice Quality, Voice Onset, Vocal Attack Time, Pathological Voice, Electroglottography},
location = {Beijing, China},
series = {ISAIMS '20}
}

@inproceedings{10.1145/3178461.3178468,
author = {Wakil, Karzan and Jawawi, Dayang N. A.},
title = {A New Adaptive Model for Web Engineering Methods to Develop Modern Web Applications},
year = {2018},
isbn = {9781450354387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3178461.3178468},
doi = {10.1145/3178461.3178468},
abstract = {With the evolution of modern web applications, several web engineering methods proposed to develop web applications. The modern web applications are; Rich Internet Application (RIA), Semantic Web Application (SWA), Ubiquitous Web Applications (UWA), and Intelligent Web Applications (IWA), with each of them having new features. The problem is that current web engineering methods cannot support new features of modern web applications. However, some of them extended for new concern of web applications but have limited, meaning these methods have a lack of adaptability to support features from modern web applications. In an attempt to solve this gap, we have defined a new adaptive model for the web engineering methods that can support the new features of modern web applications. This model very efficient in the process development and will be to increase the usability of the methods.},
booktitle = {Proceedings of the 2018 International Conference on Software Engineering and Information Management},
pages = {32–39},
numpages = {8},
keywords = {Web Engineering, Web Applications, Adaptive Model},
location = {Casablanca, Morocco},
series = {ICSIM '18}
}

@article{10.1145/3424667,
author = {Ungureanu, George and Medeiros, Jos\'{e} Edil Guimar\~{a}es De and sundstr\"{o}m, Timmy and S\"{o}derquist, Ingemar and \r{A}hlander, Anders and Sander, Ingo},
title = {ForSyDe-Atom: Taming Complexity in Cyber Physical System Design with Layers},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1539-9087},
url = {https://doi.org/10.1145/3424667},
doi = {10.1145/3424667},
abstract = {We present ForSyDe-Atom, a formal framework intended as an entry point for disciplined design of complex cyber-physical systems. This framework provides a set of rules for combining several domain-specific languages as structured, enclosing layers to orthogonalize the many aspects of system behavior, yet study their interaction in tandem. We define four layers: one for capturing timed interactions in heterogeneous systems, one for structured parallelism, one for modeling uncertainty, and one for describing component properties. This framework enables a systematic exploitation of design properties in a design flow by facilitating the stepwise projection of certain layers of interest, the isolated analysis and refinement on projections, and the seamless reconstruction of a system model by virtue of orthogonalization. We demonstrate the capabilities of this approach by providing a compact yet expressive model of an active electronically scanned array antenna and signal processing chain, simulate it, validate its conformity with the design specifications, refine it, synthesize a sub-system to VHDL and sequential code, and co-simulate the generated artifacts.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = jan,
articleno = {10},
numpages = {27},
keywords = {validation, system design language, synthesis, simulation, models of computation, modeling, design methodology, Cyber-physical systems}
}

@inproceedings{10.1145/3583780.3615514,
author = {Colas, Anthony and Ma, Haodi and He, Xuanli and Bai, Yang and Wang, Daisy Zhe},
title = {Can Knowledge Graphs Simplify Text?},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615514},
doi = {10.1145/3583780.3615514},
abstract = {Knowledge Graph (KG)-to-Text Generation has seen recent improvements in generating fluent and informative sentences which describe a given KG. As KGs are widespread across multiple domains and contain important entity-relation information, and as text simplification aims to reduce the complexity of a text while preserving the meaning of the original text, we propose KGSimple, a novel approach to unsupervised text simplification which infuses KG-established techniques in order to construct a simplified KG path and generate a concise text which preserves the original input's meaning. Through an iterative and sampling KG-first approach, our model is capable of simplifying text when starting from a KG by learning to keep important information while harnessing KG-to-text generation to output fluent and descriptive sentences. We evaluate various settings of the KGSimple model on currently-available KG-to-text datasets, demonstrating its effectiveness compared to unsupervised text simplification models which start with a given complex text. Our code is available on GitHub.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {379–389},
numpages = {11},
keywords = {text simplification, simulated annealing, natural language generation, knowledge graph, data-to-text, KG-to-text},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}

@book{10.1145/3674127,
editor = {Alonso, Omar and Baeza-Yates, Ricardo},
title = {Information Retrieval: Advanced Topics and Techniques},
year = {2024},
isbn = {9798400710506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
edition = {1},
volume = {60},
abstract = {In the last decade, deep learning and word embeddings have made significant impacts on information retrieval (IR) by adding techniques based in neural networks and language models. At the same time, certain search modalities such as neural IR and conversational search have become more popular. This book, written by international academic and industry experts, brings the field up to date with detailed discussions of these new approaches and techniques. The book is organized in three sections: Foundations, Adaptations and Concerns, and Verticals.Under Foundations, we address topics that form the basic structure of any modern IR system, including recommender systems. These new techniques are developed to augment indexing, retrieval, and ranking. Neural IR, recommender systems, evaluation, query-driven functionality, and knowledge graphs are covered in this section.IR systems need to adapt to specific user characteristics and preferences, and techniques that were considered too niche a few years ago are now a matter of system design consideration. The Adaptations and Concerns section covers the following topics: conversational search, cross-language retrieval, temporal extraction and retrieval, bias in retrieval systems, and privacy in search.While web search engines are the most popular information access point, there are cases where specific verticals provide a better experience in terms of content and relevance. The Verticals section describes eCommerce, professional search, personal collections, music retrieval, and biomedicine as examples.}
}

@inproceedings{10.1145/3613904.3642542,
author = {Mildner, Thomas and Cooney, Orla and Meck, Anna-Maria and Bartl, Marion and Savino, Gian-Luca and Doyle, Philip R and Garaialde, Diego and Clark, Leigh and Sloan, John and Wenig, Nina and Malaka, Rainer and Niess, Jasmin},
title = {Listening to the Voices: Describing Ethical Caveats of Conversational User Interfaces According to Experts and Frequent Users},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642542},
doi = {10.1145/3613904.3642542},
abstract = {Advances in natural language processing and understanding have led to a rapid growth in the popularity of conversational user interfaces (CUIs). While CUIs introduce novel benefits, they also yield risks that may exploit people’s trust. Although research looking at unethical design deployed through graphical user interfaces (GUIs) established a thorough understanding of so-called dark patterns, there is a need to continue this discourse within the CUI community to understand potentially problematic interactions. Addressing this gap, we interviewed 27 participants from three cohorts: researchers, practitioners, and frequent users of CUIs. Applying thematic analysis, we construct five themes reflecting each cohort’s insights about ethical design challenges and introduce the CUI Expectation Cycle, bridging system capabilities and user expectations while considering each theme’s ethical caveats. This research aims to inform future development of CUIs to consider ethical constraints while adopting a human-centred approach.},
booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {307},
numpages = {18},
keywords = {CUI, chatbots, conversational agents, conversational user interfaces, dark patterns, deceptive design patterns, ethical design, thematic analysis, voice agents},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@article{10.1145/3736656,
author = {de Roode, Gerard and Everts, Maarten},
title = {SoK: Unifying Definitions of Privacy and Anonymity in Cryptocurrencies \&amp; DLTs},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3736656},
doi = {10.1145/3736656},
abstract = {As interest in the practical use of cryptocurrencies continues to grow, so does the focus on the (perceived) privacy and anonymity of users within this domain. Despite this attention, there is a notable absence of standardized definitions for these terms. This paper aims to address this gap by exploring the various interpretations of privacy, anonymity, and related concepts in the context of cryptocurrencies. Drawing from a thorough review of existing literature, we propose practical definitions for both privacy and anonymity. Utilizing these definitions, we introduce an ontology designed to streamline future research, identify knowledge gaps, and facilitate clearer communication in the field.},
note = {Just Accepted},
journal = {Distrib. Ledger Technol.},
month = jul,
keywords = {cryptocurrency, privacy, anonymity, ontology}
}

@article{10.14778/3746405.3746417,
author = {Cong, Tianji and Nargesian, Fatemeh and Xing, Junjie and Jagadish, H. V.},
title = {OpenForge: Probabilistic Metadata Integration},
year = {2025},
issue_date = {May 2025},
publisher = {VLDB Endowment},
volume = {18},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3746405.3746417},
doi = {10.14778/3746405.3746417},
abstract = {Modern data stores increasingly rely on metadata to enable diverse activities such as data cataloging and search. However, metadata curation remains a labor-intensive task, and the broader challenge of metadata maintenance—ensuring its consistency and usefulness—has been largely overlooked. In this work, we tackle the problem of resolving relationships among metadata concepts from disparate sources. Inferring these relationships are critical for creating clean and consistent metadata repositories, and a central challenge for metadata integration.We propose OpenForge, a two-stage prior-posterior framework for metadata integration. In the first stage, OpenForge exploits multiple methods including fine-tuned large language models to obtain prior beliefs about concept relationships. In the second stage, OpenForge refines these predictions using the Markov Random Field, a probabilistic graphical model. We formalize metadata integration as an optimization problem, where the objective is to identify the relationship assignments that maximize the joint probability of assignments. The MRF formulation allows OpenForge to capture prior beliefs while encoding critical relationship properties, such as transitivity, in probabilistic inference. Experiments on four datasets show the effectiveness and efficiency of OpenForge. In a use case of matching two metadata vocabularies, OpenForge outperforms GPT-4, the second-best method, by 25 F1 points.},
journal = {Proc. VLDB Endow.},
month = sep,
pages = {2914–2927},
numpages = {14}
}

@article{10.1145/3736787,
author = {Meshi, Avital and Wright, Adam},
title = {in(A)n(I)mate - AI-Mediated Conversations with Inanimate Objects},
year = {2025},
issue_date = {August 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {3},
url = {https://doi.org/10.1145/3736787},
doi = {10.1145/3736787},
abstract = {in(A)n(I)mate is an interactive AI-driven system that invites participants to speak with objects. The piece showcases an innovative use of GPT’s multimodal feature, through its ability to recognize objects in an image and generate responses in its style. Participants place an object of their choice in front of a black box and engage in conversation with it by pressing buttons, often unaware that GPT is generating the responses. in(A)n(I)mate provokes a discussion about human relationships with inanimate matter, and considers the role of non-human agents in mediating and animating objects.},
journal = {Proc. ACM Comput. Graph. Interact. Tech.},
month = jul,
articleno = {36},
numpages = {6},
keywords = {Artificial Intelligence, Creative AI, New Media Art, LLM, GPT, Interactive art}
}

@article{10.1145/3447772,
author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and D’amato, Claudia and Melo, Gerard De and Gutierrez, Claudio and Kirrane, Sabrina and Gayo, Jos\'{e} Emilio Labra and Navigli, Roberto and Neumaier, Sebastian and Ngomo, Axel-Cyrille Ngonga and Polleres, Axel and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
title = {Knowledge Graphs},
year = {2021},
issue_date = {May 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3447772},
doi = {10.1145/3447772},
abstract = {In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {71},
numpages = {37},
keywords = {shapes, rule mining, ontologies, graph query languages, graph neural networks, graph databases, graph algorithms, embeddings, Knowledge graphs}
}

@inproceedings{10.1145/3184558.3186575,
author = {Wisniewski, Dawid},
title = {Automatic Translation of Competency Questions into SPARQL-OWL Queries},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186575},
doi = {10.1145/3184558.3186575},
abstract = {The process of ontology authoring is inseparably connected with the quality assurance phase. One can verify the maturity and correctness of a given ontology by evaluating how many competency questions give correct answers. Competency questions are defined as a set of questions expressed in natural language that the finished ontology should be able to answer to correctly. Although this method can easily indicate what is the development status of an ontology, one has to translate competency questions from natural language into an ontology query language. This task is very hard and time consuming. To overcome this problem, my PhD thesis focuses on methods for automatically checking answerability of competency questions for a given ontology and proposing SPARQL-OWL query (OWL-aware SPARQL query) for each question where it is possible to create the query. Because the task of automatic translation from competency questions to SPARQL-OWL queries is a novel one, besides a method, we have proposed a new benchmark to evaluate such translation.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {855–859},
numpages = {5},
keywords = {SPARQL-OWL, competency question, ontology, word embedding},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3478431.3499295,
author = {Lopez, Jake and Ross, Monique and Garcia, Atalie and Uribe-Gosselin, Carolina},
title = {What is a Computer Scientist? Unpacking the Ontological Beliefs of Black and Hispanic Female Computing Students},
year = {2022},
isbn = {9781450390705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3478431.3499295},
doi = {10.1145/3478431.3499295},
abstract = {Underrepresentation of Black and Hispanic women in computer science is a long-standing problem that looks bleak at every level - undergraduate and graduate. This is prompting scholars to explore reasons for these low participation rates. One framework used to understand participation and persistence in STEM fields is identity. Prior work in computer science education suggest that identity is a strong indicator of persistence in these fields. However, it is hard to understand students' perception of identity without also understanding ontological beliefs with regards to a computer scientist. In this study, we explore the nature of a computer scientist. Guided by social identity theory, we designed a study that asked students to describe their definition or ontological belief of what constitutes a computer scientist in contrast to their ability to ascribe a computer science identity to self. Leveraging qualitative methods, we interviewedn = 24 women in computer science (Black and Hispanic, undergraduate and graduate students), in order to explore the role their ontological beliefs had on their computer science identity salience. The research questions guiding this work are: (1) How do Black and Hispanic women describe or define computer scientists? (2) What impact does this definition have on Black and Hispanic women's ability to claim a computing identity? Results suggest that the wide variation in definitions has a negative impact on computer science identity salience. The findings from this work suggest that computing should consider the impacts of the current messaging of what constitutes a computer scientist.},
booktitle = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education - Volume 1},
pages = {369–375},
numpages = {7},
keywords = {undergraduate curriculum, computing education, computer science education, broadening participation},
location = {Providence, RI, USA},
series = {SIGCSE 2022}
}

@article{10.1145/3564156,
author = {Alqahtani, Fatimah and Dohler, Mischa},
title = {Survey of Authorship Identification Tasks on Arabic Texts},
year = {2023},
issue_date = {April 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3564156},
doi = {10.1145/3564156},
abstract = {Authorship identification is the process of extracting and analysing the writing styles of authors to identify the authorship. From the writing style, the author and his/her different characteristics can be recognised, which is very useful in digital forensics and cyber investigations. In the literature, authorship identification tasks were addressed on both long and short documents and performed on different languages, such as English, Arabic, Chinese, and Greek. This survey has reviewed the authorship identification tasks for the Arabic language to contribute to this area of research by exploring Arabic language performance and challenges. A total of 27 prominent Arabic studies of each authorship identification domain were reviewed considering the used data, selected features, utilised methods, and results. After a review of the various studies, it was concluded that the results of authorship identification tasks vary based on mostly the selected features and used dataset. Furthermore, the effective features differ from one dataset to another based on the various types of the&nbsp;Arabic language. However, all authorship identification tasks involving the Arabic language face considerable challenges with data pre-processing due to the challenging Arabic concatenative morphology.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {93},
numpages = {24},
keywords = {stylometry, Arabic texts, authorship verification, authorship attribution, Authorship identification}
}

@article{10.14778/3415478.3415557,
author = {Quamar, Abdul and \"{O}zcan, Fatma and Miller, Dorian and Moore, Robert J and Niehus, Rebecca and Kreulen, Jeffrey},
title = {Conversational BI: an ontology-driven conversation system for business intelligence applications},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415557},
doi = {10.14778/3415478.3415557},
abstract = {Business intelligence (BI) applications play an important role in the enterprise to make critical business decisions. Conversational interfaces enable non-technical enterprise users to explore their data, democratizing access to data significantly. In this paper, we describe an ontology-based framework for creating a conversation system for BI applications termed as Conversational BI. We create an ontology from a business model underlying the BI application, and use this ontology to automatically generate various artifacts of the conversation system. These include the intents, entities, as well as the training samples for each intent. Our approach builds upon our earlier work, and exploits common BI access patterns to generate intents, their training examples and adapt the dialog structure to support typical BI operations. We have implemented our techniques in Health Insights (HI), an IBM Watson Healthcare offering, providing analysis over insurance data on claims. Our user study demonstrates that our system is quite intuitive for gaining business insights from data. We also show that our approach not only captures the analysis available in the fixed application dashboards, but also enables new queries and explorations.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {3369–3381},
numpages = {13}
}

@inproceedings{10.1145/3310273.3323436,
author = {Palumbo, Francesca and Fanni, Tiziana and Sau, Carlo and Pulina, Luca and Raffo, Luigi and Masin, Michael and Shindin, Evgeny and de Rojas, Pablo Sanchez and Desnos, Karol and Pelcat, Maxime and Rodr\'{\i}guez, Alfonso and Ju\'{a}rez, Eduardo and Regazzoni, Francesco and Meloni, Giuseppe and Zedda, Katiuscia and Myrhaug, Hans and Kaliciak, Leszek and Andriaanse, Joost and de Olivieria Filho, Julio and Mu\~{n}oz, Pablo and Toffetti, Antonella},
title = {CERBERO: Cross-layer modEl-based fRamework for multi-oBjective dEsign of reconfigurable systems in unceRtain hybRid envirOnments: Invited paper: CERBERO teams from UniSS, UniCA, IBM Research, TASE, INSA-Rennes, UPM, USI, Abinsula, AmbieSense, TNO, S&amp;T, CRF},
year = {2019},
isbn = {9781450366854},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3310273.3323436},
doi = {10.1145/3310273.3323436},
abstract = {Cyber-Physical Systems (CPS) are embedded computational collaborating devices, capable of sensing and controlling physical elements and, often, responding to humans. Designing and managing systems able to respond to different, concurrent requirements during operation is not straightforward, and introduce the need of proper support at design-time and run-time. The Cross-layer modEl-based fRamework for multi-oBjective dEsign of Reconfigurable systems in unceRtain hybRid envirOnments (CERBERO) EU project has developed a design environment for adaptive CPS. CERBERO approach leverages on model-based methodologies including different technologies and tools developed to cover design and operation from user interactions down to low level computing layer implementation.},
booktitle = {Proceedings of the 16th ACM International Conference on Computing Frontiers},
pages = {320–325},
numpages = {6},
keywords = {verification, self-adaptation, SW adaptivity, HW reconfiguration, HW adaptivity, CPS},
location = {Alghero, Italy},
series = {CF '19}
}

@inproceedings{10.1145/3708319.3734180,
author = {Perez-Martinez, Roberto and Casas-Ortiz, Alberto and Santos, Olga C.},
title = {MoRTELaban: a Neurosymbolic Framework for Motion Representation and Analysis based on Labanotation and Laban Movement Analysis},
year = {2025},
isbn = {9798400713996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708319.3734180},
doi = {10.1145/3708319.3734180},
abstract = {Human motion cannot be fully modeled by subsymbolic representations. While these extract precise hidden patterns in motion data, they are often task-specific and lack a semantic understatement of motion. Symbolic systems that mirror human cognition and explicit expressive processes are necessary for richer motion synthesis and analysis, enabling physical reasoning and expert knowledge encoding. In this work, we propose a neurosymbolic framework that combines Labanotation and Laban Movement Analysis (LMA), originally developed for dance, to represent and analyze human motion symbolically. We expand the existing LabanEditor to support full-body annotation and integrate it with AMASS, Mediapipe, and Kinect inputs through a SMPL-based format. Our system supports automatic annotation for the local functional and expressive aspects of motion, and enables bidirectional conversion between symbols and motion. While still a work in progress, this framework lays the groundwork for explainable, expressive motion modeling that can support human-robot interaction, motion preservation, and psychomotor learning systems.},
booktitle = {Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {353–359},
numpages = {7},
keywords = {motion modeling, movement modeling, knowledge representation, Labanotation, LMA, expert systems},
location = {
},
series = {UMAP Adjunct '25}
}

@inproceedings{10.5555/3320516.3320631,
author = {Wagner, Gerd and Nardin, Luis G.},
title = {Adding agent concepts to object event modeling and simulation},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {Object Event Modeling and Simulation (OEM&amp;S) is a general Discrete Event Simulation paradigm combining object-oriented modeling with the event scheduling paradigm. We show how to extend OEM&amp;S by adding concepts of agent-based modeling and simulation, resulting in a framework that we call Agent/Object Event Modeling and Simulation (A/OEM&amp;S). The main point for such an extension is to define agents as special objects, which are subject to general (physical) laws of causality captured in the form of event rules, and which have their own behavior allowing them to interact with their inanimate environment and with each other. Because agent behavior is decoupled from physical causality, an A/OE simulator consists of an environment simulator, which simulates the physical world (the objective states of material objects), and agent simulators, which simulate the internal (subjective) states of agents and their behaviors.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {893–904},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/3550356.3561599,
author = {Balaban, Mira and Khitron, Igal and Maraee, Azzam and Kifer, Michael},
title = {Mediation-based MLM in FOModeLer},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561599},
doi = {10.1145/3550356.3561599},
abstract = {MLM has attracted much attention over the last two decades. MLM activities include philosophical discussions about ontologies, requirements and relevant services, and development of theories, languages, and tools. Approaches differ in their support for MLM concepts on the levels of syntax, semantics and pragmatics.The Mediation-based MLM (MedMLM), is a formal theory that defines a multilevel model as an ordered collection of levels that are inter-related by mediators, and can be enriched by inter-level relationships and interactions. The levels of MedMLM are plain class models, and the mediators define inter-level instantiation relations. MedMLM is unique in supporting a modular architecture of levels and mediators.This paper introduces the MedMLM software modeling tool, that is built on top of the FOModeLer class modeling tool. The tool supports MLM construction, querying and reasoning, meta-reasoning, validation, syntax verification, and plain computation. We also compare the MedMLM tool with older MLM approaches using semantic, syntactic, and pragmatic MLM criteria.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {444–452},
numpages = {9},
keywords = {multi-level modeling, executable logic, MLM semantics},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@article{10.1145/3677057,
author = {Hougaard, Bastian Ils\o{} and Knoche, Hendrik},
title = {Aiming, Pointing, Steering: A Core Task Analysis Framework for Gameplay},
year = {2024},
issue_date = {October 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3677057},
doi = {10.1145/3677057},
abstract = {Underneath their compelling audiovisual surface, games require players to carry out mundane interaction work, such as pointing, typing, or steering. However, many of these underlying building blocks are not defined rigorously, hampering synthesis and analysis. We elaborate on the origin of tasks within human-computer interaction (HCI) and define tasks' relationship to game terminology (game mechanics, goals, and actions). Our proposed framework draws on systemic-structural theory of activity to aid systematic analysis and exploration of game design by mapping gameplay to abstract core tasks. The framework contains four task tools, applicable when 1) uncovering design properties, 2) designing experimental manipulation, 3) creating behavioral measurements, and 4) describing gameplay in literature reviews of game genres and design techniques. We evaluated our framework as a lens to design purposeful games in three case studies within a scientific education. We invite researchers and practitioners to employ the framework as a microscope, to describe and design games rigorously.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = oct,
articleno = {292},
numpages = {48},
keywords = {abstraction, action, activity theory, core task, design landscape, feedback, game design, gameplay, imperative goals, mechanics, ontology, task analysis, task definition}
}

@inproceedings{10.1145/3511616.3513115,
author = {Thapa, Nischay Bikram and Seifollahi, Sattar and Taheri, Sona},
title = {Hospital Readmission Prediction Using Clinical Admission Notes},
year = {2022},
isbn = {9781450396066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511616.3513115},
doi = {10.1145/3511616.3513115},
abstract = {Clinical notes contain contextualised information beyond structured data relating to patients’ past and current health conditions. Despite the richness, their unstructured, long, and high dimensional nature presents challenges to traditional text representation techniques. The advancement of deep contextual representation techniques in natural language processing (NLP) has shown remarkable performance in the biomedical and clinical domains for various information extraction and predictive tasks, including hospital readmission. However, most previous works have proposed discharge summary models where on-site medical intervention is impossible, and readmission could still occur. This paper utilises clinical notes recorded during admissions to study the risk of 30-day hospital readmissions. We employ clinical notes from MIMIC-III and consider competing baselines for clinical text representation, where a set of machine learning and deep learning algorithms are used to classify hospital readmission. The study demonstrates that notes captured during admissions play a crucial role to recognise potential readmission risk supporting healthcare practitioners for practical therapeutic intervention and discharge planning.},
booktitle = {Proceedings of the 2022 Australasian Computer Science Week},
pages = {193–199},
numpages = {7},
keywords = {Natural language processing, Hospital readmission, Embedding techniques, Electronic health records},
location = {Brisbane, Australia},
series = {ACSW '22}
}

@inproceedings{10.1145/3318464.3380589,
author = {Weir, Nathaniel and Utama, Prasetya and Galakatos, Alex and Crotty, Andrew and Ilkhechi, Amir and Ramaswamy, Shekar and Bhushan, Rohin and Geisler, Nadja and H\"{a}ttasch, Benjamin and Eger, Steffen and Cetintemel, Ugur and Binnig, Carsten},
title = {DBPal: A Fully Pluggable NL2SQL Training Pipeline},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3380589},
doi = {10.1145/3318464.3380589},
abstract = {Natural language is a promising alternative interface to DBMSs because it enables non-technical users to formulate complex questions in a more concise manner than SQL. Recently, deep learning has gained traction for translating natural language to SQL, since similar ideas have been successful in the related domain of machine translation. However, the core problem with existing deep learning approaches is that they require an enormous amount of training data in order to provide accurate translations. This training data is extremely expensive to curate, since it generally requires humans to manually annotate natural language examples with the corresponding SQL queries (or vice versa). Based on these observations, we propose DBPal, a new approach that augments existing deep learning techniques in order to improve the performance of models for natural language to SQL translation. More specifically, we present a novel training pipeline that automatically generates synthetic training data in order to (1) improve overall translation accuracy, (2) increase robustness to linguistic variation, and (3) specialize the model for the target database. As we show, our DBPal training pipeline is able to improve both the accuracy and linguistic robustness of state-of-the-art natural language to SQL translation models.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {2347–2361},
numpages = {15},
keywords = {natural language to SQL, natural language interface to database, NLIDB, NL2SQL},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@article{10.1145/3653317,
author = {Masmoudi, Maroua and Ben Abdallah Ben Lamine, Sana and Karray, Mohamed Hedi and Archimede, Bernard and Baazaoui Zghal, Hajer},
title = {Semantic Data Integration and Querying: A Survey and Challenges},
year = {2024},
issue_date = {August 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3653317},
doi = {10.1145/3653317},
abstract = {Digital revolution produces massive, heterogeneous and isolated data. These latter remain underutilized, unsuitable for integrated querying and knowledge discovering. Hence the importance of this survey on data integration which identifies challenging issues and trends. First, an overview of the different generations and basics of data integration is given. Then, semantic data integration is focused, since it semantically links data allowing wider insights and decision-making. More than thirty works are reviewed. The goal is to help analysts to identify relevant criteria to compare then choose among semantic data integration approaches, focusing on the category (materialized, virtual or hybrid) and querying techniques.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {209},
numpages = {35},
keywords = {Data integration, ontology, query processing, ETL, OBDA, semantic mapping}
}

@inproceedings{10.1145/3534678.3539046,
author = {Ma, Yiming},
title = {CS-RAD: Conditional Member Status Refinement and Ability Discovery for Social Network Applications},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539046},
doi = {10.1145/3534678.3539046},
abstract = {In a social network environment, member status represents a member's social value in the network. A member's abilities represent the potential of a member projecting his/her social values to others, and also represent the level of credibility and authority for a member to hold certain status. Therefore, the concepts of status and ability are deeply related, and should be consistent with each other. In this paper, we establish the consistency models among different member status and their abilities through analyzing member data and integrating domain knowledge. We use these models to help our members refine their inconsistent status, at the same time, identify ability gaps. To reliably refine a member status, we introduce a practical and human-in-the-loop methodology to build status hierarchy. Conditioned on the hierarchical structure, our modeling process exploits the associations between status and abilities. We applied the technique to LinkedIn member titles -- one of the major types of the member status, and member skills -- the main ability representations at LinkedIn. We showed that our models are intuitive and perform well. The skill gaps identified are actionable and concise. In this paper, we also discuss the aspects of building such systems, and how we could deploy the models in production.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3486–3494},
numpages = {9},
keywords = {knowledge representation, ontology, social networks, statistical modeling, taxonomy, user modeling},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.5555/3320516.3320611,
author = {Ruscheinski, Andreas and Budde, Kai and Warnke, Tom and Wilsdorf, Pia and Hiller, Bjarne C. and Dombrowsky, Marcus and Uhrmacher, Adelinde M.},
title = {Generating simulation experiments based on model documentations and templates},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {An increasing number of approaches for specifying and executing simulation experiments emphasizes the desire to make this part of modeling and simulation studies explicit, and thus, also easier to replicate. We take this one step further by automatically generating simulation experiment specifications from documentations. Based on a template-based approach and documentations, we show how simulation experiment specifications can be generated and executed for experiments, such as statistical model checking and sensitivity analysis, and we identify crucial challenges.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {715–726},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/3534678.3539187,
author = {Srivastava, Aseem and Suresh, Tharun and Lord, Sarah P. and Akhtar, Md Shad and Chakraborty, Tanmoy},
title = {Counseling Summarization Using Mental Health Knowledge Guided Utterance Filtering},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3539187},
doi = {10.1145/3534678.3539187},
abstract = {The psychotherapy intervention technique is a multifaceted conversation between a therapist and a patient. Unlike general clinical discussions, psychotherapy's core components (viz. symptoms) are hard to distinguish, thus becoming a complex problem to summarize later. A structured counseling conversation may contain discussions about symptoms, history of mental health issues, or the discovery of the patient's behavior. It may also contain discussion filler words irrelevant to a clinical summary. We refer to these elements of structured psychotherapy as counseling components. In this paper, the aim is mental health counseling summarization to build upon domain knowledge and to help clinicians quickly glean meaning. We create a new dataset after annotating 12.9K utterances of counseling components and reference summaries for each dialogue. Further, we propose ConSum, a novel counseling-component guided summarization model. ConSum undergoes three independent modules. First, to assess the presence of depressive symptoms, it filters utterances utilizing the Patient Health Questionnaire (PHQ-9), while the second and third modules aim to classify counseling components. At last, we propose a problem-specific Mental Health Information Capture (MHIC) evaluation metric for counseling summaries. Our comparative study shows that we improve on performance and generate cohesive, semantic, and coherent summaries. We comprehensively analyze the generated summaries to investigate the capturing of psychotherapy elements. Human and clinical evaluations on the summary show that ConSum generates quality summary. Further, mental health experts validate the clinical acceptability of the ConSum. Lastly, we discuss the uniqueness in mental health counseling summarization in the real world and show evidences of its deployment on an online application with the support of mpathic.ai},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {3920–3930},
numpages = {11},
keywords = {dialogue summarization, natural language processing},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3184558.3186906,
author = {Yen, Ting-Yu and Lee, Yang-Yin and Huang, Hen-Hsen and Chen, Hsin-Hsi},
title = {That Makes Sense: Joint Sense Retrofitting from Contextual and Ontological Information},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3186906},
doi = {10.1145/3184558.3186906},
abstract = {While recent word embedding models demonstrate their abilities to capture syntactic and semantic information, the demand for sense level embedding is getting higher. In this study, we propose a novel joint sense embedding learning model that retrofits the word representation into sense representation from contextual and ontological information. The experiment shows the effectiveness and robustness of our model that outperforms previous approaches in four public available benchmark datasets.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {15–16},
numpages = {2},
keywords = {joint sense retrofitting, semantic relatedness, sense embedding},
location = {Lyon, France},
series = {WWW '18}
}

@article{10.14778/3415478.3415512,
author = {Buron, Maxime and Goasdou\'{e}, Fran\c{c}ois and Manolescu, Ioana and Mugnier, Marie-Laure},
title = {Obi-Wan: ontology-based RDF integration of heterogeneous data},
year = {2020},
issue_date = {August 2020},
publisher = {VLDB Endowment},
volume = {13},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3415478.3415512},
doi = {10.14778/3415478.3415512},
abstract = {We consider the problem of integrating heterogeneous data (relational, JSON, key-values, graphs etc.) and querying it efficiently. Traditional data integration systems fall into two classes: data warehousing, where all data source content is materialized in a single repository, and mediation, where data remains in their original stores and all data can be queried through a mediator.We propose to demonstrate Obi-Wan, a novel mediator following the Ontology-Based Data access (OBDA) paradigm. Obi-Wan integrates data sources of many data models under an interface based on RDF graphs and ontologies (classes, properties, and relations between them). The novelty of Obi-Wan is to combine maximum integration power (GLAV mappings, see below) with the highest query answering power supported by an RDF mediator: RDF queries not only over the data but also over the integration ontologies. This makes it more flexible and powerful than comparable systems.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2933–2936},
numpages = {4}
}

@inproceedings{10.1145/3543507.3583238,
author = {Yang, Yuting and Lei, Wenqiang and Huang, Pei and Cao, Juan and Li, Jintao and Chua, Tat-Seng},
title = {A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583238},
doi = {10.1145/3543507.3583238},
abstract = {Dialogue State Tracking (DST) module is an essential component of task-oriented dialog systems to understand users’ goals and needs. Collecting dialogue state labels including slots and values can be costly, requiring experts to annotate all (slot, value) information for each turn in dialogues. It is also difficult to define all possible slots and values in advance, especially with the wide application of dialogue systems in more and more new-rising applications. In this paper, we focus on improving DST module to generate dialogue states in circumstances with limited annotations and knowledge about slot ontology. To this end, we design a dual prompt learning framework for few-shot DST. The dual framework aims to explore how to utilize the language understanding and generation capabilities of pre-trained language models for DST efficiently. Specifically, we consider the learning of slot generation and value generation as dual tasks, and two kinds of prompts are designed based on this dual structure to incorporate task-related knowledge of these two tasks respectively. In this way, the DST task can be formulated as a language modeling task efficiently under few-shot settings. To evaluate the proposed framework, we conduct experiments on two task-oriented dialogue datasets. The results demonstrate that the proposed method not only outperforms existing state-of-the-art few-shot methods, but also can generate unseen slots. It indicates that DST-related knowledge can be probed from pre-trained language models and utilized to address low-resource DST efficiently with the help of prompt learning.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1468–1477},
numpages = {10},
keywords = {dialogue state tracking, few-shot learning, prompt learning},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3535511.3535513,
author = {Ferreira, Fl\'{a}vio and Duarte, Julio and Ugulino, Wallace},
title = {Automated Statistics Extraction of Public Security Events Reported Through Microtexts on Social Networks},
year = {2022},
isbn = {9781450396981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3535511.3535513},
doi = {10.1145/3535511.3535513},
abstract = {Lately, Rio de Janeiro State has been characterized by the occurrence of successive public security events (shootings, assaults, robberies, etc.), causing great insecurity, affecting the daily lives of the population, and worrying public security agencies in the fight against crime. Although the indicators of public security events recently decreased, there is still a feeling of insecurity, while the population uses social networks to notify illegal acts that occurred in their vicinity. Although this collaboration is limited to the crimes that occurred, many published messages are difficult to interpret. Knowledge Discovery is a process of extracting data in an implicit, previously unknown, and useful way that can be applied for different purposes. In this context, Natural Language Processing is a powerful tool that allows the extraction of information from these unstructured data. This work proposes a methodology for automatic knowledge extraction, in the form of statistics related to public security events posted on social networks, particularly the ones occurred in Rio de Janeiro. The main contribution of this work is the proposal of a methodology for the construction of an Information System that allows the collection of statistics of notified public security events. In addition to this methodology, which can also be used in the construction of other Information Systems, this work contributes with a public security event recognition model that has a performance of 95\%, and an available dataset that can be used to support other researches, such as: the identification of new behavior patterns, the discovery of hidden knowledge, among other fronts.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Information Systems},
articleno = {2},
numpages = {7},
keywords = {Twitter., Text Mining, Text Classification, Public Security, Natural Language Processing, Machine Learning, Data Mining, Artificial Intelligence},
location = {Curitiba, Brazil},
series = {SBSI '22}
}

@inproceedings{10.1145/3654522.3654568,
author = {Nguyen, Anh Quynh and Tran, My Tu and Nguyen, Quang Nhat and Huynh, Huy Khai and Le, Lan Thi Thu and Quach, Luyl-Da},
title = {Classification of Rice Plant Disease Based on Descriptive Information with DistilBERT's Architecture},
year = {2024},
isbn = {9798400716713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3654522.3654568},
doi = {10.1145/3654522.3654568},
abstract = {Rice has a significant role in human life, and currently, the issue of rice plant diseases is receiving attention in image-related data processing. However, the possibility of applying text classification to address this issue has yet to be explored. Nonetheless, it deserves attention due to the complexity of image-related data. The study gathered descriptive passages on four prevalent rice diseases in Vietnam, with 365 descriptions. The collected data underwent data preprocessing through stopword removal, then visualization to identify crucial words and phrases for disease identification in rice plants. The study used feature extraction and fine-tuning based on DistilBERT's architecture to build models. The research findings showed an impressive peak accuracy rate of 87\%, highlighting the potential of using text classification algorithms to classify diseases in crops using descriptions.},
booktitle = {Proceedings of the 2024 9th International Conference on Intelligent Information Technology},
pages = {155–163},
numpages = {9},
keywords = {DistilBERT, Large Language model,, Rice Disease, Text Classification},
location = {Ho Chi Minh City, Vietnam},
series = {ICIIT '24}
}

@inproceedings{10.1145/3647444.3647871,
author = {Pandita, Karan and Thakur, Purab Kulranjan Singh and Annamalai, Suresh},
title = {Contextual transcription and Summarization of audio using AI},
year = {2024},
isbn = {9798400709418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3647444.3647871},
doi = {10.1145/3647444.3647871},
abstract = {The field of Natural Language Processing (NLP) has revolutionized the way human language interacts with computer systems. NLP applications span machine translation, information extraction, summarization, and question answering, driven by vast computational resources and big data methodologies. Despite these advancements, NLP tools haven't fully integrated with Internet of Things (IoT) devices, like audio recorders, hindering their accessibility and usability. This paper introduces an innovative solution: a method for audio transcription and contextual summarization using NLP, addressing this gap and enhancing comprehension. Our approach employs cutting-edge NLP techniques, including word embedding methods and knowledge-based graphs, to create a system that efficiently converts audio content into written text and generates coherent summaries. Unlike existing AI tools, our system's summaries are not only accurate but also rich and deep, providing insightful representations of the original content. This depth is achieved through advanced linguistic analysis, surpassing tools like ChatGPT. Furthermore, our system breaks language barriers, enabling multilingual data traversal, enhancing accessibility on a global scale. Our research methodology ensures the system's adherence to industry standards like Request for Comments (RFC) and Constrained Application Protocol (CoAP), guaranteeing interoperability and reliability. By incorporating knowledge-based graphs, our system comprehensively understands audio content, enhancing the accuracy of summarization. This approach addresses the unmet need for seamlessly integrating NLP with IoT devices, making the technology accessible to a broader audience.},
booktitle = {Proceedings of the 5th International Conference on Information Management \&amp; Machine Intelligence},
articleno = {45},
numpages = {9},
keywords = {Constrained Application Protocol (CoAP), IOT, NLP, Request for Comments (RFC), audio transcription, contextual summarization, data summarization, knowledge based graphs, word embedding methods},
location = {Jaipur, India},
series = {ICIMMI '23}
}

@inproceedings{10.1145/3372020.3391564,
author = {Chondamrongkul, Nacha and Sun, Jing and Warren, Ian and Lee, Scott Uk-Jin},
title = {Semantic-based Architecture Smell Analysis},
year = {2020},
isbn = {9781450370714},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372020.3391564},
doi = {10.1145/3372020.3391564},
abstract = {Software smells have negative impacts on the reliability and modifiability of software systems. The smells in architecture design can be cascaded down to the implementation level and cause issues that require much effort to fix. Therefore, early detection of the architecture smells can benefit the overall quality of the software system. This paper presents an integration of methods that formally define the software architecture design towards architecture smell detection. Our approach serves as a framework that allows the architectural structures and behaviours to be formally analysed based on a coherent technique. We evaluated the accuracy and performance of our approach with the models generated from open source projects. The results show that our approach is effective and functions well.},
booktitle = {Proceedings of the 8th International Conference on Formal Methods in Software Engineering},
pages = {109–118},
numpages = {10},
keywords = {Software Architecture, Smell Detection, Ontology Web Language, Model Checking, Architecture Smells},
location = {Seoul, Republic of Korea},
series = {FormaliSE '20}
}

@inproceedings{10.1145/3308558.3313476,
author = {Chen, Huiyuan and Li, Jing},
title = {Modeling Relational Drug-Target-Disease Interactions via Tensor Factorization with Multiple Web Sources},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313476},
doi = {10.1145/3308558.3313476},
abstract = {Modeling the behaviors of drug-target-disease interactions is crucial in the early stage of drug discovery and holds great promise for precision medicine and personalized treatments. The growing availability of new types of data on the internet brings great opportunity of learning a more comprehensive relationship among drugs, targets, and diseases. However, existing methods often consider drug-target interactions or drug-disease interactions separately, which ignores the dependencies among these three entities. Also, many of them cannot directly incorporate rich heterogeneous information from diverse sources. In this work, we investigate the utility of tensor factorization to model the relationships of drug-target-disease, specifically leveraging different types of online data. Our motivation is two-fold. First, in human metabolic systems, many drugs interact with protein targets in cells to modulate target activities, which in turn alter biological pathways to promote healthy functions and to treat diseases. Instead of binary relationships of &lt;drug, disease&gt; or &lt;drug, target&gt;, a tighter triple relationships &lt;drug, target, disease&gt; should be exploited to better understand drug mechanism of actions (MoAs). Second, medical data could be collected from different sources (i.e., drug's chemical structure, target's sequence, or expression measurements). Therefore, effectively exploiting the complementarity among multiple sources is of great importance. Our method elegantly explores a &lt;drug, target, disease&gt; tensor together with complementarity among different data sources, thus improves prediction accuracy. We achieve this goal by formulating the problem into a coupled tensor-matrix factorization problem and directly optimize it on the nonlinear manifold. Experimental results on real-world datasets show that the proposed model outperforms several competitive methods. Our model opens up opportunities to use large Web data to predict drugs' MoAs in pharmacological studies.},
booktitle = {The World Wide Web Conference},
pages = {218–227},
numpages = {10},
keywords = {Tensor factorization, Multi-view learning, Manifold optimization, Grassmann manifold, Drug discovery, Disease analysis;},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3323503.3360638,
author = {Rolim, Tulio Vidal and Vidal, V\^{a}nia Maria Ponte and Avila, Caio Viktor S. and Cruz, Matheus Mayron Lima da and Barrio, Matheus and Queiroz, Daniel},
title = {SemanticSefaz: an ontology-based semantic portal for the government spending},
year = {2019},
isbn = {9781450367639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3323503.3360638},
doi = {10.1145/3323503.3360638},
abstract = {Supervision in the public procurement process is considered essential for society as a means of promoting greater security and control against possible fraud and illegal actions. However, the data available on government procurement alone does not allow for the identification of possible signed contracts or bidding processes won by unfit or suspended companies, making it difficult to analyze and supervise by employees of tax agencies such as SEFAZ. In addition, data are often not available in the same common format and differ in their vocabulary, making it difficult for these professionals to find interesting information. As a means of solving these problems, the present work presents SemanticSefaz, a semantic portal for integration between heterogeneous bases focused on the domain of public procurement through a homogeneous view, allowing for semantic queries and subsequent discovery of information that priori were not possible. As a case study, the databases with data on government procurement (SIASG), unhealthy and suspenseful companies (CEIS) and punished companies (CNEP) were used to construct semantic integration. Subsequently, queries of interest to the tax domain were conducted through SemanticSefaz, demonstrating its efficiency for performing faceted queries and semantic navigation. In the end, SemanticSefaz is characterized as a timely tool for integration, visualization, discovery of knowledge to facilitate the work of tax professionals.},
booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
pages = {493–496},
numpages = {4},
keywords = {semantic web, linked data, government purchasing},
location = {Rio de Janeiro, Brazil},
series = {WebMedia '19}
}

@inproceedings{10.1145/3312714.3312721,
author = {Wu, Eureeka Haishang and Wu, Raymond},
title = {Collaborative Model of Emerging Technologies in Asia Pacific},
year = {2019},
isbn = {9781450362351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3312714.3312721},
doi = {10.1145/3312714.3312721},
abstract = {Emerging technologies became pervasive in this decade due to its significant growth in terms of high-volume transactions and sophisticated businesses. According to Gartner, public cloud in Asia Pacific will continue its high growth and hit a minimum of $11.5 billion by end of 2018. However the challenge can be arise from the other side as only those enterprises who understand agility and collaboration can be the winner. Market power for a single firm is very low in competitive market furthermore, to survive in uprising competition; companies need to differentiate themselves by customizing their products and services, and to quickly adapt themselves into a common platform in the region.To envision the roadmap for regional prosperity, a three-step approach was proposed, to align culture, technologies, and economy into a transformation process, and eventually to achieve a common model of regional collaboration.},
booktitle = {Proceedings of the 5th International Conference on E-Society, e-Learning and e-Technologies},
pages = {73–77},
numpages = {5},
keywords = {Innovation, Governance, Emerging Technologies, Consolidation, Collaboration},
location = {Vienna, Austria},
series = {ICSLT '19}
}

@inproceedings{10.1145/3637528.3671793,
author = {Gyurek, Croix and Talukder, Niloy and Hasan, Mohammad Al},
title = {Binder: Hierarchical Concept Representation through Order Embedding of Binary Vectors},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671793},
doi = {10.1145/3637528.3671793},
abstract = {For natural language understanding and generation, embedding concepts using an order-based representation is an essential task. Unlike traditional point vector based representation, an order-based representation imposes geometric constraints on the representation vectors for explicitly capturing various semantic relationships that may exist between a pair of concepts. In existing literature, several approaches on order-based embedding have been proposed, mostly focusing on capturing hierarchical relationships; examples include vectors in Euclidean space, complex, Hyperbolic, order, and Box Embedding. Box embedding creates region-based rich representation of concepts, but along the process it sacrifices simplicity, requiring a custom-made optimization scheme for learning the representation. Hyperbolic embedding improves embedding quality by exploiting the ever-expanding property of Hyperbolic space, but it also suffers from the same fate as box embedding as gradient descent like optimization is not simple in the Hyperbolic space. In this work, we propose Binder, a novel approach for order-based representation. Binder uses binary vectors for embedding, so the embedding vectors are compact with an order of magnitude smaller footprint than other methods. Binder uses a simple and efficient optimization scheme for learning representation vectors with a linear time complexity. Our comprehensive experimental results show that Binder is very accurate, yielding competitive results on the representation task. But Binder stands out from its competitors on the transitive closure link prediction task as it can learn concept embeddings just from the direct edges, whereas all existing order-based approaches rely on the indirect edges. In particular, Binder achieves a whopping 70\% higher F1-score than the second best method (98.6\% vs 29\%) in our largest dataset, WordNet Nouns (743,241 edges), when using only direct edges during training.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {980–991},
numpages = {12},
keywords = {binary vector embedding, concept graph, hierarchical embedding, order embedding},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{10.1145/3544549.3582750,
author = {Henriques, Ana O. and Rafael, S\'{o}nia and Almeida, Victor M and Pinto, Jos\'{e} Gomes},
title = {The problem with gender-blind design and how we might begin to address it: A model for intersectional feminist ethical deliberation},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3582750},
doi = {10.1145/3544549.3582750},
abstract = {Gender-blind design hinges upon an assumption that designing equally is the same as designing for equality. That, however, is inaccurate, as gender-blindness is merely a synonym for neutrality. Neutrality, because it lacks a concerted effort to subvert, favors hegemonic values and epistemologies, which counters the purported aim of equality. Supposedly objective methods of analysis, such as data gathering and interpreting, are not deprived of this hegemonic bias either. As such, through an acknowledgment of ethics, the designer must recognize that they are, indeed, imbuing their values into their designs, which bears influence on the ways in which the user interacts and interprets those designs, a notion which is especially relevant to a field concerned with user experience. This may be done deliberately or by accident, but it is always inevitable. Ethics is, in this way, inextricable from the design process, and, thus, the present article aims to propose that designing for equality requires the designer to act as an ethical agent — responsibly, consciously, and knowingly — especially if one hopes to avoid a design which embodies and communicates oppressive notions. In particular, within the purview of ethics, and by making use of some case-studies and examples, it argues that designing toward gender equality requires not the more typical gender-blind approach, but rather one which is specifically gender-conscious. Further, this article also offers some suggestions as to how we might begin to act as ethical design agents and implement marginalized epistemologies into the design process.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {423},
numpages = {12},
keywords = {Conceptual Model, Ethics, Feminist Design, Gender-Blind Design},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{10.1145/3486607.3486771,
author = {Steimann, Friedrich},
title = {The kingdoms of objects and values},
year = {2021},
isbn = {9781450391108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486607.3486771},
doi = {10.1145/3486607.3486771},
abstract = {THE purpose of the following paper is to consider whether there is a fundamental division of the [data] with which [programming] is concerned into two classes, [objects] and [values], or whether there is any method of overcoming this dualism. My own opinion is that the dualism is ultimate; on the other hand, many [colleagues] with whom, in the main, I am in close agreement, hold that it is not ultimate.  (paraphrased after Bertrand Russell)},
booktitle = {Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {125–135},
numpages = {11},
keywords = {values, universals, programming languages, particulars, ontology of computing, object, metaphysics},
location = {Chicago, IL, USA},
series = {Onward! 2021}
}

@article{10.1109/TASLP.2022.3224286,
author = {Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
title = {Minimising Biasing Word Errors for Contextual ASR With the Tree-Constrained Pointer Generator},
year = {2022},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2022.3224286},
doi = {10.1109/TASLP.2022.3224286},
abstract = {Contextual knowledge is essential for reducing speech recognition errors on high-valued long-tail words. This paper proposes a novel tree-constrained pointer generator (TCPGen) component that enables end-to-end ASR models to bias towards a list of long-tail words obtained using external contextual information. With only a small overhead in memory use and computation cost, TCPGen can structure thousands of biasing words efficiently into a symbolic prefix-tree, and creates a neural shortcut between the tree and the final ASR output to facilitate the recognition of the biasing words. To enhance TCPGen, we further propose a novel minimum biasing word error (MBWE) loss that directly optimises biasing word errors during training, along with a biasing-word-driven language model discounting (BLMD) method during the test. All contextual ASR systems were evaluated on the public Librispeech audiobook corpus and the data from the dialogue state tracking challenges (DSTC) with the biasing lists extracted from the dialogue-system ontology. Consistent word error rate (WER) reductions were achieved with TCPGen, which were particularly significant on the biasing words with around 40% relative reductions in the recognition error rates. MBWE and BLMD further improved the effectiveness of TCPGen, and achieved more significant WER reductions on the biasing words. TCPGen also achieved zero-shot learning of words not in the audio training set with large WER reductions on the out-of-vocabulary words in the biasing list.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = nov,
pages = {345–354},
numpages = {10}
}

@inproceedings{10.1145/3282373.3282400,
author = {Shaaban, Abdelkader Magdy and Schmittner, Christoph and Gruber, Thomas and Mohamed, A. Baith and Quirchmayr, Gerald and Schikuta, Erich},
title = {CloudWoT - A Reference Model for Knowledge-based IoT Solutions},
year = {2018},
isbn = {9781450364799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3282373.3282400},
doi = {10.1145/3282373.3282400},
abstract = {Internet technology has changed how people work, live, communicate, learn and entertain. The internet adoption is rising rapidly, thus creating a new industrial revolution named "Industry 4.0". Industry 4.0 is the use of automation and data transfer in manufacturing technologies. It fosters several technological concepts, one of these is the Internet of Things (IoT). IoT technology is based on a big network of machines, objects, or people called "things" interacting together to achieve a common goal. These things are continuously generating vast amounts of data. Data understanding, processing, securing and storing are significant challenges in the IoT technology which restricts its development. This paper presents a new reference IoT model for future smart IoT solutions called Cloud Web of Things (CloudWoT). CloudWoT aims to overcome these limitations by combining IoT with edge computing, semantic web, and cloud computing. Additionally, this work is concerned with the security issues which threatens data in IoT application domains.},
booktitle = {Proceedings of the 20th International Conference on Information Integration and Web-Based Applications \&amp; Services},
pages = {272–281},
numpages = {10},
keywords = {Semantic Web, IoT, IACS, Edge Computing, CloudWoT, Cloud Computing, CPPS},
location = {Yogyakarta, Indonesia},
series = {iiWAS2018}
}

@inproceedings{10.1145/3564746.3587001,
author = {Adatrao, Naga Sai Krishna and Gadireddy, Gowtham Reddy and Noh, Jiho},
title = {A Survey on Conversational Search and Applications in Biomedicine},
year = {2023},
isbn = {9781450399210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3564746.3587001},
doi = {10.1145/3564746.3587001},
abstract = {This paper aims to provide a radical rundown on Conversational Search (ConvSearch), an approach to enhance the information retrieval (IR) method where users engage in a dialogue for the information-seeking tasks. In this survey, we predominantly focused on the human interactive characteristics of the ConvSearch systems, highlighting the operations of the action modules, likely the retrieval system, question-answering, and recommender system. We labeled various ConvSearch research problems in knowledge bases, natural language processing, and dialogue management systems with action modules. We further categorized the framework to ConvSearch, and the application is directed toward biomedical and healthcare fields for the utilization of clinical social technology. Finally, we conclude by talking through the challenges and issues of ConvSearch, particularly in Bio-Medicine. Our main aim is to provide an integrated and unified vision of the ConvSearch components from different fields, which benefit the information-seeking process in healthcare systems.},
booktitle = {Proceedings of the 2023 ACM Southeast Conference},
pages = {78–88},
numpages = {11},
keywords = {privacy concerns, biomedical convsearch, generative language models, recommender systems, dialogue management systems, knowledge base, question answering, conversational search, information retrieval},
location = {Virtual Event, USA},
series = {ACMSE '23}
}

@proceedings{10.1145/3719160,
title = {CUI '25: Proceedings of the 7th ACM Conference on Conversational User Interfaces},
year = {2025},
isbn = {9798400715273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1109/JCDL57899.2023.00063,
author = {Powell, James and Balakireva, Lyudmila},
title = {Measuring the Growth of Ideas in a Title Corpus},
year = {2024},
isbn = {9798350399318},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL57899.2023.00063},
doi = {10.1109/JCDL57899.2023.00063},
abstract = {Beyond bibliometrics, there is interest in characterizing the evolution of the number of ideas in scientific papers, or more generally, exploring the progress of science. We use various tokenization and phrase extraction strategies combined with lexical diversity metrics to analyze titles in our corpus. We compared four lexical diversity metrics for each corpora variants, to look for indications that new concepts might be emerging over time.},
booktitle = {Proceedings of the 2023 ACM/IEEE Joint Conference on Digital Libraries},
pages = {291–292},
numpages = {2},
keywords = {lexical diversity, natural language processing, word embeddings, science of science},
location = {Santa Fe, New Mexico, USA},
series = {JCDL '23}
}

@inproceedings{10.1145/3442442.3451381,
author = {Mansar, Youness and Kang, Juyeon and Maarouf, Ismail El},
title = {The FinSim-2 2021 Shared Task: Learning Semantic Similarities for the Financial Domain},
year = {2021},
isbn = {9781450383134},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442442.3451381},
doi = {10.1145/3442442.3451381},
abstract = {The FinSim-2 is a second edition of FinSim Shared Task on Learning Semantic Similarities for the Financial Domain, colocated with the FinWeb workshop. FinSim-2 proposed the challenge to automatically learn effective and precise semantic models for the financial domain. The second edition of the FinSim offered an enriched dataset in terms of volume and quality, and interested in systems which make creative use of relevant resources such as ontologies and lexica, as well as systems which make use of contextual word embeddings such as BERT[4]. Going beyond the mere representation of words is a key step to industrial applications that make use of Natural Language Processing (NLP). This is typically addressed using either unsupervised corpus-derived representations like word embeddings, which are typically opaque to human understanding but very useful in NLP applications or manually created resources such as taxonomies and ontologies, which typically have low coverage and contain inconsistencies, but provide a deeper understanding of the target domain. Finsim is inspired from previous endeavours in the Semeval community, which organized several competitions on semantic/lexical relation extraction between concepts/words. This year, 18 system runs were submitted by 7 teams and systems were ranked according to 2 metrics, Accuracy and Mean rank. All the systems beat our baseline 1 model by over 15 points and the best systems beat the baseline 2 by over 1 ∼ 3 points in accuracy.},
booktitle = {Companion Proceedings of the Web Conference 2021},
pages = {288–292},
numpages = {5},
keywords = {Word embeddings, Natural Language Processing, Hypernym-hyponym relation extraction, Financial documents processing, Domain specific ontology},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{10.1145/3411170.3411261,
author = {Giallonardo, Ester and Poggi, Francesco and Rossi, Davide and Zimeo, Eugenio},
title = {Making Smart Buildings and Personal Systems Cooperate via Knowledge Base Overlays},
year = {2020},
isbn = {9781450375597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411170.3411261},
doi = {10.1145/3411170.3411261},
abstract = {Reactive IoT applications often have to deal with the data source Babel arising from their need to operate on context information originated from different data sources. Semantic knowledge bases can be fruitfully deployed to alleviate this problem: they provide a unified access point for context information including both long term (such as the structure of the environment) and transient (such as sensor readings) data thanks to their ability to host elements responding to different schemas within the same container. In previous works we introduced an architecture to create reactive IoT systems based on a semantic knowledge base that also hosts the definition of their behavior and on an accompanying reactive machinery. In this paper, we introduce the use of knowledge base overlays, i.e. containers providing a live, unified view over (parts of) different underlying knowledge bases, as a mechanism to enable interoperation between multiple IoT semantics-based systems. Specifically we explore the benefits of this approach in a case study in which a semantic IoT system governing a smart building interacts with the personal semantic systems of the people entering the building.},
booktitle = {Proceedings of the 6th EAI International Conference on Smart Objects and Technologies for Social Good},
pages = {181–186},
numpages = {6},
keywords = {Smart moving, Smart Environment, Semantic modeling, Semantic Sensor Networks, Reactive systems, Ontologies, Models@runtime, Internet of Things (IoT), Context-awareness, Context modeling},
location = {Antwerp, Belgium},
series = {GoodTechs '20}
}

@inproceedings{10.1145/3344341.3368806,
author = {Arshad, Bilal and Anjum, Ashiq},
title = {High Performance Dynamic Graph Model for Consistent Data Integration},
year = {2019},
isbn = {9781450368940},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3344341.3368806},
doi = {10.1145/3344341.3368806},
abstract = {In a distributed environment, data from heterogeneous sources are brought together in a unified and consistent manner for analytics and insights. Inconsistencies arising due to the dynamic nature of sources such as addition/deletion of column or merging of columns can compromise the consistency of the distributed system. This can lead to the linking of inaccurate records and faulty data entries. Resulting in false reports and erroneous analyses. Furthermore, issues such as performance guarantees and scalability fuel the existing challenges. We have proposed an alternate graph-based approach to integrate data using an in-memory environment. The central idea of the approach is the use of graphs to integrate heterogeneous data sources in a distributed environment. The underlying approach provides both high-performance and scalability to address changes in a dynamic system for data integration. This allows the generation of graphs from individual source data and modifications in a consistent manner so that the state of the overall distributed system always remains coherent. It provides a novel way of combining consistent data integration and performance in a distributed sys-tem. Our system performs better than existing graph systems for dynamic graph evolution ensuring consistency and provides the necessary scalability guarantees as the size of the data increases. Results also show the correctness of the approach when integrating disparate data-sets},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing},
pages = {263–272},
numpages = {10},
keywords = {scalability, performance, graphs, dynamic graphs, data integration, consistency},
location = {Auckland, New Zealand},
series = {UCC'19}
}

@inproceedings{10.1145/3701716.3715459,
author = {Taffa, Tilahun Abedissa and Usbeck, Ricardo},
title = {Bridge-Generate: Scholarly Hybrid Question Answering},
year = {2025},
isbn = {9798400713316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701716.3715459},
doi = {10.1145/3701716.3715459},
abstract = {Answering scholarly hybrid questions requires access to bibliographic facts stored in structured data, such as a Knowledge Graph (KG) and textual information. Existing Scholarly Hybrid Question Answering (SHQA) approaches rely on retrieving KG triples and documents from the Wikipedia text corpus and prompt an LLM (Large Language Model) for answers. However, the retrieval is heavily keyword-based, introducing noise into the context. Furthermore, despite detecting the entities in the question, the models do not attempt any question analysis. Therefore, we propose a new SHQA system that employs a bridge-generate approach. During the bridge phase, our system recursively identifies entity-encapsulating phrases within the question and resolves the entities leveraging the underlying KGs. It then formulates assertion statements based on the resolved entities and their corresponding phrases. In the generation phase, the system auto-generates context guided by the question and the assertions. Finally, it returns an answer prompting an LLM with the generated context, the assertions, and the question. Our approach outperforms previous approaches, addressing the identified gaps.},
booktitle = {Companion Proceedings of the ACM on Web Conference 2025},
pages = {1321–1325},
numpages = {5},
keywords = {hybrid question answering, question answering, scholarly hybrid question answering, scholarly question answering},
location = {Sydney NSW, Australia},
series = {WWW '25}
}

@inproceedings{10.1145/3652620.3688216,
author = {Fadhlillah, Hafiyyan Sayyid and Greiner, Sandra and Feichtinger, Kevin and Rabiser, Rick and Zoitl, Alois},
title = {Managing Variability of Cyber-Physical Production Systems: Towards Consistency Management},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688216},
doi = {10.1145/3652620.3688216},
abstract = {Engineering Cyber-Physical Production Systems (CPPSs) involves several different disciplines, where team members range from mechanical, electrical, and automation engineers, to control software engineers. When developing variability-intensive software systems such as CPPSs, engineers create heterogeneous engineering artifacts of varying granularity, structure, and level of abstraction in the problem and solution space, e.g., CAD drawings, delta models, and control software artifacts. Managing consistency among these heterogeneous artifacts is essential during the development and maintenance of these systems to reduce development costs and runtime failures. Software product line engineering provides approaches to manage the variability of heterogeneous artifacts. However, these approaches must be adapted and extended to manage consistency in CPPSs and address the additional multidimensional challenges in CPPSs. In this short paper, we outline these challenges, motivate them using a case study, and discuss potential solutions to manage the consistency of engineering artifacts expressing CPPS control software variability. We thereby lay the grounds for a deeper understanding of possible inconsistencies and exploring new methods for managing consistency in control software variability in CPPSs.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {945–949},
numpages = {5},
keywords = {cyber-physical production systems engineering, heterogeneous multi-modeling, software modeling consistency},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3589334.3645649,
author = {Gong, Jiaying and Eldardiry, Hoda},
title = {Multi-Label Zero-Shot Product Attribute-Value Extraction},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645649},
doi = {10.1145/3589334.3645649},
abstract = {E-commerce platforms should provide detailed product descriptions (attribute values) for effective product search and recommendation. However, attribute value information is typically not available for new products. To predict unseen attribute values, large quantities of labeled training data are needed to train a traditional supervised learning model. Typically, it is difficult, time-consuming, and costly to manually label large quantities of new product profiles. In this paper, we propose a novel method to efficiently and effectively extract unseen attribute values from new products in the absence of labeled data (zero-shot setting). We propose HyperPAVE, a multi-label zero-shot attribute value extraction model that leverages inductive inference in heterogeneous hypergraphs. In particular, our proposed technique constructs heterogeneous hypergraphs to capture complex higher-order relations (i.e. user behavior information) to learn more accurate feature representations for graph nodes. Furthermore, our proposed HyperPAVE model uses an inductive link prediction mechanism to infer future connections between unseen nodes. This enables HyperPAVE to identify new attribute values without the need for labeled training data. We conduct extensive experiments with ablation studies on different categories of the MAVE dataset. The results demonstrate that our proposed HyperPAVE model significantly outperforms existing classification-based, generation-based large language models for attribute value extraction in the zero-shot setting.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {2259–2270},
numpages = {12},
keywords = {attribute value extraction, heterogeneous hypergraph, inductive link prediction, zero-shot learning},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3167132.3167287,
author = {Autili, Marco and Di Salle, Amleto and Gallo, Francesco and Pompilio, Claudio and Tivoli, Massimo},
title = {Model-driven adaptation of service choreographies},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167287},
doi = {10.1145/3167132.3167287},
abstract = {Service choreographies represent a powerful and flexible approach to compose software services in a fully distributed way. A key enabler for the actual realization of choreographies is the ability to automatically compose services, and perform exogenous coordination and adaptation of their interaction. This is a nontrivial and error prone task. Automatic support for realizing choreographies is needed. In this paper we focus on adapter generation and describe our novel approach to the synthesis of service Adapters. When needed, adapters permit to correctly bind concrete services to (abstract) choreography roles by solving possible protocol mismatches. Enterprise Integration Patterns are used as adaptation primitives and composed to realize complex adaptation policies.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1441–1450},
numpages = {10},
keywords = {service-oriented computing, service choreography, model-driven, enterprise integration pattern, adaptation},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3594536.3595124,
author = {van Drie, Romy A. N. and de Boer, Maaike H. T. and Bakker, Roos M. and Tolios, Ioannis and Vos, Daan},
title = {The Dutch Law as a Semantic Role Labeling Dataset},
year = {2023},
isbn = {9798400701979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594536.3595124},
doi = {10.1145/3594536.3595124},
abstract = {Legal documents, and specifically law texts, are not easy to understand by humans. The specific terminology and sentence constructions are particular, which also makes it a difficult machine understanding task. In this paper, we present a publicly available benchmark dataset containing Dutch law texts which can be used to train AI models that assist humans equipped with the task of interpreting legal texts. However, the dataset can be used in a broader context, such as semantic role labeling of Dutch (legal) texts. Our dataset contains 4463 annotated sentences from 55 different Dutch laws, in which four roles are annotated by human annotators: action, actor, object and recipient. The inter-annotator agreement is substantial (κ=0.75). In experiments with a rule-based and a transformer-based method, results show that the transformer-based method performs quite well on the dataset (accuracy &gt; 0.8). These results indicate that we can reliably predict actions, actors, objects and recipients in legal texts. This can help people equipped with the task of formal interpretation of legal texts.},
booktitle = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},
pages = {316–322},
numpages = {7},
keywords = {datasets, legal interpretation, legal text, natural language processing, norms, semantic role labeling},
location = {Braga, Portugal},
series = {ICAIL '23}
}

@article{10.1145/3585387,
author = {Garc\'{\i}a, Roberto and Cediel, Ana and Teixid\'{o}, Merc\`{e} and Gil, Rosa},
title = {Semantics and Non-fungible Tokens for Copyright Management on the Metaverse and Beyond},
year = {2024},
issue_date = {July 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {7},
issn = {1551-6857},
url = {https://doi.org/10.1145/3585387},
doi = {10.1145/3585387},
abstract = {Recent initiatives related to the Metaverse focus on better visualization, like augmented or virtual reality, but also persistent digital objects. To guarantee real ownership of these digital objects, open systems based on public blockchains and Non-Fungible Tokens (NFTs) are emerging together with a nascent decentralized and open creator economy. To manage this emerging economy in a more organized way, and fight the so common NFT plagiarism, we propose CopyrightLY, a decentralized application for authorship and copyright management. It provides means to claim content authorship, including supporting evidence. Content and metadata are stored in decentralized storage and registered on the blockchain. A token is used to curate these claims, and potential complaints, by staking it on them. Staking is incentivized by the fact that the token is minted using a bonding curve. The tokenomics include the resolution of complaints and enabling the monetization of curated claims. Monetization is achieved through licensing NFTs with metadata enhanced by semantic technologies. Semantic data makes explicit the reuse conditions transferred with the token while keeping the connection to the underlying copyright claims to improve the trustability of the NFTs. Moreover, the semantic metadata is flexible enough to enable licensing not just in the real world. Licenses can refer to reuses in specific locations in a metaverse, thus facilitating the emergence of creative economies in them.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = mar,
articleno = {186},
numpages = {20},
keywords = {Metaverse, Non-Fungible Token, copyright, social media, blockchain, ontology}
}

@article{10.1145/3230713,
author = {Russo, Daniel and Ciancarini, Paolo and Falasconi, Tommaso and Tomasi, Massimo},
title = {A Meta-Model for Information Systems Quality: A Mixed Study of the Financial Sector},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3230713},
doi = {10.1145/3230713},
abstract = {Information Systems Quality (ISQ) is a critical source of competitive advantages for organizations. In a scenario of increasing competition on digital services, ISQ is a competitive differentiation asset. In this regard, managing, maintaining, and evolving IT infrastructures have become a primary concern of organizations. Thus, a technical perspective on ISQ provides useful guidance to meet current challenges. The financial sector is paradigmatic, since it is a traditional business, with highly complex business-critical legacy systems, facing a tremendous change due to market and regulation drivers. We carried out a Mixed-Methods study, performing a Delphi-like study on the financial sector. We developed a specific research framework to pursue this vertical study. Data were collected in four phases starting with a high-level randomly stratified panel of 13 senior managers and then a target panel of 124 carefully selected and well-informed domain experts. We have identified and dealt with several quality factors; they were discussed in a comprehensive model inspired by the ISO 25010, 42010, and 12207 standards, corresponding to software quality, software architecture, and software process, respectively. Our results suggest that the relationship among quality, architecture, and process is a valuable technical perspective to explain the quality of an information system. Thus, we introduce and illustrate a novel meta-model, named SQuAP (Software Quality, Architecture, Process), which is intended to give a comprehensive picture of ISQ by abstracting and connecting detailed individual ISO models.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = sep,
articleno = {11},
numpages = {38},
keywords = {software quality, software process, software architecture, mixed methods, management information systems, delphi study, Information systems quality}
}

@inproceedings{10.1145/3570945.3607343,
author = {Aicher, Annalena and Weber, Klaus and Andr\'{e}, Elisabeth and Minker, Wolfgang and Ultes, Stefan},
title = {The Influence of Avatar Interfaces on Argumentative Dialogues},
year = {2023},
isbn = {9781450399944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3570945.3607343},
doi = {10.1145/3570945.3607343},
abstract = {Humans form opinions and justify different points of view by exchanging arguments and knowledge. Likewise to human-human interaction, the way arguments are presented influence the user's willingness to engage into a critical reflection. Especially when interacting with conversational agents the user's engagement and motivation are important factors and highly influence the success or failure of such a mixed team. To maintain the users' trust and satisfaction, the users' perception of the respective system is an important indicator. Thus, this work investigates the design of a cooperative argumentative dialogue system using a virtual avatar compared to a non-avatar interface by evaluating a crowdsourcing study conducted with 84 participants. The results indicate, that the avatar system is perceived as significantly more appealing and natural and thus, engaging which also influences the acceptance and perception of the quality of presented arguments. Furthermore, we found that the presence of the avatar often led to an increase in the anticipated level of conversational proficiency similar to that of a human interlocutor. Therefore, this work provides important insights for the design of future cooperative argumentative virtual avatar interfaces.},
booktitle = {Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents},
articleno = {24},
numpages = {8},
keywords = {Avatar Interface, Conversational Engagement, Crowdsourcing Study, Human-Computer Interaction, User Trust},
location = {W\"{u}rzburg, Germany},
series = {IVA '23}
}

@inproceedings{10.1145/3689050.3704801,
author = {Bertmark, Anna My},
title = {Artefacts as Pedagogy for Futuring},
year = {2025},
isbn = {9798400711978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689050.3704801},
doi = {10.1145/3689050.3704801},
abstract = {This PhD explores the potential of interactive artefacts as tangible pedagogy for generating perspective shifts towards ecocentric thinking and doing. Research has highlighted how deliberate societal transformative change is required to reach the targets of sustainability goals and how perspective shifts for instigating these changes may be supported and achieved. The research investigates how design and HCI seek to challenge anthropocentric approaches and perspectives, motivated by the growing number of works in technology for the more-than-human and feminist post-humanism. Speculative, more-than-human design and sustainable HCI highlight ecosystem interdependence through transmedia narratives and interactive ecocentric artefacts. However, their inaccessibility and unexplored impact limit their potential to bridge the knowledge-action gap for local development to stay within necessary earth-system boundaries. This paper emphasises the necessity for perceiving how interactive artefacts generate epistemological and ontological shifts and how this may be utilised to advise future design towards ethics of care and regenerative development.},
booktitle = {Proceedings of the Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {120},
numpages = {9},
keywords = {Design ontology, Ecocentric design, Ecological HCI, Embodied learning, Futuring, Interaction for sustainability},
location = {
},
series = {TEI '25}
}

@inproceedings{10.1145/3472306.3478360,
author = {Ishii, Ryo and Ren, Xutong and Muszynski, Michal and Morency, Louis-Philippe},
title = {Multimodal and Multitask Approach to Listener's Backchannel Prediction: Can Prediction of Turn-changing and Turn-management Willingness Improve Backchannel Modeling?},
year = {2021},
isbn = {9781450386197},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472306.3478360},
doi = {10.1145/3472306.3478360},
abstract = {The listener's backchannel has the important function of encouraging a current speaker to hold their turn and continue to speak, which enables smooth conversation. The listener monitors the speaker's turn-management (a.k.a. speaking and listening) willingness and his/her own willingness to display backchannel behavior. Many studies have focused on predicting the appropriate timing of the backchannel so that conversational agents can display backchannel behavior in response to a user who is speaking. To the best of our knowledge, none of them added the prediction of turn-changing and participants' turn-management willingness to the backchannel prediction model in dyad interactions. In this paper, we proposed a novel backchannel prediction model that can jointly predict turn-changing and turn-management willingness. We investigated the impact of modeling turn-changing and willingness to improve backchannel prediction. Our proposed model is based on trimodal inputs, that is, acoustic, linguistic, and visual cues from conversations. Our results suggest that adding turn-management willingness as a prediction task improves the performance of backchannel prediction within the multi-modal multi-task learning approach, while adding turn-changing prediction is not useful for improving the performance of backchannel prediction.},
booktitle = {Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents},
pages = {131–138},
numpages = {8},
keywords = {turn-management willingness, turn-changing, multitask learning, multimodal signal processing, backchannel},
location = {Virtual Event, Japan},
series = {IVA '21}
}

@article{10.1145/3539608,
author = {Anwar, Sibgha and Beg, Mirza Omer and Saleem, Kiran and Ahmed, Zeeshan and Javed, Abdul Rehman and Tariq, Usman},
title = {Social Relationship Analysis Using State-of-the-art Embeddings},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {5},
issn = {2375-4699},
url = {https://doi.org/10.1145/3539608},
doi = {10.1145/3539608},
abstract = {Detection of human relationships from their interactions on social media is a challenging problem with a wide range of applications in different areas, like targeted marketing, cyber-crime, fraud, defense, planning, and human resource, to name a few. All previous work in this area has only dealt with the most basic types of relationships. The proposed approach goes beyond the previous work to efficiently handle the hierarchy of social relationships. This article introduces a novel technique named Quantifiable Social Relationship (QSR) analysis for quantifying social relationships to analyze relationships between agents from their textual conversations. QSR uses cross-disciplinary techniques from computational linguistics and cognitive psychology to identify relationships. QSR utilizes sentiment and behavioral styles displayed in the conversations for mapping them onto level II relationship categories. Then, for identifying the level III relationship categories, QSR uses level II relationships, sentiments, interactions, and word embeddings as key features. QSR employs natural language processing techniques for feature engineering and state-of-the-art embeddings generated by word2vec, global vectors (glove), and bidirectional encoder representations from transformers (bert). QSR combines the intrinsic conversational features with word embeddings for classifying relationships. QSR achieves an accuracy of up to 89\% for classifying relationship subtypes. The evaluation shows that QSR can accurately identify the hierarchical relationships between agents by extracting intrinsic and extrinsic features from textual conversations between agents.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
articleno = {138},
numpages = {21},
keywords = {machine learning, behavioral model, quantifiable relationships, hierarchical relationship analysis, social relationship, Agents interaction model}
}

@article{10.1145/3556538,
author = {Benedetto, Luca and Cremonesi, Paolo and Caines, Andrew and Buttery, Paula and Cappelli, Andrea and Giussani, Andrea and Turrin, Roberto},
title = {A Survey on Recent Approaches to Question Difficulty Estimation from Text},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3556538},
doi = {10.1145/3556538},
abstract = {Question Difficulty Estimation from Text (QDET) is the application of Natural Language Processing techniques to the estimation of a value, either numerical or categorical, which represents the difficulty of questions in educational settings. We give an introduction to the field, build a taxonomy based on question characteristics, and present the various approaches that have been proposed in recent years, outlining opportunities for further research. This survey provides an introduction for researchers and practitioners into the domain of question difficulty estimation from text and acts as a point of reference about recent research in this topic to date.},
journal = {ACM Comput. Surv.},
month = jan,
articleno = {178},
numpages = {37},
keywords = {student assessment, question calibration, Question difficulty estimation}
}

@inproceedings{10.1145/3634737.3645000,
author = {Kumarasinghe, Udesh and Lekssays, Ahmed and Sencar, Husrev Taha and Boughorbel, Sabri and Elvitigala, Charitha and Nakov, Preslav},
title = {Semantic Ranking for Automated Adversarial Technique Annotation in Security Text},
year = {2024},
isbn = {9798400704826},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3634737.3645000},
doi = {10.1145/3634737.3645000},
abstract = {We introduce a novel approach for mapping attack behaviors described in threat analysis reports to entries in an adversarial techniques knowledge base. Our method leverages a multi-stage ranking architecture to efficiently rank the most related techniques based on their semantic relevance to the input text. Each ranker in our pipeline uses a distinct design for text representation. To enhance relevance modeling, we leverage pretrained language models, which we fine-tune for the technique annotation task. While generic large language models are not yet capable of fully addressing this challenge, we obtain very promising results. We achieve a recall rate improvement of +35\% compared to the previous state-of-the-art results. We further create new public benchmark datasets for training and validating methods in this domain, which we release to the research community aiming to promote future research in this important direction.},
booktitle = {Proceedings of the 19th ACM Asia Conference on Computer and Communications Security},
pages = {49–62},
numpages = {14},
keywords = {threat intelligence, TTP annotation, text ranking, text attribution},
location = {Singapore, Singapore},
series = {ASIA CCS '24}
}

@inbook{10.1145/3233795.3233805,
author = {Hornung, Rachel and Chen, Nutan and van der Smagt, Patrick},
title = {Early integration for movement modeling in latent spaces},
year = {2019},
isbn = {9781970001754},
publisher = {Association for Computing Machinery and Morgan \&amp; Claypool},
url = {https://doi.org/10.1145/3233795.3233805},
booktitle = {The Handbook of Multimodal-Multisensor Interfaces: Language Processing, Software, Commercialization, and Emerging Directions},
pages = {305–345},
numpages = {41}
}

@inproceedings{10.1145/3543507.3587428,
author = {de Berardinis, Jacopo and Mero\~{n}o-Pe\~{n}uela, Albert and Poltronieri, Andrea and Presutti, Valentina},
title = {The Harmonic Memory: a Knowledge Graph of harmonic patterns as a trustworthy framework for computational creativity},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3587428},
doi = {10.1145/3543507.3587428},
abstract = {Computationally creative systems for music have recently achieved impressive results, fuelled by progress in generative machine learning. However, black-box approaches have raised fundamental concerns for ethics, accountability, explainability, and musical plausibility. To enable trustworthy machine creativity, we introduce the Harmonic Memory, a Knowledge Graph (KG) of harmonic patterns extracted from a large and heterogeneous musical corpus. By leveraging a cognitive model of tonal harmony, chord progressions are segmented into meaningful structures, and patterns emerge from their comparison via harmonic similarity. Akin to a music memory, the KG holds temporal connections between consecutive patterns, as well as salient similarity relationships. After demonstrating the validity of our choices, we provide examples of how this design enables novel pathways for combinational creativity. The memory provides a fully accountable and explainable framework to inspire and support creative professionals – allowing for the discovery of progressions consistent with given criteria, the recomposition of harmonic sections, but also the co-creation of new progressions.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {3873–3882},
numpages = {10},
keywords = {computational creativity, knowledge graphs, music technology},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3592813.3592915,
author = {Izo, Flavio and Vereau, Luis Enrique Santos Prado and Pirovani, Juliana Pinheiro Campos and Oliveira, Elias and Badue, Claudine},
title = {An Intelligent Report Generator for Chemical Documents},
year = {2023},
isbn = {9798400707599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592813.3592915},
doi = {10.1145/3592813.3592915},
abstract = {Context: Scientific articles and patents contain academic, industrial, and scientific information. Automatically retrieving information from these documents is necessary for supporting upcoming scientific research development. Problem: Difficulties in manually identifying and analyzing the chemical information in documents make it nearly impossible to access specific contents of chemical investigations and generate reports to support ongoing research. Solution: In this article, we present a system that recognizes chemical entities (elements, classes, compounds, methods, and equipment) and generates intelligent reports from free texts. IS Theory: We developed this work under the support of Soft Systems Theory. Method: This research was evaluated through proof of concept. We used 30 chemical patents from Brazilian National Institute of Industrial Property and 20 scientific articles from Revista Virtual de Qu\'{\i}mica (RVq). For validation, we extracted the texts and recognized the named entities through, for instance, the hybrid method Conditional Random Field (CRF) + Local Grammar (LG). We then apply rules to generate intelligent reports. Summary of Results: The system can generate seven types of intelligent reports, two of which are customized by the user. For datasetPat our model obtained mean values of 98.96\% for Precision, 91.12\% for Recall, and 94.17\% for F-Score. The datasetArt reached average values of 97.31\%, 86.94\%, and 91.29\% for Precision, Recall, and F-Score, respectively. Contributions and Impact in the IS Area: This research presents as the main contribution the availability of an Information System for the generation of intelligent reports from documents based on the recognition of named entities in the chemical area. In addition the hybrid method CRF+LG can contribute to the evolution of Information Systems, helping people and organizations. The model is described throughout the paper and can be replicated in other contexts.},
booktitle = {Proceedings of the XIX Brazilian Symposium on Information Systems},
pages = {276–283},
numpages = {8},
keywords = {Natural Language Processing, Named Entity Recognition, Intelligent Report., Artificial Intelligence},
location = {Macei\'{o}, Brazil},
series = {SBSI '23}
}

@inproceedings{10.1145/3477314.3507256,
author = {Kanwal, Neel and Rizzo, Giuseppe},
title = {Attention-based clinical note summarization},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507256},
doi = {10.1145/3477314.3507256},
abstract = {In recent years, the trend of deploying digital systems in numerous industries has hiked. The health sector has observed an extensive adoption of digital systems and services that generate significant medical records. Electronic health records contain valuable information for prospective and retrospective analysis that is often not entirely exploited because of the complicated dense information storage. The crude purpose of condensing health records is to select the information that holds most characteristics of the original documents based on a reported disease. These summaries may boost diagnosis and save a doctor's time during a saturated workload situation like the COVID-19 pandemic. In this paper, we are applying a multi-head attention-based mechanism to perform extractive summarization of meaningful phrases on clinical notes. Our method finds major sentences for a summary by correlating tokens, segments, and positional embeddings of sentences in a clinical note. The model outputs attention scores that are statistically transformed to extract critical phrases for visualization on the heat-mapping tool and for human use.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {813–820},
numpages = {8},
keywords = {ICD-9, MIMIC-III, clinical notes, deep learning, electronic health records, extractive summarization, information extraction, medical records, multi-head attention, natural language processing, transformer models},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1145/3430984.3431049,
author = {R, Tharaniya Sairaj and S. R., Balasundaram},
title = {An Entailment Analysis Based Entity Mapping To Improve Automatic Question Generation},
year = {2021},
isbn = {9781450388177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430984.3431049},
doi = {10.1145/3430984.3431049},
abstract = {Automatic generation of assessment questions to enhance learning requires natural language understanding. To enhance the process, the entities extracted from the subject domain, may be mapped to form knowledge graphs. But entity mapping faces the challenge of analysing entailment. The proposed work analyses the semantic relevance in the process and aims at generating heuristics to map entities.},
booktitle = {Proceedings of the 3rd ACM India Joint International Conference on Data Science \&amp; Management of Data (8th ACM IKDD CODS \&amp; 26th COMAD)},
pages = {424},
numpages = {1},
keywords = {Ontology, Natural Language Understanding, Entity Mapping, Entailment Analysis, E-Assessment},
location = {Bangalore, India},
series = {CODS-COMAD '21}
}

@inproceedings{10.1145/3731763.3731782,
author = {Dang, Dung Thi and Nguyen, Khoi Tan and Huynh, Hiep Xuan},
title = {Efficient Object Detection Using Total Energy Function: An Alternative to Anchor Box},
year = {2025},
isbn = {9798400710841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3731763.3731782},
doi = {10.1145/3731763.3731782},
abstract = {Bounding box regression is a popular technique for fine-tuning or predicting the localization boxes in object detection methods that are trained to regress from proposed regions or fixed anchor boxes to bounding boxes. But anchor-based methods face computational costs and challenges in optimal anchor configuration, which constrain their adaptability and performance. This paper proposes an anchor-free object detection method using multi-component energy optimization that integrates edge, region, size, and shape information to determine the optimal bounding box parameters. Experiments on PASCAL VOC 2012 show superior performance with 85.7\% loss reduction for small objects and 3-4 times faster convergence speed, demonstrating the effectiveness of the energy optimization method.},
booktitle = {Proceedings of the 2025 10th International Conference on Intelligent Information Technology},
pages = {28–34},
numpages = {7},
keywords = {Object Detection, Energy-Based Models, Anchor-free Detection, Bounding Box Optimization, Multi-component Energy Function},
location = {
},
series = {ICIIT '25}
}

@inproceedings{10.1145/3575882.3575926,
author = {Pinem, Josua Geovani and Septiadi, Agung and Shaleha, Siti and Alfin, Muhammad Reza and Subekti, Aulia Haritsuddin Karisma Muhammad and Muliadi, Jemie and Wibowanto, Gembong and Santosa, Agung and Uliniansyah, M. Teduh and Jarin, Asril and Latief, Andi Djalal and Gunarso and Riza, Hammam},
title = {Developing Semantic Annotation Representation of Social Media Sentiments and Metadata as Resource Description Framework: A Study of Indonesian New Capital Related Tweets Written in Bahasa},
year = {2023},
isbn = {9781450397902},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575882.3575926},
doi = {10.1145/3575882.3575926},
abstract = {Social Media has become a tool abiding the press in this modern society. Everyone can write their minds and build their mass media to publish opinions. Thus, in this manuscript, we develop a resource description framework scheme (RDFS) to enrich the information and metadata from Indonesian tweets regarding their New Capitol. This work focused on applying a popular method (i.e., the Tweetskb scheme) to construct the RDF of those tweets. We also developed the Schema to fulfill our need to contain all the information to RDF. RDF Triples were generated by connecting several established vocabularies to ensure the connection between its related nodes has meaning. The sentiment polarity (i.e., neutral, positive, and negative sentiment) is used in this manuscript. Thus, our proposal can be used as an initial work to make use of twitter's metadata to predict how reliable a user is, how the community interact with a certain topic, spam detection, clustering, and even implementing machine learning and deep learning sentiment analysis in a manner of knowledge graph.},
booktitle = {Proceedings of the 2022 International Conference on Computer, Control, Informatics and Its Applications},
pages = {229–234},
numpages = {6},
keywords = {Twitter, Sentiment Analysis RDF/S, Ontology, Natural language processing, Knowledge Graph},
location = {Virtual Event, Indonesia},
series = {IC3INA '22}
}

@inproceedings{10.1145/3293881.3295782,
author = {Frezza, Stephen and Daniels, Mats and Pears, Arnold and Cajander, \r{A}sa and Kann, Viggo and Kapoor, Amanpreet and McDermott, Roger and Peters, Anne-Kathrin and Sabin, Mihaela and Wallace, Charles},
title = {Modelling competencies for computing education beyond 2020: a research based approach to defining competencies in the computing disciplines},
year = {2018},
isbn = {9781450362238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293881.3295782},
doi = {10.1145/3293881.3295782},
abstract = {How might the content and outcomes of tertiary education programmes be described and analysed in order to understand how they are structured and function? To address this question we develop a framework for modelling graduate competencies linked to tertiary degree programmes in the computing disciplines. While the focus of our work is computing the framework is applicable to education more broadly.  The work presented here draws upon the pioneering curricular document for information technology (IT2017), curricular competency frameworks, other related documents such as the software engineering competency model (SWECOM), the Skills Framework for the Information Age (SFIA), current research in competency models, and elicitation workshop results from recent computing conferences. The aim is to inform the ongoing Computing Curricula (CC2020) project, an endeavour supported by the Association for Computing Machinery (ACM) and the IEEE Computer Society. We develop the Competency Learning Framework (CoLeaF), providing an internationally relevant tool for describing competencies. We argue that this competency based approach is well suited for constructing learning environments and assists degree programme architects in dealing with the challenge of developing, describing and including competencies relevant to computer and IT professionals.  In this paper we demonstrate how the CoLeaF competency framework can be applied in practice, and though a series of case studies demonstrate its effectiveness and analytical power as a tool for describing and comparing degree programmes in the international higher education landscape.},
booktitle = {Proceedings Companion of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education},
pages = {148–174},
numpages = {27},
keywords = {curriculum guidelines, Professional competencies, Computing competencies, CC2020},
location = {Larnaca, Cyprus},
series = {ITiCSE 2018 Companion}
}

@inproceedings{10.1145/3544548.3581175,
author = {Benjamin, Jesse Josua and Biggs, Heidi and Berger, Arne and Rukanskaitundefined, Julija and Heidt, Michael B. and Merrill, Nick and Pierce, James and Lindley, Joseph},
title = {The Entoptic Field Camera as Metaphor-Driven Research-through-Design with AI Technologies},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581175},
doi = {10.1145/3544548.3581175},
abstract = {Artificial intelligence (AI) technologies are widely deployed in smartphone photography; and prompt-based image synthesis models have rapidly become commonplace. In this paper, we describe a Research-through-Design (RtD) project which explores this shift in the means and modes of image production via the creation and use of the Entoptic Field Camera. Entoptic phenomena usually refer to perceptions of floaters or bright blue dots stemming from the physiological interplay of the eye and brain. We use the term entoptic as a metaphor to investigate how the material interplay of data and models in AI technologies shapes human experiences of reality. Through our case study using first-person design and a field study, we offer implications for critical, reflective, more-than-human and ludic design to engage AI technologies; the conceptualisation of an RtD research space which contributes to AI literacy discourses; and outline a research trajectory concerning materiality and design affordances of AI technologies.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {178},
numpages = {19},
keywords = {GAN, artificial intelligence, image synthesis, materiality, research through design, technological mediation},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{10.1145/3745026,
author = {Yitagesu, Sofonias and Xing, Zhenchang and Zhang, Xiaowang and Feng, Zhiyong and Bi, Tingting and Han, Linyi and Li, Xiaohong},
title = {Systematic Literature Review on Software Security Vulnerability Information Extraction},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3745026},
doi = {10.1145/3745026},
abstract = {Background. Software vulnerabilities are increasing in complexity and scale, posing great security risks to many software systems. Extracting information about software vulnerabilities is a critical area of research that aims to identify and create a structured representation of vulnerability-related information. This structured data helps software systems better understand vulnerabilities and provides security professionals with timely information to mitigate the impact of rapidly growing vulnerabilities while guiding future research to develop more secure systems. However, this process relies on the effectiveness of information extraction to transform manual vulnerability analysis from security experts to digital solutions. Despite its importance, the unique nature of vulnerability information and the fast pace at which machine learning-based extraction methods and techniques have evolved make it challenging to assess the current successes, failures, challenges, and opportunities within this research area. This study presents a systematic literature review aimed at clarifying this complex landscape.Methods. In this study, we conduct a systematic literature review (SLR) to explore existing research focusing on extracting information about software security vulnerabilities. We search for 829 primary studies on security vulnerability information extraction from seven widely used online digital libraries, focusing on top peer-reviewed journals and conferences published between 2001 and 2024. After applying our inclusion and exclusion criteria and the snowballing technique, we narrowed our selection to 87 studies for in-depth analysis and addressed four main research questions. We collect qualitative and quantitative data from each study, identifying 34 components such as research problems, methods, contributions, evaluation metrics, results, types of extracted vulnerability information, challenges, and limitations. We use meta-analysis, statistical machine learning, and text-mining techniques to identify themes, patterns, and trends across the primary studies and visualize findings.Results. The study provides an overview of the security vulnerability data landscape, identifies key resources, and guides efforts to improve vulnerability information extraction and analysis. The study finds a diverse landscape of learning algorithms used in security vulnerability information extraction, with Bidirectional Encoder Representations from Transformers (BERT), Long Short-term Memory (LSTM), and Support Vector Machine (SVM) being the most dominant. The study identifies key challenges, including feature engineering complexity, lack of a gold-standard corpus, preprocessing errors, generating accurate training data, addressing imbalanced data, multimodality fusion, and graph sparsity in security knowledge graphs.Insights for Future Research Directions. The study underscores the need for advanced extraction approaches, robust datasets, automated annotation methods, and advanced machine learning algorithms to improve the extraction of security vulnerability information. This study also suggests using large language models (LLMs) and transformer models to facilitate the automatic extraction of security-related words, terms, concepts, and phrases and introduce new filtering parameters for user requirements. We provide all our implementations; it can be found at .},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = jun,
keywords = {Software Vulnerability, Vulnerability Information Extraction, systematic literature review (SLR), Meta-analysis, Statistical Machine Learning, Text Mining}
}

@article{10.1145/3555312,
author = {Asprino, Luigi and Daga, Enrico and Gangemi, Aldo and Mulholland, Paul},
title = {Knowledge Graph Construction with a Fa\c{c}ade: A Unified Method to Access Heterogeneous Data Sources on the Web},
year = {2023},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1533-5399},
url = {https://doi.org/10.1145/3555312},
doi = {10.1145/3555312},
abstract = {Data integration is the dominant use case for RDF Knowledge Graphs. However, Web resources come in formats with weak semantics (for example, CSV and JSON), or formats specific to a given application (for example, BibTex, HTML, and Markdown). To solve this problem, Knowledge Graph Construction (KGC) is gaining momentum due to its focus on supporting users in transforming data into RDF. However, using existing KGC frameworks result in complex data processing pipelines, which mix structural and semantic mappings, whose development and maintenance constitute a significant bottleneck for KG engineers. Such frameworks force users to rely on different tools, sometimes based on heterogeneous languages, for inspecting sources, designing mappings, and generating triples, thus making the process unnecessarily complicated. We argue that it is possible and desirable to equip KG engineers with the ability of interacting with Web data formats by relying on their expertise in RDF and the well-established SPARQL query language&nbsp;[2]. In this article, we study a unified method for data access to heterogeneous data sources with Facade-X, a meta-model implemented in a new data integration system called SPARQL Anything. We demonstrate that our approach is theoretically sound, since it allows a single meta-model, based on RDF, to represent data from (a) any file format expressible in BNF syntax, as well as (b) any relational database. We compare our method to state-of-the-art approaches in terms of usability (cognitive complexity of the mappings) and general performance. Finally, we discuss the benefits and challenges of this novel approach by engaging with the reference user community.},
journal = {ACM Trans. Internet Technol.},
month = feb,
articleno = {6},
numpages = {31},
keywords = {re-engineering, meta-model, RDF, SPARQL}
}

@inproceedings{10.1145/3715275.3732091,
author = {Ferrario, Andrea},
title = {A Trustworthiness-based Metaphysics of Artificial Intelligence Systems},
year = {2025},
isbn = {9798400714825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715275.3732091},
doi = {10.1145/3715275.3732091},
abstract = {Modern AI systems are man-made objects that leverage machine learning to support our lives across a myriad of contexts and applications. Despite extensive epistemological and ethical debates, their metaphysical foundations remain relatively under explored. The orthodox view simply suggests that AI systems, as artifacts, lack well-posed identity and persistence conditions—their metaphysical kinds are no real kinds. In this work, we challenge this perspective by introducing a theory of metaphysical identity of AI systems. We do so by characterizing their kinds and introducing identity criteria—formal rules that answer the questions “When are two AI systems the same?” and “When does an AI system persist, despite change?” Building on Carrara and Vermaas’ account of fine-grained artifact kinds, we argue that AI trustworthiness provides a lens to understand AI system kinds and formalize the identity of these artifacts by relating their functional requirements to their physical make-ups. The identity criteria of AI systems are determined by their trustworthiness profiles—the collection of capabilities that the systems must uphold over time throughout their artifact histories, and their effectiveness in maintaining these capabilities. Our approach suggests that the identity and persistence of AI systems is sensitive to the socio-technical context of their design and utilization via their trustworthiness, providing a solid metaphysical foundation to the epistemological, ethical, and legal discussions about these artifacts.},
booktitle = {Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1360–1370},
numpages = {11},
keywords = {artificial intelligence, machine learning, deep learning, identity, metaphysics, ontology, change, time},
location = {
},
series = {FAccT '25}
}

@inproceedings{10.1145/3514094.3534178,
author = {Skorupa Parolin, Erick and Hosseini, MohammadSaleh and Hu, Yibo and Khan, Latifur and Brandt, Patrick T. and Osorio, Javier and D'Orazio, Vito},
title = {Multi-CoPED: A Multilingual Multi-Task Approach for Coding Political Event Data on Conflict and Mediation Domain},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534178},
doi = {10.1145/3514094.3534178},
abstract = {Political and social scientists monitor, analyze and predict political unrest and violence, preventing (or mitigating) harm, and promoting the management of global conflict. They do so using event coder systems, which extract structured representations from news articles to design forecast models and event-driven continuous monitoring systems. Existing methods rely on expensive manual annotated dictionaries and do not support multilingual settings. To advance the global conflict management, we propose a novel model, Multi-CoPED (Multilingual Multi-Task Learning BERT for Coding Political Event Data), by exploiting multi-task learning and state-of-the-art language models for coding multilingual political events. This eliminates the need for expensive dictionaries by leveraging BERT models' contextual knowledge through transfer learning. The multilingual experiments demonstrate the superiority of Multi-CoPED over existing event coders, improving the absolute macro-averaged F1-scores by 23.3\% and 30.7\% for coding events in English and Spanish corpus, respectively. We believe that such expressive performance improvements can help to reduce harms to people at risk of violence.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {700–711},
numpages = {12},
keywords = {transfer learning, social conflict, political conflict, natural language processing, event coding, artificial intelligence and geopolitics},
location = {Oxford, United Kingdom},
series = {AIES '22}
}

@article{10.1177/26339137231207634,
author = {L\'{e}vy, Pierre},
title = {Semantic computing with IEML},
year = {2023},
issue_date = {October-December 2023},
publisher = {Sage Publications, Inc.},
address = {USA},
volume = {2},
number = {4},
url = {https://doi.org/10.1177/26339137231207634},
doi = {10.1177/26339137231207634},
abstract = {This paper presents IEML, Information Economy MetaLanguage, a constructed language with the same expressive power as a natural language and with computable semantics. Distinguished from pragmatic and referential semantics, linguistic semantics have not yet been completely formalized. Only its syntagmatic dimension has been mathematized in the form of regular languages. Its paradigmatic dimension remained to be formalized. In order to complete the mathematizing of language, including its paradigmatic dimension, I have coded linguistic semantics with IEML. This article introduces its 3000-word dictionary, its formal grammar, and its integrated tools for building semantic graphs. For the future, IEML could become a vector for a fluid calculation and communication of meaning—semantic interoperability—capable of de-compartmentalizing the digital memory, and of advancing the progress of collective intelligence, artificial intelligence, and digital humanities. I conclude by indicating some research directions.},
journal = {Collective Intelligence},
month = nov,
numpages = {28},
keywords = {Ieml, artificial intelligence, collective intelligence, semantics, Linguistics}
}

@inproceedings{10.1145/3539618.3591877,
author = {Zhan, Haolan and Li, Zhuang and Wang, Yufei and Luo, Linhao and Feng, Tao and Kang, Xiaoxi and Hua, Yuncheng and Qu, Lizhen and Soon, Lay-Ki and Sharma, Suraj and Zukerman, Ingrid and Semnani-Azad, Zhaleh and Haffari, Gholamreza},
title = {SocialDial: A Benchmark for Socially-Aware Dialogue Systems},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591877},
doi = {10.1145/3539618.3591877},
abstract = {Content Warning: this paper may contain content that is offensive or upsetting.Dialogue systems have been widely applied in many scenarios and are now more powerful and ubiquitous than ever before. With large neural models and massive available data, current dialogue systems have access to more knowledge than any people in their life. However, current dialogue systems still do not perform at a human level. One major gap between conversational agents and humans lies in their abilities to be aware of social norms. The development of socially-aware dialogue systems is impeded due to the lack of resources. In this paper, we present the first socially-aware dialogue corpus -- SocialDial based on Chinese social culture. SocialDial consists of two parts: 1,563 multi-turn dialogues between two human speakers with fine-grained labels, and 4,870 synthetic conversations generated by ChatGPT. The human corpus covers five categories of social norms, which have 14 sub-categories in total. Specifically, it contains social factor annotations including social relation, context, social distance, and social norms. However, collecting sufficient socially-aware dialogues is costly. Thus, we harness the power of ChatGPT and devise an ontology-based synthetic data generation framework. This framework is able to generate synthetic data at scale. To ensure the quality of synthetic dialogues, we design several mechanisms for quality control during data collection. Finally, we evaluate our dataset using several pre-trained models, such as BERT and RoBERTa. Comprehensive empirical results based on state-of-the-art neural models demonstrate that modeling of social norms for dialogue systems is a promising research direction. To the best of our knowledge, SocialDial is the first socially-aware dialogue dataset that covers multiple social factors and has fine-grained labels.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2712–2722},
numpages = {11},
keywords = {datasets, social norms, socially-aware dialogue},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@article{10.1145/3474829,
author = {Debruyne, Christophe and Munnelly, Gary and Kilgallon, Lynn and O’Sullivan, Declan and Crooks, Peter},
title = {Creating a Knowledge Graph for Ireland’s Lost History: Knowledge Engineering and Curation in the Beyond 2022 Project},
year = {2022},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3474829},
doi = {10.1145/3474829},
abstract = {The Beyond 2022 project aims to create a virtual archive by digitally reconstructing and digitizing historical records lost in a catastrophic fire which consumed items in the Public Record Office of Ireland in 1922. The project is developing a knowledge graph (KG) to facilitate information retrieval and discovery over the reconstructed items. The project decided to adopt Semantic Web technologies to support its distributed KG and reasoning. In this article, we present our approach to KG generation and management. We elaborate on how we help historians contribute to the KG (via a suite of spreadsheets) and its ontology. We furthermore demonstrate how we use named graphs to store different versions of factoids and their provenance information and how these are serviced in two different endpoints. Modeling data in this manner allows us to acknowledge that history is, to some extent, subjective and different perspectives can exist in parallel. The construction of the KG is driven by competency questions elicited from subject matter experts within the consortium. We avail of CIDOC-CRM as our KG’s foundation, though we needed to extend this ontology with various qualifiers (types) and relations to support the competency questions. We illustrate how one can explore the KG to gain insights and answer questions. We conclude that CIDOC-CRM provides an adequate, albeit complex, foundation for the KG and that named graphs and Linked Data principles are a suitable mechanism to manage sets of factoids and their provenance.},
journal = {J. Comput. Cult. Herit.},
month = apr,
articleno = {25},
numpages = {25},
keywords = {digital humanities, knowledge graph management, Knowledge graph creation}
}

@article{10.1145/3687486,
author = {Gagnon, Michel and Font, Ludovic and Zouaq, Amal},
title = {An Exploration of IFLA LRM for Literature Data Representation},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3687486},
doi = {10.1145/3687486},
abstract = {The digital humanities have witnessed a clear development in recent years due partly to their adoption of Semantic Web and linked data technologies and the creation of knowledge bases. In this work, we target the creation of an ontology and knowledge base for literature data representation based on the IFLA Library Reference Model (LRM). IFLA LRM is the main model for book-related data, allowing for a fine representation of the various layers that constitute a book. However, by design, it doesn’t deal with some aspects usually available in literature databases, such as information about authors, literary awards or book themes. As a result, LRM requires some extensions to be able to represent ancillary data. Another challenge is the querying of IFLA LRM knowledge bases, with a performance cost that comes with the fine-grained expressivity of the LRM model, which creates longer and therefore typically slower SPARQL queries. In this work, we propose an extension to the IFLA LRM ontology called IFLA LRM* that targets these limitations including a connection to the vocabulary Schema.org and to the taxonomies Thema and Dewey Decimal, and the representation of literary awards. We also present a practical case study on using our extended model to create a Quebec literature knowledge base, discussing the interest of our extensions.},
journal = {J. Comput. Cult. Herit.},
month = sep,
articleno = {50},
numpages = {21},
keywords = {Linked Open Data, IFLA LRM, Cultural Heritage, Literature}
}

@inproceedings{10.1145/3701551.3708813,
author = {Huang, Hao and Vidal, Maria-Esther},
title = {HyKG-CF: A Hybrid Approach for Counterfactual Prediction using Domain Knowledge},
year = {2025},
isbn = {9798400713293},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701551.3708813},
doi = {10.1145/3701551.3708813},
abstract = {Predictive models are gaining attention as powerful tools for aiding clinicians in diagnosis, prognosis, and treatment recommendations. However, their reliance on associative patterns may raise concerns about reliability of decision support, as association does not necessarily imply causation. To address this limit, we propose HyKG-CF, a hybrid approach to counterfactual prediction that leverages data and domain knowledge encoded in knowledge graph (KG). HyKG-CF integrates symbolic reasoning (on knowledge) with numerical learning (on data) using large language models (LLMs) and statistical models to learn causal Bayesian networks (CBNs) for accurate counterfactual prediction. Using data and knowledge, HyKG-CF improves the accuracy of causal discovery and counterfactual prediction. We evaluate HyKG-CF on a non-small cell lung cancer (NSCLC) KG, demonstrating that it outperforms other baselines. The results highlight the promise of combining domain knowledge with causal models to improve counterfactual prediction.},
booktitle = {Proceedings of the Eighteenth ACM International Conference on Web Search and Data Mining},
pages = {1104–1105},
numpages = {2},
keywords = {causality, counterfactual prediction, knowledge graphs},
location = {Hannover, Germany},
series = {WSDM '25}
}

@article{10.1145/3708504,
author = {Tsakalakis, Niko and Stalla-Bourdillon, Sophie and Huynh, Dong and Moreau, Luc},
title = {A typology of explanations to support Explainability-by-Design},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3708504},
doi = {10.1145/3708504},
abstract = {As automated decision-making permeates almost all aspects of everyday life, capabilities to generate meaningful explanations for various stakeholders (i.e., decision-makers, addressees of decisions including individuals, auditors, and regulators) should be carefully deployed. This article presents a typology of explanations intended to support the first pillar of an explainability-by-design strategy. Its production has been achieved by pursuing a responsible innovation approach and introducing a new persona within the research and innovation process, i.e., a legal engineer, whose role is to work at the interface of two teams, the compliance and the engineering teams, and to oversee the process of requirement elicitation, which is often opinionated and narrowing. Once explanation requirements have been derived from applicable regulatory requirements, compliance rules, or business policies, they have been mapped to the dimensions of the typology to produce fine-grained explanation requirements, forming computable building blocks that can then be translated into system requirements during the technical design phase. The typology has been co-created with industry partners operating in two sectors: finance and education. Two pilot studies have thus been conducted to test both the feasibility of the generation and computation of explanations on the basis of the typology and the usefulness of the outputs in the light of the state-of-the-art. The typology comprises nine hierarchical dimensions. It can be leveraged to operate a stand-alone classifier of explanations that acts as detective controls within a broader partially automated compliance strategy. A machine-readable format of the typology is provided in the form of a light ontology.},
journal = {ACM J. Responsib. Comput.},
month = feb,
articleno = {1},
numpages = {36},
keywords = {Artificial intelligence, explainability, typology, data protection, automated decisions}
}

@inproceedings{10.1145/3726302.3729880,
author = {Cai, Yongxin and Qiu, Jing and Zhang, Fan and Li, Qiang and Chen, Lei},
title = {A Knowledge Extraction Framework on Cyber Threat Reports with Enhanced Security Profiles},
year = {2025},
isbn = {9798400715921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726302.3729880},
doi = {10.1145/3726302.3729880},
abstract = {Knowledge extraction on Cyber Threat Reports (CTRs) is critical for attack investigation and defenses. The granularity and the usability of the knowledge are key issues: the former is determined by entity recognition on CTRs, whereas the latter mainly depends on proper relation extraction. Nevertheless, in the state-of-the-art entity recognition methods on CTRs using span representation, the local semantics of behavior are not considered and the sequential features of entity labels within behavior descriptions are not utilized. Besides, domain-specific definitions/forms of the relation types and knowledge representations are also crucial for effective utilization of knowledge. In this paper, we propose a novel knowledge extraction framework on CTRs to address the above concerns. The framework is formed by the Enhanced Security Profiles (ESP) that can be directly utilized by security detection devices. In the ESP framework, we propose 3 modules to facilitate fine-grained and accurate knowledge extractions: (1) The entity recognition module utilizes a label-aware subsequence autoregressive algorithm to integrate local semantic and label sequence features, enabling accurate identification of cybersecurity entities; (2) The relation extraction module employs LLM-based strategies with shared partition representations to enhance semantic understanding and domain relevance; and (3) The security profile generation module leverages Chain-of-Thought reasoning and In-Context Learning to produce machine-readable rules executable in security detection systems. Extensive experiments on 6 datasets demonstrate that the ESP framework largely outperform the state-of-the-art solutions e.g., the Micro-Fl scores on entity recognition and relation extraction are at least 1.54\% and 13.12\% better, respectively. Our code can be found in https://github.com/YxinMiracle/ESP.},
booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {326–336},
numpages = {11},
keywords = {entity recognition, knowledge extraction, relation extraction},
location = {Padua, Italy},
series = {SIGIR '25}
}

@article{10.1145/3704848,
author = {Choudhury, Vikraman and Gay, Simon J.},
title = {The Duality of λ-Abstraction},
year = {2025},
issue_date = {January 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {POPL},
url = {https://doi.org/10.1145/3704848},
doi = {10.1145/3704848},
abstract = {In this paper, we develop and study the following perspective -- just as higher-order functions give exponentials, higher-order continuations give coexponentials. From this, we design a language that combines exponentials and coexponentials, producing a duality of lambda abstraction.                                                                We formalise this language by giving an extension of a call-by-value simply-typed lambda-calculus with covalues, coabstraction, and coapplication. We develop the semantics of this language using the axiomatic structure of continuations, which we use to produce an equational theory, that gives a complete axiomatisation of control effects. We give a computational interpretation to this language using speculative execution and backtracking, and use this to derive the classical control operators and computational interpretation of classical logic, and encode common patterns of control flow using continuations. By dualising functional completeness, we further develop duals of first-order arrow languages using coexponentials. Finally, we discuss the implementation of this duality as control operators in programming, and develop some applications.},
journal = {Proc. ACM Program. Lang.},
month = jan,
articleno = {12},
numpages = {30},
keywords = {category theory, classical logic, continuations, control effects, control operators, curry-howard, denotational semantics, duality, equational theory, lambda-calculus, type theory}
}

@inproceedings{10.1145/3411763.3451619,
author = {Salminen, Joni and Jung, Soon-Gyo and Chhirang, Kamal and Jansen, Bernard},
title = {Instilling Knowledge Claims of Personas from 346 Research Articles},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451619},
doi = {10.1145/3411763.3451619},
abstract = {Our research goal is to summarize the body of persona knowledge by identifying knowledge claims. This can aid HCI researchers to (a) navigate persona knowledge to form an understanding of what is known about personas quickly, (b) identify central research gaps of what is not known (or said) about personas, and (c) identify claims that are not substantiated with strong empirical evidence and warrant future work. To this end, we use computational and manual techniques to extract 130 knowledge claims based on 9139 sentences from 346 persona articles and analyze whether the existing literature supports these claims. The results, clustered into four groups (“Definition”, “Creation”, “Evaluation”, and “Use”), indicate that claims regarding persona definition are characterized by a higher degree of consensus. In contrast, persona creation and use contain a high proportion of unverified claims. There are few claims concerning evaluation. Empirical research should address unverified claims and develop the ontological understanding on persona evaluation.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {450},
numpages = {9},
keywords = {Personas, knowledge claims, natural language processing, summary},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@inproceedings{10.1145/3587259.3630082,
author = {Van Erp, Marieke},
title = {Unflattening Knowledge Graphs},
year = {2023},
isbn = {9798400701412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587259.3630082},
doi = {10.1145/3587259.3630082},
abstract = {Large general-purpose knowledge graphs (KGs) are a critical component for knowledge-driven applications. However, most KGs represent only a limited view of the entities and concepts they describe. The concept coffee can, for example, refer to the plant that yields coffee seeds, the beverage ‘coffee’, and the activity of drinking the beverage. Moreover, it has a long history that is deeply connected to colonialism and status. All of these notions are an intricate part of national identities, have changed dramatically over time, and connect to many different narratives with different opinions on them. This complexity is not captured in current KGs. In this vision paper, I present the three crucial challenges for unflattening knowledge graphs and directions for future work.},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {223–224},
numpages = {2},
keywords = {digital humanities, knowledge graphs, language technology},
location = {Pensacola, FL, USA},
series = {K-CAP '23}
}

@inproceedings{10.1145/3379597.3387448,
author = {Tsay, Jason and Braz, Alan and Hirzel, Martin and Shinnar, Avraham and Mummert, Todd},
title = {AIMMX: Artificial Intelligence Model Metadata Extractor},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387448},
doi = {10.1145/3379597.3387448},
abstract = {Despite all of the power that machine learning and artificial intelligence (AI) models bring to applications, much of AI development is currently a fairly ad hoc process. Software engineering and AI development share many of the same languages and tools, but AI development as an engineering practice is still in early stages. Mining software repositories of AI models enables insight into the current state of AI development. However, much of the relevant metadata around models are not easily extractable directly from repositories and require deduction or domain knowledge. This paper presents a library called AIMMX that enables simplified AI Model Metadata eXtraction from software repositories. The extractors have five modules for extracting AI model-specific metadata: model name, associated datasets, references, AI frameworks used, and model domain. We evaluated AIMMX against 7,998 open-source models from three sources: model zoos, arXiv AI papers, and state-of-the-art AI papers. Our platform extracted metadata with 87\% precision and 83\% recall. As preliminary examples of how AI model metadata extraction enables studies and tools to advance engineering support for AI development, this paper presents an exploratory analysis for data and method reproducibility over the models in the evaluation dataset and a catalog tool for discovering and managing models. Our analysis suggests that while data reproducibility may be relatively poor with 42\% of models in our sample citing their datasets, method reproducibility is more common at 72\% of models in our sample, particularly state-of-the-art models. Our collected models are searchable in a catalog that uses existing metadata to enable advanced discovery features for efficiently finding models.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {81–92},
numpages = {12},
keywords = {Model Mining, Model Metadata, Model Catalog, Metadata Extraction, Machine Learning, Artificial Intelligence},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@article{10.1109/TASLP.2024.3426331,
author = {Gunasekara, Chulaka and Kim, Seokhwan and D'Haro, Luis Fernando and Rastogi, Abhinav and Chen, Yun-Nung and Eric, Mihail and Hedayatnia, Behnam and Gopalakrishnan, Karthik and Liu, Yang and Huang, Chao-Wei and Hakkani-T\"{u}r, Dilek and Li, Jinchao and Zhu, Qi and Luo, Lingxiao and Liden, Lars and Huang, Kaili and Shayandeh, Shahin and Liang, Runze and Peng, Baolin and Zhang, Zheng and Shukla, Swadheen and Huang, Minlie and Gao, Jianfeng and Mehri, Shikib and Feng, Yulan and Gordon, Carla and Alavi, Seyed Hossein and Traum, David and Eskenazi, Maxine and Beirami, Ahmad and Cho, Eunjoon and Crook, Paul A. and De, Ankita and Geramifard, Alborz and Kottur, Satwik and Moon, Seungwhan and Poddar, Shivani and Subba, Rajen},
title = {Overview of the Ninth Dialog System Technology Challenge: DSTC9},
year = {2024},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2024.3426331},
doi = {10.1109/TASLP.2024.3426331},
abstract = {This paper introduces the Ninth Dialog System Technology Challenge (DSTC-9). This edition of the DSTC focuses on applying end-to-end dialog technologies for four distinct tasks in dialog systems, namely, 1. Task-oriented dialog Modeling with Unstructured Knowledge Access, 2. Multi-domain task-oriented dialog, 3. Interactive evaluation of dialog and 4. Situated interactive multimodal dialog. This paper describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jul,
pages = {4066–4076},
numpages = {11}
}

@inproceedings{10.1145/3490725.3490737,
author = {Huang, Chao and Di, Hui and Wang, Lina and Ouchi, Kazushige},
title = {ECO-DST: An Efficient Cross-lingual Dialogue State Tracking Framework},
year = {2022},
isbn = {9781450384247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490725.3490737},
doi = {10.1145/3490725.3490737},
abstract = {Data efficiency is a critical challenge for cross-lingual task-oriented dialogue state tracking (DST) due to high cost of collecting large amount of task-related labeled training set for specific language. Therefore, we focus on adapting high-performance source language DST to target language by using only bilingual dictionary, without accessing labeled target data. We propose a novel data efficient cross-lingual DST framework (ECO-DST), which consists of cross-lingual encoder and language independent decoder. To support cross-lingual zero-shot adaptation, we leverage two advanced methods in encoder: 1) pre-trained cross-lingual model XLM-RoBERTa (XLM-R), 2) dynamic local phrase code-switching data augmentation for cross-lingual representation alignment. We evaluate the proposed method on The Ninth Dialogue System Technology Challenge (DSTC9) cross-lingual tasks. For target language DST, we compare our proposed framework with submitted systems in DSTC9, our model achieves state-of-the-art result on CrossWOZ dataset and promising result on MultiWOZ 2.1 dataset. Meanwhile on source language DST, the same model keeps competitive performance compared with original source DST model.},
booktitle = {Proceedings of the 2021 4th International Conference on Machine Learning and Machine Intelligence},
pages = {77–82},
numpages = {6},
keywords = {Task-oriented Dialogue, Dynamic Local Phrase Code-Switching, Dialogue State Tracking, Data Efficiency, Cross-Lingual Transfer},
location = {Hangzhou, China},
series = {MLMI '21}
}

@article{10.1145/3539223,
author = {Malviya, Shrikant and Kumar, Piyush and Namasudra, Suyel and Tiwary, Uma Shanker},
title = {Experience Replay-based Deep Reinforcement Learning for Dialogue Management Optimisation},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3539223},
doi = {10.1145/3539223},
abstract = {Dialogue policy is a crucial component in task-oriented Spoken Dialogue Systems (SDSs). As a decision function, it takes the current dialogue state as input and generates appropriate system’s response. In this paper, we explore the reinforcement learning approaches to solve this problem in an Indic language scenario. Recently, Deep Reinforcement Learning (DRL) has been used to optimise the dialogue policy. However, many DRL approaches are not sample-efficient. Hence, particular attention is given to actor-critic methods based on off-policy reinforcement learning that utilise the Experience Replay (ER) technique for reducing the bias and variance to achieve high sample efficiency. ER based actor-critic methods, such as Advantage Actor-Critic Experience Replay (A2CER) are proven to deliver competitive results in gaming environments that are fully observable and have a very small action-set. While, in SDSs, the states are not fully observable and often have to deal with the large action space. Describing the limitations of traditional methods, i.e., value-based and policy-based methods, such as high variance, low sample-efficiency, and often converging to local optima, we firstly explore the use of A2CER in dialogue policy learning. It is shown to beat the current state-of-the-art deep learning methods for SDS. Secondly, to handle the issues of early-stage performance, we utilise a demonstration corpus to pre-train the models prior to on-line policy learning. We thus experiment with the A2CER on a larger action space and find it significantly faster than the current state-of-the-art. Combining both approaches, we present a novel DRL based dialogue policy optimisation method, A2CER and its effectiveness for a task-oriented SDS in the Indic language.},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = may,
keywords = {deep reinforcement learning, dialogue management, Spoken dialogue systems}
}

@inproceedings{10.1145/3502223.3502227,
author = {Kume, Satoshi and Kozaki, Kouji},
title = {Extracting Domain-specific Concepts from Large-scale Linked Open Data},
year = {2022},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502227},
doi = {10.1145/3502223.3502227},
abstract = {We propose a methodology for extracting concepts for a target domain from large-scale linked open data (LOD) to support the construction of domain ontologies providing field-specific knowledge and definitions. The proposed method defines search entities by linking the LOD vocabulary with technical terms related to the target domain. The search entities are then used as a starting point for obtaining upper-level concepts in the LOD, and the occurrences of common upper-level entities and the chain-of-path relationships are examined to determine the range of conceptual connections in the target domain. A technical dictionary index and natural language processing are used to evaluate whether the extracted concepts cover the domain. As an example of extracting a class hierarchy from LOD, we used Wikidata to construct a domain ontology for polymer materials and physical properties. The proposed method can be applied to general datasets with class hierarchies, and it allows ontology developers to create an initial model of the domain ontology for their own purposes.},
booktitle = {Proceedings of the 10th International Joint Conference on Knowledge Graphs},
pages = {28–37},
numpages = {10},
keywords = {Wikidata, Ontology construction, Linked open data, Graph analysis, Domain ontology},
location = {Virtual Event, Thailand},
series = {IJCKG '21}
}

@article{10.1145/3652149,
author = {Singh, Upendra and Abhishek, Kumar and Azad, Hiteshwar Kumar},
title = {A Survey of Cutting-edge Multimodal Sentiment Analysis},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3652149},
doi = {10.1145/3652149},
abstract = {The rapid growth of the internet has reached the fourth generation, i.e., web 4.0, which supports Sentiment Analysis (SA) in many applications such as social media, marketing, risk management, healthcare, businesses, websites, data mining, e-learning, psychology, and many more. Sentiment analysis is a powerful tool for governments, businesses, and researchers to analyse users’ emotions and mental states in order to generate opinions and reviews about products, services, and daily activities. In the past years, several SA techniques based on Machine Learning (ML), Deep Learning (DL), and other soft computing approaches were proposed. However, growing data size, subjectivity, and diversity pose a significant challenge to enhancing the efficiency of existing techniques and incorporating current development trends, such as Multimodal Sentiment Analysis (MSA) and fusion techniques. With the aim of assisting the enthusiastic researcher to navigating the current trend, this article presents a comprehensive study of various literature to handle different aspects of SA, including current trends and techniques across multiple domains. In order to clarify the future prospects of MSA, this article also highlights open issues and research directions that lead to a number of unresolved challenges.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {227},
numpages = {38},
keywords = {Multimodal sentiment analysis, sentiment classifier, machine learning, emotion detection, modelling techniques}
}

@article{10.1145/3747321.3747323,
author = {Kostovska, Ana},
title = {Representing and Exploiting Benchmarking Data for Optimisation and Learning},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {2},
url = {https://doi.org/10.1145/3747321.3747323},
doi = {10.1145/3747321.3747323},
abstract = {The rapid advancements in Machine Learning (ML) and Black-Box Optimisation (BBO) have led to an increased reliance on benchmarking data for evaluating and comparing algorithms across diverse domain tasks. However, the effective exploitation of this data is hindered by challenges such as syntactic variability, semantic ambiguity, and lack of standardization. In this dissertation, we address these challenges by advocating for formal semantic representation of benchmarking data through the use of ontologies. By providing standardized vocabularies and ontologies, we improve knowledge sharing and promote data interoperability across studies in ML and BBO. In the ML domain, focusing on multi-label classification (MLC), we design an ontology-based framework for semantic annotation of benchmarking data, facilitating the creation of MLCBench - a semantic catalog that enhances data accessibility and reusability. In the BBO domain, we introduce the OPTION (OPTImization algorithm benchmarking ONtology) ontology to formally represent benchmarking data, including performance data, algorithm metadata, and problem landscapes. This ontology enables the automatic integration and interoperability of knowledge and data from diverse benchmarking studies.},
journal = {SIGEVOlution},
month = jul,
articleno = {2},
numpages = {4}
}

@inproceedings{10.1145/3589334.3645631,
author = {Zhao, Rui and Zhao, Jun},
title = {Perennial Semantic Data Terms of Use for Decentralized Web},
year = {2024},
isbn = {9798400701719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589334.3645631},
doi = {10.1145/3589334.3645631},
abstract = {In today's digital landscape, the Web has become increasingly centralized, raising concerns about user privacy violations. Decentralized Web architectures, such as Solid, offer a promising solution by empowering users with better control over their data in their personal 'Pods'. However, a significant challenge remains: users must navigate numerous applications to decide which application can be trusted with access to their data Pods. This often involves reading lengthy and complex Terms of Use agreements, a process that users often find daunting or simply ignore. This compromises user autonomy and impedes detection of data misuse. We propose a novel formal description of Data Terms of Use (DToU), along with a DToU reasoner. Users and applications specify their own parts of the DToU policy with local knowledge, covering permissions, requirements, prohibitions and obligations. Automated reasoning verifies compliance, and also derives policies for output data. This constitutes a "perennial'' DToU language, where the policy authoring only occurs once, and we can conduct ongoing automated checks across users, applications and activity cycles. Our solution is built on Turtle, Notation 3 and RDF Surfaces, for the language and the reasoning engine. It ensures seamless integration with other semantic tools for enhanced interoperability. We have successfully integrated this language into the Solid framework, and conducted performance benchmark. We believe this work demonstrates a practicality of a perennial DToU language and the potential of a paradigm shift to how users interact with data and applications in a decentralized Web, offering both improved privacy and usability.},
booktitle = {Proceedings of the ACM Web Conference 2024},
pages = {2238–2249},
numpages = {12},
keywords = {automated reasoning, data terms of use, decentralized web, formal modelling, notation 3, usage control},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3574318.3574338,
author = {Paul, Soumen and Saha, Rounak and Padhi, Swarup and Majumdar, Srijoni and Das, Partha Pratim and Rao, K Sreenivas},
title = {NrityaManch: An Annotation and Retrieval System for Bharatanatyam Dance},
year = {2023},
isbn = {9798400700231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3574318.3574338},
doi = {10.1145/3574318.3574338},
abstract = {This paper presents an annotation and retrieval application named NrityaManch dedicated explicitly to the Indian classical dance. We primarily choose Bharatanatyam dance for the application development. We exploit ontology technique which captures dance image’s annotation details and structurally organizes the dance database. An OWL2 ontology is developed in Prot\'{e}g\'{e} 5.5.0 which is validated using HermiT 1.4.3.456 reasoner to maintain consistency. A user interface is provided for the manual annotation of dance images. Initially, we focus on dancer details, dance details, and elements of static dance posture like hasta mudra during the annotation. All annotation details are saved in RDF/XML file. A search window is provided, which facilitates two types of search - natural language query search and tight query search. Named Entity Recognition (NER) pipeline mechanism is utilized in this work which facilitates keyword extraction from natural language queries. A SPARQL query is automatically generated by the system which is applied to the RDF corpus in order to retrieve distinct images. The NER pipeline mechanism achieves an accuracy of 80\% for our dance dataset. The system achieves an average f-score value of 0.8547 for the retrieval functionality. The proposed system intends to help dance learners to find dance resources in a dedicated place and will also help in Indian classical dance preservation.},
booktitle = {Proceedings of the 14th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {65–73},
numpages = {9},
keywords = {search, natural language query, dance retrieval, annotation, Bharatanatyam},
location = {Kolkata, India},
series = {FIRE '22}
}

@article{10.1109/TCBB.2016.2640303,
author = {Min, Wenwen and Liu, Juan and Zhang, Shihua},
title = {Network-Regularized Sparse Logistic Regression Models for Clinical Risk Prediction and Biomarker Discovery},
year = {2018},
issue_date = {May 2018},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
volume = {15},
number = {3},
issn = {1545-5963},
url = {https://doi.org/10.1109/TCBB.2016.2640303},
doi = {10.1109/TCBB.2016.2640303},
abstract = {Molecular profiling data e.g., gene expression has been used for clinical risk prediction and biomarker discovery. However, it is necessary to integrate other prior knowledge like biological pathways or gene interaction networks to improve the predictive ability and biological interpretability of biomarkers. Here, we first introduce a general regularized Logistic Regression LR framework with regularized term $lambda Vert boldsymbol {w}Vert _1 + eta boldsymbol {w}^Tboldsymbol {M}boldsymbol {w}$, which can reduce to different penalties, including Lasso, elastic net, and network-regularized terms with different $boldsymbol {M}$. This framework can be easily solved in a unified manner by a cyclic coordinate descent algorithm which can avoid inverse matrix operation and accelerate the computing speed. However, if those estimated $boldsymbol {w}_i$ and $boldsymbol {w}_j$ have opposite signs, then the traditional network-regularized penalty may not perform well. To address it, we introduce a novel network-regularized sparse LR model with a new penalty $lambda Vert boldsymbol {w}Vert _1 + eta |boldsymbol {w}|^Tboldsymbol {M}|boldsymbol {w}|$ to consider the difference between the absolute values of the coefficients. We develop two efficient algorithms to solve it. Finally, we test our methods and compare them with the related ones using simulated and real data to show their efficiency.},
journal = {IEEE/ACM Trans. Comput. Biol. Bioinformatics},
month = may,
pages = {944–953},
numpages = {10}
}

@inproceedings{10.1145/3643657.3643910,
author = {Cabrera, Christian and Paleyes, Andrei and Lawrence, Neil David},
title = {Self-sustaining Software Systems (S4): Towards Improved Interpretability and Adaptation},
year = {2024},
isbn = {9798400705601},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643657.3643910},
doi = {10.1145/3643657.3643910},
abstract = {Software systems impact society at different levels as they pervasively solve real-world problems. Modern software systems are often so sophisticated that their complexity exceeds the limits of human comprehension. These systems must respond to changing goals, dynamic data, unexpected failures, and security threats, among other variable factors in real-world environments. Systems' complexity challenges their interpretability and requires autonomous responses to dynamic changes. Two main research areas explore autonomous systems' responses: evolutionary computing and autonomic computing. Evolutionary computing focuses on software improvement based on iterative modifications to the source code. Autonomic computing focuses on optimising systems' performance by changing their structure, behaviour, or environment variables. Approaches from both areas rely on feedback loops that accumulate knowledge from the system interactions to inform autonomous decision-making. However, this knowledge is often limited, constraining the systems' interpretability and adaptability. This paper proposes a new concept for interpretable and adaptable software systems: self-sustaining software systems (S4). S4 builds knowledge loops between all available knowledge sources that define modern software systems to improve their interpretability and adaptability. This paper introduces and discusses the S4 concept.},
booktitle = {Proceedings of the 1st International Workshop on New Trends in Software Architecture},
pages = {5–9},
numpages = {5},
keywords = {autonomous systems, software engineering, knowledge graphs, data-oriented architectures, large language models},
location = {Lisbon, Portugal},
series = {SATrends '24}
}

@inproceedings{10.1145/3216122.3216155,
author = {McClatchey, Richard},
title = {The Deployment of an Enhanced Model-Driven Architecture for Business Process Management},
year = {2018},
isbn = {9781450365277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3216122.3216155},
doi = {10.1145/3216122.3216155},
abstract = {Business systems these days need to be agile to address the needs of a changing world. Business modelling requires process management to be highly adaptable with the ability to support dynamic workflows, inter-application integration (potentially between businesses) and process reconfiguration. Designing in the ability to cater for evolution is critical to success. To handle change, systems need the capability to adapt as and when necessary to changes in users' requirements. Using our implementation of a self-describing system, a so-called description-driven approach, new versions of data structures or processes can be created alongside older versions providing a log of changes to the underlying data schema and enabling the gathering of traceable ("provenance") data. The CRISTAL software, which originated at CERN for handling physics data, uses versions of stored descriptions to define data and workflows which can be evolved over time and thereby to handle evolving system needs. It has been customised for use in business as the Agilium-NG product. This paper reports on how the Agilium-NG software has enabled the deployment of an unique business process management solution that can be dynamically evolved to cater for changing user requirements.},
booktitle = {Proceedings of the 22nd International Database Engineering \&amp; Applications Symposium},
pages = {217–225},
numpages = {9},
keywords = {system evolution, business provenance, business process management, Description-driven systems},
location = {Villa San Giovanni, Italy},
series = {IDEAS '18}
}

@inproceedings{10.1145/3575882.3575912,
author = {Wardani, Dewi and Susmawati, Mauluah},
title = {SESS: Utilization of SPIN for Ethnomedicine Semantic Search},
year = {2023},
isbn = {9781450397902},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575882.3575912},
doi = {10.1145/3575882.3575912},
abstract = {Indonesia has biodiversity which is very beneficial for human life. Existing applications for ethnomedicine have been developed using conventional methods that only utilized SPARQL Protocol and RDF Query Language (SPARQL), so they still have limitations in representing knowledge and its retrieval. Those conventional methods are which based of relational database and ontology that has not utilized inference in its query process. Therefore, this work proposed SPIN for Enthnomedicine Semantic Search (SESS), a framework of the semantic search for medicinal plants that were developed by using SPIN (SPARQL Inferencing Notation). SESS has two main parts, the ontology design included SPARQL Inferencing Notation (SPIN) library and query process. The experiments were assessed in terms of execution time, query variation and accuracy. The obtained results showed a ratio of precision at 1, recall at 0.98 and the average value of the f-measure was 0.99. Utilizing SPIN also decrease the time consuming to obtain the result by around .},
booktitle = {Proceedings of the 2022 International Conference on Computer, Control, Informatics and Its Applications},
pages = {153–157},
numpages = {5},
keywords = {spin, sparql, semantic search, owl, ontology, ethnomedicine},
location = {Virtual Event, Indonesia},
series = {IC3INA '22}
}

@inproceedings{10.1145/3625156.3625159,
author = {Wang, Hongwei and Zhang, Ziling and Liu, Xiuhua and Cao, Mengyuan},
title = {Knowledge Graph Completion Using Multiple Embedding Representations for Intelligent Information Extraction from Technical Reports},
year = {2023},
isbn = {9798400708206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625156.3625159},
doi = {10.1145/3625156.3625159},
abstract = {As a new data structure, knowledge graphs are widely used in search engines, recommendation systems, question and answer systems and other related fields. The knowledge graph is helpful to realize intelligent and digital knowledge service of nuclear power accidents, in which link prediction can solve the discovery and restoration of missing information in Knowledge graph. This is also one of the research hotspot in the field of knowledge graph applications. Currently, the text description of entity information is rarely considered in the completion of nuclear power Knowledge graph. In this paper, we propose MEK-ConvKB (Multi-Embedding Knowledge Graph Prediction based on ConvKB), a reasoning model combined with multi-embedding techniques to improve performance. The embedded expression of Knowledge graph is enhanced by text description in nuclear power accident reports, which improves the accuracy of link prediction and expands the Knowledge graph of nuclear power accidents. The results show that our model can effectively express the semantic association between entities, Our model achieved the best performance compared to the baseline methods., which can provide a research basis for solving the discovery and restoration of missing information in knowledge graphs.},
booktitle = {Proceedings of the 2023 6th International Conference on Information Science and Systems},
pages = {15–21},
numpages = {7},
location = {Edinburgh, United Kingdom},
series = {ICISS '23}
}

@inproceedings{10.1145/3523227.3547412,
author = {Anelli, Vito Walter and Basile, Pierpaolo and de Melo, Gerard and Donini, Francesco Maria and Ferrara, Antonio and Musto, Cataldo and Narducci, Fedelucio and Ragone, Azzurra and Zanker, Markus},
title = {Fourth Knowledge-aware and Conversational Recommender Systems Workshop (KaRS)},
year = {2022},
isbn = {9781450392785},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3523227.3547412},
doi = {10.1145/3523227.3547412},
abstract = {In the last few years, a renewed interest of the research community in conversational recommender systems (CRSs) has been emerging. This is likely due to the massive proliferation of Digital Assistants (DAs) such as Amazon Alexa, Siri, or Google Assistant that are revolutionizing the way users interact with machines. DAs allow users to execute a wide range of actions through an interaction mostly based on natural language utterances. However, although DAs are able to complete tasks such as sending texts, making phone calls, or playing songs, they still remain at an early stage in terms of their recommendation capabilities via a conversation. In addition, we have been witnessing the advent of increasingly precise and powerful recommendation algorithms and techniques able to effectively assess users’ tastes and predict information that may be of interest to them. Most of these approaches rely on the collaborative paradigm (often exploiting machine learning techniques) and neglect the huge amount of knowledge, both structured and unstructured, describing the domain of interest of a recommendation engine. Although very effective in predicting relevant items, collaborative approaches miss some very interesting features that go beyond the accuracy of results and move in the direction of providing novel and diverse results as well as generating explanations for recommended items. Knowledge-aware side information becomes crucial when a conversational interaction is implemented, in particular for preference elicitation, explanation, and critiquing steps.},
booktitle = {Proceedings of the 16th ACM Conference on Recommender Systems},
pages = {663–666},
numpages = {4},
keywords = {Conversational Agents, Knowledge Graphs, Knowledge Representation, Natural Language Processing, Recommender systems, Semantic Web},
location = {Seattle, WA, USA},
series = {RecSys '22}
}

@inproceedings{10.5555/3586210.3586504,
author = {Weaver, Gabriel A. and Shusko, Jacob and Hasenbein, John J. and Kutanoglu, Erhan and Martinez-Medina, Gonzalo and Castillo-Villar, Krystel K. and Costa, Paulo C. G.},
title = {Simulating Energy and Security Interactions in Semiconductor Manufacturing: Insights from the Intel Minifab Model},
year = {2023},
publisher = {IEEE Press},
abstract = {Semiconductor manufacturing, particularly wafer fabrication, is a highly complex system of processes and workflows. Fabrication facilities must deal with re-entrant flows to support multiple types of wafers being produced simultaneously, each with their own deadlines and specifications. The manufacturing process itself depends upon the ability to control and programmatically adjust a variety of environmental conditions. In addition, wafer fabrication consumes large amounts of energy, particularly electricity. Emerging technologies including networked devices may help reduce the energy footprint but can introduce cybersecurity risks. Therefore, this paper presents its modeling and simulation framework to quantify tradeoffs between operational measures of performance, energy consumption, and cybersecurity risks. We augment the Intel Minifab model with an Industrial Control Systems (ICS) reference model based on the Purdue Enterprise Reference Architecture (PERA) as well as tool-level energy consumption data from a semiconductor manufacturing testbed.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3477–3488},
numpages = {12},
location = {Singapore, Singapore},
series = {WSC '22}
}

@article{10.1145/3183639.3183641,
author = {Zhang, Jie and Mitrovic, Tanja and Chin, David and Chen, Li},
title = {ACM UMAP 2018 - User Modeling, Adaptation and Personalization: 8-11 July, 2018 at NTU, Singapore},
year = {2018},
issue_date = {Winter 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2018},
number = {Winter},
issn = {1931-1745},
url = {https://doi.org/10.1145/3183639.3183641},
doi = {10.1145/3183639.3183641},
abstract = {UMAP (User Modeling, Adaptation and Personalization) is the premier international conference for researchers and practitioners working on systems that adapt to individual users, to groups of users, and that collect, represent, and model user information. UMAP is the successor to the biennial User Modeling (UM) and Adaptive Hypermedia and Adaptive Web-based Systems (AH) conferences that were merged in 2009. It is sponsored by ACM SIGCHI and SIGWEB, and organized under the auspices of User Modeling Inc. The proceedings are published by ACM and will be part of the ACM Digital Library.},
journal = {SIGWEB Newsl.},
month = mar,
articleno = {2},
numpages = {5}
}

@inproceedings{10.1145/3632410.3632489,
author = {Mitra, Aniket and Venugopal, Vinu},
title = {Enhancing Region-Based Geometric Embedding for Gene-Disease Associations},
year = {2024},
isbn = {9798400716348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632410.3632489},
doi = {10.1145/3632410.3632489},
abstract = {In recent times, Geometric Knowledge Graph Embedding (KGE) methods that focus on regions have proven valuable for effectively representing structured knowledge, such as ontologies, in various reasoning tasks. Nevertheless, transforming the richly expressive semantics present in ontologies may require a transition to less expressive interim representations before initiating the actual embedding process. In our research, we explore current approaches and offer recommendations to enhance the state-of-the-art, drawing from extensive experiments conducted on the Human Phenotype Ontology (HPO) for predicting gene-disease (g-d) associations. Our findings indicate that incorporating these new suggestions can lead to results that outperform other leading KGE models.},
booktitle = {Proceedings of the 7th Joint International Conference on Data Science \&amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)},
pages = {584–585},
numpages = {2},
keywords = {EL++ OWL, Knowledge Graph Embedding, n-ball Embedding},
location = {Bangalore, India},
series = {CODS-COMAD '24}
}

@inproceedings{10.1145/3338501.3357365,
author = {Alperin, Kenneth and Wollaber, Allan and Ross, Dennis and Trepagnier, Pierre and Leonard, Leslie},
title = {Risk Prioritization by Leveraging Latent Vulnerability Features in a Contested Environment},
year = {2019},
isbn = {9781450368339},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338501.3357365},
doi = {10.1145/3338501.3357365},
abstract = {Cyber network defenders face an overwhelming volume of software vulnerabilities. Resource limitations preclude them mitigating all but a small number of vulnerabilities on an enterprise network, so proper prioritization of defensive actions are of paramount importance. Current methods of risk prioritization are predominantly expert-based, and many include leveraging Common Vulnerability Scoring System (CVSS) risk scores. These scores are assigned by subject matter experts according to conventional methods of qualifying risk. Vulnerability mitigation strategies are then often applied in CVSS score order. Our vulnerability assessment system, in contrast, takes a predominantly data-driven approach. In general, we associate a risk metric of vulnerabilities with existence of corresponding exploits. Our assumption is that if an entity has invested time and money to exploit a particular vulnerability, this is a critical gauge of that vulnerability's importance, and hence risk.Prior work presented a model that allows for the creation of prioritized vulnerabilities based on their association-likelihood with exploits, outperforming then-current methods. Because the initial approach only leveraged one vulnerability feature, we extended the vulnerability feature space by incorporating additional features derived from natural language processing. The importance metric is still given by a vulnerability-exploit relationship, but by processing text descriptions and other available information, our system became significantly more accurate and predictive. We next propose a mechanism that customizes vulnerability risks according to their exploitation likelihood in a contested environment given site-specific threat intelligence information, namely, attacks by an Advanced Persistent Threat (APT) group. Utilizing held-back data, we then demonstrate that latently similar vulnerabilities, which could be targeted by the same adversary, see higher risk ratings.},
booktitle = {Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security},
pages = {49–57},
numpages = {9},
keywords = {vulnerability, risk model, natural language processing, machine learning, exploit},
location = {London, United Kingdom},
series = {AISec'19}
}

@inproceedings{10.1145/3686169.3686204,
author = {Quesnel, Denise T. and Losev, Tatiana and Stepanova, Ekaterina R. and Carpendale, Sheelagh and Riecke, Bernhard E.},
title = {The Inbetweeny Collective: Reflexive Dialogues on the Liminality of Researchers' Lived Experiences},
year = {2024},
isbn = {9798400710421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3686169.3686204},
doi = {10.1145/3686169.3686204},
abstract = {In this collaborative autoethnography, we critically explore our lived experiences within a wider context of HCI research and practice. We reflect on the epistemological ways of knowing through ‘insider’ lived experience of, and ‘outsider’ knowledge of our research topic(s) via the concept of “liminal space” as a process ontology. To embrace liminality entails inhabiting the space ‘in-between’ these ways of knowing, suspended on a threshold of uncertainty and transformative growth. All authors identify as ‘inbetweenies’, because we are neither just ‘insiders’ nor ‘outsiders’, and we collect our respective stories to share. Drawing from these stories and our dialogues, we discuss how ways of knowing have historically been dichotomously categorized with their associated subjective or objective characterizations, resulting in power hierarchies and tensions. We propose that for an ‘inbetweeny’ researcher, thoughtful approaches to navigating this liminal space could potentially bridge persistent tensions in HCI research and practices toward personal and systemic transformation. Five areas of reflection are discussed, with proposed learnings that can be applied towards sustained practices for individuals and collectives at any stage of their journey and development.},
booktitle = {Proceedings of the Halfway to the Future Symposium},
articleno = {6},
numpages = {10},
keywords = {Autoethnography, Critical Reflexivity, Design Research, Experiential Knowledge, Human-Computer Interaction, Insider and Outsider Research, Liminality, Lived Experience, Philosophy, Process Ontology, Research Methodology},
location = {Santa Cruz, CA, USA},
series = {HttF '24}
}

@inproceedings{10.1145/3620666.3651344,
author = {Bisbas, George and Lydike, Anton and Bauer, Emilien and Brown, Nick and Fehr, Mathieu and Mitchell, Lawrence and Rodriguez-Canal, Gabriel and Jamieson, Maurice and Kelly, Paul H. J. and Steuwer, Michel and Grosser, Tobias},
title = {A shared compilation stack for distributed-memory parallelism in stencil DSLs},
year = {2024},
isbn = {9798400703867},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620666.3651344},
doi = {10.1145/3620666.3651344},
abstract = {Domain Specific Languages (DSLs) increase programmer productivity and provide high performance. Their targeted abstractions allow scientists to express problems at a high level, providing rich details that optimizing compilers can exploit to target current- and next-generation supercomputers. The convenience and performance of DSLs come with significant development and maintenance costs. The siloed design of DSL compilers and the resulting inability to benefit from shared infrastructure cause uncertainties around longevity and the adoption of DSLs at scale. By tailoring the broadly-adopted MLIR compiler framework to HPC, we bring the same synergies that the machine learning community already exploits across their DSLs (e.g. Tensorflow, PyTorch) to the finite-difference stencil HPC community. We introduce new HPC-specific abstractions for message passing targeting distributed stencil computations. We demonstrate the sharing of common components across three distinct HPC stencil-DSL compilers: Devito, PSyclone, and the Open Earth Compiler, showing that our framework generates high-performance executables based upon a shared compiler ecosystem.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {38–56},
numpages = {19},
keywords = {message passing, MPI, MLIR, SSA, domain-specific languages, intermediate representations, stencil computations},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@inproceedings{10.1145/3511808.3557520,
author = {Fissore, Giancarlo and Vasiloglou, Nikolaos},
title = {Simulating Complex Problems Inside a Database},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557520},
doi = {10.1145/3511808.3557520},
abstract = {The standard way to store and interact with the large amount of data that are central to the functioning of any modern business is through the use of a relational Knowledge Graph Management System (KGMS). In this paper we show how the relational model can be successfully exploited to model complex analytic scenarios while enjoying the same characteristics of clarity and flexibility as when modeling the data themselves. Using the Rel language, we simulate the daily schedule of an airline company as an agentbased system, and we will show how modeling this system through a set of relationships and logical rules will let us focus directly on the inherent complexity of our model, taking away most of the incidental effort in actually implementing our simulation.},
booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
pages = {5086–5087},
numpages = {2},
keywords = {simulation, database, agent based modelling},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@inproceedings{10.1145/3660853.3660886,
author = {Tabaza, Abdulrahman and Quishawi, Omar and Yaghi, Abdelrahman and Qawasmeh, Omar},
title = {Binding Text, Images, Graphs, and Audio for Music Representation Learning},
year = {2024},
isbn = {9798400716928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3660853.3660886},
doi = {10.1145/3660853.3660886},
abstract = {Abstract In the field of Information Retrieval and Natural Language Processing, text embeddings play a significant role in tasks such as classification, clustering, and topic modeling. However, extending these embeddings to abstract concepts such as music, which involves multiple modalities, presents a unique challenge. Our work addresses this challenge by integrating rich multi-modal data into a unified joint embedding space. This space includes: (1) textual, (2) visual, (3) acoustic, and (4) graph-based modality features. By doing so, we mirror cognitive processes associated with music interaction and overcome the disjoint nature of individual modalities. The resulting joint low-dimensional vector space facilitates retrieval, clustering, embedding space arithmetic, and cross-modal retrieval tasks. Importantly, our approach carries implications for music information retrieval and recommendation systems. Furthermore, we propose a novel multi-modal model that integrates various data types—text, images, graphs, and audio—for music representation learning. Our model aims to capture the complex relationships between different modalities, enhancing the overall understanding of music. By combining textual descriptions, visual imagery, graph-based structures, and audio signals, we create a comprehensive representation that can be leveraged for a wide range of music-related tasks. Notably, our model demonstrates promising results in music classification, and recommendation systems. Code Availability: The source code for the multi-modal music representation model described in this paper is available on GitHub. Access and further details can be found at the following repository link: //github.com/a-tabaza/binding_music/},
booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference},
pages = {139–146},
numpages = {8},
location = {undefinedstanbul, Turkiye},
series = {AICCONF '24}
}

@inproceedings{10.1145/3550356.3561602,
author = {Jeusfeld, Manfred and Mezei, Gergely and B\'{a}csi, S\'{a}ndor},
title = {DeepTelos and DMLA: a contribution to the MULTI 2022 collaborative comparison challenge},
year = {2022},
isbn = {9781450394673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3550356.3561602},
doi = {10.1145/3550356.3561602},
abstract = {The MULTI 2022 Collaborative Comparison Challenge was created to promote in-depth discussion between multi-level modeling approaches. This paper presents a comparison of DeepTelos- and DMLA-based solutions in response to the challenge. We first present each approach and solution separately, and then list the similarities and differences between the two solutions, discussing their relative strengths and weaknesses.},
booktitle = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {414–423},
numpages = {10},
keywords = {multi-level modeling, collaborative challenge, DeepTelos, DMLA},
location = {Montreal, Quebec, Canada},
series = {MODELS '22}
}

@inproceedings{10.1145/3639233.3639335,
author = {Nuipian, Vatinee and Chuaykhun, Jirawat},
title = {Book Recommendation System based on Course Descriptions using Cosine Similarity},
year = {2024},
isbn = {9798400709227},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3639233.3639335},
doi = {10.1145/3639233.3639335},
abstract = {Ensuring the retrieval of books that match users' preferences is of paramount importance. A significant challenge users encounter is uncertainty regarding their choice of search terms, often stemming from a limited understanding of the content or exposure to new concepts. Offering users results that closely resemble their query represents one potential solution. This research aims to suggest books relevant to students' course topics, utilizing cosine similarity to compute similarity values within each document in the collection.Performance evaluation using a similarity threshold greater than 0.1 revealed that the retrieved book results achieved an average precision of 0.7 and a recall value of 0.73, indicating substantial alignment with the search terms. The anticipated benefits of the recommendation system encompass the elimination of the need for manual book suggestions by staff, the provision of personalized book recommendations tailored to readers' preferences, a deeper understanding of library user behavior, and the effective promotion of new books that align with users' interests.},
booktitle = {Proceedings of the 2023 7th International Conference on Natural Language Processing and Information Retrieval},
pages = {273–277},
numpages = {5},
keywords = {Book Recommendation, Cosine similarity, Course Descriptions, Text mining},
location = {Seoul, Republic of Korea},
series = {NLPIR '23}
}

@inproceedings{10.5555/3712729.3712833,
author = {Anagnostou, Anastasia and Brailford, Sally and Eldabi, Tillal and Mustafee, Navonil and Tako, Antuela},
title = {Ten Years of the Hybrid Simulation Track: Reflections and Vision for the Future},
year = {2025},
isbn = {9798331534202},
publisher = {IEEE Press},
abstract = {The Hybrid Simulation (HS) track was included in the Winter Simulation Conference (WSC) proceedings as a full conference track for the first time in 2014. A decade has passed since that inaugural track, and HS research and practice has seen impressive advancements during this time. This paper, based on a high-level review of the published works in the last ten years of the HS track, reflects on its successes and challenges and sets the scene for the future of the field. The paper is authored by the HS track organizers, both past and present, who report on the track's history, the nature of HS applications, the modeling tools and software available, as well as implementation challenges and the users' perspective. Finally, the paper discusses the future of HS.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {1245–1259},
numpages = {15},
location = {Orlando, Florida, USA},
series = {WSC '24}
}

@inproceedings{10.1145/3591106.3592223,
author = {Nebbia, Giacomo and Kovashka, Adriana},
title = {Hypernymization of named entity-rich captions for grounding-based multi-modal pretraining},
year = {2023},
isbn = {9798400701788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591106.3592223},
doi = {10.1145/3591106.3592223},
abstract = {Named entities are ubiquitous in text that naturally accompanies images, especially in domains such as news or Wikipedia articles. In previous work, named entities have been identified as a likely reason for low performance of image-text retrieval models pretrained on Wikipedia and evaluated on named entities-free benchmark datasets. Because they are rarely mentioned, named entities could be challenging to model. They also represent missed learning opportunities for self-supervised models: the link between named entity and object in the image may be missed by the model, but it would not be if the object were mentioned using a more common term. In this work, we investigate hypernymization as a way to deal with named entities for pretraining grounding-based multi-modal models and for fine-tuning on open-vocabulary detection. We propose two ways to perform hypernymization: (1) a “manual” pipeline relying on a comprehensive ontology of concepts, and (2) a “learned” approach where we train a language model to learn to perform hypernymization. We run experiments on data from Wikipedia and from The New York Times. We report improved pretraining performance on objects of interest following hypernymization, and we show the promise of hypernymization on open-vocabulary detection, specifically on classes not seen during training.},
booktitle = {Proceedings of the 2023 ACM International Conference on Multimedia Retrieval},
pages = {67–75},
numpages = {9},
keywords = {grounding, hypernymization, named entities, open-vocabulary detection},
location = {Thessaloniki, Greece},
series = {ICMR '23}
}

@article{10.1145/3586075,
author = {Das, Ringki and Singh, Thoudam Doren},
title = {Multimodal Sentiment Analysis: A Survey of Methods, Trends, and Challenges},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3586075},
doi = {10.1145/3586075},
abstract = {Sentiment analysis has come long way since it was introduced as a natural language processing task nearly 20 years ago. Sentiment analysis aims to extract the underlying attitudes and opinions toward an entity. It has become a powerful tool used by governments, businesses, medicine, marketing, and others. The traditional sentiment analysis model focuses mainly on text content. However, technological advances have allowed people to express their opinions and feelings through audio, image and video channels. As a result, sentiment analysis is shifting from unimodality to multimodality. Multimodal sentiment analysis brings new opportunities with the rapid increase of sentiment analysis as complementary data streams enable improved and deeper sentiment detection which goes beyond text-based analysis. Audio and video channels are included in multimodal sentiment analysis in terms of broadness. People have been working on different approaches to improve sentiment analysis system performance by employing complex deep neural architectures. Recently, sentiment analysis has achieved significant success using the transformer-based model. This paper presents a comprehensive study of different sentiment analysis approaches, applications, challenges, and resources then concludes that it holds tremendous potential. The primary motivation of this survey is to highlight changing trends in the unimodality to multimodality for solving sentiment analysis tasks.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {270},
numpages = {38},
keywords = {transfer learning, audio sentiment analysis, image sentiment analysis, text sentiment analysis, Multimodal sentiment analysis}
}

@inproceedings{10.1145/3706599.3716239,
author = {Sarkar, Advait},
title = {AI Could Have Written This: Birth of a Classist Slur in Knowledge Work},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3716239},
doi = {10.1145/3706599.3716239},
abstract = {AI shaming is a social phenomenon in which negative judgements are associated with the use of Artificial Intelligence (AI). This includes comparing someone’s work with AI-generated work as a means of disparagement, voicing suspicion or alleging that someone has used AI to undermine their reputation, or blaming the poor quality of an artefact on AI use. Common justifications of AI shaming include recourse to AI’s societal harms, its technical limitations, and lack of creativity. I argue that, more fundamentally than any of these, AI shaming arises from a class anxiety induced in middle class knowledge workers, and is a form of boundary work to maintain class solidarity and limit mobility into knowledge work. I discuss the role of AI shaming in protecting the privileged class of knowledge work and its attendant harms.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {621},
numpages = {12},
keywords = {class identity, sumptuary laws, protectionism, epistemic injustice, colonialism, sociotechnical imaginaries, moral panic},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3394486.3403330,
author = {Li, Ying and Zakhozhyi, Vitalii and Zhu, Daniel and Salazar, Luis J.},
title = {Domain Specific Knowledge Graphs as a Service to the Public: Powering Social-Impact Funding in the US},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394486.3403330},
doi = {10.1145/3394486.3403330},
abstract = {Web and mobile technologies enable ubiquitous access to information. Yet, it is getting harder, even for subject matter experts, to quickly identify quality, trustworthy, and reliable content available online through search engines powered by advanced knowledge graphs. This paper explores the practical applications of Domain Specific Knowledge Graphs that allow for the extraction of information from trusted published and unpublished sources, to map the extracted information to an ontology defined in collaboration with sector experts, and to enable the public to go from single queries into ongoing conversations meeting their knowledge needs reliably. We focused on Social-Impact Funding, an area of need for over one million nonprofit organizations, foundations, government entities, social entrepreneurs, impact investors, and academic institutions in the US.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
pages = {2793–2801},
numpages = {9},
keywords = {social-impact funding, domain specific knowledge graph, domain ontology},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@inproceedings{10.5555/3378680.3378700,
author = {Oliveira, Raquel and Arriaga, Patr\'{\i}cia and Correia, Filipa and Paiva, Ana},
title = {The stereotype content model applied to human-robot interactions in groups},
year = {2020},
isbn = {9781538685556},
publisher = {IEEE Press},
abstract = {In this paper we sought to understand how the display of different levels of warmth and competence, as well as, different roles (opponent versus partner) portrayed by a robot, affect the display of emotional responses towards robots and how they can be used to predict future intention to work. For this purpose we devised an entertainment card-game group scenario involving two humans and two robots (n=54). The results suggest that different levels of warmth and competence are associated with distinct emotional responses from users and that these variables are useful in predicting future intention to work, thus hinting at the importance of considering warmth and competence stereotypes in Human-Robot Interaction.},
booktitle = {Proceedings of the 14th ACM/IEEE International Conference on Human-Robot Interaction},
pages = {123–132},
numpages = {10},
keywords = {stereotypes, human-robot interaction, emotions, autonomous robots},
location = {Daegu, Republic of Korea},
series = {HRI '19}
}

@inproceedings{10.1145/3360901.3364428,
author = {Mecharnia, Thamer and Chibout Khelifa, Lydia and Pernelle, Nathalie and Hamdi, Fay\c{c}al},
title = {An Approach Toward a Prediction of the Presence of Asbestos in Buildings Based on Incomplete Temporal Descriptions of Marketed Products},
year = {2019},
isbn = {9781450370080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3360901.3364428},
doi = {10.1145/3360901.3364428},
abstract = {Since 1997, the production, import and sale of asbestosfootnoteNaturally occurring mineral fibres which were used due to their insulating properties. have been banned in France. However, there are still millions of tons scattered in factories, buildings, or hospitals. In this paper we propose a method for predicting the presence of asbestos products in buildings based on temporal data that describes the probability of the presence of asbestos in marketed products.},
booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
pages = {239–242},
numpages = {4},
keywords = {uncertain information, temporal data, prediction, ontology},
location = {Marina Del Rey, CA, USA},
series = {K-CAP '19}
}

@inproceedings{10.1145/3594536.3595168,
author = {Habba, Eliya and Keydar, Renana and Bareket, Dan and Stanovsky, Gabriel},
title = {The Perfect Victim: Computational Analysis of Judicial Attitudes towards Victims of Sexual Violence},
year = {2023},
isbn = {9798400701979},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3594536.3595168},
doi = {10.1145/3594536.3595168},
abstract = {We develop computational models to analyze court statements in order to assess judicial attitudes toward victims of sexual violence in the Israeli court system. The study examines the resonance of "rape myths" in the criminal justice system's response to sex crimes, in particular in judicial assessment of victim's credibility. We begin by formulating an ontology for evaluating judicial attitudes toward victim's credibility, with eight ordinal labels and binary categorizations. Second, we curate a manually annotated dataset for judicial assessments of victim's credibility in the Hebrew language, as well as a model that can extract credibility labels from court cases. The dataset consists of 855 verdict decision documents in sexual assault cases from 1990-2021, annotated with the help of legal experts and trained law students. The model uses a combined approach of syntactic and latent structures to find sentences that convey the judge's attitude towards the victim and classify them according to the credibility label set. Our ontology, data, and models will be made available upon request, in the hope they spur future progress in this judicial important task.},
booktitle = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law},
pages = {111–120},
numpages = {10},
keywords = {Judicial decision making, Rape myths, Sexual violence, Witness credibility},
location = {Braga, Portugal},
series = {ICAIL '23}
}

@inproceedings{10.5555/3427510.3427540,
author = {Kon\'{e}, Youssouf and Ma\"{\i}ga, Oumar and Traor\'{e}, Mamadou K.},
title = {Using hills as a common concrete syntax for ses and devs: application to microscopic simulation of traffic},
year = {2020},
isbn = {9781713814290},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {We propose to use the High Level Language for System Specification as a common visual language for DEVS-based simulation model specification and SES-based domain ontological description. In doing so, we ensure continuity in knowledge representation and therefore reduces accidental errors while compressing the time to obtain the simulation model from the domain knowledge. This approach also offers a documentation basis to objective-based selection of elements of the ontological representation. We use a traffic modeling case to illustrate the effectiveness of our conceptual proposal.},
booktitle = {Proceedings of the 2020 Summer Simulation Conference},
articleno = {29},
numpages = {11},
keywords = {system entity structure /model base (SES/MB), microscopic traffic modeling, high level language for system specification (HiLLS), discrete event systems specification (DEVS)},
location = {Virtual Event, Spain},
series = {SummerSim '20}
}

@inproceedings{10.1145/3539618.3591904,
author = {Bonisoli, Giovanni and Di Buono, Maria Pia and Po, Laura and Rollo, Federica},
title = {DICE: a Dataset of Italian Crime Event news},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591904},
doi = {10.1145/3539618.3591904},
abstract = {Extracting events from news stories as the aim of several Natural Language Processing (NLP) applications (e.g., question answering, news recommendation, news summarization) is not a trivial task, due to the complexity of natural language and the fact that news reporting is characterized by journalistic style and norms. Those aspects entail scattering an event description over several sentences within one document (or more documents), applying a mechanism of gradual specification of event-related information. This implies a widespread use of co-reference relations among the textual elements, conveying non-linear temporal information. In addition to this, despite the achievement of state-of-the-art results in several tasks, high-quality training datasets for non-English languages are rarely available.This paper presents our preliminary study to develop an annotated Dataset for Italian Crime Event news (DICE). The contribution of the paper are: (1) the creation of a corpus of 10,395 crime news; (2) the annotation schema; (3) a dataset of 10,395 news with automatic annotations; (4) a preliminary manual annotation using the proposed schema of 1000 documents. The first tests on DICE have compared the performance of a manual annotator with that of single-span and multi-span question answering models and shown there is still a gap in the models, especially when dealing with more complex annotation tasks and limited training data. This underscores the importance of investing in the creation of high-quality annotated datasets like DICE, which can provide a solid foundation for training and testing a wide range of NLP models.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2985–2995},
numpages = {11},
keywords = {5ws, crime news, event extraction, nlp, question answering},
location = {Taipei, Taiwan},
series = {SIGIR '23}
}

@inproceedings{10.1145/3267851.3267895,
author = {Querrec, Ronan and Taoum, Joanna and Nakhal, Bilal and Bevacqua, Elisabetta},
title = {Model for Verbal Interaction between an Embodied Tutor and a Learner in Virtual Environments},
year = {2018},
isbn = {9781450360135},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267851.3267895},
doi = {10.1145/3267851.3267895},
abstract = {This research work introduce virtual embodied tutors in Virtual Environments for learning devoted to learning of procedures for industrial systems. We present a communicative behavior which, integrated in pedagogical scenario, permits on the one hand to realize the pedagogical communicative actions at a semantic level (e.g., the tutor explains the goal of an action) and on the other hand to realize such actions through human-like communicative channels (i.e., the virtual tutor's voice, facial expressions and gestures). The communicative behavior relies on a taxonomy of questions in order to interpret the learner's communicative actions and to generate the tutor's own questions.},
booktitle = {Proceedings of the 18th International Conference on Intelligent Virtual Agents},
pages = {197–202},
numpages = {6},
keywords = {Virtual Learning Environment, Verbal Interaction, Interface, Intelligent Tutoring System, Embodied Conversational Agent},
location = {Sydney, NSW, Australia},
series = {IVA '18}
}

@inproceedings{10.1145/3582768.3582780,
author = {Moharkar, Kunal and Kshirsagar, Kartik and Shrey, Suruchi and Pasine, Neha and Kumar, Rishu and Radke, Mansi A.},
title = {Responding to customer queries automatically by customer reviews’ based Question Answering},
year = {2023},
isbn = {9781450397629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582768.3582780},
doi = {10.1145/3582768.3582780},
abstract = {The entire world has been undergoing its own digital transformation over the past few decades as technology has advanced in leaps and bounds. Following this, an increase in the number of people using digital platforms for buying products online likewise increases the number of questions or enquiries posted about a product on an online shopping platform like Amazon on a day to day basis. Though we have gone completely digital in posting these questions, the answering of these questions is still manual. The forums are rarely active. By the time the user gets an answer to his question, either he has bought that product already through offline means or has lost interest in buying that product since it is time consuming. Moreover, the questions which are asked are mostly repetitive. At times the answers are already out there since they have already been given to some other user who had asked the same question. Also, lot of answers are embedded in the user reviews. Therefore, the answers can be extracted from the existing product reviews. This may lead to increase in sale and greater customer satisfaction as his query is resolved in much lower response time. We have review-based question answering systems that aim at answering the questions from the reviews given on the product by other customers. However, the existing systems have certain drawbacks due to the use of RNN, like missing attention mechanism etc. In this work, we enhance the performance of the existing review based QA systems by carrying out some prototypical experiments with the basic models of NLP and then moving towards more advanced Language Models while identifying and rectifying the shortcomings of the existing model. Further, in this work a thorough comparative analysis of the models and approaches that have been worked on is presented. We have enhanced the current state of the art existing review QA systems by using BERT, BART and also applied various heuristics for comparison. We achieved the best BLEU score of 0.58 by using BERT, which is an improvement of 0.19 on the current existing system.},
booktitle = {Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval},
pages = {228–233},
numpages = {6},
location = {Bangkok, Thailand},
series = {NLPIR '22}
}

@article{10.1145/3713073,
author = {Wang, Haijie and Jiao, Jiajia},
title = {Sentiment Analysis of MOOC Reviews Based on Knowledge Dependency Tree},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {2375-4699},
url = {https://doi.org/10.1145/3713073},
doi = {10.1145/3713073},
abstract = {As an important online learning resource, Massive Open Online Courses have a large amount of comments, which can be exploited by aspect-level sentiment analysis to optimize MOOC teaching from different perspectives. However, there are two essential problems. One is that there is no open-source dataset on Chinese MOOC. The other problem is semantic information confusion caused by inherent polysemy of Chinese words and ambiguous expressions relatively relying on the context. To further characterize the special features of Chinese MOOC reviews, we build an open-source dataset with clean 5,000 MOOC reviews and propose a sentiment knowledge dependency tree–based graph neural network. The proposed model first uses the latest term frequency–inverse document frequency algorithm to extract high-frequency words and combines it with the Semantic Orientation Pointwise Mutual Information algorithm so a sentiment dictionary in the field of Chinese MOOCs is constructed. Then, the grammatical information of the dependency tree is merged with the sentiment knowledge information of the sentiment dictionary. Next, this novel model uses GCN to capture the long-distance feature information of the sentiment dependency tree and finally adopts the softmax function for sentiment classification. To further improve the model's performance, we also use BERT to enhance the text representation for higher accuracy. Meanwhile, the comparative experiments demonstrate that our proposed model takes advantages of the customized dependency tree by knowledge dictionary to achieve more accurate sentiment analysis than the state-of-the-art methods under different word embedding approaches.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = mar,
articleno = {24},
numpages = {15},
keywords = {MOOCs, Sentiment dictionary, Sentiment dependency tree, Graph convolutional networks}
}

@article{10.1145/3199668,
author = {Ahmad, Kashif and Mekhalfi, Mohamed Lamine and Conci, Nicola and Melgani, Farid and Natale, Francesco De},
title = {Ensemble of Deep Models for Event Recognition},
year = {2018},
issue_date = {May 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1551-6857},
url = {https://doi.org/10.1145/3199668},
doi = {10.1145/3199668},
abstract = {In this article, we address the problem of recognizing an event from a single related picture. Given the large number of event classes and the limited information contained in a single shot, the problem is known to be particularly hard. To achieve a reliable detection, we propose a combination of multiple classifiers, and we compare three alternative strategies to fuse the results of each classifier, namely: (i) induced order weighted averaging operators, (ii) genetic algorithms, and (iii) particle swarm optimization. Each method is aimed at determining the optimal weights to be assigned to the decision scores yielded by different deep models, according to the relevant optimization strategy. Experimental tests have been performed on three event recognition datasets, evaluating the performance of various deep models, both alone and selectively combined. Experimental results demonstrate that the proposed approach outperforms traditional multiple classifier solutions based on uniform weighting, and outperforms recent state-of-the-art approaches.},
journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
month = may,
articleno = {51},
numpages = {20},
keywords = {multiple classifiers, multimedia indexing and retrieval, genetic algorithms, fusion, deep neural networks, PSO, IOWA, Event recognition, CNN}
}

@proceedings{10.1145/3681772,
title = {IWCTS'24: Proceedings of the 17th ACM SIGSPATIAL International Workshop on Computational Transportation Science GenAI and Smart Mobility Session},
year = {2024},
isbn = {9798400711510},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {The 17th International Workshop on Computational Transportation Science (IWCTS 2024) will feature a Smart Mobility track, emphasizing the growing relevance of human mobility data from sources like cell phones, connected vehicles, and volunteered geographic information. This data integration is advancing smart city frameworks, intelligent transportation systems, and urban planning. Managing and analyzing large-scale datasets highlights the critical role of advanced computational and AI techniques, including Generative AI (GenAI), Large Language Models (LLMs), and Retrieval-Augmented Generation (RAG). The workshop builds on previous success, focusing on computational and informatics approaches for optimized urban mobility. We will build upon the success of previous workshops to continue to focus on the computational and informatics approaches for (not limited to):},
location = {Atlanta, GA, USA}
}

@inproceedings{10.1145/3290605.3300298,
author = {Choi, In Kwon and Childers, Taylor and Raveendranath, Nirmal Kumar and Mishra, Swati and Harris, Kyle and Reda, Khairi},
title = {Concept-Driven Visual Analytics: an Exploratory Study of Model- and Hypothesis-Based Reasoning with Visualizations},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300298},
doi = {10.1145/3290605.3300298},
abstract = {Visualization tools facilitate exploratory data analysis, but fall short at supporting hypothesis-based reasoning. We conducted an exploratory study to investigate how visualizations might support a concept-driven analysis style, where users can optionally share their hypotheses and conceptual models in natural language, and receive customized plots depicting the fit of their models to the data. We report on how participants leveraged these unique affordances for visual analysis. We found that a majority of participants articulated meaningful models and predictions, utilizing them as entry points to sensemaking. We contribute an abstract typology representing the types of models participants held and externalized as data expectations. Our findings suggest ways for rearchitecting visual analytics tools to better support hypothesis- and model-based reasoning, in addition to their traditional role in exploratory analysis. We discuss the design implications and reflect on the potential benefits and challenges involved.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14},
keywords = {visual analytics, sensemaking, mental models, hypothesis- and model-based reasoning},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@article{10.1145/3606370,
author = {Chan, Chia-Pang and Yang, Jun-He},
title = {Instagram Text Sentiment Analysis Combining Machine Learning and NLP},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2375-4699},
url = {https://doi.org/10.1145/3606370},
doi = {10.1145/3606370},
note = {Just Accepted},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = jul,
keywords = {word embedding technology, deep learning, machine learning, natural language processing, Instagram}
}

@inproceedings{10.1145/3655497.3655513,
author = {Chen, Yan and Ma, Ding},
title = {Detection of greenwashing in ESG reports of Chinese listed companies based on Word2vec and TF-IDF},
year = {2024},
isbn = {9798400709302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3655497.3655513},
doi = {10.1145/3655497.3655513},
abstract = {With the growing emphasis on ESG (Environmental, Social, and Governance) issues, the mandatory disclosure of ESG reports is on the horizon. However, due to the lack of a regulatory framework and a unified international ESG evaluation, the phenomenon of greenwashing in corporate ESG reporting is prevalent. We collected social responsibility reports and actual ESG performance data from A-share companies from 2011 to 2021 and innovatively employed text mining techniques to quantitatively investigate the extent of greenwashing in ESG reports. Our study initially utilized the Word2Vec method, combined with Skip-gram and Continuous Bag of Words models to train word vectors, and built an ESG lexicon using seed words. ESG reports is subsequently segmented based on a defined sentence splitting function and TF-IDF algorithm is employed to extract keywords. By matching the keywords with the ESG lexicon, we precisely extracted the annual ESG discourse for each company and conducted sentiment analysis to derive a greenwashing score. Heterogeneity analysis reveals that firm ownership has no significant impact on the level of greenwashing, yet the industry and region in which the enterprise operates considerably influence the greenwashing level. This study holds implications for enhancing the quality of ESG reporting and optimizing investment decisions.},
booktitle = {Proceedings of the 2024 International Conference on Innovation in Artificial Intelligence},
pages = {159–164},
numpages = {6},
keywords = {ESG, Greenwashing detection, TF-IDF algorithm, Text mining, Word2vec},
location = {Tokyo, Japan},
series = {ICIAI '24}
}

@inproceedings{10.5555/3721488.3721623,
author = {Pritchard, Michael and Ratnayake, Kalana and Gamage, Buddhi and Jayasuriya, Maleen and Herath, Damith},
title = {Capabilities2 for ROS2: Advanced Skill-Based Control for Human-Robot Interaction},
year = {2025},
publisher = {IEEE Press},
abstract = {In the early days of the Open Source Robotics Foundation, a lesser-known project aimed to design an ''app-able robot'', leading to the creation of the ''Capabilities'' package for the Robot Operating System. Over a decade later, formulating robot capabilities remains a significant technical hurdle in bringing robots from the lab into everyday life. This paper introduces Capabilities2, a successor to the original Capabilities package, now reimagined for ROS2. Capabilities2 enhances the original design by enabling advancements in skill-based control techniques and offering a more efficient, extensible framework for defining and utilising robot capabilities. We delve into its application in new real-world scenarios, with a particular focus on human-robot interactions and the deployment of collaborative mobile robots in human-centric environments. Capabilities2 addresses challenges in implementing intuitive, collaborative robots by introducing an abstracted database handler, an object-relational mapping for capability models, and a plugin architecture for capability execution. These features support dynamic capability representation, runtime adaptability, and integration with modern AI techniques for skill-based task planning. By providing a standardised yet flexible framework, Capabilities2 reduces the integration effort required to develop top-level controls for real-world scenarios, facilitating rapid development and deployment. Our contributions include the reimplementation of the Capabilities package in ROS2, enhancements to support contemporary robotic applications, and demonstrations of new use cases enabled by Capabilities2. We believe that Capabilities2 significantly advances the field of robotics by equipping developers with tools to create more capable, adaptable, and interactive robots. Capabilities2 is available at https://github.com/CollaborativeRoboticsLab/capabilities2},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1067–1071},
numpages = {5},
keywords = {api, capability, communication, hri, interface, package, provider, ros2, service, skill, task},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3549737.3549769,
author = {Konstantinidis, Ioannis and Maragoudakis, Manolis and Magnisalis, Ioannis and Berberidis, Christos and Peristeras, Vassilios},
title = {Knowledge-driven Unsupervised Skills Extraction for Graph-based Talent Matching},
year = {2022},
isbn = {9781450395977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3549737.3549769},
doi = {10.1145/3549737.3549769},
abstract = {In human resource management of large organisations, finding the best candidate for a job description requires an extensive examination of a large number of resume profiles. Even with the advent of Deep Information Retrieval and the supported semantic similarity search, identification of relevant skills within profiles requires thorough investigation over several aspects, including educational background, professional experience, achievements, etc. However, these techniques are based on the existence of domain-specific, human-annotated datasets, a laborious task that portrays high cost and a slow labeling progress. In this paper, we propose Resume2Skill-SE, an end-to-end architecture for interpretable skill-based talent matching. The solution consists of two components. The first module uses an unsupervised approach for skills extraction based on state-of-the-art text embeddings and efficient semantic similarity search. The second module creates a profile-skills bipartite graph and uses a proposed ranking formula for similar resume profiles, minimising the effect of potential errors from the skills extraction module. The optimal ranking formula was identified through an intuitive and automated evaluation method for getting relevance scores. The proposed technique delivers promising results while also including an interpretability layer by showing the common skills of a pair of resume profiles.},
booktitle = {Proceedings of the 12th Hellenic Conference on Artificial Intelligence},
articleno = {28},
numpages = {7},
keywords = {unsupervised skills extraction, search engine, natural language processing, graph analytics},
location = {Corfu, Greece},
series = {SETN '22}
}

@inproceedings{10.1145/3687123.3698284,
author = {Tsiligkaridis, Athanasios and Kalinowski, Nicholas and Li, Zhongheng and Hou, Elizabeth},
title = {Encoding Agent Trajectories as Representations with Sequence Transformers},
year = {2024},
isbn = {9798400711763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3687123.3698284},
doi = {10.1145/3687123.3698284},
abstract = {Spatiotemporal data faces many analogous challenges to natural language text including the ordering of locations (words) in a sequence, long range dependencies between locations, and locations having multiple meanings. In this work, we propose a novel model for representing high dimensional spatiotemporal trajectories as sequences of discrete locations and encoding them with a Transformer-based neural network architecture. Similar to language models, our Sequence Transformer for Agent Representation Encodings (STARE) model can learn representations and structure in trajectory data through both supervisory tasks (e.g., classification), and self-supervisory tasks (e.g., masked modelling). We present experimental results on various synthetic and real trajectory datasets and show that our proposed model can learn meaningful encodings that are useful for many downstream tasks including discriminating between labels and indicating similarity between locations. Using these encodings, we also learn relationships between agents and locations present in spatiotemporal data.},
booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
pages = {38–49},
numpages = {12},
keywords = {Transformers, encoders, human mobility, spatiotemporal data, trajectory modeling},
location = {Atlanta, GA, USA},
series = {GeoAI '24}
}

@article{10.1145/3410569,
author = {Laatar, Rim and Aloulou, Chafik and Belguith, Lamia Hadrich},
title = {Disambiguating Arabic Words According to Their Historical Appearance in the Document Based on Recurrent Neural Networks},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {6},
issn = {2375-4699},
url = {https://doi.org/10.1145/3410569},
doi = {10.1145/3410569},
abstract = {How can we determine the semantic meaning of a word in relation to its context of appearance? We eventually have to grabble with this difficult question, as one of the paramount problems of Natural Language Processing (NLP). In other words, this issue is commonly defined as Word Sense Disambiguation (WSD). The latter is one of the crucial difficulties within the NLP field. In this respect, word vectors extracted from a neural network model have been successfully applied for resolving the WSD problem. Accordingly, this article presents an unprecedented method to disambiguate Arabic words according to both their contextual appearance in a source text and the era in which they emerged. In fact, in the few previous decades, many researchers have been grabbling with Arabic Word Sense Disambiguation.It should be noted that the Arabic language can be divided into three major historical periods: old Arabic, middle-age Arabic, and contemporary Arabic. Actually, contemporary Arabic has proved to be the greatest concern of many researchers. The main gist of our work is to disambiguate Arabic words according to the historical period in which they appeared. To perform such a task, we suggest a method that deploys contextualized word embeddings to better gather valid syntactic and semantic information of the same word by taking into account its contextual uses. The preponderant thing is to convert both the senses and the contextual uses of an ambiguous item to vectors, then determine which of the possible conceptual meanings of the target word is closer to the given context.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = oct,
articleno = {86},
numpages = {16},
keywords = {word sense disambiguation, recurrent neural networks, old arabic, middle-age arabic, historical dictionary, contextualized word embeddings, contemporary arabic, Natural language processing}
}

@article{10.1145/3699953,
author = {Dadure, Pankaj and Pakray, Partha and Bandyopadhyay, Sivaji},
title = {Mathematical Information Retrieval: A Review},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3699953},
doi = {10.1145/3699953},
abstract = {Mathematical formulas are commonly used to demonstrate theories and basic fundamentals in the Science, Technology, Engineering, and Mathematics (STEM) domain. The burgeoning research in the STEM domain results in the mass production of scientific documents that contain both textual and mathematical terms. In scientific information, the definition of mathematical formulas is expressed through context and symbolic structure that adheres to strong domain-specific notions. Whereas the retrieval of textual information is well-researched, and numerous text-based search engines are present. However, textual information retrieval systems are inadequate for searching scientific information containing mathematical formulas, including simple symbols to complicated mathematical structures. The retrieval of mathematical information is in its infancy, and it requires the inclusion of new technologies and tools to promote the retrieval of scientific information and the management of digital libraries. This article provides a comprehensive study of mathematical information retrieval and highlights their challenges and future opportunities.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {61},
numpages = {34},
keywords = {Artificial intelligence, natural language processing, information retrieval, formula retrieval, mathematical knowledge discovery, digital libraries}
}

@inproceedings{10.1145/3646548.3676597,
author = {Rabiser, Rick},
title = {Industry Adoption of UVL: What We Will Need},
year = {2024},
isbn = {9798400705939},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3646548.3676597},
doi = {10.1145/3646548.3676597},
abstract = {Since 2018, in the Software Product Line community, the MODEVAR initiative is working on coming up with a simple, standard variability modeling language and has proposed the Universal Variability Language (UVL). UVL has already been integrated in multiple tools such as FeatureIDE and FLAMA and it has also already been adopted by other academics outside the MODEVAR initiative. This short position paper outlines the challenges for industry adoption of UVL the community needs to work on.},
booktitle = {Proceedings of the 28th ACM International Systems and Software Product Line Conference},
pages = {46–49},
numpages = {4},
keywords = {Challenges, Industry Adoption, Variability Modeling},
location = {Dommeldange, Luxembourg},
series = {SPLC '24}
}

@inproceedings{10.1145/3611643.3616314,
author = {Win, Hsu Myat and Wang, Haibo and Tan, Shin Hwei},
title = {Towards Automated Detection of Unethical Behavior in Open-Source Software Projects},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616314},
doi = {10.1145/3611643.3616314},
abstract = {Given the rapid growth of Open-Source Software (OSS) projects, ethical considerations are becoming more important. Past studies focused on specific ethical issues (e.g., gender bias and fairness in OSS). There is little to no study on the different types of unethical behavior in OSS projects. We present the first study of unethical behavior in OSS projects from the stakeholders’ perspective. Our study of 316 GitHub issues provides a taxonomy of 15 types of unethical behavior guided by six ethical principles (e.g., autonomy). Examples of new unethical behavior include soft forking (copying a repository without forking) and self-promotion (promoting a repository without self-identifying as contributor to the repository). We also identify 18 types of software artifacts affected by the unethical behavior. The diverse types of unethical behavior identified in our study (1) call for attentions of developers and researchers when making contributions in GitHub, and (2) point to future research on automated detection of unethical behavior in OSS projects. From our study, we propose Etor, an approach that can automatically detect six types of unethical behavior by using ontological engineering and Semantic Web Rule Language (SWRL) rules to model GitHub attributes and software artifacts. Our evaluation on 195,621 GitHub issues (1,765 GitHub repositories) shows that Etor can automatically detect 548 unethical behavior with 74.8\% average true positive rate (up to 100\% true positive rate). This shows the feasibility of automated detection of unethical behavior in OSS projects.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {644–656},
numpages = {13},
keywords = {Ethics in Software Engineering, Open-source software projects},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3485557.3485566,
author = {Wagih, Heba M. and Mokhtar, Hoda M. O.},
title = {Coronavirus: A Curse or A Bless ?},
year = {2021},
isbn = {9781450384186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485557.3485566},
doi = {10.1145/3485557.3485566},
abstract = {Nowadays crime is one of the major threats that affect human lives. The current pandemic has a great impact on changing the criminal landscape. Extensive investigations for crime and criminal behaviors have revealed new crime patterns and led to the generation of a large amount of data and relations that need to be presented in a proper model. In this paper, we conduct several experiments on different datasets representing some major cities in the USA to study the effect of the current pandemic on crime types, rates, and intensity which can be used in crime prediction and prevention. we also introduce an ontology model with its underlying description logics as the knowledge representation model to represent crime information.},
booktitle = {The 7th Annual International Conference on Arab Women in Computing in Conjunction with the 2nd Forum of Women in Research},
articleno = {9},
numpages = {4},
keywords = {ontology, knowledge representation model, description logic},
location = {Sharjah, United Arab Emirates},
series = {ArabWIC 2021}
}

@inproceedings{10.1145/3486187.3490204,
author = {Ducatteeuw, Vincent},
title = {Developing an Urban Gazetteer: A Semantic Web Database for Humanities Data},
year = {2021},
isbn = {9781450391023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486187.3490204},
doi = {10.1145/3486187.3490204},
abstract = {This talk discusses the development of a spatiotemporal data model for an urban gazetteer. The function of gazetteers is to obtain descriptions uniquely identifying places referred to in discourse. Often, they are lists of places containing place name, feature type and geographical extent. Contemporary digital gazetteers (e.g. World Historical Gazetteer and Pleiades) are valuable tools for geographical knowledge of the past and the structuring of humanities data. However, scholars and GLAM (Galleries, Libraries, Archives and Museums) specialists often require information about entities on an intra-city scale. This presentation explores the model and implementation of an urban gazetteer using CIDOC CRM as a top-level ontology. The model will closely follow international gazetteer standards (i.e. Linked Places Format) in order to ensure interoperability with other gazetteer datasets. To move towards a FAIR (Findable, Accessible, Interoperable, and Reusable) approach, humanities data from the urban gazetteer will be published as Linked Open Data (LOD) and searchable via (Geo)SPARQL.},
booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Geospatial Humanities},
pages = {36–39},
numpages = {4},
keywords = {urban history, urban gazetteer, spatiotemporal analysis, spatial humanities, spatial history, semantic technologies, modelling geohistorical data, linked open data, gazetteer development, digital humanities, GeoSPARQL, CIDOC CRM},
location = {Beijing, China},
series = {GeoHumanities '21}
}

@inproceedings{10.1145/3708359.3712110,
author = {Bao, Calvin and Shiue, Yow-Ting and Carpuat, Marine and Chan, Joel},
title = {Words as Bridges: Exploring Computational Support for Cross-Disciplinary Translation Work},
year = {2025},
isbn = {9798400713064},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708359.3712110},
doi = {10.1145/3708359.3712110},
abstract = {Scholars often explore literature outside of their home community of study. This exploration process is frequently hampered by field-specific jargon. Past computational work often focuses on supporting translation work by removing jargon through simplification and summarization; here, we explore a different approach that preserves jargon as useful bridges to new conceptual spaces. Specifically, we cast different scholarly domains as different language-using communities, and explore how to adapt techniques from unsupervised cross-lingual alignment of word embeddings to explore conceptual alignments between domain-specific word embedding spaces.We developed a prototype cross-domain search engine that uses aligned domain-specific embeddings to support conceptual exploration, and tested this prototype in two case studies. We discuss qualitative insights into the promises and pitfalls of this approach to translation work, and suggest design insights for future interfaces that provide computational support for cross-domain information seeking.},
booktitle = {Proceedings of the 30th International Conference on Intelligent User Interfaces},
pages = {1598–1623},
numpages = {26},
keywords = {cross-domain information seeking, information foraging, scholarly term translation},
location = {
},
series = {IUI '25}
}

@inproceedings{10.1145/3339252.3339282,
author = {Mahaini, Mohamad Imad and Li, Shujun and Sa\u{g}lam, Rahime Belen},
title = {Building Taxonomies based on Human-Machine Teaming: Cyber Security as an Example},
year = {2019},
isbn = {9781450371643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3339252.3339282},
doi = {10.1145/3339252.3339282},
abstract = {Taxonomies and ontologies are handy tools in many application domains such as knowledge systematization and automatic reasoning. In the cyber security field, many researchers have proposed such taxonomies and ontologies, most of which were built based on manual work. Some researchers proposed the use of computing tools to automate the building process, but mainly on very narrow sub-areas of cyber security. Thus, there is a lack of general cyber security taxonomies and ontologies, possibly due to the difficulties of manually curating keywords and concepts for such a diverse, inter-disciplinary and dynamically evolving field.This paper presents a new human-machine teaming based process to build taxonomies, which allows human experts to work with automated natural language processing (NLP) and information retrieval (IR) tools to co-develop a taxonomy from a set of relevant textual documents. The proposed process could be generalized to support non-textual documents and to build (more complicated) ontologies as well. Using the cyber security as an example, we demonstrate how the proposed taxonomy building process has allowed us to build a general cyber security taxonomy covering a wide range of data-driven keywords (topics) with a reasonable amount of human effort.},
booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
articleno = {30},
numpages = {9},
keywords = {visualization, taxonomy, ontology, online social network (OSN), natural language processing (NLP), knowledge representation, information retrieval (IR), cyber security, Twitter},
location = {Canterbury, CA, United Kingdom},
series = {ARES '19}
}

@inproceedings{10.1145/3502223.3502236,
author = {Shimizu, Cogan and Zhu, Rui and Mai, Gengchen and Fisher, Colby and Cai, Ling and Schildhauer, Mark and Janowicz, Krzysztof and Hitzler, Pascal and Zhou, Lu and Stephen, Shirly},
title = {A Pattern for Features on a Hierarchical Spatial Grid},
year = {2022},
isbn = {9781450395656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3502223.3502236},
doi = {10.1145/3502223.3502236},
abstract = {The integration of data along a common spatial component remains an obstacle in many problem spaces. One promising method for integrating data in such a way is through the use of a common, underlying spatial reference system, such as a Discrete Global Grid (e.g., the S2 Grid System), and pre-computing spatial relations between features and the constituent components at a spatial resolution appropriate for the data and use case. That is, by emphasizing the notion of the cell, we can examine what is in a cell, predict contents of its parent and child cells, and quickly get an overview of spatially co-located features and regions of interest without having to directly compute spatial interactions. This paper provides an ontology design pattern, to be used as a structural template, for modeling how features or regions map onto a hierarchical grid system and addresses how the attributes of these features may be inherited upwards or downwards through the hierarchy. We furthermore provide a motivating example and implementation.},
booktitle = {Proceedings of the 10th International Joint Conference on Knowledge Graphs},
pages = {108–114},
numpages = {7},
keywords = {ontology engineering, ontology design pattern, geoinformation science},
location = {Virtual Event, Thailand},
series = {IJCKG '21}
}

@inproceedings{10.1145/3543873.3587359,
author = {Alobaid, Ahmad and Toledo, Jhon and Corcho, Oscar and Poveda-Villal\'{o}n, Mar\'{\i}a},
title = {Depicting Vocabulary Summaries with Devos},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587359},
doi = {10.1145/3543873.3587359},
abstract = {Communicating ontologies to potential users is still a difficult and time-consuming task. Even for small ones, users need to invest time to determine whether to reuse them. Providing diagrams together with the ontologies facilitates the task of understanding the model from a user perspective. While some tools are available for depicting ontologies, and the code could also be inspected using ontology editors’ graphical interfaces, in many cases, the diagrams are too big or complex. The main objective of this demo is to present Devos, a system to generate ontology diagrams based on different strategies for summarizing the ontology.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {250–253},
numpages = {4},
keywords = {ontology diagrams, ontology engineering, ontology summarization},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@article{10.1145/3464383,
author = {Crovari, Pietro and Pid\`{o}, Sara and Pinoli, Pietro and Bernasconi, Anna and Canakoglu, Arif and Garzotto, Franca and Ceri, Stefano},
title = {GeCoAgent: A Conversational Agent for Empowering Genomic Data Extraction and Analysis},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
url = {https://doi.org/10.1145/3464383},
doi = {10.1145/3464383},
abstract = {With the availability of reliable and low-cost DNA sequencing, human genomics is relevant to a growing number of end-users, including biologists and clinicians. Typical interactions require applying comparative data analysis to huge repositories of genomic information for building new knowledge, taking advantage of the latest findings in applied genomics for healthcare. Powerful technology for data extraction and analysis is available, but broad use of the technology is hampered by the complexity of accessing such methods and tools.This work presents GeCoAgent, a big-data service for clinicians and biologists. GeCoAgent uses a dialogic interface, animated by a chatbot, for supporting the end-users’ interaction with computational tools accompanied by multi-modal support. While the dialogue progresses, the user is accompanied in extracting the relevant data from repositories and then performing data analysis, which often requires the use of statistical methods or machine learning. Results are returned using simple representations (spreadsheets and graphics), while at the end of a session the dialogue is summarized in textual format. The innovation presented in this article is concerned with not only the delivery of a new tool but also our novel approach to conversational technologies, potentially extensible to other healthcare domains or to general data science.},
journal = {ACM Trans. Comput. Healthcare},
month = oct,
articleno = {3},
numpages = {29},
keywords = {genomic computing, natural language understanding, Conversational agents}
}

@inproceedings{10.1145/3241653.3241655,
author = {Verbitskaia, Ekaterina and Kirillov, Ilya and Nozkin, Ilya and Grigorev, Semyon},
title = {Parser combinators for context-free path querying},
year = {2018},
isbn = {9781450358361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241653.3241655},
doi = {10.1145/3241653.3241655},
abstract = {Transparent integration of a domain-specific language for specification of context-free path queries (CFPQs) into a general-purpose programming language as well as static checking of errors in queries may greatly simplify the development of applications using CFPQs. LINQ and ORM can be used for the integration, but they have issues with flexibility: query decomposition and reusing of subqueries are a challenge. Adaptation of parser combinators technique for paths querying may solve these problems. Conventional parser combinators process linear input, and only the Trails library is known to apply this technique for path querying. Trails suffers the common parser combinators issue: it does not support left-recursive grammars and also experiences problems in cycles handling. We demonstrate that it is possible to create general parser combinators for CFPQ which support arbitrary context-free grammars and arbitrary input graphs. We implement a library of such parser combinators and show that it is applicable for realistic tasks.},
booktitle = {Proceedings of the 9th ACM SIGPLAN International Symposium on Scala},
pages = {13–23},
numpages = {11},
keywords = {Scala, Parser Combinators, Neo4j, Language-Constrained Path Problem, Graph Databases, Generalized LL, GLL, Context-Free Path Querying, Context-Free Language Reachability},
location = {St. Louis, MO, USA},
series = {Scala 2018}
}

@article{10.1145/3575865,
author = {Banar, Nikolay and Daelemans, Walter and Kestemont, Mike},
title = {Transfer Learning for the Visual Arts: The Multi-modal Retrieval of Iconclass Codes},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1556-4673},
url = {https://doi.org/10.1145/3575865},
doi = {10.1145/3575865},
abstract = {Iconclass is an iconographic thesaurus, which is widely used in the digital heritage domain to describe subjects depicted in artworks. Each subject is assigned a unique descriptive code, which has a corresponding textual definition. The assignment of Iconclass codes is a challenging task for computational systems, due to the large number of available labels in comparison to the limited amount of training data available. Transfer learning has become a common strategy to overcome such a data shortage. In deep learning, transfer learning consists in fine-tuning the weights of a deep neural network for a downstream task. In this work, we present a deep retrieval framework, which can be fully fine-tuned for the task under consideration. Our work is based on a recent approach to this task, which already yielded state-of-the-art performance, although it could not be fully fine-tuned yet. This approach exploits the multi-linguality and multi-modality that is inherent to digital heritage data. Our framework jointly processes multiple input modalities, namely, textual and visual features. We extract the textual features from the artwork titles in multiple languages, whereas the visual features are derived from photographic reproductions of the artworks. The definitions of the Iconclass codes, containing useful textual information, are used as target labels instead of the codes themselves. As our main contribution, we demonstrate that our approach outperforms the state-of-the-art by a large margin. In addition, our approach is superior to the M3P feature extractor and outperforms the multi-lingual CLIP in most experiments due to the better quality of the visual features. Our out-of-domain and zero-shot experiments show poor results and demonstrate that the Iconclass retrieval remains a challenging task. We make our source code and models publicly available to support heritage institutions in the further enrichment of their digital collections.},
journal = {J. Comput. Cult. Herit.},
month = jun,
articleno = {32},
numpages = {16},
keywords = {multi-lingual retrieval, multi-modal retrieval, natural language processing, deep learning, transfer learning, cultural heritage, Iconclass}
}

@article{10.1145/3610581,
author = {Osman, Taha and Khalil, Hussein and Miltan, Mohammed and Shaalan, Khaled and Alfrjani, Rowida},
title = {Exploiting Functional Discourse Grammar to Enhance Complex Arabic Relation Extraction using a Hybrid Semantic Knowledge Base - Machine Learning Approach},
year = {2023},
issue_date = {August 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {8},
issn = {2375-4699},
url = {https://doi.org/10.1145/3610581},
doi = {10.1145/3610581},
abstract = {Relation extraction from unstructured Arabic text is especially challenging due to the Arabic language complex morphology and the variation in word semantics and lexical categories. The research documented in this paper presents a hybrid Semantic Knowledge base - Machine Learning (SKML) approach for extracting complex Arabic relations from unstructured Arabic documents; the proposed approach exploits the principles of Functional Discourse Grammar (FDG) to emphasise the semantic and pragmatic properties of the language and facilitate the identification of relation elements. At the initial phase, the novel FDG-SKML relation extraction approach deploys a lexical-based mechanism that utilises a purposely built domain-specific Semantic Knowledge to encode the semantic association between the identified relations’ elements. The evaluation of the initial stage evidenced improved accuracy for extracting most complex Arabic relations. The initial relation extraction mechanism was further extended by integrating its output into a Machine Learning classifier that facilitated extracting especially complex relations with significant disparity in the relation elements’ presence, order, and correlation. Using Economics as the problem domain, experimental evaluation evidenced the high accuracy of our FDG-SKML approach in complex Arabic relation extraction task and demonstrated its further improvement upon integration with machine learning classifiers.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = aug,
articleno = {214},
numpages = {30},
keywords = {hybrid knowledge-based machine learning classification, Functional Discourse Grammar, semantic web base, Natural Language Processing, Arabic relation extraction}
}

@inproceedings{10.1145/3599640.3599647,
author = {Lin, Longcheng and Wang, Fang},
title = {Adaptive Learning System Based on Knowledge Graph},
year = {2023},
isbn = {9781450399593},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599640.3599647},
doi = {10.1145/3599640.3599647},
abstract = {Since the rapid development of "Internet+Education" , various artificial intelligence technologies have been applied to teaching and learning. The development of technology has brought great impetus and potential to education. Based on the background of big data, how to use AI technology to mine valuable information in massive data to meet the adaptive learning needs of learners is an important topic that deserves attention and research. This paper builds learner ontology and course knowledge ontology, links learning resources to course knowledge ontology to build domain knowledge graph, designs and implements an adaptive learning system based on knowledge graph, including learner model, domain knowledge model, adaptive learning engine and interactive interface. Finally, Pellet inference engine is used to infer the designed SWRL rules. The result shows that the adaptive learning system proposed in this paper can recommend appropriate learning paths according to the learners' learning status, and present personalized learning resources.},
booktitle = {Proceedings of the 9th International Conference on Education and Training Technologies},
articleno = {7},
numpages = {7},
keywords = {recommendation, personalized learning, ontology construction, knowledge graph, Adaptive learning system},
location = {Macau, China},
series = {ICETT '23}
}

@inproceedings{10.1145/3579375.3579391,
author = {Rani, Nanda and Saha, Bikash and Maurya, Vikas and Shukla, Sandeep Kumar},
title = {TTPHunter: Automated Extraction of Actionable Intelligence as TTPs from Narrative Threat Reports},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579375.3579391},
doi = {10.1145/3579375.3579391},
abstract = {With the proliferation of attacks from various Advanced Persistent Threats (APT) groups, it is essential to comprehend the threat actor’s attack patterns to accelerate threat detection and response. The MITRE ATT&amp;CK framework’s Tactics, Techniques, and Procedures (TTPs) help to decipher attack patterns. The APT reports, published by security firms, contain rich information on tools and techniques used by threat actors. These reports are available in unstructured and natural language texts. There is a need for an automated tool to extract TTPs present in natural language text. However, there are few tools available in the literature, but their performance is not very satisfactory. In this work, we propose TTPHunter, to extract TTPs from APT reports by mapping sentence context to relevant TTPs. We fine-tune linear classifiers, which take input as BERT (Bidirectional Encoder Representations from Transformers) embeddings of sentences. We create two datasets: sentence-based (8,387 sentence samples) and document-based (50 threat reports) to validate TTPHunter. TTPHunter achieves the F1-score of 88\% and 75\% for both datasets, respectively. We compare the TTPHunter with rcATT and AttacKG baseline models, and it outperforms both baselines.},
booktitle = {Proceedings of the 2023 Australasian Computer Science Week},
pages = {126–134},
numpages = {9},
keywords = {Threat Intelligence, TTP Extraction, Natural Language Processing, MITRE ATT&amp;CK, Cybersecurity},
location = {Melbourne, VIC, Australia},
series = {ACSW '23}
}

@inproceedings{10.1145/3357384.3358025,
author = {Amsterdamer, Yael and Milo, Tova and Somech, Amit and Youngmann, Brit},
title = {Declarative User Selection with Soft Constraints},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358025},
doi = {10.1145/3357384.3358025},
abstract = {In applications with large userbases such as crowdsourcing, social networks or recommender systems, selecting users is a common and challenging task. Different applications require different policies for selecting users, and implementing such policies is applicationspecific and laborious. To this end, we introduce a novel declarative framework that abstracts common components of the user selection problem, while allowing for domain-specific tuning. The framework is based on an ontology view of user profiles, with respect to which we define a query language for policy specification. Our language extends SPARQL with means for capturing soft constraints which are essential for worker selection. At the core of our query engine is then a novel efficient algorithm for handling these constraints. Our experimental study on real-life data indicates the effectiveness and flexibility of our approach, showing in particular that it outperforms existing task-specific solutions in prominent user selection tasks.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {931–940},
numpages = {10},
keywords = {user selection, sparql, semantic similarity},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3637528.3671542,
author = {Baughman, Aaron and Morales, Eduardo and Agarwal, Rahul and Akay, Gozde and Feris, Rogerio and Johnson, Tony and Hammer, Stephen and Karlinsky, Leonid},
title = {Large Scale Generative AI Text Applied to Sports and Music},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637528.3671542},
doi = {10.1145/3637528.3671542},
abstract = {We address the problem of scaling up the production of media content, including commentary and personalized news stories, for large-scale sports and music events worldwide. Our approach relies on generative AI models to transform a large volume of multimodal data (e.g., videos, articles, real-time scoring feeds, statistics, and fact sheets) into coherent and fluent text. Based on this approach, we introduce, for the first time, an AI commentary system, which was deployed to produce automated narrations for highlight packages at the 2023 US Open, Wimbledon, and Masters tournaments. In the same vein, our solution was extended to create personalized content for ESPN Fantasy Football and stories about music artists for the GRAMMY awards. These applications were built using a common software architecture achieved a 15x speed improvement with an average Rouge-L of 82.00 and perplexity of 6.6. Our work was successfully deployed at the aforementioned events, supporting 90 million fans around the world with 8 billion page views, continuously pushing the bounds on what is possible at the intersection of sports, entertainment, and AI.},
booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4784–4792},
numpages = {9},
keywords = {applied computing, generative ai, large scale computing, neural networks, sports and entertainment},
location = {Barcelona, Spain},
series = {KDD '24}
}

@article{10.1145/3638062,
author = {Aouachria, Moufida and Leshob, Abderrahmane and Ghomari, Abdessamed R\'{e}da and Aouache, Mustapha},
title = {A Process Mining Method for Inter-organizational Business Process Integration},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {2158-656X},
url = {https://doi.org/10.1145/3638062},
doi = {10.1145/3638062},
abstract = {Business process integration (BPI) allows organizations to connect and automate their business processes in order to deliver the right economic resources at the right time, place, and price. BPI requires the integration of business processes and their supporting systems across multiple autonomous organizations. However, such integration is complex and can face coordination complexities that occur during the resource exchanges between the partners’ processes. This article proposes a new method called Process Mining for Business Process Integration (PM4BPI) that helps process designers to perform BPI by creating new process models that cross the boundaries of multiple organizations from a collection of process event logs. PM4BPI uses federated process mining techniques to detect incompatibilities before the integration of the partners’ processes. Then, it applies process adaptation patterns to solve detected incompatibilities. Finally, organizations’ processes are merged to build a collaborative process model that crosses the organizations’ boundaries. AdaptWF_Net, an extension of a Petri net, is used to design inter-organizational business processes and adaptation patterns. An integrated care pathway is used as a case study to assess the applicability and effectiveness of the proposed method.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = mar,
articleno = {2},
numpages = {29},
keywords = {Business process integration, inter-organizational business process model, process mining, adaptation patterns, process modelling}
}

@inproceedings{10.1145/3501409.3501592,
author = {Shan, Ruikang and Jiang, Tao and Wang, Yetong},
title = {Research on the Construction of Domain Sentiment Lexicon Based on Label Propagation Algorithm},
year = {2022},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501592},
doi = {10.1145/3501409.3501592},
abstract = {With the rapid development of the Internet and the explosive growth of online information, timely monitoring and guidance of public opinion is the key to maintaining a safe online environment. Sentiment lexicon is an important corpus resource in fields such as opinion analysis, and when faced with tasks in different domains, generic sentiment lexicon has been difficult to meet the demand, so researchers have remembered to focus on domain sentiment lexicon. In this paper, we propose a method based on improved label propagation to achieve automatic construction of Chinese sentiment lexicon from the sentence-level text. The key idea is to use lexical rules and sentiment association corrections to take the computational approach of optimizing mutual information of points, both of which improve the algorithm's effectiveness in analyzing complex sentences and thus enhance the accuracy of sentiment word recognition. The experimental results show that the method can automatically build a sentiment lexicon according to the corpus, with high quality and certain domain adaptability.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {1024–1029},
numpages = {6},
keywords = {Domain sentiment lexicon, Label propagation, Pointwise mutual information, Public opinion analysis, Sentiment analysis},
location = {Xiamen, China},
series = {EITCE '21}
}

@inproceedings{10.1109/JCDL57899.2023.00038,
author = {Sierra-M\'{u}nera, Alejandro and Westphal, Jan and Krestel, Ralf},
title = {Efficient Ultrafine Typing of Named Entities},
year = {2024},
isbn = {9798350399318},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL57899.2023.00038},
doi = {10.1109/JCDL57899.2023.00038},
abstract = {Ultrafine named entity typing (UFET) refers to the assignment of predefined labels to entity mentions in a given context. In contrast to traditional named entity typing, the number of potential labels is in the thousands and one mention can have more than one assigned type. Previous approaches either depend on large training datasets, or require inefficient encoding of all input-type combinations. Therefore, there is a need for investigating the efficiency during training and prediction of entity typing models in the ultrafine-grained setting, considering its distinctively bigger search space, compared to the coarse- and fine-grained tasks. To efficiently solve UFET, we propose Decent, a lightweight model that encodes, using a pretrained language model, the input sentences separately from the type labels. Additionally, we make use of negative oversampling to speed up the training while improving the generalization of unseen types. Using an openly available UFET dataset, we evaluated the classification and runtime performance of Decent and observed that training and prediction runtime is orders of magnitude faster than the current state-of-the-art approaches, while maintaining a competitive classification performance.},
booktitle = {Proceedings of the 2023 ACM/IEEE Joint Conference on Digital Libraries},
pages = {205–214},
numpages = {10},
keywords = {ultrafine enity typing, named entity recognition},
location = {Santa Fe, New Mexico, USA},
series = {JCDL '23}
}

@inproceedings{10.1145/3627673.3679175,
author = {Egami, Shusaku and Ugai, Takanori and Htun, Swe Nwe Nwe and Fukuda, Ken},
title = {VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627673.3679175},
doi = {10.1145/3627673.3679175},
abstract = {Multi-modal knowledge graphs (MMKGs), which ground various non-symbolic data (e.g., images and videos) into symbols, have attracted attention as resources enabling knowledge processing and machine learning across modalities. However, the construction of MMKGs for videos consisting of multiple events, such as daily activities, is still in the early stages. In this paper, we construct an MMKG based on synchronized multi-view simulated videos of daily activities. Besides representing the content of daily life videos as event-centric knowledge, our MMKG also includes frame-by-frame fine-grained changes, such as bounding boxes within video frames. In addition, we provide support tools for querying our MMKG. As an application example, we demonstrate that our MMKG facilitates benchmarking vision-language models by providing the necessary vision-language datasets for a tailored task.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {5360–5364},
numpages = {5},
keywords = {daily life video, event-centric knowledge graph, multi-modal knowledge graph, synthetic data, visual question answering},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@inproceedings{10.1145/3194658.3194668,
author = {Reda, Roberto and Piccinini, Filippo and Carbonaro, Antonella},
title = {Towards Consistent Data Representation in the IoT Healthcare Landscape},
year = {2018},
isbn = {9781450364935},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194658.3194668},
doi = {10.1145/3194658.3194668},
abstract = {Nowadays, the enormous volume of health and fitness data gathered from IoT wearable devices offers favourable opportunities to the research community. For instance, it can be exploited using sophisticated data analysis techniques, such as automatic reasoning, to find patterns and, extract information and new knowledge in order to enhance decision-making and deliver better healthcare. However, due to the high heterogeneity of data representation formats, the IoT healthcare landscape is characterised by an ubiquitous presence of data silos which prevents users and clinicians from obtaining a consistent representation of the whole knowledge. Semantic web technologies, such as ontologies and inference rules, have been shown as a promising way for the integration and exploitation of data from heterogeneous sources. In this paper, we present a semantic data model useful to: (1) consistently represent health and fitness data from heterogeneous IoT sources; (2) integrate and exchange them; and (3) enable automatic reasoning by inference engines.},
booktitle = {Proceedings of the 2018 International Conference on Digital Health},
pages = {5–10},
numpages = {6},
keywords = {semantic web technologies, ontology-based data representation, internet of things, health informatics},
location = {Lyon, France},
series = {DH '18}
}

@inproceedings{10.1145/3573381.3596150,
author = {Robert, Florent and Wu, Hui-Yin and Sassatelli, Lucile and Ramano\"{e}l, Stephen and Gros, Auriane and Winckler, Marco},
title = {An Integrated Framework for Understanding Multimodal Embodied Experiences in Interactive Virtual Reality},
year = {2023},
isbn = {9798400700286},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3573381.3596150},
doi = {10.1145/3573381.3596150},
abstract = {Virtual Reality (VR) technology enables “embodied interactions” in realistic environments where users can freely move and interact, with deep physical and emotional states. However, a comprehensive understanding of the embodied user experience is currently limited by the extent to which one can make relevant observations, and the accuracy at which observations can be interpreted. Paul Dourish proposed a way forward through the characterisation of embodied interactions in three senses: ontology, intersubjectivity, and intentionality. In a joint effort between computer and neuro-scientists, we built a framework to design studies that investigate multimodal embodied experiences in VR, and apply it to study the impact of simulated low-vision on user navigation. Our methodology involves the design of 3D scenarios annotated with an ontology, modelling intersubjective tasks, and correlating multimodal metrics such as gaze and physiology to derive intentions. We show how this framework enables a more fine-grained understanding of embodied interactions in behavioural research.},
booktitle = {Proceedings of the 2023 ACM International Conference on Interactive Media Experiences},
pages = {14–26},
numpages = {13},
keywords = {user experience analysis, task modeling, scene ontology, navigation, interaction, immersion, Embodied experiences, 3D environments},
location = {Nantes, France},
series = {IMX '23}
}

@inproceedings{10.1145/3643662.3643962,
author = {Falcarin, Paolo and Dainese, Fabio},
title = {Building a Cybersecurity Knowledge Graph with CyberGraph},
year = {2024},
isbn = {9798400705656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3643662.3643962},
doi = {10.1145/3643662.3643962},
abstract = {Software engineers and security professionals rely on a variety of sources of information, including known vulnerabilities, newly identified weaknesses, and threats, as well as attack patterns and current mitigations. Such information, spread across different places, results in an increased effort for developers in following all the cross-referenced data and finding appropriate solutions to their security issues in a timely manner. Software developers cannot have a good knowledge of the breadth of the different issues and vulnerabilities that are constantly increasing in time; the raising number of security issues to tackle cannot be matched by software developers which need more help from intelligent tools. Therefore, in this work, we present CyberGraph, a tool to automatically build and update a single, easily queryable cybersecurity knowledge graph by automatically linking heterogeneous data from different public repositories. The resulting unique integrated dataset, thanks to its magnitude, allows the execution of sophisticated queries that can quickly provide new insights and valuable perspectives.},
booktitle = {Proceedings of the 2024 ACM/IEEE 4th International Workshop on Engineering and Cybersecurity of Critical Systems (EnCyCriS) and 2024 IEEE/ACM Second International Workshop on Software Vulnerability},
pages = {29–36},
numpages = {8},
keywords = {cybersecurity, knowledge graph, software vulnerabilities, visualization, Neo4j, MITRE},
location = {Lisbon, Portugal},
series = {EnCyCriS/SVM '24}
}

@inproceedings{10.1145/3641584.3641793,
author = {Guan, Wei and Lian, Xiaoru and Ma, Li},
title = {CA-WGE: A two-view graph neural network-based knowledge graph completion approach combining common sense perception},
year = {2024},
isbn = {9798400707674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641584.3641793},
doi = {10.1145/3641584.3641793},
abstract = {The knowledge graph completion algorithm can make the knowledge graph more complete and is currently a research hotspot in the field of artificial intelligence. The knowledge graph completion model is mainly defined in three aspects, the way of negative example generation, the design of scoring function and the design of loss function. The previous knowledge graph completion models only rely on factual view data to predict the missing links between entities and ignore the valuable common sense knowledge, and there is invalid negative sampling in knowledge graph embedding techniques; on the other hand, the existing graph neural network-based knowledge graph embedding models mainly consider capturing the graph structure around entities, and the relational representation is only used to update the entity embedding, which may miss the potentially useful information about the relational structure of potentially useful information. To address the above challenges, this paper proposes a two-view graph neural network-based knowledge graph completion model combined with common sense awareness. Common knowledge is first automatically extracted from fact triples with entity concepts to facilitate high-quality negative sampling, and then positive and weighted negative triples are fed into the two-view graph neural network-based knowledge graph embedding model to capture entity- and relationship-centric graph structures and learn vector representations of entities and relationships, and then the learned entity and relationship representations are fed into a weighted score function to return the final the final score. Extensive experimental and ablation studies on four datasets, FB15K, FB15K237, NELL995, and DBpedia-242, show that the model achieves better performance compared to the state-of-the-art models.},
booktitle = {Proceedings of the 2023 6th International Conference on Artificial Intelligence and Pattern Recognition},
pages = {1382–1389},
numpages = {8},
keywords = {Common Sense Awareness, Graph Neural Network, Knowledge Graph Completion, Knowledge Graph Embedding, Negative Sampling},
location = {Xiamen, China},
series = {AIPR '23}
}

@article{10.1145/3754450,
author = {Lian, Xiaoli and Wu, Jiajun and Gao, Xiaoyun and Wang, Shuaisong and Zhang, Li},
title = {Vision to Specification: Automating the Transition from Conceptual Features to Functional Requirements},
year = {2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3754450},
doi = {10.1145/3754450},
abstract = {The translation of high-level abstract features into clear, and testable functional requirements (FRs) is a crucial step in software development, bridging the gap between user needs and technical specifications. In engineering practice, significant expert effort is needed for this translation. Our approach, EasyFR, streamlines the process by recommending Semantic Role Labeling (SRL) sequences for the given abstract features to guide Pre-trained Language Models (PLMs) in producing cohesive FR statements. By analyzing ten diverse datasets, we induce two variable SRL templates, each including two configurable parts. For concrete features, our proposed Key2Temp model can construct the appropriate variant of the SRL template by identifying a variable SRL template and placing the feature tokens in the appropriate slots. In this way, our approach reframes the process of requirement generation into a structured slot-filling activity. Experimental validation on four open datasets demonstrates that EasyFR outperforms three advanced Natural language generation (NLG) approaches, including GPT-4, particularly when existing FRs are available for training. The positive influence of our SRL template variant recommendations is further confirmed through an ablation study. We believe that our results indicate a notable step forward in the realm of automated requirements synthesis, holding potential to improve the process of requirements specification in future software projects.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
keywords = {Software Requirements Synthesis, Features, Semantic Templates, Slot Filling}
}

@inproceedings{10.1145/3395027.3419585,
author = {Alpizar-Chacon, Isaac and Sosnovsky, Sergey},
title = {Order out of Chaos: Construction of Knowledge Models from PDF Textbooks},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419585},
doi = {10.1145/3395027.3419585},
abstract = {Textbooks are educational documents created, structured and formatted by domain experts with the main purpose to explain the knowledge in the domain to a novice. Authors use their understanding of the domain when structuring and formatting the content of a textbook to facilitate this explanation. As a result, the formatting and structural elements of textbooks carry the elements of domain knowledge implicitly encoded by their authors. Our paper presents an extendable approach towards automated extraction of this knowledge from textbooks taking into account their formatting rules and internal structure. We focus on PDF as the most common textbook representation format; however, the overall method is applicable to other formats as well. The evaluation experiments examine the accuracy of the approach, as well as the pragmatic quality of the obtained knowledge models using one of their possible applications -- semantic linking of textbooks in the same domain. The results indicate high accuracy of model construction on symbolic, syntactic and structural levels across textbooks and domains, and demonstrate the added value of the extracted models on the semantic level.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {8},
numpages = {10},
keywords = {textbook, model extraction, knowledge modeling, PDF processing},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}

@inproceedings{10.1145/3691620.3695037,
author = {Guo, An and Zhou, Yuan and Tian, Haoxiang and Fang, Chunrong and Sun, Yunjian and Sun, Weisong and Gao, Xinyu and Luu, Anh Tuan and Liu, Yang and Chen, Zhenyu},
title = {SoVAR: Build Generalizable Scenarios from Accident Reports for Autonomous Driving Testing},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695037},
doi = {10.1145/3691620.3695037},
abstract = {Autonomous driving systems (ADSs) have undergone remarkable development and are increasingly employed in safety-critical applications. However, recently reported data on fatal accidents involving ADSs suggests that the desired level of safety has not yet been fully achieved. Consequently, there is a growing need for more comprehensive and targeted testing approaches to ensure safe driving. Scenarios from real-world accident reports provide valuable resources for ADS testing, including critical scenarios and high-quality seeds. However, existing scenario reconstruction methods from accident reports often exhibit limited accuracy in information extraction. Moreover, due to the diversity and complexity of road environments, matching current accident information with the simulation map data for reconstruction poses significant challenges.In this paper, we design and implement SoVAR, a tool for automatically generating road-generalizable scenarios from accident reports. SoVAR utilizes well-designed prompts with linguistic patterns to guide the large language model (LLM) in extracting accident information from textual data. Subsequently, it formulates and solves accident-related constraints in conjunction with the extracted accident information to generate accident trajectories. Finally, SoVAR reconstructs accident scenarios on various map structures and converts them into test scenarios to evaluate its capability to detect defects in industrial ADSs. We experiment with SoVAR, using the accident reports from the National Highway Traffic Safety Administration's (NHTSA) database to generate test scenarios for the industrial-grade ADS Apollo. The experimental findings demonstrate that SoVAR can effectively generate generalized accident scenarios across different road structures. Furthermore, the results confirm that SoVAR identified 5 distinct safety violation types that contributed to the crash of Baidu Apollo.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {268–280},
numpages = {13},
keywords = {software testing, automatic test generation, constraint solving, autonomous driving system},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@inproceedings{10.1109/MiSE.2019.00019,
author = {Babur, \"{O}nder and Stephan, Matthew},
title = {MoCoP: towards a model clone portal},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MiSE.2019.00019},
doi = {10.1109/MiSE.2019.00019},
abstract = {Widespread and mature practice of model-driven engineering is leading to a growing number of modeling artifacts and challenges in their management. Model clone detection (MCD) is an important approach for managing and maintaining modeling artifacts. While its counterpart in traditional source code development, code clone detection, is enjoying popularity and more than two decades of development, MCD is still in its infancy in terms of research and tooling. We aim to develop a portal for model clone detection, MoCoP, as a central hub to mitigate adoption barriers and foster MCD research. In this short paper, we present our vision for MoCoP and its features and goals. We discuss MoCoP's key components that we plan on realizing in the short term including public tooling, curated data sets, and a body of MCD knowledge. Our longer term goals include a dedicated service-oriented infrastructure, contests, and forums. We believe MoCoP will strengthen MCD research, tooling, and the community, which in turn will lead to better quality, maintenance, and scalability for model-driven engineering practices.},
booktitle = {Proceedings of the 11th International Workshop on Modelling in Software Engineerings},
pages = {78–81},
numpages = {4},
keywords = {software maintenance, model-driven engineering, model repositories, model management, model clone detection, model analytics},
location = {Montreal, Quebec, Canada},
series = {MiSE '19}
}

@article{10.1109/TASLP.2023.3302232,
author = {Lim, Jungwoo and Whang, Taesun and Lee, Dongyub and Lim, Heuiseok},
title = {Adaptive Multi-Domain Dialogue State Tracking on Spoken Conversations},
year = {2023},
issue_date = {2024},
publisher = {IEEE Press},
volume = {32},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3302232},
doi = {10.1109/TASLP.2023.3302232},
abstract = {The main objective of the task-oriented dialogue system is to identify the intent and needs of human dialogue. Many existing studies are conducted under the setting of written dialogue, but there always exists a difficulty in coping with real-world spoken dialogues. To this end, DSTC10 challenge organizers propose the task of building robust dialogue state tracking (DST) models on spoken dialogues. With the powerful existing DST model (i.e., MinTL), this article suggests integral components for building a dialogue state tracker; 1) Data augmentation effectively enhances the capability of the model to catch the entities that exist in the evaluation dataset. 2) Levenshtein post-processing aims to prevent the distortion in model prediction caused by automatic speech recognition errors. To validate the effectiveness of our methods, we evaluate our model on DSTC10 datasets and conduct qualitative analysis by ablating each component of the model. Experimental results show that our model significantly outperforms baselines in all evaluation metrics and took 3rd place in the challenge.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = aug,
pages = {727–732},
numpages = {6}
}

@inproceedings{10.1145/3691620.3695019,
author = {Zhao, Jiuang and Yang, Zitian and Zhang, Li and Lian, Xiaoli and Yang, Donghao and Tan, Xin},
title = {DRMiner: Extracting Latent Design Rationale from Jira Issue Logs},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695019},
doi = {10.1145/3691620.3695019},
abstract = {Software architectures are usually meticulously designed to address multiple quality concerns and support long-term maintenance. However, there may be a lack of motivation for developers to document design rationales (i.e., the design alternatives and the underlying arguments for making or rejecting decisions) when they will not gain immediate benefit, resulting in a lack of standard capture of these rationales. With the turnover of developers, the architecture inevitably becomes eroded. This issue has motivated a number of studies to extract design knowledge from open-source communities in recent years. Unfortunately, none of the existing research has successfully extracted solutions alone with their corresponding arguments due to challenges such as the intricate semantics of online discussions and the lack of benchmarks for design rationale extraction.In this paper, we propose a novel approach, named DRMiner, to automatically mine latent design rationales from developers' live discussion in open-source community (i.e., issue logs in Jira). To better identify solutions and their relevant arguments, DRMiner skillfully decomposes the problem into multiple text classification tasks and tackles them using prompt tuning of large language models (LLMs) and specific heuristic features. To evaluate DRMiner, we acquire issue logs from Cassandra, Flink, and Solr repositories in Jira and form a dataset for design rationale mining. Experimental results show that DRMiner outperforms all baselines and achieves F1 improvements of 24\%, 22\%, and 20\% for mining design rationales, solutions, and arguments, respectively, compared to the best baseline. Furthermore, we investigate the usefulness of the design rationales mined by DRMiner for automated program repair (APR) and find that advanced LLMs, when prompted with these extracted rationales, generate 10\texttimes{}-18\texttimes{} more full-match patches and achieve a 10\%-13\% gain in CodeBLEU scores.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {468–480},
numpages = {13},
keywords = {design rationale, issue logs, design discussion, design recovery, program maintenance},
location = {Sacramento, CA, USA},
series = {ASE '24}
}

@article{10.1145/3451219,
author = {Nayak, Stuti and Zaveri, Amrapali and Serrano, Pedro Hernandez and Dumontier, Michel},
title = {Experience: Automated Prediction of Experimental Metadata from Scientific Publications},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {4},
issn = {1936-1955},
url = {https://doi.org/10.1145/3451219},
doi = {10.1145/3451219},
abstract = {While there exists an abundance of open biomedical data, the lack of high-quality metadata makes it challenging for others to find relevant datasets and to reuse them for another purpose. In particular, metadata are useful to understand the nature and provenance of the data. A common approach to improving the quality of metadata relies on expensive human curation, which itself is time-consuming and also prone to error. Towards improving the quality of metadata, we use scientific publications to automatically predict metadata key:value pairs. For prediction, we use a Convolutional Neural Network (CNN) and a Bidirectional Long-short term memory network (BiLSTM). We focus our attention on the NCBI Disease Corpus, which is used for training the CNN and BiLSTM. We perform two different kinds of experiments with these two architectures: (1) we predict the disease names by using their unique ID in the MeSH ontology and (2) we use the tree structures of MeSH ontology to move up in the hierarchy of these disease terms, which reduces the number of labels. We also perform various multi-label classification techniques for the above-mentioned experiments. We find that in both cases CNN achieves the best results in predicting the superclasses for disease with an accuracy of 83\%.},
journal = {J. Data and Information Quality},
month = aug,
articleno = {21},
numpages = {11},
keywords = {natural language processing, quality, metadata, neural networks, Datasets}
}

@proceedings{10.1145/3643666,
title = {MO2RE 2024: Proceedings of the 1st IEEE/ACM Workshop on Multi-disciplinary, Open, and RElevant Requirements Engineering},
year = {2024},
isbn = {9798400705694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Requirements engineering (RE) is a critical sub-field of software engineering (SE) that deals with identifying, specifying, modeling, analyzing, and validating the needs of stakeholders and constraints of a system [1]. RE covers human-related aspects, as stakeholders need to be involved in eliciting and validating the requirements, as well as more technical aspects, as requirements can be systematically collected (e.g., from app reviews) using data mining techniques and analyzed with natural language processing (NLP) approaches, e.g., to identify quality issues or trace links [2]. Despite the broad spectrum of activities that RE covers, researchers from outside RE often have a misconception that RE is limited to writing and analyzing requirements specifications. Consequently, many researchers in the SE community working on RE-relevant problems (e.g., human-centric SE) are often unaware that such problems belong to the RE research strands. Broadly speaking, RE is under-represented and under-appreciated in the SE community.Despite this limited presence, RE is more and more fundamental to cope with the current state of SE, especially considering the recent disruptive changes in artificial intelligence (AI) and NLP caused by large language models (LLMs) and their applications, ChatGPT being a notable example. Given the increasing pervasiveness of AI-based systems in our daily life, there is a growing need for RE techniques to support sound and structured development of AI systems [3], with a particular interest in explainability, interpretability, reliability, fairness, and other ethical concerns [4]. At the same time, current developments in AI can solve long-standing RE problems, such as automatic requirements tracing, completeness checking, and modeling. AI can further create better connections between RE and other automated SE fields.The 1st International Workshop on (Multi-disciplinary, Open, and RElevant RE) (MO2RE) has the goal to address these issues by raising awareness of RE's diverse aspects and fostering collaboration within the SE community.},
location = {Lisbon, Portugal}
}

@inproceedings{10.1145/3734436.3734439,
author = {Hasel Mehri, Gelareh and Morisset, Charles and Zannone, Nicola},
title = {Towards Explainable Access Control [BlueSky Paper]},
year = {2025},
isbn = {9798400715037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3734436.3734439},
doi = {10.1145/3734436.3734439},
abstract = {Access control (AC) systems play an important role in ensuring security by regulating how resources are accessed, protecting sensitive information, and maintaining system integrity. Their complexity arises not only from diverse policies and mechanisms but also from the involvement of multiple stakeholders, including resource owners, administrators, and end-users. Taking inspiration from explainable AI and explainable security, we define the first model of access control explainability, as a quality measure of the explanation graph constructed around the decisions made within the AC system. We then explore the literature to identify how existing work can be integrated as explanatory processes. Finally, we leverage our framework to articulate three open research challenges: the collection and interpretation of AC decisions, the effective construction of AC explanation graphs, and the definition of meaningful and computationally efficient explanation quality metrics.},
booktitle = {Proceedings of the 30th ACM Symposium on Access Control Models and Technologies},
pages = {117–126},
numpages = {10},
keywords = {explainable access control, explainability framework, policy comprehension},
location = {USA},
series = {SACMAT '25}
}

@inproceedings{10.1145/3526242.3526254,
author = {Chatzipanagiotou, Marita and Machotka, Ewa and Pavlopoulos, John},
title = {Automated recognition of geographical named entities in titles of Ukiyo-e prints},
year = {2022},
isbn = {9781450387361},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3526242.3526254},
doi = {10.1145/3526242.3526254},
abstract = {This paper investigates the application of Natural Language Processing as a means to study the relationship between topography and its visual renderings in early modern Japanese ukiyo-e landscape prints. We introduce a new dataset with titles of landscape prints that have been annotated by an art historian for any included place-names. The prints are hosted by the digital database of the Art Research Center at the Ritsumeikan University, Kyoto, one of the hubs of Digital Humanities in Japan. By applying, calibrating and assessing a Named Entity Recognition (NER) tool, we argue that ‘distant viewing’ or macroanalysis of visual datasets can be facilitated, which is needed to assist art historical studies of this rich, complex and diverse research material. Experimental results indicated that the performance of NER can be improved by 30\% and reach 50\% precision, by using part of the introduced dataset.},
booktitle = {Digital Humanities Workshop},
pages = {70–77},
numpages = {8},
keywords = {natural language processing, named entity recognition, art history, Ukiyo-e prints},
location = {Kyiv, Ukraine},
series = {DHW 2021}
}

@inproceedings{10.1145/3421766.3421812,
author = {Zhu, Dengyun and Guo, Qi and Zhang, Dongjiao and Wan, Fucheng},
title = {Research on the Labelling Technology of Morphology and Syntax},
year = {2020},
isbn = {9781450375535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3421766.3421812},
doi = {10.1145/3421766.3421812},
abstract = {This paper aimed the integration tagging and tree-bank transformation of morphology and syntax on the basis of phrase and syntax tree-bank, tagged the nested named entity in combination with the ontological linguistic clues. Finally, it integrates the named entity to carry out the integrative experimental analysis; according to the experimental results, both the accuracy rate and recall rate have been improved somewhat.},
booktitle = {Proceedings of the 2nd International Conference on Artificial Intelligence and Advanced Manufacture},
pages = {181–184},
numpages = {4},
keywords = {tagging, named entity identification, Integration of morphology and syntax},
location = {Manchester, United Kingdom},
series = {AIAM2020}
}

@article{10.1145/3543508,
author = {Opdahl, Andreas L. and Al-Moslmi, Tareq and Dang-Nguyen, Duc-Tien and Gallofr\'{e} Oca\~{n}a, Marc and Tessem, Bj\o{}rnar and Veres, Csaba},
title = {Semantic Knowledge Graphs for the News: A Review},
year = {2022},
issue_date = {July 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {7},
issn = {0360-0300},
url = {https://doi.org/10.1145/3543508},
doi = {10.1145/3543508},
abstract = {ICT platforms for news production, distribution, and consumption must exploit the ever-growing availability of digital data. These data originate from different sources and in different formats; they arrive at different velocities and in different volumes. Semantic knowledge graphs (KGs) is an established technique for integrating such heterogeneous information. It is therefore well-aligned with the needs of news producers and distributors, and it is likely to become increasingly important for the news industry. This article reviews the research on using semantic knowledge graphs for production, distribution, and consumption of news. The purpose is to present an overview of the field; to investigate what it means; and to suggest opportunities and needs for further research and development.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {140},
numpages = {38},
keywords = {literature review, Semantic Web, Linked Open Data, Linked Data, semantic technologies, ontology, knowledge graphs, news consumption, news distribution, news production, journalism, News}
}

@inproceedings{10.1145/3673277.3673306,
author = {Peng, Zhen and Du, Ye and Chen, Qifang and Zheng, Tianshuai},
title = {Research on Knowledge Graph Construction for Smart Grid Cybersecurity},
year = {2024},
isbn = {9798400716959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3673277.3673306},
doi = {10.1145/3673277.3673306},
abstract = {This paper proposes a construction method for smart grid cybersecurity knowledge graph and solves the difficulty of multilingual entity extraction with a small amount of labeled data. First, the construction method of smart grid cybersecurity knowledge graph is proposed with the multi-source heterogeneous data in the field of electric power cybersecurity collected by subject crawlers. Then, for the problems of insufficient labeled data and language mixing in the electric power cybersecurity domain, a DA-XLMR-BiLSTM-FC-CRF model based on a five-layer architecture is proposed to realize the entity extraction of multilingual unstructured text. Finally, comparative and ablation experiments are designed to prove the effectiveness of the proposed model, and the F1 value of the model reaches 94.04\% and the accuracy rate reaches 94.48\%.},
booktitle = {Proceedings of the 2024 3rd International Conference on Cryptography, Network Security and Communication Technology},
pages = {164–170},
numpages = {7},
location = {Harbin, China},
series = {CNSCT '24}
}

@inproceedings{10.1145/3649921.3659847,
author = {Li, Cynthia and Osborn, Joseph},
title = {Translating Between Game Generators with Asterism and Ceptre},
year = {2024},
isbn = {9798400709555},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649921.3659847},
doi = {10.1145/3649921.3659847},
abstract = {In this paper, we present in-progress work that converts games made with Ceptre, a genre-agnostic game description language, into graphical games using the framework of operational logics. Our preliminary code targets the translation of tilemap-based dungeon crawlers, but we present strategies for generalizing this process to other Ceptre games and Asterism engines. We gesture at the potential of operational logics and Asterism as a tool to communicate across the many frameworks surrounding game development and playing.},
booktitle = {Proceedings of the 19th International Conference on the Foundations of Digital Games},
articleno = {70},
numpages = {4},
keywords = {formal models, game generators, operational logics},
location = {Worcester, MA, USA},
series = {FDG '24}
}

@inproceedings{10.1145/3722237.3722276,
author = {Ming, Jing},
title = {The personalized university English learning system in computer-driven research},
year = {2025},
isbn = {9798400712692},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3722237.3722276},
doi = {10.1145/3722237.3722276},
abstract = {The aim of this study is to develop a computer-assisted personalised university English learning system to improve learning efficiency and effectiveness. Machine learning algorithms were used to analyse student data and build a personalised learning model. Experimental results show that the system significantly outperforms traditional methods in vocabulary acquisition (22\% improvement), listening comprehension (18\% improvement) and oral fluency (15\% improvement). The conclusion suggests that a personalised learning system based on artificial intelligence and big data can effectively improve university English teaching, and provide a new direction for the future development of educational technology. During the Eastern Han period, the production of everyday pottery, pottery sculptures, architectural bricks and tiles, a variety of ceramic sculptures, living statues, horses, acrobatics and other funerary objects, as well as new products such as large lofts, chariots and horses, became the main features of the industry.},
booktitle = {Proceedings of the 2024 3rd International Conference on Artificial Intelligence and Education},
pages = {222–225},
numpages = {4},
keywords = {Artificial Intelligence, Adaptive Learning, Data Mining, Language Acquisition, Educational Technology},
location = {
},
series = {ICAIE '24}
}

@inproceedings{10.1145/3587828.3587846,
author = {Tsiounis, Konstantinos and Kontogiannis, Kostas},
title = {Goal Driven Code Generation for Smart Contract Assemblies},
year = {2023},
isbn = {9781450398589},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3587828.3587846},
doi = {10.1145/3587828.3587846},
abstract = {We are currently witnessing the proliferation of blockchain environments to support a wide spectrum of corporate applications through the use of smart contracts. It is of no surprise that smart contract programming language technology constantly evolves to include not only specialized languages such as Solidity, but also general purpose languages such as GoLang and JavaScript. Furthermore, blockchain technology imposes unique challenges related to the monetary cost of deploying smart contracts, and handling roll-back issues when a smart contract fails. It is therefore evident that the complexity of systems involving smart contracts will only increase over time thus making the maintenance and evolution of such systems a very challenging task. One solution to these problems is to approach the implementation and deployment of such systems in a disciplined and automated way. In this paper, we propose a model-driven approach where the structure and inter-dependencies of smart contract, as well as stakeholder objectives, are denoted by extended goal models which can then be transformed to yield Solidity code that conforms with those models. More specifically, we present first a Domain Specific Language (DSL) to denote extended goal models and second, a transformation process which allows for the Abstract Syntax Trees of such a DSL program to be transformed into Solidity smart contact source code. The transformation process ensures that the generated smart contract skeleton code yields a system that is conformant with the model, which serves as a specification of said system so that subsequent analysis, understanding, and maintenance will be easier to achieve.},
booktitle = {Proceedings of the 2023 12th International Conference on Software and Computer Applications},
pages = {112–121},
numpages = {10},
keywords = {Smart contracts, Modular Design, Model-driven engineering, Goal models, Compliance, Code generation},
location = {Kuantan, Malaysia},
series = {ICSCA '23}
}

@inproceedings{10.1145/3610978.3640715,
author = {Wilson, Jason and Yang, Yuqi},
title = {Software Architecture to Generate Assistive Behaviors for Social Robots},
year = {2024},
isbn = {9798400703232},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3610978.3640715},
doi = {10.1145/3610978.3640715},
abstract = {To facilitate the design of socially assistive robots (SARs), we present an architecture to generate assistive behavior for social robots given a high-level description of the intent of the assistance. Our approach features an ontology of assistive intents, a hierarchical task network planner, and robot middleware. We demonstrate the behaviors on two robot platforms and compare the behaviors. While many of the behaviors are similar, challenges remain in generating behaviors that will be presented consistently across multiple platforms.},
booktitle = {Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1119–1123},
numpages = {5},
keywords = {HTN planning, behavior generation, ontology, socially assistive robot, software architecture},
location = {Boulder, CO, USA},
series = {HRI '24}
}

@inproceedings{10.1145/3490099.3511130,
author = {Smith, Ronnie and Dragone, Mauro},
title = {A Dialogue-Based Interface for Active Learning of Activities of Daily Living},
year = {2022},
isbn = {9781450391443},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3490099.3511130},
doi = {10.1145/3490099.3511130},
abstract = {While Human Activity Recognition (HAR) systems may benefit from Active Learning (AL) by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in HAR systems, which utilises a dataset of natural language descriptions of common activities (which we make publicly available) and semantic similarity measures. Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply our work to an existing CASAS dataset in an active learning scenario, to demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) technically, as an effective way to seek users’ input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to use our approach and an established method, and to subsequently compare the two. Results show the potential of our approach as a user-friendly mechanism for annotation of sensor data as part of an active learning system.},
booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
pages = {820–831},
numpages = {12},
keywords = {Active Learning (AL), Human Activity Recognition (HAR) labelling, Human-in-the-Loop (HITL) annotation, natural language, semantic similarity},
location = {Helsinki, Finland},
series = {IUI '22}
}

@article{10.1109/TASLP.2023.3240661,
author = {Liu, Hong and Cai, Yucheng and Lin, Zhenru and Ou, Zhijian and Huang, Yi and Feng, Junlan},
title = {Variational Latent-State GPT for Semi-Supervised Task-Oriented Dialog Systems},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3240661},
doi = {10.1109/TASLP.2023.3240661},
abstract = {Recently, two approaches, fine-tuning large pre-trained language models and variational training, have attracted significant interests, separately, for semi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper, we propose Variational Latent-State GPT model (VLS-GPT), which is the first to combine the strengths of the two approaches. Among many options of models, we propose the generative model and the inference model for variational learning of the end-to-end TOD system, both as auto-regressive language models based on GPT-2, which can be further trained over a mix of labeled and unlabeled dialog data in a semi-supervised manner. Variational training of VLS-GPT is both statistically and computationally more challenging than previous variational learning works for sequential latent variable models, which use turn-level first-order Markovian. The inference model in VLS-GPT is non-Markovian due to the use of the Transformer architecture. In this work, we establish Recursive Monte Carlo Approximation (RMCA) to the variational objective with non-Markovian inference model and prove its unbiasedness. Further, we develop the computational strategy of sampling-then-forward-computation to realize RMCA, which successfully overcomes the memory explosion issue of using GPT in variational learning and speeds up training. Semi-supervised TOD experiments are conducted on two benchmark multi-domain datasets of different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to significantly outperform both supervised-only and semi-supervised self-training baselines.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = jan,
pages = {970–984},
numpages = {15}
}

@inproceedings{10.1145/3600100.3623737,
author = {Ramanathan, Ganesh and Mayer, Simon},
title = {Reasoning about Physical Processes in Buildings through Component Stereotypes},
year = {2023},
isbn = {9798400702303},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600100.3623737},
doi = {10.1145/3600100.3623737},
abstract = {Buildings employ an ensemble of technical systems like those for heating and ventilation and each of them orchestrate complex physical processes. Ontologies such as Brick, IFC, SSN/SOSA, and SAREF have been created to describe the technical systems in a machine-understandable manner. However, such ontologies focus largely on describing system topology, whereas several use cases, such as automated fault detection and diagnostics (AFDD), also need knowledge about the physical processes. Physical processes can be described using mathematical simulation models, but this is practically too expensive for building automation systems and their integration with mainstream technical systems ontologies is still under-explored. We propose to address these challenges by introducing the concept of component stereotypes that describe the effect of component actuation on the state its underlying physical mechanism. These stereotypes are then linked to actual component instances in the technical system description, thereby accomplishing an integration of structural description with knowledge about physical processes. We contribute an ontology for such stereotypes and evaluate it with respect to the coverage of HVAC components in Brick and its ability to automatically infer relationships between components in a real-world building. We show how the resulting knowledge graph can be queried by AFDD applications to know about expected consequences of an action, or conversely, identify components that may be responsible for an observed state of the process. While we are able to report a coverage of 100\% of Brick HVAC components, the automatic inference underreports component dependencies in real-world installations. This points at a group of concepts which we propose should be considered in future versions of the Brick ontology.},
booktitle = {Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
pages = {120–129},
numpages = {10},
location = {Istanbul, Turkey},
series = {BuildSys '23}
}

@inproceedings{10.1145/3459930.3469533,
author = {Noh, Jiho and Kavuluru, Ramakanth},
title = {Joint learning for biomedical NER and entity normalization: encoding schemes, counterfactual examples, and zero-shot evaluation},
year = {2021},
isbn = {9781450384506},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459930.3469533},
doi = {10.1145/3459930.3469533},
abstract = {Named entity recognition (NER) and normalization (EN) form an indispensable first step to many biomedical natural language processing applications. In biomedical information science, recognizing entities (e.g., genes, diseases, or drugs) and normalizing them to concepts in standard terminologies or thesauri (e.g., Entrez, ICD-10, or RxNorm) is crucial for identifying more informative relations among them that drive disease etiology, progression, and treatment. In this effort we pursue two high level strategies to improve biomedical ER and EN. The first is to decouple standard entity encoding tags (e.g., "B-Drug" for the beginning of a drug) into type tags (e.g., "Drug") and positional tags (e.g., "B"). A second strategy is to use additional counterfactual training examples to handle the issue of models learning spurious correlations between surrounding context and normalized concepts in training data. We conduct elaborate experiments using the MedMentions dataset, the largest dataset of its kind for ER and EN in biomedicine. We find that our first strategy performs better in entity normalization when compared with the standard coding scheme. The second data augmentation strategy uniformly improves performance in span detection, typing, and normalization. The gains from counterfactual examples are more prominent when evaluating in zero-shot settings, for concepts that have never been encountered during training.},
booktitle = {Proceedings of the 12th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
articleno = {55},
numpages = {10},
keywords = {biomedical natural language processing, deep neural networks, entity normalization, information extraction, named entity recognition},
location = {Gainesville, Florida},
series = {BCB '21}
}

@article{10.5555/3586589.3586815,
author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
title = {Underspecification presents challenges for credibility in modern machine learning},
year = {2022},
issue_date = {January 2022},
publisher = {JMLR.org},
volume = {23},
number = {1},
issn = {1532-4435},
abstract = {Machine learning (ML) systems often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification in ML pipelines as a key reason for these failures. An ML pipeline is the full procedure followed to train and validate a predictor. Such a pipeline is underspecified when it can return many distinct predictors with equivalently strong test performance. Underspecification is common in modern ML pipelines that primarily validate predictors on held-out data that follow the same distribution as the training data. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We provide evidence that underspecfication has substantive implications for practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {226},
numpages = {61},
keywords = {genomics, electronic health records, medical imaging, natural language processing, computer vision, identifiability, fairness, spurious correlation, distribution shift}
}

@inproceedings{10.1145/3706598.3713715,
author = {D\"{u}ck, Moritz and Holter, Steffen and Chan, Robin Shing Moon and Sevastjanova, Rita and El-Assady, Mennatallah},
title = {Finding Needles in Document Haystacks: Augmenting Serendipitous Claim Retrieval Workflows},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713715},
doi = {10.1145/3706598.3713715},
abstract = {Preliminary exploration of vast text corpora for generating and validating hypotheses, typical in academic inquiry, requires flexible navigation and rapid validation of claims. Navigating the corpus by titles, summaries, and abstracts might neglect information, whereas identifying the relevant context-specific claims through in-depth reading is unfeasible with rapidly increasing publication numbers. Our paper identifies three typical user pathways for hypothesis exploration and operationalizes sentence-based retrieval combined with effective contextualization and provenance tracking in a unified workflow. We contribute an interface that augments the previously laborious tasks of claim identification and consistency checking using NLP techniques while balancing user control and serendipity. Use cases, expert interviews, and a user study with 10 participants demonstrate how the proposed workflow enables users to traverse literature corpora in novel and efficient ways. For the evaluation, we instantiate the tool within two independent domains, providing novel insights into the analysis of political discourse and medical research.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1003},
numpages = {17},
keywords = {human-AI interaction, natural language processing, provenance, serendipity, text data},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3632314.3632332,
author = {Guo, Dongdong and Ma, Haitao and Zhao, Can and Peng, Hao and Du, Wenbo and Jiang, Zongrui and Zhang, Yan},
title = {Construction and Application of the Knowledge Graph Method in Maintenance of Robot in Automotive Manufacturing Industry},
year = {2023},
isbn = {9798400709401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632314.3632332},
doi = {10.1145/3632314.3632332},
abstract = {Based on the spare parts and structure data of industrial robots, the entity list of robot parts is established to form a query dictionary, and entity annotation is performed on the robot corpus by means of dictionary query, which reduces the cost of manual annotation and ensures the quality of annotation data. In the process of entity recognition training, Bert+Bilstm+CRF model structure is used to initially use 70\% of the dictionary data for annotation, and the model is trained by iteratively increasing the annotation data in a continuous cycle, so that the model can extract all the entities in the robot corpus as much as possible. In addition, the material number/model information and PM maintenance content/strategy of the entity have been used as attributes of the entity. Meanwhile, the experience summarized by the failure model and effect analysis of industrial robots is fully utilized to connect the phenomena, causes and measures through the entities in order to build the industrial robot knowledge graph relationships. The constructed knowledge graph relationship is stored in a Neo4j graphical database, making it convenient for content retrieval and inquiry of application systems.In the industrial robot knowledge graph application side, the field maintenance personnel requirements are collected through a questionnaire survey and the requirements are classified into intent. A Bert+TextCNN structure model is built to realize the intention recognition of user inquiries. By combining entity recognition models and intent classification models, the system is able to better understand user inquiry needs, leading to the implementation of an intelligent maintenance system for industrial robots.},
booktitle = {Proceedings of the 2023 International Conference on Intelligent Sensing and Industrial Automation},
articleno = {15},
numpages = {9},
keywords = {Knowledge Graph, Robot Maintenance},
location = {Virtual Event, China},
series = {ISIA '23}
}

@inproceedings{10.1145/3664476.3664523,
author = {Ruman, \'{A}d\'{a}m and Dra\v{s}ar, Martin and Sadlek, Luk\'{a}\v{s} and Yang, Shanchieh Jay and Celeda, Pavel},
title = {Adversary Tactic Driven Scenario and Terrain Generation with Partial Infrastructure Specification},
year = {2024},
isbn = {9798400717185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3664476.3664523},
doi = {10.1145/3664476.3664523},
abstract = {Diverse, accurate, and up-to-date training environments are essential for training cybersecurity experts and autonomous systems. However, preparation of their content is time-consuming and requires experts to provide detailed specifications. In this paper, we explore the challenges of automated generation of the content (composed of scenarios and terrains) for these environments. We propose new models to represent the cybersecurity domain and associated action spaces. These models are used to create sound and complex training content based on partial specifications provided by users. We compare the results with a real-world complex malware campaign to assess the realism of the synthesized content. To further evaluate the correctness and variability of the results, we utilize the kill-chain attack graph generation for the generated training content to asses the internal correspondence of its key components. Our results demonstrate that the proposed approach can create complex training content similar to advanced attack campaigns, which passes evaluation for soundness and practicality. Our proposed approach and its implementation significantly contribute to the state of the art, enabling novel approaches to cybersecurity training and autonomous system development.},
booktitle = {Proceedings of the 19th International Conference on Availability, Reliability and Security},
articleno = {33},
numpages = {11},
keywords = {adversary framework, attack scenario generation, cyber terrain generation, cybersecurity model},
location = {Vienna, Austria},
series = {ARES '24}
}

@article{10.1145/3480238,
author = {Finkel, Raphael and Kaufman, Daniel and Shamim, Ahmed},
title = {Analyzing Code-mixing in Linguistic Corpora Using Kratylos},
year = {2022},
issue_date = {February 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1556-4673},
url = {https://doi.org/10.1145/3480238},
doi = {10.1145/3480238},
abstract = {Code-switching, code-mixing, and, more generally, multilingualism pose technological challenges for language documentation, the sub-discipline of linguistics that deals with the annotation and basic analysis of field recordings and other primary data. We focus here on a case study involving code-mixing in the endangered Koda language, which poses special problems for morphosyntactic analysis. We offer a robust approach to multilingual annotations that involves a combination of the popular open source software FieldWorks Language Explorer (FLEx) with Kratylos, a web-based corpus tool for display and query. Kratylos exposes linguistic data from various formats to powerful regular-expression queries that can exploit tier structure and other aspects of interlinear glossed text. We show how Kratylos can target mixed structures in our FLEx database of Koda that cannot be easily identified within the original FLEx software itself.},
journal = {J. Comput. Cult. Herit.},
month = jan,
articleno = {3},
numpages = {15},
keywords = {lexicons, interlinear glossed texts, linguistics, Language archives}
}

@inproceedings{10.1145/3715335.3735456,
author = {Bou Nassar, Jessica and Anwar, Misita and Bartram, Lyn and Sharp, Darren and Goodwin, Sarah},
title = {‘Unsolvable within existing regimes’: Using a Systems Thinking Approach to Co-design for Data Governance in Cities},
year = {2025},
isbn = {9798400714849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715335.3735456},
doi = {10.1145/3715335.3735456},
abstract = {Despite people’s significant role in generating data in cities, their involvement in data governance (DG) remains limited, failing to address the inherent complexity of DG and undermining their ’right to the city’. We propose a collaborative systems thinking approach as a scoping tool for co-design, enabling researchers and designers to involve people in co-creating an understanding of the systemic structures underpinning DG in cities and developing prototypes and solutions informed by these structures. Using causal loop diagrams, we facilitated the development of a conceptual model of DG. Participants, representing diverse perspectives, created individual causal loop diagrams that were merged into a collaborative causal loop diagram (C-CLD). This C-CLD was employed in an interactive workshop to identify intervention points and develop targeted solutions. Our findings demonstrate how C-CLDs can accommodate multiplicity, foster agonism, and enable participants to challenge political dimensions and existing systemic structures. Moreover, the engagement process revealed the complexity of DG in the city, as perceived by the collective of participants, resulting in three key submodules that highlight tensions between citizen sensitisation to data collection, the private sector’s role in fulfilling citizens’ needs, and the struggles faced by local governments. This work draws on and extends HCI research that engages with systems thinking ontologies, contributing to an HCI that includes the political, moves beyond solutionism, and advances social justice-oriented approaches.},
booktitle = {Proceedings of the 2025 ACM SIGCAS/SIGCHI Conference on Computing and Sustainable Societies},
pages = {48–67},
numpages = {20},
keywords = {Data governance, Systems thinking, Causal loop diagram, Co-design, Cities},
location = {
},
series = {COMPASS '25}
}

@inproceedings{10.1145/3701571.3701608,
author = {Zargham, Nima and Dubiel, Mateusz and Desai, Smit and Mildner, Thomas and Belz, Hanz-Joachim},
title = {Designing AI Personalities: Enhancing Human-Agent Interaction Through Thoughtful Persona Design},
year = {2024},
isbn = {9798400712838},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3701571.3701608},
doi = {10.1145/3701571.3701608},
abstract = {In the rapidly evolving field of artificial intelligence (AI) agents, designing the agent’s characteristics is crucial for shaping user experience. This workshop aims to establish a research community focused on AI agent persona design for various contexts, such as in-car assistants, educational tools, and smart home environments. We will explore critical aspects of persona design, such as voice, embodiment, and demographics, and their impact on user satisfaction and engagement. Through discussions and hands-on activities, we aim to propose practices and standards that enhance the ecological validity of agent personas. Topics include the design of conversational interfaces, the influence of agent personas on user experience, and approaches for creating contextually appropriate AI agents. This workshop will provide a platform for building a community dedicated to developing AI agent personas that better fit diverse, everyday interactions.},
booktitle = {Proceedings of the International Conference on Mobile and Ubiquitous Multimedia},
pages = {490–494},
numpages = {5},
keywords = {conversational user interfaces, AI Agents, Personas, Speech Interfaces, Conversational Agents},
location = {
},
series = {MUM '24}
}

@article{10.1145/3631483.3631499,
author = {Lisboa Malaquias, Felipe and Giantamidis, Georgios and Basagiannis, Stylianos and Fulvio Rollini, Simone and Amundson, Isaac},
title = {Towards a Methodology to Design Provably Secure Cyber-physical Systems},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {1},
issn = {1094-3641},
url = {https://doi.org/10.1145/3631483.3631499},
doi = {10.1145/3631483.3631499},
abstract = {The inordinate financial cost of mitigating post-production cybersecurity vulnerabilities in cyber-physical systems (CPS) is forcing the industry to rethink systems design cycles: greater attention is being given to the design phase - with the goal of reducing the attack surface of systems at an early stage (i.e., before silicon tape out). Fortunately, formal methods have advanced to the point that they can address such needs and contribute towards achieving security certification. However, new methods and tools focusing on industrial scalability and usability for systems engineers are required. In this ongoing research paper, we describe a framework that will help systems engineers to: a) design cyber-assured CPS using a Model Based Engineering (MBE) approach; b) formally map security requirements to different hardware and software blocks in the model; and c) formally verify security requirements. Based on the nature of each requirement, our framework collects formal correctness evidence from different tools: while high-level architectural properties are suitable for a contract- or ontology-based reasoning, more complex properties with rich semantics require the use of model checking or theorem proving techniques.},
journal = {Ada Lett.},
month = oct,
pages = {94–99},
numpages = {6}
}

@inproceedings{10.1145/3589335.3651263,
author = {Jain, Monika},
title = {Knowledge Enabled Relation Extraction},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651263},
doi = {10.1145/3589335.3651263},
abstract = {Relation extraction is the task of extracting relationships from input text, where input can be a sentence, document, or multiple documents. This task has been popular for decades and is still of keen interest. Various techniques have been proposed to solve the relation extraction problem, among which the most popular are using distant supervision, deep learning-based models, reasoning-based models, and transformer-based models. We propose three approaches (named ReOnto, DocRE-CLip, and KDocRE) for relation extraction from text at three levels of granularity (sentence, document and across documents). These approaches embed knowledge in a deep learning based model to improve performance. ReOnto and DocRE-CLip have been evaluated and the source code is publicly available. We are currently implementing and evaluating KDocRE.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1210–1213},
numpages = {4},
keywords = {graph neural network, knowledge graph, neurosymbolic ai, ontology, relation extraction},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3624062.3624099,
author = {Mcquaigue, Matthew and Saule, Erik and Subramanian, Kalpathi and Payton, Jamie},
title = {Data-Driven Discovery of Anchor Points for PDC Content},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624099},
doi = {10.1145/3624062.3624099},
abstract = {The Parallel and Distributed Computing community has been interested in integrating PDC content into early CS curriculum to prime the students for more advanced materials and build a workforce able to leverage advanced computing infrastructure. To deploy this strategy at scale, it is important to identify anchor points in early CS courses where we can insert PDC content. We present an analysis of CS courses that primarily focuses on CS1 and Data Structure courses. We collected data on course content through in-person workshops, where instructors of courses classified their course materials against standard curriculum guidelines. By using these classification, we make sense of how Computer Science is being taught. We highlight different types of CS1 and Data Structure courses. And we provide reflection on how that knowledge can be used by PDC experts to identify anchoring points for PDC content, while being sensitive to the needs of instructors.},
booktitle = {Proceedings of the SC '23 Workshops of the International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {335–342},
numpages = {8},
keywords = {CS Education, Course Model, Curriculum Guidelines, Integrating PDC in Early CS},
location = {Denver, CO, USA},
series = {SC-W '23}
}

@inproceedings{10.1145/3659677.3659827,
author = {Bystrov, Dmitriy},
title = {Information Retrieval Multi-Agent System Established on the Metaphysics Lexical Database},
year = {2024},
isbn = {9798400709296},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3659677.3659827},
doi = {10.1145/3659677.3659827},
abstract = {The system, retrieving information from heterogeneous sources is discussed. Architecture of the system is based on multi-agent approach. The user's queries could be presented in the native language. Retrieving process based on using knowledge, representing via ontology scheme. For this purpose the wordnet ontology is used. The development process of the ontology is discussed. The presented system works on a distributed environment, where component agents collaborate via XML web services and SOAP protocols.},
booktitle = {Proceedings of the 7th International Conference on Networking, Intelligent Systems and Security},
articleno = {53},
numpages = {3},
location = {Meknes, AA, Morocco},
series = {NISS '24}
}

@inproceedings{10.1145/3603781.3603871,
author = {Huang, Pengcheng and Li, Li and Wu, Chunyan and Zhang, Xiaoqian and Liu, Zhigui},
title = {A Study of Sentence-BERT Based Essay Off-topic Detection},
year = {2023},
isbn = {9798400700705},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3603781.3603871},
doi = {10.1145/3603781.3603871},
abstract = {Automated essay scoring systems are widely used in education, and essay off-topic detection is an integral part of this. Traditionally off-topic essay detection is based on text features represented as spatial vectors, however, this approach only addresses the structure of essay statements and requires the use of manual features. This paper proposed to use the Sentence-BERT model to detect off-topic essays, the method first obtains a large amount of high-quality data to build a corpus of off-topic essays, and two Siamese twin pre-trained models are used to embed sentences in the essay topic, and the body of the essay, generate semantically rich sentence vectors and then use cosine similarity to calculate the similarity between the topic and the body of the essay after averaging the pooled sentence vectors, and select the optimal threshold to determine off-topic essays through continuous training. The experimental results show that the proposed method improves the accuracy, recall, and F1 values by 9.5\%, 11.2\%, and 10.4\% respectively over the C-BGRU (Convolutional-Bidirectional Gate Recurrent Unit) based Siamese twin network and also has an excellent performance in topics with different degrees of divergence.},
booktitle = {Proceedings of the 2023 4th International Conference on Computing, Networks and Internet of Things},
pages = {515–519},
numpages = {5},
keywords = {Siamese network, Pre-training models, Off-topic essay detection, Cosine similarity},
location = {Xiamen, China},
series = {CNIOT '23}
}

@proceedings{10.1145/3689944,
title = {SCORED '24: Proceedings of the 2024 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses},
year = {2024},
isbn = {9798400712401},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to ACM SCORED '24, the third edition of the ACM Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses. This edition is held in Salt Lake City, Utah, United States with extensive support for in-person and virtual attendance. This year's program includes exciting work along many different dimensions of research on supply chain security: the development of security policies for software supply chains, the use of artificial intelligence and large language models, approaches on software bills of materials, and the proposals of risk mitigation techniques. Consistent with its focus, SCORED brings researchers, legislators and practitioners in both open- and closed-source ecosystems to the center of current and emerging challenges and opportunities in software supply chain security.},
location = {Salt Lake City, UT, USA}
}

@article{10.1145/3642979.3642995,
author = {B\'{e}n\'{e}dict, Gabriel and Zhang, Ruqing and Metzler, Donald and Yates, Andrew and Deffayet, Romain and Hager, Philipp and Jullien, Sami},
title = {Report on the 1st Workshop on Generative Information Retrieval (Gen-IR 2023) at SIGIR 2023},
year = {2024},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {2},
issn = {0163-5840},
url = {https://doi.org/10.1145/3642979.3642995},
doi = {10.1145/3642979.3642995},
abstract = {The first edition of the workshop on Generative Information Retrieval (Gen-IR 2023) took place in July 2023 in a hybrid fashion, co-located with the ACM SIGIR Conference 2023 in Taipei (SIGIR 2023). The aim was to bring information retrieval researchers together around the topic of generative AI that gathered attention in 2022 and 2023 with large language models and diffusion models. Given the novelty of the topic, the workshop was focused around multi-sided discussions, namely panels and poster sessions of the accepted proceedings papers. Two main research outcomes are the proceedings of the workshop1 and the potential research directions discussed in this report.Date: 27 July 2023.Website: https://coda.io/@sigir/gen-ir.},
journal = {SIGIR Forum},
month = jan,
articleno = {13},
numpages = {23}
}

@inproceedings{10.1145/3724363.3729049,
author = {McDermott, Roger and Daniels, Mats and Brown, John N.A. and Cajander, \r{A}sa},
title = {Determining the Scope of the Philosophy of Computing Education},
year = {2025},
isbn = {9798400715679},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3724363.3729049},
doi = {10.1145/3724363.3729049},
abstract = {There are a number of different approaches to the investigation of teaching and learning within the subject of Computing Education. Many of the advances in pedagogy that have taken place over the past thirty years have been due to careful statistical analysis of empirical data, enhancing the reputation of the subject within the broader Computing discipline. Empirical, qualitative methodologies, of the kinds used extensively in the Social Sciences, have also appeared in the Computing Education literature, often investigating the socio-cultural aspects of the subject. More recently, there has been a proposal to develop a role for philosophical inquiry in Computing Education, which mirrors similar historical developments in Engineering Education. Rather than focus on the quantitative or qualitative analysis of the student experience, philosophical investigation instead relies on the use of conceptual analysis to investigate the detailed semantic content of ideas raised in the practice of computing education, careful analysis of the methodologies used to do such work, and a critique of the assumptions that underlie the subject.In this paper, we investigate ways in which an understanding of the Philosophy of Computing Education can assist research within the subject. We consider how it emerges from basic questions about nature of the subject, its scope, and how it can be applied fruitfully within the discipline.},
booktitle = {Proceedings of the 30th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {403–409},
numpages = {7},
keywords = {axiology, conceptual analysis, epistemology, methodology, ontology, philosophy of computing education},
location = {Nijmegen, Netherlands},
series = {ITiCSE 2025}
}

@proceedings{10.1145/3622758,
title = {Onward! 2023: Proceedings of the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2023},
isbn = {9798400703881},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2023 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2023), the premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities and applications. Onward! is more radical, more visionary, and more open than other conferences to ideas that are well-argued but not yet fully proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research.    Onward! 2023 is co-located with SPLASH 2023, running from Sunday 22nd of October till Friday 27th of October, in Cascais, Portugal. We are delighted to have Felienne Hermans giving the Onward! keynote, on Wednesday 25th of October, on "Creating a learnable and inclusive programming language".    All papers and essays that lie here before you received at least three reviews, leading to a decision of accept, reject, or conditional accept. Authors of conditionally accepted papers were provided with explicit requirements for acceptance, and were carefully re-reviewed in the second phase. The essays track received six submissions, out of which four were accepted. The papers track accepted nine out of nineteen submissions.   We hope that the papers and essays in these proceedings will stimulate and challenge your thinking about programming and software engineering, and we are looking forward to many discussions at the conference.},
location = {Cascais, Portugal}
}

@inproceedings{10.1145/3685651.3686699,
author = {Nizamis, Alexandros and Ioannidis, Dimosthenis and Gkonis, Panagiotis and Trakadas, Panagiotis},
title = {Introducing an Enhanced Metadata Broker for Manufacturing Data Spaces},
year = {2024},
isbn = {9798400709845},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3685651.3686699},
doi = {10.1145/3685651.3686699},
abstract = {Nowadays, collaborative ecosystems and value networks have been established based on data sharing mechanisms and principles coming from concepts like Data Spaces. This data-centric approach has also increased the need for effective metadata management that enables entities participating in data sharing scenarios to find and trust available data. In this paper, a metadata broker for manufacturing related Data Spaces is introduced. It is based on an ontology that has been implemented to describe data related to Industries 4.0 and 5.0 implementations. The proposed broker is based on Data Spaces principles and artefacts that it extends by enabling semantic-based modeling and search capabilities.},
booktitle = {Proceedings of the 4th Eclipse Security, AI, Architecture and Modelling Conference on Data Space},
pages = {37–40},
numpages = {4},
keywords = {Data Spaces, Industry 4.0 / 5.0, Metadata Broker, Metadata Registry},
location = {Mainz, Germany},
series = {eSAAM '24}
}

@article{10.1145/3648360,
author = {Yu, Yang and Qiu, Dong and Wan, Huanyu},
title = {Sentiment Analysis Method of Epidemic-related Microblog Based on Hesitation Theory},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {2375-4699},
url = {https://doi.org/10.1145/3648360},
doi = {10.1145/3648360},
abstract = {The COVID-19 pandemic in 2020 brought an unprecedented global crisis. After two years of control efforts, life gradually returned to the pre-pandemic state, but localized outbreaks continued to occur. Toward the end of 2022, COVID-19 resurged in China, leading to another disruption of people’s lives and work. Many pieces of information on social media reflected people’s views and emotions toward the second outbreak, which showed distinct differences compared to the first outbreak in 2020. To explore people’s emotional attitudes toward the pandemic at different stages and the underlying reasons, this study collected microblog data from November 2022 to January 2023 and from January to June 2020, encompassing Chinese reactions to the COVID-19 pandemic. Based on hesitancy and the Fuzzy Intuition theory, we proposed a hypothesis: hesitancy can be integrated into machine learning models to select suitable corpora for training, which not only improves accuracy but also enhances model efficiency. Based on this hypothesis, we designed a hesitancy-integrated model. The experimental results demonstrated the model’s positive performance on a self-constructed database. By applying this model to analyze people’s attitudes toward the pandemic, we obtained their sentiments in different months. We found that the most negative emotions appeared at the beginning of the pandemic, followed by emotional fluctuations influenced by social events, ultimately showing an overall positive trend. Combining word cloud techniques and the Latent Dirichlet Allocation (LDA) model effectively helped explore the reasons behind the changes in pandemic attitude.},
journal = {ACM Trans. Asian Low-Resour. Lang. Inf. Process.},
month = apr,
articleno = {50},
numpages = {25},
keywords = {COVID-19, sentiment analysis, hesitancy, fuzzy intuition theory, machine learning}
}

@inproceedings{10.1145/3726122.3726123,
author = {Zufarova, Nozima and Kasimova, Zilola and Aripkhodjaev, Saidamir},
title = {Branding Strategies for Education Using NLP and Knowledge Based Systems},
year = {2025},
isbn = {9798400711701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3726122.3726123},
doi = {10.1145/3726122.3726123},
abstract = {Knowledge-based systems can provide educational institutions with tailored branding strategies based on their data-driven insights, so it is necessary to integrate advanced computational methods. People are accustomed to using informal and dynamic language on the internet to express their preferences and expectations. In order to obtain actionable insights from large-scale textual data, it is necessary to employ natural language processing techniques to extract meaningful patterns and trends. Therefore, the study introduced TF-IDF analysis and Topic Modeling for textual data analysis. At the same time, the study introduced TF-IDF analysis and Topic Modeling for identifying key branding themes and introduced a knowledge-based system combining rule-based decision-making models and ontological frameworks based on branding domain knowledge, thereby constructing a comprehensive branding framework. The innovation of the research lies in the synergistic combination of traditional frequency-based analysis and coherence-driven topic modeling feature weight calculation methods with knowledge-based systems to formulate adaptive branding strategies, thereby improving the efficiency and precision of branding methodologies. The outcomes indicated that the accuracy of the classification model combining Topic Modeling and TF-IDF based on educational branding datasets was 94\%, the coherence score was as high as 0.724, and the term frequency recall was 89\%. Meanwhile, the classification error rate of the model was only 6\%. In addition, the proposed branding system, which adopted a knowledge-driven framework, had an average of 2,500 daily visits per person, with an effective browsing time of 12 minutes per person, and a daily browsing page count of 15 pages per person. This indicates that the knowledge-based branding framework has strong practical application effects and provides reliable technical support for current research in the field of educational branding.},
booktitle = {Proceedings of the 8th International Conference on Future Networks \&amp; Distributed Systems},
pages = {1–7},
numpages = {7},
location = {
},
series = {ICFNDS '24}
}

@inproceedings{10.5555/3721488.3721611,
author = {Ferrini, Lorenzo and Lemaignan, S\'{e}verin},
title = {VDB-based Spatially Grounded Semantics for Interactive Robots},
year = {2025},
publisher = {IEEE Press},
abstract = {This paper presents a new approach for representing spatially-grounded semantics in interactive robots. The method combines spatial and symbolic data to improve robot interactions in human-occupied environments. A key feature is a voxel-based data structure optimized for dynamic and sparse information, along with a global lookup table to manage and track spatially-grounded entities and their relationships. The implementation, which is integrated into a ROS 2-based framework, allows for seamless querying through semantic web APIs such as SPARQL. Initial tests demonstrate the efficiency of this system in supporting advanced scenarios in human-robot interaction. All the repositories developed as part of this contribution can be found at github.com/RepresentationMaps.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1005–1009},
numpages = {5},
keywords = {human-robot interaction, interactive robots, knowledge representation, semantic mapping},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3460210.3493564,
author = {Alghamdi, Ghadah and Schmidt, Renate A. and Del-Pinto, Warren and Gao, Yongsheng},
title = {Upwardly Abstracted Definition-Based Subontologies},
year = {2021},
isbn = {9781450384575},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460210.3493564},
doi = {10.1145/3460210.3493564},
abstract = {In this paper, we present a method for extracting subontologies from $mathcalELH $ ontologies for a set of symbols. The approach is focused on the generation of upwardly abstracted definitions of concepts, which is a technique for computing definitions expressed using closest primitive ancestors. The subontologies returned by the method are evaluated for quality and compared to extracts computed with locality-based modularisation and uniform interpolation. Our subontology generation method produces promising results in terms of size and relevance to the needs of domain experts.},
booktitle = {Proceedings of the 11th Knowledge Capture Conference},
pages = {209–216},
numpages = {8},
keywords = {subontologies, snomed ct, ontology summarisation, ontology modularisation, ontology engineering},
location = {Virtual Event, USA},
series = {K-CAP '21}
}

@proceedings{10.1145/3737609,
title = {AAR Adjunct '25: Adjunct Proceedings of the Sixth Decennial Aarhus Conference: Computing X Crisis},
year = {2025},
isbn = {9798400719684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {
}
}

@inproceedings{10.1145/3374587.3374637,
author = {Bogacheva, Evgenia and Puchkovskaia, Antonina and Smetannikov, Ivan},
title = {Named Entity Recognition for Russian Historical Texts},
year = {2020},
isbn = {9781450376273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374587.3374637},
doi = {10.1145/3374587.3374637},
abstract = {With the raise of big data, machine learning and crowdsourcing, the volume of existing datasets for different machine learning problems have greatly increased. The natural language processing field is not an exception; so, as a result, most of the researches have transitioned into investigating and applying different deep architectures for it. One of the main issues of this trend is as follows: it is hard to adopt such approaches for somewhat poorly studied languages, which do not have training data enough as for natural language processing perspective. In this paper, we investigate some modern approaches to named entity recognition as for Russian language and show that for historical texts their results are much lower than for general ones. In addition, we propose our own algorithm that improves the results of for these historical texts.},
booktitle = {Proceedings of the 2019 3rd International Conference on Computer Science and Artificial Intelligence},
pages = {13–17},
numpages = {5},
keywords = {named entity recognition, historical texts, Russian language, Natural language processing},
location = {Normal, IL, USA},
series = {CSAI '19}
}

@inproceedings{10.1145/3727505.3727544,
author = {Yang, Yan and Yao, Wenxu and Zhu, Yuangeng and Gao, Xiaoxin and Zheng, Yanrong and Liu, Yanan},
title = {Dynamic Construction and Application of Electric Power Thesaurus Based on Semantic Analysis},
year = {2025},
isbn = {9798400713620},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3727505.3727544},
doi = {10.1145/3727505.3727544},
abstract = {In order to improve the efficiency of knowledge management in the electric power industry, a lexicon of electric power topics based on semantic parsing is established in response to the dynamic changes of terminology and domain characteristics. It also integrates the semantic network with the ontology, constructs a multi-dimensional semantic association model, and utilizes deep learning, natural language processing, and other technologies to realize the automatic identification and classification of the vocabulary. Experiments prove that the method can effectively improve the learning effect of the lexicon. In particular, the coverage increases from 72\% to 92\% and from 12 to 16 months. In addition, the number of correctly recognized words has increased to 200 with a correction accuracy of 95\%. This series of improvements not only reflects the significant improvement of the new method in terms of accuracy and applicability, but also enhances the robustness of the new method in application-specific domain-oriented semantic analysis of power systems.},
booktitle = {Proceedings of the 2025 International Conference on Big Data, Communication Technology and Computer Applications},
pages = {226–232},
numpages = {7},
keywords = {electric power subject thesaurus, semantic analysis, semantic relations},
location = {
},
series = {BDCTA '25}
}

@article{10.1109/TASLP.2023.3313415,
author = {Wang, Ante and Song, Linfeng and Jin, Lifeng and Yao, Junfeng and Mi, Haitao and Lin, Chen and Su, Jinsong and Yu, Dong},
title = {D&lt;inline-formula&gt;&lt;tex-math notation="LaTeX"&gt;$^{2}$&lt;/tex-math&gt;&lt;/inline-formula&gt;PSG: Multi-Party Dialogue Discourse Parsing as Sequence Generation},
year = {2023},
issue_date = {2023},
publisher = {IEEE Press},
volume = {31},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2023.3313415},
doi = {10.1109/TASLP.2023.3313415},
abstract = {Conversational discourse analysis aims to extract the interactions between dialogue turns, which is crucial for modeling complex multi-party dialogues. As the benchmarks are still limited in size and human annotations are costly, the current standard approaches apply pretrained language models, but they still require randomly initialized classifiers to make predictions. These classifiers usually require massive data to work smoothly with the pretrained encoder, causing severe data hunger issue. We propose two convenient strategies to formulate this task as a sequence generation problem, where classifier decisions are carefully converted into sequence of tokens. We then adopt a pretrained T5 [C. Raffel et al., 2020] model to solve this task so that no parameters are randomly initialized. We also leverage the descriptions of the discourse relations to help model understand their meanings. Experiments on two popular benchmarks show that our approach outperforms previous state-of-the-art models by a large margin, and it is also more robust in zero-shot and few-shot settings.&lt;sup&gt;1&lt;/sup&gt;},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = sep,
pages = {4004–4013},
numpages = {10}
}

@inproceedings{10.1145/3183713.3193562,
author = {Basik, Fuat and H\"{a}ttasch, Benjamin and Ilkhechi, Amir and Usta, Arif and Ramaswamy, Shekar and Utama, Prasetya and Weir, Nathaniel and Binnig, Carsten and Cetintemel, Ugur},
title = {DBPal: A Learned NL-Interface for Databases},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3193562},
doi = {10.1145/3183713.3193562},
abstract = {In this demo, we present DBPal, a novel data exploration tool with a natural language interface. DBPal leverages recent advances in deep models to make query understanding more robust in the following ways: First, DBPal uses novel machine translation models to translate natural language statements to SQL, making the translation process more robust to paraphrasing and linguistic variations. Second, to support the users in phrasing questions without knowing the database schema and the query features, DBPal provides a learned auto-completion model that suggests to users partial query extensions during query formulation and thus helps to write complex queries.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1765–1768},
numpages = {4},
keywords = {robust natural language interface, relational database, nlidb, natural language to sql},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3744367.3744383,
author = {Xie, Zhixian and Hong, Yina},
title = {AI + Art and Design Education: Research on Intelligent Algorithm-Driven Educational Content Generation Mechanism},
year = {2025},
isbn = {9798400715068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3744367.3744383},
doi = {10.1145/3744367.3744383},
abstract = {Targeting at building up a framework for producing educational contents, this study provides a thorough investigation of cutting-edge applications of artificial intelligence technologies in art and design education with the assistance of intelligent algorithms. Generative AI, deep learning, natural language processing (NLP), computer vision (CV) and other significant tools are employed in this study to deal with varied issues in related areas such as outdated content updates, insufficient personalized teaching, and excessive teacher workload. In this study, various technologies including domain knowledge graph construction, AI-generated content optimization, personalized learning path recommendation, and human-AI collaborative creation mechanisms are all combined to form a novel approach. Based on controlled experimental analysis, this framework is proven to deliver great improvements in teaching quality (p&lt;0.05), learning efficiency by 32.7\%, and help build up the expression abilities. Besides acting as the theoretical foundation for the combined AI, art, and design education, the research can also contribute to the intelligent transformation of education, holding important value for advancing educational paradigms toward greater precision and personalization.},
booktitle = {Proceedings of the 2025 International Conference on Artificial Intelligence and Educational Systems},
pages = {88–94},
numpages = {7},
keywords = {AI Collaborative Creativity, AI + Art and Design Education, Adaptive Learning, Intelligent Algorithm-Driven Content Generation},
location = {
},
series = {ICAIES '25}
}

@proceedings{10.1145/3616855,
title = {WSDM '24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining},
year = {2024},
isbn = {9798400703713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to the 17th ACM International Conference on Web Search and Data Mining - WSDM 2024. WSDM is one of the premier conferences in the fields of web search and data mining, with a dynamic and growing community from academia and industry. After two years of virtual conferences and in-person conferences in Singapore, the 2024 edition is an in-person conference with virtual elements. We hope you enjoy the conference at the "Centro Internacional de Congresos de Yucatan (CIC)" in Merida from March 4 to March 8, 2024.We are excited to kick off the program with a dynamic mix of Tutorials and Industry Day. Our seven tutorials will cover a broad range of search and data mining topics. Industry Day will provide valuable insights from leaders at major technology companies. The core technical program continues WSDM's tradition of a single-track format, featuring 109 thought-provoking papers from both academic and industry experts. We're honored to have inspiring keynote speakers each day: Nicolas Christin (CMU), Elizabeth Reid (Google), and Saiph Savage (Civic A.I. Lab). Additionally, 17 interactive demonstrations will showcase the latest prototypes and systems. The final day offers a stimulating Doctoral Consortium and six engaging workshops on topics including integrity in social networks, large language model for society, psychology-informed information access system, interactive and scalable information retrieval system and machine learning on graphs. WSDM 2024 proudly presents WSDM day on information retrieval and Web in the region. WSDM Cup Day highlights finalists' presentations addressing challenges in Conversational Multi-Doc QA. This diverse and stimulating program promises to be an enriching experience for all!.},
location = {Merida, Mexico}
}

@inproceedings{10.1145/3589335.3651238,
author = {Jiomekong, Azanzi and Auer, S\"{o}ren and Oelen, Allard},
title = {Linked Open Literature Review using the Neuro-symbolic Open Research Knowledge Graph},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651238},
doi = {10.1145/3589335.3651238},
abstract = {The way scholarly knowledge and in particular literature reviews are communicated today rather resembles static, unstructured, pseudo-digitized articles, which are hardly processable by machines and AI. This demo showcases a novel way to create and publish scholarly literature reviews, also called semantic reviews. The neuro-symbolic approach consists of extracting key insights from scientific papers leveraging neural models and organizing them using a symbolic scholarly knowledge graph. The food information engineering review case study will allow participants to see how this approach is implemented using the Open Research Knowledge Graph (ORKG). The real-time demo will allow participants to play with the ORKG and create their own living, semantic review.},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1015–1018},
numpages = {4},
keywords = {fair principle, linked open data, literature review, neuro-symbolic ai, scholarly knowledge graph},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3706598.3713317,
author = {Dwyer, Andrew C and Coles-Kemp, Lizzie and Heath, Claude P R and Crivellaro, Clara},
title = {Friend or Foe? Navigating and Re-configuring “Snipers' Alley“},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713317},
doi = {10.1145/3706598.3713317},
abstract = {In a ‘digital by default’ society, essential services must be accessed online. This opens users to digital deception not only from criminal fraudsters but from a range of actors in a marketised digital economy. Using grounded empirical research from northern England, we show how supposedly ‘trusted’ actors, such as governments, (re)produce the insecurities and harms that they seek to prevent. Enhanced by a weakening of social institutions amid a drive for efficiency and scale, this has built a constricted, unpredictable digital channel. We conceptualise this as a “snipers’ alley”. Four key snipers articulated by participants’ lived experiences are examined: 1) Governments; 2) Business; 3) Criminal Fraudsters; and 4) Friends and Family to explore how snipers are differentially experienced and transfigure through this constricted digital channel. We discuss strategies to re-configure the alley, and how crafting and adopting opportunity models can enable more equitable forms of security for all.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {210},
numpages = {15},
keywords = {Digital Access, Digital Economy, Security Models, Threat Models, Dark Patterns},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706599.3719878,
author = {Vella, Kellie and Dobson, Madeleine and Brereton, Margot},
title = {"Hello, Mr Tree": Toying with Playful Conversational AI in the Early Years},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719878},
doi = {10.1145/3706599.3719878},
abstract = {The use of generative AI is increasingly integrated into childhood education, primarily with text-to-image generation and older age ranges. This late-breaking work looks at the use of a prototype technology using voiced conversational AI (CAI) to engage young children’s interest in nature, through the role-play of a character: the ‘Talking Tree’. Using research-through-design, we conducted six interactive sessions with children aged 3 to 5 years. These drove the iterative development of the device and provided insight into how CAI might be applied within the context of early learning. We found that the device operated within children’s performative social interactions and within their imagination to prompt recollections of nature and fantastic diversions. We contribute insight into the use of conversational AI for learning in the busy environments of early childhood education centres and the use of CAI-performed fictional characters to build children’s connection with nature.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {8},
numpages = {6},
keywords = {Child-computer interaction, design, Early childhood education, Conversational agent, Voice recognition, Digital play, Play-based learning, LLM, AI},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3589132.3625615,
author = {Bilidas, Dimitris and Mantas, Anastasios and Yfantis, Filippos and Stamoulis, George and Koubarakis, Manolis and Kondylatos, Spyros and Prapas, Ioannis and Papoutsis, Ioannis},
title = {Fire Risk Management using Data Cubes, Machine Learning and OBDA systems},
year = {2023},
isbn = {9798400701689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589132.3625615},
doi = {10.1145/3589132.3625615},
abstract = {We present a fire risk management system which takes input data from various sources (e.g., meteorological data, satellite indicators for vegetation, historical burned areas), produces a harmonized spatio-temporal data cube to compute fire risk and enables semantic querying to assist fire risk management. The distinguishing implementation features of the system is the use of data cubes, machine learning algorithms and, most importantly, geospatial ontology-based data access technologies. The system has been implemented in the European project DeepCube for the geographic area of Greece and can be used operationally to assist authorities to determine fire risk during the summer fire season.},
booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
articleno = {34},
numpages = {4},
keywords = {fire risk, machine learning, ontology based data access, data cubes},
location = {Hamburg, Germany},
series = {SIGSPATIAL '23}
}

@inproceedings{10.1145/3297001.3297032,
author = {Subramanian, Asha and RR, Pavan Kumar and Vikkurthi, Manikanta and Buttigieg, Pier Luigi},
title = {Semantic Harmonisation of Numeric Data from Open Government Data},
year = {2019},
isbn = {9781450362078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297001.3297032},
doi = {10.1145/3297001.3297032},
abstract = {Open tabular data published as part of the open government initiatives typically contain a spatial dimension, a temporal dimension and the actual numeric data capturing information such as health indicators, pollution readings, sanitation status etc. "Semantic Harmonisation" of numeric data entails linking numeric data columns with web-accessible semantic entities from an ontology - a machine readable knowledge representation. These semantic entities are embedded in a knowledge graph, allowing integration of information from disparate sources under common semantic definitions across spatial and temporal dimensions. Multiple research efforts have contributed to recovering semantics of numeric columns in tables, however they are either restricted to a single domain or rely on the existence of numeric data as linked data tuples in known ontologies. We present a novel yet simple approach using a supervised machine learning classifier (Random Forests) and semantic web techniques to generate semantics for numeric columns in tabular data. This approach has been tested with encouraging results for over 100 tabular datasets from data.gov.in (Indian Open Government Data Portal) downloaded from multiple domains such as "Health and Family Welfare", "Agriculture", "Environment" etc. We also present a use case for this work, being implemented in collaboration with the ministries of the Government of Karnataka for knowledge aggregation and dissemination of sustainable development data.},
booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
pages = {238–244},
numpages = {7},
keywords = {Semantic Harmonisation, Ontologies, Government Data},
location = {Kolkata, India},
series = {CODS-COMAD '19}
}

@article{10.1145/3625301,
author = {Cornut, Murielle and Raemy, Julien Antoine and Spiess, Florian},
title = {Annotations as Knowledge Practices in Image Archives: Application of Linked Open Usable Data and Machine Learning},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1556-4673},
url = {https://doi.org/10.1145/3625301},
doi = {10.1145/3625301},
abstract = {We reflect on some of the preliminary findings of the Participatory Knowledge Practices in Analogue and Digital Image Archives (PIA) research project around annotations of photographic archives from the Swiss Society for Folklore Studies (SSFS) as knowledge practices, the underlying technological decisions, and their impact. The aim is not only to seek more information but to find new approaches of understanding the way in which people’s memory relate to the collective, public form of archival memory and ultimately how users figure in and shape the digital archive.We provide a proof-of-concept workflow based on automatically generated annotations comprising 53,481 photos that were subjected to object detection using Faster R-CNN Inception ResNet V2. Of the detected objects, 184,609 have a detection score greater than 0.5, 123,529 have a score greater than 0.75, and 88,442 have a score greater than 0.9. A threshold of 0.75 was set for the dissemination of our annotations, compatible with the W3C Web Annotation Data Model (WADM) and embedded in our IIIF Manifests.In the near future, the workflow will be upgraded to allow for the co-existence of various, and occasionally conflicting, assertions made by both human and machine users. We believe that Linked Open Usable Data (LOUD) standards should be used to improve the sustainability of such an ecosystem and to foster collaboration between actors in cultural heritage.},
journal = {J. Comput. Cult. Herit.},
month = nov,
articleno = {80},
numpages = {19},
keywords = {web annotation data model, participatory knowledge practices in analogue and digital image archives, object detection, memory, machine learning, linked open usable data, linked art, international image interoperability framework, digital materiality, cultural heritage, cultural anthropology, Citizen science}
}

@article{10.1145/3564275,
author = {van der Linden, Sanne and Sevastjanova, Rita and Funk, Mathias and El-Assady, Mennatallah},
title = {MediCoSpace: Visual Decision-Support for Doctor-Patient Consultations using Medical Concept Spaces from EHRs},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {2158-656X},
url = {https://doi.org/10.1145/3564275},
doi = {10.1145/3564275},
abstract = {Healthcare systems are under pressure from an aging population, rising costs, and increasingly complex conditions and treatments. Although data are determined to play a bigger role in how doctors diagnose and prescribe treatments, they struggle due to a lack of time and an abundance of structured and unstructured information. To address this challenge, we introduce MediCoSpace, a visual decision-support tool for more efficient doctor-patient consultations. The tool links patient reports to past and present diagnoses, diseases, drugs, and treatments, both for the current patient and other patients in comparable situations. MediCoSpace uses textual medical data, deep-learning supported text analysis and concept spaces to facilitate a visual discovery process. The tool is evaluated by five medical doctors. The results show that MediCoSpace facilitates a promising, yet complex way to discover unlikely relations and thus suggests a path toward the development of interactive visual tools to provide physicians with more holistic diagnoses and personalized, dynamic treatments for patients.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jan,
articleno = {15},
numpages = {20},
keywords = {electronic health records, interaction design, natural language processing, Visual analytics}
}

@inproceedings{10.1145/3534678.3542634,
author = {Chua, Watson W.K. and Li, Lu and Goh, Alvina},
title = {Classifying Multimodal Data Using Transformers},
year = {2022},
isbn = {9781450393850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3534678.3542634},
doi = {10.1145/3534678.3542634},
abstract = {The increasing prevalence of multimodal data in our society has led to the increased need for machines to make sense of such data holistically. However, data scientists and machine learning engineers aspiring to work on such data face challenges fusing the knowledge from existing tutorials which often deal with each mode separately. Drawing on our experience in classifying multimodal municipal issue feedback in the Singapore government, we conduct a hands-on tutorial to help flatten the learning curve for practitioners who want to apply machine learning to multimodal data.},
booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {4780–4781},
numpages = {2},
keywords = {computer vision, deep learning, multimodal learning, natural language processing, transformers, vision-language representation},
location = {Washington DC, USA},
series = {KDD '22}
}

@inproceedings{10.1145/3340531.3414073,
author = {Bowles, Juliana and Broccia, Giovanna and Nanni, Mirco},
title = {DataMod2020: 9th International Symposium "From Data to Models and Back"},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3414073},
doi = {10.1145/3340531.3414073},
abstract = {DataMod 2020 aims to bring together practitioners and researchers from academia, industry and research institutions interested in the combined application of computational modelling methods with data-driven techniques from the areas of knowledge management, data mining and machine learning. Modelling methodologies of interest include automata, agents, Petri nets, process algebras and rewriting systems. Application domains include social systems, ecology, biology, medicine, smart cities, governance, security, education, software engineering, and any other field that deals with complex systems and large amounts of data. Papers can present research results in any of the themes of interest for the symposium as well as application experiences, tools and promising preliminary ideas. Papers dealing with synergistic approaches that integrate modelling and knowledge management/discovery or that exploit knowledge management/discovery to develop/syntesise system models are especially welcome.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
pages = {3531–3532},
numpages = {2},
keywords = {text mining, processing mining, process calculi, machine learning, formal methods, deep learning, big data analytics},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@article{10.1145/3728365,
author = {Long, Yuan and Rai, Arun},
title = {Decoding Digital Risk From Corporate Disclosure: A Neural Network Approach},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3728365},
doi = {10.1145/3728365},
abstract = {Digital risk—or the likelihood of losses from key digital activities (i.e., information system [IS] sourcing, digital infrastructure, data management, IS applications, IS use, and digital product offerings)—constitutes a key consideration in firm valuation. Firms’ public disclosures (e.g., 10-K reports, earnings conference calls) are a key source of data to learn about digital risks. Although text analytics approaches (e.g., word frequency, topic modeling, and sentiment analysis) have been applied to a firm's public disclosures to assess various types of risk (e.g., political risk, tax risk, cybersecurity), they do not consider the structural linguistic relations embedded in the text that are potentially relevant in measuring risk.We apply a neural network approach to address this gap and extract linguistic relations from a firm's 10-K disclosure (Section “Item 1A”). We develop novel firm-level digital risk measures based on these linguistic relations. Specifically, we measure firm-level digital risk from three perspectives: (1) presence (whether digital risk is mentioned or not), (2) intensity (text coverage of digital risk relative to other issues), and (3) diversity (the types of digital risk mentioned).We validate our digital risk measures by demonstrating their significant correlation with firm risk, proxied by stock market volatility. Our research reveals that investors’ perceptions of digital risk diversity and digital risk intensity differ between IT and non-IT companies. First, across all firms, digital risk intensity is negatively associated with firm risk, indicating that investors do not incorporate intensity of digital risk when assessing firm risk. Second, in non-IT firms, digital risk diversity is positively associated with firm risk, suggesting that managers in these firms may influence investor perceptions through strategic disclosure of digital risk types. Overall, our findings suggest that text-based digital risk measurement is practically feasible, scalable, and economically meaningful.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = may,
articleno = {22},
numpages = {44},
keywords = {Digital risk, Textual analysis, Linguistic structure, Deep learning, Corporate disclosure}
}

@inproceedings{10.1145/3708319.3733657,
author = {Perez-Martinez, Roberto and Casas-Ortiz, Alberto and Santos, Olga C.},
title = {Towards Cultural Preservation of Traditional Motion Knowledge through Automated Annotations with MoRTELaban},
year = {2025},
isbn = {9798400713996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708319.3733657},
doi = {10.1145/3708319.3733657},
abstract = {Movement disciplines like dance or martial arts are carriers of cultural knowledge, identity, and tradition. However, oral traditions and video recordings make the preservation of this knowledge susceptible to being lost. Expert movement notation, in turn, holds the potential for precise capture and knowledge inheritance. However, motion notation approaches are not widespread, the process is often time-consuming, and the movements are hard to visualize without expert knowledge. In this work, we use Labanotation and Laban Movement Analysis (LMA), a notation system and method originally developed for dance, as a symbolic, interpretable framework for motion representation and preservation. Our contribution resides in the expansion of an existing annotation system, the LabanEditor, to handle full-body motion and data from multiple sources, and support the work of experts in annotating the movements. Our development, called MoRTELaban, supports motion-to-notation and inverse mapping from notation to keyframes, enabling exchange between video, motion capture, and Labanotation formats. This allows for the documentation and reconstruction of traditional motion practices using expert-readable scores and 3D skeletons.},
booktitle = {Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {459–463},
numpages = {5},
keywords = {motion modeling, movement modeling, knowledge representation, Labanotation, Laban Movement Analysis (LMA), expert systems},
location = {
},
series = {UMAP Adjunct '25}
}

@inproceedings{10.1145/3543507.3583457,
author = {Zhao, Mingjun and Wang, Mengzhen and Ma, Yinglong and Niu, Di and Wu, Haijiang},
title = {CEIL: A General Classification-Enhanced Iterative Learning Framework for Text Clustering},
year = {2023},
isbn = {9781450394161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543507.3583457},
doi = {10.1145/3543507.3583457},
abstract = {Text clustering, as one of the most fundamental challenges in unsupervised learning, aims at grouping semantically similar text segments without relying on human annotations. With the rapid development of deep learning, deep clustering has achieved significant advantages over traditional clustering methods. Despite the effectiveness, most existing deep text clustering methods rely heavily on representations pre-trained in general domains, which may not be the most suitable solution for clustering in specific target domains. To address this issue, we propose CEIL, a novel Classification-Enhanced Iterative Learning framework for short text clustering, which aims at generally promoting the clustering performance by introducing a classification objective to iteratively improve feature representations. In each iteration, we first adopt a language model to retrieve the initial text representations, from which the clustering results are collected using our proposed Category Disentangled Contrastive Clustering (CDCC) algorithm. After strict data filtering and aggregation processes, samples with clean category labels are retrieved, which serve as supervision information to update the language model with the classification objective via a prompt learning approach. Finally, the updated language model with improved representation ability is used to enhance clustering in the next iteration. Extensive experiments demonstrate that the CEIL framework significantly improves the clustering performance over iterations, and is generally effective on various clustering algorithms. Moreover, by incorporating CEIL on CDCC, we achieve the state-of-the-art clustering performance on a wide range of short text clustering benchmarks outperforming other strong baseline methods.},
booktitle = {Proceedings of the ACM Web Conference 2023},
pages = {1784–1792},
numpages = {9},
keywords = {Classification-enhanced Clustering, Iterative Framework, Text Clustering},
location = {Austin, TX, USA},
series = {WWW '23}
}

@inproceedings{10.1145/3747227.3747237,
author = {Gao, Feifei and Zhang, Lin and Zhang, Bo and Wang, Wenfeng and Liu, Wei and Zhang, Jingyi and Liu, Han and Qiu, Shi and Huang, Kai and Zhang, Mingang},
title = {Research on construction technology and application of knowledge graph in equipment fault Diagnosis domain},
year = {2025},
isbn = {9798400714382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3747227.3747237},
doi = {10.1145/3747227.3747237},
abstract = {In recent years, the construction technology of general knowledge graph (KG) has been developing continuously. The medical industry, manufacturing industry, financial industry and other industries have constructed domain KGs. The research of KG in the equipment field is mainly focused on general equipment knowledge, and the field of equipment fault diagnosis also needs to build its own domain KG. Combined with the definition of the general KG, the definition and construction process of the equipment fault diagnosis domain KG are expounded, the specific technical methods of each link of the construction process are summarized, and the application of the equipment fault diagnosis domain KG is explained, and some positive exploration is carried out for the subsequent construction of the equipment fault diagnosis domain KG.},
booktitle = {Proceedings of the 2025 International Conference on Machine Learning and Neural Networks},
pages = {61–69},
numpages = {9},
keywords = {Data acquisition, Domain knowledge graph, Equipment fault diagnosis, Knowledge extraction, Knowledge processing, Knowledge storage, Ontology building},
location = {
},
series = {MLNN '25}
}

@inproceedings{10.1145/3571884.3604310,
author = {Mannekote, Amogh and Celepkolu, Mehmet and Wiggins, Joseph B. and Boyer, Kristy Elizabeth},
title = {Exploring Usability Issues in Instruction-Based and Schema-Based Authoring of Task-Oriented Dialogue Agents},
year = {2023},
isbn = {9798400700149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571884.3604310},
doi = {10.1145/3571884.3604310},
abstract = {Platforms such as Google DialogFlow and Amazon Lex have enabled easier development of conversational agents. The standard approach to training these agents involve collecting and annotating in-domain data in the form of labelled utterances. However, obtaining in-domain data for training machine learning models remains a bottleneck. Schema-based dialogue, which involves laying out a structured representation of the flow of a “typical” dialogue, and prompt-based methods, which involve writing instructions in natural language to large language models such as GPT-3, are promising ways to tackle this problem. However, usability issues when translating these methods into practice are less explored. Our study takes a first step towards addressing this gap by having 23 students who had finished a graduate-level course on spoken dialogue systems report their experiences as they defined structured schemas and composed instruction-based prompts for two task-oriented dialogue scenarios. Through inductive coding and subsequent thematic analysis of the survey data, we explored users’ authoring experiences with schema and prompt-based methods. The findings provide insights for future data collection and authoring tool design for dialogue systems.},
booktitle = {Proceedings of the 5th International Conference on Conversational User Interfaces},
articleno = {41},
numpages = {6},
keywords = {dialogue systems, schema-based dialogue, user studies;, zero-shot prompting},
location = {Eindhoven, Netherlands},
series = {CUI '23}
}

@inproceedings{10.1145/3641399.3641412,
author = {Balwani, Shivani and Tiwari, Saurabh and Dasgupta, Sourish and Sharma, Akhilesh},
title = {An Approach for Providing Recommendation for Requirements Non-Conformant with Requirement Templates (RTs)},
year = {2024},
isbn = {9798400717673},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641399.3641412},
doi = {10.1145/3641399.3641412},
abstract = {RTs generally possess a fixed syntactic structure and comprise pre-defined slots, and requirements written in the format of RTs must conform with the template structure. Suppose the requirements do not conform to the RT. In that case, manually verifying the conformity of requirements to RTs becomes a tedious task due to the large size of industry requirement documents and introduces the possibility of errors. Furthermore, rewriting requirements to conform to the template structure when they initially do not conform presents a significant challenge. This paper proposes a tool-based approach that automatically verifies whether Functional Requirements (FRs) conform to RTs. It recommends a Template Conformance (TC) requirement by generating a semantically identical requirement that Conforms to the template structure. Our study focused on two well-known RTs, Easy Approach to Requirements Syntax (EARS) and RUPPs, for checking conformance and making recommendations. We utilized Natural Language Processing (NLP) techniques and applied our approach to industrial and publicly available case studies. Our results demonstrate that the proposed tool-based approach facilitates requirement analysis and aids in recommending requirements based on their conformity with RTs. Our results show an accuracy of 83.9\% for providing recommendations to non-conformant requirements with RTs.},
booktitle = {Proceedings of the 17th Innovations in Software Engineering Conference},
articleno = {9},
numpages = {11},
keywords = {Analysis, Natural Language Processing (NLP), Quality, Recommendation, Requirement Templates (RTs)},
location = {Bangalore, India},
series = {ISEC '24}
}

@inproceedings{10.1145/3604915.3608759,
author = {Anelli, Vito Walter and Basile, Pierpaolo and De Melo, Gerard and Donini, Francesco M and Ferrara, Antonio and Musto, Cataldo and Narducci, Fedelucio and Ragone, Azzurra and Zanker, Markus},
title = {Fifth Knowledge-aware and Conversational Recommender Systems Workshop (KaRS)},
year = {2023},
isbn = {9798400702419},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604915.3608759},
doi = {10.1145/3604915.3608759},
abstract = {Recommender systems have become ubiquitous in daily life, but their limitations in interacting with human users have become evident. Deep learning approaches have led to the development of data-driven algorithms that identify connections between users and items, but they often miss a critical actor in the loop - the end-user. Knowledge-based approaches are gaining attention due to the availability of knowledge-graphs, such as DBpedia and Wikidata, which provide semantics-aware information on different knowledge domains. These approaches are being used for recommendation and challenges such as knowledge graph embeddings, hybrid recommendation, and interpretable recommendation. Moreover, the emergence of neural-symbolic systems, which combine data-driven and symbolic methods, can significantly improve recommendation systems. A growing number of research papers on such topics demonstrate the growing interest and research potential of these systems. Furthermore, content features become crucial when interaction requires it. The development of conversational recommender systems presents new challenges, as they require multi-turn dialogues between users and systems, blurring the line between recommendation and retrieval. Evaluation of these systems goes beyond simple accuracy metrics and is hampered by the limited availability of datasets. While research and development into conversational recommender systems has been less prominent in the past, recent literature shows growing interest and potential for these systems.},
booktitle = {Proceedings of the 17th ACM Conference on Recommender Systems},
pages = {1259–1262},
numpages = {4},
keywords = {Conversational Agents, Knowledge Graphs, Knowledge Representation, Natural Language Processing, Neural-Symbolic Reasoning, Recommender systems, Semantic Web},
location = {Singapore, Singapore},
series = {RecSys '23}
}

@article{10.1162/coli_a_00354,
author = {Ustalov, Dmitry and Panchenko, Alexander and Biemann, Chris and Ponzetto, Simone Paolo},
title = {Watset: Local-Global Graph Clustering with Applications in Sense and Frame Induction},
year = {2019},
issue_date = {September 2019},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {45},
number = {3},
issn = {0891-2017},
url = {https://doi.org/10.1162/coli_a_00354},
doi = {10.1162/coli_a_00354},
abstract = {We present a detailed theoretical and computational analysis of the Watset meta-algorithm for fuzzy graph clustering, which has been found to be widely applicable in a variety of domains. This algorithm creates an intermediate representation of the input graph, which reflects the “ambiguity” of its nodes. Then, it uses hard clustering to discover clusters in this “disambiguated” intermediate graph. After outlining the approach and analyzing its computational complexity, we demonstrate that Watset shows competitive results in three applications: unsupervised synset induction from a synonymy graph, unsupervised semantic frame induction from dependency triples, and unsupervised semantic class induction from a distributional thesaurus. Our algorithm is generic and can also be applied to other networks of linguistic data.},
journal = {Comput. Linguist.},
month = sep,
pages = {423–479},
numpages = {57}
}

@inproceedings{10.1145/3640543.3645208,
author = {Bendeck, Alexander and Bromley, Dennis and Setlur, Vidya},
title = {SlopeSeeker: A Search Tool for Exploring a Dataset of Quantifiable Trends},
year = {2024},
isbn = {9798400705083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3640543.3645208},
doi = {10.1145/3640543.3645208},
abstract = {Natural language and search interfaces intuitively facilitate data exploration and provide visualization responses to diverse analytical queries based on the underlying datasets. However, these interfaces often fail to interpret more complex analytical intents, such as discerning subtleties and quantifiable differences between terms like “bump’’ and “spike’’ in the context of COVID cases, for example. We address this gap by extending the capabilities of a data exploration search interface for interpreting semantic concepts in time series trends. We first create a comprehensive dataset of semantic concepts by mapping quantifiable univariate data trends such as slope and angle to crowdsourced, semantically meaningful trend labels. The dataset contains quantifiable properties that capture the slope-scalar effect of semantic modifiers like “sharply” and “gradually,” as well as multi-line trends (e.g., “peak,” “valley”). We demonstrate the utility of this dataset in SlopeSeeker, a tool that supports natural language querying of quantifiable trends, such as “show me stocks that tanked in 2010.” The tool incorporates novel scoring and ranking techniques based on semantic relevance and visual prominence to present relevant trend chart responses containing these semantic trend concepts. In addition, SlopeSeeker provides a faceted search interface for users to navigate a semantic hierarchy of concepts from general trends (e.g., “increase’’) to more specific ones (e.g., “sharp increase’’). A preliminary user evaluation of the tool demonstrates that the search interface supports greater expressivity of queries containing concepts that describe data trends. We identify potential future directions for leveraging our publicly available quantitative semantics dataset in other data domains and for novel visual analytics interfaces.},
booktitle = {Proceedings of the 29th International Conference on Intelligent User Interfaces},
pages = {817–836},
numpages = {20},
keywords = {Semantics, quantifiable metadata, search, trends, visual analysis.},
location = {Greenville, SC, USA},
series = {IUI '24}
}

@inproceedings{10.1145/3658644.3670318,
author = {Al Rahat, Tamjid and Feng, Yu and Tian, Yuan},
title = {AuthSaber: Automated Safety Verification of OpenID Connect Programs},
year = {2024},
isbn = {9798400706363},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3658644.3670318},
doi = {10.1145/3658644.3670318},
abstract = {Single Sign-On (SSO)-based authentication protocols, like OpenID Connect (OIDC), play a crucial role in enhancing security and privacy in today's interconnected digital world, gaining widespread adoption among the majority of prominent authentication service providers. These protocols establish a structured framework for verifying and authenticating the identities of individuals, organizations, and devices, while avoiding the necessity of sharing sensitive credentials (e.g., passwords) with external entities. However, the security guarantees of these protocols rely on their proper implementation, and real-world implementations can, and indeed often do, contain logical programming errors leading to severe attacks, including authentication bypass and user account takeover. In response to this challenge, we present AuthSaber, an automated verifier designed to assess the real-world OIDC protocol implementations against their standard safety specifications in a scalable manner. AuthSaber addresses the challenges of expressiveness for OIDC properties, modeling multi-party interactions, and automation by first designing a novel specification language based on linear temporal logic, leveraging an automaton-based approach to constrain the space of possible interactions between OIDC entities, and incorporating several domain-specific transformations to obtain programs and properties that can be directly reasoned about by software model checkers. We evaluate AuthSaber on the 15 most popular and widely used OIDC libraries and discover 16 previously unknown vulnerabilities, all of which are responsively disclosed to the developers. Five categories of these vulnerabilities also led to new CVEs.},
booktitle = {Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
pages = {2949–2962},
numpages = {14},
keywords = {authentication, authorization, automated analysis, openid connect security, safety verification, single sign-on},
location = {Salt Lake City, UT, USA},
series = {CCS '24}
}

@inproceedings{10.1145/3390557.3394128,
author = {Wang, Jiawei and Cui, Guorong and Zhu, Xiaoke and Liu, Huijian and Liu, Junsong and Jia, Xuebin},
title = {GSR: A Resource Model and Semantics-based API Recommendation Algorithm},
year = {2020},
isbn = {9781450376587},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3390557.3394128},
doi = {10.1145/3390557.3394128},
abstract = {With the rapid development of Web services, more and more Web services are published on the Internet. A Mashup application that aggregates multiple Web APIs is also becoming more popular. But it also brings a problem that is how to find a suitable API among a wide variety of APIs has become a challenge. To this end, this paper proposes a web service recommendation algorithm that combines graph databases and semantics. In this algorithm, we propose to use graph database to build a two-layer structure resource model. First, we use LDA for topic classification and classify Mashup and API of the same classification into the same category respectively. This helps reduce the number of searches for Mashup and API. When a user enters a requirement document, Word2vec and WMD algorithms are used to find similar Web API description text. Finally, we use similarity and API history invokes to propose a ranking algorithm to generate a recommendation list. Through real-world data, this experiment has a better-recommended performance.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Innovation in Artificial Intelligence},
pages = {184–188},
numpages = {5},
keywords = {Word Mover's Distance, Resource Model, LDA, API recommendation},
location = {Xiamen, China},
series = {ICIAI '20}
}

@inproceedings{10.1145/3497775.3503685,
author = {Conrad, Esther and Titolo, Laura and Giannakopoulou, Dimitra and Pressburger, Thomas and Dutle, Aaron},
title = {A compositional proof framework for FRETish requirements},
year = {2022},
isbn = {9781450391825},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3497775.3503685},
doi = {10.1145/3497775.3503685},
abstract = {Structured natural languages provide a trade space between ambiguous natural languages that make up most written requirements, and mathematical formal specifications such as Linear Temporal Logic. FRETish is a structured natural language for the elicitation of system requirements developed at NASA. The related open-source tool Fret provides support for translating FRETish requirements into temporal logic formulas that can be input to several verification and analysis tools. In the context of safety-critical systems, it is crucial to ensure that a generated formula captures the semantics of the corresponding FRETish requirement precisely. This paper presents a rigorous formalization of the FRETish language including a new denotational semantics and a proof of semantic equivalence between FRETish specifications and their temporal logic counterparts computed by Fret. The complete formalization and the proof have been developed in the Prototype Verification System (PVS) theorem prover.},
booktitle = {Proceedings of the 11th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {68–81},
numpages = {14},
keywords = {Structured Natural Language, Requirements, PVS, Metric Temporal Logic, Formal Proofs},
location = {Philadelphia, PA, USA},
series = {CPP 2022}
}

@article{10.1145/3485847,
author = {Costa, L\'{a}zaro and Freitas, Nuno and da Silva, Jo\~{a}o Rocha},
title = {An Evaluation of Graph Databases and Object-Graph Mappers in CIDOC CRM-Compliant Digital Archives},
year = {2022},
issue_date = {September 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {1556-4673},
url = {https://doi.org/10.1145/3485847},
doi = {10.1145/3485847},
abstract = {The Portuguese General Directorate for Book, Archives and Libraries (DGLAB) has selected CIDOC CRM as the basis for its next-generation digital archive management software. Given the ontological foundations of the Conceptual Reference Model (CRM), a graph database or a triplestore was seen as the best candidate to represent a CRM-based data model for the new software. We thus decided to compare several of these databases, based on their maturity, features, performance in standard tasks and, most importantly, the Object-Graph Mappers (OGM) available to interact with each database in an object-oriented way. Our conclusions are drawn not only from a systematic review of related works but from an experimental scenario. For our experiment, we designed a simple CRM-compliant graph designed to test the ability of each OGM/database combination to tackle the so-called “diamond-problem” in Object-Oriented Programming (OOP) to ensure that property instances follow domain and range constraints.&nbsp;&nbsp;Our results show that (1) ontological consistency enforcement in graph databases and triplestores is much harder to achieve than in a relational database, making them more suited to an analytical rather than a transactional role; (2) OGMs are still rather immature solutions; and (3) neomodel, an OGM for the Neo4j graph database, is the most mature solution in the study as it satisfies all requirements, although it is also the least performing.},
journal = {J. Comput. Cult. Herit.},
month = sep,
articleno = {44},
numpages = {18},
keywords = {comparison, CIDOC CRM, digital archives, graph databases, Object-graph mapping}
}

@inproceedings{10.1109/ASE56229.2023.00150,
author = {Li, Linyu and Xu, Sihan and Liu, Yang and Gao, Ya and Cai, Xiangrui and Wu, Jiarun and Song, Wenli and Liu, Zheli},
title = {LiSum: Open Source Software License Summarization with Multi-Task Learning},
year = {2024},
isbn = {9798350329964},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE56229.2023.00150},
doi = {10.1109/ASE56229.2023.00150},
abstract = {Open source software (OSS) licenses regulate the conditions under which users can reuse, modify, and distribute the software legally. However, there exist various OSS licenses in the community, written in a formal language, which are typically long and complicated to understand. In this paper, we conducted a 661-participants online survey to investigate the perspectives and practices of developers towards OSS licenses. The user study revealed an indeed need for an automated tool to facilitate license understanding. Motivated by the user study and the fast growth of licenses in the community, we propose the first study towards automated license summarization. Specifically, we released the first high quality text summarization dataset and designed two tasks, i.e., license text summarization (LTS), aiming at generating a relatively short summary for an arbitrary license, and license term classification (LTC), focusing on the attitude inference towards a predefined set of key license terms (e.g., Distribute). Aiming at the two tasks, we present LiSum, a multi-task learning method to help developers overcome the obstacles of understanding OSS licenses. Comprehensive experiments demonstrated that the proposed jointly training objective boosted the performance on both tasks, surpassing state-of-the-art baselines with gains of at least 5 points w.r.t. F1 scores of four summarization metrics and achieving 95.13\% micro average F1 score for classification simultaneously. We released all the datasets, the replication package, and the questionnaires for the community.},
booktitle = {Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering},
pages = {787–799},
numpages = {13},
keywords = {open source software licenses, multi-task learning, license comprehension},
location = {Echternach, Luxembourg},
series = {ASE '23}
}

@inbook{10.1145/3729706.3729796,
author = {Wang, Yu and Wang, Liguang and Ma, Jun},
title = {Research on the Construction and Application of the Knowledge Graph of Qin Dynasty Historical Celebrities' Deeds by Spatio-Temporal Integration},
year = {2025},
isbn = {9798400712715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3729706.3729796},
abstract = {[Objective] Construct a spatial and temporal integration of the Qin Dynasty historical celebrities knowledge map, in order to lay the foundation for information retrieval and visualization when customers learn the Qin Dynasty historical knowledge. [Methods] We use the “seven-step method” and the Prot\'{e}g\'{e} tool to construct the ontology model of the Qin Dynasty historical celebrities with spatial and temporal correlation from the top down, and then construct the spatial and temporal fusion knowledge map instance through knowledge extraction, attribute fusion and knowledge storage, and finally implant the model into the network, so that we can provide the users with an accurate, intuitive, high-capacity, fast, and in-depth spatial and temporal correlation historical knowledge map. Finally, the model is implanted into the network to provide users with accurate, intuitive, high-capacity, fast, and deeply spatio-temporally related popularization services. [Results] The constructed knowledge graph is stored and visualized by Neo4j, supports Cypher language for knowledge query, and realizes accurate promotion in three ways: webpage, SVG vector image of knowledge graph, and short video. [Limitations] As the historical knowledge is more cumbersome, data acquisition takes a long time while ensuring correctness. [Conclusion] Compared with traditional knowledge mapping, knowledge mapping based on spatio-temporal fusion integrates the spatio-temporal factors associated with events into the model, and the events shown in the mapping are more abundant, so that the users can understand the events and their spatio-temporal backgrounds clearly, accurately, intuitively, and quickly through various network terminals, and it is more efficient and more acceptable than the traditional popularization methods.},
booktitle = {Proceedings of the 2025 4th International Conference on Cyber Security, Artificial Intelligence and the Digital Economy},
pages = {565–571},
numpages = {7}
}

@article{10.1109/TASLP.2021.3138670,
author = {Li, Qian and Peng, Hao and Li, Jianxin and Wu, Jia and Ning, Yuanxing and Wang, Lihong and Yu, Philip S. and Wang, Zheng},
title = {Reinforcement Learning-Based Dialogue Guided Event Extraction to Exploit Argument Relations},
year = {2022},
issue_date = {2022},
publisher = {IEEE Press},
volume = {30},
issn = {2329-9290},
url = {https://doi.org/10.1109/TASLP.2021.3138670},
doi = {10.1109/TASLP.2021.3138670},
abstract = {Event extraction is a ftask for natural language processing. Finding the roles of event arguments like event participants is essential for event extraction. However, doing so for real-life event descriptions is challenging because an argument’s role often varies in different contexts. While the relationship and interactions between multiple arguments are useful for settling the argument roles, such information is largely ignored by existing approaches. This paper presents a better approach for event extraction by explicitly utilizing the relationships of event arguments. We achieve this through a carefully designed task-oriented dialogue system. To model the argument relation, we employ reinforcement learning and incremental learning to extract multiple arguments via a multi-turned, iterative process. Our approach leverages knowledge of the already extracted arguments of the same sentence to determine the role of arguments that would be difficult to decide individually. It then uses the newly obtained information to improve the decisions of previously extracted arguments. This two-way feedback process allows us to exploit the argument relations to effectively settle argument roles, leading to better sentence understanding and event extraction. Experimental results show that our approach consistently outperforms seven state-of-the-art event extraction methods for the classification of events and argument role and argument identification.},
journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
month = dec,
pages = {520–533},
numpages = {14}
}

@inproceedings{10.1109/JCDL57899.2023.00065,
author = {Engel, Felix and Krdzavac, Nenad and Tuncay, Erhun Giray and Klinger, Axel and Hughes, John},
title = {Semantification of Space Data - A Feasibility Study},
year = {2024},
isbn = {9798350399318},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/JCDL57899.2023.00065},
doi = {10.1109/JCDL57899.2023.00065},
abstract = {This paper presents a new approach to the semantic representation of NASA planetary mission data. The Open Archival Information Systems (OAIS) Information Model (IM) is used as a model to represent this data in a Knowledge Graph (KG). To prepare the data for this requirement, a machine learning approach is used to extract information (entities) and connect to Wikidata. A demo shows the advantages of using federated SPARQL queries over the created KG and Wikidata.},
booktitle = {Proceedings of the 2023 ACM/IEEE Joint Conference on Digital Libraries},
pages = {295–296},
numpages = {2},
keywords = {knowledge graph, natural language processing, extracting semantics},
location = {Santa Fe, New Mexico, USA},
series = {JCDL '23}
}

@article{10.1145/3440755,
author = {Chandrasekaran, Dhivya and Mago, Vijay},
title = {Evolution of Semantic Similarity—A Survey},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3440755},
doi = {10.1145/3440755},
abstract = {Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. To address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods beginning from traditional NLP techniques such as kernel-based methods to the most recent research work on transformer-based models, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network–based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {41},
numpages = {37},
keywords = {word embeddings, supervised and unsupervised methods, linguistics, knowledge-based methods, corpus-based methods, Semantic similarity}
}

@inproceedings{10.1145/3593013.3594011,
author = {Kang, Edward B.},
title = {On the Praxes and Politics of AI Speech Emotion Recognition},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594011},
doi = {10.1145/3593013.3594011},
abstract = {There is no scientific consensus on what is meant by “emotion” – researchers have examined various phenomena spanning brain modes, feelings, sensations, and cognitive structures, among others, in their study of emotional experiences. For the purposes of developing an AI speech emotion recognition (SER) system, however, emotion must be defined, bounded, and instantiated as ground truth in the training data. This means practical choices must be made in which particular emotional ontologies are prioritized over others in the construction of SER datasets. In this paper, I explore these tensions around fairness, accountability, and transparency by analyzing open-source datasets used for SER applications along with their accompanying methodology papers. Specifically, I critique the centrality of discrete emotion theory in SER applications as a contestable emotional framework that is invoked primarily for its practical utility and alignment – as opposed to scientific rigor – with machine learning epistemologies. In so doing, I also shed light on the role of the dataset creators as emotional designers in their attempt to produce, elicit, record, and index emotional expressions for the purposes of crafting SER training datasets. Ultimately, by further querying SER through the aperture of Critical Disability Studies, I use this empirical work to examine the sociopolitical stakes of SER as a normative and regulatory technology that siphons emotion into a broader agenda of capitalistic productivity in the context of call center optimization.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {455–466},
numpages = {12},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@inproceedings{10.1145/3375462.3375515,
author = {Peri, Sai Santosh Sasank and Chen, Bodong and Dougall, Angela Liegey and Siemens, George},
title = {Towards understanding the lifespan and spread of ideas: epidemiological modeling of participation on Twitter},
year = {2020},
isbn = {9781450377126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375462.3375515},
doi = {10.1145/3375462.3375515},
abstract = {How ideas develop and evolve is a topic of interest for educators. By understanding this process, designers and educators are better able to support and guide collaborative learning activities. This paper presents an application of our Lifespan of an Idea framework to measure engagement patterns among individuals in communal socio-technical spaces like Twitter. We correlated engagement with social participation, enabling the process of idea expression, spread, and evolution. Social participation leads to transmission of ideas from one individual to another and can be gauged in the same way as evaluating diseases. The temporal dynamics of the social participation can be modeled through the lens of epidemiological modeling. To test the plausibility of this framework, we investigated social participation on Twitter using the tweet posting patterns of individuals in three academic conferences and one long term chat space. We used a basic SIR epidemiological model, where the rate parameters were estimated through Euler's solutions to SIR model and non-linear least squares optimization technique. We discuss the differences in the social participation among individuals in these spaces based on their transition behavior into different categories of the SIR model. We also made inferences on how the total lifetime of these different twitter spaces affects the engagement among individuals. We conclude by discussing implications of this study and planned future research of refining the Lifespan of an Idea Framework.},
booktitle = {Proceedings of the Tenth International Conference on Learning Analytics \&amp; Knowledge},
pages = {197–202},
numpages = {6},
keywords = {networked learning, knowledge creation, ideas, epidemiology, engagement patterns, connectivism},
location = {Frankfurt, Germany},
series = {LAK '20}
}

@inproceedings{10.1145/3543873.3587585,
author = {Timmer, Roelien C. and Mark, Megan and Khoo, Fech Scen and Ribeiro Martins, Marcella Scoczynski and Berea, Anamaria and Renard, Gregory and Bugbee, Kaylin},
title = {NASA Science Mission Directorate Knowledge Graph Discovery},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587585},
doi = {10.1145/3543873.3587585},
abstract = {The size of the National Aeronautics and Space Administration (NASA) Science Mission Directorate (SMD) data catalog is growing exponentially, allowing researchers to make discoveries. However, making discoveries is challenging and time-consuming due to the size of the data catalogs, and as many concepts and data are indirectly connected. This paper proposes a pipeline to generate knowledge graphs (KGs) representing different NASA SMD domains. These KGs can be used as the basis for dataset search engines, saving researchers time and supporting them in finding new connections. We collected textual data and used several modern natural language processing (NLP) methods to create the nodes and the edges of the KGs. We explore the cross-domain connections, discuss our challenges, and provide future directions to inspire researchers working on similar challenges.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {795–799},
numpages = {5},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@article{10.1145/3399630,
author = {Tao, Jie and Zhou, Lina},
title = {A Weakly Supervised WordNet-Guided Deep Learning Approach to Extracting Aspect Terms from Online Reviews},
year = {2020},
issue_date = {September 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {2158-656X},
url = {https://doi.org/10.1145/3399630},
doi = {10.1145/3399630},
abstract = {The unstructured nature of online reviews makes it inefficient and inconvenient for prospective consumers to research and use in support of purchase decision making. The aspects of products provide a fine-grained meaningful perspective for understanding and organizing review texts. Traditional aspect term extraction approaches rely on discrete language models that treat words in isolation. Despite that continuous-space language models have demonstrated promise in addressing a wide range of problems, their application in aspect term extraction faces significant challenges. For instance, existing continuous-space language models typically require large collections of labeled data, which remain difficult to obtain in many domains. More importantly, previous methods are largely data driven but overlook the role of human knowledge in guiding model development. To address these limitations, this study designs and develops weakly supervised WordNet-guided deep learning to aspect term extraction. The approach draws on deep-level semantic information from WordNet to guide not only the selection representative seed terms but also the pruning of aspect candidate terms. The weak supervision is provided by a very small set of labeled data. We conduct a comprehensive evaluation of the proposed method using both direct and indirect methods. The evaluation results with Yelp restaurant reviews demonstrate that our proposed method consistently outperforms all baseline methods including discrete models and the state-of-the-art continuous-space language models for aspect term extraction across both direct and indirect evaluations. The research findings have broad research, technical, and practical implications for various stakeholders of online reviews.},
journal = {ACM Trans. Manage. Inf. Syst.},
month = jul,
articleno = {13},
numpages = {22},
keywords = {text analytics, semantic knowledge, deep learning, continuous-space language model, Aspect term extraction}
}

@inproceedings{10.1145/3652620.3688212,
author = {K\"{u}hne, Thomas and Maier, Pierre},
title = {FMMLx and DLM -- A Contribution to the MULTI Collaborative Comparison Challenge},
year = {2024},
isbn = {9798400706226},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3652620.3688212},
doi = {10.1145/3652620.3688212},
abstract = {This paper is a response to the MULTI 2022 Collaborative Comparison Challenge [23]. We compare FMMLx- and DLM-based solutions. We first present each approach and solution separately, and then discuss trade-offs of both the solutions and the approaches.},
booktitle = {Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems},
pages = {800–809},
numpages = {10},
keywords = {MLM, modeling challenge, FMMLx, DLM},
location = {Linz, Austria},
series = {MODELS Companion '24}
}

@inproceedings{10.1145/3243907.3243908,
author = {Viola, Fabio and Stolfi, Ariane and Milo, Alessia and Ceriani, Miguel and Barthet, Mathieu and Fazekas, Gy\"{o}rgy},
title = {Playsound.space: enhancing a live music performance tool with semantic recommendations},
year = {2018},
isbn = {9781450364959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243907.3243908},
doi = {10.1145/3243907.3243908},
abstract = {Playsound is a simple and intuitive web-based tool for music composition based on sounds from Freesound, an online repository of diverse audio content with Creative Commons licenses. In this paper, we present an approach based on Semantic Web technologies to provide recommendations to Playsound users. A Semantic Web of Things architecture is outlined, showing loosely coupled, independent software agents interoperating by means of a semantic publish/subscribe platform and a set of ontologies to describe agents, audio contents, input/output of audio analytics tools and recommendations. Preliminary tests confirm that the designed architecture adapts well to environments where services can be discovered and seamlessly orchestrated on the fly, resulting in a dynamic workflow.},
booktitle = {Proceedings of the 1st International Workshop on Semantic Applications for Audio and Music},
pages = {46–53},
numpages = {8},
keywords = {Web of Things, Semantic Web, Recommendations, Ontologies},
location = {Monterey, CA, USA},
series = {SAAM '18}
}

@inproceedings{10.1145/3544538.3544668,
author = {Kourtiche, Ali and Felici-Castell, S. and Perez Solano, J. J. and Segura-Garcia, J. and Soriano-Asensi, A. and Navarro-Camba, E. and Pinto, J.},
title = {Internet of Things under a semantic perspective with user profiles},
year = {2022},
isbn = {9781450397384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544538.3544668},
doi = {10.1145/3544538.3544668},
abstract = {Internet of Things (IoT) is a network made up of different types of devices integrated into the Internet. For the use of IoT in cities to assist humans beings in their daily activities, it is necessary to extract and process as much information as possible and therefore, semantic web technologies are a key point. This new environment is known as the Semantic Web of Things (SWoT). The application of semantic techniques to IoT can improve interoperability, effective access to data, discovery of integration resources, reasoning and processing of knowledge extraction from data, opening up opportunities for new applications. We show an overview of the most relevant semantic technologies, focusing on SWoT and well-accepted ontologies for IoT and we stress that considering users’ preferences, interests and needs are of vital importance, as a complement to the semantic information extracted. This new scenario enables the development of advanced applications and services to meet user requirements and needs, in particular for smart city applications.},
booktitle = {Proceedings of the 11th Euro American Conference on Telematics and Information Systems},
articleno = {19},
numpages = {4},
keywords = {web of things, user profiles, smart cities, semantic web, ontology, e-public, citizens, IoT},
location = {Aveiro, Portugal},
series = {EATIS '22}
}

@article{10.1145/3522586,
author = {Tama\v{s}auskaitundefined, Gytundefined and Groth, Paul},
title = {Defining a Knowledge Graph Development Process Through a Systematic Review},
year = {2023},
issue_date = {January 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3522586},
doi = {10.1145/3522586},
abstract = {Knowledge graphs are widely used in industry and studied within the academic community. However, the models applied in the development of knowledge graphs vary. Analysing and providing a synthesis of the commonly used approaches to knowledge graph development would provide researchers and practitioners a better understanding of the overall process and methods involved. Hence, this article aims at defining the overall process of knowledge graph development and its key constituent steps. For this purpose, a systematic review and a conceptual analysis of the literature was conducted. The resulting process was compared to case studies to evaluate its applicability. The proposed process suggests a unified approach and provides guidance for both researchers and practitioners when constructing and managing knowledge graphs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = feb,
articleno = {27},
numpages = {40},
keywords = {information integration, development process semantic network, knowledge graph construction, Knowledge graphs}
}

@inproceedings{10.1145/3589335.3651454,
author = {Ragab, Mohamed and Savateev, Yury and Oliver, Helen and Tiropanis, Thanassis and Poulovassilis, Alexandra and Chapman, Adriane and Roussos, George},
title = {Unlocking the Potential of Health Data with Decentralised Search in Personal Health Datastores},
year = {2024},
isbn = {9798400701726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589335.3651454},
doi = {10.1145/3589335.3651454},
abstract = {In the digital age, where health data and digital lives converge, data privacy and control are crucial. The advent of AI and Large Language Models (LLMs) brings advanced data analysis and healthcare predictions, but also privacy concerns. The ESPRESSO project 1 asserts that for AI to be trustworthy and effective in healthcare, it must prioritize user control over corporate interests. The shift towards decentralized personal online datastores (pods) and Solid 2 principles represents a new era of private, controllable Web interactions, balancing AI data protection and machine intelligence. This balance is particularly important for applications involving health data. However, decentralization poses challenges, particularly in secure, efficient data search and data retrieval, that need to be addressed first. We argue that a decentralized search system that provides a large-scale search across Solid pods, while considering data owners' control of their data and users' different access rights, is crucial for this new paradigm. In this paper, we describe how our current decentralized search system's prototype (ESPRESSO) helps to query structured and unstructured personal health data in Solid servers. The paper also describes a search scenario that shows how ESPRESSO can search health data combined with fitness personal data stored in different personal datastores},
booktitle = {Companion Proceedings of the ACM Web Conference 2024},
pages = {1154–1157},
numpages = {4},
keywords = {decentralized web search, health and well-being data, linked data, personal online datastores, solid framework},
location = {Singapore, Singapore},
series = {WWW '24}
}

@inproceedings{10.1145/3644523.3644601,
author = {Wang, Kaijie and Wang, Tiejun and Lu, Ziling},
title = {Construction of Gesar Epic Event Graph Based on Event Extraction},
year = {2024},
isbn = {9798400709517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644523.3644601},
doi = {10.1145/3644523.3644601},
abstract = {The Biography of King Gesar is the largest heroic epic in world history and also one of the world's intangible cultural heritage sites. The Tibetan culture it carries is an important component of Chinese civilization. In order to better showcase and protect this intangible cultural heritage, and provide data support for research on digital retrieval, intelligent Q&amp;A, and reading comprehension of Gesar cultural resources, the construction of Gesar event graph was studied. Firstly, the BTCNN event federation model is used to extract event trigger words and event elements from semi structured and unstructured event texts, assisting in manually customized event relationships, and completing the preliminary construction of the Gesar event graph. Based on the constructed event graph, a visualization system for the graph was built, and the graph retrieval function was designed and implemented, supporting entity query, relationship query, and entity relationship hybrid query.},
booktitle = {Proceedings of the 2023 4th International Conference on Computer Science and Management Technology},
pages = {431–436},
numpages = {6},
location = {Xi'an, China},
series = {ICCSMT '23}
}

@inproceedings{10.1145/3243907.3243914,
author = {Nurmikko-Fuller, Terhi and Bangert, Daniel and Hao, Yun and Downie, J. Stephen},
title = {Swinging Triples: Bridging Jazz Performance Datasets using Linked Data},
year = {2018},
isbn = {9781450364959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243907.3243914},
doi = {10.1145/3243907.3243914},
abstract = {The jazz performance metadata prototype JazzCats:Jazz Collection of Aggregated Triples uses Linked Data to bridge four discrete jazz music datasets: Linked Jazz, with prosopographical and interpersonal information about musicians; the Weimar Jazz Database (WJazzD), containing musicological metadata; a discography of the jazz standard Body&amp;Soul; and J-DISC, a fourth independent but complementary and extensive discographic project. Through the use of custom-built ontological structures the data, originally stored in various different information structures, has been converted to RDF and merged together in a single triplestore. The result is a new digital resource that can be used to support and enrich scholarship and research in musicology and performance studies.},
booktitle = {Proceedings of the 1st International Workshop on Semantic Applications for Audio and Music},
pages = {42–45},
numpages = {4},
keywords = {semantic web, performance, ontologies, metadata, jazz, digital musicology, SPARQL, Linked Data},
location = {Monterey, CA, USA},
series = {SAAM '18}
}

@inproceedings{10.1145/3371140.3371147,
author = {Yousaf, Madiha and Wolter, Diedrich},
title = {How to identify appropriate key-value pairs for querying OSM},
year = {2019},
isbn = {9781450372602},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371140.3371147},
doi = {10.1145/3371140.3371147},
abstract = {This paper presents a study on how natural language words that designate types of spatial entities (metropolis, city, creek, etc.) can automatically be translated to the entity classification used in OpenStreetMap (OSM) that assigns key-value tags to entities. The problem of identifying key-value pairs for querying OSM occurs in geographic information retrieval based on natural language text and is difficult for three reasons: Conceptualisation of entities in natural language text and in OSM often differs. Even classification of a single entity type is subject to variations throughout the OSM database. Language is rich and offers many words to communicate nuances of a single entity type. The contribution of this paper is to analyse the contribution of semantic word similarity using Word-Net to identify a mapping from natural language to OSM tags. We present a strategy to identify key-value pairs for natural language words using WordNet and analyse its effectiveness.},
booktitle = {Proceedings of the 13th Workshop on Geographic Information Retrieval},
articleno = {7},
numpages = {6},
keywords = {semantics of spatial language, geo-referencing, OpenStreetMap (OSM)},
location = {Lyon, France},
series = {GIR '19}
}

@proceedings{10.1145/3689492,
title = {Onward! '24: Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
year = {2024},
isbn = {9798400712159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2024), the premier multidisciplinary conference focused on everything to do with programming and software, including processes, methods, languages, communities and applications. Onward! is more radical, more visionary and more open than other conferences to ideas that are well-argued but not yet proven. We welcome different ways of thinking about, approaching, and reporting on programming language and software engineering research.},
location = {Pasadena, CA, USA}
}

@inproceedings{10.1145/3276954.3276961,
author = {Gavran, Ivan and Mailahn, Ortwin and M\"{u}ller, Rainer and Peifer, Richard and Zufferey, Damien},
title = {Tᴏᴏʟ: accessible automated reasoning for human robot collaboration},
year = {2018},
isbn = {9781450360319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3276954.3276961},
doi = {10.1145/3276954.3276961},
abstract = {We present an expressive, concise, and extendable domain specific language for planning of assembly systems, such as industrial human robot cooperation. Increased flexibility requirements in manufacturing processes call for more automation at the description and planning stages of manufacturing. Procedural models are good candidates to meet this demand as programs offer a high degree of flexibility and are easily composed. Furthermore, we aim to make our programs close to declarative specification and integrate automatic reasoning tools to help the users. The constraints come both from specific programs and preexisting knowledge base from the target domain. The case of human robot collaboration is interesting as there is a number of constraints and regulations around this domain. Unfortunately, automated reasoners are often too unpredictable and cannot be used directly by non-experts. In this paper, we present our domain specific language ``Tool Ontology and Optimization Language'' (Tool) and describe how we integrated automated reasoners and planners in a way that makes them accessible to users which have little programming knowledge, but expertise in manufacturing domain and no previous experience with or knowledge about the underlying reasoners. We present encouraging results by applying Tool to a case study from the automotive and aerospace industry.},
booktitle = {Proceedings of the 2018 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {44–56},
numpages = {13},
keywords = {robotics and automation, knowledge integration, industry 4.0, human-robot cooperation, domain specific language, cyber-physical systems, automated reasoning, assembly planning},
location = {Boston, MA, USA},
series = {Onward! 2018}
}

@proceedings{10.1145/3640794,
title = {CUI '24: Proceedings of the 6th ACM Conference on Conversational User Interfaces},
year = {2024},
isbn = {9798400705113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
location = {Luxembourg, Luxembourg}
}

@article{10.1145/3152889,
author = {Dumitrache, Anca and Aroyo, Lora and Welty, Chris},
title = {Crowdsourcing Ground Truth for Medical Relation Extraction},
year = {2018},
issue_date = {June 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {2},
issn = {2160-6455},
url = {https://doi.org/10.1145/3152889},
doi = {10.1145/3152889},
abstract = {Cognitive computing systems require human labeled data for evaluation and often for training. The standard practice used in gathering this data minimizes disagreement between annotators, and we have found this results in data that fails to account for the ambiguity inherent in language. We have proposed the CrowdTruth method for collecting ground truth through crowdsourcing, which reconsiders the role of people in machine learning based on the observation that disagreement between annotators provides a useful signal for phenomena such as ambiguity in the text. We report on using this method to build an annotated data set for medical relation extraction for the cause and treat relations, and how this data performed in a supervised training experiment. We demonstrate that by modeling ambiguity, labeled data gathered from crowd workers can (1) reach the level of quality of domain experts for this task while reducing the cost, and (2) provide better training data at scale than distant supervision. We further propose and validate new weighted measures for precision, recall, and F-measure, which account for ambiguity in both human and machine performance on this task.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = jul,
articleno = {11},
numpages = {20},
keywords = {relation extraction, natural language ambiguity, inter-annotator disagreement, crowdtruth, crowd truth, clinical natural language processing, Ground truth}
}

@inproceedings{10.1145/3227609.3227654,
author = {Mandi\'{c}, Milinko},
title = {Semantic Web based software platform for curriculum harmonization},
year = {2018},
isbn = {9781450354899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3227609.3227654},
doi = {10.1145/3227609.3227654},
abstract = {This paper presents a software platform for comparing informatics teacher education curricula. The ontological model of the chosen informatics teachers' curriculum from the Republic of Serbia and the reference informatics teachers' curriculum model were created. The semi-automatic software platform is based on the standard techniques and methods of ontology matching. The created ontological models of the teacher education curricula are compared using the developed software. Analysis of the results of comparison includes consideration of classes' matching, obtained system evaluation (by the expert team) and a harmonization of the Revised Bloom's taxonomy categories. Also, the obtained results are compared with the results obtained for other combinations of input ontological models (secondary informatics and informatics teacher education curricula models). The analysis of the results revealed the need to improve the content and structure of the observed model curricula as well as the limits of the developed system and possibilities of improving the software platform.},
booktitle = {Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics},
articleno = {31},
numpages = {9},
keywords = {teacher education curriculum, ontology, matching, informatics, alignment},
location = {Novi Sad, Serbia},
series = {WIMS '18}
}

@article{10.1145/3527635,
author = {Steimann, Friedrich},
title = {Containerless Plurals: Separating Number from Type in Object-Oriented Programming},
year = {2022},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/3527635},
doi = {10.1145/3527635},
abstract = {To let expressions evaluate to no or many objects, most object-oriented programming languages require the use of special constructs that encode these cases as single objects or values. While the requirement to treat these standard situations idiomatically seems to be broadly accepted, I argue that its alternative, letting expressions evaluate to any number of objects directly, has several advantages that make it worthy of consideration. As a proof of concept, I present a core object-oriented programming language, dubbed Num, which separates number from type so that the type of an expression is independent of the number of objects it may evaluate to, thus removing one major obstacle to using no, one, and many objects uniformly. Furthermore, Num abandons null references, replaces the nullability of reference types with the more general notion of countability, and allows methods to be invoked on any number of objects, including no object. To be able to adapt behavior to the actual number of receivers, Num complements instance methods with plural methods, that is, with methods that operate on a number of objects jointly and that replace static methods known from other languages. An implementation of Num in Prolog and accompanying type and number safety proofs are presented.},
journal = {ACM Trans. Program. Lang. Syst.},
month = sep,
articleno = {21},
numpages = {56},
keywords = {object-relational programming, null-safety, bunches, collections, Multiplicities in programming}
}

@inproceedings{10.1145/3734947.3734958,
author = {Katwe, Praveen Kumar and Balabantaray, Rakesh Chandra and Vittala, Kali Prasad},
title = {Evaluating Relation Hallucination in Text Summarization: An Introduction to the Relation Hallucination Index},
year = {2025},
isbn = {9798400713187},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3734947.3734958},
doi = {10.1145/3734947.3734958},
abstract = {Text summarization involves generating a concise, precise, and coherent summary that captures the essence of a longer document. Relation Hallucination refers to the generation of summary sentences that imply or state relationships between entities or events that were not present or implied in the original document. Relation Hallucination is a pivotal concern in abstractive text summarization, where models fabricate or exaggerate connections, leading to misleading summaries. This paper delves deep into the evaluation of Relation Hallucination in abstractive summarization. We introduce a novel metric, the Relation Hallucination Index, designed to evaluate and quantify Relation hallucinations across various state-of-the-art models. Emphasizing the paramount importance of maintaining accurate relational context in summaries, this article showcases the efficacy of the Relation hallucination index in discerning fabricated relationships. The Relation hallucination index provides a quantitative measure of the various levels of Relation Hallucination present in generated summaries, enabling practitioners from industry as well as Academia to select models aligned with desired hallucination parameters for their tailored applications.},
booktitle = {Proceedings of the 16th Annual Meeting of the Forum for Information Retrieval Evaluation},
pages = {88–94},
numpages = {7},
keywords = {Hallucination;Abstractive summarization; Relation Tuples; Positive Hallucination; Negative Hallucination; Lost Focus; Extractiveness Factor;Over Focus},
location = {
},
series = {FIRE '24}
}

@inproceedings{10.1145/3453483.3454047,
author = {Chen, Qiaochu and Lamoreaux, Aaron and Wang, Xinyu and Durrett, Greg and Bastani, Osbert and Dillig, Isil},
title = {Web question answering with neurosymbolic program synthesis},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454047},
doi = {10.1145/3453483.3454047},
abstract = {In this paper, we propose a new technique based on program synthesis for extracting information from webpages. Given a natural language query and a few labeled webpages, our method synthesizes a program that can be used to extract similar types of information from other unlabeled webpages. To handle websites with diverse structure, our approach employs a neurosymbolic DSL that incorporates both neural NLP models as well as standard language constructs for tree navigation and string manipulation. We also propose an optimal synthesis algorithm that generates all DSL programs that achieve optimal F1 score on the training examples. Our synthesis technique is compositional, prunes the search space by exploiting a monotonicity property of the DSL, and uses transductive learning to select programs with good generalization power. We have implemented these ideas in a new tool called WebQA and evaluate it on 25 different tasks across multiple domains. Our experiments show that WebQA significantly outperforms existing tools such as state-of-the-art question answering models and wrapper induction systems.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {328–343},
numpages = {16},
keywords = {Web Information Extraction, Programming by Example, Program Synthesis},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{10.1145/3672758.3672868,
author = {Wang, Yi Long and Wen, Ying Zi and Wang, Yao Hui and She, Xin Peng and Sun, Xiao Hu and Lyu, Xue Qiang and Hao, Qiang},
title = {Named entity recognition method for mine electromechanical equipment field},
year = {2024},
isbn = {9798400716942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3672758.3672868},
doi = {10.1145/3672758.3672868},
abstract = {Aiming at the lack of annotated corpus in the field of mine electromechanical equipment, the high similarity between different categories of entities and the long names of some entities, this paper proposed an entity extraction method of mine electromechanical equipment based on the fusion of technical words and comparative learning. Firstly, an ontology library was constructed according to the characteristics of mine electromechanical equipment, Word2Vec was used to obtain the vector representation of characters and words, and the term words were obtained by matching in the ontology library. The multi-term multi-head attention mechanism was used to assign larger weights to these term words and then they were fused with the character vector. Then, the Bi-LSTM model was used for feature extraction, and a contrastive learning strategy based on R-drop was used to reduce the recognition bias error caused by the similarity of entity names. We also improved the loss function by using the relative entropy loss calculated by the Bi-LSTM layer as the regularization term of the loss in the CRF layer to form a constraint on the loss function and enhance the robustness of the model to Dropout. Finally, we used the CRF model to decode to obtain the optimal label. Experimental results show that compared with the existing mainstream baseline method Lattice-LSTM, our proposed method achieves better results on the self-constructed mine electromechanical equipment corpus. The precision, recall and F1 value are improved by 4.73, 5.5 and 5.09 percentage points, respectively.},
booktitle = {Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering},
pages = {660–664},
numpages = {5},
location = {Xi' an, China},
series = {CAICE '24}
}

@inproceedings{10.1145/3366650.3366661,
author = {Rabut, Benedict A. and Fajardo, Arnel C. and Medina, Ruji P.},
title = {Multi-class Document Classification Using Improved Word Embeddings},
year = {2019},
isbn = {9781450372909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366650.3366661},
doi = {10.1145/3366650.3366661},
abstract = {In this paper, we conducted an experiment to build a classification model that combines different techniques in most of the Natural Language Processing Tasks. We used the word embedding method to transform every word in the dataset and to obtain the custom-built word embedding vectors. This is in contrast to the approaches in the previous literature that implement word embedding using the pre-trained word embedding vectors. We enriched the custom-built word embedding vectors by incorporating Part-of-Speech (POS) tag vectors to provide additional semantic information about the word to be used in training our proposed classification model. The proposed model was built using the neural network approach, which is considered to be more efficient and reliable in solving real problems for document classification tasks. We fine-tuned the parameters during the training of our neural network classification model with our aim to increase the performance in terms of classification accuracy. The experimental result demonstrates that our model performs remarkably well and increase the percentage accuracy up to 1.7\% compared to the accuracy results obtained by the previous baseline word embedding methods using the same dataset. It was also observed that our model outperforms some other traditional classification models implemented using different techniques and machine learning algorithms.},
booktitle = {Proceedings of the 2nd International Conference on Computing and Big Data},
pages = {42–46},
numpages = {5},
keywords = {Document Classification, Natural Language Processing, Word Embeddings},
location = {Taichung, Taiwan},
series = {ICCBD '19}
}

@inproceedings{10.5555/3643142.3643364,
author = {Wilsdorf, Pia and Zuska, Marian and Andelfinger, Philipp and Uhrmacher, Adelinde M. and Peters, Florian},
title = {Validation Without Data - Formalizing Stylized Facts of Time Series},
year = {2024},
isbn = {9798350369663},
publisher = {IEEE Press},
abstract = {A stylized fact is a simplified presentation of an empirical finding. When modeling and simulating complex systems and real data are sparse, stylized facts have become a key instrument for building trust in a model as they represent important requirements regarding the model's behavior. However, automatically validating stylized facts has remained limited as they are usually expressed in natural language. Therefore, we develop a formal language with a custom syntax and tailored predicates allowing modelers to unambiguously and succinctly describe important (temporal) characteristics of simulation traces or relationships between multiple traces via statistical tests. The proposed formal language is able to express numerous facts from the literature in different application domains, as well as to automatically check stylized facts. If stylized facts are defined at the beginning of a simulation study, formally expressing and checking them can streamline and guide the development of simulation models and their successive revisions.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2674–2685},
numpages = {12},
location = {San Antonio, Texas, USA},
series = {WSC '23}
}

@inproceedings{10.1145/3303772.3303834,
author = {Fiallos, Angel and Ochoa, Xavier},
title = {Semi-Automatic Generation of Intelligent Curricula to Facilitate Learning Analytics},
year = {2019},
isbn = {9781450362566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303772.3303834},
doi = {10.1145/3303772.3303834},
abstract = {Several Learning Analytics applications are limited by the cost of generating a computer understandable description of the course domain, what is called an Intelligent Curriculum. The following work contributes a novel approach to (semi-)automatically generate Intelligent Curriculum through ontologies extracted from existing learning materials such as digital books or web content. Through a series of natural language processing steps, the semi-structured information present in existing content is transformed into a concept-graph. This work also evaluates the proposed methodology by applying it to learning content for two different courses and measuring the quality of the extracted ontologies against manually generated ones. The results obtained suggest that the technique can be readily used to provide domain information to other Learning Analytics tools.},
booktitle = {Proceedings of the 9th International Conference on Learning Analytics \&amp; Knowledge},
pages = {46–50},
numpages = {5},
keywords = {ontologies, intelligent curriculum, NLP},
location = {Tempe, AZ, USA},
series = {LAK19}
}

@inproceedings{10.5555/3721488.3721682,
author = {Jokinen, Kristiina and Wilcock, Graham},
title = {Towards Domain Graphs and Dialogue Graphs for Conversational Grounding in HRI},
year = {2025},
publisher = {IEEE Press},
abstract = {Knowledge graphs have been used to improve robot dialogues by providing more sophisticated world knowledge. We now propose a new role for knowledge graphs in GenAI-based HRI that aims to reduce dialogue errors by better conversational grounding. This approach uses both domain knowledge graphs and dialogue history graphs, constructing shared knowledge via entity linking. We present first steps towards these aims, and also address sustainability by supporting the use of smaller models.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1373–1377},
numpages = {5},
keywords = {conversational grounding, human-robot dialogues, knowledge graphs, sustainability},
location = {Melbourne, Australia},
series = {HRI '25}
}


Scopus
EXPORT DATE: 11 September 2025

@ARTICLE{Kumar2025,
	author = {Kumar, Nilesh and Mukhtar, Muhammad Shahid},
	title = {An NLP-based method to mine gene and function relationships from published articles},
	year = {2025},
	journal = {Scientific Reports},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.1038/s41598-025-91809-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000066172&doi=10.1038%2Fs41598-025-91809-z&partnerID=40&md5=af4e198496f6558fda75e72ccc886700},
	abstract = {Understanding the intricacies of genes function within biological systems is paramount for scientific advancement and medical progress. Owing to the evolving landscape of this research and the complexity of biological processes, however, this task presents challenges. We introduce PATHAK, a natural language processing (NLP)-based method that mines relationships between genes and their functions from published scientific articles. PATHAK utilizes a pre-trained Transformer language model to generate sentence embeddings from a vast dataset of scientific documents. This enables the identification of meaningful associations between genes and their potential functional annotations. Our approach is adaptable and applicable across diverse scientific domains. Applying PATHAK to over 17,000 research articles focused on Arabidopsis thaliana, we assigned approximately 1493 GO terms to 10,976 genes by analyzing article sentences, comparing their embeddings to GO term embeddings, and mapping potential matches. The model demonstrates moderate-to-high predictive accuracy, capturing ~ 57% overlap of GO terms (6258 out of 10,976) between predicted and known annotations on TAIR, including 1271 and 161 exact matches and 4826 partially related terms. This method promises to significantly advance our understanding of gene functionality and potentially accelerate discoveries in the context of plant development, growth and stress responses in plants and other systems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Function; Gene Ontology; Nlp; Published Articles; Arabidopsis Thaliana; Article; Gene; Gene Function; Gene Ontology; Human; Language Model; Natural Language Processing; Physiological Stress; Plant Development; Arabidopsis; Bioinformatics; Data Mining; Genetics; Molecular Genetics; Procedures; Computational Biology; Data Mining; Gene Ontology; Molecular Sequence Annotation; Natural Language Processing},
	keywords = {Arabidopsis thaliana; article; gene; gene function; gene ontology; human; language model; natural language processing; physiological stress; plant development; Arabidopsis; bioinformatics; data mining; genetics; molecular genetics; procedures; Computational Biology; Data Mining; Gene Ontology; Molecular Sequence Annotation; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Subramaniam2025,
	author = {Subramaniam, Suganya and Rizvi, Sara and Ramesh, Ramya and Sehgal, Vibhor and Gurusamy, Brinda and Arif, Hikmatullah and Tran, Jeffrey and Thamman, Ritu and Anyanwu, Emeka C. and Mastouri, Ronald A.},
	title = {Ontology-guided machine learning outperforms zero-shot foundation models for cardiac ultrasound text reports},
	year = {2025},
	journal = {Scientific Reports},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.1038/s41598-024-83540-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218835532&doi=10.1038%2Fs41598-024-83540-y&partnerID=40&md5=afd2b6e3a7a675443fb8214643183d98},
	abstract = {Big data can revolutionize research and quality improvement for cardiac ultrasound. Text reports are a critical part of such analyses. Cardiac ultrasound reports include structured and free text and vary across institutions, hampering attempts to mine text for useful insights. Natural language processing (NLP) can help and includes both statistical- and large language model based techniques. We tested whether we could use NLP to map cardiac ultrasound text to a three-level hierarchical ontology. We used statistical machine learning (EchoMap) and zero-shot inference using GPT. We tested eight datasets from 24 different institutions and compared both methods against clinician-scored ground truth. Despite all adhering to clinical guidelines, institutions differed in their structured reporting. EchoMap performed best with validation accuracy of 98% for the first ontology level, 93% for first and second levels, and 79% for all three. EchoMap retained performance across external test datasets and could extrapolate to examples not included in training. EchoMap’s accuracy was comparable to zero-shot GPT at the first level of the ontology and outperformed GPT at second and third levels. We show that statistical machine learning can map text to structured ontology and may be especially useful for small, specialized text datasets. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Echocardiography Report; Large Language Models; Machine Learning; Natural Language Processing; Ontology; Echocardiography; Human; Machine Learning; Natural Language Processing; Procedures; Echocardiography; Humans; Machine Learning; Natural Language Processing},
	keywords = {echocardiography; human; machine learning; natural language processing; procedures; Echocardiography; Humans; Machine Learning; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@ARTICLE{Peng2025,
	author = {Peng, Yue and Wu, Junze and Sun, Yi and Zhang, Yuanxing and Wang, Qiyao and Shao, Shuai},
	title = {Contrastive-learning of language embedding and biological features for cross modality encoding and effector prediction},
	year = {2025},
	journal = {Nature Communications},
	volume = {16},
	number = {1},
	pages = {},
	doi = {10.1038/s41467-025-56526-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217875320&doi=10.1038%2Fs41467-025-56526-1&partnerID=40&md5=6f829f98852e195f307bad1417d19ae8},
	abstract = {Identifying and characterizing virulence proteins secreted by Gram-negative bacteria are fundamental for deciphering microbial pathogenicity as well as aiding the development of therapeutic strategies. Effector predictors utilizing pre-trained protein language models (PLMs) have shown sound performance by leveraging extensive evolutionary and sequential protein features. However, the accuracy and sensitivity of effector prediction remain challenging. Here, we introduce a model named Contrastive-learning of Language Embedding and Biological Features (CLEF) leveraging contrastive learning to integrate PLM representations with supplementary biological features. Biologically information is captured in learned contextualized embeddings to yield meaningful representations. With cross-modality biological features, CLEF outperforms state-of-the-art (SOTA) models in predicting type III, type IV, and type VI secreted effectors (T3SEs/T4SEs/T6SEs) in enteric pathogens. All experimentally verified effectors in Enterohemorrhagic Escherichia coli and 41 of 43 experimentally verified T3SEs of Salmonella Typhimurium are recognized. Moreover, 12 predicted T3SEs and 11 predicted T6SEs are validated by extensive experiments in Edwardsiella piscicida. Furthermore, integrating omics data via CLEF framework enhances protein representations to illustrate effector-effector interactions and determine in vivo colonization-essential genes. Collectively, CLEF provides a blueprint to bridge the gap between in silico PLM’s capacity and experimental biological information to fulfill complicated tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adenylate Cyclase; Bacterial Proteins; Virulence Factors; Adenylate Cyclase; Bacterial Protein; Transcriptome; Translation Termination Factor; Virulence Factor; Bacterium; Biological Processes; Gene Expression; Host-pathogen Interaction; Microbial Activity; Microbiology; Protein; Amino Acid Sequence; Article; Autoencoder; Bacterial Secretion System; Bacterial Virulence; Comparative Study; Contrastive Learning; Controlled Study; Cross Validation; Deep Learning; Edwardsiella; Edwardsiella Piscicida; Effector Prediction; Embedding; Enterohemorrhagic Escherichia Coli; Enteropathogen; Few Shot Learning; Five Fold Cross Validation; Gene Ontology; Immunoblotting; Learning Algorithm; Multilayer Perceptron; Nonhuman; Predictive Model; Protein Interaction; Protein Language Model; Protein Secretion; Protein Structure; Salmonella Enterica Serovar Typhimurium; Supernatant; Transposon; Type Iii Secretion System; Type Vi Secretion System; Western Blotting; Bioinformatics; Genetics; Gram Negative Bacterium; Machine Learning; Metabolism; Pathogenicity; Procedures; Virulence; Bacterial Proteins; Computational Biology; Gram-negative Bacteria; Machine Learning; Salmonella Typhimurium; Virulence; Virulence Factors},
	keywords = {adenylate cyclase; bacterial protein; transcriptome; translation termination factor; virulence factor; bacterium; biological processes; gene expression; host-pathogen interaction; microbial activity; microbiology; protein; amino acid sequence; Article; autoencoder; bacterial secretion system; bacterial virulence; comparative study; contrastive learning; controlled study; cross validation; deep learning; Edwardsiella; Edwardsiella piscicida; effector prediction; embedding; enterohemorrhagic Escherichia coli; enteropathogen; few shot learning; five fold cross validation; gene ontology; immunoblotting; learning algorithm; multilayer perceptron; nonhuman; predictive model; protein interaction; protein language model; protein secretion; protein structure; Salmonella enterica serovar Typhimurium; supernatant; transposon; type III secretion system; type VI secretion system; Western blotting; bioinformatics; genetics; Gram negative bacterium; machine learning; metabolism; pathogenicity; procedures; virulence; Bacterial Proteins; Computational Biology; Gram-Negative Bacteria; Machine Learning; Salmonella typhimurium; Virulence; Virulence Factors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access}
}

@ARTICLE{Zhang2025,
	author = {Zhang, Xiao and Liu, Qian},
	title = {A graph neural network approach for hierarchical mapping of breast cancer protein communities},
	year = {2025},
	journal = {BMC Bioinformatics},
	volume = {26},
	number = {1},
	pages = {},
	doi = {10.1186/s12859-024-06015-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216588998&doi=10.1186%2Fs12859-024-06015-x&partnerID=40&md5=bc7d2916b4fa4b1db389b5c75fa72f18},
	abstract = {Background: Comprehensively mapping the hierarchical structure of breast cancer protein communities and identifying potential biomarkers from them is a promising way for breast cancer research. Existing approaches are subjective and fail to take information from protein sequences into consideration. Deep learning can automatically learn features from protein sequences and protein–protein interactions for hierarchical clustering. Results: Using a large amount of publicly available proteomics data, we created a hierarchical tree for breast cancer protein communities using a novel hierarchical graph neural network, with the supervision of gene ontology terms and assistance of a pre-trained deep contextual language model. Then, a group-lasso algorithm was applied to identify protein communities that are under both mutation burden and survival burden, undergo significant alterations when targeted by specific drug molecules, and show cancer-dependent perturbations. The resulting hierarchical map of protein communities shows how gene-level mutations and survival information converge on protein communities at different scales. Internal validity of the model was established through the convergence on BRCA2 as a breast cancer hotspot. Further overlaps with breast cancer cell dependencies revealed SUPT6H and RAD21, along with their respective protein systems, HOST:37 and HOST:861, as potential biomarkers. Using gene-level perturbation data of the HOST:37 and HOST:861 gene sets, three FDA-approved drugs with high therapeutic value were selected as potential treatments to be further evaluated. These drugs include mercaptopurine, pioglitazone, and colchicine. Conclusion: The proposed graph neural network approach to analyzing breast cancer protein communities in a hierarchical structure provides a novel perspective on breast cancer prognosis and treatment. By targeting entire gene sets, we were able to evaluate the prognostic and therapeutic value of genes (or gene sets) at different levels, from gene-level to system-level biology. Cancer-specific gene dependencies provide additional context for pinpointing cancer-related systems and drug-induced alterations can highlight potential therapeutic targets. These identified protein communities, in conjunction with other protein communities under strong mutation and survival burdens, can potentially be used as clinical biomarkers for breast cancer. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomarker; Breast Cancer; Graph Neural Network; Group Lasso; Hierarchical Clustering; Protein Communities; Biomarkers, Tumor; Neoplasm Proteins; Hierarchical Clustering; Breast Cancer; Gene Levels; Gene Sets; Graph Neural Networks; Group Lasso; Hier-archical Clustering; Hierarchical Clustering; Hierarchical Structures; Protein Community; Protein Sequences; Lung Cancer; Tumor Marker; Tumor Protein; Algorithm; Artificial Neural Network; Breast Tumor; Female; Genetics; Human; Metabolism; Mutation; Procedures; Protein Protein Interaction; Proteomics; Algorithms; Biomarkers, Tumor; Breast Neoplasms; Female; Humans; Mutation; Neoplasm Proteins; Neural Networks, Computer; Protein Interaction Maps; Proteomics},
	keywords = {Hierarchical clustering; Breast Cancer; Gene levels; Gene sets; Graph neural networks; Group LASSO; Hier-archical clustering; Hierarchical Clustering; Hierarchical structures; Protein community; Protein sequences; Lung cancer; tumor marker; tumor protein; algorithm; artificial neural network; breast tumor; female; genetics; human; metabolism; mutation; procedures; protein protein interaction; proteomics; Algorithms; Biomarkers, Tumor; Breast Neoplasms; Female; Humans; Mutation; Neoplasm Proteins; Neural Networks, Computer; Protein Interaction Maps; Proteomics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Taskiran2025,
	author = {Taskiran, Naz Pinar and Tsai, Chia En Jacklyn and Huang, Shuxin and Chakraborty, Arijit and Venkatasubramanian, Venkat},
	title = {A knowledge-graph-based pharmaceutical engineering chatbot for drug discovery},
	year = {2025},
	journal = {Computers and Chemical Engineering},
	volume = {203},
	pages = {},
	doi = {10.1016/j.compchemeng.2025.109318},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013583716&doi=10.1016%2Fj.compchemeng.2025.109318&partnerID=40&md5=49440150e0eb019332fe653a9af41b3a},
	abstract = {Despite their success in day-to-day applications, ChatGPT and other large language models (LLMs) have not covered as much ground in scientific and engineering domains. One key challenge is the abundance of domain-specific terminology, which an LLM is not trained to extract in accordance with the underlying physical laws. Such black-box models can also lead to unreliable results or hallucinations. Hybrid AI, which combines data-driven and symbolic methods, leverages domain knowledge to add explainability and reliability to answers. Our group has previously developed a domain-informed ontology-based information extraction tool called SUSIE, which extracts key terms and their context to present them to the user as knowledge graphs (KGs). Although KGs are used to visualize relationships between different entities, they are not easily accessible for user questions. However, they serve as a structured input for LLMs. Thus, KGs can efficiently query a corpus of pharmaceutical documents, streamlining drug discovery and manufacturing processes. In this work, we propose methods to improve the information extraction capabilities of SUSIE by expanding its knowledge base and improving its ability to understand scientific material through a sentence-restructuring module. Additionally, we present a customized question-and-answer module that enables the user to query from generated KGs and get an answer in natural language. Unlike black-box models such as those purely powered by OpenAI's models and the LangChain GraphQA packages, combining our KGs with Neo4j limits hallucinations and provides reliable and traceable answers in a user-friendly chatbot interface. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Hybrid Ai; Knowledge Graph; Llms; Neo4j; Ontology; Pharmaceutical Engineering; Chatbots; Data Mining; Domain Knowledge; Drug Discovery; Drug Products; Extraction; Graphic Methods; Information Retrieval; Knowledge Graph; Knowledge Management; Query Processing; Black Box Modelling; Graph-based; Hybrid Ai; Knowledge Graphs; Language Model; Large Language Model; Neo4j; Ontology's; Pharmaceutical Engineering; Ontology},
	keywords = {Chatbots; Data mining; Domain Knowledge; Drug discovery; Drug products; Extraction; Graphic methods; Information retrieval; Knowledge graph; Knowledge management; Query processing; Black box modelling; Graph-based; Hybrid AI; Knowledge graphs; Language model; Large language model; Neo4j; Ontology's; Pharmaceutical engineering; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Garcia2025,
	author = {Garcia, Brandon T. and Westerfield, Lauren E. and Yelemali, Priya and Gogate, Nikhita and Rivera-Muñoz, Edgar Andres and Du, Haowei and Dawood, Moez and Jolly, Angad and Lupski, James R. and Posey, Jennifer E.},
	title = {Improving automated deep phenotyping through large language models using retrieval-augmented generation},
	year = {2025},
	journal = {Genome Medicine},
	volume = {17},
	number = {1},
	pages = {},
	doi = {10.1186/s13073-025-01521-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013557478&doi=10.1186%2Fs13073-025-01521-w&partnerID=40&md5=e436c625e97769d29fd3ce27db2f0b48},
	abstract = {Background: Diagnosing rare genetic disorders relies on precise phenotypic and genotypic analysis, with the Human Phenotype Ontology (HPO) providing a standardized language for capturing clinical phenotypes. Rule-based HPO extraction tools use concept recognition to automatically identify phenotypes, but they often struggle with incomplete phenotype assignment, requiring significant manual review. While large language models (LLMs) hold promise for more context-driven phenotype extraction, they are prone to errors and “hallucinations,” making them less reliable without further refinement. We present RAG-HPO, a Python-based tool that leverages retrieval-augmented generation (RAG) to elevate accuracy of HPO term assignment by LLM. This approach bypasses the limitations of baseline models and eliminates the need for time- and resource-intensive fine-tuning. RAG-HPO integrates a dynamic vector database, containing > 54,000 phenotypic phrases mapped to HPO IDs, which allows real-time retrieval and contextual matching. The RAG-HPO workflow begins by extracting phenotypic phrases from clinical text via an LLM and then matching them via semantic similarity to entries within the database. The best term matches are returned to the LLM as context for final HPO term assignment of each phrase. Results: Performance was benchmarked on 112 published case reports with 1792 manually assigned HPO terms and compared to Doc2HPO, ClinPhen, and FastHPOCR. In evaluations, RAG-HPO + LLaMa-3.1 70B achieved a mean precision of 0.81, recall of 0.76, and an F1 score of 0.78—significantly surpassing conventional tools (p < 0.00001). RAG-HPO returned 1648 terms, of which 19.1% (315) were false positives that did not exactly match our manually annotated standard. Among these, < 1% (1/315) represented hallucinations, and 1.3% (4/315) represented terms with no ontological relationship to the desired target; the remaining false positives (95.2%, 300/315) were broader ancestor terms of the target term, which may still be relevant to users in many contexts. Conclusions: RAG-HPO is a user-friendly, adaptable tool designed for secure evaluation of clinical text and outperforms standard HPO-matching tools in precision, recall, and F1. Its enhanced precision and recall represent a substantial advancement in phenotypic analysis, accelerating the identification of genetic mechanisms underlying rare diseases and driving progress in genetic research and clinical genomics. RAG-HPO is available at https://github.com/PoseyPod/RAG-HPO. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Genomics; Generative Ai; Generative Pre-trained Transformer (gpt); Human Phenotype Ontology (hpo); Large Language Models (llms); Llama-3; Natural Language Processing (nlp); Phenotyping; Retrieval-augmented Generation (rag); Alanine Aminotransferase; Article; Automation; Cohort Analysis; Comparative Study; F1 Score; False Positive Result; Gene Function; Gene Ontology; Genetic Analysis; Human; Large Language Model; Phenotype; Publication; Rare Disease; Retrieval Augmented Generation; Scoring System; Article; Biological Ontology; Case Report; Generative Artificial Intelligence; Generative Pretrained Transformer; Genotype; Hallucination; Llama; Natural Language Processing; Workflow; Alanine Aminotransferase},
	keywords = {Article; automation; cohort analysis; comparative study; F1 score; false positive result; gene function; gene ontology; genetic analysis; human; large language model; phenotype; publication; rare disease; retrieval augmented generation; scoring system; article; biological ontology; case report; generative artificial intelligence; generative pretrained transformer; genotype; hallucination; llama; natural language processing; workflow; alanine aminotransferase},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Shi2025,
	author = {Shi, Xuanyu and Zhao, Wenjing and Chen, Ting and Yang, Chao and Du, Jian},
	title = {Evidence triangulator: using large language models to extract and synthesize causal evidence across study designs},
	year = {2025},
	journal = {Nature Communications},
	volume = {16},
	number = {1},
	pages = {},
	doi = {10.1038/s41467-025-62783-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012984973&doi=10.1038%2Fs41467-025-62783-x&partnerID=40&md5=6c7c0d7e6de22060d6fb918902827e12},
	abstract = {Health strategies increasingly emphasize both behavioural and biomedical interventions, yet the complex and often contradictory guidance on diet, behavior, and health outcomes complicates evidence-based decision-making. Evidence triangulation across diverse study designs is essential for balancing biases and establishing causality, but scalable, automated methods for achieving this are lacking. In this study, we assess the performance of large language models in extracting both ontological and methodological information from scientific literature to automate evidence triangulation. A two-step extraction approach—focusing on exposure-outcome concepts first, followed by relation extraction—outperforms a one-step method, particularly in identifying the direction of effect (F1 = 0.86) and statistical significance (F1 = 0.96). Using salt intake and blood pressure as a case study, we calculate the Convergency of Evidence and Level of Convergency, finding a strong excitatory effect of salt on blood pressure (942 studies), and weak excitatory effect on cardiovascular diseases and deaths (124 studies). This approach complements traditional meta-analyses by integrating evidence across study designs, and enabling rapid, dynamic assessment of scientific controversies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Sodium Chloride; Sodium Chloride, Dietary; Sodium Chloride; Accuracy Assessment; Blood; Detection Method; Language; Triangulation; Accuracy; Analytical Error; Article; Blood Pressure; Cardiovascular Disease; Cardiovascular Mortality; Case Study; Causality; Cochrane Library; Data Extraction; Dietary Intake; Evidence Synthesis; Gold Standard; Human; Information Retrieval; Internal Validity; Ischemic Heart Disease; Knowledge Graph; Large Language Model; Medical Subject Headings; Medline; Mendelian Randomization Analysis; Methodology; Nomenclature; Observational Study; Ontology; Outcome Assessment; Predictive Model; Probability; Randomized Controlled Trial (topic); Salt Intake; Scientific Literature; Study Design; Adverse Event; Drug Effect; Etiology; Blood Pressure; Cardiovascular Diseases; Causality; Humans; Large Language Models; Research Design; Sodium Chloride, Dietary},
	keywords = {sodium chloride; accuracy assessment; blood; detection method; language; triangulation; accuracy; analytical error; Article; blood pressure; cardiovascular disease; cardiovascular mortality; case study; causality; Cochrane Library; data extraction; dietary intake; evidence synthesis; gold standard; human; information retrieval; internal validity; ischemic heart disease; knowledge graph; large language model; Medical Subject Headings; Medline; Mendelian randomization analysis; methodology; nomenclature; observational study; ontology; outcome assessment; predictive model; probability; randomized controlled trial (topic); salt intake; scientific literature; study design; adverse event; drug effect; etiology; Blood Pressure; Cardiovascular Diseases; Causality; Humans; Large Language Models; Research Design; Sodium Chloride, Dietary},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2025,
	author = {Liu, Qigang and Wang, Yinfan and Mu, Lifeng and Li, Jun},
	title = {Research on the construction and application of problem-method-oriented academic graph empowered by LLM},
	year = {2025},
	journal = {Discover Computing},
	volume = {28},
	number = {1},
	pages = {},
	doi = {10.1007/s10791-025-09675-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012962905&doi=10.1007%2Fs10791-025-09675-2&partnerID=40&md5=a5e9b29e96ba8299450eb6d8d25e78df},
	abstract = {Nowadays, the volume of literature in each field is huge and is growing rapidly, which posts challenge to researchers’ literature review. In this circumstance, developing useful tool for achieving efficient literature management is of high value. Traditional literature management tools, such as tools for key word searching, paper recommendation, relation visualization, and keyword cloud drawing, are not suitable for conducting content-level literature review. To address the issues of traditional literature management tools, a novel problem and method-oriented fine-grained academic graph is proposed to facilitate the exploration of research questions, methodologies, study perspectives, and their connections hidden in massive literature. For building such graph, a new ontology dedicated for describing the features of research paper is developed, an innovative multi-relation join extraction model is proposed, and a creative approach for leveraging the Large Language Models (LLM) to augment the triplet extraction results generated by supervised-learning model is developed. Experiments on widely used benchmark datasets show that the proposed multi-relation extraction model is able to achieve at least 8.01% and 8.65% improvement on entity identification and relation classification respectively, compared with state-of-the-art models. The visualized demonstration of the proposed graph shows that our graph is capable of accurately capturing the problem network, method network and hot topics hidden in massive literature. The Q&A system supported by the proposed graph demonstrates that our graph is really helpful for conducting literature review. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Academic Graph; Artificial Intelligence; Large Language Model; Named Entity Recognition; Relation Extraction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Koné2025,
	author = {Koné, Bamory Ahmed Toru and Boukadi, Khouloud and Grati, Rima and Ben-Abdallah, Emna and Mecella, Massimo},
	title = {LLM-driven semantic explanations for soil moisture prediction models},
	year = {2025},
	journal = {Smart Agricultural Technology},
	volume = {12},
	pages = {},
	doi = {10.1016/j.atech.2025.101174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011092015&doi=10.1016%2Fj.atech.2025.101174&partnerID=40&md5=effe7071eed31d4131791748bdc66492},
	abstract = {Efficient soil moisture prediction is crucial for sustainable agricultural practices, especially in the face of climate change and increasing water scarcity. However, the adoption of machine learning (ML) models in this context is frequently limited by their lack of interpretability, particularly among non-expert users such as farmers. This study proposes a novel approach to soil moisture prediction that combines high predictive performance with enhanced explainability. We propose a framework that leverages large language models (LLMs) to generate textual explanations based on a proposed irrigation and soil moisture ontology, thus making the model's predictions more understandable to farmers. The ontology formalizes essential agricultural concepts and their interrelationships, enabling semantically rich explanations to bridge the gap between sophisticated model results and practical decision-making. Our approach is exemplified by a prototype system that provides both predictions and user-friendly explanations. The findings highlight the potential of combining advanced ML techniques with semantic reasoning to improve the interpretability and adoption of Artificial Intelligence in agriculture. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Llm; Machine Learning; Ontology; Soil Moisture; Xai},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Höltgen2025,
	author = {Höltgen, L. and Zentgraf, Sven and Hagedorn, Philipp and König, Markus},
	title = {Utilizing large language models for semantic enrichment of infrastructure condition data: a comparative study of GPT and Llama models},
	year = {2025},
	journal = {AI in Civil Engineering},
	volume = {4},
	number = {1},
	pages = {},
	doi = {10.1007/s43503-025-00055-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010910273&doi=10.1007%2Fs43503-025-00055-9&partnerID=40&md5=b3c8e5c70382506c88926aeba1af7937},
	abstract = {Relational databases containing construction-related data are widely used in the Architecture, Engineering, and Construction (AEC) industry to manage diverse datasets, including project management and building-specific information. This study explores the use of large language models (LLMs) to convert construction data from relational databases into formal semantic representations, such as the resource description framework (RDF). Transforming this data into RDF-encoded knowledge graphs enhances interoperability and enables advanced querying capabilities. However, existing methods like R2RML and Direct Mapping face significant challenges, including the need for domain expertise and scalability issues. LLMs, with their advanced natural language processing capabilities, offer a promising solution by automating the conversion process, reducing the reliance on expert knowledge, and semantically enriching data through appropriate ontologies. This paper evaluates the potential of four LLMs (two versions of GPT and Llama) to enhance data enrichment workflows in the construction industry and examines the limitations of applying these models to large-scale datasets. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Architecture, Engineering, And Construction (aec); Gpt; Large Language Models; Llama; R2rml; Relational Databases; Semantic Enrichment},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2025,
	author = {Liu, Jiangfeng and Liu, Zhiyuan and Shen, Yu and Zhang, Ran and Song, Ningyuan and Liu, Jialong and Pei, Lei},
	title = {Multi-dimensional intelligent reorganization and utilization of knowledge in ‘Biographies of Chinese Thinkers’},
	year = {2025},
	journal = {npj Heritage Science},
	volume = {13},
	number = {1},
	pages = {},
	doi = {10.1038/s40494-025-01669-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010641220&doi=10.1038%2Fs40494-025-01669-z&partnerID=40&md5=59ec4a456398ba79af1b8af13c9fe662},
	abstract = {Biographical texts often fail to fully showcase their rich semantic knowledge due to traditional narrative modes and knowledge gaps between authors and readers. A multidimensional knowledge reorganization framework for biographical texts involves semantic description, fine-grained knowledge extraction, and knowledge reorganization applications. Based on ontology theory, a core conceptual model for biographical texts was established, employing GPT-4 and BERT for entity recognition. Knowledge reorganization strategies were proposed for key application scenarios and validated through case visualizations. A conceptual model for biographical texts was constructed. Significant enhancement of tag corpora was achieved through LLMs and the RoBERTa-BiLSTM-CRF model, achieving optimal fine-tuning in NER. Strategies based on temporal-spatial transformation, social network analysis, and thematic evolution were proposed, culminating in a knowledge graph centered on “Character-Works-Ideas”. Based on methods proposed by us, issues in semantic description and knowledge extraction of biographical texts have been effectively resolved, enhancing the application value of biographical resources. © 2025 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Ghasemizadeh2025,
	author = {Ghasemizadeh, Mehrdad},
	title = {From answers to questions: the Q-centric model of intelligence},
	year = {2025},
	journal = {Discover Artificial Intelligence},
	volume = {5},
	number = {1},
	pages = {},
	doi = {10.1007/s44163-025-00402-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010560488&doi=10.1007%2Fs44163-025-00402-w&partnerID=40&md5=53b190a31805b8f48597251b948c02c8},
	abstract = {While LLMs score highly on reference alignment and produce increasingly human-like responses, a gap still prevents us from reaching AGI or genuine self-awareness. Is this merely a technical problem—or a philosophical one? If ever-better “answers” in AGI are not absolute endpoints, what are they? One way to understand “answers” is as recombinations within lived interpretation—temporary syntheses that organize experience into coherent conceptual structures. This article posits that no philosophical “answer” is a terminal truth; it is an integration of conceptual form that stabilizes meaning in the face of ambiguity. Creativity, exploration, and the very act of questioning—rather than the pursuit of definitive answers—may be the true engines of both philosophical insight and human-like intelligence. Here, we argue that genuine understanding emerges through continuous inquiry and recursive reorganization. Answers, in this view, remain provisional constructions, shaped by dissonance and forever open to revision. To ground this thesis, we embed multiple traditions as epistemic momentum vectors. Phenomenological ambiguity, ontological instability, hermeneutic dissonance, pragmatic friction, and erotetic incompleteness each introduce distinct tensions that propel the Q-Centric architecture toward recursive questioning. Their divergence is not noise, but the generative condition of meaning-making. By integrating these layered vectors, we sketch a pathway to AGI systems that do more than respond coherently; they orient themselves toward what remains unresolved. Reconceptualizing “answers” as momentary crystallizations of tension, the Q-Centric model offers a philosophical foundation for building reflective, adaptive machines that question—rather than merely conclude. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Agi; Consciousness; Embodied Intelligence; Epistemic Architecture; Intelligent Systems Design; Question-centric Cognition; Cognitive Systems; Computer Architecture; Intelligent Systems; Iodine Compounds; Ontology; Philosophical Aspects; Agi; Conceptual Structures; Consciousness; Embodied Intelligence; Epistemic Architecture; Human Like; Human-like Intelligence; Intelligent System Design; Question-centric Cognition; Self Awareness; Systems Analysis},
	keywords = {Cognitive systems; Computer architecture; Intelligent systems; Iodine compounds; Ontology; Philosophical aspects; AGI; Conceptual structures; Consciousness; Embodied intelligence; Epistemic architecture; Human like; Human-like intelligence; Intelligent system design; Question-centric cognition; Self awareness; Systems analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Borghoff2025,
	author = {Borghoff, Uwe M. and Bottoni, Paolo and Pareschi, Remo},
	title = {An organizational theory for multi-agent interactions integrating human agents, LLMs, and specialized AI},
	year = {2025},
	journal = {Discover Computing},
	volume = {28},
	number = {1},
	pages = {},
	doi = {10.1007/s10791-025-09667-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010417545&doi=10.1007%2Fs10791-025-09667-2&partnerID=40&md5=6184778f63595e0f2ad57de80c4a1b47},
	abstract = {Purpose: Recent advances in AI, especially in large language models, have created new opportunities to integrate human and artificial agents through shared linguistic capabilities. This paper presents a multi-agent organizational framework in which human agents, LLMs, and specialized agents (narrow AIs) collaborate via dynamic, topic-based group formation. Topic-driven interactions enable agents to coalesce around evolving interests, supported by threshold-based protocols for temporal adaptation, topic emergence, and participation. Methods: Within our framework, human agents guide the overall system objectives, while consultant agents (LLMs) provide semantic analysis and mediation, and specialized agents perform focused domain tasks. By leveraging automated topic modeling, the approach eschews rigid ontologies and instead supports adaptive and interpretable content management. Mathematical properties ensure system coherence—across roles, tasks, and timescales—while allowing the natural evolution of interests and groups. Results: We illustrate the framework’s versatility with example scenarios in emergency response, healthcare research, and financial decision-making, emphasizing how human decision-makers, LLM-based consultants, and specialized worker agents jointly fulfill complex goals through transparent topic alignment and threshold-driven coordination. This formalization advances human-computer interaction as a multi-agent phenomenon that integrates human insight with the strengths of next-generation AI models in a cohesive, evolving system. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adaptive Interaction Protocols; Human-computer Interaction; Large Language Models; Multi-agent Systems; Natural Language Processing; Topic-based Group Formation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@ARTICLE{Trofymenko2025,
	author = {Trofymenko, Maksym O. and Korchmar, Eduard and Kaduk, Denys and Vikhrak, Marta and Khilchevskyi, Bohdan and Nesmiian, Tetiana S. and Talapova, Polina S. and Ved, Max M. and Ageeva, Inna},
	title = {Jackalope Plus tool for post-coordination, ontology development, and precise mapping in observational health studies},
	year = {2025},
	journal = {Scientific Reports},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.1038/s41598-025-04046-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010088212&doi=10.1038%2Fs41598-025-04046-9&partnerID=40&md5=1239ded7fc6508b7b2fff55fa3335f55},
	abstract = {Accurate mapping of complex health data to the OMOP CDM while preserving clinical nuance remains a challenge. We introduce Jackalope Plus, a novel tool leveraging SNOMED CT post-coordination and a GPT-4o mini LLM, to significantly enhance the precision and efficiency of this process. Our two-step approach combines semantic search with LLM-driven standardization, enabling accurate conversion of intricate medical concepts. Evaluation on benchmark and custom datasets demonstrates that Jackalope Plus identifies correct mappings for over 77.5% of complex terminologies, substantially outperforming Usagi (52.5%) and matching the accuracy of manual mapping while offering up to 50% time savings. Jackalope Plus offers a versatile solution for diverse healthcare data environments. Future work will focus on refining the tool through user feedback integration and addressing ambiguities in overlapping concepts. A free beta version is available for research and feedback. Ethical review confirms no storage of patient-identifiable information. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biological Ontology; Human; Observational Study; Software; Systematized Nomenclature Of Medicine; Biological Ontologies; Humans; Observational Studies As Topic; Software},
	keywords = {biological ontology; human; observational study; software; Systematized Nomenclature of Medicine; Biological Ontologies; Humans; Observational Studies as Topic; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Noll2025,
	author = {Noll, Richard and Berger, Alexandra and Kieu, Dominik and Mueller, Tobias and O Bohmann, Ferdinand and Müller, Angelina and Holtz, Svea and Stoffers, Philipp and Hoehl, Sebastian and Guengoeze, Oya},
	title = {Assessing GPT and DeepL for terminology translation in the medical domain: A comparative study on the human phenotype ontology},
	year = {2025},
	journal = {BMC Medical Informatics and Decision Making},
	volume = {25},
	number = {1},
	pages = {},
	doi = {10.1186/s12911-025-03075-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009889147&doi=10.1186%2Fs12911-025-03075-8&partnerID=40&md5=bd18b655e161c3d85fd30a755b06af0d},
	abstract = {Background: This paper presents a comparative study of two state-of-the-art language models, OpenAI’s GPT and DeepL, in the context of terminology translation within the medical domain. Methods: This study was conducted on the human phenotype ontology (HPO), which is used in medical research and diagnosis. Medical experts assess the performance of both models on a set of 120 translated HPO terms and their 180 synonyms, employing a 4-point Likert scale (strongly agree = 1, agree = 2, disagree = 3, strongly disagree = 4). An independent reference translation from the HeTOP database was used to validate the quality of the translation. Results: The average Likert rating for the selected HPO terms was 1.29 for GPT-3.5 and 1.37 for DeepL. The quality of the translations was also found to be satisfactory for multi-word terms with greater ontological depth. The comparison with HeTOP revealed a high degree of similarity between the models’ translations and the reference translations. Conclusions: Statistical analysis revealed no significant differences in the mean ratings between the two models, indicating their comparable performance in terms of translation quality. The study not only illustrates the potential of machine translation but also shows incomplete coverage of translated medical terminology. This underscores the relevance of this study for cross-lingual medical research. However, the evaluation methods need to be further refined, specific translation issues need to be addressed, and the sample size needs to be increased to allow for more generalizable conclusions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Controlled Vocabulary; Gpt; Translations; Biological Ontology; Comparative Study; Human; Nomenclature; Phenotype; Translating (language); Biological Ontologies; Humans; Phenotype; Terminology As Topic; Translating},
	keywords = {biological ontology; comparative study; human; nomenclature; phenotype; translating (language); Biological Ontologies; Humans; Phenotype; Terminology as Topic; Translating},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhou2025,
	author = {Zhou, Zihan and Zeng, Ziyi and Jiang, Wenhao and Zhu, Yihui and Mao, Jiaxin and Yuan, Yonggui and Xia, Min and Zhao, Shubin and Yao, Mengyu and Chen, Yunqian},
	title = {Research on the proximity relationships of psychosomatic disease knowledge graph modules extracted by large language models},
	year = {2025},
	journal = {Scientific Reports},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.1038/s41598-025-05499-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009613875&doi=10.1038%2Fs41598-025-05499-8&partnerID=40&md5=e57adaf1927e58a5cae21419293977d0},
	abstract = {As social changes accelerate, the incidence of psychosomatic disorders has significantly increased, becoming a major challenge in global health issues. This necessitates an innovative knowledge system and analytical methods to aid in diagnosis and treatment. Here, we establish the ontology model and entity types, using the BERT model and LoRA-tuned LLM for named entity recognition, constructing the knowledge graph with 9668 triples. Next, by analyzing the network distances between disease, symptom, and drug modules, it was found that closer network distances among diseases can predict greater similarities in their clinical manifestations, treatment approaches, and psychological mechanisms, and closer distances between symptoms indicate that they are more likely to co-occur. Lastly, by comparing proximity scores, it was shown that symptom-disease pairs in primary diagnostic relationships have a stronger association and are of higher referential value than those in diagnostic relationships. The research results revealed the potential connections between diseases, co-occurring symptoms, and similarities in treatment strategies, providing new perspectives for the diagnosis and treatment of psychosomatic disorders and valuable information for future mental health research and practice. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Manifestation; Graph Structure Analysis; Network Distance; Proximity Metric; Psychological Disease; Diagnosis; Human; Language; Large Language Model; Psychosomatic Disorder; Humans; Language; Large Language Models; Psychophysiologic Disorders},
	keywords = {diagnosis; human; language; large language model; psychosomatic disorder; Humans; Language; Large Language Models; Psychophysiologic Disorders},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Breimann2025,
	author = {Breimann, Stephan and Kamp, Frits and Basset, Gabriele and Abou-Ajram, Claudia and Güner, Gökhan and Yanagida, Kanta and Okochi, Masayasu and Müller, Stephan A. and Lichtenthaler, Stefan Frieder and Langosch, Dieter J.},
	title = {Charting γ-secretase substrates by explainable AI},
	year = {2025},
	journal = {Nature Communications},
	volume = {16},
	number = {1},
	pages = {},
	doi = {10.1038/s41467-025-60638-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009542701&doi=10.1038%2Fs41467-025-60638-z&partnerID=40&md5=a8781b571cae536b1c50c692b9e9bf99},
	abstract = {Proteases recognize substrates by decoding sequence information—an essential cellular process elusive when recognition motifs are absent. Here, we unravel this problem for γ-secretase, an intramembrane-cleaving protease associated with Alzheimer’s disease and cancer, by developing Comparative Physicochemical Profiling (CPP), a sequence-based algorithm for identifying interpretable physicochemical features. We show that CPP deciphers a γ-secretase substrate signature with single-residue resolution, which can explain the conformational transitions observed in substrates upon γ-secretase binding. Using machine learning, we predict the entire human γ-secretase substrate scope, revealing numerous previously unknown substrates. Our approach outperforms state-of-the-art protein language models, improving prediction accuracy from 60% to 90%, and achieves an 88% success rate in experimental validation. Building on these advancements, we identify pathways and diseases not linked before to γ-secretase. Generally, CPP decodes physicochemical signatures—a concept that extends beyond sequence motifs. We anticipate that our approach will be broadly applicable to diverse molecular recognition processes. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Rev Protein; Amyloid Precursor Protein Secretases; Coxsackie Virus And Adenovirus Receptor Like Membrane Protein; Gamma Secretase; Rev Protein; Protein Binding; Secretase; Artificial Intelligence; Cancer; Machine Learning; Nervous System Disorder; Physicochemical Property; Protein; Substrate; Algorithm; Amino Acid Sequence; Article; Benchmarking; Bioinformatics; Cell Communication; Comparative Physicochemical Profiling; Conformational Transition; Controlled Study; Deep Learning; Explainable Artificial Intelligence; Feature Ranking; Gene Ontology; Human; Human Cell; Immunoblotting; Immunoregulation; Language Model; Measurement Accuracy; Molecular Fingerprinting; Molecular Interaction; Molecular Recognition; Network Analysis; Pathway Enrichment Analysis; Physical Chemistry; Position Weight Matrix; Prediction; Protein Conformation; Protein Expression; Protein Expression Level; Protein Function; Protein Language Model; Random Forest; Sequence Analysis; Shapley Additive Explanation; Support Vector Machine; Surface Area; Alzheimer Disease; Chemistry; Enzyme Specificity; Genetics; Metabolism; Algorithms; Alzheimer Disease; Amyloid Precursor Protein Secretases; Humans; Machine Learning; Protein Binding; Substrate Specificity},
	keywords = {coxsackie virus and adenovirus receptor like membrane protein; gamma secretase; Rev protein; protein binding; secretase; artificial intelligence; cancer; machine learning; nervous system disorder; physicochemical property; protein; substrate; algorithm; amino acid sequence; Article; benchmarking; bioinformatics; cell communication; comparative physicochemical profiling; conformational transition; controlled study; deep learning; explainable artificial intelligence; feature ranking; gene ontology; human; human cell; immunoblotting; immunoregulation; language model; measurement accuracy; molecular fingerprinting; molecular interaction; molecular recognition; network analysis; pathway enrichment analysis; physical chemistry; position weight matrix; prediction; protein conformation; protein expression; protein expression level; protein function; protein language model; random forest; sequence analysis; Shapley additive explanation; support vector machine; surface area; Alzheimer disease; chemistry; enzyme specificity; genetics; metabolism; Algorithms; Alzheimer Disease; Amyloid Precursor Protein Secretases; Humans; Machine Learning; Protein Binding; Substrate Specificity},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wen2025,
	author = {Wen, Baole and Shi, Sheng and Long, Yi and Dang, Yanan and Tian, Weidong},
	title = {PhenoDP: leveraging deep learning for phenotype-based case reporting, disease ranking, and symptom recommendation},
	year = {2025},
	journal = {Genome Medicine},
	volume = {17},
	number = {1},
	pages = {},
	doi = {10.1186/s13073-025-01496-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007459631&doi=10.1186%2Fs13073-025-01496-8&partnerID=40&md5=efe0e82e8841f07885f7dcb883388718},
	abstract = {Background: Current phenotype-based diagnostic tools often struggle with accurate disease prioritization due to incomplete phenotypic data and the complexity of rare disease presentations. Additionally, they lack the ability to generate patient-centered clinical insights or recommend further symptoms for differential diagnosis. Methods: We developed PhenoDP, a deep learning-based toolkit with three modules: Summarizer, Ranker, and Recommender. The Summarizer fine-tuned a distilled large language model to create clinical summaries from a patient’s Human Phenotype Ontology (HPO) terms. The Ranker prioritizes diseases by combining information content-based, phi-based, and semantic-based similarity measures. The Recommender employs contrastive learning to recommend additional HPO terms for enhanced diagnostic accuracy. Results: PhenoDP’s Summarizer produces more clinically coherent and patient-centered summaries than the general-purpose language model FlanT5. The Ranker achieves state-of-the-art diagnostic performance, consistently outperforming existing phenotype-based methods across both simulated and real-world datasets. The Recommender also outperformed GPT-4o and PhenoTips in improving diagnostic accuracy when its suggested terms were incorporated into different ranking pipelines. Conclusions: PhenoDP enhances Mendelian disease diagnosis through deep learning, offering precise summarization, ranking, and symptom recommendation. Its superior performance and open-source design make it a valuable clinical tool, with potential to accelerate diagnosis and improve patient outcomes. PhenoDP is freely available at https://github.com/TianLab-Bioinfo/PhenoDP. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Summarization; Contrastive Learning; Deep Learning; Disease Ranking; Human Phenotype Ontology; Large Language Models; Mendelian Disease; Phenotype-driven Diagnosis; Symptom Recommendation; Article; Chatgpt; Controlled Study; Deep Learning; Diagnostic Accuracy; Diagnostic Test Accuracy Study; Disease Classification; Disease Ranking; Human; Human Phenotype Ontology; Information Processing; Large Language Model; Major Clinical Study; Medical Literature; Monogenic Disorder; Ontology; Phenodp; Phenotype; Phenotype Driven Diagnosis; Real World Data; Semantics; Simulation; Symptom; Symptom Recommendation; Software; Deep Learning; Humans; Phenotype; Software},
	keywords = {Article; ChatGPT; controlled study; deep learning; diagnostic accuracy; diagnostic test accuracy study; disease classification; disease ranking; human; human phenotype ontology; information processing; large language model; major clinical study; medical literature; monogenic disorder; ontology; PhenoDP; phenotype; phenotype driven diagnosis; real world data; semantics; simulation; symptom; symptom recommendation; software; Deep Learning; Humans; Phenotype; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Potu2025,
	author = {Potu, Sai Teja and Niranjan Murthy, Rachana and Thomas, Akhil and Mishra, Lokesh and Prange, Natalie and Durmaz, Ali Riza},
	title = {Ontology-conformal recognition of materials entities using language models},
	year = {2025},
	journal = {Scientific Reports},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.1038/s41598-025-03619-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006735493&doi=10.1038%2Fs41598-025-03619-y&partnerID=40&md5=5f91821f165340010073eac7a0d7c3f1},
	abstract = {Extracting structured and semantically annotated materials information from unstructured scientific literature is a crucial step toward constructing machine-interpretable knowledge graphs and accelerating data-driven materials research. This is especially important in materials science, which is adversely affected by data scarcity. Data scarcity further motivates employing solutions such as foundation language models for extracting information which can in principle address several subtasks of the information extraction problem in a range of domains without the need of generating costly large-scale annotated datasets for each downstream task. However, foundation language models struggle with tasks like Named Entity Recognition (NER) due to domain-specific terminologies, fine-grained entities, and semantic ambiguity. The issue is even more pronounced when entities must map directly to pre-existing domain ontologies. This work aims to assess whether foundation large language models (LLMs) can successfully perform ontology-conformal NER in the materials mechanics and fatigue domain. Specifically, we present a comparative evaluation of in-context learning (ICL) with foundation models such as GPT-4 against fine-tuned task-specific language models, including MatSciBERT and DeBERTa. The study is performed on two materials fatigue datasets, which contain annotations at a comparatively fine-grained level adhering to the class definitions of a formal ontology to ensure semantic alignment and cross-dataset interoperability. Both datasets cover adjacent domains to assess how well both NER methodologies generalize when presented with typical domain shifts. Task-specific models are shown to significantly outperform general foundation models on an ontology-constrained NER. Our findings reveal a strong dependence on the quality of few-shot demonstrations in ICL to handle domain-shift. The study also highlights the significance of domain-specific pre-training by comparing task-specific models that differ primarily in their pre-training corpus. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Fatigue; Foundation Models; Large Language Models; Literature Mining; Materials Science; Named Entity Recognition; Ontology; Parameter-efficient Fine-tuning; Prompt Engineering; Alanine Aminotransferase; Alanine Aminotransferase; Ambiguity; Article; Context Learning; Fatigue; Human; Knowledge Graph; Language Model; Large Language Model; Mechanics; Mining; Ontology; Prompt Engineering},
	keywords = {alanine aminotransferase; ambiguity; article; context learning; fatigue; human; knowledge graph; language model; large language model; mechanics; mining; ontology; prompt engineering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wang2025,
	author = {Wang, Zhigang and Li, Xingxian and Zheng, Jie and He, Yongqun Oliver},
	title = {Unveiling differential adverse event profiles in vaccines via LLM text embeddings and ontology semantic analysis},
	year = {2025},
	journal = {Journal of Biomedical Semantics},
	volume = {16},
	number = {1},
	pages = {},
	doi = {10.1186/s13326-025-00331-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005987831&doi=10.1186%2Fs13326-025-00331-8&partnerID=40&md5=d8a909a9ba1cda702693298628cc509d},
	abstract = {Background: Vaccines are crucial for preventing infectious diseases; however, they may also be associated with adverse events (AEs). Conventional analysis of vaccine AEs relies on manual review and assignment of AEs to terms in terminology or ontology, which is a time-consuming process and constrained in scope. This study explores the potential of using Large Language Models (LLMs) and LLM text embeddings for efficient and comprehensive vaccine AE analysis. Results: We used Llama-3 LLM to extract AE information from FDA-approved vaccine package inserts for 111 licensed vaccines, including 15 influenza vaccines. Text embeddings were then generated for each vaccine’s AEs using the nomic-embed-text and mxbai-embed-large models. Llama-3 achieved over 80% accuracy in extracting AE text from vaccine package inserts. To further evaluate the performance of text embedding, the vaccines were clustered using two clustering methods: (1) LLM text embedding-based clustering and (2) ontology-based semantic similarity analysis. The ontology-based method mapped AEs to the Human Phenotype Ontology (HPO) and Ontology of Adverse Events (OAE), with semantic similarity analyzed using Lin’s method. Text embeddings were generated for each vaccine’s AE description using the LLM nomic-embed-text and mxbai-embed-large models. Compared to the semantic similarity analysis, the LLM approach was able to capture more differential AE profiles. Furthermore, LLM-derived text embeddings were used to develop a Lasso logistic regression model to predict whether a vaccine is “Live” or “Non-Live”. The term “Non-Live” refers to all vaccines that do not contain live organisms, including inactivated and mRNA vaccines. A comparative analysis showed that, despite similar clustering patterns, the nomic-embed-text model outperformed the other. It achieved 80.00% sensitivity, 83.06% specificity, and 81.89% accuracy in a 10-fold cross-validation. Many AE patterns, with examples demonstrated, were identified from our analysis with AE LLM embeddings. Conclusion: This study demonstrates the effectiveness of LLMs for automated AE extraction and analysis, and LLM text embeddings capture latent information about AEs, enabling more comprehensive knowledge discovery. Our findings suggest that LLMs demonstrate substantial potential for improving vaccine safety and public health research. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adverse Event; Fda Package Inserts; Human Phenotype Ontology (hpo); Large Language Models (llm); Liama-3 Model; Ontology Of Adverse Events (oae); Vaccine; Vaccine Ontology (vo); Vaccines; Vaccine; Adverse Drug Reaction; Biological Ontology; Human; Natural Language Processing; Semantics; Biological Ontologies; Drug-related Side Effects And Adverse Reactions; Humans; Natural Language Processing; Semantics; Vaccines},
	keywords = {vaccine; adverse drug reaction; biological ontology; human; natural language processing; semantics; Biological Ontologies; Drug-Related Side Effects and Adverse Reactions; Humans; Natural Language Processing; Semantics; Vaccines},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hein2025,
	author = {Hein, David M. and Christie, Alana L. and Holcomb, Michael J. and Xie, Bingqing and Jain, A. J. and Vento, Joseph A. and Rakheja, Neil and Shakur, Ameer Hamza and Christley, Scott and Cowell, Lindsay Grey},
	title = {Iterative refinement and goal articulation to optimize large language models for clinical information extraction},
	year = {2025},
	journal = {npj Digital Medicine},
	volume = {8},
	number = {1},
	pages = {},
	doi = {10.1038/s41746-025-01686-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005656975&doi=10.1038%2Fs41746-025-01686-z&partnerID=40&md5=9b544c8fe0d89cf73c9c8359ab350192},
	abstract = {Extracting structured data from free-text medical records at scale is laborious, and traditional approaches struggle in complex clinical domains. We present a novel, end-to-end pipeline leveraging large language models (LLMs) for highly accurate information extraction and normalization from unstructured pathology reports, focusing initially on kidney tumors. Our innovation combines flexible prompt templates, the direct production of analysis-ready tabular data, and a rigorous, human-in-the-loop iterative refinement process guided by a comprehensive error ontology. Applying the finalized pipeline to 2297 kidney tumor reports with pre-existing templated data available for validation yielded a macro-averaged F1 of 0.99 for six kidney tumor subtypes and 0.97 for detecting kidney metastasis. We further demonstrate flexibility with multiple LLM backbones and adaptability to new domains, utilizing publicly available breast and prostate cancer reports. Beyond performance metrics or pipeline specifics, we emphasize the critical importance of task definition, interdisciplinary collaboration, and complexity management in LLM-based clinical workflows. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Online Searching; Recommender Systems; Clinical Information; End To End; Free Texts; Highly Accurate; Iterative Refinement; Language Model; Medical Record; Structured Data; Traditional Approaches; Traditional Approachs; Image Retrieval; Article; Chromophobe Renal Cell Carcinoma; Clear Cell Papillary Renal Cell Carcinoma; Clear Cell Renal Cell Carcinoma; Controlled Study; Data Extraction; Female; Gold Standard; Histopathology; Human; Human Tissue; Immunohistochemistry; Invasive Breast Cancer; Kidney Metastasis; Large Language Model; Male; Nomenclature; Papillary Renal Cell Carcinoma; Prostate Adenocarcinoma; Validation Process; Workflow},
	keywords = {Online searching; Recommender systems; Clinical information; End to end; Free texts; Highly accurate; Iterative refinement; Language model; Medical record; Structured data; Traditional approaches; Traditional approachs; Image retrieval; Article; chromophobe renal cell carcinoma; clear cell papillary renal cell carcinoma; clear cell renal cell carcinoma; controlled study; data extraction; female; gold standard; histopathology; human; human tissue; immunohistochemistry; invasive breast cancer; kidney metastasis; large language model; male; nomenclature; papillary renal cell carcinoma; prostate adenocarcinoma; validation process; workflow},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Kafkas2025,
	author = {Kafkas, Şenay and Abdelhakim, Marwa and Althagafi, Azza and Toonsi, Sumyyah and Alghamdi, Malak Ali and Schofield, Paul N. and Hoehndorf, Robert},
	title = {The application of Large Language Models to the phenotype-based prioritization of causative genes in rare disease patients},
	year = {2025},
	journal = {Scientific Reports},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.1038/s41598-025-99539-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003831962&doi=10.1038%2Fs41598-025-99539-y&partnerID=40&md5=3c972fe815f8824632d3f2d3e0c5b4fc},
	abstract = {Computational methods for identifying gene–disease associations can use both genomic and phenotypic information to prioritize genes and variants that may be associated with genetic diseases. Phenotype-based methods commonly rely on comparing phenotypes observed in a patient with databases of genotype-to-phenotype associations using measures of semantic similarity. They are constrained by the quality and completeness of these resources as well as the quality and completeness of patient phenotype annotation. Genotype-to-phenotype associations used by these methods are largely derived from the literature and coded using phenotype ontologies. Large Language Models (LLMs) have been trained on large amounts of text and data and have shown their potential to answer complex questions across multiple domains. Here, we evaluate the effectiveness of LLMs in prioritizing disease-associated genes compared to existing bioinformatics methods. We show that LLMs can prioritize disease-associated genes as well, or better than, dedicated bioinformatics methods relying on pre-defined phenotype similarity, when gene sets range from 5 to 100 candidates. We apply our approach to a cohort of undiagnosed patients with rare diseases and show that LLMs can be used to provide diagnostic support that helps in identifying plausible candidate genes. Our results show that LLMs may offer an alternative to traditional bioinformatics methods to prioritize disease-associated genes based on disease phenotypes. They may, therefore, potentially enhance diagnostic accuracy and simplify the process for rare genetic diseases. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Diagnosis Support; Gene Prioritization; Large Language Models; Phenotypes; Rare Diseases; Bioinformatics; Diagnosis; Genetic Association Study; Genetic Predisposition; Genetics; Human; Large Language Model; Phenotype; Procedures; Rare Disease; Computational Biology; Genetic Association Studies; Genetic Predisposition To Disease; Humans; Large Language Models; Phenotype; Rare Diseases},
	keywords = {bioinformatics; diagnosis; genetic association study; genetic predisposition; genetics; human; large language model; phenotype; procedures; rare disease; Computational Biology; Genetic Association Studies; Genetic Predisposition to Disease; Humans; Large Language Models; Phenotype; Rare Diseases},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yazdani2025,
	author = {Yazdani, Anthony and Bornet, Alban and Khlebnikov, Philipp and Zhang, Boya and Rouhizadeh, Hossein and Amini, Poorya and Teodoro, Douglas H.},
	title = {An Evaluation Benchmark for Adverse Drug Event Prediction from Clinical Trial Results},
	year = {2025},
	journal = {Scientific Data},
	volume = {12},
	number = {1},
	pages = {},
	doi = {10.1038/s41597-025-04718-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000173742&doi=10.1038%2Fs41597-025-04718-1&partnerID=40&md5=5bc530ce73218d4d93e3985145687dc0},
	abstract = {Adverse drug events (ADEs) are a major safety issue in clinical trials. Thus, predicting ADEs is key to developing safer medications and enhancing patient outcomes. To support this effort, we introduce CT-ADE, a dataset for multilabel ADE prediction in monopharmacy treatments. CT-ADE encompasses 2,497 drugs and 168,984 drug-ADE pairs from clinical trial results, annotated using the MedDRA ontology. Unlike existing resources, CT-ADE integrates treatment and target population data, enabling comparative analyses under varying conditions, such as dosage, administration route, and demographics. In addition, CT-ADE systematically collects all ADEs in the study population, including positive and negative cases. To provide a baseline for ADE prediction performance using the CT-ADE dataset, we conducted analyses using large language models (LLMs). The best LLM achieved an F1-score of 56%, with models incorporating treatment and patient information outperforming by 21%–38% those relying solely on the chemical structure. These findings underscore the importance of contextual information in ADE prediction and establish CT-ADE as a robust resource for safety risk assessment in pharmaceutical research and development. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adverse Drug Reaction; Benchmarking; Clinical Trial (topic); Human; Benchmarking; Clinical Trials As Topic; Drug-related Side Effects And Adverse Reactions; Humans},
	keywords = {adverse drug reaction; benchmarking; clinical trial (topic); human; Benchmarking; Clinical Trials as Topic; Drug-Related Side Effects and Adverse Reactions; Humans},
	type = {Data paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Alkarmouty2025,
	author = {Alkarmouty, Mariam and Ooka, Junya and Yamashita, Fumiyoshi},
	title = {Harnessing large language models for structured extraction of cytochrome P450–substance interactions from biomedical texts},
	year = {2025},
	journal = {International Journal of Pharmaceutics},
	volume = {684},
	pages = {},
	doi = {10.1016/j.ijpharm.2025.126111},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014640213&doi=10.1016%2Fj.ijpharm.2025.126111&partnerID=40&md5=0d76cc5503c64f07a446f85b01d6954d},
	abstract = {Building on our previous work in biomedical text mining, we revisit the extraction of cytochrome P450 (CYP) and substance interactions using recent advances in large language models (LLMs). We present a scalable, high-accuracy framework that leverages the ChatGPT O3-mini model, employing prompt engineering with structured output formatting, embedded definitions, and selected few-shot examples, combined with batch processing without relying on dictionaries or domain-specific ontologies. Our system achieves strong performance, with recall and precision of 0.963 and 0.987 across all CYP targets, and 0.923 and 0.993 for CYP3A4 specifically. This represents a substantial improvement over our earlier rule-based method. The resulting large-scale analysis not only reflects existing knowledge but also enables a more systematic and comprehensive integration of CYP isoform–substance interaction data, addressing the limitations of previous fragmented efforts. While previous studies have attempted to catalog these interactions, the scale, precision, and automation demonstrated here represent a significant step forward. These findings underscore the potential of LLM-driven pipelines to accelerate biomedical text mining and to support research in drug metabolism and related fields. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Cytochrome P450; Information Extraction; Large Language Models; Text Mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lee2025,
	author = {Lee, Jeyoon and Li, Jiteng and Yoon, Sungmin},
	title = {From design to operation: Multi-agent AI for virtual in-situ modeling of digital twins in BIM},
	year = {2025},
	journal = {Automation in Construction},
	volume = {179},
	pages = {},
	doi = {10.1016/j.autcon.2025.106477},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013801586&doi=10.1016%2Fj.autcon.2025.106477&partnerID=40&md5=c1934f55e28a626a49a229325ad99756},
	abstract = {A virtual building model (VBM) is a mathematical representation that describes the behavior of a physical building. Accurate VBMs provide insightful information about the physical building within the digital twin (DT) framework. However, there is limited research on the autonomous construction of VBMs. To address this gap, this paper proposes a multi-agent artificial intelligence (AI) system that autonomously constructs VBM. The proposed system autonomously develops, calibrates, and manages virtual models that constitute the VBM by leveraging data and information generated throughout the building lifecycle within a BIM environment. The proposed method is validated on a real operating heating, ventilation, and air-conditioning (HVAC) system, autonomously developing a chilled water flow rate model (MAPE 2.27 %) and an evaporator inlet temperature model (RMSE 0.33°C). These results suggest the feasibility of autonomously constructing VBMs and contribute to shifting the DT paradigm from physical construction automation to the autonomous construction of VBM. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Agent; Building Information Modeling; Digital Twin; Large Language Model; Ontology; Virtual In-situ Modeling; Air Conditioning; Architectural Design; Autonomous Agents; Building Information Model; Construction; Digital Twin; Intelligent Agents; Intelligent Virtual Agents; Life Cycle; Modeling Languages; Multi Agent Systems; Artificial Intelligence Agent; Building Information Modelling; Building Model; Language Model; Large Language Model; Multi Agent; Ontology's; Situ Models; Virtual Building; Virtual In-situ Modeling; Ontology},
	keywords = {Air conditioning; Architectural design; Autonomous agents; Building Information Model; Construction; Digital twin; Intelligent agents; Intelligent virtual agents; Life cycle; Modeling languages; Multi agent systems; Artificial intelligence agent; Building Information Modelling; Building model; Language model; Large language model; Multi agent; Ontology's; Situ models; Virtual building; Virtual in-situ modeling; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Shan2025,
	author = {Shan, Tianlong and Zhang, Fan and Chan, Albert P.C. and Zhu, Shiyao and Li, Kaijian},
	title = {Large language Models-empowered automatic knowledge graph development based on multi-modal data for building health resilience},
	year = {2025},
	journal = {Advanced Engineering Informatics},
	volume = {68},
	pages = {},
	doi = {10.1016/j.aei.2025.103655},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010563058&doi=10.1016%2Fj.aei.2025.103655&partnerID=40&md5=329bb060f38e387aae8af0f77b054252},
	abstract = {Improving the health resilience of building (BHR) helps keep stable health status of both the building and its occupants under disasters. As BHR is an emerging concept, there is no structured knowledge graph to understand the whole process of BHR under disasters. Therefore, this study aims to build a structured BHR knowledge graph based on multi-modal data, providing sufficient structured knowledge for BHR enhancement. An automated knowledge graph construction approach is proposed to empower the ontology design and triple extraction by large language models (LLMs), and validation processes based on In-context Learning (ICL) prompts. A case study is conducted to construct the knowledge graph of BHR under rainstorms in Hong Kong. The performance of the proposed LLMs-empowered knowledge extraction is also validated based on natural language processing metrics and LLMs-based Evaluation (LLMs-Eval). BHR knowledge graph indicates the potential relations between disasters, factors, response actions, and the health status of the building and occupants, and provides insight to guide the BHR enhancement. The superiority of the proposed LLMs-empowered automated knowledge graph construction approach is proven, implying LLMs have great potential in knowledge graph construction, not only for BHR but also for other concepts that require structured knowledge for further explorations and analyses. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Building Health Resilience; Knowledge Graph; Large Language Models; Multi-modal Data; Rainstorm; Graph Theory; Graphic Methods; Knowledge Graph; Modal Analysis; Natural Language Processing Systems; Ontology; Building Health Resilience; Construction Approaches; Graph Construction; Health Status; Knowledge Graphs; Language Model; Large Language Model; Multi-modal Data; Rainstorm; Structured Knowledge; Extraction},
	keywords = {Graph theory; Graphic methods; Knowledge graph; Modal analysis; Natural language processing systems; Ontology; Building health resilience; Construction approaches; Graph construction; Health status; Knowledge graphs; Language model; Large language model; Multi-modal data; Rainstorm; Structured knowledge; Extraction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Manns2025,
	author = {Manns, Aurélia and Millet, Alix and Campeotto, Florence and Vivien, Benoît and Dupic, Laurent and Guillard, Olivier and Hage, Christelle El and Castéla, Florence and Fogel, Stephanie and Burgun, Anita},
	title = {Towards a decision support system for pediatric emergency telephone triage},
	year = {2025},
	journal = {International Journal of Medical Informatics},
	volume = {203},
	pages = {},
	doi = {10.1016/j.ijmedinf.2025.106012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008505984&doi=10.1016%2Fj.ijmedinf.2025.106012&partnerID=40&md5=5fbdfbf91a42ee9cfe0d4adf1e8716a6},
	abstract = {Background: Telephone triage could limit admissions to emergency departments. However, telephone triage is challenging in pediatrics due to nonspecific symptoms, reliance on parental description, and emotional distress. Clinical decision support systems (CDSSs) could improve the accuracy and quality of telephone triage. Despite proven benefits, current CDSSs are not well suited to the nuances of pediatrics. This study aims to develop a CDSS for pediatric emergency telephone triage. Methods: We developed a formal knowledge base (KB) for pediatric telephone triage inspired by the ontology model and implemented a generic medical reasoning system that mimics the clinical reasoning used in pediatric emergency triage. The CDSS is built in three layers (a knowledge layer, a Python-based decision layer, and a web interface layer) and provides real-time recommendations. We assessed its accuracy on 96 fictitious clinical cases. Results: The CDSS uses an ontology-oriented KB that includes 303 concepts and 1780 axioms and a generic algorithm that provides recommendations based on user input, exploring and updating decisions continuously. It demonstrated 100 % internal validity compared to written recommendations and 77.1 % accuracy compared to a trio of experts. The 22.9 % discrepancies were due to experts using additional elements not documented in the written recommendations (11.5 %) or experts making different decisions despite consistent rules in the textual recommendations (10.4 %), emphasizing the challenges of standardized guidelines in this narrow but complex field. Discussion/conclusion: The CDSS provides explainable and interpretable recommendations designed to alleviate healthcare professionals’ cognitive load so that they can focus on complex clinical situations. Future improvements involve enriching the KB, enhancing user interaction with patient-friendly language, and combining this knowledge-based approach with data-driven approaches. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Decision-support System; Pediatric Emergency; Telephone Triage; Knowledge Based Systems; Ontology; User Interfaces; 'current; Clinical Decision Support Systems; Decision Supports; Emergency Departments; Emergency Telephones; Formal Knowledge; Ontology Model; Pediatric Emergency; Support Systems; Telephone Triage; Decision Support Systems; Pediatrics; Article; Clinical Reasoning; Controlled Study; Decision Support System; Evidence Based Practice; Exploratory Behavior; Human; Icd-10; Intensive Care Unit; Internal Validity; Knowledge Base; Language Model; Measurement Accuracy; Patient Triage; Personal Experience; Qualitative Analysis; Algorithm; Child; Clinical Decision Support System; Pediatrics; Procedures; Telephone; Algorithms; Child; Decision Support Systems, Clinical; Humans; Knowledge Bases; Telephone; Triage},
	keywords = {Knowledge based systems; Ontology; User interfaces; 'current; Clinical decision support systems; Decision supports; Emergency departments; Emergency telephones; Formal knowledge; Ontology model; Pediatric emergency; Support systems; Telephone triage; Decision support systems; Pediatrics; Article; clinical reasoning; controlled study; decision support system; evidence based practice; exploratory behavior; human; ICD-10; intensive care unit; internal validity; knowledge base; language model; measurement accuracy; patient triage; personal experience; qualitative analysis; algorithm; child; clinical decision support system; pediatrics; procedures; telephone; Algorithms; Child; Decision Support Systems, Clinical; Humans; Knowledge Bases; Telephone; Triage},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Taboada2025,
	author = {Taboada, Maria Jesús Salvador and Martínez, Diego and Arideh, Mohammed and Mosquera, Rosa},
	title = {Ontology matching with Large Language Models and prioritized depth-first search},
	year = {2025},
	journal = {Information Fusion},
	volume = {123},
	pages = {},
	doi = {10.1016/j.inffus.2025.103254},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004876116&doi=10.1016%2Fj.inffus.2025.103254&partnerID=40&md5=60448dc85d882e6b0594bd7acd43f80a},
	abstract = {Ontology matching (OM) plays a key role in enabling data interoperability and knowledge sharing. Recently, methods based on Large Language Model (LLMs) have shown great promise in OM, particularly through the use of a retrieve-then-prompt pipeline. In this approach, relevant target entities are first retrieved and then used to prompt the LLM to predict the final matches. Despite their potential, these systems still present limited performance and high computational overhead. To address these issues, we introduce MILA, a novel approach that embeds a retrieve-identify-prompt pipeline within a prioritized depth-first search (PDFS) strategy. This approach efficiently identifies a large number of semantic correspondences with high accuracy, limiting LLM requests to only the most borderline cases. We evaluated MILA using three challenges from the 2024 edition of the Ontology Alignment Evaluation Initiative. Our method achieved the highest F-Measure in five of seven unsupervised tasks, outperforming state-of-the-art OM systems by up to 17%. It also performed better than or comparable to the leading supervised OM systems. MILA further exhibited task-agnostic performance, remaining stable across all tasks and settings, while significantly reducing runtime. These findings highlight that high-performance LLM-based OM can be achieved through a combination of programmed (PDFS), learned (embedding vectors), and prompting-based heuristics, without the need of domain-specific heuristics or fine-tuning. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Greedy Search; Large Language Models; Ontology Matching; Retrieval Augmented Generation; Zero-shot Setting; Data Interoperability; Depth First; Greedy Search; Language Model; Large Language Model; Matching System; Ontology Matching; Performance; Retrieval Augmented Generation; Zero-shot Setting; Interoperability},
	keywords = {Data interoperability; Depth first; Greedy search; Language model; Large language model; Matching system; Ontology matching; Performance; Retrieval augmented generation; Zero-shot setting; Interoperability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Zhang2025,
	author = {Zhang, Zitong and Zou, Quan and Wang, Chunyu and Wang, Junjie and Zhao, Lingling},
	title = {Improving protein–protein interaction modulator predictions via knowledge-fused language models},
	year = {2025},
	journal = {Information Fusion},
	volume = {123},
	pages = {},
	doi = {10.1016/j.inffus.2025.103227},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004221272&doi=10.1016%2Fj.inffus.2025.103227&partnerID=40&md5=18e1f6a35a462186cf280af1fddfd86f},
	abstract = {Protein-protein interactions (PPIs) play key roles in numerous biological processes and their dysregulation can lead to various human diseases. Modulating these interactions with small molecule PPI modulators has emerged as a promising strategy for treating such diseases. However, current computational approaches for screening PPI modulators often fail to integrate biomolecular expertise and lack the elucidation of interaction mechanisms. Here, we propose a knowledge-fused modulator-PPI interaction prediction method (KFPPIMI) to alleviate these issues. KFPPIMI constructs separate representation models for modulators and proteins, each of which integrates external knowledge from textual and graph-based data sources via a language modeling framework. The fusion of the nuanced expression of natural language with the structural attributes of biomolecules provides KFPPIMI with a holistic view of modulator-PPI interactions. Extensive experiments are conducted to evaluate the effectiveness of KFPPIMI and its individual components. The results show that KFPPIMI outperforms existing methods in different scenarios. Moreover, the modulator and protein representation model can be successfully applied to their respective downstream tasks with comparable performance. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontology; Knowledge Fusion; Language Model; Pretrain; Protein-protein Interaction Modulator; Gene Transfer; Biological Process; Gene Ontology; Human Disease; Knowledge Fusion; Language Model; Pretrain; Protein-protein Interaction Modulator; Protein-protein Interactions; Representation Model; Small Molecules; Genome},
	keywords = {Gene transfer; Biological process; Gene ontology; Human disease; Knowledge fusion; Language model; Pretrain; Protein-protein interaction modulator; Protein-protein interactions; Representation model; Small molecules; Genome},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Gonçalves Lopes Junior2025,
	author = {null, null and Gonçalves Lopes Junior, Alcides and Fabrício Henrique Rodrigues, null and Carbonera, Joel Lúis},
	title = {Using natural language definitions and language models for relationship classification},
	year = {2025},
	journal = {Knowledge-Based Systems},
	volume = {327},
	pages = {},
	doi = {10.1016/j.knosys.2025.114154},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012097374&doi=10.1016%2Fj.knosys.2025.114154&partnerID=40&md5=9c1eb3850223c70caaa10eca2f95f2ff},
	abstract = {Identifying relationships between concepts is a very important task for several NLP tasks, as well as for building explicit knowledge models (ontologies and knowledge graphs). In many of these tasks, experts usually manually establish these relationships by carefully analyzing each concept's meaning and considering the domain knowledge elicited from domain practitioners or from domain literature. While some studies automate parts of the process of building knowledge models, most focus on identifying general concepts or rely on static word embeddings, which fail to address challenges like polysemy and contextual ambiguity. This research addresses the problem of classifying semantic relationships between concepts, focusing on hypernym and holonym relations. We propose an approach based on the pre-trained language model BERT to classify these relationships between concepts. We assume that we can represent the concept's semantics using their definitions in natural language. To evaluate this approach, we developed a methodology to construct a labeled dataset of definitions of concepts using WordNet as a reference. Thus, our proposed approach classifies the relations based solely on natural language expressions representing the concept's definition. Our experiments showed notable classification results, achieving an F1 score of 96 % in the classification of holonyms, hypernyms, and concepts that are not related by any of these relations, indicating that our approach can accurately predict semantic relations between concepts using only their natural language definitions as input. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Lexical Classification; Machine Learning; Nlp; Relation Classification; Computational Linguistics; Domain Knowledge; Knowledge Acquisition; Knowledge Graph; Labeled Data; Learning Systems; Natural Language Processing Systems; Bert; Explicit Knowledge; Hypernyms; Knowledge Model; Language Model; Lexical Classification; Machine-learning; Natural Languages; Relation Classifications; Relationship Between Concepts; Semantics},
	keywords = {Computational linguistics; Domain Knowledge; Knowledge acquisition; Knowledge graph; Labeled data; Learning systems; Natural language processing systems; BERT; Explicit knowledge; Hypernyms; Knowledge model; Language model; Lexical classification; Machine-learning; Natural languages; Relation classifications; Relationship between concepts; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Çelikten2025,
	author = {Çelikten, Tuğba and Onan, Aytuǧ},
	title = {Medcongtm: Interpretable multi-label clinical code prediction with dual-view graph contrastive topic modeling},
	year = {2025},
	journal = {Knowledge-Based Systems},
	volume = {327},
	pages = {},
	doi = {10.1016/j.knosys.2025.114103},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011099983&doi=10.1016%2Fj.knosys.2025.114103&partnerID=40&md5=4b2e9e50f49d15d48a3decbc3f49e546},
	abstract = {BackgroundAccurate and interpretable clinical code assignment from free-text medical records is a fundamental challenge in healthcare informatics. Traditional machine learning and language model-based methods often lack transparency and struggle with multi-label prediction across complex taxonomies such as ICD, CPT, and LOINC. Existing topic modeling techniques, while interpretable, are rarely optimized for the clinical coding task and fail to leverage the rich semantic structure inherent in medical texts and ontologies. MethodsWe propose MedConGTM, a novel dual-view graph contrastive topic modeling framework tailored for interpretable and multi-label clinical code prediction. MedConGTM constructs two semantic views of each document: a document-token semantic graph and a document-code co-assignment graph. These views are jointly optimized through a novel dual-view contrastive learning objective that maximizes the mutual information between topic distributions inferred from text and task-specific code views. We introduce a code-aware word co-occurrence graph enhanced with medical ontologies and propose a hierarchy-sensitive contrastive loss that incorporates structural relationships between clinical codes. To ensure transparency, we design a topic-to-code attention decoder that links predicted codes to interpretable latent topics and salient textual evidence. ResultsExperiments on MIMIC-III and i2b2 datasets demonstrate that MedConGTM outperforms state-of-the-art baselines in both code prediction accuracy and topic coherence. It also provides interpretable code rationales aligned with clinical semantics. ConclusionsMedConGTM offers a powerful, interpretable, and clinically grounded solution for automated ICD/CPT/LOINC code assignment, bridging the gap between topic modeling, contrastive learning, and real-world healthcare applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Code Prediction; Contrastive Learning; Multi-label Classification; Topic Modeling; Classification (of Information); Health Care; Latent Semantic Analysis; Learning Systems; Medical Computing; Medical Informatics; Ontology; Semantics; Text Processing; Clinical Code Prediction; Code Assignments; Code Predictions; Free Texts; Health Care Informatics; Machine Languages; Medical Record; Multi-label Classifications; Multi-labels; Topic Modeling; Forecasting},
	keywords = {Classification (of information); Health care; Latent semantic analysis; Learning systems; Medical computing; Medical informatics; Ontology; Semantics; Text processing; Clinical code prediction; Code assignments; Code predictions; Free texts; Health care informatics; Machine languages; Medical record; Multi-label classifications; Multi-labels; Topic Modeling; Forecasting},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{de Barros Vanzin2025,
	author = {de Barros Vanzin, Vinícius João and Moreira, Dilvan De Abreu and Marcondes Marcacini, Ricardo},
	title = {LLM-based approaches for automated vocabulary mapping between SIGTAP and OMOP CDM concepts},
	year = {2025},
	journal = {Artificial Intelligence in Medicine},
	volume = {168},
	pages = {},
	doi = {10.1016/j.artmed.2025.103204},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011174963&doi=10.1016%2Fj.artmed.2025.103204&partnerID=40&md5=1b775df6ed0d57e4182e5af0d7b58848},
	abstract = {In the context of global healthcare systems, integrating diverse medical terminologies and classification systems has become a priority due to the adoption of Electronic Health Record (EHR) systems and the imperative for information exchange between healthcare systems. This study addresses the necessity for mapping between the SIGTAP vocabulary used in Brazilian healthcare systems and the broader medical terms of the OMOP CDM terminologies. Two distinct pipelines are evaluated for the vocabulary mapping process, focusing on two subsets of the SIGTAP vocabulary: medicines and medical procedures. The first pipeline utilizes textual embeddings for semantic similarity evaluation, followed by Large Language Models (LLMs) for correspondences selection through a retrieval-augmented generation (RAG) approach. In the second pipeline, LLM agents employ predefined protocols for vocabulary mapping and query refinement. Our results show comparable performance between pipelines in both the Procedures subset (F<inf>1</inf> of 0.684 versus 0.678), and the Medicines subset (F<inf>1</inf> of 0.846 versus 0.839), indicating the viability of the multi-stage filtering approach. The second pipeline demonstrates an advantage over the first in terms of recall, highlighting the efficacy of dynamic query refinement by the agent. These findings provide evidence that LLM-based methods significantly reduce manual effort required by experts, enabling domain specialists to focus on more challenging cases. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Llm Agent; Ontology Mapping; Retrieval-augmented Generation; Classification (of Information); Electronic Document Exchange; Electronic Health Record; Health Care; Mapping; Medical Computing; Medical Information Systems; Medicine; Ontology; Pipelines; Query Processing; Terminology; Thesauri; Vocabulary Control; Healthcare Systems; Language Model; Large Language Model Agent; Medical Classification; Medical Terminologies; Model Agents; Model Based Approach; Ontology Mapping; Query Refinement; Retrieval-augmented Generation; Semantics; Article; Electronic Health Record; Female; Filtration; Health Care System; Large Language Model; Male; Medical Specialist; Medical Terminology; Ontology; Protocol; Retrieval Augmented Generation; Vocabulary; Brazil; Controlled Vocabulary; Human; Natural Language Processing; Semantics; Electronic Health Records; Humans; Natural Language Processing; Vocabulary, Controlled},
	keywords = {Classification (of information); Electronic document exchange; Electronic health record; Health care; Mapping; Medical computing; Medical information systems; Medicine; Ontology; Pipelines; Query processing; Terminology; Thesauri; Vocabulary control; Healthcare systems; Language model; Large language model agent; Medical classification; Medical terminologies; Model agents; Model based approach; Ontology mapping; Query refinement; Retrieval-augmented generation; Semantics; article; electronic health record; female; filtration; health care system; large language model; male; medical specialist; medical terminology; ontology; protocol; retrieval augmented generation; vocabulary; Brazil; controlled vocabulary; human; natural language processing; semantics; Electronic Health Records; Humans; Natural Language Processing; Vocabulary, Controlled},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025,
	author = {Li, Xiaobo and Zhang, Yijia and Hou, Xiaodi and Wang, Shilong and Lin, Hongfei},
	title = {Deep learning for automatic ICD coding: Review, opportunities and challenges},
	year = {2025},
	journal = {Artificial Intelligence in Medicine},
	volume = {168},
	pages = {},
	doi = {10.1016/j.artmed.2025.103187},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010429861&doi=10.1016%2Fj.artmed.2025.103187&partnerID=40&md5=2673060b83729319d8230a34a8c8fbe2},
	abstract = {Background: The automatic International Classification of Diseases (ICD) coding task assigns unique medical codes to diseases in clinical texts for further data statistics, quality control, billing and other tasks. The efficiency and accuracy of medical code assignment is a significant challenge affecting healthcare. However, in clinical practice, Electronic Health Records (EHRs) data are usually complex, heterogeneous, non-standard and unstructured, and the manual coding process is time-consuming, laborious and error-prone. Traditional machine learning methods struggle to extract significant semantic information from clinical texts accurately, but the latest progress in Deep Learning (DL) has shown promising results to address these issues. Objective: This paper comprehensively reviewed recent advancements in utilizing deep learning for automatic ICD coding, which aimed to reveal prominent challenges and emerging development trends by summarizing and analyzing the model's year, design motivation, deep neural networks, and auxiliary data. Methods: This review introduced systematic literature on automatic ICD coding methods based on deep learning. We screened 5 online databases, including Web of Science, SpringerLink, PubMed, ACM, and IEEE digital library, and collected 53 published articles related to deep learning-based ICD coding from 2017 to 2023. Results: These deep neural network methods aimed to overcome some challenges, such as lengthy and noisy clinical text, high dimensionality and functional relationships of medical codes, and long-tail label distribution. The Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), attention mechanisms, Transformers, Pre-trained Language Models (PLMs), etc, have become popular to address prominent issues in ICD coding. Meanwhile, introducing medical ontology within the ICD coding system (code description and code hierarchy) and external knowledge (Wikipedia articles, tabular data, Clinical Classification Software (CCS), fine-tuning PLMs based on biomedical corpus, entity recognition and concept extraction) has become an emerging trend for automatic ICD coding. Conclusion: This paper provided a comprehensive review of recent literature on applying deep learning technology to improve medical code assignment from a unique perspective. Multiple neural network methods (CNNs, RNNs, Transformers, PLMs, especially attention mechanisms) have been successfully applied in ICD tasks and achieved excellent performance. Various medical auxiliary data has also proven valuable in enhancing model feature representation and classification performance. Our in-depth and systematic analysis suggested that the automatic ICD coding method based on deep learning has a bright future in healthcare. Finally, we discussed some major challenges and outlined future development directions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Automatic Icd Coding; Deep Learning; Electronic Health Records; Medical Code Assignment; Classification (of Information); Codes (symbols); Convolutional Neural Networks; Deep Neural Networks; Electronic Health Record; Learning Systems; Medical Computing; Network Coding; Ontology; Text Processing; Automatic International Classification Of Disease Coding; Auxiliary Data; Code Assignments; Deep Learning; Electronic Health; Health Records; International Classification Of Disease; Language Model; Medical Code Assignment; Neural-networks; Digital Libraries; Clinical Classification; Clinical Practice; Cohort Analysis; Convolutional Neural Network; Deep Learning; Deep Neural Network; Diagnosis; Electronic Health Record; Health Auxiliary; Human; International Classification Of Diseases; Language Model; Machine Learning; Quality Control; Recurrent Neural Network; Review; Springerlink; Web Of Science; Artificial Neural Network; Coding; Procedures; Clinical Coding; Deep Learning; Electronic Health Records; Humans; Neural Networks, Computer},
	keywords = {Classification (of information); Codes (symbols); Convolutional neural networks; Deep neural networks; Electronic health record; Learning systems; Medical computing; Network coding; Ontology; Text processing; Automatic international classification of disease coding; Auxiliary data; Code assignments; Deep learning; Electronic health; Health records; International classification of disease; Language model; Medical code assignment; Neural-networks; Digital libraries; clinical classification; clinical practice; cohort analysis; convolutional neural network; deep learning; deep neural network; diagnosis; electronic health record; health auxiliary; human; International Classification of Diseases; language model; machine learning; quality control; recurrent neural network; review; SpringerLink; Web of Science; artificial neural network; coding; procedures; Clinical Coding; Deep Learning; Electronic Health Records; Humans; Neural Networks, Computer},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Oyekan2025,
	author = {Oyekan, John Oluwagbemiga and Turner, Christopher J. and Bax, Michael and Graf, Erich W.},
	title = {From Ontologies to Knowledge Augmented Large Language Models for Automation: A decision-making guidance for achieving human–robot collaboration in Industry 5.0},
	year = {2025},
	journal = {Computers in Industry},
	volume = {171},
	pages = {},
	doi = {10.1016/j.compind.2025.104329},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010198971&doi=10.1016%2Fj.compind.2025.104329&partnerID=40&md5=cab8af912a8d7594d8c450df211aba6c},
	abstract = {The rapid advancement of Large Language Models (LLMs) has resulted in interest in their potential applications within manufacturing systems, particularly in the context of Industry 5.0. However, determining when to implement LLMs versus other Natural Language Processing (NLP) techniques, ontologies or knowledge graphs, remains an open question. This paper offers decision-making guidance for selecting the most suitable technique in various industrial contexts, emphasizing human–robot collaboration and resilience in manufacturing. We examine the origins and unique strengths of LLMs, ontologies, and knowledge graphs, assessing their effectiveness across different industrial scenarios based on the number of domains or disciplines required to bring a product from design to manufacture. Through this comparative framework, we explore specific use cases where LLMs could enhance robotics for human–robot collaboration, while underscoring the continued relevance of ontologies and knowledge graphs in low-dependency or resource-constrained sectors. Additionally, we address the practical challenges of deploying these technologies, such as computational cost and interpretability, providing a roadmap for manufacturers to navigate the evolving landscape of Language based AI tools in Industry 5.0. Our findings offer a foundation for informed decision-making, helping industry professionals optimize the use of Language Based models for sustainable, resilient, and human-centric manufacturing. We also propose a Large Knowledge Language Model architecture that offers the potential for transparency and configuration based on complexity of task and computing resources available. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Generative Pre-trained Transformers; Language Models; Manufacturing; Reasoning; Robotics; Behavioral Research; Decision Making; Design For Manufacturability; Graphic Methods; Knowledge Graph; Natural Language Processing Systems; Robots; Decisions Makings; Generative Pre-trained Transformer; Human-robot Collaboration; Knowledge Graphs; Language Model; Language Processing Techniques; Natural Languages; Ontology Graphs; Ontology's; Reasoning; Product Design},
	keywords = {Behavioral research; Decision making; Design for manufacturability; Graphic methods; Knowledge graph; Natural language processing systems; Robots; Decisions makings; Generative pre-trained transformer; Human-robot collaboration; Knowledge graphs; Language model; Language processing techniques; Natural languages; Ontology graphs; Ontology's; Reasoning; Product design},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Shi2025,
	author = {Shi, Dachuan and Li, Jianzhang and Meyer, Olga and Bauernhansl, Thomas},
	title = {Enhancing retrieval-augmented generation for interoperable industrial knowledge representation and inference toward cognitive digital twins},
	year = {2025},
	journal = {Computers in Industry},
	volume = {171},
	pages = {},
	doi = {10.1016/j.compind.2025.104330},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009051298&doi=10.1016%2Fj.compind.2025.104330&partnerID=40&md5=72eee7bc8baadf88a7dca027313faf6c},
	abstract = {The escalating volume and complexity of digital data within the manufacturing sector highlight an urgent need for an efficient knowledge representation and inference solution. Traditional approaches, which often rely on ontologies, knowledge graphs, or digital twins (DTs) for knowledge representation, and rule-based algorithms for inference, are becoming insufficient. The emergence of generative AI, particularly large language models (LLM) and retrieval-augmented generation (RAG), offers a more efficient and intelligent alternative. However, the performance of an RAG system is heavily dependent on the quality of retrieval results, which can be compromised by domain-specific knowledge and retrieval distractors. To address this challenge, we propose to enhance RAG systems tailored for the manufacturing industry in two aspects. First, we utilize the Asset Administration Shell (AAS), which represents the German industrial perspective on cognitive DTs, to create a representation of assets and knowledge in standardized information models. This establishes a robust foundation for the retrieval sources. Second, we propose a contrastive selection loss (CSL) to fine-tune an open-source LLM to refine the retrieval results. Fine-tuned LLMs possess higher efficiency and accuracy on task- and domain-specific datasets, while the CSL further enhances the model's ability to distinguish true positives from similar distractors. The enhanced RAG system is demonstrated in a robotic work cell integration use case and evaluated through a novel evaluation protocol. Additionally, the retrieval effectiveness of the RAG system, specifically the LLM fine-tuned with CSL, is extensively validated through statistical experiments. The results confirm its superior performance over state-of-the-art methods, including GPT-4 with in-context learning prompts and other fine-tuned models. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Asset Administration Shell; Digital Twin; Entity Matching; Large Language Model; Retrieval-augmented Generation; Digital Twin; Interoperability; Knowledge Representation; Robotics; Asset Administration Shell; Entity Matching; Generation Systems; Knowledge-representation; Language Model; Large Language Model; Manufacturing Sector; Performance; Retrieval-augmented Generation; Traditional Approachs; Inference Engines},
	keywords = {Digital twin; Interoperability; Knowledge representation; Robotics; Asset administration shell; Entity matching; Generation systems; Knowledge-representation; Language model; Large language model; Manufacturing sector; Performance; Retrieval-augmented generation; Traditional approachs; Inference engines},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2025,
	author = {Liu, Tony and Zanocco, Chad M. and Wang, Zhecheng and Huang, Tianyuan and Flora, June A. and Rajagopal, Ram S.},
	title = {Large language model enabled knowledge discovery of building-level electrification using permit data},
	year = {2025},
	journal = {Energy and Buildings},
	volume = {343},
	pages = {},
	doi = {10.1016/j.enbuild.2025.115890},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006876397&doi=10.1016%2Fj.enbuild.2025.115890&partnerID=40&md5=55641cdb377f6fc74df931c0aa1a20c3},
	abstract = {Wide scale electrification is essential for decarbonization of the building sector, yet there is a significant knowledge gap regarding the specific locations, timelines, and types of electrification technologies that are being deployed. To address this gap, we developed an information framework powered by large language models (LLMs) to extract detailed electrification-related technology information from building permit text data. While U.S. building permit data is publicly available, it is often unstructured, incomplete, and highly variable. Our LLM-enabled system addresses these challenges by constructing a comprehensive building-level ontology that captures detailed attributes for six key electrification technologies: photovoltaics, electric vehicle chargers, energy storage systems, electric service panels, water heaters, and heat pumps. Our information extraction system exhibits strong performance, achieving 0.96 recall and 0.88 precision on our human-annotated test dataset. We experimentally deploy our framework on permit data in San Francisco County, California, demonstrating that it surpasses all existing public sources of electrification information in both spatiotemporal resolution and coverage. Our work provides new visibility into building electrification trends at scale, offering valuable insights for grid planners, policymakers, installers, and end-users to inform decision-making processes. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Building Permits; Distributed Energy Resources; Electrification; Large Language Models; Spatiotemporal Mapping; Building Levels; Building Permits; Buildings Sector; Decarbonisation; Distributed Energy Resources; Knowledge Gaps; Language Model; Large Language Model; Spatiotemporal Mapping; Specific Location; Building Information Model},
	keywords = {Building levels; Building permits; Buildings sector; Decarbonisation; Distributed Energy Resources; Knowledge gaps; Language model; Large language model; Spatiotemporal mapping; Specific location; Building Information Model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Chandra2025,
	author = {Chandra, Ritesh and Kumar, Shashi Shekhar and Patra, Rushil and Agarwal, Sonali},
	title = {Decision support system for Forest fire management using Ontology with Big Data and LLMs},
	year = {2025},
	journal = {Cluster Computing},
	volume = {28},
	number = {8},
	pages = {},
	doi = {10.1007/s10586-025-05383-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013848357&doi=10.1007%2Fs10586-025-05383-0&partnerID=40&md5=cbef5489515633cf69e441c2ac079c41},
	abstract = {Forests are crucial for ecological balance, but wildfires are a major cause of forest loss, posing significant risks. Fire weather indices, which assess wildfire risk and predict resource demands, are vital. With the rise of sensor networks in fields like healthcare and environmental monitoring, semantic sensor networks are increasingly used to gather climatic data such as wind speed, temperature, and humidity. However, processing these data streams to determine fire weather indices presents challenges, underscoring the growing importance of effective forest fire detection. This paper discusses using Apache Spark for early forest fire detection, enhancing fire risk prediction with meteorological and geographical data. Building on our previous development of Semantic Sensor Network (SSN) ontologies and Semantic Web Rules Language (SWRL) for managing forest fires in Monesterial Natural Park, we expanded SWRL to improve a Decision Support System (DSS) using a Large Language Models (LLMs) and Spark framework. We implemented real-time alerts with Spark streaming, tailored to various fire scenarios, and validated our approach using ontology metrics, query-based evaluations, LLMs score precision, F1 score, and recall measures. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Dss; Forest Fire; Llms; Spark; Ssn; Big Data; Data Streams; Decision Making; Decision Support Systems; Deforestation; Fire Detectors; Fire Hazards; Fires; Information Management; Meteorology; Ontology; Semantic Web; Sensor Networks; Wind; Decision Supports; Fire Weather Index; Forest Fires; Language Model; Large Language Model; Ontology's; Semantic Sensor Network; Semantic Sensors; Sensors Network; Support Systems; Electric Sparks},
	keywords = {Big data; Data streams; Decision making; Decision support systems; Deforestation; Fire detectors; Fire hazards; Fires; Information management; Meteorology; Ontology; Semantic Web; Sensor networks; Wind; Decision supports; Fire weather index; Forest fires; Language model; Large language model; Ontology's; Semantic sensor network; Semantic sensors; Sensors network; Support systems; Electric sparks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hasan2025,
	author = {Hasan, Abul Kalam and Wu, Jinge and Nguyen, Quang Ngoc and Andres, Salomé and Guellil, Imane and Zhang, Huayu and Casey, Arlene J. and Alex, Beatrice and Guthrie, Bruce and Wu, Honghan},
	title = {Infusing clinical knowledge into language models by subword optimisation and embedding initialisation},
	year = {2025},
	journal = {Computers in Biology and Medicine},
	volume = {196},
	pages = {},
	doi = {10.1016/j.compbiomed.2025.110747},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012629577&doi=10.1016%2Fj.compbiomed.2025.110747&partnerID=40&md5=70b0e0419a2187b52dd8378be05b1d39},
	abstract = {Objective: This study introduces a novel tokenisation methodology, K-Tokeniser, to infuse clinical knowledge into language models for clinical text processing. Methods: Technically, at initialisation stage, K-Tokeniser populates global representations of tokens based on semantic types of domain concepts (such as drugs or diseases) from either a domain ontology like Unified Medical Language System or the training data of the task related corpus. At training or inference stage, sentence level localised context will be utilised for choosing the optimal global token representation to realise the semantic-based tokenisation. To avoid pretraining using the new tokeniser, an embedding initialisation approach is proposed to generate representations for new tokens. Results: Using three transformer-based language models, a comprehensive set of experiments are conducted on four real-world datasets for evaluating K-Tokeniser in a wide range of clinical text analytics tasks including clinical concept and relation extraction, automated clinical coding, clinical phenotype identification, and clinical research article classification. Overall, our models demonstrate consistent improvements over their counterparts in all tasks. In particular, substantial improvements are observed in the automated clinical coding task with 13% increase on Micro F<inf>1</inf> score. Furthermore, K-Tokeniser also shows significant capacities in facilitating quicker convergence of language models. Conclusion: Models built using K-Tokeniser have shown faster convergence. Specifically,the language models would only require 50% of the training data to achieve the best performance of the baseline tokeniser using all training data in the concept extraction task and less than 20% of the data for the automated coding task. It is worth mentioning that all these improvements require no pre-training process, making the approach generalisable. Code availability: Our full implementation is openly available at https://github.com/abulhasanbbk/K-Tokenizer. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Clinical Concept And Relation Extraction; Document Classification; Icd-9 Coding Classification; Language Model; Phenotype Identification; Tokenisation; Automation; Classification (of Information); Clinical Research; Codes (symbols); Computational Linguistics; Embeddings; Information Retrieval; Natural Language Processing Systems; Ontology; Semantics; Text Processing; Bert; Clinical Concept And Relation Extraction; Concept Extraction; Document Classification; Icd-9 Coding Classification; Language Model; Phenotype Identification; Relation Extraction; Tokenization; Training Data; Extraction; Accuracy; Article; Classification; Clinical Research; Embedding; Human; Icd-9; Intensive Care Unit; Knowledge; Language Model; Ontology; Phenotype; Unified Medical Language System; Word Processing},
	keywords = {Automation; Classification (of information); Clinical research; Codes (symbols); Computational linguistics; Embeddings; Information retrieval; Natural language processing systems; Ontology; Semantics; Text processing; BERT; Clinical concept and relation extraction; Concept extraction; Document Classification; ICD-9 coding classification; Language model; Phenotype identification; Relation extraction; Tokenization; Training data; Extraction; accuracy; Article; classification; clinical research; embedding; human; ICD-9; intensive care unit; knowledge; language model; ontology; phenotype; Unified Medical Language System; word processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wang2025,
	author = {Wang, Mingguo and Wang, Chengbin and Chen, Jianguo and Wang, Bo and Wang, Wei and Ma, Xiaogang and Ren, Jiangtao and Li, Zichen and Ye, Yicai and Zhang, Jiakai},
	title = {A lightweight knowledge graph-driven question answering system for field-based mineral resource survey},
	year = {2025},
	journal = {Applied Computing and Geosciences},
	volume = {27},
	pages = {},
	doi = {10.1016/j.acags.2025.100268},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011484817&doi=10.1016%2Fj.acags.2025.100268&partnerID=40&md5=309d3e0cc80c0e289babdafa6da29ef8},
	abstract = {Geoscience data associated with mineral resource surveys have become essential digital assets for governments and mining companies. The rapid increase in the volume of geoscience data makes it challenging to acquire knowledge quickly. In this study, we proposed and built a workflow that employs knowledge graph techniques, deep learning, question templates, and matching algorithms to provide a lightweight question-answering service for field-based geologists involved in mineral resource surveys. Initially, we utilized deep-learning-based geological entities and their semantic relation recognition, along with relational data mapping, to construct the mineral resource survey knowledge graph based on the ontology model. We then employed question template matching, a geological entity recognition model, and a sentence transformer to determine the optimal question template and generate a query statement for knowledge acquisition from a knowledge graph based on the Cypher language. Subsequently, we utilized a subgraph and a short abstract to express the results. The comparison with large language models and retrieval-augmented generation indicates that our solution is suitable for field-based mineral source surveys in a poor network environment with low-performance devices, data privacy concerns, and narrowly focused topics. The results also suggest that further studies on geoscience pre-trained models, an informative library of question templates, and multimodal knowledge graphs are necessary to improve the performance of the knowledge graph-driven question-answering system. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Intelligent Service; Knowledge Graph-driven Question Answering; Mineral Resource Survey; Question Answering; Sentence Transformer; Data Mining; Deep Learning; Geology; Graph Algorithms; Graphic Methods; Knowledge Acquisition; Knowledge Graph; Knowledge Transfer; Learning Algorithms; Mineral Exploration; Mineral Resources; Ontology; Query Languages; Question Answering; Semantics; Surveying; Geological-entities; Geoscience Data; Intelligent Services; Knowledge Graph-driven Question Answering; Knowledge Graphs; Mineral Resource Survey; Question Answering; Question Answering Systems; Sentence Transformer; Template Matching},
	keywords = {Data mining; Deep learning; Geology; Graph algorithms; Graphic methods; Knowledge acquisition; Knowledge graph; Knowledge transfer; Learning algorithms; Mineral exploration; Mineral resources; Ontology; Query languages; Question answering; Semantics; Surveying; Geological-entities; Geoscience data; Intelligent Services; Knowledge graph-driven question answering; Knowledge graphs; Mineral resource survey; Question Answering; Question answering systems; Sentence transformer; Template matching},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wang2025,
	author = {Wang, Boheng and Zhao, Xiaoyang and Zuo, Haoyu and Song, Yaxuan and Han, Ji and Childs, Peter R.N. and Chen, Liuqing},
	title = {From analogy to innovation: A creative conceptual design approach leveraging large language models},
	year = {2025},
	journal = {Advanced Engineering Informatics},
	volume = {67},
	pages = {},
	doi = {10.1016/j.aei.2025.103427},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007293444&doi=10.1016%2Fj.aei.2025.103427&partnerID=40&md5=3feb077d842aa4d1aba158445e9379a6},
	abstract = {Integrating creative concepts into Product Design and Manufacturing Systems (PDMS) is important for product innovation. However, current PDMS lack cognitive capabilities, particularly in reasoning and synthesis, which are essential for conceptual design. As a result, designers face challenges in retrieving relevant analogies, establishing meaningful mappings, and integrating knowledge into new design concepts. This paper proposes a computational conceptual product design approach that integrates Large Language Models’ (LLMs) knowledge representation with an analogy-based structured retrieval mechanism, supporting designers to explore and recombine design patterns and functionalities in an intuitive manner. Benefiting from the zero-shot learning and prompting capabilities of LLMs, given a source domain, this approach allows reasoning target domain based on abstract correspondences in both morphological and semantic associations. A template for the combinational regulation of the reuse of analogical knowledge has also been formulated. By decomposing analogical knowledge into ontological distinction, inspirational feature recognition, and associative mapping explanation, a creative conceptual design stimulation path is formed. An interactive tool named ViMimic based on this approach has been developed through a case study with 18 participants. Evaluation results demonstrate that the approach improves creative performance, increasing the novelty and functionality of conceptual designs by 51% and 22% respectively according to expert evaluations. It also boosts the efficiency and diversity of analogy mapping by 30% based on objective measures, while enhancing creative experiences and reducing cognitive load as measured in the participants’ self-assessment. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Analogy; Artificial Intelligence; Conceptual Product Design; Creativity; Innovation; Large Language Models; Domain Knowledge; Integrated Circuit Layout; Intellectual Property Core; Photomapping; Printed Circuit Design; Zero-shot Learning; 'current; Analogy; Cognitive Capability; Creatives; Creativity; Design Approaches; Innovation; Language Model; Large Language Model; Product Innovation; Conceptual Design},
	keywords = {Domain Knowledge; Integrated circuit layout; Intellectual property core; Photomapping; Printed circuit design; Zero-shot learning; 'current; Analogy; Cognitive capability; Creatives; Creativity; Design approaches; Innovation; Language model; Large language model; Product innovation; Conceptual design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Chan2025,
	author = {Chan, Chak Fu and Wong, Peter Kok Yiu and Guo, Xiaowen and Cheng, Jack Chin Pang and Chan, Jolly P.C. and Leung, Pak Him and Tao, Xingyu},
	title = {Context-aware vision-language model agent enriched with domain-specific ontology for construction site safety monitoring},
	year = {2025},
	journal = {Automation in Construction},
	volume = {177},
	pages = {},
	doi = {10.1016/j.autcon.2025.106305},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006881913&doi=10.1016%2Fj.autcon.2025.106305&partnerID=40&md5=07ecfd7b6fa89ca1bb1faa1bf130f03c},
	abstract = {Traditional approaches of construction site safety monitoring heavily rely on manual on-site inspection, which are prone to overlooked incidents. Existing computer vision methods require time-consuming and case-by-case data labeling, and lack high-level reasoning capability. This paper develops a human-alike virtual assistant agent by integrating a multi-modal vision-language model into video analytics: (1) To efficiently generate image-text data for model development, a semi-automatic image-text labeling pipeline based on in-context learning is designed; (2) To optimize a virtual agent from pre-trained to domain-tailored, a two-stage curriculum learning paradigm is designed to enhance model fine-tuning effectiveness toward domain-specific tasks; (3) To inject construction-domain knowledge more effectively into the virtual agent, a hierarchical prompting framework driven by a construction safety ontology is developed for more domain-tailored reasoning capability. The virtual agent has been deployed on a real construction site for real-time video analytics, with over 90 % accuracy in identifying violations of work-at-height safety regulations. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Construction Safety Ontology; Construction Site Safety Monitoring; Context-aware Vision-language Model; Domain-tailored Prompt Engineering; Virtual Construction Safety Assistant; Case Based Reasoning; Visual Languages; Construction Safety; Construction Safety Ontology; Construction Site Safety; Construction Site Safety Monitoring; Context-aware; Context-aware Vision-language Model; Domain-tailored Prompt Engineering; Language Model; Ontology's; Safety Monitoring; Virtual Construction; Virtual Construction Safety Assistant; Intelligent Virtual Agents},
	keywords = {Case based reasoning; Visual languages; Construction safety; Construction safety ontology; Construction site safety; Construction site safety monitoring; Context-Aware; Context-aware vision-language model; Domain-tailored prompt engineering; Language model; Ontology's; Safety monitoring; Virtual construction; Virtual construction safety assistant; Intelligent virtual agents},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ntoumanis2025,
	author = {Ntoumanis, Nikos and Moller, Arlen C.},
	title = {Self-determination theory informed research for promoting physical activity: Contributions, debates, and future directions},
	year = {2025},
	journal = {Psychology of Sport and Exercise},
	volume = {80},
	pages = {},
	doi = {10.1016/j.psychsport.2025.102879},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005601521&doi=10.1016%2Fj.psychsport.2025.102879&partnerID=40&md5=3b3f43d43c0f2db2c1051e1f97d534b2},
	abstract = {In this review we evaluate the applications of self-determination theory (SDT) research to promote motivation for physical activity (PA) and exercise. The evidence suggests that SDT-informed interventions are often effective at changing health behaviors, including PA/exercise, and associated health outcomes. The effect sizes are small to moderate and are often mediated by increases in autonomous motivation (primarily), interpersonal support for basic psychological needs, and competence need satisfaction. We also identify conceptual debates within the SDT literature and between SDT and other literatures, and discuss their relevance with respect to PA. We particularly focus on tripartite conceptualizations of interpersonal styles and psychological needs, whether there are more than three basic psychological needs, and the use of financial incentives and competition to promote PA. Our review also provides future conceptual and methodological directions for SDT-based research, building on advances in technology (e.g., generative Artificial Intelligence and Large Language Models) and the broader field of behavioral science (e.g., optimization designs, system-level interventions, behavior change intervention ontologies). © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Behavioral Science; Competition; Financial Incentives; Motivation; Narrative Review; Tripartite Model; Adult; Behavior Change; Competition; Economic Incentive; Effect Size; Exercise; Generative Artificial Intelligence; Health Behavior; Health Outcome; Human; Intrinsic Motivation; Large Language Model; Motivation; Narrative; Physical Activity; Review; Satisfaction; Theory; Therapy},
	keywords = {adult; behavior change; competition; economic incentive; effect size; exercise; generative artificial intelligence; health behavior; health outcome; human; intrinsic motivation; large language model; motivation; narrative; physical activity; review; satisfaction; theory; therapy},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Golnari2025,
	author = {Golnari, Pedram and Prantzalos, Katrina and Hood, Veronica and Meskis, Mary Anne and Isom, Lori L. and Wilcox, Karen S. and Parent, Jack M. and Lal, Dennis and Lhatoo, Samden D. and Goodkin, Howard G.},
	title = {Ontology accelerates few-shot learning capability of large language model: A study in extraction of drug efficacy in a rare pediatric epilepsy},
	year = {2025},
	journal = {International Journal of Medical Informatics},
	volume = {201},
	pages = {},
	doi = {10.1016/j.ijmedinf.2025.105942},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003927877&doi=10.1016%2Fj.ijmedinf.2025.105942&partnerID=40&md5=400ee810a20e5f257a499f7709f6b77e},
	abstract = {Objective: Dravet Syndrome (DS) is a developmental and epileptic encephalopathy that is characterized by severe, prolonged motor seizures and high resistance to multiple antiseizure medications (ASMs) with multiple comorbidities. Evaluating the efficacy of new drugs in DS preclinical models and mapping them to human phenotypes of DS through analysis of published literature is an important goal for improving outcomes in this rare pediatric epilepsy. Materials and Methods: Large language models (LLM) have demonstrated great promise in parsing published literature; however, the performance of LLMs falls short in medical applications. In this study, we investigate the effectiveness of domain ontology developed by human experts to optimize LLMs for medical text processing in a rare disease. Utilizing a benchmark dataset that describes the efficacy of 17 ASMs tested in preclinical models and DS patients, we define a new ontology-augmented phased in-context learning (PCL) approach to process 4935 full-text DS articles. We expand this analysis to 7 new drugs that demonstrate efficacy in reducing seizures to identify gaps in current knowledge for designing new experimental studies for drug discovery in DS. Results: Few-shot or in-context learning is a foundational capability of LLMs and the few-shot learning capability of the Gemini 1.0 Pro version LLM dramatically increases when we augment prompts with the DS epilepsy ontology. The DS epilepsy ontology is the largest epilepsy and seizure ontology in clinical use that was developed by DS basic scientists and clinical neurologists. The ontology-augmented PCL prompt achieves 100% accuracy in reproducing the benchmark drug efficacy dataset for 17 ASMs with only two examples for in-context learning. Conclusion: The new ontology-augmented PCL approach significantly accelerates the few-shot learning capabilities of the Gemini LLM, thereby reducing the number of required examples and time needed to optimize LLMs for medical applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Ontology; Dravet Syndrome; Drug Efficacy; Few-shot Learning; Optimizing Large Language Models; Bromide; Cannabidiol; Carbamazepine; Clemizole; Clobazam; Clonazepam; Diazepam; Ethosuximide; Fenfluramine; Gabapentin; Lamotrigine; Levetiracetam; Lorcaserin; Oxcarbazepine; Perampanel; Phenobarbital; Phenytoin; Rufinamide; Soticlestat; Stiripentol; Topiramate; Trazodone; Valproic Acid; Zonisamide; Anticonvulsants; Belviq; Biomedical Ontologies; Context Learning; Dravet Syndrome; Drug Efficacy; Few-shot Learning; In Contexts; Language Model; Learning Capabilities; Ontology's; Optimizing Large Language Model; Neurophysiology; Anticonvulsive Agent; Bromide; Cannabidiol; Carbamazepine; Clemizole; Clobazam; Clonazepam; Diazepam; Ethosuximide; Fenfluramine; Gabapentin; Lamotrigine; Levetiracetam; Lorcaserin; New Drug; Oxcarbazepine; Perampanel; Phenobarbital; Phenytoin; Rufinamide; Soticlestat; Stiripentol; Topiramate; Trazodone; Valproic Acid; Zonisamide; Animal Experiment; Animal Model; Article; Benchmarking; Child; Context Learning; Controlled Study; Data Extraction; Drug Efficacy; Experimental Seizure; Few Shot Learning; Human; Large Language Model; Mouse; Nonhuman; Ontology; Preclinical Study; Rare Disease; Severe Myoclonic Epilepsy In Infancy; Unprovoked Seizure; Word Processing; Biological Ontology; Drug Therapy; Machine Learning; Myoclonus Epilepsy; Natural Language Processing; Anticonvulsants; Biological Ontologies; Child; Epilepsies, Myoclonic; Humans; Large Language Models; Machine Learning; Natural Language Processing; Rare Diseases},
	keywords = {Biomedical ontologies; Context learning; Dravet syndrome; Drug efficacy; Few-shot learning; In contexts; Language model; Learning capabilities; Ontology's; Optimizing large language model; Neurophysiology; anticonvulsive agent; bromide; cannabidiol; carbamazepine; clemizole; clobazam; clonazepam; diazepam; ethosuximide; fenfluramine; gabapentin; lamotrigine; levetiracetam; lorcaserin; new drug; oxcarbazepine; perampanel; phenobarbital; phenytoin; rufinamide; soticlestat; stiripentol; topiramate; trazodone; valproic acid; zonisamide; animal experiment; animal model; Article; benchmarking; child; context learning; controlled study; data extraction; drug efficacy; experimental seizure; few shot learning; human; large language model; mouse; nonhuman; ontology; preclinical study; rare disease; severe myoclonic epilepsy in infancy; unprovoked seizure; word processing; biological ontology; drug therapy; machine learning; myoclonus epilepsy; natural language processing; Anticonvulsants; Biological Ontologies; Child; Epilepsies, Myoclonic; Humans; Large Language Models; Machine Learning; Natural Language Processing; Rare Diseases},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Gujral2025,
	author = {Gujral, Onkar Singh and Bafna, Mihir and Alm, Eric John and Berger, Bonnie A.},
	title = {Sparse autoencoders uncover biologically interpretable features in protein language model representations},
	year = {2025},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	volume = {122},
	number = {34},
	pages = {},
	doi = {10.1073/pnas.2506316122},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013688397&doi=10.1073%2Fpnas.2506316122&partnerID=40&md5=c009d1bc672b26f486b70e5ce8cad46c},
	abstract = {Foundation models in biology—particularly protein language models (PLMs)—have enabled ground-breaking predictions in protein structure, function, and beyond. However, the “black-box” nature of these representations limits transparency and explainability, posing challenges for human–AI collaboration and leaving open questions about their human-interpretable features. Here, we leverage sparse autoencoders (SAEs) and a variant, transcoders, from natural language processing to extract, in a completely unsupervised fashion, interpretable sparse features present in both protein-level and amino acid (AA)-level representations from ESM2, a popular PLM. Unlike other approaches such as training probes for features, the extraction of features by the SAE is performed without any supervision. We find that many sparse features extracted from SAEs trained on protein-level representations are tightly associated with Gene Ontology (GO) terms across all levels of the GO hierarchy. We also use Anthropic’s Claude to automate the interpretation of sparse features for both protein-level and AA-level representations and find that many of these features correspond to specific protein families and functions such as the NAD Kinase, IUNH, and the PTH family, as well as proteins involved in methyltransferase activity and in olfactory and gustatory sensory perception. We show that sparse features are more interpretable than ESM2 neurons across all our trained SAEs and transcoders. These findings demonstrate that SAEs offer a promising unsupervised approach for disentangling biologically relevant information present in PLM representations, thus aiding interpretability. This work opens the door to safety, trust, and explainability of PLMs and their applications, and paves the way to extracting meaningful biological insights across increasingly powerful models in the life sciences. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Interpretability; Protein Language Models; Sparse Autoencoders; Transcoders; Methyltransferase; Protein; Proteins; Methyltransferase; Protein; Amino Acid Sequence; Article; Computer Model; Feature Extraction; Gene Ontology; Histogram; Mathematical Model; Multilayer Perceptron; Predictive Value; Reproducibility; Sensation; Sequence Alignment; Sparse Autoencoder; Autoencoder; Bioinformatics; Chemistry; Genetics; Human; Metabolism; Natural Language Processing; Procedures; Autoencoder; Computational Biology; Gene Ontology; Humans; Natural Language Processing; Proteins},
	keywords = {methyltransferase; protein; amino acid sequence; Article; computer model; feature extraction; gene ontology; histogram; mathematical model; multilayer perceptron; predictive value; reproducibility; sensation; sequence alignment; sparse autoencoder; autoencoder; bioinformatics; chemistry; genetics; human; metabolism; natural language processing; procedures; Autoencoder; Computational Biology; Gene Ontology; Humans; Natural Language Processing; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Alves2025102,
	author = {Alves, Luis Felipe Medeiro and de Oliveira, José Maria Parente and Bonacin, Rodrigo and de Franco Rosa, Ferrucio},
	title = {An Ontology of Tobacco Production: Enriching Large Language Model-based Decision Support; Ontologia da Produção de Tabaco: enriquecendo o suporte à decisão baseado em LLM},
	year = {2025},
	journal = {Revista de Informatica Teorica e Aplicada},
	volume = {32},
	number = {3},
	pages = {102 - 111},
	doi = {10.22456/2175-2745.146658},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014364959&doi=10.22456%2F2175-2745.146658&partnerID=40&md5=a098f6ed75ca25e2651007ca1da70e7d},
	abstract = {Tobacco (Nicotiana tabacum) production plays a crucial role in the agricultural economy of several regions around the world, especially in developing countries, such as Brazil. However, the lack of well-defined terminology and semantic models harms the development of decision support systems. Conceptualizing the tobacco production domain is challenging due to its ambiguous terminology and the complexity involved in considering environmental, soil, disease, and pest management factors. We present an Ontology of Tobacco Production (OnTop), designed to assist tobacco production in optimizing crop management practices and placing environmental factors within a formal framework. To the best of available knowledge, this study is the first to formalize the tobacco production lifecycle integrated with soil and climate data. It includes symbolic and description logic for reasoning and automation. This paper presents the proposal for the core set (main classes) and the application of OnTop to enrich a Large Language Model (LLM) to provide an ontology-based decision support prompt. OnTop allows actionable recommendations based on environmental, agronomic, and productivity data. The main contributions of this paper are: i) a domain ontology that formalizes knowledge on data-driven tobacco production; and ii) a queryable framework (enriched LLM), which allows experts to obtain complex agronomic answers. OnTop offers an extensible framework for decision making in tobacco farming and paves the way for further innovations in ontologies and LLM-based decision support systems for the agricultural domain. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Representation; Large Language Model; Ontology; Tobacco},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Shi2025,
	author = {Shi, Meilin and Janowicz, Krzysztof W. and Liu, Zilong and Karimi, Mina and Majic, Ivan and Fortacz, Alexandra},
	title = {What, When, and Where Do You Mean? Detecting Spatio-Temporal Concept Drift in Scientific Texts},
	year = {2025},
	journal = {Leibniz International Proceedings in Informatics, LIPIcs},
	volume = {346},
	pages = {},
	doi = {10.4230/LIPIcs.GIScience.2025.16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013844283&doi=10.4230%2FLIPIcs.GIScience.2025.16&partnerID=40&md5=3d6cc0d0787198314bebfdb835c7ab26},
	abstract = {Inundated by the rapidly expanding AI research nowadays, the research community requires more effective research data management than ever. A key challenge lies in the evolving nature of concepts embedded in the growing body of research publications. As concepts evolve over time (e.g., keywords like global warming become more commonly referred to as climate change), past research may become harder to find and interpret in a modern context. This phenomenon, known as concept drift, affects how research topics and keywords are understood, categorized, and retrieved. Beyond temporal drift, such variations also occur across geographic space, reflecting differences in local policies, research priorities, and so forth. In this work, we introduce the notion of spatio-temporal concept drift to capture how concepts in scientific texts evolve across both space and time. Using a scientometric dataset in geographic information science, we detect how research keywords drifted across countries and years using word embeddings. By detecting spatio-temporal concept drift, we can better align archival research and bridge regional differences, ensuring scientific knowledge remains findable and interoperable within evolving research landscapes. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Drift; Large Language Models; Ontology; Research Data Management; Artificial Intelligence; Global Warming; Information Management; Natural Language Processing Systems; Concept Drifts; Growing Bodies; Language Model; Large Language Model; Ontology's; Research Communities; Research Data Managements; Scientific Texts; Spatio-temporal; Temporal Concepts; Ontology},
	keywords = {Artificial intelligence; Global warming; Information management; Natural language processing systems; Concept drifts; Growing bodies; Language model; Large language model; Ontology's; Research communities; Research data managements; Scientific texts; Spatio-temporal; Temporal concepts; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yoon2025,
	author = {Yoon, Sungmin and Song, Jihwan and Li, Jiteng},
	title = {Ontology-enabled AI agent-driven intelligent digital twins for building operations and maintenance},
	year = {2025},
	journal = {Journal of Building Engineering},
	volume = {108},
	pages = {},
	doi = {10.1016/j.jobe.2025.112802},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004815812&doi=10.1016%2Fj.jobe.2025.112802&partnerID=40&md5=fd420f45fa49eb960af27b15db48f480},
	abstract = {Building digital twins (DTs) are essential for enhancing operational efficiency, optimizing energy consumption, and reducing costs in buildings. However, the inherent complexity of buildings, their long operational lifespans, and the specific nature of the construction industry pose significant challenges in creating digital twins for buildings. Intelligent digital twins (IDTs) address these challenges by integrating existing digital twin models with AI, enabling a comprehensive representation of the building lifecycle while incorporating expert input. This study proposes an AI agent-based IDT framework using an ontological approach, where AI agents are engineered by DT administrators with building operations and maintenance (O&M) data, information, and applications within an ontological DT environment. Data and information generated within this environment are expressed in the DT ontology, enabling AI agents to gain a holistic understanding of the target system. Applications are integrated as a tool, thereby enabling AI agents to expand their actions and gain additional information from results. To validate this framework, virtual in-situ modeling (VIM) and fault detection and diagnosis (FDD) algorithms were implemented as DT applications to demonstrate the operation of the IDT system. Four case studies were conducted to demonstrate IDT-enabled O&M services, and LangSmith was used to visualize the AI agents' reasoning process as part of the result validation. It shows that AI agents have capabilities of performing building O&M tasks with high-level reasoning. The significance of this study lies in demonstrating the feasibility of implementing IDT models in building O&M by enabling AI agents to provide comprehensive, domain-specific knowledge and perform operational tasks, thereby serving as an assistant for both users and operators. Finally, this study underscores the critical role of engineers in managing and maintaining ontology and applications within the DT environment. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Agent; Building Informatics; Built Environments; Digital Twin (dt); Intelligent Digital Twin (idt); Large Language Model (llm); Ontology; Operation And Maintenance (o&m); Architectural Design; Ai Agent; Building Informatics; Built Environment; Digital Twin; Intelligent Digital Twin; Language Model; Large Language Model; Ontology's; Operation And Maintenance; Operations And Maintenance; Construction Industry},
	keywords = {Architectural design; AI agent; Building informatics; Built environment; Digital twin; Intelligent digital twin; Language model; Large language model; Ontology's; Operation and maintenance; Operations and maintenance; Construction industry},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Yuan2025,
	author = {Yuan, Wenhao and Chen, Guangyao and Wang, Zhilong and You, Fengqi},
	title = {Empowering Generalist Material Intelligence with Large Language Models},
	year = {2025},
	journal = {Advanced Materials},
	volume = {37},
	number = {32},
	pages = {},
	doi = {10.1002/adma.202502771},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004691895&doi=10.1002%2Fadma.202502771&partnerID=40&md5=94751b9cd7c38a6c166259d5efafd174},
	abstract = {Large language models (LLMs) are steering the development of generalist materials intelligence (GMI), a unified framework integrating conceptual reasoning, computational modeling, and experimental validation. Central to this framework is the agent-in-the-loop paradigm, where LLM-based agents function as dynamic orchestrators, synthesizing multimodal knowledge, specialized models, and experimental robotics to enable fully autonomous discovery. Drawing from a comprehensive review of LLMs’ transformative impact across representative applications in materials science, including data extraction, property prediction, structure generation, synthesis planning, and self-driven labs, this study underscores how LLMs are revolutionizing traditional tasks, catalyzing the agent-in-the-loop paradigm, and bridging the ontology-concept-computation-experiment continuum. Then the unique challenges of scaling up LLM adoption are discussed, particularly those arising from the misalignment of foundation LLMs with materials-specific knowledge, emphasizing the need to enhance adaptability, efficiency, sustainability, interpretability, and trustworthiness in the pursuit of GMI. Nonetheless, it is important to recognize that LLMs are not universally efficient. Their substantial resource demands and inconsistent performance call for careful deployment based on demonstrated task suitability. To address these realities, actionable strategies and a progressive roadmap for equitably and democratically implementing materials-aware LLMs in real-world practices are proposed. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Autonomous Agents; Generative Artificial Intelligence; Large Language Models; Material Intelligence; Structure (composition); Computational Modelling; Experimental Validations; Generative Artificial Intelligence; Language Model; Large Language Model; Material Intelligence; Model Validation; Model-based Opc; Multi-modal; Unified Framework; Materials Properties; Artificial Intelligence; Language; Large Language Model; Materials Science; Procedures; Artificial Intelligence; Language; Large Language Models; Materials Science},
	keywords = {Structure (composition); Computational modelling; Experimental validations; Generative artificial intelligence; Language model; Large language model; Material intelligence; Model validation; Model-based OPC; Multi-modal; Unified framework; Materials properties; artificial intelligence; language; large language model; materials science; procedures; Artificial Intelligence; Language; Large Language Models; Materials Science},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Otsuki2025804,
	author = {Otsuki, Yuka and Yada, Shuntaro and Nishiyama, Tomohiro and Sakurai, Toshiyuki and Okada, Masafumi and Kudo, Noriko and Kawabata, Kyoko and Fujimaki, Takako and Nagai, Hiroyuki and Wakamiya, Shoko},
	title = {Efficient Maintenance of Large-Scale Medical Dictionaries Using Large Language Models: A Case for Biomarkers},
	year = {2025},
	journal = {Studies in Health Technology and Informatics},
	volume = {329},
	pages = {804 - 808},
	doi = {10.3233/SHTI250951},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013383391&doi=10.3233%2FSHTI250951&partnerID=40&md5=f2e9ca4306b90c3c282c72bd9b48ba1a},
	abstract = {Dictionaries are essential in natural language processing and provide significant value across tasks; however, their construction and maintenance are expensive. Leveraging manual revision histories to suggest automatic corrections for unedited terms offers a promising solution to enhance quality while reducing costs. This study proposes a method for automatically correcting metadata in a large-scale medical dictionary containing more than 500,000 terms. By utilizing large language models that excel in zero-shot settings, the system estimates the dictionary information without task-specific configurations. This method was demonstrated through experiments on variations in gene biomarker expression, a task that requires specialized medical knowledge. The results indicate that this approach can significantly reduce the dictionary maintenance burden. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine},
	author_keywords = {Biomarker; Data Resource; Human-in-the-loop; Large Language Models; Natural Language Processing; Ontology; Biomarkers; Biological Marker; Book; Human; Large Language Model; Natural Language Processing; Biomarkers; Dictionaries As Topic; Humans; Large Language Models; Natural Language Processing},
	keywords = {biological marker; book; human; large language model; natural language processing; Biomarkers; Dictionaries as Topic; Humans; Large Language Models; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Golnari2025658,
	author = {Golnari, Pedram and Prantzalos, Katrina and Upadhyaya, Dipak Prasad and Buchhalter, Jeffrey R. and Sahoo, Satya Sanket},
	title = {Human in the Loop: Embedding Medical Expert Input in Large Language Models for Clinical Applications},
	year = {2025},
	journal = {Studies in Health Technology and Informatics},
	volume = {329},
	pages = {658 - 662},
	doi = {10.3233/SHTI250922},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013337344&doi=10.3233%2FSHTI250922&partnerID=40&md5=f8610aee92a393d37c972d43eb1f36fa},
	abstract = {The state-of-the-art performance of large language models (LLMs) in medical natural language (NLP) tasks, including medical query answering, summarization of clinical notes, and generation of medical reports has led to the development of a large number of application studies. However, many of these studies have also identified the key role of human input in generating accurate results with significant efforts focused on identifying an effective mechanism to elicit, model, and integrate human medical expertise in optimizing LLMs. In this paper, we introduce a new approach based on biomedical ontologies as a knowledge model to significantly improve the performance of LLMs in biomedical natural language processing (NLP) applications. Specifically, we focus on a rare pediatric epilepsy called Dravet syndrome (DS) which there is very limited understanding about the mechanisms that result in seizure and demonstrate the effectiveness of a unique epilepsy ontology in improving the accuracy of results. The results of this study create a new pathway for integrating human expertise in LLMs to support high accuracy and consistent results in medical applications. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine},
	author_keywords = {Dravet Syndrome; Large Language Models; Model Systems; Ontology; Biological Ontology; Diagnosis; Electronic Health Record; Expert System; Human; Large Language Model; Myoclonus Epilepsy; Natural Language Processing; Organization And Management; Biological Ontologies; Electronic Health Records; Epilepsies, Myoclonic; Expert Systems; Humans; Large Language Models; Natural Language Processing},
	keywords = {biological ontology; diagnosis; electronic health record; expert system; human; large language model; myoclonus epilepsy; natural language processing; organization and management; Biological Ontologies; Electronic Health Records; Epilepsies, Myoclonic; Expert Systems; Humans; Large Language Models; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Bowles2025769,
	author = {Bowles, Annie E. and Gan, Qiwei and Hanchrow, Elizabeth E. and DuVall, Scott L. and Alba, Patrick R. and Shi, Jianlin},
	title = {Comparative Analysis of NLP Models for Automatic LOINC Document Ontology Named Entity Recognition in Clinical Note Titles},
	year = {2025},
	journal = {Studies in Health Technology and Informatics},
	volume = {329},
	pages = {769 - 773},
	doi = {10.3233/SHTI250944},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013326994&doi=10.3233%2FSHTI250944&partnerID=40&md5=1f8db405df3dc83d8eef1b9e31cbf3a0},
	abstract = {In order to utilize clinical notes for research studies, it is necessary to identify the most relevant notes. Mapping to the LOINC Document Ontology makes this process easier by reducing the variability of note types. We experimented with three models to automatically identify LOINC DO entities in VA note titles. The supervised BERT model performed best, but the open-source large language models (LLMs) performed well despite a lack of fine-tuning. Future work will aim to improve note classification by including additional note metadata and contents, hybridizing with rule-based approaches, testing fine-tuned LLMs, and mapping to exact LOINC codes. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine},
	author_keywords = {Llm; Loinc; Natural Language Processing; Ontologies; Reproducibility; Vocabularies; Automated Pattern Recognition; Classification; Comparative Study; Electronic Health Record; Logical Observation Identifiers Names And Codes; Natural Language Processing; Procedures; Electronic Health Records; Natural Language Processing; Pattern Recognition, Automated},
	keywords = {automated pattern recognition; classification; comparative study; electronic health record; Logical Observation Identifiers Names and Codes; natural language processing; procedures; Electronic Health Records; Natural Language Processing; Pattern Recognition, Automated},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ma2025381,
	author = {Ma, Meg and Yu, Ping and Hickman, Louise D.},
	title = {Designing an Ontology for a Smart Learning Health System Framework},
	year = {2025},
	journal = {Studies in Health Technology and Informatics},
	volume = {329},
	pages = {381 - 385},
	doi = {10.3233/SHTI250866},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013229804&doi=10.3233%2FSHTI250866&partnerID=40&md5=4b64c52220c7159b257289573f45cae0},
	abstract = {The concept of a "Learning Health System Framework" has been defined in various forms, making it difficult for healthcare leaders to adopt an appropriate framework for organisational transformation. There is a pressing need to standardise its components and relationships. Limitations of the existing LHS frameworks include their lack of flexibility to adapt to sociotechnical changes and inability to guide the automatic evaluation initiative on health organisational performance. Ontologies facilitate consensus by aligning stakeholders on a shared vocabulary and structure, which can reduce ambiguity in communication and foster better collaboration across different stakeholder groups. Therefore, this study designed a Smart Learning Health System Ontology, abbreviated as "SMARTLHS". We followed a four-step realist synthesis methodology to develop SMARTLHS: (1) ontology requirements specification, (2) iterative and inductive ontology conceptualisation, (3) concept and relationship comparison and development, (4) ontology evaluation and refinement. The resultant SMARTLHS ontology comprises 475 unique classes and subclasses (concepts) and 135 object properties. The object properties define the relationships between these classes. SMARTLHS addresses the critical limitations of the existing LHS frameworks. It serves as a knowledge base for healthcare professionals. It can also be integrated into Retrieval Augmented Generative (RAG) applications and feed into large language models such as GPT 4o for advanced exploration of healthcare organisations' performance in developing learning health systems. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine},
	author_keywords = {Framework; Knowledge Graph; Learning Health System; Ontology; Biological Ontology; Controlled Vocabulary; Human; Learning Health System; Organization And Management; Biological Ontologies; Humans; Learning Health System; Vocabulary, Controlled},
	keywords = {biological ontology; controlled vocabulary; human; learning health system; organization and management; Biological Ontologies; Humans; Learning Health System; Vocabulary, Controlled},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Huettemann2025,
	author = {Huettemann, Sebastian and Mueller, Roland M. and Dinter, Barbara},
	title = {Designing ontology-based search systems for research articles},
	year = {2025},
	journal = {International Journal of Information Management},
	volume = {83},
	pages = {},
	doi = {10.1016/j.ijinfomgt.2025.102901},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000759636&doi=10.1016%2Fj.ijinfomgt.2025.102901&partnerID=40&md5=76ecbb3f1b252ad1b2d9d17354be6ab0},
	abstract = {The process of conducting scientific literature reviews is becoming increasingly complex and time-consuming due to the rapid expansion of available research. Popular academic search engines offer limited filtering capabilities and suffer from low precision. Machine learning-enhanced approaches tend to target rather specific areas, and novel approaches based on generative artificial intelligence suffer from hallucinations. Drawing on information foraging theory, this article presents a design science research project aimed at generating design knowledge for developing domain-specific search systems for research articles. Our contributions include: (1) integrating domain ontologies with large language models to design ontology-based search systems, (2) generating descriptive design knowledge by exploring the problem space, (3) generating prescriptive design knowledge for developing domain-specific search systems, and (4) presenting an ontology-based search engine prototype. Our results indicate that the proposed solution supports researchers in conducting literature reviews by increasing information gain while reducing interaction costs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Design Science Research; Domain Ontologies; Knowledge Extraction; Large Language Models; Literature Review; Ontologies; Search Engines; Contrastive Learning; Ontology; Reviews; Design Knowledge; Design-science Researches; Domain Ontologies; Knowledge Extraction; Language Model; Large Language Model; Literature Reviews; Ontology's; Ontology-based Searches; Search System; Domain Knowledge},
	keywords = {Contrastive Learning; Ontology; Reviews; Design knowledge; Design-science researches; Domain ontologies; Knowledge extraction; Language model; Large language model; Literature reviews; Ontology's; Ontology-based searches; Search system; Domain Knowledge},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Zhang2025,
	author = {Zhang, Min and Zhang, Xiaodong and Wang, Liang and Han, Xuehua and Xu, Dandan and Liu, Yu},
	title = {Entity–Relation Joint Extraction from Urban Planning Standards without Annotated Data Resources},
	year = {2025},
	journal = {Transactions in GIS},
	volume = {29},
	number = {5},
	pages = {},
	doi = {10.1111/tgis.70111},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014516224&doi=10.1111%2Ftgis.70111&partnerID=40&md5=84db66eb3b3b2ee585da6ab760490697},
	abstract = {Urban planning standards serve as critical guidelines for global urban development, yet the inefficiency of their knowledge services severely hinders the intelligent processes of planning formulation and evaluation. Existing generic knowledge extraction techniques struggle to adapt to highly specialized urban planning standard texts due to terminology-intensive content, complex inter-entity relations, and scarce annotated resources. To address these challenges, this study proposes a knowledge extraction method that combines large language models (LLMs) with conventional small models. Twenty-three entity types and 35 relation types were defined, and an urban planning standards ontology was constructed. With annotation rules designed, an LLM-based annotated corpus, including 4905 sentences and 17,056 triples, was constructed. The Urban Plan-Bert-Global Pointer-based Linking (UPB-GPLinker) was built for entity–relation joint extraction based on a customized dictionary and a pretrained Urban Plan-Bert (UPB) model. Compared with the baseline model, UPB-GPLinker significantly improves the F1 score by at least 2.48% and the Recall by at least 0.37%, verifying the feasibility and effectiveness of the improved method. This study fills the gap of knowledge extraction in urban planning standards and lays a foundation for forming standard knowledge graphs and providing intelligent services. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain-adaptive; Global Pointer Network; Joint Extraction; Large Language Model; Urban Planning Standards; Knowledge Graph; Urban Growth; Data Resources; Domain-adaptive; Global Pointer Network; Joint Extraction; Knowledge Extraction; Language Model; Large Language Model; Planning Standard; Urban Development; Urban Planning Standard; Extraction; Feasibility Study; Guideline; Planning Practice; Planning Process; Standard (reference); Terminology; Urban Development; Urban Planning},
	keywords = {Knowledge graph; Urban growth; Data resources; Domain-adaptive; Global pointer network; Joint extraction; Knowledge extraction; Language model; Large language model; Planning standard; Urban development; Urban planning standard; Extraction; feasibility study; guideline; planning practice; planning process; standard (reference); terminology; urban development; urban planning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zajac2025,
	author = {Zajac, Michal and Kulawiak, Connor and Li, Shenglin and Erickson, Caleb and Hubbell, Nathan and Gong, Jiaqi},
	title = {Unifying Flood-Risk Communication: Empowering Community Leaders Through AI-Enhanced, Contextualized Storytelling},
	year = {2025},
	journal = {Hydrology},
	volume = {12},
	number = {8},
	pages = {},
	doi = {10.3390/hydrology12080204},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014391633&doi=10.3390%2Fhydrology12080204&partnerID=40&md5=d0d730a2a4eb05ca617a66064e342e63},
	abstract = {Floods pose a growing threat globally, causing tragic loss of life, billions in economic damage annually, and disproportionately affecting socio-economically vulnerable populations. This paper aims to improve flood-risk communication for community leaders by exploring the application of artificial intelligence. We categorize U.S. flood information sources, review communication modalities and channels, synthesize the literature on community leaders’ roles in risk communication, and analyze existing technological tools. Our analysis reveals three key challenges: the fragmentation of flood information, information overload that impedes decision-making, and the absence of a unified communication platform to address these issues. We find that AI techniques can organize data and significantly enhance communication effectiveness, particularly when delivered through infographics and social media channels. Based on these findings, we propose FLAI (Flood Language AI), an AI-driven flood communication platform that unifies fragmented flood data sources. FLAI employs knowledge graphs to structure fragmented data sources and utilizes a retrieval-augmented generation (RAG) framework to enable large language models (LLMs) to produce contextualized narratives, including infographics, maps, and cost–benefit analyses. Beyond flood management, FLAI’s framework demonstrates how AI can transform public service data management and institutional AI readiness. By centralizing and organizing information, FLAI can significantly reduce the cognitive burden on community leaders, helping them communicate timely, actionable insights to save lives and build flood resilience. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Community Resilience; Flood-risk Communication; Knowledge Graph; Ontology; Retrieval-augmented Generation; Storytelling},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Qi2025,
	author = {Qi, Rui and Xiang, Ga and Zhang, Yangsen and Yang, Qunsheng and Cheng, Mingyue and Zhang, Haoyang and Ma, Mingming and Sun, Lu and Ma, Zhixing},
	title = {A Trustworthy Dataset for APT Intelligence with an Auto-Annotation Framework},
	year = {2025},
	journal = {Electronics (Switzerland)},
	volume = {14},
	number = {16},
	pages = {},
	doi = {10.3390/electronics14163251},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014382130&doi=10.3390%2Felectronics14163251&partnerID=40&md5=002c1db6272c2d73130ecf28fc5da21c},
	abstract = {Advanced Persistent Threats (APTs) pose significant cybersecurity challenges due to their multi-stage complexity. Knowledge graphs (KGs) effectively model APT attack processes through node-link architectures; however, the scarcity of high-quality, annotated datasets limits research progress. The primary challenge lies in balancing annotation cost and quality, particularly due to the lack of quality assessment methods for graph annotation data. This study addresses these issues by extending existing APT ontology definitions and developing a dynamic, trustworthy annotation framework for APT knowledge graphs. The framework introduces a self-verification mechanism utilizing large language model (LLM) annotation consistency and establishes a comprehensive graph data metric system for problem localization in annotated data. This metric system, based on structural properties, logical consistency, and APT attack chain characteristics, comprehensively evaluates annotation quality across representation, syntax semantics, and topological structure. Experimental results show that this framework significantly reduces annotation costs while maintaining quality. Using this framework, we constructed LAPTKG, a reliable dataset containing over 10,000 entities and relations. Baseline evaluations show substantial improvements in entity and relation extraction performance after metric correction, validating the framework’s effectiveness in reliable APT knowledge graph dataset construction. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Apt; Automated Annotation Framework; Entity Relationship Dataset; Graph Data Evaluation; Threat Intelligence; Cybersecurity; Data Consistency; Data Quality; Graph Theory; Graphic Methods; Knowledge Graph; Ontology; Advanced Persistent Threat; Auto-annotation; Automated Annotation Framework; Data Evaluation; Entity Relationship Dataset; Entity-relationship; Graph Data; Graph Data Evaluation; Knowledge Graphs; Threat Intelligence; Semantics},
	keywords = {Cybersecurity; Data consistency; Data quality; Graph theory; Graphic methods; Knowledge graph; Ontology; Advanced persistent threat; Auto-annotation; Automated annotation framework; Data evaluation; Entity relationship dataset; Entity-relationship; Graph data; Graph data evaluation; Knowledge graphs; Threat intelligence; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang2025,
	author = {Zhang, Daokun and Zhang, Jian and Jia, Yanhe and Liao, Mengjie},
	title = {A Novel Dual-Strategy Approach for Constructing Knowledge Graphs in the Home Appliance Fault Domain},
	year = {2025},
	journal = {Algorithms},
	volume = {18},
	number = {8},
	pages = {},
	doi = {10.3390/a18080485},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014352969&doi=10.3390%2Fa18080485&partnerID=40&md5=2f3d9626ee0b08b84c8e0aa9e2c7f935},
	abstract = {Knowledge graph technology holds significant importance for efficient fault diagnosis in household appliances. However, the scarcity of public fault diagnosis data and the lack of automated knowledge extraction pose major challenges to knowledge graph construction. To address issues such as ambiguous entity boundaries, severe entity nesting, and poor entity extraction performance in fault diagnosis texts, this paper proposes a dual-strategy progressive knowledge extraction framework. First, to tackle the high complexity of fault diagnosis texts, an entity recognition model named RoBERTa-zh-BiLSTM-MUL-CRF is designed, improving the accuracy of nested entity extraction. Second, leveraging the semantic understanding capability of large language models, a progressive prompting strategy is adopted for ontology alignment and relation extraction, achieving automated knowledge extraction. Experimental results show that the proposed named entity recognition model outperforms traditional models, with improvements of 3.87%, 5.82%, and 2.05% in F1-score, recall, and precision, respectively. Additionally, the large language model demonstrates better performance in ontology alignment compared to traditional machine learning models. The constructed knowledge graph for household appliance fault diagnosis integrates structured fault diagnosis information. It effectively processes unstructured fault texts and supports visual queries and entity tracing. This framework can assist maintenance personnel in making rapid judgments, thereby improving fault diagnosis efficiency. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Home Appliance Faults; Knowledge Graphs; Large Language Models; Named Entity Recognition; Relationship Extraction; Extraction; Failure Analysis; Fault Detection; Graph Theory; Knowledge Graph; Knowledge Management; Learning Systems; Ontology; Semantics; Entity Extractions; Faults Diagnosis; Home Appliance Fault; Knowledge Extraction; Knowledge Graphs; Language Model; Large Language Model; Named Entity Recognition; Performance; Relationship Extraction; Domestic Appliances},
	keywords = {Extraction; Failure analysis; Fault detection; Graph theory; Knowledge graph; Knowledge management; Learning systems; Ontology; Semantics; Entity extractions; Faults diagnosis; Home appliance fault; Knowledge extraction; Knowledge graphs; Language model; Large language model; Named entity recognition; Performance; Relationship extraction; Domestic appliances},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yang2025,
	author = {Yang, Libo and Li, Yuan and Tan, Junhua and Mao, Libo},
	title = {Research on risk decision-making generation method for water conservancy project based on multimodal knowledge graph and large language model},
	year = {2025},
	journal = {PLOS ONE},
	volume = {20},
	number = {8 August},
	pages = {},
	doi = {10.1371/journal.pone.0330258},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014147690&doi=10.1371%2Fjournal.pone.0330258&partnerID=40&md5=0ac85ebe710485ba0a4abbc4818c923e},
	abstract = {Traditional knowledge graphs of water conservancy project risks have supported risk decision-making. However, they are constrained by limited data modalities and low accuracy in information extraction. A multimodal water conservancy project risk knowledge graph is proposed in this study, along with a synergistic strategy involving multimodal large language models Risk decision-making generation is facilitated through a multi-agent agentic retrieval-augmented generation framework. To enhance visual recognition, a DenseNet-based image classification model is improved by incorporating single-head self-attention and coordinate attention mechanisms. For textual data, risk entities such as locations, components, and events are extracted using a BERT-BiLSTM-CRF architecture. These extracted entities serve as the foundation for constructing the multimodal knowledge graph. To support generation, a multi-agent agentic retrieval-augmented generation mechanism is introduced. This mechanism enhances the reliability and interpretability of risk decision-making outputs. In experiments, the enhanced DenseNet model outperforms the original baseline in both precision and recall for image recognition tasks. In risk decision-making tasks, the proposed approach—combining a multimodal knowledge graph with a multi-agent agentic retrieval-augmented generation method—achieves strong performance on BERTScore and ROUGE-L metrics. This work presents a novel perspective for leveraging multimodal knowledge graphs in water conservancy project risk management. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cuda Version 12.6; Python Version 3.11.11; Pytorch Version 2.1.2; Vscode 1.98.2; Architecture; Article; Benchmarking; Bert; Bilstm; Coordatt; Crf Model; Data Processing; Decision Making Task; Densenet; Entity Recognition Model; Environment; Image Analysis; Image Recognition (algorithm); Knowledge Fusion; Knowledge Base; Knowledge Graph; Large Language Model; Multi Source Heterogeneous Data Preprocessing; Multimodal Knowledge Graph; Ontology Development; Open Access Publishing; Recognition; Reliability; Retrieval Augmented Generation; Semantic Memory; Shsa; Synergistic Effect; Validation Study; Visual Memory; Water Conservancy Project Risks},
	keywords = {architecture; Article; benchmarking; BERT; BiLSTM; CoordAtt; CRF model; data processing; decision making task; DenseNet; entity recognition model; environment; image analysis; image recognition (algorithm); knowledge fusion; knowledge base; knowledge graph; large language model; multi source heterogeneous data preprocessing; multimodal knowledge graph; ontology development; open access publishing; recognition; reliability; retrieval augmented generation; semantic memory; SHSA; synergistic effect; validation study; visual memory; water conservancy project risks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2025,
	author = {Liu, Yiwei and Bao, Ting and Yin, Peng and Wang, Shumin and Wang, Yanbin},
	title = {RPIPLM: Prediction of ncRNA-protein interaction by post-training a dual-tower pretrained biological model with supervised contrastive learning},
	year = {2025},
	journal = {PLOS ONE},
	volume = {20},
	number = {8 August},
	pages = {},
	doi = {10.1371/journal.pone.0329174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013257797&doi=10.1371%2Fjournal.pone.0329174&partnerID=40&md5=001570a0335e9cf522ffc051e705da60},
	abstract = {The field of biological research has been profoundly impacted by the emergence of biological pre-trained models, which have resulted in remarkable advancements in life sciences and medicine. However, the current landscape of biological pre-trained language models suffers from a shortcoming, i.e., their inability to grasp the intricacies of molecular interactions, such as ncRNA-protein interactions. It is in this context that our paper introduces a two-tower computational framework, termed RPIPLM, which brings forth a new paradigm for the prediction of ncRNA-protein interactions. The core of RPIPLM lies in its harnessing of the pre-trained RNA language model and protein language model to process ncRNA and protein sequences, thereby enabling the transfer of the general knowledge gained from self-supervised learning of vast data to ncRNA-protein interaction tasks. Additionally, to learn the intricate interaction patterns between RNA and protein embeddings across diverse scales, we employ a fusion of scaled dot-product self-attention mechanism and Multi-scale convolution operations on the output of the dual-tower architecture, effectively capturing both global and local information. Furthermore, we introduce supervised contrastive learning into the training of RPIPLM, enabling the model to effectively capture discriminative information by distinguishing between interacting and non-interacting samples in the learned representations. Through extensive experiments and an interpretability study, we demonstrate the effectiveness of RPIPLM and its superiority over other methods, establishing new state-of-the-art performance. RPIPLM is a powerful and scalable computational framework that holds the potential to unlock enormous insights from vast biological data, thereby accelerating the discovery of molecular interactions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Rna, Untranslated; Untranslated Rna; Area Under The Curve; Article; Biological Model; Computer Model; Controlled Study; Correlation Coefficient; Cross Validation; Diagnostic Test Accuracy Study; Gene Ontology; Human; Knowledge; Language Model; Machine Learning; Natural Language Processing; Prediction; Protein Language Model; Protein Protein Interaction; Receiver Operating Characteristic; Rna Sequence; Supervised Contrastive Learning; Algorithm; Bioinformatics; Genetics; Metabolism; Procedures; Supervised Machine Learning; Algorithms; Computational Biology; Humans; Models, Biological; Rna, Untranslated; Supervised Machine Learning},
	keywords = {untranslated RNA; area under the curve; Article; biological model; computer model; controlled study; correlation coefficient; cross validation; diagnostic test accuracy study; gene ontology; human; knowledge; language model; machine learning; natural language processing; prediction; protein language model; protein protein interaction; receiver operating characteristic; RNA sequence; supervised contrastive learning; algorithm; bioinformatics; genetics; metabolism; procedures; supervised machine learning; Algorithms; Computational Biology; Humans; Models, Biological; RNA, Untranslated; Supervised Machine Learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Denger2025,
	author = {Denger, Andreas and Helms, Volkhard},
	title = {Application of Protein Structure Encodings and Sequence Embeddings for Transporter Substrate Prediction},
	year = {2025},
	journal = {Molecules},
	volume = {30},
	number = {15},
	pages = {},
	doi = {10.3390/molecules30153226},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013217668&doi=10.3390%2Fmolecules30153226&partnerID=40&md5=31ad861ddc28212e5e1a7ebe0351994b},
	abstract = {Membrane transporters play a crucial role in any cell. Identifying the substrates they translocate across membranes is important for many fields of research, such as metabolomics, pharmacology, and biotechnology. In this study, we leverage recent advances in deep learning, such as amino acid sequence embeddings with protein language models (pLMs), highly accurate 3D structure predictions with AlphaFold 2, and structure-encoding 3Di sequences from FoldSeek, for predicting substrates of membrane transporters. We test new deep learning features derived from both sequence and structure, and compare them to the previously best-performing protein encodings, which were made up of amino acid k-mer frequencies and evolutionary information from PSSMs. Furthermore, we compare the performance of these features either using a previously developed SVM model, or with a regularized feedforward neural network (FNN). When evaluating these models on sugar and amino acid carriers in A. thaliana, as well as on three types of ion channels in human, we found that both the DL-based features and the FNN model led to a better and more consistent classification performance compared to previous methods. Direct encodings of 3D structures with Foldseek, as well as structural embeddings with ProstT5, matched the performance of state-of-the-art amino acid sequence embeddings calculated with the ProtT5-XL model when used as input for the FNN classifier. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Alphafold; Deep Learning; Feature Extraction; Gene Ontology; Machine Learning; Membrane Bioinformatics; Membrane Transport; Protein Function Prediction; Protein Language Model; Substrate Prediction; Carrier Protein; Membrane Transport Proteins; Carrier Protein; Amino Acid Sequence; Arabidopsis; Artificial Neural Network; Bioinformatics; Chemistry; Deep Learning; Enzyme Specificity; Human; Metabolism; Molecular Model; Procedures; Protein Conformation; Amino Acid Sequence; Computational Biology; Deep Learning; Humans; Membrane Transport Proteins; Models, Molecular; Neural Networks, Computer; Protein Conformation; Substrate Specificity},
	keywords = {carrier protein; amino acid sequence; Arabidopsis; artificial neural network; bioinformatics; chemistry; deep learning; enzyme specificity; human; metabolism; molecular model; procedures; protein conformation; Amino Acid Sequence; Computational Biology; Deep Learning; Humans; Membrane Transport Proteins; Models, Molecular; Neural Networks, Computer; Protein Conformation; Substrate Specificity},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Díaz2025179,
	author = {Díaz, Raúl and Xin, Hongliang},
	title = {Knowledge graphs in heterogeneous catalysis: Recent advances and future opportunities},
	year = {2025},
	journal = {Chinese Journal of Chemical Engineering},
	volume = {84},
	pages = {179 - 189},
	doi = {10.1016/j.cjche.2025.06.008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012920030&doi=10.1016%2Fj.cjche.2025.06.008&partnerID=40&md5=60a4114a5b879071373450d25c31b9f7},
	abstract = {Knowledge graphs (KGs) offer a structured, machine-readable format for organizing complex information. In heterogeneous catalysis, where data on catalytic materials, reaction conditions, mechanisms, and synthesis routes are dispersed across diverse sources, KGs provide a semantic framework that supports data integration under the FAIR (Findable, Accessible, Interoperable, and Reusable) principles. This review aims to survey recent developments in catalysis KGs, describe the main techniques for graph construction, and highlight how artificial intelligence, particularly large language models (LLMs), enhances graph generation and query. We conducted a systematic analysis of the literature, focusing on ontology-guided text mining pipelines, graph population methods, and maintenance strategies. Our review identifies key trends: ontology-based approaches enable the automated extraction of domain knowledge, LLM-driven retrieval-augmented generation supports natural-language queries, and scalable graph architectures range from a few thousand to over a million triples. We discuss state-of-the-art applications, such as catalyst recommendation systems and reaction mechanism discovery tools, and examine the major challenges, including data heterogeneity, ontology alignment, and long-term graph curation. We conclude that KGs, when combined with AI methods, hold significant promise for accelerating catalyst discovery and knowledge management, but progress depends on establishing community standards for ontology development and maintenance. This review provides a roadmap for researchers seeking to leverage KGs to advance heterogeneous catalysis research. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Heterogeneous Catalysis; Knowledge Graph; Large Language Models; Ontology; Catalysis; Catalysts; Chemical Industry; Data Integration; Data Mining; Deep Learning; Graph Theory; Graphic Methods; Knowledge Graph; Knowledge Management; Natural Language Processing Systems; Query Languages; Query Processing; Semantics; Catalyse; Complex Information; Knowledge Graphs; Language Model; Large Language Model; Machine-readable Format; Ontology's; Reaction Mechanism; ]+ Catalyst; Ontology},
	keywords = {Catalysis; Catalysts; Chemical industry; Data integration; Data mining; Deep learning; Graph theory; Graphic methods; Knowledge graph; Knowledge management; Natural language processing systems; Query languages; Query processing; Semantics; Catalyse; Complex information; Knowledge graphs; Language model; Large language model; Machine-readable format; Ontology's; Reaction mechanism; ]+ catalyst; Ontology},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wang2025,
	author = {Wang, Jiawen and Liu, Xin and Liang, Rui and Li, Jing and Bi, Mengmeng and Li, Jinwei and Mu, Ce and Yang, Yingdong and Li, Shujie and Yang, Panpan},
	title = {LlbZIP11 promotes anthocyanin accumulation in raised spots of Lilium leichtlinii by inducing LlMYB19S expression},
	year = {2025},
	journal = {International Journal of Biological Macromolecules},
	volume = {320},
	pages = {},
	doi = {10.1016/j.ijbiomac.2025.146085},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010845569&doi=10.1016%2Fj.ijbiomac.2025.146085&partnerID=40&md5=48350a2f1bedf22301ce6de252782af8},
	abstract = {Lilium leichtlinii is valued for its distinctive floral pattern, characterized by yellow tepals consisting of raised spots. These raised spots result from anthocyanin accumulation in the proliferating parenchymal and epidermal cells. However, molecular mechanisms regulating anthocyanin accumulation in raised spots remain uncharacterized. In this study, through a comparative transcriptome analysis of L. leichtlinii during raised spot formation, we identified LlbZIP11, a bZIP family transcription factor, as a candidate regulator of this process. Gene expression analysis revealed that the expression pattern of LlbZIP11 was similar to those of LlMYB19S, a MYB transcription factor involved in raised spot anthocyanin pigmentation, and anthocyanin biosynthesis genes. Functional validation demonstrated that LlbZIP11 positively regulates tepal spot formation and anthocyanin accumulation by upregulating LlMYB19S and anthocyanin-related genes. Dual-luciferase and yeast one-hybrid assays confirmed that LlbZIP11 binds to the LlMYB19S promoter to activate its transcription. These findings indicate that LlbZIP11 activates LlMYB19S, promoting anthocyanin accumulation in raised spots of L. leichtlinii. This study expands the understanding of anthocyanin regulatory networks in diverse floral pigmentation patterns and provides a foundation for further investigations into the mechanisms underlying raised spot formation in Lilium. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Anthocyanin Accumulation; Lilium Leichtlinii; Raised Spots; Anthocyanins; Basic-leucine Zipper Transcription Factors; Plant Proteins; Transcription Factors; Ccd Camera; Cdna Synthesis Supermix; Diamond Software; Graphpad Prism 8.0.2; Hifair Ii 1st Strand Cdna Synthesis Supermix For Qpcr; Hmmer Software; Ibm Spss Statistics 25; Rnaprep Pure Plant Plus Kit; Transscript One-step Gdna Removal; Wgcna Package In R Version 4.4.1; Anthocyanins; Cell Culture; Factor Analysis; Transcription; Anthocyanin Accumulation; Epidermal Cells; Expression Patterns; Gene Expression Analysis; Lilium Leichtlinii; Molecular Mechanism; Myb Transcription Factors; Parenchymal Cells; Raised Spot; Transcriptome Analysis; Biochemistry; Anthocyanin; Basic Leucine Zipper Transcription Factor; Protein Myb; Transcription Factor; Transcriptome; Plant Protein; Article; Confocal Laser Scanning Microscopy; Controlled Study; Drug Accumulation; Gene Expression; Gene Ontology; Lilium Leichtlinii; Lily; Network Analysis; Nonhuman; Phylogeny; Plant Leaf; Polymerase Chain Reaction; Promoter Region; Protein Expression; Raised Spot; Rna Extraction; Rna Isolation; Transcriptome Sequencing; Western Blotting; Yeast; Biosynthesis; Flower; Gene Expression Profiling; Gene Expression Regulation; Genetics; Metabolism; Pigmentation; Basic-leucine Zipper Transcription Factors; Flowers; Gene Expression Profiling; Gene Expression Regulation, Plant; Lilium; Pigmentation; Plant Proteins; Promoter Regions, Genetic; Transcription Factors},
	keywords = {Anthocyanins; Cell culture; Factor analysis; Transcription; Anthocyanin accumulation; Epidermal cells; Expression patterns; Gene expression analysis; Lilium leichtlinii; Molecular mechanism; MYB transcription factors; Parenchymal cells; Raised spot; Transcriptome analysis; Biochemistry; anthocyanin; basic leucine zipper transcription factor; protein Myb; transcription factor; transcriptome; plant protein; Article; confocal laser scanning microscopy; controlled study; drug accumulation; gene expression; gene ontology; lilium leichtlinii; lily; network analysis; nonhuman; phylogeny; plant leaf; polymerase chain reaction; promoter region; protein expression; raised spot; RNA extraction; RNA isolation; transcriptome sequencing; Western blotting; yeast; biosynthesis; flower; gene expression profiling; gene expression regulation; genetics; metabolism; pigmentation; Basic-Leucine Zipper Transcription Factors; Flowers; Gene Expression Profiling; Gene Expression Regulation, Plant; Lilium; Pigmentation; Plant Proteins; Promoter Regions, Genetic; Transcription Factors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kollapally2025,
	author = {Kollapally, Navya Martin and Geller, J. and Keloth, Vipina Kuttichi and He, Zhe and Xu, Julia},
	title = {Ontology enrichment using a large language model: Applying lexical, semantic, and knowledge network-based similarity for concept placement},
	year = {2025},
	journal = {Journal of Biomedical Informatics},
	volume = {168},
	pages = {},
	doi = {10.1016/j.jbi.2025.104865},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009015837&doi=10.1016%2Fj.jbi.2025.104865&partnerID=40&md5=048cd0ae96fafa94aebcf8534267778c},
	abstract = {Objective: Ontologies are essential for representing the knowledge of a domain. To make ontologies useful, they must encompass a comprehensive domain view. To achieve ontology enrichment, there is a need to discover new concepts to be added, either because they were missed in the first place, or the state-of-the-art has advanced to develop new real-world concepts. Our goal is to develop an automatic enrichment pipeline using a seed ontology, a Large Language Model (LLM), and source of text. The pipeline is applied to the domain of Social Determinants of Health (SDoH), using PubMed as a source of concepts. In this work, the applicability and effectiveness of the enrichment pipeline is demonstrated by extending the SDoH Ontology called SOHOv1, however our methodology could be used in other domains as well. Methods: We first retrieved PubMed abstracts of candidate articles with existing SOHOv1 concepts as search terms. Next, we used GPT-4-1201 to extract semantic triples from the abstracts. We identified concepts from these triples utilizing lexical, semantic, and knowledge network-based filtering. We also compared the granularity of semantic triples extracted with our method to the triples in the SemMedDB (Semantic MEDLINE Database). The results were evaluated by human experts and standard ontology tools for checking consistency and semantic correctness. Results: We expanded SOHOv1, which contained 173 concepts and 585 axioms, including 207 logical axioms to SOHOv2, which contains 572 concepts, 1,542 axioms, including 725 logical axioms. Our methods identified more concepts than those extracted from SemMedDB for the same task. While we have shown the feasibility of our approach for an SDoH ontology, the methodology is generalizable to other ontologies with an existing seed ontology and text corpus. Conclusions: The contributions of this work are: Extracting semantic triples from PubMed abstracts using GPT-4-1201 utilizing prompt chaining; showing the superiority of triples from GPT-4-1201 over triples from SemMedDB for SDoH; using lexical and semantic similarity search techniques with knowledge network-based search to identify the concepts to be added to the ontology; confirming the quality of the new concepts with human experts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Model; Ontology Enrichment; Ontology Evaluation; Semantic Medline; Semantic Medline Database; Semmeddb Database; Similarity Search; Social Determinants Of Health; Abstracting; Computational Linguistics; Data Mining; Database Systems; Knowledge Based Systems; Knowledge Organization; Learning Systems; Natural Language Processing Systems; Search Engines; Semantic Web; Semantics; Language Model; Large Language Model; Medline Database; Ontology Enrichment; Ontology Evaluations; Semantic Medline; Semantic Medline Database; Semantic Medline Database Database; Similarity Search; Social Determinants Of Healths; Ontology; Academic Failure; Alcohol Consumption; Article; Built Environment; Community; Data Extraction; Depression; Economic Instability; Education; Generative Pretrained Transformer; Health Care Access; Health Care Disparity; High Risk Behavior; Human; Intermethod Comparison; Knowledge; Large Language Model; Natural Disaster; Neighborhood; Ontology; Ontology Development; Ontology Enrichment; Safety; Semantics; Social Aspect; Social Determinants Of Health; Biological Ontology; Language; Medline; Natural Language Processing; Biological Ontologies; Humans; Language; Large Language Models; Natural Language Processing; Pubmed},
	keywords = {Abstracting; Computational linguistics; Data mining; Database systems; Knowledge based systems; Knowledge organization; Learning systems; Natural language processing systems; Search engines; Semantic Web; Semantics; Language model; Large language model; MEDLINE database; Ontology enrichment; Ontology evaluations; Semantic MEDLINE; Semantic MEDLINE database; Semantic MEDLINE database database; Similarity search; Social determinants of healths; Ontology; academic failure; alcohol consumption; Article; built environment; community; data extraction; depression; economic instability; education; generative pretrained transformer; health care access; health care disparity; high risk behavior; human; intermethod comparison; knowledge; large language model; natural disaster; neighborhood; ontology; ontology development; ontology enrichment; safety; semantics; social aspect; social determinants of health; biological ontology; language; Medline; natural language processing; Biological Ontologies; Humans; Language; Large Language Models; Natural Language Processing; PubMed},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ludwig2025,
	author = {Ludwig, Heiner and Schmidt, Thorsten Lars and Kuhn, Mathias},
	title = {An ontology-based retrieval augmented generation procedure for a voice-controlled maintenance assistant},
	year = {2025},
	journal = {Computers in Industry},
	volume = {169},
	pages = {},
	doi = {10.1016/j.compind.2025.104289},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001491829&doi=10.1016%2Fj.compind.2025.104289&partnerID=40&md5=22c9f2677690b0f846817941883c23f0},
	abstract = {This paper presents a novel approach to support complex maintenance procedures through a dialogue-driven digital assistant using an ontology-based retrieval augmented generation method. The core of the proposed system relies on the strong formalisation capabilities of the graph-based Web Ontology Language (OWL), combined with various retrieval algorithms and different Large Language Models (LLMs) to determine the most useful context for answering user queries. To do this, we use the popular principle of Retrieval Augmented Generation (RAG). Graph traversal enriches the contextual knowledge, enabling more accurate and context-aware responses. An evaluation using an OWL example ontology and an extensive Q&A dataset demonstrates the improved retrieval quality achieved by combining classical and vector-based semantic matching methods. The community-driven analysis of generation quality illustrates the usability of an OWL-based assistant for maintenance procedures on the basis of contexts and LLMs of varying configurations. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Model (llm); Maintenance Assistant; Retrieval Augmented Generation (rag); Web Ontology Language (owl); Content Based Retrieval; Modeling Languages; Ontology; Structured Query Language; Digital Assistants; Language Model; Large Language Model; Maintenance Assistant; Maintenance Procedures; Ontology-based; Retrieval Augmented Generation; Voice-controlled; Web Ontology Language (owl); Semantics},
	keywords = {Content based retrieval; Modeling languages; Ontology; Structured Query Language; Digital assistants; Language model; Large language model; Maintenance assistant; Maintenance procedures; Ontology-based; Retrieval augmented generation; Voice-controlled; Web ontology language (OWL); Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Halpin20254589,
	author = {Halpin, Harry},
	title = {Artificial intelligence versus collective intelligence},
	year = {2025},
	journal = {AI and Society},
	volume = {40},
	number = {6},
	pages = {4589 - 4604},
	doi = {10.1007/s00146-025-02240-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000281882&doi=10.1007%2Fs00146-025-02240-x&partnerID=40&md5=42d482e826614988bf83669e44ff3083},
	abstract = {The ontological presupposition of artificial intelligence (AI) is the liberal autonomous human subject of Locke and Kant, and the ideology of AI is the automation of this particular conception of intelligence. This is demonstrated in detail in classical AI by the work of Simon, who explicitly connected his work on AI to a wider programme in cognitive science, economics, and politics to perfect capitalism. Although Dreyfus produced a powerful Heideggerian critique of classical AI, work on neural networks in AI was ultimately based on the individual as the locus of intelligence. Yet this conception of AI both fails to grasp the essence of large language models, which are a statistical model of human language on the Web. The training data that enables AI is the surveillance and capture of data, where the data creates a model to approximate the entire world. However, there is a more hidden ideology inherent in AI where the goal is not to perfect a model but to control the world. As prompted by an argument between Mead and Bateson, social change is prevented by the application of cybernetics to society as a whole. The goal of AI is not just to replace human beings, but to manage humans to preserve existing power relations. As the source of intelligence in AI is distributed cognition between humans and machines, the alternative to AI is collective intelligence. As theorized by Licklider and Engelbart at the dawn of the Internet, collective intelligence explains how computers weave together both human and non-human intelligence. Rather than replace human intelligence, this produces ever more complex collective forms of intelligence. Rather than meta-stabilize a society of control, collective intelligence can go outside individualist capitalist ontology by incorporating the open world of the pluriverse, as theorized by Escobar. Collective intelligence then stands as an alternative ontological path for AI which puts intelligence at the service of humanity and the world rather than a technocratic elite. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Collective Intelligence; Cybernetics; Individualism; Economic And Social Effects; Neural Networks; Cognitive Science; Collective Intelligences; Human Intelligence; Human Language; Human Subjects; Individualism; Language Model; Neural-networks; Statistic Modeling; Training Data; Ontology},
	keywords = {Economic and social effects; Neural networks; Cognitive science; Collective intelligences; Human intelligence; Human language; Human subjects; Individualism; Language model; Neural-networks; Statistic modeling; Training data; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Li2025155,
	author = {Li, Bowen and Wang, Yongzh and Ding, Zhengjiang and Wang, Bin and Wen, Shibo and Dong, Yuhao and Ji, Zheng},
	title = {Intelligent search technology for Jiaodong gold mines based on large models and GraphRAG},
	year = {2025},
	journal = {Earth Science Frontiers},
	volume = {32},
	number = {4},
	pages = {155 - 164},
	doi = {10.13745/j.esf.sf.2025.4.77},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014886324&doi=10.13745%2Fj.esf.sf.2025.4.77&partnerID=40&md5=79ba9877bf17de141655dfdeb22a5f11},
	abstract = {The Jiaodong gold deposit is a major concentration area of gold resources in eastern China, characterized by complex geological information and an extensive knowledge system. Traditional information retrieval methods struggle to meet the advanced demands of semantic understanding and knowledge reasoning in mineral exploration. To enhance geological knowledge service efficiency, this study develops an intelligent question-answering system for the Jiaodong gold deposit domain based on GraphRAG (Graph-enhanced Retrieval-Augmented Generation) technology. The research utilizes academic papers from CNKI as the corpus, employs OCR and large language models (LLMs) for text parsing and semantic standardization to establish an ontological knowledge system covering core concepts such as mineralization types, ore-controlling structures, and mineral assemblages. The system uses prompt engineering-driven LLMs to automatically extract entities and relationships, constructing a structured knowledge graph integrated into Neo4j. Furthermore, by combining semantic embedding with community clustering algorithms, a knowledge indexing network enables natural language question answering, semantic query expansion, and knowledge provenance. Evaluation results demonstrate the system’s superiority over traditional RAG and general models (e.g., ChatGPT-4o) in answer accuracy, contextual precision, and knowledge interpretability, exhibiting enhanced professional adaptability and reasoning capabilities. The findings provide a novel technical pathway for intelligent information services in gold deposits and theoretical support for knowledge of graph-enhanced language models in geoscience knowledge management. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Rag; Jiaodong Gold Deposit; Knowledge Graph; Knowledge Question Answering; Large Language Mode; Clustering Algorithms; Economic Geology; Gold Mines; Information Retrieval; Knowledge Graph; Knowledge Organization; Mineral Exploration; Ontology; Query Processing; Question Answering; Search Engines; Semantic Web; Semantics; Graph Rag; Intelligent Search; Jiaodong; Jiaodong Gold Deposit; Knowledge Graphs; Knowledge Question Answering; Knowledge System; Language Model; Large Language Mode; Question Answering; Gold Deposits},
	keywords = {Clustering algorithms; Economic geology; Gold mines; Information retrieval; Knowledge graph; Knowledge organization; Mineral exploration; Ontology; Query processing; Question answering; Search engines; Semantic Web; Semantics; Graph RAG; Intelligent search; Jiaodong; Jiaodong gold deposit; Knowledge graphs; Knowledge question answering; Knowledge system; Language model; Large language mode; Question Answering; Gold deposits},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Zheng2025588,
	author = {Zheng, Hanqi and Ouyang, Guige and Huang, Yongzhong},
	title = {An Unsupervised Ontology Construction Method Based on Pre-trained Language Model},
	year = {2025},
	pages = {588 - 595},
	doi = {10.1145/3730436.3730532},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013077743&doi=10.1145%2F3730436.3730532&partnerID=40&md5=e921376d1dd8149b1f6af466fd0859fb},
	abstract = {With the fast growth of text data, the importance of automatic ontology construction has grown significantly. The proposed article provides a novel approach by applying pre-trained language models to automatically construct the ontology. The framework consists of two sequential phases: automatic concept discovery and automatic relation discovery. In the context of automatic concept discovery, instances and their embedding vectors are extracted first through Named Entity Recognition (NER). Then, the unsupervised affinity propagation (AP) clustering algorithm is applied to classify these embedding vectors, resulting in the discovery of the concepts. A denoising method is discussed to obtain higher accuracy with respect to the concepts obtained to reduce noise caused by complete clustering. Related to automatic relation discovery between concepts (mentioned in the next section), the process generates inexplicable concepts that are contextually similar based on the embedded vectors of instances mapping over the two entities. This can enable unsupervised automatic discovery of relations between the contextually related concepts. This method shows a certain feasibility and achieves early effectiveness in unsupervised automatic ontology construction with the experimental results. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Affinity Propagation; Automatic Ontology Construction; Knowledge Graph; Pre-trained Language Model; Clustering Algorithms; Data Mining; Embeddings; Knowledge Graph; Affinity Propagation; Automatic Ontology; Automatic Ontology Construction; Concept Discoveries; Construction Method; Knowledge Graphs; Language Model; Ontology Construction; Pre-trained Language Model; Ontology},
	keywords = {Clustering algorithms; Data mining; Embeddings; Knowledge graph; Affinity propagation; Automatic ontology; Automatic ontology construction; Concept discoveries; Construction method; Knowledge graphs; Language model; Ontology construction; Pre-trained language model; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Xiong2025337,
	author = {Xiong, Qiushi and Xu, Zhipeng and Liu, Zhenghao and Wang, Mengjia and Chen, Zulong and Sun, Yue and Gu, Yu and Li, Xiaohua and Yu, Ge},
	title = {Enhancing the Patent Matching Capability of Large Language Models via the Memory Graph},
	year = {2025},
	pages = {337 - 347},
	doi = {10.1145/3726302.3729970},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011826206&doi=10.1145%2F3726302.3729970&partnerID=40&md5=d453007e3d1034f1f9b64fd51c40defe},
	abstract = {Intellectual Property (IP) management involves strategically protecting and utilizing intellectual assets to enhance organizational innovation, competitiveness, and value creation. Patent matching is a crucial task in intellectual property management, which facilitates the organization and utilization of patents. Existing models often rely on the emergent capabilities of Large Language Models (LLMs) and leverage them to identify related patents directly. However, these methods usually depend on matching keywords and overlook the hierarchical classification and categorical relationships of patents. In this paper, we propose MemGraph, a method that augments the patent matching capabilities of LLMs by incorporating a memory graph derived from their parametric memory. Specifically, MemGraph prompts LLMs to traverse their memory to identify relevant entities within patents, followed by attributing these entities to corresponding ontologies. After traversing the memory graph, we utilize extracted entities and ontologies to improve the capability of LLM in comprehending the semantics of patents. Experimental results on the PatentMatch dataset demonstrate the effectiveness of MemGraph, achieving a 17.68% performance improvement over baseline LLMs. The further analysis highlights the generalization ability of MemGraph across various LLMs, both in-domain and out-of-domain, and its capacity to enhance the internal reasoning processes of LLMs during patent matching. All data and codes are available at https://github.com/NEUIR/MemGraph. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Memory Graph; Patent Matching; Retrieval-augmented Generation; Copyrights; Ontology; Patents And Inventions; Intellectual Assets; Intellectual Property Management; Language Model; Large Language Model; Matchings; Memory Graph; Ontology's; Organizational Innovation; Patent Matching; Retrieval-augmented Generation; Semantics},
	keywords = {Copyrights; Ontology; Patents and inventions; Intellectual assets; Intellectual property management; Language model; Large language model; Matchings; Memory graph; Ontology's; Organizational innovation; Patent matching; Retrieval-augmented generation; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Zhao20254360,
	author = {Zhao, Mengyue and Nokleby, Matthew S. and Shen, Bo and Dong, Wenbo and Pachauri, Deepti and Yang, Andrew},
	title = {Ontology-Guided Knowledge Graph Retrieval for Multi-Hop and Cross-Granularity Store Fulfillment Queries},
	year = {2025},
	pages = {4360 - 4364},
	doi = {10.1145/3726302.3731964},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011821757&doi=10.1145%2F3726302.3731964&partnerID=40&md5=0dfc1b7c03245d565af1cb6d6d95d2ad},
	abstract = {Answering complex queries in store fulfillment, such as "What percentage of employee-assigned actions remain unresolved?" or "How many worklists for a specific product type were completed within a timeframe at each location?" requires precise, multi-hop reasoning across datasets with varying granularities. This paper introduces an ontology-based knowledge graph (KG) approach integrated with a structured text-to-Cypher generation pipeline, enabling accurate retrieval for such queries. Benchmarking against a robust hybrid search baseline combining BM25 and semantic search, our method demonstrates superior performance in addressing multi-hop and cross-granularity questions. Leveraging a KG schema designed to capture intricate relationships (e.g., (OrderLineItem)-[:INVOLVES_ACTION]>(Action)-[:INVOLVES]->(BatchProcess)[:IS_COMPLETED_AT]->(Location)), we reveal universal patterns for constructing and querying highly relational data. This work highlights the transformative potential of ontology-driven KGs to improve reasoning, data aggregation, and decision-making, with broader implications for any domain requiring structured, multi-relational data analysis. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Hybrid Search; Knowledge Graph; Large Language Model; Multi-hop Reasoning; Natural Language Processing; Ontology-driven Retrieval; Store Fulfillment; Text-to-cypher; Computational Linguistics; Data Aggregation; Decision Making; Human Engineering; Information Retrieval; Knowledge Graph; Knowledge Management; Ontology; Query Processing; Semantics; Hybrid Search; Knowledge Graphs; Language Model; Language Processing; Large Language Model; Multi-hop Reasoning; Multi-hops; Natural Language Processing; Natural Languages; Ontology's; Ontology-driven Retrieval; Store Fulfillment; Text-to-cipher; Natural Language Processing Systems},
	keywords = {Computational linguistics; Data aggregation; Decision making; Human engineering; Information retrieval; Knowledge graph; Knowledge management; Ontology; Query processing; Semantics; Hybrid search; Knowledge graphs; Language model; Language processing; Large language model; Multi-hop reasoning; Multi-hops; Natural language processing; Natural languages; Ontology's; Ontology-driven retrieval; Store fulfillment; Text-to-cipher; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhao2025,
	author = {Zhao, Chuang and Tang, Hui and Zhao, Hongke and Li, Xiaomeng},
	title = {Beyond Sequential Patterns: Rethinking Healthcare Predictions with Contextual Insights},
	year = {2025},
	journal = {ACM Transactions on Information Systems},
	volume = {43},
	number = {4},
	pages = {},
	doi = {10.1145/3733234},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011390504&doi=10.1145%2F3733234&partnerID=40&md5=ccac77d0d85dab141257e8b453b26a76},
	abstract = {Healthcare predictions, such as readmission prediction, stand as a cornerstone of societal well-being, exerting a profound influence on individual health outcomes and communal vitality. Existing research primarily employs advanced graph neural networks and sequential algorithms for patient modeling, with a focus on discerning the connections and sequential patterns inherent in Electronic Health Records (EHRs). However, the heterogeneity of entity interactions, the locality of EHR data, and the oversight of target relevance hinder further improvements. To address these limitations, we introduce a novel framework Beyond Sequential Patterns (BSP), which facilitates precise healthcare predictions by incorporating tri-contextual information. Specifically, we establish a symptom-driven hypergraph network with four semantic hyperedges tailored to the intricacies of the healthcare scenario, such as ontology. This serves as a global context, tracking the heterogeneous entity collaboration within and across patients. Moreover, we construct an extensive knowledge graph leveraging existing medical databases and large language models. By sampling and refining knowledge subgraphs as local context, we bolster the semantic associations of medical entities from closed-set EHR data to the open world. Finally, we introduce the candidate context, an explicit entity-relation loss. It enforces the neighbor consistency between the target and the representation during optimization, thus accounting for correlations among targets. Extensive experiments and rigorous robustness analysis on five tasks derived from four large medical datasets underscore the BSP's superiority over the leading baselines, with improvements of 11%, 3%, 11%, 3.5%, and 2% across five tasks, demonstrating the efficacy of incorporating diverse contexts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Healthcare Prediction; Hypergraph Learning; Large Language Model; Electronic Health Record; Health Care; Knowledge Graph; Knowledge Management; Medical Computing; Medical Informatics; Semantics; Electronic Health; Health Outcomes; Health Records; Healthcare Prediction; Hyper Graph; Hypergraph Learning; Language Model; Large Language Model; Sequential Patterns; Well Being; Forecasting},
	keywords = {Electronic health record; Health care; Knowledge graph; Knowledge management; Medical computing; Medical informatics; Semantics; Electronic health; Health outcomes; Health records; Healthcare prediction; Hyper graph; Hypergraph learning; Language model; Large language model; Sequential patterns; Well being; Forecasting},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Yang2025174,
	author = {Yang, Jiaxing and Wen, Peihan and Hu, Yaping},
	title = {Research on Knowledge Graph Construction of Nuclear Power Equipment Quality Control with Value Chain Collaboration; 核电装备价值链协同质量管控知识图谱构建研究},
	year = {2025},
	journal = {Jixie Gongcheng Xuebao/Chinese Journal of Mechanical Engineering},
	volume = {61},
	number = {13},
	pages = {174 - 191},
	doi = {10.3901/JME.2025.13.174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012904208&doi=10.3901%2FJME.2025.13.174&partnerID=40&md5=e6895c850f6da861f74da628791ba455},
	abstract = {To address the issues of information isolation, lack of connectivity, and data loss and deficiency during the flow of information between organizations and platforms in the value chain of nuclear power equipment, the quality control requirements of the nuclear power equipment value chain are analyzed. The knowledge graph, whose construction and application are further explored, introduced to organize the knowledge embedded in quality texts, facilitating sharing and reusing, to support collaborative quality control. Firstly, a multi-stage hierarchical ontology model across time and space is designed based on an analysis of characteristics of quality texts in the nuclear power equipment value chain. Secondly, an improved few-shot-learning-based entity recognition model and a relative position-based entity relationship matching method are proposed, and then knowledge triples are extracted to construct the knowledge graph. Thirdly, through comprehensive scenario analysis and expert interviews, a set of question-and-answer templates for user interaction with the knowledge graph is designed, a frame based on Retrieval Augmented Generation is defined, and an application for questions and answers aiming at collaborative quality control in the nuclear power equipment value chain is developed. Finally, the effectiveness and superiority of the above methods are validated by comparing with the large language models ChatGPT and keyword matching model. The improved entity recognition model demonstrates better performance in handling smaller quality texts. And the nuclear power equipment quality control with value chain collaboration can be effectively supported through user interaction with the knowledge graph, which contributes positively to the improvement of management levels and efficiency. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Meta-learning; Quality Control; Value Chain Collaboration; Chains; Construction Equipment; Knowledge Graph; Learning Systems; Nuclear Energy; Nuclear Fuels; Ontology; Quality Assurance; User Interfaces; Entity Recognition; Equipment Quality; Graph Construction; Knowledge Graphs; Metalearning; Nuclear Power Equipments; Recognition Models; User Interaction; Value Chain Collaboration; Value Chains; Quality Control},
	keywords = {Chains; Construction equipment; Knowledge graph; Learning systems; Nuclear energy; Nuclear fuels; Ontology; Quality assurance; User interfaces; Entity recognition; Equipment quality; Graph construction; Knowledge graphs; Metalearning; Nuclear power equipments; Recognition models; User interaction; Value chain collaboration; Value chains; Quality control},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lima Carneiro Alves de2025423,
	author = {Lima Carneiro Alves de, Bruno Rucy and Kramer, Merlin and Pinheiro, Victor Henrique Cabral},
	title = {Distributed Incremental Ontology Reasoning over Dynamic T-boxes},
	year = {2025},
	pages = {423 - 428},
	doi = {10.1145/3719384.3719446},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011704495&doi=10.1145%2F3719384.3719446&partnerID=40&md5=53605e9aae97f467b3f0e4ffde60efd1},
	abstract = {With the advent of Retrieval Augmented Generation (RAG), Knowledge Graphs (KGs) have yet again had a surge in interest in both Academia and Industry, as their use allows for extending the context of Large Language Models (LLMs) by combining traditional vector search with reasoning over Ontologies or Property Graphs encoded as KGs. RAG is a highly dynamic scenario, where the LLM agent might not only retrieve information from a KG or vector store but mutate it as well. This implies eventually there being a greater demand for equally-dynamic KG reasoning systems. We provide a solution to this for the popular ontology language RDF-Schema (RDFS) by showing that computing entailment as a bottom-up query over RDFS graphs with dynamic Terminological Boxes (TBox) and Assertional Boxes (ABox), those where edges and nodes belonging to both boxes can be freely added and removed, can be expressed as an incremental DBSP computation. This computation is then implemented with the distributed computation framework Differential Dataflow (DD), that subsumes DBSP, and compared with a state-of-the-art commercial ontology reasoner. We find that our approach provides more even performance across additions and deletions and a higher potential for scalability across benchmarks with up to 250 GBs of data. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology Reasoning; Rdfs; Stream Processing; Benchmarking; Data Flow Analysis; Distributed Computer Systems; Graph Theory; Knowledge Graph; Knowledge Management; Query Processing; Resource Description Framework (rdf); Dynamic Scenarios; Knowledge Graphs; Language Model; Model Agents; Ontology Reasonings; Ontology's; Property; Rdf Schemas; Rdfs; Stream Processing; Ontology},
	keywords = {Benchmarking; Data flow analysis; Distributed computer systems; Graph theory; Knowledge graph; Knowledge management; Query processing; Resource Description Framework (RDF); Dynamic scenarios; Knowledge graphs; Language model; Model agents; Ontology reasonings; Ontology's; Property; RDF schemas; Rdfs; Stream processing; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhu2025,
	author = {Zhu, Yiheng and Zhu, Shuxin and Yu, Xuan and Yan, He and Liu, Yan and Xie, Xiaojun and Yu, Dongjun and Ye, Rui},
	title = {MKFGO: integrating multi-source knowledge fusion with pretrained language model for high-accuracy protein function prediction},
	year = {2025},
	journal = {Briefings in Bioinformatics},
	volume = {26},
	number = {4},
	pages = {},
	doi = {10.1093/bib/bbaf420},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013381854&doi=10.1093%2Fbib%2Fbbaf420&partnerID=40&md5=b8e1a2eecf7ebff9c704dd35223ac874},
	abstract = {Accurately identifying protein functions is essential to understand life mechanisms and thus advance drug discovery. Although biochemical experiments are the gold standard for determining protein functions, they are often time-consuming and labor-intensive. Here, we proposed a novel composite deep-learning method, Multi-source Knowledge Fusion for Gene Ontology prediction (MKFGO), to infer Gene Ontology (GO) attributes through integrating five complementary pipelines built on multi-source biological data. MKFGO was rigorously benchmarked on 1522 nonredundant proteins, demonstrating superior performance over 12 state-of-the-art function prediction methods. Comprehensive data analyses revealed that the major advantage of MKFGO lies in its two deep-learning components, handcrafted feature representation–based GO prediction (HFRGO) and protein large language model (PLM)–based GO prediction (PLMGO), which derive handcrafted features and PLM–based features, respectively, from protein sequences in different biological views, with effective knowledge fusion at the decision-level. HFRGO leverages a long short-term memory (LSTM)–attention network embedded with handcrafted features, in which the triplet loss–based guilt-by-association strategy is designed to enhance the correlation between feature similarity and function similarity. PLMGO employs the PLM to capture feature embeddings with discriminative functional patterns from sequences. Meanwhile, another three components provide complementary insights for further improving prediction accuracy, driven by protein–protein interaction, GO term probability, and protein-coding gene sequence, respectively. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Lstm-attention Network; Multi-source Knowledge Fusion; Pretrained Language Model; Protein Function; Protein; Proteins; Amino Acid Sequence; Article; Attention Network; Controlled Study; Data Analysis; Deep Learning; Diagnostic Test Accuracy Study; Gene Ontology; Gene Sequence; Guilt; Language Model; Large Language Model; Long Short Term Memory Network; Prediction; Probability; Protein Function; Protein Protein Interaction; Bioinformatics; Chemistry; Genetics; Human; Metabolism; Procedures; Protein Database; Software; Protein; Computational Biology; Databases, Protein; Deep Learning; Gene Ontology; Humans; Proteins; Software},
	keywords = {amino acid sequence; article; attention network; controlled study; data analysis; deep learning; diagnostic test accuracy study; gene ontology; gene sequence; guilt; language model; large language model; long short term memory network; prediction; probability; protein function; protein protein interaction; bioinformatics; chemistry; genetics; human; metabolism; procedures; protein database; software; protein; Computational Biology; Databases, Protein; Deep Learning; Gene Ontology; Humans; Proteins; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Borhani2025,
	author = {Borhani, Niloofar and Izadi, Iman and Motahharynia, Ali and Sheikholeslami, Mahsa and Gheisari, Yousof},
	title = {DrugTar improves druggability prediction by integrating large language models and gene ontologies},
	year = {2025},
	journal = {Bioinformatics},
	volume = {41},
	number = {7},
	pages = {},
	doi = {10.1093/bioinformatics/btaf360},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012873980&doi=10.1093%2Fbioinformatics%2Fbtaf360&partnerID=40&md5=d5101d0ccc9de99193b84463c652fcae},
	abstract = {Motivation Target discovery is crucial in drug development, especially for complex chronic diseases. Recent advances in high-throughput technologies and the explosion of biomedical data have highlighted the potential of computational druggability prediction methods. However, most current methods rely on sequence-based features with machine learning, which often face challenges related to hand-crafted features, reproducibility, and accessibility. Moreover, the potential of raw sequence and protein structure has not been fully investigated. Results Here, we leveraged both protein sequence and structure using deep learning techniques, revealing that protein sequence, especially pre-trained embeddings, is more informative than protein structure. Next, we developed DrugTar, a high-performance deep learning algorithm integrating sequence embeddings from the ESM-2 pre-trained protein language model with gene ontologies to predict druggability. DrugTar achieved areas under the curve and precision-recall curve values of 0.94, outperforming state-of-the-art methods. In conclusion, DrugTar streamlines target discovery as a bottleneck in developing novel therapeutics. Availability and implementation DrugTar is available as a web server at www.DrugTar.com. The data and source code are at https://github.com/NBorhani/DrugTar. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Protein; Algorithm; Bioinformatics; Chemistry; Deep Learning; Drug Development; Gene Ontology; Genetics; Human; Large Language Model; Procedures; Software; Algorithms; Computational Biology; Deep Learning; Drug Discovery; Gene Ontology; Humans; Large Language Models; Proteins; Software},
	keywords = {protein; algorithm; bioinformatics; chemistry; deep learning; drug development; gene ontology; genetics; human; large language model; procedures; software; Algorithms; Computational Biology; Deep Learning; Drug Discovery; Gene Ontology; Humans; Large Language Models; Proteins; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Li2025102,
	author = {Li, Hui and null, null and Huang, Hanbing and Chen, Chao and Yuan, Ying and Huang, Zongcai},
	title = {Construction of remote sensing knowledge graph for typical tree species in subtropical region considering spatiotemporal features; 顾及时空特征的亚热带地区典型树种遥感知识图谱构建},
	year = {2025},
	journal = {Beijing Linye Daxue Xuebao/Journal of Beijing Forestry University},
	volume = {47},
	number = {7},
	pages = {102 - 116},
	doi = {10.12171/j.1000-1522.20250124},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012123570&doi=10.12171%2Fj.1000-1522.20250124&partnerID=40&md5=58bfa7dbe9432c5bdf0b8ff54da3fa4e},
	abstract = {[Objective] Massive multi-source remote-sensing data are a critical resource for delivering high-precision services for typical tree species in subtropical regions. Yet the great diversity of species and the cloudy, rainy, mountainous environment characteristics of these regions complicate multi-source remote-sensing features, hindering clear spatio-temporal expression of both basic and composite (state and process) tree-species characteristics. Inspired by geographic knowledge mapping, this study integrated tree-species fundamentals, geomorphological patterns, and multi-source remote-sensing feature knowledge. It proposes a framework for constructing a spatio-temporal remote-sensing knowledge graph for typical subtropical tree species, and by incorporating the DeepSeek large language model (LLM), develops an intelligent Q&A system organizing and expressing tree-species remote-sensing knowledge more effectively. [Method] To achieve the above objectives, the study first organized and integrated the basic concepts of tree species, geographic-law knowledge, and remote-sensing interpretation information at three levels: data knowledge, conceptual knowledge, and rule knowledge to build a comprehensive system describing the spatial distribution, temporal dynamics, and geographic context of subtropical tree species. Taking forest class as the interpretation unit, ontology modelling was used to build tree-species ontology knowledge and remote-sensing ontology knowledge for individual temporal states. Temporal and spatial correlations reflecting tree-growth evolution were then introduced to form a spatio-temporal sequence ontology model and to extract relationships among core elements of remote-sensing knowledge for typical subtropical tree species. In addition, forest-inventory data, Baidu Encyclopaedia entries, and remote-sensing pre-interpretation results were fused to enrich the data layer with ontology and attribute information. Finally, entity relationships were stored in a Neo4j graph database to construct the spatio-temporal remote-sensing knowledge graph, while the DeepSeek LLM deployed a local knowledge base to enable knowledge-graph-driven interactive Q&A services. [Result] The study successfully constructed a spatio-temporal remote-sensing knowledge graph for typical subtropical tree species, and through deep integration with DeepSeek, developed an intelligent Q&A system supporting natural-language interaction. The system enables associated queries and map-based visualisation of tree-species knowledge, geomorphological knowledge, and remote-sensing knowledge across different states. [Conclusion] The proposed approach realises the transformation from “data to knowledge” for subtropical tree species and through the synergistic computing of DeepSeek and the knowledge graph, offers flexible, convenient remote-sensing knowledge queries and services. It supplies data support for knowledge-guided, intelligent remote-sensing interpretation of typical subtropical tree species and provides a demonstrative application for remote-sensing knowledge mapping in other domains. Future work will expand the scale of the knowledge graph to meet the demand for high-quality knowledge services in high-precision forestry remote-sensing monitoring. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Knowledge Service; Multi-source Remote Sensing Data; Spatiotemporal Characteristics; Subtropical Area; Tree Species},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wang2025,
	author = {Wang, Boyan and Geng, Yangliao and Cheng, Xingyi and Chen, Bo and Bei, Zhilei and Wang, Wei and Tang, Jie and Song, Le},
	title = {ProtGO: universal protein function prediction utilizing multi-modal gene ontology knowledge},
	year = {2025},
	journal = {Bioinformatics},
	volume = {41},
	number = {7},
	pages = {},
	doi = {10.1093/bioinformatics/btaf390},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011670125&doi=10.1093%2Fbioinformatics%2Fbtaf390&partnerID=40&md5=0a97cc4102a4b6dde4c7df1d377b890c},
	abstract = {Motivation: As one of the recalcitrant challenges in life sciences and biomedicine, protein function prediction suffers from a deluge of AI-designed proteins, particularly having to face multi-modal information in the era of big data. Importing the high-throughput neural-network-based prediction framework to replace the low-throughput biological experiments, a universal multi-modal method is straightforward in addressing the growing gap between known sequences and predicting functions. Results: To bridge the gap, we propose ProtGO, a three-step framework for predicting protein function, which leverages the credible Gene Ontology (GO) knowledge base and integrates four common modalities. Specifically, we first introduce frontier pre-trained protein language models (PLMs) for representation learning of mainstay functional protein sequences. For the remaining multi-modal data, we design a text alignment module for explainable text descriptions, a taxonomy encoding module for species-specific taxonomy, and a GO graph embedding module for biological GO relations. Each module is independent and adaptive for the referenced modalities. By harnessing these four knowledge representations, ProtGO maximizes the potential of GO resources, enhancing the performance of vanilla PLMs and biological language models (LMs) in downstream GO prediction tasks. Extensive experiments demonstrate that ProtGO significantly advances the abilities of state-of-the-art PLMs to predict protein functions: approximately 8% to 27% increase in the maximum F1 measure (Fmax) compared to base models. These comprehensive studies confirm ProtGO's capability to deliver outstanding performance in protein function prediction by utilizing a rich blend of functional and evolutionary knowledge. Availability and implementation Our source code and all the data are available at https://github.com/sunyatawang/ProtGO. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Amino Acid Sequence; Article; Big Data; Controlled Study; Diagnosis; Drug Therapy; Feature Learning (machine Learning); Gene Ontology; Human; Language Model; Nerve Cell Network; Nonhuman; Prediction; Protein Function; Protein Language Model; Vanilla; Artificial Neural Network; Bioinformatics; Chemistry; Genetics; Metabolism; Physiology; Procedures; Protein Database; Software; Protein; Computational Biology; Databases, Protein; Gene Ontology; Neural Networks, Computer; Proteins; Software},
	keywords = {amino acid sequence; article; big data; controlled study; diagnosis; drug therapy; feature learning (machine learning); gene ontology; human; language model; nerve cell network; nonhuman; prediction; protein function; protein language model; Vanilla; artificial neural network; bioinformatics; chemistry; genetics; metabolism; physiology; procedures; protein database; software; protein; Computational Biology; Databases, Protein; Gene Ontology; Neural Networks, Computer; Proteins; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Berlato2025,
	author = {Berlato, Michele and Binni, Leonardo and Durmus, Dilan and Gatto, Chiara and Giusti, Letizia and Massari, Alessia and Toldo, Beatrice Maria and Cascone, Stefano and Mirarchi, Claudio},
	title = {Digital Platforms for the Built Environment: A Systematic Review Across Sectors and Scales},
	year = {2025},
	journal = {Buildings},
	volume = {15},
	number = {14},
	pages = {},
	doi = {10.3390/buildings15142432},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011647207&doi=10.3390%2Fbuildings15142432&partnerID=40&md5=054c9aab07410e57997e351e05b4e84b},
	abstract = {The digital transformation of the Architecture, Engineering and Construction sector is accelerating the adoption of digital platforms as critical enablers of data integration, stakeholder collaboration and process optimization. This paper presents a systematic review of 125 peer-reviewed journal articles (2015–2025), selected through a PRISMA-guided search using the Scopus database, with inclusion criteria focused on English-language academic literature on platform-enabled digitalization in the built environment. Studies were grouped into six thematic domains, i.e., artificial intelligence in construction, digital twin integration, lifecycle cost management, BIM-GIS for underground utilities, energy systems and public administration, based on a combination of literature precedent and domain relevance. Unlike existing reviews focused on single technologies or sectors, this work offers a cross-sectoral synthesis, highlighting shared challenges and opportunities across disciplines and lifecycle stages. It identifies the functional roles, enabling technologies and systemic barriers affecting digital platform adoption, such as fragmented data sources, limited interoperability between systems and siloed organizational processes. These barriers hinder the development of integrated and adaptive digital ecosystems capable of supporting real-time decision-making, participatory planning and sustainable infrastructure management. The study advocates for modular, human-centered platforms underpinned by standardized ontologies, explainable AI and participatory governance models. It also highlights the importance of emerging technologies, including large language models and federated learning, as well as context-specific platform strategies, especially for applications in the Global South. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence (ai); Bim-gis Integration; Building Information Modeling (bim); Cross-sectoral Integration; Digital Twin; Lifecycle Cost Management; Participatory Governance; Sustainable Infrastructure; Architectural Design; Artificial Life; Building Information Model; Costs; Data Integration; Digital Twin; Information Management; Integration; Interoperability; Metadata; Public Administration; Real Time Systems; Sustainable Development; Artificial Intelligence; Building Information Modeling; Building Information Modeling-gis Integration; Building Information Modelling; Cost Management; Cross-sectoral Integration; Gis Integration; Lifecycle Cost Management; Lifecycle Costs; Participatory Governance; Sustainable Infrastructure; Life Cycle},
	keywords = {Architectural design; Artificial life; Building Information Model; Costs; Data integration; Digital twin; Information management; Integration; Interoperability; Metadata; Public administration; Real time systems; Sustainable development; Artificial intelligence; Building information modeling; Building information modeling-GIS integration; Building Information Modelling; Cost management; Cross-sectoral integration; GIS Integration; Lifecycle cost management; Lifecycle costs; Participatory governance; Sustainable infrastructure; Life cycle},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Cappelli2025,
	author = {Cappelli, Maria Assunta and Di Marzo Serugendo, Giovanna},
	title = {Methodological Exploration of Ontology Generation with a Dedicated Large Language Model},
	year = {2025},
	journal = {Electronics (Switzerland)},
	volume = {14},
	number = {14},
	pages = {},
	doi = {10.3390/electronics14142863},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011611837&doi=10.3390%2Felectronics14142863&partnerID=40&md5=bba003bc82fccbcb202e71d79931bc06},
	abstract = {Ontologies are essential tools for representing, organizing, and sharing knowledge across various domains. This study presents a methodology for ontology construction supported by large language models (LLMs), with an initial application in the automotive sector. Specifically, a user preference ontology for adaptive interfaces in autonomous machines was developed using ChatGPT-4o. Based on this case study, the results were generalized into a reusable methodology. The proposed workflow integrates classical ontology engineering methodologies with the generative and analytical capabilities of LLMs. Each phase follows well-established steps: domain definition, term elicitation, class hierarchy construction, property specification, formalization, population, and validation. A key innovation of this approach is the use of a guiding table that translates domain knowledge into structured prompts, ensuring consistency across iterative interactions with the LLM. Human experts play a continuous role throughout the process, refining definitions, resolving ambiguities, and validating outputs. The ontology was evaluated in terms of logical consistency, structural properties, semantic accuracy, and inferential completeness, confirming its correctness and coherence. Additional validation through SPARQL queries demonstrated its reasoning capabilities. This methodology is generalizable to other domains, if domain experts adapt the guiding table to the specific context. Despite the support provided by LLMs, domain expertise remains essential to guarantee conceptual rigor and practical relevance. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adaptive Interfaces; Automotive Domain; Autonomous Cars; Generative Ai; Human-in-the-loop; Knowledge Representation; Large Language Models (llms); Ontology; Ontology Development; Ontology Evaluation Metrics; Automotive Industry; Autonomous Vehicles; Computer Software Reusability; Domain Knowledge; Iterative Methods; Knowledge Management; Knowledge Representation; Query Processing; Semantics; Adaptive Interface; Automotive Domains; Autonomous Car; Evaluation Metrics; Generative Ai; Human-in-the-loop; Knowledge-representation; Language Model; Large Language Model; Ontology Development; Ontology Evaluation Metric; Ontology Evaluations; Ontology's; Ontology},
	keywords = {Automotive industry; Autonomous vehicles; Computer software reusability; Domain Knowledge; Iterative methods; Knowledge management; Knowledge representation; Query processing; Semantics; Adaptive interface; Automotive domains; Autonomous car; Evaluation metrics; Generative AI; Human-in-the-loop; Knowledge-representation; Language model; Large language model; Ontology development; Ontology evaluation metric; Ontology evaluations; Ontology's; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025,
	author = {Li, Zexu and Prabhu, Suraj P. and Popp, Zachary and Jain, Shubhi S. and Balakundi, Vijetha and Ang, Ting Fang Alvin and Au, Rhoda and Chen, Jinying},
	title = {A natural language processing approach to support biomedical data harmonization: Leveraging large language models},
	year = {2025},
	journal = {PLOS ONE},
	volume = {20},
	number = {7 July},
	pages = {},
	doi = {10.1371/journal.pone.0328262},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011525696&doi=10.1371%2Fjournal.pone.0328262&partnerID=40&md5=5789b5a68bba2e68124ac11fe8f60a74},
	abstract = {Background Biomedical research requires large, diverse samples to produce unbiased results. Retrospective data harmonization is often used to integrate existing datasets to create these samples, but the process is labor-intensive. Automated methods for matching variables across datasets can accelerate this process, particularly when harmonizing datasets with numerous variables and varied naming conventions. Research in this area has been limited, primarily focusing on lexical matching and ontology-based semantic matching. We aimed to develop new methods, leveraging large language models (LLMs) and ensemble learning, to automate variable matching. Methods This study utilized data from two GERAS cohort studies (European [EU] and Japan [JP]) obtained through the Alzheimer’s Disease (AD) Data Initiative’s AD workbench. We first manually created a dataset by matching 347 EU variables with 1322 candidate JP variables and treated matched variable pairs as positive instances and unmatched pairs as negative instances. We then developed four natural language processing (NLP) methods using state-of-the-art LLMs (E5, MPNet, MiniLM, and BioLORD-2023) to estimate variable similarity based on variable labels and derivation rules. A lexical matching method using fuzzy matching was included as a baseline model. In addition, we developed an ensemble-learning method, using the Random Forest (RF) model, to integrate individual NLP methods. RF was trained and evaluated on 50 trials. Each trial had a random split (4:1) of training and test sets, with the model’s hyperparameters optimized through cross-validation on the training set. For each EU variable, 1322 candidate JP variables were ranked based on NLP-derived similarity scores or RF’s probability scores, denoting their likelihood to match the EU variable. Ranking performance was measured by top-n hit ratio (HR-n) and mean reciprocal rank (MRR). Results E5 performed best among individual methods, achieving 0.898 HR-30 and 0.700 MRR. RF performed better than E5 on all metrics over 50 trials (P<0.001) and achieved an average HR-30 of 0.986 and MRR of 0.744. LLM-derived features contributed most to RF’s performance. One major cause of errors in automatic variable matching was ambiguous variable definitions. Conclusion NLP techniques (especially LLMs), combined with ensemble learning, hold great potential in automating variable matching and accelerating biomedical data harmonization. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Alzheimer Disease; Anthropometry; Article; Benchmarking; Bioinformatics; Biomedical Engineering; Body Mass; Classifier; Cognition; Cohort Analysis; Cross Validation; Daily Life Activity; Demographics; Feature Extraction; Female; Human; Hypertension; Large Language Model; Lexical Matching Method; Male; Mini Mental State Examination; Natural Language Processing; Ontology; Probability; Protein Misfolding; Questionnaire; Random Forest; Retrospective Study; Semantics; Japan; Language; Medical Research; Alzheimer Disease; Biomedical Research; Humans; Language; Large Language Models; Natural Language Processing; Semantics},
	keywords = {Alzheimer disease; anthropometry; Article; benchmarking; bioinformatics; biomedical engineering; body mass; classifier; cognition; cohort analysis; cross validation; daily life activity; demographics; feature extraction; female; human; hypertension; large language model; lexical matching method; male; Mini Mental State Examination; natural language processing; ontology; probability; protein misfolding; questionnaire; random forest; retrospective study; semantics; Japan; language; medical research; Alzheimer Disease; Biomedical Research; Humans; Language; Large Language Models; Natural Language Processing; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Arocha2025141,
	author = {Arocha, Jorge González},
	title = {Critical Phenomenology of Prompting in Artificial Intelligence; Fenomenología crítica del prompting en la inteligencia artificial},
	year = {2025},
	journal = {Sophia (Ecuador)},
	volume = {2025-July-December},
	number = {39},
	pages = {141 - 165},
	doi = {10.17163/soph.n39.2025.04},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011486050&doi=10.17163%2Fsoph.n39.2025.04&partnerID=40&md5=3f32ab990569f7f8fe37eba47c2ba072},
	abstract = {This paper analyzes the philosophy of prompting as a tool within the context of the rise of Artificial Intelligence (AI), particularly in large language models (LLMs). The topic is justified by the need to understand the prompt as a mediating space between human intentionality, language, and the sociopolitical structures that shape interactions with these technologies. The central objective is to examine how prompting reflects ethical, ontological, and epistemological tensions that arise in the construction of meaning within AI systems. Methodologically, the study adopts a critical-phenomenological approach, combining first-person experiences (user) with practical experimentation of prompts in different scenarios. The results demonstrate that the prompt is not merely a technical instruction but a discursive practice, where human decisions, such as the configuration of “parameters” (e. g., temperature and Top P), directly influence the outputs generated by AI systems. While these decisions appear technical, they carry significant ethical and epistemological implications that demand critical examination. The study concludes that it is essential to adopt an interdisciplinary approach that integrates technical development with philosophical reflection. This approach would foster an ethical, conscious, and responsible use of AI while recognizing the central role of humans in interactions with these emerging technologies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Chatgpt; Discourse; Language Models; Phenomenology; Prompting},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2025,
	author = {Liu, Yubao and Wang, Benrui and Yan, Bocheng and Jiang, Haiyue and Dai, Yinfei},
	title = {POSA-GO: Fusion of Hierarchical Gene Ontology and Protein Language Models for Protein Function Prediction},
	year = {2025},
	journal = {International Journal of Molecular Sciences},
	volume = {26},
	number = {13},
	pages = {},
	doi = {10.3390/ijms26136362},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010308057&doi=10.3390%2Fijms26136362&partnerID=40&md5=a532c98dae102e95b06447490243df72},
	abstract = {Protein function prediction plays a crucial role in uncovering the molecular mechanisms underlying life processes in the post-genomic era. However, with the widespread adoption of high-throughput sequencing technologies, the pace of protein function annotation significantly lags behind that of sequence discovery, highlighting the urgent need for more efficient and reliable predictive methods. To address the problem of existing methods ignoring the hierarchical structure of gene ontology terms and making it challenging to dynamically associate protein features with functional contexts, we propose a novel protein function prediction framework, termed Partial Order-Based Self-Attention for Gene Ontology (POSA-GO). This cross-modal collaborative modelling approach fuses GO terms with protein sequences. The model leverages the pre-trained language model ESM-2 to extract deep semantic features from protein sequences. Meanwhile, it transforms the partial order relationships among Gene Ontology (GO) terms into topological embeddings to capture their biological hierarchical dependencies. Furthermore, a multi-head self-attention mechanism is employed to dynamically model the association weights between proteins and GO terms, thereby enabling context-aware functional annotation. Comparative experiments on the CAFA3 and SwissProt datasets demonstrate that POSA-GO outperforms existing state-of-the-art methods in terms of Fmax and AUPR metrics, offering a promising solution for protein functional studies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontology; Multiple Self-attention Mechanisms; Protein Function Prediction; Protein Language Model; Protein; Proteins; Amino Acid Sequence; Article; Benchmarking; Controlled Study; Etiology; Gene Ontology; High Throughput Sequencing; Human; Language Model; Prediction; Protein Function; Protein Language Model; Swiss-prot; Bioinformatics; Chemistry; Genetics; Metabolism; Molecular Genetics; Procedures; Protein Database; Semantics; Protein; Computational Biology; Databases, Protein; Gene Ontology; Humans; Molecular Sequence Annotation; Proteins; Semantics},
	keywords = {amino acid sequence; article; benchmarking; controlled study; etiology; gene ontology; high throughput sequencing; human; language model; prediction; protein function; protein language model; SWISS-PROT; bioinformatics; chemistry; genetics; metabolism; molecular genetics; procedures; protein database; semantics; protein; Computational Biology; Databases, Protein; Gene Ontology; Humans; Molecular Sequence Annotation; Proteins; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Dar2025,
	author = {Dar, Rayees Ahmad and Hashmy, Rana and Anwar, Muhammad Shahid and Bohm, Patrik and Frnda, Jaroslav},
	title = {Semantic knowledge graph fusion for fake news detection: Unifying content-based features and evidence-based analysis in the COVID-19 infodemic},
	year = {2025},
	journal = {PLOS ONE},
	volume = {20},
	number = {7 July},
	pages = {},
	doi = {10.1371/journal.pone.0321919},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009646134&doi=10.1371%2Fjournal.pone.0321919&partnerID=40&md5=add7c60ab43d7253013c323d336dec0b},
	abstract = {In the era of digital communication, the rapid spread of information has brought both benefits and challenges. While it has democratized access to knowledge, it has also led to an increase in fake news, with significant societal repercussions. The COVID-19 pandemic has exacerbated this issue, resulting in what the World Health Organization has termed an “infodemic." In light of this, developing effective methods for detecting fake news is of paramount importance. In this paper, we introduce a novel approach that integrates knowledge graphs and Named Entity Recognition (NER) based on a biomedical language model to address the challenge of fake news detection. Our method aims to enhance detection accuracy by combining content analysis with entity-level insights. Our approach involves three key components. First, content analysis uses a contextual language model to capture the semantic context of the content, enabling the extraction of meaningful insights essential for identifying fake news. Second, the NER component, built on a biomedical language model, precisely identifies and categorizes entities within the content, offering a deeper understanding crucial for detecting misinformation in the biomedical domain. Finally, entity integration employs knowledge graph embeddings to transform identified entities into a format that facilitates enhanced processing and detection. By blending these components, our method creates a unified representation of the content, incorporating both semantic context and entity-based insights. This comprehensive approach significantly improves the accuracy of fake news detection. Our extensive experiments demonstrate the effectiveness of this method, particularly in the early identification of false information. The results underscore the potential of our approach as a powerful tool in combating misinformation, particularly in critical areas such as public health. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Nvidia Ai Server; Accuracy; Article; Content Analysis; Controlled Study; Convolutional Neural Network; Coronavirus Disease 2019; Disease Exacerbation; Disinformation; Electric Potential; Embedding; Encoding; Evidence Based Practice; Feature Extraction; Graph Attention Network; Graph Convolutional Network; Graphsage; Human; Immunization; Infodemic; Information; Knowledge; Knowledge Graph; Language Model; Long Short Term Memory Network; Mass Communication; Misinformation; Multilayer Perceptron; Natural Language Processing; Ontology; Open Access Publishing; Predictive Model; Public Health; Recurrent Neural Network; Semantics; Social Media; Training; World Health Organization; Deception; Epidemiology; Isolation And Purification; Pandemic; Severe Acute Respiratory Syndrome Coronavirus 2; Covid-19; Deception; Humans; Pandemics; Sars-cov-2; Semantics},
	keywords = {accuracy; Article; content analysis; controlled study; convolutional neural network; coronavirus disease 2019; disease exacerbation; disinformation; electric potential; embedding; encoding; evidence based practice; feature extraction; Graph Attention Network; Graph Convolutional Network; GraphSAGE; human; immunization; infodemic; information; knowledge; knowledge graph; language model; long short term memory network; mass communication; misinformation; multilayer perceptron; natural language processing; ontology; open access publishing; predictive model; public health; recurrent neural network; semantics; social media; training; World Health Organization; deception; epidemiology; isolation and purification; pandemic; Severe acute respiratory syndrome coronavirus 2; COVID-19; Deception; Humans; Pandemics; SARS-CoV-2; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Chen202514,
	author = {Chen, Wenjie and Hu, Zhengyin and Shi, Xi and Lu, Ying},
	title = {Research on Scientific and Technological Literature Complex Knowledge Object Extraction Fusing Knowledge Graph and Large Language Model; 融合知识图谱与大语言模型的科技文献复杂知识对象抽取研究},
	year = {2025},
	journal = {Journal of Modern Information},
	volume = {45},
	number = {7},
	pages = {14 - 63},
	doi = {10.3969/j.issn.1008-0821.2025.07.002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009636485&doi=10.3969%2Fj.issn.1008-0821.2025.07.002&partnerID=40&md5=59495fef0cbb592eb4aa160bc15ea7e8},
	abstract = {[ Purpose / Significance] The complex knowledge objects in scientific and technological literature provide fine-grained and comprehensive knowledge representation of the deep knowledge content in scientific and technological literature, which can effectively support data-driven scientific and knowledge discovery and is an important element of technological innovation. [Method/ Process] Firstly, the domain knowledge graph was constructed through steps such as lightweight ontology construction, BRAT knowledge annotation, and Neo4j knowledge storage. Next, the large language model ChatGLM2-6B was locally deployed and fine tuned through LoRA technology. Finally, based on the MOT mechanism, the knowledge in the knowledge graph was injected into the prompts, and complex knowledge objects were extracted from scientific literature through multiple rounds of Q&A with the large language model. [Result/ Conclusion] Taking organic solar cells(OSCs) as an example to verify the effectiveness of the method, the results show that the extraction method intergrating knowledge graph and large language model is superior to the extraction method supported by large language model alone, with improvements of 14. 1%, 10. 3%, and 12. 3% in accuracy P, recall R, and F1 score, respectively. Knowledge graph can enhance the ability of large language models to extract complex knowledge objects from scientific literature, and improve the efficiency and accuracy of scientific literature mining in the OSC field. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Extraction; Knowledge Graph; Large Language Model; Prompt Building},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025,
	author = {Li, Yang and Gao, Mengli and Bian, Jilong and Zhao, Kaiqi and Li, Dan and Wang, Guohua},
	title = {OntoGene: knowledge-enhanced BERT for promoter identification},
	year = {2025},
	journal = {Science China Information Sciences},
	volume = {68},
	number = {7},
	pages = {},
	doi = {10.1007/s11432-024-4481-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009246876&doi=10.1007%2Fs11432-024-4481-9&partnerID=40&md5=62490df1086dd6cd7a7a9d7deb679100},
	abstract = {Promoters are crucial parts of DNA sequences and play a significant role in understanding the process of gene regulation. Despite the existence of numerous methods for predicting promoters, they are typically built on single-sequence information and seldom incorporate external knowledge graphs (KGs). Knowledge graphs, however, can provide rich factual knowledge that enhances the accuracy of promoter identification. In this study, we propose a novel method called OntoGene, which integrates Gene Ontology (GO) into the bidirectional encoder representations from transformers model (BERT) for promoter identification. Specifically, OntoGene encodes both gene sequences and external knowledge into vectors using a BERT encoder. Our method jointly optimizes knowledge embedding and masked language modeling objectives, and utilizes a contrastive learning method with knowledge-aware negative sampling to optimize gene and knowledge graph embeddings. OntoGene is a BERT-based model designed to predict not only the presence of promoters but also their strength (strong or weak promoters) from gene sequences. Experimental results demonstrate OntoGene’s high accuracy in predicting DNA promoters and their strengths, validating the effectiveness of this knowledge-enhanced approach. OntoGene and the dataset can be obtained from https://github.com/gaomengli7/OntoGene. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Knowledge Enhancement; Knowledge Graph; Promoter Identification; Dna; Gene Ontology; Genes; Graph Theory; Knowledge Graph; Learning Systems; Modeling Languages; Signal Encoding; Bidirectional Encoder Representation From Transformer Model; External Knowledge; Gene Sequences; Gene-regulation; Knowledge Enhancement; Knowledge Graphs; Promoter Identification; Sequence Informations; Single Sequences; Transformer Modeling; Dna Sequences},
	keywords = {DNA; Gene Ontology; Genes; Graph theory; Knowledge graph; Learning systems; Modeling languages; Signal encoding; Bidirectional encoder representation from transformer model; External knowledge; Gene sequences; Gene-regulation; Knowledge enhancement; Knowledge graphs; Promoter identification; Sequence informations; Single sequences; Transformer modeling; DNA sequences},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ponte2025,
	author = {Ponte, Daniel and Aguilar Torres, Eduardo and Ribera, Mireia and Radeva, Petia I.},
	title = {Multi-task visual food recognition by integrating an ontology supported with LLM},
	year = {2025},
	journal = {Journal of Visual Communication and Image Representation},
	volume = {110},
	pages = {},
	doi = {10.1016/j.jvcir.2025.104484},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006825216&doi=10.1016%2Fj.jvcir.2025.104484&partnerID=40&md5=e0b72744d7283cb596ec3ca71d7bf826},
	abstract = {Food image analysis is a crucial task with far-reaching implications across various domains, including culinary arts, nutrition, and food technology. This paper presents a novel approach to multi-task visual food analysis, using large language models to obtain recipes and support the creation of a comprehensive food ontology. The approach integrates the food ontology into an end-to-end model, with prior knowledge on the relationships of food concepts at different semantic levels, within a multi-task deep learning visual food analysis approach, to generate better and more consistent class predictions. Evaluated on two benchmark datasets, MAFood-121 and VireoFood-172, this method demonstrates its effectiveness in single-label food recognition and multi-label food group classification. The ontology enhances accuracy, consistency, and generalization by effectively transferring knowledge to the learning model. This study underscores the potential of ontology-based methods to address food image classification complexities, with implications for broad applications, including automated recipe generation and nutritional assessment. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Food Image Analysis; Food Ontology; Large Language Models; Multi-task Learning; Boning; Breadmaking; Brining; Food Chemistry; Food Ingredients; Food Preservation; Food Storage; Food Supply; Meats; Nutrients; Processed Foods; Food Image; Food Image Analyze; Food Ontology; Image Analyze; Image-analysis; Language Model; Large Language Model; Multi Tasks; Multitask Learning; Ontology's; Food Preservatives},
	keywords = {Boning; Breadmaking; Brining; Food chemistry; Food ingredients; Food preservation; Food storage; Food supply; Meats; Nutrients; Processed foods; Food image; Food image analyze; Food ontology; Image analyze; Image-analysis; Language model; Large language model; Multi tasks; Multitask learning; Ontology's; Food preservatives},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang2025,
	author = {Zhang, Dongliang and Ma, Gang and Qu, Tongming and Wang, Xudong and Zhou, Wei and Wang, Xiaomao},
	title = {A knowledge graph-enhanced large language model for question answering of hydraulic structure safety management},
	year = {2025},
	journal = {Advanced Engineering Informatics},
	volume = {66},
	pages = {},
	doi = {10.1016/j.aei.2025.103468},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005395178&doi=10.1016%2Fj.aei.2025.103468&partnerID=40&md5=3b54659b8d7a315b49c0c6f6656155fa},
	abstract = {Early detection and mitigation of hazards in hydraulic structures are crucial for effectively reducing economic and life losses. However, traditional hydraulic structure safety management methods rely on error-prone individual experience and emergency manuals, which are insufficient for making timely, scientifically informed decisions during crises. To address this challenge, this study presents an AI-driven framework for hydraulic structure safety management based on knowledge-based question answering. First, an ontology model was developed through a detailed analysis of safety management texts. Next, a partition fusion Kolmogorov-arnold network (PFKAN) enhanced with attention mechanisms was designed to jointly extract entities and relational knowledge. A safety management knowledge graph (KG) was then constructed from this knowledge. Subsequently, a large language model (LLM) was employed with a voting strategy to interpret query intent and extract relevant domain-specific knowledge from the KG. Finally, domain knowledge was integrated into the LLM to generate professional responses. Experimental results show that the F1 scores for entity and relation extraction with PFKAN reached 0.91 and 0.90, respectively, and the F1 score for query intent parsing with the voting strategy was 0.95, demonstrating competitive performance. The KG-enhanced LLM significantly improves decision-making in hydraulic structure safety management, providing an accurate and scalable tool for engineering safety managers. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain Knowledge Qa; Hydraulic Structure Safety Management; Intent Parsing; Knowledge Graph; Large Language Model; Health Hazards; Inference Engines; Knowledge Acquisition; Risk Management; Risk Perception; Domain Knowledge; Domain Knowledge Qa; Hydraulic Structure Safety Management; Intent Parsing; Knowledge Graphs; Language Model; Large Language Model; Question Answering; Safety Management; Structure Safety; Knowledge Graph},
	keywords = {Health hazards; Inference engines; Knowledge acquisition; Risk management; Risk perception; Domain knowledge; Domain knowledge QA; Hydraulic structure safety management; Intent parsing; Knowledge graphs; Language model; Large language model; Question Answering; Safety management; Structure safety; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Moayyed2025,
	author = {Moayyed, Erfan and Anumba, Chimay J. and Morteza, Azita},
	title = {Systematic analysis of large language models for automating document-to-smart contract transformation},
	year = {2025},
	journal = {Automation in Construction},
	volume = {175},
	pages = {},
	doi = {10.1016/j.autcon.2025.106209},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002568005&doi=10.1016%2Fj.autcon.2025.106209&partnerID=40&md5=b11cd26b93e073f503812eea2f2639d0},
	abstract = {Fragmentation and poor collaboration in contract-heavy industries hinder innovation. While smart contracts offer promising automation for digital documents, the transformation process presents significant challenges. Current approaches are promising but are often constrained by technical limitations, domain-specific requirements, and limited flexibility, restricting widespread adoption. This paper systematically reviews the development of smart contracts using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework to examine methodologies, challenges, and solutions through a thematic analysis of 30 key studies. The findings are grouped into three categories: Natural Language Processing (NLP)-based, template-based and ontology-based, and model-driven approaches. After analyzing the cross-industrial challenges of each category, this paper proposes a Large Language Model (LLM)-based smart contract generation solution to address the identified challenges validated through real-world use cases. This comprehensive analysis contributes to the ongoing dialogue on smart contracting, offering directions for future research and practical implementation in the digital infrastructure. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Blockchain; Data Conversion; Document Automation; Large Language Models (llms); Nlp; Smart Contracts; Systematic Review; Block-chain; Data Conversion; Document Automation; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Systematic Analysis; Systematic Review; Smart Contract},
	keywords = {Block-chain; Data conversion; Document automation; Language model; Language processing; Large language model; Natural language processing; Natural languages; Systematic analysis; Systematic Review; Smart contract},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang2025603,
	author = {Zhang, Chengxin},
	title = {Challenges and opportunities in text mining-based protein function annotation; 基于文本数据挖掘的蛋白功能预测:机遇与挑战},
	year = {2025},
	journal = {Synthetic Biology Journal},
	volume = {6},
	number = {3},
	pages = {603 - 616},
	doi = {10.12211/2096-8280.2025-002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012889829&doi=10.12211%2F2096-8280.2025-002&partnerID=40&md5=92e2bfaacf8a5a76129bc2d59272fe78},
	abstract = {Understanding the biological function of proteins is crucial for advancing quantitative synthetic biology. Except for a small number of model organisms, most species contain many proteins whose functions have not been experimentally verified, necessitating the development of accurate, automated protein function annotation methods. Recent progress in protein bioinformatics, particularly in predicting protein structures and functions, has been driven significantly by the application of artificial intelligence (AI) algorithms, with a notable emphasis on deep learning models. For instance, the top-ranked methods in recent Critical Assessment of Function Annotation (CAFA) challenge have used deep learning models, primarily large language models, to perform text mining-based protein function annotation. These methods either predict Gene Ontology (GO) terms directly from text features extracted from scientific literatures or from template proteins with databases. Despite the extensive work in developing increasingly powerful deep learning models for text mining-based protein function annotation, several major challenges have been overlooked when parsing scientific literature data. This manuscript reviews existing methods and challenges in protein function annotation. First, many text mining-based protein function predictors rely exclusively on PubMed abstracts collected by UniProt curators for the query protein, ignoring literatures that have not been reviewed by biocurators. Consequently, protein functions predicted by text mining might overlap with those from manual curation of the UniProt Gene Ontology Annotation. Second, nearly all methods only parse PubMed abstracts, ignoring the more informative full-text documents often available in the PubMed Central and Europe PMC repositories. Third, few studies have been proposed to automatically differentiate between different categories of literatures, such as low and high throughput experiments, and computational predictions. This manuscript also proposes promising approaches to enhance text mining-based protein function annotation using the latest development in AI, which is expected to contribute to the development of next-generation text mining tools for more accurate function annotation.[Figure Presented] © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biological Functions; Deep Learning; Gene Ontology (go) Terms; Proteins; Text Mining},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025,
	author = {Li, Wenhui and Huang, Baicheng and Guo, Menghao and Zeng, Zihan and Cai, Tao and Feng, Linqing and Zhang, Xinpeng and Guo, Ling and Jiang, Xianyue and Yin, Yanbin Bin},
	title = {Unveiling the evolution of antimicrobial peptides in gut microbes via foundation-model-powered framework},
	year = {2025},
	journal = {Cell Reports},
	volume = {44},
	number = {6},
	pages = {},
	doi = {10.1016/j.celrep.2025.115773},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006749567&doi=10.1016%2Fj.celrep.2025.115773&partnerID=40&md5=9538db309f9ea59b09babefc9c695661},
	abstract = {Antimicrobial resistance poses a major threat to public health, prompting the development of alternative therapies such as antimicrobial peptides (AMPs). Protein language models (PLMs) have advanced protein structure and function predictions, facilitating AMP discovery. We developed antimicrobial peptide structural evolution miner (AMP-SEMiner), an AI-driven framework that integrates PLMs, structural clustering, and evolutionary analysis to systematically identify AMPs encoded by small open reading frames and AMP-containing proteins in metagenome-assembled genomes. AMP-SEMiner identified over 1.6 million AMP candidates across diverse environments. Experimental validation showed antimicrobial activity in 9 of the 20 tested candidates, with 5 surpassing antibiotic effectiveness; variant peptides derived from these candidates similarly demonstrated strong antimicrobial efficacy. AMPs from human gut microbiomes revealed both conserved and adaptive evolutionary strategies, reflecting their dynamic ecological roles. AMP-SEMiner thus represents a valuable tool for expanding AMP discovery and has significant potential to inform the development of alternative antimicrobial treatments. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Antimicrobial Peptides; Cp: Microbiology; Evolution; Human Gut Microbiota; Protein Language Model; Protein Structure; Anti-bacterial Agents; Antimicrobial Peptides; Biopython V.1.79; Colabfold V1.5.5; Dm4b Upright Microscope; Dulbecco’s Modified Eagle Medium; Infinite Eplex Plate Reader; Mafft V7.508; Mmseq2; R Software Version 4.3.3; Antiinfective Agent; Polypeptide Antibiotic Agent; Transcriptome; Amino Acid Sequence; Antimicrobial Activity; Article; Bioinformatics; Circular Dichroism; Computer Model; Erythrocyte; Escherichia Coli; Evolution; Gene Expression; Gene Ontology; Hemolysis; Hydrophobicity; Intestine Flora; Mass Spectrometry; Metagenomics; Microbial Community; Microbiome; Nonhuman; Prediction; Protein Protein Interaction; Proteomics; Rank Sum Test; Sample Size; Transcriptomics; Chemistry; Genetics; Human; Molecular Evolution; Anti-bacterial Agents; Antimicrobial Peptides; Evolution, Molecular; Gastrointestinal Microbiome; Humans},
	keywords = {antiinfective agent; polypeptide antibiotic agent; transcriptome; amino acid sequence; antimicrobial activity; Article; bioinformatics; circular dichroism; computer model; erythrocyte; Escherichia coli; evolution; gene expression; gene ontology; hemolysis; hydrophobicity; intestine flora; mass spectrometry; metagenomics; microbial community; microbiome; nonhuman; prediction; protein protein interaction; proteomics; rank sum test; sample size; transcriptomics; chemistry; genetics; human; molecular evolution; Anti-Bacterial Agents; Antimicrobial Peptides; Evolution, Molecular; Gastrointestinal Microbiome; Humans},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hamed2025,
	author = {Hamed, Ahmed Abdeen and Crimi, Alessandro and Misiak, Magdalena M. and Lee, Byung-suk},
	title = {From knowledge generation to knowledge verification: examining the biomedical generative capabilities of ChatGPT},
	year = {2025},
	journal = {iScience},
	volume = {28},
	number = {6},
	pages = {},
	doi = {10.1016/j.isci.2025.112492},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004760776&doi=10.1016%2Fj.isci.2025.112492&partnerID=40&md5=d364b6295f0ccb9369a239eca153c09e},
	abstract = {The generative capabilities of LLM models offer opportunities for accelerating tasks but raise concerns about the authenticity of the knowledge they produce. We present a computational approach that evaluates the factual accuracy of biomedical knowledge generated by an LLM. Our approach consists of generating disease-centric associations and verifying them using biomedical ontologies. Using ChatGPT, we designed prompt-engineering processes to establish linkages between diseases and related drugs, symptoms, and genes, and assessed consistency across multiple ChatGPT models (e.g., GPT-4, GPT-4o, and GPT-4o-mini). Results demonstrate high accuracy in identifying disease terms (88%–97%), drug names (90%–91%), and genetic information (88%–98%). Symptom term identification was lower (49%–61%) due to informal symptom descriptions. Verification reveals coverage of 89%–91% for disease-drug and disease-gene pairs; symptom-related associations show lower coverage (49%–62%). Despite high term accuracy, generated IDs were often invalid or redundant. GenAI tools can be reliable if used with care. Retrieval Augmented Generation (RAG) may enhance reliability. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Health Informatics; Health Sciences; Health Technology; Medicine},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Ghafarollahi2025,
	author = {Ghafarollahi, Alireza and Buehler, Markus J.},
	title = {SciAgents: Automating Scientific Discovery Through Bioinspired Multi-Agent Intelligent Graph Reasoning},
	year = {2025},
	journal = {Advanced Materials},
	volume = {37},
	number = {22},
	pages = {},
	doi = {10.1002/adma.202413523},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212471317&doi=10.1002%2Fadma.202413523&partnerID=40&md5=a006af894529c47bad4a143dc72aac05},
	abstract = {A key challenge in artificial intelligence (AI) is the creation of systems capable of autonomously advancing scientific understanding by exploring novel domains, identifying complex patterns, and uncovering previously unseen connections in vast scientific data. In this work, SciAgents, an approach that leverages three core concepts is presented: (1) large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts, (2) a suite of large language models (LLMs) and data retrieval tools, and (3) multi-agent systems with in-situ learning capabilities. Applied to biologically inspired materials, SciAgents reveals hidden interdisciplinary relationships that were previously considered unrelated, achieving a scale, precision, and exploratory power that surpasses human research methods. The framework autonomously generates and refines research hypotheses, elucidating underlying mechanisms, design principles, and unexpected material properties. By integrating these capabilities in a modular fashion, the system yields material discoveries, critiques and improves existing hypotheses, retrieves up-to-date data about existing research, and highlights strengths and limitations. This is achieved by harnessing a “swarm of intelligence” similar to biological systems, providing new avenues for discovery. How this model accelerates the development of advanced materials by unlocking Nature's design principles, resulting in a new biocomposite with enhanced mechanical properties and improved sustainability through energy-efficient production is shown. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bio-inspired Materials; Biological Design; Knowledge Graph; Large Language Model; Materials Design; Multi-agent System; Natural Language Processing; Scientific Ai; Bio-inspired Materials; Biological Design; Knowledge Graphs; Language Model; Language Processing; Large Language Model; Materials Design; Multiagent Systems (mass); Natural Language Processing; Natural Languages; Scientific Artificial Intelligence; Knowledge Graph; Artificial Intelligence; Human; Artificial Intelligence; Humans},
	keywords = {Bio-inspired materials; Biological design; Knowledge graphs; Language model; Language processing; Large language model; Materials design; Multiagent systems (MASs); Natural language processing; Natural languages; Scientific artificial intelligence; Knowledge graph; artificial intelligence; human; Artificial Intelligence; Humans},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Heimann2025,
	author = {Heimann, Marc and Hübener, Anne Friederike},
	title = {Circling the void: Using Heidegger and Lacan to think about large language models},
	year = {2025},
	journal = {Cognitive Systems Research},
	volume = {91},
	pages = {},
	doi = {10.1016/j.cogsys.2025.101349},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219500386&doi=10.1016%2Fj.cogsys.2025.101349&partnerID=40&md5=0e0796a0874a87a8c0c264dc8f3dec3d},
	abstract = {The essay aims to unite two currently distinct lines of thinking and working with language. Large Language Models and continental philosophy, especially Martin Heidegger's thinking about language and, building upon Sigmund Freud, Jacques Lacan's structural psychoanalysis. We show that the concept of language that Heidegger, Freud and Lacan discuss and utilize in clinical frameworks is matched quite strongly by modern LLMs. This allows us to discuss a problem of negation and negativity that is central to the continental discourse but missing in current LLM research. This also means that we offer a radically different approach than is usual in the philosophy of artificial intelligence, since we base our concepts on thinkers that are often disregarded in the analytic philosophy discourse that is closer linked to AI research. To this end we also highlight, where the ontological differences of the proposed approach lie. However, our aim is to address AI researcher and continental philosophers. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai; Continental Philosophy; Heidegger; Lacan; Llm; Psychoanalytic Language Theory; 'current; Analytic Philosophy; Continental Philosophy; Heidegger; Lacan; Language Model; Language Theory; Llm; Psychoanalytic Language Theory; Psychoacoustic; Article; Artificial Intelligence; Diagnosis; Human; Large Language Model; Philosophy; Psychoanalysis},
	keywords = {'current; Analytic philosophy; Continental philosophy; Heidegger; Lacan; Language model; Language theory; LLM; Psychoanalytic language theory; Psychoacoustic; article; artificial intelligence; diagnosis; human; large language model; philosophy; psychoanalysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Dhanda2025,
	author = {Dhanda, Mandeep and Rogers, Benedict Alexander and Hall, Stephanie and Dekoninck, Elies A. and Dhokia, V. G.},
	title = {Reviewing human-robot collaboration in manufacturing: Opportunities and challenges in the context of industry 5.0},
	year = {2025},
	journal = {Robotics and Computer-Integrated Manufacturing},
	volume = {93},
	pages = {},
	doi = {10.1016/j.rcim.2024.102937},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214277411&doi=10.1016%2Fj.rcim.2024.102937&partnerID=40&md5=bfc35847cca22b93bfdae54b56fd128d},
	abstract = {Industry 4.0 (I4.0) has been characterized by the increasing use of automation, artificial intelligence, and big data in manufacturing. It has brought different machines, tools, robots and devices together through integration with cyber-physical systems as well as Internet of Things and computer systems. This has dramatically improved efficiency, productivity, and flexibility of automated systems, but it has also raised concerns about the impact of automation on jobs, the ethical considerations and the future of work in general. Industry 5.0 (I5.0) is the next manufacturing paradigm evolution and builds on I4.0 with the addition of ‘people’, in which robots will be designed to work alongside humans in a safe and efficient manner. Human-robot collaboration (HRC) is its key enabler. In manufacturing, HRC has the potential to improve safety, efficiency, and productivity by allowing humans to focus on tasks that require creativity, judgment, and flexibility, while robots perform more repetitive and dangerous tasks. This paper explores the concept of HRC and its advancement within 21st century industry. It identifies the opportunities and challenges arising from the interactions between robots and humans in manufacturing applications, assembly, and inspection. It also highlights the significance of HRC in I4.0 and its potential in I5.0. In addition, the role of artificial intelligence, machine learning, large language models, information modelling (ontologies) and new emerging digital technologies (augmented reality, virtual reality, digital twins, cyber-physical system) in the development of HRC and I5.0 is documented and discussed adding new perspectives to the growing literature in this area. This investigation sheds light on the emerging paradigms that have come about as parts of I5.0 and the transformative role of human-robot interaction in shaping the future of manufacturing. This critical review provides a realistic picture of manufacturing automation and the benefits and weaknesses of current HRC systems. It presents a researched view on the concept, needs, enabling technologies and system frameworks of human-robot interaction in manufacturing, providing a practical vision and research agenda for future work in this area and its associated systems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Digital Manufacturing; Human Robot Collaboration; Industry 5.0; Ontology; Augmented Reality; Computer Aided Manufacturing; Human Robot Interaction; Industrial Robots; Ontology; Robot Learning; Virtual Environments; Automated Systems; Cybe-physical Systems; Cyber-physical Systems; Digital Manufacturing; Human-robot Collaboration; Humans-robot Interactions; Industry 5.0; Manufacturing Challenges; Manufacturing Opportunities; Ontology's; Smart Manufacturing},
	keywords = {Augmented reality; Computer aided manufacturing; Human robot interaction; Industrial robots; Ontology; Robot learning; Virtual environments; Automated systems; Cybe-physical systems; Cyber-physical systems; Digital manufacturing; Human-robot collaboration; Humans-robot interactions; Industry 5.0; Manufacturing challenges; Manufacturing opportunities; Ontology's; Smart manufacturing},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Wang20251293,
	author = {Wang, Juan and Tang, Shu Kun},
	title = {Ethical risks of the generative informational privacy gap; 生成式信息隐私鸿沟伦理风险探讨},
	year = {2025},
	journal = {Studies in Science of Science},
	volume = {43},
	number = {6},
	pages = {1293 - 1301},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010899246&partnerID=40&md5=edbbe81b45ce9ca5bfac67150e949ea3},
	abstract = {In the era of generative AI, the privacy crisis is intensifying. Current research primarily focuses on new methods of privacy invasion brought about by large language models and their countermeasures, yet there is scarce attention to the more fundamental issue of the generative informational privacy gap. This gap refers to the discrepancy between the “ought” and the “is” of individual informational privacy, exacerbated by the predictive analytical capabilities and generative functions of multi - modal large language models. These technologies enhance the privacy awareness and manipulation power of both human and machine agents, resulting in individuals facing a widening disparity in their level of informational privacy. This concept helps to more intuitively reveal the “entanglement” between AI technology, privacy owners, and privacy itself. The generative informational privacy gap may lead to ontological privacy alienation, epistemological cognitive reification, and praxeological behavior domestication. The key to addressing these issues lies in two main strategies: Firstly, achieving three major paradigm shifts in regulation; Secondly, a tripartite governance approach that simultaneously employs heteronomy, technonomy, and autonomy. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Generative Informational Privacy Gap; Informational Privacy Gap; Predictive Informational Privacy Gap},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hier2025,
	author = {Hier, Daniel B. and Carrithers, Michael A. and Platt, Steven K. and Nguyen, Anh and Giannopoulos, Ioannis and Obafemi-Ajayi, Tayo},
	title = {Preprocessing of Physician Notes by LLMs Improves Clinical Concept Extraction Without Information Loss},
	year = {2025},
	journal = {Information (Switzerland)},
	volume = {16},
	number = {6},
	pages = {},
	doi = {10.3390/info16060446},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009037563&doi=10.3390%2Finfo16060446&partnerID=40&md5=10a82c489768c9464fcdecf3b1288a0e},
	abstract = {Clinician notes are a rich source of patient information, but often contain inconsistencies due to varied writing styles, abbreviations, medical jargon, grammatical errors, and non-standard formatting. These inconsistencies hinder their direct use in patient care and degrade the performance of downstream computational applications that rely on these notes as input, such as quality improvement, population health analytics, precision medicine, clinical decision support, and research. We present a large-language-model (LLM) approach to the preprocessing of 1618 neurology notes. The LLM corrected spelling and grammatical errors, expanded acronyms, and standardized terminology and formatting, without altering clinical content. Expert review of randomly sampled notes confirmed that no significant information was lost. To evaluate downstream impact, we applied an ontology-based NLP pipeline (Doc2Hpo) to extract biomedical concepts from the notes before and after editing. F1 scores for Human Phenotype Ontology extraction improved from 0.40 to 0.61, confirming our hypothesis that better inputs yielded better outputs. We conclude that LLM-based preprocessing is an effective error correction strategy that improves data quality at the level of free text in clinical notes. This approach may enhance the performance of a broad class of downstream applications that derive their input from unstructured clinical documentation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Extraction; Data Interoperability; Doc2hpo; Electronic Health Records; Human Phenotype Ontology; Large Language Models; Physician Notes; Computational Linguistics; Electronic Health Record; Error Correction; Medical Computing; Ontology; Concept Extraction; Data Interoperability; Doc2hpo; Electronic Health; Health Records; Human Phenotype Ontology; Language Model; Large Language Model; Ontology's; Physician Note; Clinical Research},
	keywords = {Computational linguistics; Electronic health record; Error correction; Medical computing; Ontology; Concept extraction; Data interoperability; Doc2hpo; Electronic health; Health records; Human phenotype ontology; Language model; Large language model; Ontology's; Physician note; Clinical research},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Gallifant2025,
	author = {Gallifant, Jack and Chen, Shan and Jain, Sandeep Kumar and Moreira, Pedro and Topaloglu, Umit and Aerts, Hugo J.W.L. and Warner, Jeremy Lyle and La Cava, William G. and Bitterman, Danielle S.},
	title = {Reliability of Large Language Model Knowledge Across Brand and Generic Cancer Drug Names},
	year = {2025},
	journal = {JCO Clinical Cancer Informatics},
	volume = {9},
	pages = {},
	doi = {10.1200/CCI-24-00257},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008823467&doi=10.1200%2FCCI-24-00257&partnerID=40&md5=e1e51d817cb0a7f7589816015e7a42f6},
	abstract = {PURPOSETo evaluate the performance and consistency of large language models (LLMs) across brand and generic oncology drug names in various clinical tasks, addressing concerns about potential fluctuations in LLM performance because of subtle phrasing differences that could affect patient care.METHODSThis study evaluated three LLMs (GPT-3.5-turbo-0125, GPT-4-turbo, and GPT-4o) using drug names from HemOnc ontology. The assessment included 367 generic-to-brand and 2,516 brand-to-generic pairs, 1,000 drug-drug interaction (DDI) synthetic patient cases, and 2,438 immune-related adverse event (irAE) cases. LLMs were tested on drug name recognition, word association, DDI (DDI) detection, and irAE diagnosis using both brand and generic drug names.RESULTSLLMs demonstrated high accuracy in matching brand and generic names (GPT-4o: 97.38% for brand, 94.71% for generic, P <.01). However, they showed significant inconsistencies in word association tasks. GPT-3.5-turbo-0125 exhibited biases favoring brand names for effectiveness (odds ratio [OR], 1.43, P <.05) and being side-effect-free (OR, 1.76, P <.05). DDI detection accuracy was poor across all models (<26%), with no significant differences between brand and generic names. Sentiment analysis revealed significant differences, particularly in GPT-3.5-turbo-0125 (brand mean 0.67, generic mean 0.95, P <.01). Consistency in irAE diagnosis varied across models.CONCLUSIONDespite high proficiency in name-matching, LLMs exhibit inconsistencies when processing brand versus generic drug names in more complex tasks. These findings highlight the need for increased awareness, improved robustness assessment methods, and the development of more consistent systems for handling nomenclature variations in clinical applications of LLMs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Antineoplastic Agents; Drugs, Generic; Antineoplastic Agent; Generic Drug; Accuracy; Article; Clinical Practice; Differential Diagnosis; Drug Interaction; Human; Large Language Model; Multiple Choice Test; Sentiment Analysis; Temperature; Classification; Drug Therapy; Language; Neoplasm; Nomenclature; Reproducibility; Antineoplastic Agents; Drug Interactions; Drugs, Generic; Humans; Language; Large Language Models; Neoplasms; Reproducibility Of Results; Terminology As Topic},
	keywords = {antineoplastic agent; generic drug; accuracy; Article; clinical practice; differential diagnosis; drug interaction; human; large language model; multiple choice test; sentiment analysis; temperature; classification; drug therapy; language; neoplasm; nomenclature; reproducibility; Antineoplastic Agents; Drug Interactions; Drugs, Generic; Humans; Language; Large Language Models; Neoplasms; Reproducibility of Results; Terminology as Topic},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Su20251297,
	author = {Su, Yvonne and Babore, Yonatan B. and Kahn, Charles E.},
	title = {A Large Language Model to Detect Negated Expressions in Radiology Reports},
	year = {2025},
	journal = {Journal of Imaging Informatics in Medicine},
	volume = {38},
	number = {3},
	pages = {1297 - 1303},
	doi = {10.1007/s10278-024-01274-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008273555&doi=10.1007%2Fs10278-024-01274-9&partnerID=40&md5=2a59458798b8fc0182bc85bd49d93d83},
	abstract = {Natural language processing (NLP) is crucial to extract information accurately from unstructured text to provide insights for clinical decision-making, quality improvement, and medical research. This study compared the performance of a rule-based NLP system and a medical-domain transformer-based model to detect negated concepts in radiology reports. Using a corpus of 984 de-identified radiology reports from a large U.S.-based academic health system (1000 consecutive reports, excluding 16 duplicates), the investigators compared the rule-based medspaCy system and the Clinical Assertion and Negation Classification Bidirectional Encoder Representations from Transformers (CAN-BERT) system to detect negated expressions of terms from RadLex, the Unified Medical Language System Metathesaurus, and the Radiology Gamuts Ontology. Power analysis determined a sample size of 382 terms to achieve α = 0.05 and β = 0.8 for McNemar’s test; based on an estimate of 15% negated terms, 2800 randomly selected terms were annotated manually as negated or not negated. Precision, recall, and F1 of the two models were compared using McNemar’s test. Of the 2800 terms, 387 (13.8%) were negated. For negation detection, medspaCy attained a recall of 0.795, precision of 0.356, and F1 of 0.492. CAN-BERT achieved a recall of 0.785, precision of 0.768, and F1 of 0.777. Although recall was not significantly different, CAN-BERT had significantly better precision (χ2 = 304.64; p < 0.001). The transformer-based CAN-BERT model detected negated terms in radiology reports with high precision and recall; its precision significantly exceeded that of the rule-based medspaCy system. Use of this system will improve data extraction from textual reports to support information retrieval, AI model training, and discovery of causal relationships. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Named Entity Recognition; Natural Language Processing; Negated Expression (negex) Detection; Radiology Reports; Artificial Intelligence; Behavioral Research; Computational Linguistics; Decision Making; Extraction; Information Retrieval; Information Use; Medical Information Systems; Natural Language Processing Systems; Radiology; Text Processing; Expression Detections; Language Model; Language Processing; Large Language Model; Named Entity Recognition; Natural Language Processing; Natural Languages; Negated Expression Detection; Radiology Reports; Rule Based; Clinical Research; Aged; Article; Assertiveness; Clinical Decision Making; Data Extraction; Diagnosis; Human; Information Retrieval; Large Language Model; Natural Language Processing; Power Analysis; Radiology; Sample Size; Total Quality Management; Unified Medical Language System},
	keywords = {Artificial intelligence; Behavioral research; Computational linguistics; Decision making; Extraction; Information retrieval; Information use; Medical information systems; Natural language processing systems; Radiology; Text processing; Expression detections; Language model; Language processing; Large language model; Named entity recognition; Natural language processing; Natural languages; Negated expression detection; Radiology reports; Rule based; Clinical research; aged; article; assertiveness; clinical decision making; data extraction; diagnosis; human; information retrieval; large language model; natural language processing; power analysis; radiology; sample size; total quality management; Unified Medical Language System},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Soares2025265,
	author = {Soares, Filipi Miranda and Saraiva, Antônio Mauro and Ferreira Pires, L. Ferreira and da Silva Santos, Luiz Olavo Bonino and Moreira, Dilvan De Abreu and Correa, Fernando Elias and Braghetto, Kelly Rosa and Drucker, Debora P. and Delbem, Alexandre Claudio Botazzo},
	title = {Exploring a Large Language Model for Transforming Taxonomic Data into OWL: Lessons Learned and Implications for Ontology Development},
	year = {2025},
	journal = {Data Intelligence},
	volume = {7},
	number = {2},
	pages = {265 - 302},
	doi = {10.3724/2096-7004.di.2025.0020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008033640&doi=10.3724%2F2096-7004.di.2025.0020&partnerID=40&md5=6058e3e1f68e29e82ea9f1764e6f68fb},
	abstract = {Managing scientific names in ontologies that represent species taxonomies is challenging due to the ever-evolving nature of these taxonomies. Manually maintaining these names becomes increasingly difficult when dealing with thousands of scientific names. To address this issue, this paper investigates the use of ChatGPT-4 to automate the development of the Organism module in the Agricultural Product Types Ontology (APTO) for species classification. Our methodology involved leveraging ChatGPT-4 to extract data from the GBIF Backbone API and generate OWL files for further integration in APTO. Two alternative approaches were explored: (1) issuing a series of prompts for ChatGPT-4 to execute tasks via the BrowserOP plugin and (2) directing ChatGPT-4 to design a Python algorithm to perform analogous tasks. Both approaches rely on a prompting method where we provide instructions, context, input data, and an output indicator. The first approach showed scalability limitations, while the second approach used the Python algorithm to overcome these challenges, but it struggled with typographical errors in data handling. This study highlights the potential of Large language models like ChatGPT-4 to streamline the management of species names in ontologies. Despite certain limitations, these tools offer promising advancements in automating taxonomy-related tasks and improving the efficiency of ontology development. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Agriculture; Artificial Intelligence; Chatgpt; Knowledge Graph; Taxonomy; Agriculture; Birds; Data Assimilation; Data Handling; High Level Languages; Knowledge Graph; Metadata; Ontology; Python; Chatgpt; Knowledge Graphs; Language Model; Ontology Development; Ontology's; Plug-ins; Product Types; Species Classification; Typographical Errors; Taxonomies},
	keywords = {Agriculture; Birds; Data assimilation; Data handling; High level languages; Knowledge graph; Metadata; Ontology; Python; ChatGPT; Knowledge graphs; Language model; Ontology development; Ontology's; Plug-ins; Product types; Species classification; Typographical errors; Taxonomies},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Alamoudi202523664,
	author = {Alamoudi, Sumayyah and Al Khuzayem, Lama A. and Jamal, Amani Tariq},
	title = {Optimizing Automated Question Generation for Educational Assessments A Semantic Analysis of LLMs with Structured and Unstructured Ontologies},
	year = {2025},
	journal = {Engineering, Technology and Applied Science Research},
	volume = {15},
	number = {3},
	pages = {23664 - 23671},
	doi = {10.48084/etasr.10662},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008004146&doi=10.48084%2Fetasr.10662&partnerID=40&md5=9fb65f94aecc277c591f78adbac81554},
	abstract = {This study explores the optimization of Automated Question Generation (AQG) for educational assessments using Large Language Models (LLMs) and ontologies. Three approaches are evaluated: template-based structured ontology question generation, LLM-based structured ontology question generation, and LLM-based flat concept list question generation, using BERT Precision, Recall, F1-score, and Semantic Similarity as performance metrics. The results show that: i) the template-based structured ontology approach achieved a BERT Precision of 0.833, Recall of 0.844, and F1-score of 0.838, with a Semantic Similarity of 0.563, ii) the LLM-based structured ontology method showed improvements with a BERT Precision of 0.856, Recall of 0.863, and F1-score of 0.859, but a lower Semantic Similarity of 0.534, and iii) the LLM-based flat concept list approach provided the best results, achieving BERT Precision, Recall, and F1-score of 0.859, along with the highest Semantic Similarity of 0.567. Despite the higher semantic similarity of the LLM-based flat concept list, qualitative analysis revealed that the unstructured ontology sometimes produced hallucinated or unrelated questions. These findings suggest that LLM-based methods provide a balance of relevance and diversity in question generation, with LLM-based flat concept list offering the most optimal results for question generation, while LLM-based structured ontology strikes a balance between Precision and Recall. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai In Education; Large Language Models (llms); Ontologies; Question Generation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Dennler2025,
	author = {Dennler, Olivier and Ryan, Colm J.},
	title = {Evaluating sequence and structural similarity metrics for predicting shared paralog functions},
	year = {2025},
	journal = {NAR Genomics and Bioinformatics},
	volume = {7},
	number = {2},
	pages = {},
	doi = {10.1093/nargab/lqaf051},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003866563&doi=10.1093%2Fnargab%2Flqaf051&partnerID=40&md5=28a1de2f836cfa02e01ed830af6fce5d},
	abstract = {Gene duplication is the primary source of new genes, resulting in most genes having identifiable paralogs. Over time, paralog pairs may diverge in some respects but many retain the ability to perform the same functional role. Protein sequence identity is often used as a proxy for functional similarity and can predict shared functions between paralogs as revealed by synthetic lethal experiments. However, the advent of alternative protein representations, including embeddings from protein language models (PLMs) and predicted structures from AlphaFold, raises the possibility that alternative similarity metrics could better capture functional similarity between paralogs. Here, using two species (budding yeast and human) and two different definitions of shared functionality (shared protein–protein interactions and synthetic lethality), we evaluated a variety of alternative similarity metrics. For some tasks, predicted structural similarity or PLM similarity outperform sequence identity, but more importantly these similarity metrics are not redundant with sequence identity, i.e. combining them with sequence identity leads to improved predictions of shared functionality. By adding contextual features, representing similarity to homologous proteins within and across species, we can significantly enhance our predictions of shared paralog functionality. Overall, our results suggest that alternative similarity metrics capture complementary aspects of functional similarity beyond sequence identity alone. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Amino Acid Sequence; Article; Embryo; Evaluation Study; Gene Function; Gene Interaction; Gene Ontology; Genetic Distance; Human; Human Cell; Nonhuman; Paralogy; Protein Language Model; Protein Protein Interaction; Protein Structure; Saccharomyces Cerevisiae; Synthetic Lethality},
	keywords = {amino acid sequence; Article; embryo; evaluation study; gene function; gene interaction; gene ontology; genetic distance; human; human cell; nonhuman; paralogy; protein language model; protein protein interaction; protein structure; Saccharomyces cerevisiae; synthetic lethality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Rafols20253229,
	author = {Rafols, Ismael},
	title = {Towards multiple ontologies in science mapping. A tribute to Loet Leydesdorff},
	year = {2025},
	journal = {Scientometrics},
	volume = {130},
	number = {6},
	pages = {3229 - 3255},
	doi = {10.1007/s11192-025-05323-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003418484&doi=10.1007%2Fs11192-025-05323-0&partnerID=40&md5=c2df4aa58514160b7440797f05fb3efb},
	abstract = {This article reviews Loet Leydesdorff’s contributions to science mapping. It explains how over the years, his mapping techniques evolved from journal mapping to global maps of science and finally towards interactive interfaces portraying multiple classifications and ontologies. It then critically reviews the challenges faced by current approaches to science mapping, which implicitly assume a ‘natural’ epistemic structure, with examples from two recent case studies. We observe that bottom-up algorithmic approaches, either based on citation or semantic approaches, lack conceptual consistency regarding the type of categories used: in a same classification a category captures methods, another one has materials, a third one contains empirical objects and a fourth is focused on theories, rather than having a single logic. I argue that science mapping would produce more useful representations by using ontologies based on a single logic that aligns with the particular conceptual needs of the analysis. Novel classification methods based on machine learning and language models hold promise to produce these tailored, question-driven ontologies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Classification; Ontologies; Science Maps},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Li2025,
	author = {Li, Zhengzuo and Piao, Chengxi and Chu, Dianhui and Tu, Zhiying and Hu, Xin and Ding, Deqiong},
	title = {Resource state adaptive collaboration mechanism based on resource modeling and multi-agent system},
	year = {2025},
	journal = {Complex and Intelligent Systems},
	volume = {11},
	number = {6},
	pages = {},
	doi = {10.1007/s40747-025-01882-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003100864&doi=10.1007%2Fs40747-025-01882-0&partnerID=40&md5=c21b118d0e00c4d405f545f06d7e77fe},
	abstract = {The management of complex, dynamic, and cross-domain resources in cyber-physical-human systems (CPHS) faces significant challenges under spatiotemporal dynamics, particularly resource state conflicts caused by rapid environmental changes and interdependent resource interactions. To address these challenges, this study proposes an integrated framework combining resource modeling and resource state adaptive collaboration mechanism. First, a resource modeling framework for state coordination (RMFS) is developed to unify the representation of heterogeneous resources, their functionalities, and collaborative relationships through hybrid structural and semantic modeling. A resource state adaptive collaboration mechanism (RSACM) integrates multi-agent systems with knowledge graph to achieve real-time state synchronization. Agents utilize the collaborative relationships in the graph to make adaptive collaborative decisions on resource states, in order to perform state transitions and alleviate resource availability conflicts. Further, a meta-path-based resource inference (MPRI) method enables efficient resource retrieval and applies to simulation experiments by leveraging conceptual-instance meta-paths and large language model (LLM)-augmented substitution strategies to resolve resource unavailability. Experimental validation across emergency healthcare scenario demonstrates the framework’s effectiveness. An extension study was conducted on RMFS and RSACM through two cases from different fields. The proposed approach advances CPHS resource management by addressing heterogeneity, availability, and cooperativity in dynamic environments, offering theoretical and practical insights for complex system collaboration under spatiotemporal constraints. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Meta Modeling; Multi-agent Systems; Ontology Modeling; Resource State Collaboration; State Transformation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hwang2025,
	author = {Hwang, Jaemin and Yoon, Sungmin},
	title = {AI agent-based indoor environmental informatics: Concept, methodology, and case study},
	year = {2025},
	journal = {Building and Environment},
	volume = {277},
	pages = {},
	doi = {10.1016/j.buildenv.2025.112879},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001303229&doi=10.1016%2Fj.buildenv.2025.112879&partnerID=40&md5=65860c5e63d34f17ef12066924e24e8d},
	abstract = {Analyzing and improving indoor environments requires continuous intervention from human experts, which is challenging in practice. To address this limitation, an ontology-based AI agent system can be employed. This study proposes a conceptual model of AI agent-based indoor environmental informatics (IEI), develops an indoor environmental ontology by extending Brick schema, and demonstrates its application in real indoor environment. AI agent-based IEI is an approach that builds an integrated information system to capture relationships among indoor environments, occupants, and building systems so that utilizes AI agent for continuous indoor environment management. The AI agent leverages indoor environmental ontology, intrusive data, and indoor environmental toolkit to perform holistic analysis of indoor thermal environments and provides strategies for enhancing thermal comfort during the operational phase. The proposed concept was implemented in the Dynamo environment and applied to an office space. For the collection of real indoor environmental data, intrusive measurement was conducted over five days, and an indoor environmental ontology for the target space was developed. Indoor environmental toolkit used for the system included spatial coordinate extractor (SCE) for extracting spatial element coordinates, ontology file generator (OFG) for creating ontology files, and predicted mean vote (PMV) model for calculating PMV. The AI agent identified a PMV variation of 0.77, a discomfort rate of 28.2 %, and the disparity between physical sensor data and occupants’ subjective thermal comfort. Furthermore, the AI agent suggested practical strategies for improvement, including determining window status based on outdoor temperature, adjusting air conditioner operation, and modifying occupant seating arrangements. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Agent; Brick Schema; Indoor Environments; Llm; Ontology; Pmv; Thermal Environment; Indoor Air Pollution; Agent Based; Ai Agent; Brick Schema; Environmental Informatics; Indoor Environment; Llm; Ontology's; Predicted Mean Vote; Thermal; Thermal Environment; Brick; Air Conditioning; Artificial Intelligence; Building; Information System; Sensor},
	keywords = {Indoor air pollution; Agent based; AI agent; Brick schema; Environmental Informatics; Indoor environment; LLM; Ontology's; Predicted mean vote; Thermal; Thermal environment; Brick; air conditioning; artificial intelligence; building; information system; sensor},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Zhou2025,
	author = {Zhou, Zhipeng and Yu, Xinhui and Magoua, Joseph Jonathan and Cui, Jianqiang and Luan, Haiying and Lin, Dong},
	title = {Integrating machine learning and a large language model to construct a domain knowledge graph for reducing the risk of fall-from-height accidents},
	year = {2025},
	journal = {Accident Analysis and Prevention},
	volume = {215},
	pages = {},
	doi = {10.1016/j.aap.2025.108009},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000442308&doi=10.1016%2Fj.aap.2025.108009&partnerID=40&md5=290ad24146bac1b5976c44b0d0f5d7c3},
	abstract = {Fall-from-height (FFH) accidents remain a major source of workplace injuries and fatalities. Fall protection systems (FPS) are critical for preventing falls in the work-at-height (WAH) environment. However, challenges in designing and selecting effective FPS persist across various industries, and existing tools often lack practical references. This study aims to develop an FFH-specific knowledge graph (FFH-KG) to support FPS design. By structuring accident data, the FFH-KG provides empirical insights to help designers improve FPS frameworks, aiding safety planning and decision-making. It serves as a decision support system for FPS designers and safety professionals, guiding the selection and design of appropriate protection solutions for diverse WAH scenarios. The FFH-KG was constructed using a hybrid natural language processing approach, combining manual extraction, entity recognition, text segmentation, and rule-based relation extraction. It was grounded in a schema layer (i.e., ontology) established by experts. A text-mining approach, integrating machine learning with a large language model, facilitated the categorization of fall types, refinement of WAH scenarios, and identification of fall causes, enhancing the content and applicability of knowledge graph. A total of 2,200 entities and 4,820 relationships were created based on fall protection equipment standard documents and fall-from-height accident investigation reports, forming a foundation for developing countermeasures. The retrieval performance of FFH-KG was validated through three case studies. This research has also made significant progress in intelligent safety engineering and management across industries. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain Knowledge Graph; Fall-from-height; Knowledge Extraction; Large Language Model; Named Entity Recognition; Natural Language Process; Domain Knowledge; Fall Detection; Machine Learning; Natural Language Processing Systems; Domain Knowledge; Domain Knowledge Graph; Fall From Height; Fall Protection; Knowledge Extraction; Knowledge Graphs; Language Model; Large Language Model; Named Entity Recognition; Natural Language Process; Knowledge Graph; Data Mining; Falling; Human; Large Language Model; Machine Learning; Occupational Accident; Prevention And Control; Protective Equipment; Accidental Falls; Accidents, Occupational; Data Mining; Humans; Large Language Models; Machine Learning; Protective Devices},
	keywords = {Domain Knowledge; Fall detection; Machine learning; Natural language processing systems; Domain knowledge; Domain knowledge graph; Fall from height; Fall protection; Knowledge extraction; Knowledge graphs; Language model; Large language model; Named entity recognition; Natural language process; Knowledge graph; data mining; falling; human; large language model; machine learning; occupational accident; prevention and control; protective equipment; Accidental Falls; Accidents, Occupational; Data Mining; Humans; Large Language Models; Machine Learning; Protective Devices},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Antoniou2025287,
	author = {Antoniou, Christina and Bassiliades, Nick},
	title = {Utilizing LLMs and ontologies to query educational knowledge graphs},
	year = {2025},
	pages = {287 - 295},
	doi = {10.1145/3716554.3716598},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010772666&doi=10.1145%2F3716554.3716598&partnerID=40&md5=8b67385f81e02dcf1b4c2fc5a7ff002d},
	abstract = {Knowledge Graphs (KGs) provide knowledge and data in a structured format with content from various fields. But the access to the knowledge graphs is done by experienced users, that is, users who know the syntax of the SPARQL language and the KG vocabulary (either in RDF Schema or in OWL) in order to be able to ask questions to exploit the knowledge graphs. However, this requires a lot of time and effort for most of the users, which makes KGs inaccessible to a large number of users. Large Language Models (LLMs) that have appeared recently can provide an alternative way to query knowledge graphs without the need to learn SPARQL and/or know the schema and vocabulary of them, eliminating the time and effort that ordinary users need to spend in order to use them. In this article, we present some experiments and their results illustrating how ChatGPT can help ordinary users to generate SPARQL queries, without knowing SPARQL, to effectively use knowledge graphs and exploit their wealth of data. We experimented with ChatGPT to explore whether it can generate SPARQL queries based on user's natural language input and a given vocabulary (ontology) about an educational knowledge graph. To this end we have devised a specific prompt template. Results indicate that LLMs can indeed help in this direction, given the fact that they are prompted properly, using good English language. We have also discussed some practical lessons learned through this experiment. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Application; Chatgpt; Knowledge Graphs; Large Language Model Use Cases; Rdf; Computational Linguistics; Knowledge Graph; Natural Language Processing Systems; Query Languages; Query Processing; Structured Query Language; Ai Applications; Chatgpt; Educational Knowledge; Knowledge Graphs; Language Model; Large Language Model Use Case; Model Use; Ontology's; Rdf; Rdf Schemas; Graphic Methods},
	keywords = {Computational linguistics; Knowledge graph; Natural language processing systems; Query languages; Query processing; Structured Query Language; AI applications; ChatGPT; Educational knowledge; Knowledge graphs; Language model; Large language model use case; Model use; Ontology's; RDF; RDF schemas; Graphic methods},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Zeginis2025324,
	author = {Zeginis, Dimitris and Kalampokis, Evangelos and Tarabanis, Konstantinos A.},
	title = {Applying an ontology-aware zero-shot LLM prompting approach for information extraction in Greek: The case of DIAVGEIA gov gr},
	year = {2025},
	pages = {324 - 330},
	doi = {10.1145/3716554.3716603},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010764047&doi=10.1145%2F3716554.3716603&partnerID=40&md5=3d39e3a361590aded92f4becb50fda53},
	abstract = {Large Language Models (LLMs) have attracted considerable attention, primarily due to their potential to revolutionize sectors that heavily rely on textual information. Governance is one such sector. Public administrations around the globe produce millions of documents including laws, administrative decisions and acts (e.g., travel/budget approvals) that contain valuable information in unstructured way. The documents are usually stored at document-centered repositories. As a result the actual data of the documents cannot be further searched or processed. The availability of structured metadata of the documents (e.g., who has traveled, where, when) could further enhance the searching and processing of the documents as well as enable data analytics. The construction of metadata can be done through information extraction approaches such as Named Entity Recognition (NER), Relation Extraction (RE) and Event Extraction (EE) on the documents. LLMs are recently used successfully for information extraction tasks, while ontologies are traditionally used for meaningful data modeling. The aim of the paper is to apply and evaluate an ontology-aware zero-shot LLM prompting approach for information extraction in Greek language documents available in DIAVGEIA.gov.gr - the Greek Open Government portal for administrative documents. The evaluation assesses various LLM models/sizes for various difficulties of information extraction tasks. Overall the results are very promising, since most LLM models, even smaller ones, performed very well for all tasks in Greek. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Greek; Information Extraction; Llm; Ner; Ontology; Public Administration; Data Mining; Information Retrieval; Information Retrieval Systems; Metadata; Modeling Languages; Public Administration; Data Analytics; Greek; Information Extraction; Language Model; Large Language Model; Named Entity Recognition; Ontology's; Relation Extraction; Structured Metadatas; Textual Information; Ontology},
	keywords = {Data mining; Information retrieval; Information retrieval systems; Metadata; Modeling languages; Public administration; Data analytics; Greek; Information extraction; Language model; Large language model; Named entity recognition; Ontology's; Relation extraction; Structured metadatas; Textual information; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kousa2025701,
	author = {Kousa, Ilona},
	title = {Data-Driven Democracy? Using Social Media and AI for Knowledge Co-production in Energy Transition Research},
	year = {2025},
	pages = {701 - 704},
	doi = {10.1145/3701716.3715286},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009246686&doi=10.1145%2F3701716.3715286&partnerID=40&md5=721aafed71f72404a5c80e03ecdbbfa6},
	abstract = {As global challenges grow increasingly difficult to address, the need for relevant knowledge in policymaking has become more critical than ever. In fields such as sustainability research, discussions about the co-production of knowledge emphasise the involvement of diverse stakeholders in generating relevant information for democratic decision-making processes. Particularly in technology-intensive areas like energy, the voices of experts have traditionally dominated, often marginalising the perspectives of citizens affected by energy policies and leading to epistemic injustices. Social media serves as a central forum for political activity and civic engagement, making it an important research environment for understanding the complexities of knowledge production and sharing. In my dissertation, I explore the opportunities and challenges of integrating citizen voices into the co-production of knowledge through social media using artificial intelligence (AI) based methods. My approach includes analysing stakeholder perspectives across social media, policymakers' speeches, and traditional media, using the discourse on energy justice in the context of sustainable energy transition as empirical material. Additionally, I conduct a comparative analysis of a rule-based ontological classification tool and a large language model (LLM) chatbot as qualitative content analysis tools. I demonstrate that social media data can reveal critical perspectives and marginalised discourses that might otherwise be overlooked. However, social media platforms are also used to spread disinformation and incite polarisation, which undermines their reliability as accurate reflections of prevailing attitudes and opinions in society. In addition, although AI methods can make information processing more effective and support the use of large datasets, it is important to consider their inherent limitations. I address the biases and ethical concerns associated with social media data and AI-based analysis tools and discuss potential solutions to mitigate these challenges. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Social Sciences; Large Language Models; Natural Language Processing; Social Media; Artificial Intelligence; Decision Making; Energy Policy; Energy Transition; Knowledge Management; Knowledge Transfer; Large Datasets; Sustainable Development; Co-production Of Knowledge; Computational Social Science; Energy; Energy Transitions; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Social Media; Social Networking (online)},
	keywords = {Artificial intelligence; Decision making; Energy policy; Energy transition; Knowledge management; Knowledge transfer; Large datasets; Sustainable development; Co-production of knowledge; Computational social science; Energy; Energy transitions; Language model; Language processing; Large language model; Natural language processing; Natural languages; Social media; Social networking (online)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Frey20252831,
	author = {Frey, Johannes and Ferraz, Lucas and Hofer, Marvin},
	title = {POTS - A Polyparadigmatic Ontology Term Search with Fine-Grained Context Steering using Hyper-Level Vector Spaces},
	year = {2025},
	pages = {2831 - 2834},
	doi = {10.1145/3701716.3715194},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009243705&doi=10.1145%2F3701716.3715194&partnerID=40&md5=8878c6e73e0b257e491cdb890b3f1308},
	abstract = {We present a novel microservice-based system, that facilitates a polyparadigmatic ontology term search (leveraging semantic search via vector embeddings, keyword search, and attribute filters). The search index strategy intends to preserve important semantic aspects of the ontological context of a term (selected attributes and term relationships) using structured search fields and multilevel vector spaces assembling hyper-level vector spaces. The flexible, yet simple query API allows fine-grained search requests based on a combination of fuzzy and exact filters. The architecture is based on a highly automatable and flexible Docker Compose setup strategy. While deploying the system for a local ontology is only one command away, the setup also allows ingesting a configurable subset of over 1, 800 published ontologies in over 12, 000 versions via DBpedia Archivo. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Llms; Ontology & Graph Retrieval Augmented Generation; Ontology Retrieval; Ontology Terms Embedding; Owl; Semantic Search; Terminology Lookup Service; Embeddings; Information Retrieval; Ontology; Search Engines; Semantic Web; Semantics; Terminology; Vectors; Llm; Lookup Services; Ontology & Graph Retrieval Augmented Generation; Ontology Graphs; Ontology Retrieval; Ontology Term Embedding; Ontology Terms; Ontology's; Owl; Semantic Search; Terminology Lookup Service; Vector Spaces},
	keywords = {Embeddings; Information retrieval; Ontology; Search engines; Semantic Web; Semantics; Terminology; Vectors; LLM; Lookup services; Ontology & graph retrieval augmented generation; Ontology graphs; Ontology retrieval; Ontology term embedding; Ontology terms; Ontology's; OWL; Semantic search; Terminology lookup service; Vector spaces},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Vakaj20252545,
	author = {Vakaj, Edlira and Mihindukulasooriya, Nandana and Tiwari, Sanju M. and Rodríguez Méndez, Sergio José},
	title = {4th International Workshop on Natural Language Processing for Knowledge Graph Construction},
	year = {2025},
	pages = {2545 - 2548},
	doi = {10.1145/3701716.3717822},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009228339&doi=10.1145%2F3701716.3717822&partnerID=40&md5=a13668b91fa1acb52802885cc826f53f},
	abstract = {Knowledge graphs (KG) are becoming increasingly popular, at the heart of Gartner's emerging tech impact radar, especially as a complementary theme for addressing the challenges of recent advances in natural language processing (NLP) with large language models related to responsible AI such as fairness, transparency, accountability, and explainability. Sir Tim Berners-Lee's seminal work “Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web”, envisioned a World Wide Web where information is not only accessible but structured, allowing machines to interpret data meaningfully. This vision laid the groundwork for technologies such as RDF (Resource Description Framework) and OWL (Web Ontology Language), which serve as foundational components for modern KGs. However, the process of building domain-specific KGs from extensive text corpora is highly complex and resource-intensive, requiring careful task design for entity recognition, disambiguation, and relationship extraction, among others. These tasks are essential to ensure accuracy and relevance in knowledge representation, but they pose considerable challenges. Addressing these complexities is crucial for the continued advancement and application of KGs across domains. In this context, the 4<sup>th</sup> NLP4KGC workshop is held to create a collaborative platform for researchers, practitioners, and industry experts in NLP and KG construction. Following the success and growing community engagement in the previous three editions, this year's workshop aims to deepen collaboration and encourage innovative solutions in this rapidly evolving field. The 4<sup>th</sup> NLP4KGC will continue to bridge academia and industry, fostering the exchange of insights, tools, and methodologies at the intersection of NLP and KG development. The 4<sup>th</sup> NLP4KGC will consist of five accepted papers and three keynotes from distinguished speakers. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Large Language Models; Natural Language Understanding; Responsible Ai; Semantic Web; Web Of Data; Construction; Human Engineering; Knowledge Graph; Ontology; Resource Description Framework (rdf); World Wide Web; Graph Construction; Knowledge Graphs; Language Model; Language Processing; Large Language Model; Natural Language Understanding; Natural Languages; Responsible Ai; Semantic-web; Web Of Data; Natural Language Processing Systems},
	keywords = {Construction; Human engineering; Knowledge graph; Ontology; Resource Description Framework (RDF); World Wide Web; Graph construction; Knowledge graphs; Language model; Language processing; Large language model; Natural language understanding; Natural languages; Responsible AI; Semantic-Web; Web of data; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Sun2025801,
	author = {Sun, Qiang and Luo, Yuanyi and Zhang, Wenxiao and Li, Sirui and Li, Jichunyang and Niu, Kai and Kong, Xiangrui and Liu, Wei},
	title = {Docs2KG: A Human-LLM Collaborative Approach to Unified Knowledge Graph Construction from Heterogeneous Documents},
	year = {2025},
	pages = {801 - 804},
	doi = {10.1145/3701716.3715309},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009228050&doi=10.1145%2F3701716.3715309&partnerID=40&md5=c56efab98d080827d98f5f279e4e49fd},
	abstract = {Enterprises generate vast amounts of unstructured documents, posing challenges for knowledge extraction and representation. Large language models (LLMs) offer strong potential for processing such data but struggle with factual accuracy and provenance. Knowledge graphs (KGs) provide a structured framework to address these limitations [6], yet constructing high-quality KGs from heterogeneous data remains a challenge. To address this issue, we present Docs2KG, a modular framework to build high-quality KGs from diverse unstructured documents. We first employs state-of-the-art document processing techniques to extract textual content, tabular data, and figures. The extracted information is then unified into a multifaceted KG with three aspects: (1) a Layout KG capturing document structural hierarchies, (2) a Metadata KG preserving document properties, and (3) a Semantic KG representing domain-specific entities and relationships. Docs2KG supports multiple construction paradigms for Semantic KG: ontology-based approaches, hybrid NLP pipelines with LLM verification, LLM-guided ontology generation, and specialized models for named entity recognition, event extraction, and causal relationship identification to enhance semantic coverage and accuracy. A key feature of Docs2KG is its human-in-the-loop verification interface, enabling iterative quality assessment and refinement of the resulting KGs. Docs2KG is openly available at https://docs2kg.ai4wa.com, with the aim of advancing knowledge graph construction research and accelerating enterprise applications through high-quality knowledge graph construction. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Heterogeneous Data; Knowledge Graph; Unstructured Data; Data Accuracy; Data Handling; Data Mining; Graph Theory; Information Retrieval Systems; Iterative Methods; Knowledge Graph; Ontology; Semantics; Collaborative Approach; Graph Construction; Heterogeneous Data; High Quality; Knowledge Graphs; Language Model; Quality Knowledge; Semantics Knowledge; Unstructured Data; Unstructured Documents; Extraction},
	keywords = {Data accuracy; Data handling; Data mining; Graph theory; Information retrieval systems; Iterative methods; Knowledge graph; Ontology; Semantics; Collaborative approach; Graph construction; Heterogeneous data; High quality; Knowledge graphs; Language model; Quality knowledge; Semantics knowledge; Unstructured data; Unstructured documents; Extraction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yang202510299,
	author = {Yang, Yin and Wang, Shisheng and Chen, Yuzhe and Wang, Xinyuan and Jiang, Wei and Jin, Youmei and Zeng, Wenjuan and Wu, Dongbo and Shen, Bairong and Yang, Hao},
	title = {Ontolomics-P: Advancing Proteomics Data Interpretation through GPT-4o Reannotated Topic Ontology and Data-Driven Analysis},
	year = {2025},
	journal = {Analytical Chemistry},
	volume = {97},
	number = {19},
	pages = {10299 - 10308},
	doi = {10.1021/acs.analchem.5c00390},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004388278&doi=10.1021%2Facs.analchem.5c00390&partnerID=40&md5=36ee536b6454ac61a50e333a69ce7101},
	abstract = {The interpretation of proteomics data often relies on functional enrichment analysis, such as Gene Ontology (GO) enrichment, to uncover the biological functions of proteins, as well as the examination of protein expression patterns across data sets like the Clinical Proteomic Tumor Analysis Consortium (CPTAC) database. However, conventional approaches to functional enrichment frequently produce extensive and redundant term lists, complicating interpretation and synthesis. Moreover, the absence of specialized tools tailored to proteomics researchers limits the efficient exploration of protein expression within specific biological contexts. To address these challenges, we developed Ontolomics-P, a user-friendly web-based tool designed to advance proteomics data interpretation. Ontolomics-P integrates topic modeling using latent Dirichlet allocation (LDA) with GO semantic similarity analysis, enabling the consolidation of redundant terms into coherent topics. These topics are further refined and reannotated using the GPT-4o language model, creating a novel topics database that provides precise and interpretable insights into shared biological functions. Additionally, Ontolomics-P incorporates quantitative proteomic data from 10 diverse cancer types archived in the CPTAC database, allowing for a comprehensive exploration of protein expression profiles from a data-driven perspective. Through detailed case studies, we demonstrate the tool’s capacity to streamline workflows, simplify interpretation, and provide actionable biological insights. Ontolomics-P represents a significant advancement in proteomics data analysis, offering innovative solutions for functional annotation, quantitative exploration, and visualization, ultimately empowering researchers to accelerate discoveries in systems biology and beyond. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomolecules; Cloning; Data Reduction; Data Visualization; Gene Transfer; Genome; Biological Functions; Clinical Proteomics; Data Interpretation; Data-driven Analysis; Functional Enrichment Analysis; Gene Ontology; Ontology Enrichment; Ontology's; Protein Expression Patterns; Proteomics},
	keywords = {Biomolecules; Cloning; Data reduction; Data visualization; Gene transfer; Genome; Biological functions; Clinical proteomics; Data interpretation; Data-driven analysis; Functional enrichment analysis; Gene ontology; Ontology enrichment; Ontology's; Protein expression patterns; Proteomics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Carvalho.2025,
	author = {Carvalho., Marco M. and Nembhard, Fitzroy and Mehta, Dhanish},
	title = {Towards the Application of GraphRAG to Network Security},
	year = {2025},
	journal = {Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS},
	volume = {38},
	pages = {},
	doi = {10.32473/flairs.38.1.138895},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007936734&doi=10.32473%2Fflairs.38.1.138895&partnerID=40&md5=ebb4a5ff2358907c5f3718a388f62c78},
	abstract = {The adoption of large language models (LLMs) has facilitated significant advancements in natural language processing. In a short space of time, LLMs have permeated a wide array of disciplines including healthcare, finance, education, etc. However, in their native form, LLMs retain information in their parameters, which sometimes causes the underlying models to produce inaccurate results or hallucinations. To that end, Retrieval-Augmented Generation (RAG) has been proposed to address some of the challenges of LLMs by referencing an external knowledge base while formulating a response to queries. Still, traditional RAG fails to handle the complex structure of relationships among different entities in structured data such as knowledge graphs. GraphRAG, a successor of basic RAG, leverages structural information contained in graphs to enable more precise and comprehensive retrieval thereby facilitating more accurate, context-aware responses. GraphRAG has been applied in many domains, but its use in a cybersecurity context has not been widely explored. In this research, we propose a framework that applies GraphRAG to network security monitoring. By generating knowledge graphs from network logs, we provide LLMs with more structured data, backed by an ontology, that enables the models to perform high-level reasoning to answer questions regarding the security posture of an organization more accurately. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cybersecurity; Graph Structures; Knowledge Graph; Knowledge Management; Natural Language Processing Systems; Query Processing; Complexes Structure; External Knowledge; Knowledge Graphs; Language Model; Language Processing; Native Forms; Natural Languages; Networks Security; Structural Information; Structured Data; Network Security},
	keywords = {Cybersecurity; Graph structures; Knowledge graph; Knowledge management; Natural language processing systems; Query processing; Complexes structure; External knowledge; Knowledge graphs; Language model; Language processing; Native forms; Natural languages; Networks security; Structural information; Structured data; Network security},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Dina2025,
	author = {Dina, Ayesha Siddiqua and Needham, Elijah and Ulybyshev, Denis},
	title = {Large Language Models for Automated Characterization of Cybersecurity Vulnerabilities using N-Shot Learning},
	year = {2025},
	journal = {Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS},
	volume = {38},
	pages = {},
	doi = {10.32473/flairs.38.1.138858},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007898507&doi=10.32473%2Fflairs.38.1.138858&partnerID=40&md5=48fad2183b3a68b68e563f3a6dbd030a},
	abstract = {The US National Vulnerability Database is a public repository of cybersecurity vulnerabilities in software and hardware. This repository is maintained by the National Institute of Standards and Technology (NIST) that developed a Vulnerability Description Ontology framework for characterizing vulnerabilities. Despite advancements in secure software development and vulnerability detection techniques, the number of registered cybersecurity vulnerabilities continues to grow. Characterizing vulnerabilities is essential for selecting effective protection mechanisms to prevent or mitigate cybersecurity vulnerabilities in software and hardware and reduce cyber risks. Manual characterization of vulnerabilities is both time-consuming and costly. While many researchers employ Machine Learning (ML)-based methods to predict characterizations, these methods heavily rely on large amounts of labeled training data. To overcome the challenge of limited labeled data, this paper proposes a solution utilizing three Large Language Models (LLMs) - GPT-4o, Llama-3.1-405B, and Gemini-1.5-flash - to automate the characterization of vulnerabilities across 27 categories, grouped into five noun groups. We use both few-shot and zero-shot learning to prompt the LLMs. Our experimental results show that GPT-4o achieves F1-scores of 80%, 90%, 90%, and 73% in the context, impact-method, attack theater, and logical impact noun groups, respectively, using a few labeled samples. Additionally, Llama achieves an F1-score of 83% in the mitigation noun group. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cybersecurity; Hardware Security; Labeled Data; Labeling; Learning Systems; National Security; Network Security; Zero-shot Learning; Cyber Security; F1 Scores; Language Model; National Institute Of Standards And Technology; National Vulnerability Database; Ontology's; Public Repositories; Secure Software Development; Software And Hardwares; Vulnerability Description; Computational Linguistics},
	keywords = {Cybersecurity; Hardware security; Labeled data; Labeling; Learning systems; National security; Network security; Zero-shot learning; Cyber security; F1 scores; Language model; National Institute of Standards and Technology; National vulnerability database; Ontology's; Public repositories; Secure software development; Software and hardwares; Vulnerability description; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Cain2025,
	author = {Cain, Pallas Athena and Jumadinova, Janyl and Brand, Heather},
	title = {Am.I.: Investigating AI Bias, Expression, and Humanities Through GPT-Powered Robotics},
	year = {2025},
	journal = {Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS},
	volume = {38},
	pages = {},
	doi = {10.32473/flairs.38.1.138887},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007878221&doi=10.32473%2Fflairs.38.1.138887&partnerID=40&md5=3e7ca930b52522562d3c51cad3b97d0a},
	abstract = {Am.I. lies at the intersection of artificial intelligence, robotics, and the humanities, integrating large language models (LLMs) with responsive robotic systems. The project features a reactive robotic head that engages in real-time philosophical dialogues using GPT models. This robotic head converses with a parallel AI system, generating dynamic and thought-provoking exchanges. Its expressions are synchronized with the perceived emotional tone of its dialogue, analyzed in real-time using LLM-based sentiment analysis. The central focus of Am.I. is to explore the ontological beliefs and implicit biases embedded in LLMs by employing diverse prompting techniques. These techniques demonstrate how conversational strategies can steer AI toward specific philosophical perspectives on human existence. This process uncovers the extent to which these systems reflect or deviate from human thought patterns, raising critical questions about the role of AI in interpreting and shaping humanities discourse. Ultimately, this research underscores the potential of LLMs to advance the humanities, particularly in examining their capacity to articulate and engage with complex philosophical ideas. By bridging technological innovation and artistic expression, Am.I. offers an immersive exploration of AI’s role in understanding, reflecting on, and potentially challenging our perspectives on existence. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ethical Technology; Ai Systems; Critical Questions; Language Model; Model-based Opc; Reactive Robotics; Real- Time; Robotic Head; Robotic Systems; Sentiment Analysis; Technological Innovation; Intelligent Robots},
	keywords = {Ethical technology; AI systems; Critical questions; Language model; Model-based OPC; Reactive robotics; Real- time; Robotic head; Robotic systems; Sentiment analysis; Technological innovation; Intelligent robots},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ruehle20251534,
	author = {Ruehle, Bastian},
	title = {Natural language processing for automated workflow and knowledge graph generation in self-driving labs},
	year = {2025},
	journal = {Digital Discovery},
	volume = {4},
	number = {6},
	pages = {1534 - 1543},
	doi = {10.1039/d5dd00063g},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005436131&doi=10.1039%2Fd5dd00063g&partnerID=40&md5=4dbb7d935f4b0ea3727f57c56f5094a9},
	abstract = {Natural language processing with the help of large language models such as ChatGPT has become ubiquitous in many software applications and allows users to interact even with complex hardware or software in an intuitive way. The recent concepts of Self-Driving Labs and Material Acceleration Platforms stand to benefit greatly from making them more accessible to a broader scientific community through enhanced user-friendliness or even completely automated ways of generating experimental workflows that can be run on the complex hardware of the platform from user input or previously published procedures. Here, two new datasets with over 1.5 million experimental procedures and their (semi)automatic annotations as action graphs, i.e., structured output, were created and used for training two different transformer-based large language models. These models strike a balance between performance, generality, and fitness for purpose and can be hosted and run on standard consumer-grade hardware. Furthermore, the generation of node graphs from these action graphs as a user-friendly and intuitive way of visualizing and modifying synthesis workflows that can be run on the hardware of a Self-Driving Lab or Material Acceleration Platform is explored. Lastly, it is discussed how knowledge graphs - following an ontology imposed by the underlying node setup and software architecture - can be generated from the node graphs. All resources, including the datasets, the fully trained large language models, the node editor, and scripts for querying and visualizing the knowledge graphs are made publicly available. © 2025 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Perini2025,
	author = {Perini, Marco and Antonucci, Daniele and Giudice, Rocco and Piscitelli, Marco Savino and Capozzoli, Alfonso},
	title = {BrickLLM: A Python library for generating Brick-compliant RDF graphs using LLMs},
	year = {2025},
	journal = {SoftwareX},
	volume = {30},
	pages = {},
	doi = {10.1016/j.softx.2025.102121},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000797991&doi=10.1016%2Fj.softx.2025.102121&partnerID=40&md5=7f6b3f13a501848795a1ca05f2d8be99},
	abstract = {One of the key challenges of Energy Management and Information Systems in buildings is related to the lack of interoperability, due to the absence of standardization of the underlying data models. In recent years, there has been a growing interest in using ontology-based metadata models to address this issue, as they offer a structured approach to organize and share information across diverse systems (e.g. Brick ontology). However, the creation of ontology-based metadata models is often a labor-intensive task that requires specific domain expertise, hindering the practical use of such data models. For this reason, in this work the BrickLLM Python library is introduced, which addresses this issue by generating Brick-compliant Resource Description Framework graphs through Large Language Models, automating the process of converting natural language building descriptions into machine-readable metadata. The library supports both cloud-based APIs (e.g., OpenAI, Anthropic, Fireworks AI), local models (e.g. LLaMa3.2, etc.) and evenfine-tuned ones. This paper explores the architecture, key functionalities, and practical applications of BrickLLM, showcasing its potential impact on the future of building systems monitoring and automation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Brick; Energy Management And Information Systems; Large Language Models; Portability; Rdf; Information Management; Metadata; Network Security; Ontology; Problem Oriented Languages; Energy; In-buildings; Language Model; Large Language Model; Metadata Model; Ontology-based; Portability; Rdf; Rdf Graph; Structured Approach; Brick},
	keywords = {Information management; Metadata; Network security; Ontology; Problem oriented languages; Energy; In-buildings; Language model; Large language model; Metadata model; Ontology-based; Portability; RDF; RDF graph; Structured approach; Brick},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kabeli2025,
	author = {Kabeli, Romi Goldner and Boursi, Ben and Zilberberg, Alona and Efroni, Sol},
	title = {Leveraging machine learning for integrative analysis of T-cell receptor repertoires in colorectal cancer: Insights into MAIT cell dynamics and risk assessment},
	year = {2025},
	journal = {Translational Oncology},
	volume = {55},
	pages = {},
	doi = {10.1016/j.tranon.2025.102358},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000732468&doi=10.1016%2Fj.tranon.2025.102358&partnerID=40&md5=52d2a242ed679be00853e6e57d729690},
	abstract = {This study investigates the T-cell receptor (TCR) repertoires in colorectal cancer (CRC) patients by analyzing three distinct datasets: one bulk sequencing dataset of 205 patients with various tumor stages, all newly diagnosed at Sheba Medical Center between 2017 and 2022, with minimal recruitment in 2014 and 2016, and two (public) single-cell sequencing datasets of 10 and 12 patients. Despite the significant variability in the TCR repertoire and the low likelihood of sequence overlap, our analysis reveals an interesting set of TCR sequences across these data. Notably, we observe elevated presence of mucosal-associated invariant T (MAIT) cells in both metastatic and non-metastatic patients. Furthermore, we identify nine identical TCR alpha and TCR beta pairs that appear in both single-cell datasets, with 13 out of 18 sequences from these sequences also appearing in the bulk data. Clinical risk analysis over the bulk dataset, using a subset of these unique sequences, demonstrates a correlation between TCR repertoire disease stage and risk. These findings enhance our understanding of the TCR landscape in CRC and underscore the potential of TCR sequences as biomarkers for disease outcome. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Crc; Deep Learning; Mait Cells; Risk Assessment; T Cell Repertoire; Tcr; A10 Gpu With 2 Device; Immunarch Package; Nextseq 500/550 Mid Output Kit V2.5; Open-source R Version 4.1.0; Python 3.8; Rneasy Mini Kit; Scanpy Package; Smarter Human Tcr Profiling Kit V2; Biological Marker; T Lymphocyte Receptor; Transcriptome; Adult; Article; B Lymphocyte; Bioinformatics; Cancer Risk; Cancer Staging; Cd4+ T Lymphocyte; Cd8+ T Lymphocyte; Cell Infiltration; Colorectal Cancer; Controlled Study; Deep Learning; Diagnostic Test Accuracy Study; Distant Metastasis; Female; Gene Expression; Gene Expression Profiling; Gene Ontology; Gene Set Enrichment Analysis; High Throughput Sequencing; Human; Human Cell; Human Tissue; Immune Response; Immunocompetent Cell; Immunohistochemistry; Integrative Oncology; Language Model; Large Language Model; Learning; Liver Metastasis; Machine Learning; Major Clinical Study; Male; Metastasis; Mucosal-associated Invariant T Cell; Outcome Assessment; Overall Survival; Peripheral Blood Mononuclear Cell; Polymerase Chain Reaction; Principal Component Analysis; Receiver Operating Characteristic; Regulatory T Lymphocyte; Risk Assessment; Rna Sequence; Rna Sequencing; Single Cell Analysis; Single Cell Rna Seq; T Lymphocyte; Transcriptomics; Treatment Response; Tumor Microenvironment; Whole Exome Sequencing; Xgboost},
	keywords = {biological marker; T lymphocyte receptor; transcriptome; adult; Article; B lymphocyte; bioinformatics; cancer risk; cancer staging; CD4+ T lymphocyte; CD8+ T lymphocyte; cell infiltration; colorectal cancer; controlled study; deep learning; diagnostic test accuracy study; distant metastasis; female; gene expression; gene expression profiling; gene ontology; gene set enrichment analysis; high throughput sequencing; human; human cell; human tissue; immune response; immunocompetent cell; immunohistochemistry; integrative oncology; language model; large language model; learning; liver metastasis; machine learning; major clinical study; male; metastasis; mucosal-associated invariant T cell; outcome assessment; overall survival; peripheral blood mononuclear cell; polymerase chain reaction; principal component analysis; receiver operating characteristic; regulatory T lymphocyte; risk assessment; RNA sequence; RNA sequencing; single cell analysis; single cell RNA seq; T lymphocyte; transcriptomics; treatment response; tumor microenvironment; whole exome sequencing; xgBoost},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ellouze2025,
	author = {Ellouze, Mourad and Rekik, Sonda and Hadrich-Belguith, Lamia},
	title = {Management of psychological emergency cases on social media: A hybrid approach combining knowledge graphs and graph neural networks},
	year = {2025},
	journal = {Online Social Networks and Media},
	volume = {46},
	pages = {},
	doi = {10.1016/j.osnem.2025.100308},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219273747&doi=10.1016%2Fj.osnem.2025.100308&partnerID=40&md5=10f0a0dd19257e694f64d3237bdc7990},
	abstract = {The effects of psychological crises are evolving at an astounding rate nowadays, presenting a significant challenge for everyone involved in tracking these disorders. Therefore, we propose in this paper a hybrid approach based on linguistic processing and numerical techniques allowing to: (i) identify the presence of psychological emergencies among social network users by analyzing their textual production, (ii) determine the specific type of emergency case, (iii) elaborate a graph for each type of emergency, reflecting the different dimensions linked to the psychological emergency, allowing for a better diagnosis of the situation and providing an overall view of the crisis type, (iv) combine the separate graphs for each emergency to address the various semantic aspects. The work was accomplished using advanced language model techniques, knowledge graphs and neural network graphs. The combination of these techniques ensures that their advantages are leveraged while overcoming their limitations in terms of result generalization. The evaluation of different parts related to detecting the presence of psychological problems, predicting specific type of emergency cases, and detecting links between knowledge graphs was measured using the F-measure metric. The values derived from this measure, corresponding to the evaluation of these three tasks, are, respectively, 83%, 87% and 80%. For the evaluation of the elaboration of each graph related to specific type of emergency cases, this was accomplished using qualitative metric standards. The results obtained can be considered encouraging given the significant scale of our approach. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Neural Networks; Natural Language Processing; Ontology; Personality Disorder; Psychological Emergency; Social Media; Graph Neural Networks; Hybrid Approach; Knowledge Graphs; Language Processing; Natural Language Processing; Natural Languages; Ontology's; Personality Disorder; Psychological Emergency; Social Media; Knowledge Graph},
	keywords = {Graph neural networks; Hybrid approach; Knowledge graphs; Language processing; Natural language processing; Natural languages; Ontology's; Personality disorder; Psychological emergency; Social media; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Jähde2025,
	author = {Jähde, Orlando and Weber, Thorsten and Buchkremer, Rüdiger},
	title = {Unraveling media perspectives: a comprehensive methodology combining large language models, topic modeling, sentiment analysis, and ontology learning to analyse media bias},
	year = {2025},
	journal = {Journal of Computational Social Science},
	volume = {8},
	number = {2},
	pages = {},
	doi = {10.1007/s42001-025-00372-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218922863&doi=10.1007%2Fs42001-025-00372-0&partnerID=40&md5=8921fb3bae2482a1eaa3f0b0eac10ea8},
	abstract = {Biased news reporting poses a significant threat to informed decision-making and the functioning of democracies. This study introduces a novel methodology for scalable, minimally biased analysis of media bias in political news. The proposed approach examines event selection, labeling, word choice, and commission and omission biases across news sources by leveraging natural language processing techniques, including hierarchical topic modeling, sentiment analysis, and ontology learning with large language models. Through three case studies related to current political events, we demonstrate the methodology’s effectiveness in identifying biases across news sources at various levels of granularity. This work represents a significant step towards scalable, minimally biased media bias analysis, laying the groundwork for tools to help news consumers navigate an increasingly complex media landscape. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Model; Machine Learning; Media Bias; Natural Language Processing; Ontology Learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Shimizu2025,
	author = {Shimizu, Cogan Matthew and Hitzler, Pascal Al},
	title = {Accelerating knowledge graph and ontology engineering with large language models},
	year = {2025},
	journal = {Journal of Web Semantics},
	volume = {85},
	pages = {},
	doi = {10.1016/j.websem.2025.100862},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217920373&doi=10.1016%2Fj.websem.2025.100862&partnerID=40&md5=dfcc88d8658becbb8800941289e092e2},
	abstract = {Large Language Models bear the promise of significant acceleration of key Knowledge Graph and Ontology Engineering tasks, including ontology modeling, extension, modification, population, alignment, as well as entity disambiguation. We lay out LLM-based Knowledge Graph and Ontology Engineering as a new and coming area of research, and argue that modular approaches to ontologies will be of central importance. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Entity Disambiguation; Knowledge Graph Engineering; Large Language Models; Modular Ontologies; Ontology Alignment; Ontology Engineering; Ontology Modeling; Ontology Population; Entity Disambiguation; Knowledge Graph Engineering; Knowledge Graphs; Language Model; Large Language Model; Modular Ontologies; Ontology Alignment; Ontology Engineering; Ontology Model; Ontology Population; Knowledge Graph},
	keywords = {Entity disambiguation; Knowledge graph engineering; Knowledge graphs; Language model; Large language model; Modular ontologies; Ontology alignment; Ontology engineering; Ontology model; Ontology Population; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Sahadevan2025,
	author = {Sahadevan, Vijayalaxmi and Joshi, Rohin and Borg, Kane and Singh, Vishal and Singh, Abhishek Raj and Muhammed, Bilal and Beemaraj, Soban Babu and Joshi, Amol Dilip},
	title = {Knowledge augmented generalizer specializer: A framework for early stage design exploration},
	year = {2025},
	journal = {Advanced Engineering Informatics},
	volume = {65},
	pages = {},
	doi = {10.1016/j.aei.2025.103141},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216774787&doi=10.1016%2Fj.aei.2025.103141&partnerID=40&md5=4244983c7cb226335b3b04acc08b02d4},
	abstract = {In non-routine engineering design projects, the design outcome is determined by how the problem is formulated and represented in the early conceptual stage. The problem representation comprises schemas, ontologies, variables, and parameters relevant to the given problem class. Despite the critical role of early conceptual decisions in shaping the eventual design outcome, most of the computational support and automation are focused on the latter stages of parametric modelling, problem-solving, and optimization. There is inadequate support for aiding and automating problem formulation, variable and parameter identification and representation, and early-stage conceptual decisions. Therefore, this paper presents an innovative, transparent, and explainable method employing semantic reasoning to automate the step-by-step conceptual design generation process, including problem formulation, identification and representation of the variables and parameters and their dependencies. The method is realized through a novel framework called Knowledge Augmented Generalizer Specializer (KAGS). KAGS employs the Function-Behavior-Structure (FBS) ontology and the Graph-of-Thought (GoT) mechanism to enable automated reasoning with a Large Language Model (LLM). The workflow comprises various stages: problem breakdown, design prototype creation, assessment, and prototype merging. The framework is implemented and tested on a Subsea Layout (SSL) planning problem, a special class of infrastructure planning projects in deep-sea oil and gas production systems. The experimentations with KAGS demonstrate its capacity to support problem formulation, hierarchical decomposition, and solution generation. The research also provides new insights into the FBS framework and meta-level reasoning in early design stages. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Design Exploration; Design Outcomes; Early Stage Designs; Engineering Design Programs; Model Problems; Ontology's; Parametric Models; Problem Formulation; Problem Representation; Routine Engineerings; Gas Industry},
	keywords = {Design Exploration; Design outcomes; Early stage designs; Engineering design programs; Model problems; Ontology's; Parametric models; Problem formulation; Problem representation; Routine engineerings; Gas industry},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Li2025,
	author = {Li, Jiangmeng and Mo, Wenyi and Song, Fei and Sun, Chuxiong and Qiang, Wenwen and Su, Bing and Zheng, Changwen},
	title = {Supporting vision-language model few-shot inference with confounder-pruned knowledge prompt},
	year = {2025},
	journal = {Neural Networks},
	volume = {185},
	pages = {},
	doi = {10.1016/j.neunet.2025.107173},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215571763&doi=10.1016%2Fj.neunet.2025.107173&partnerID=40&md5=4b47d7da135080e10916d1f6eedd914b},
	abstract = {Vision-language models are pre-trained by aligning image-text pairs in a common space to deal with open-set visual concepts. Recent works adopt fixed or learnable prompts, i.e., classification weights are synthesized from natural language descriptions of task-relevant categories, to reduce the gap between tasks during the pre-training and inference phases. However, how and what prompts can improve inference performance remains unclear. In this paper, we explicitly clarify the importance of incorporating semantic information into prompts, while existing prompting methods generate prompts without sufficiently exploring the semantic information of textual labels. Manually constructing prompts with rich semantics requires domain expertise and is extremely time-consuming. To cope with this issue, we propose a knowledge-aware prompt learning method, namely Confounder-pruned Knowledge Prompt (CPKP), which retrieves an ontology knowledge graph by treating the textual label as a query to extract task-relevant semantic information. CPKP further introduces a double-tier confounder-pruning procedure to refine the derived semantic information. Adhering to the individual causal effect principle, the graph-tier confounders are gradually identified and phased out. The feature-tier confounders are eliminated by following the maximum entropy principle in information theory. Empirically, the evaluations demonstrate the effectiveness of CPKP in few-shot inference, e.g., with only two shots, CPKP outperforms the manual-prompt method by 4.64% and the learnable-prompt method by 1.09% on average. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Large-scale Pre-training; Maximum Entropy; Multi-modal Model; Prompt Learning; Semantics; Visual Languages; Confounder; Knowledge Graphs; Large-scale Pre-training; Large-scales; Maximum-entropy; Modal Models; Multi-modal; Multi-modal Model; Pre-training; Prompt Learning; Knowledge Graph; Article; Confounding Variable; Human; Knowledge; Language Model; Learning; Maximum Entropy Model; Semantics; Training; Vision; Algorithm; Artificial Neural Network; Language; Machine Learning; Natural Language Processing; Algorithms; Humans; Knowledge; Language; Machine Learning; Natural Language Processing; Neural Networks, Computer},
	keywords = {Semantics; Visual languages; Confounder; Knowledge graphs; Large-scale pre-training; Large-scales; Maximum-entropy; Modal models; Multi-modal; Multi-modal model; Pre-training; Prompt learning; Knowledge graph; article; confounding variable; human; knowledge; language model; learning; maximum entropy model; semantics; training; vision; algorithm; artificial neural network; language; machine learning; natural language processing; Algorithms; Humans; Knowledge; Language; Machine Learning; Natural Language Processing; Neural Networks, Computer},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liao2025,
	author = {Liao, Xingming and Chen, Chong and Wang, Zhuowei and Liu, Ying and Wang, Tao and Cheng, Lianglun},
	title = {Large language model assisted fine-grained knowledge graph construction for robotic fault diagnosis},
	year = {2025},
	journal = {Advanced Engineering Informatics},
	volume = {65},
	pages = {},
	doi = {10.1016/j.aei.2025.103134},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215555067&doi=10.1016%2Fj.aei.2025.103134&partnerID=40&md5=90b6374c947ab57f039cf5dc80239c66},
	abstract = {With the rapid deployment of industrial robots in manufacturing, the demand for advanced maintenance techniques to sustain operational efficiency has become crucial. Fault diagnosis Knowledge Graph (KG) is essential as it interlinks multi-source data related to industrial robot faults, capturing multi-level semantic associations among different fault events. However, the construction and application of fine-grained fault diagnosis KG face significant challenges due to the inherent complexity of nested entities in maintenance texts and the severe scarcity of annotated industrial data. In this study, we propose a Large Language Model (LLM) assisted data augmentation approach, which handles the complex nested entities in maintenance corpora and constructs a more fine-grained fault diagnosis KG. Firstly, the fine-grained ontology is constructed via LLM Assistance in Industrial Nested Named Entity Recognition (assInNNER). Then, an Industrial Nested Label Classification Template (INCT) is designed, enabling the use of nested entities in Attention-map aware keyword selection for the Industrial Nested Language Model (ANLM) data augmentation methods. ANLM can effectively improve the model's performance in nested entity extraction when corpora are scarce. Subsequently, a Confidence Filtering Mechanism (CFM) is introduced to evaluate and select the generated data for enhancement, and assInNNER is further deployed to recall the negative samples corpus again to further improve performance. Experimental studies based on multi-source corpora demonstrate that compared to existing algorithms, our method achieves an average F1 increase of 8.25 %, 3.31 %, and 1.96 % in 5%, 10 %, and 25 % in few-shot settings, respectively. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Fault Diagnosis; Industrial Robots; Knowledge Graph; Large Language Model; Industrial Robots; Semantics; Data Augmentation; Faults Diagnosis; Fine Grained; Graph Construction; Knowledge Graphs; Language Model; Large Language Model; Multi-sources; Named Entity Recognition; Rapid Deployments; Knowledge Graph},
	keywords = {Industrial robots; Semantics; Data augmentation; Faults diagnosis; Fine grained; Graph construction; Knowledge graphs; Language model; Large language model; Multi-Sources; Named entity recognition; Rapid deployments; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@ARTICLE{Val-Calvo2025,
	author = {Val-Calvo, Mikel and Egaña Aranguren, Mikel and Mulero-Hernández, Juan and Almagro-Hernández, Ginés and Deshmukh, Prashant and Bernabé-Díaz, José Antonio and Espinoza-Arias, Paola and Sánchez-Fernández, José Luis and Mueller, Juergen and Fernández-Breis, Jesualdo Tomás},
	title = {OntoGenix: Leveraging Large Language Models for enhanced ontology engineering from datasets},
	year = {2025},
	journal = {Information Processing and Management},
	volume = {62},
	number = {3},
	pages = {},
	doi = {10.1016/j.ipm.2024.104042},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212843729&doi=10.1016%2Fj.ipm.2024.104042&partnerID=40&md5=d6b1cfb2b4b74e70c55de22ed3c61f10},
	abstract = {Knowledge Graphs integrate data from multiple, heterogeneous sources, using ontologies to facilitate data interoperability. Ontology development is a resource-consuming task that requires the collaborative work of domain experts and ontology engineers. Therefore, companies invest considerable resources in order to generate and maintain Enterprise Knowledge Graphs and ontologies from large and complex datasets, most of which can be unfamiliar for ontology engineers. In this work, we study the use of Large Language Models to aid in the development of ontologies from datasets, ultimately increasing the automation of the generation of ontology-based Knowledge Graphs. As a result we have developed a structured workflow that leverages Large Language Models to enhance ontology engineering through data pre-processing, ontology planning, building, and entity improvement. Our method is also able to generate mappings and RDF data, but in this work we focus on the ontologies. The pipeline has been implemented in the OntoGenix tool. In this work we show the results of the application of OntoGenix to six datasets related to commercial activities. The findings indicate that the ontologies produced exhibit patterns of coherent modeling, and features that closely resemble those created by humans, although the most complex situations are better reflected by the ontologies developed by humans. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graphs; Large Language Models; Ontology Engineering; Data Assimilation; Modeling Languages; Collaborative Work; Data Interoperability; Domain Experts; Heterogeneous Sources; Knowledge Graphs; Language Model; Large Language Model; Ontology Development; Ontology Engineering; Ontology's; Knowledge Graph},
	keywords = {Data assimilation; Modeling languages; Collaborative Work; Data interoperability; Domain experts; Heterogeneous sources; Knowledge graphs; Language model; Large language model; Ontology development; Ontology engineering; Ontology's; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Dong2025325,
	author = {Dong, Lei and Wu, Fuju and Shi, Jianyong and Pan, Longfei},
	title = {Construction and Application of Multimodal Knowledge Graph in Construction Safety Field Based on Large Language Model; 基于大语言模型的施工安全多模态知识图谱的构建与应用},
	year = {2025},
	journal = {Computer Engineering and Applications},
	volume = {61},
	number = {9},
	pages = {325 - 333},
	doi = {10.3778/j.issn.1002-8331.2408-0036},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007312979&doi=10.3778%2Fj.issn.1002-8331.2408-0036&partnerID=40&md5=38294bcdba3db4bf72e051b7be11d912},
	abstract = {It is difficult for existing construction safety management methods to effectively integrate multi-modal information of text and picture, and the knowledge expression and reasoning ability in the field of construction site safety accidents are limited, and the processing and application of data require a wide range of domain knowledge and professional background. To solve this problem, this paper proposes a multimodal knowledge graph construction method based on multimodal large language model. Through three steps of data collection and preprocessing, ontology construction at concept level and instance level, the multimodal knowledge graph is constructed to solve the problems of multimodal integration of text and picture and the limited knowledge expression and reasoning ability in the field. The knowledge map constructed not only integrates the accident safety knowledge in the text, but also includes the scene picture information, which improves the comprehensiveness and practicality of the knowledge. Three indexes of accuracy, recall rate and F1 value are used to evaluate the extraction results, and high scores are obtained, which verify the rationality and accuracy of the large model for image extraction. In practical application, this method is helpful for safety managers to discover safety accidents on construction site in time, and provides important support for management decision-making and intelligent reasoning. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Extraction; Large Language Model; Multimodal Knowledge Graph; Ontology Construction; Safety Management; Highway Accidents; Knowledge Graph; Construction Safety; Knowledge Extraction; Knowledge Graphs; Knowledge Reasoning; Language Model; Large Language Model; Multi-modal; Multimodal Knowledge Graph; Ontology Construction; Safety Management; Decision Making},
	keywords = {Highway accidents; Knowledge graph; Construction safety; Knowledge extraction; Knowledge graphs; Knowledge reasoning; Language model; Large language model; Multi-modal; Multimodal knowledge graph; Ontology construction; Safety management; Decision making},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025,
	author = {Li, Bei and Li, Changbiao and Sun, Jianwei and Zeng, Xu and Chen, Xiaofan and Zheng, Jing},
	title = {ERNIE-UIE: Advancing information extraction in Chinese medical knowledge graph},
	year = {2025},
	journal = {PLOS ONE},
	volume = {20},
	number = {5 May},
	pages = {},
	doi = {10.1371/journal.pone.0325082},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006545313&doi=10.1371%2Fjournal.pone.0325082&partnerID=40&md5=3597a24d4c3511060023944baefbfb17},
	abstract = {Background The field of information extraction (IE) is currently exploring more versatile and efficient methods for minimization of reliance on extensive annotated datasets and integration of knowledge across tasks and domains. Objective We aim to evaluate and refine the application of the universal IE (UIE) technology in the field of Chinese medical expertise in terms of processing accuracy and efficiency. Methods Our model integrates ontology modeling, web scraping, UIE, fine-tuning strategies, and graph databases, thereby covering knowledge modeling, extraction, and storage techniques. The Enhanced Representation through Knowledge Integration-UIE (ERNIE-UIE) model is fine-tuned and optimized using a small amount of annotated data. A medical knowledge graph is then constructed, followed by validating the graph and conducting knowledge mining on the data stored within it. Results Incorporating the characteristics of whole-course management, we implemented a comprehensive medical knowledge graph–construction model and methodology. Entities and relationships were jointly extracted using the pretrained language model, resulting in 8,525 entity data points and 9,522 triple data points. The accuracy of the knowledge graph was verified using graph algorithms. Conclusion We optimized the construction process of a Chinese medical knowledge graph with minimal annotated data by utilizing a generative extraction paradigm, validating the graph’s efficacy and achieving commendable results. This approach addresses the challenge of insufficient annotated training corpora in low-resource knowledge graph construction, thereby contributing to cost savings in the development of knowledge graphs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Algorithm; Article; Cost Control; Data Base; Drug Therapy; Human; Knowledge; Knowledge Graph; Language Model; Mining; Ontology; Open Access Publishing; China; Data Mining; Factual Database; Information Retrieval; Medical Informatics; Procedures; Algorithms; Data Mining; Databases, Factual; Information Storage And Retrieval; Medical Informatics},
	keywords = {algorithm; article; cost control; data base; drug therapy; human; knowledge; knowledge graph; language model; mining; ontology; open access publishing; China; data mining; factual database; information retrieval; medical informatics; procedures; Algorithms; Data Mining; Databases, Factual; Information Storage and Retrieval; Medical Informatics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Xiao20251318,
	author = {Xiao, Gang and Fang, Jingwen and Zhang, Hao and Liu, Ying and Zhou, Xiaofeng and Xu, Jun},
	title = {Celadon cross-modal knowledge graph construction via a visual-language model; 视觉语言模型引导的青瓷跨模态知识图谱构建},
	year = {2025},
	journal = {Journal of Image and Graphics},
	volume = {30},
	number = {5},
	pages = {1318 - 1333},
	doi = {10.11834/jig.240234},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005448057&doi=10.11834%2Fjig.240234&partnerID=40&md5=319cc7a30319aeb25ddaab7686acc84c},
	abstract = {Objective Celadon is not only a dazzling pearl among the cultural treasures of the Chinese nation but also a cultural messenger in cultural exchanges between China and other countries. It has rich historical and cultural connotations and demonstrates excellent artistic value. Its elegant shape and moist glaze make it an outstanding representative of traditional Chinese craft aesthetics. The production of celadon embodies the wisdom and creativity of ancient craftsmen and is an important carrier for the inheritance of excellent traditional Chinese culture. In the context of cultural digitization, constructing a cross-modal knowledge graph of celadon is one of the key technologies for promoting the protection and inheritance of celadon culture. In this process, matching the same entities across different modalities, which involves aligning the different modal features of equivalent entities, is crucial. However, the inherent structural differences between cross-modal data present challenges for alignment tasks. Traditional methods that rely on manually annotated data can ensure the accuracy of alignment to some extent, but they have problems such as low efficiency and high cost. In addition, coarse-grained annotated data can hardly meet the requirements for fine-grained concepts and for entity recognition when constructing a cross-modal knowledge graph. At present, the vision-language pretraining (VLP) model can effectively capture cross-modal semantic associations by learning rich cross-modal representations from large-scale unmarked image-text pair data. The strong cross-modal understanding ability of the VLP model can provide precise semantic associations and fine-grained entity recognition for aligning entities of different modalities in graph construction. Here, a cross-modal entity alignment method based on the VLP model, which can map multiple features of images, is proposed to maximize the degree of matching between celadon images and text. Method The cross-modal entity alignment method proposed in this study, which maps multiple features of images, is initialized with the publicly available VLP model for both the image and the text encoders, and the parameters of the encoders remain unchanged during the training process. The method mainly consists of four parts. First, on the basis of the visual characteristics of celadon images, local features in terms of contour, texture, and color are extracted. Then, a gated multifusion unit is introduced to adaptively assign weights to the image features, and the extracted multiple local image features are used to generate reliable fused features. Furthermore, a multilayer fully connected mapper is designed to learn the mapping of the fused features to an appropriate intermediate representation space by using multiple layers of nonlinear transformations, guiding the text encoder to generate text features that match the image features more closely. Finally, the model is trained and optimized via the information noise contrastive estimation loss function, that is, by optimizing the similarity of positive sample pairs and the difference in negative sample pairs through calculating the cosine similarity between cross-modality features, thereby establishing the connection between image features and text features. Result The proposed method was compared with four of the latest benchmark methods in an experimental comparison, namely, contrastive VLP in Chinese (CN-CLIP), context optimization (CoOp), conditional context optimization (CoCoOp), and mapping pictures to words (Pic2Word). The quantitative evaluation metrics are the recall rates, including R@1, R@5, R@10, and the mean recall(MR). The experiments were conducted using the ChinaWare dataset, so all methods were trained on this dataset. A data table comparing each method’s performance on recall rate metrics was provided. In terms of the MR metric, the proposed method outperformed zero-shot CN-CLIP<inf>ViT−B/16</inf> by 3. 2% in the text-to-image alignment task and by 7. 5% in the image-to-text task. CoOp focuses on text features;it also outperforms CoOp by 11. 4% and 12. 1%, respectively. Moreover, CoCoOp considers image features on the basis of CoOp, and the proposed method outperforms CoCoOp by 8. 4% and 9. 5%, respectively. Pic2Word also focuses on original image features and does not fully utilize other local image features to improve model performance, and the proposed method outperforms Pic2Word by 5. 8% and 5. 6%, respectively. Conclusion The cross-modal entity alignment method proposed in this study can fully explore the effective intermediate representation of image features to reconstruct text features without changing the parameters of the VLP model, thereby improving the cross-modal recognition accuracy of the details of celadon. The experimental results show that this method is superior to several state-of-the-art methods and has improved the performance of alignment. Ultimately, a celadon cross-modal knowledge graph with 8 949 nodes and 18 211 relationships was successfully constructed by applying technologies such as ontology modeling, data mining, and the cross-modal entity alignment method proposed in this study. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Celadon; Cross-modal; Entity Alignment; Knowledge Graph(kg); Vision-language Model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang202599,
	author = {Zhang, Lulu and Zhao, Weisong and Cheng, Zhiwei and Jiang, Yafei and Tian, Kai and Shi, Jia and Jiang, Zhenyu and Hua, Yingqi},
	title = {Osteosarcoma knowledge graph question answering system: deep learning-based knowledge graph and large language model fusion},
	year = {2025},
	journal = {Intelligent Medicine},
	volume = {5},
	number = {2},
	pages = {99 - 110},
	doi = {10.1016/j.imed.2024.12.001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003949553&doi=10.1016%2Fj.imed.2024.12.001&partnerID=40&md5=7033bc67ad5d1b9834e68a244b41416b},
	abstract = {Objective: Osteosarcoma is a prevalent primary malignant bone tumor in children and adolescents, accounting for approximately 5 % of childhood malignancies. Because of its rarity and biological complexity, treatment breakthroughs for osteosarcoma have been limited. To advance research in this field, we aimed to construct the first comprehensive osteosarcoma knowledge graph (OSKG) using the PubMed database. Methods: A systematic search of PubMed (2003–2023) using the keyword “osteosarcoma” yielded 25,415 abstracts. Leveraging BioBERT, pretrained on biomedical corpora and fine-tuned with osteosarcoma-specific manual annotations, we identified 16 entity types and 17 biological relationships. The extracted elements were synthesized to create the OSKG, resulting in a deep learning-based knowledge base to explore osteosarcoma pathogenesis and molecular mechanisms. We then developed a specialized question-answering system (knowledge graph question answering (KGQA)) powered by ChatGLM3. This system employs advanced natural language processing and incorporates the OSKG to ensure optimal response quality and accuracy. Results: The pretrained BioBERT averaged > 92 % accuracy in entity and relationship training. Evaluation using 100 pairs of gold-standard quizzes showed that the final quiz system outperformed other large language models in accuracy and robustness. Conclusion: The system is designed to provide accurate disease-related queries and answers, effectively facilitating knowledge acquisition and reasoning in medical research and clinical practice. This project offers a robust tool for osteosarcoma research and promotes the deep integration of knowledge graphs and artificial intelligence technologies in the medical field. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Large Language Model; Osteosarcoma; Text Mining; Biobert-base-cased-v1.2; Domain Knowledge; Expert Systems; Knowledge Engineering; Natural Language Processing Systems; Pathogens; Biological Complexity; Bone Tumor; Children And Adolescents; Knowledge Graphs; Language Model; Language Model Fusion; Large Language Model; Osteosarcomas; Question Answering Systems; Text-mining; Deep Learning; Accuracy; Article; Artificial Intelligence; Cancer Growth; Deep Learning; Ewing Sarcoma; Gene Ontology; Gene Regulatory Network; Human; Knowledge Graph; Large Language Model; Natural Language Processing; Osteosarcoma; Prediction; Training},
	keywords = {Domain Knowledge; Expert systems; Knowledge engineering; Natural language processing systems; Pathogens; Biological complexity; Bone tumor; Children and adolescents; Knowledge graphs; Language model; Language model fusion; Large language model; Osteosarcomas; Question answering systems; Text-mining; Deep learning; accuracy; Article; artificial intelligence; cancer growth; deep learning; Ewing sarcoma; gene ontology; gene regulatory network; human; knowledge graph; large language model; natural language processing; osteosarcoma; prediction; training},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Groza20251158,
	author = {Groza, Tudor and Rayabsri, Warittha and Gration, Dylan and Hariram, Harshini and Jamuar, Saumya Shekhar and Baynam, Gareth S.},
	title = {First steps toward building natural history of diseases computationally: Lessons learned from the Noonan syndrome use case},
	year = {2025},
	journal = {American Journal of Human Genetics},
	volume = {112},
	number = {5},
	pages = {1158 - 1172},
	doi = {10.1016/j.ajhg.2025.03.014},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003553182&doi=10.1016%2Fj.ajhg.2025.03.014&partnerID=40&md5=c8248ed01d985d13d6cb618c1774f824},
	abstract = {Rare diseases (RDs) are conditions affecting fewer than 1 in 2,000 people, with over 7,000 identified, primarily genetic in nature, and more than half impacting children. Although each RD affects a small population, collectively, between 3.5% and 5.9% of the global population, or 262.9–446.2 million people, live with an RD. Most RDs lack established treatment protocols, highlighting the need for proper care pathways addressing prognosis, diagnosis, and management. Advances in generative AI and large language models (LLMs) offer new opportunities to document the temporal progression of phenotypic features, addressing gaps in current knowledge bases. This study proposes an LLM-based framework to capture the natural history of diseases, specifically focusing on Noonan syndrome. The framework aims to document phenotypic trajectories, validate against RD knowledge bases, and integrate insights into care coordination using electronic health record (EHR) data from the Undiagnosed Diseases Program Singapore. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Generative Ai; Human Phenotype Ontology; Large Language Models; Natural History Of Disease; Noonan Syndrome; Rare Diseases; Article; Clinical Feature; Computer Model; Disease Exacerbation; Electronic Health Record; Feature Extraction; Human; Large Language Model; Noonan Syndrome; Onset Age; Phenotype; Prognosis; Rare Disease; Risk Factor; Singapore; Treatment Outcome; Child; Diagnosis; Epidemiology; Genetics; Knowledge Base; Pathology; Child; Electronic Health Records; Humans; Knowledge Bases; Noonan Syndrome; Phenotype; Rare Diseases},
	keywords = {Article; clinical feature; computer model; disease exacerbation; electronic health record; feature extraction; human; large language model; Noonan syndrome; onset age; phenotype; prognosis; rare disease; risk factor; Singapore; treatment outcome; child; diagnosis; epidemiology; genetics; knowledge base; pathology; Child; Electronic Health Records; Humans; Knowledge Bases; Noonan Syndrome; Phenotype; Rare Diseases},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Naqvi2025,
	author = {Naqvi, Syed Meesam Raza and Ghufran, Mohammad and Varnier, Christophe and Nicod, Jean Marc and Zerhouni, Noureddine},
	title = {Enhancing semantic search using ontologies: A hybrid information retrieval approach for industrial text},
	year = {2025},
	journal = {Journal of Industrial Information Integration},
	volume = {45},
	pages = {},
	doi = {10.1016/j.jii.2025.100835},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000520964&doi=10.1016%2Fj.jii.2025.100835&partnerID=40&md5=d1855c1e6d4832568aae169d3a04e567},
	abstract = {Despite the increased focus on data in Industry 4.0, textual data has received little attention in the production and engineering management literature. Data sources such as maintenance records and machine documentation usually are not used to help maintenance decision-making. Available studies mainly focus on categorizing maintenance records or extracting meta-data, such as time of failure, maintenance cost, etc. One of the main reasons behind this underutilization is the complexity and unstructured nature of the industrial text. In this study, we propose a novel hybrid information retrieval approach for industrial text using multi-modal learning. Maintenance operators can use the proposed system to query maintenance records and find similar solutions to a given problem. The proposed system utilizes heterogeneous (multi-modal) data, a combination of maintenance records, and machine ontology to enhance semantic search results. We used the state-of-the-art Large Language Models (LLMs); BERT (Bidirectional Encoder Representations from Transformers) for textual similarity. For similarity among ontology labels, we used a modified version of Wu-Palmer's similarity. A hybrid weighted similarity is proposed, incorporating text and ontology similarities to enhance semantic search results. The proposed approach was validated using an open-source dataset of real maintenance records from excavators collected over ten years from different mining sites. A retrieval comparison using only text and multi-modal data is performed to estimate the proposed system's effectiveness. Quantitative and qualitative analysis of results indicates a performance improvement of 8% using the proposed hybrid similarity approach compared to only text-based retrieval. To the best of our knowledge, this is the first study to combine LLMs and machine ontology for semantic search in maintenance records. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Industrial Information Integration; Industry 4.0; Large Language Models (llms); Machine Documentation; Multi-modal Learning; Semantic Search; Industrial Information Integration; Information Integration; Information Retrieval Approach; Language Model; Large Language Model; Machine Documentation; Maintenance Records; Multi-modal Learning; Ontology's; Semantic Search; Semantics},
	keywords = {Industrial information integration; Information integration; Information retrieval approach; Language model; Large language model; Machine documentation; Maintenance records; Multi-modal learning; Ontology's; Semantic search; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wei2025,
	author = {Wei, Yinyi and Li, Xiao},
	title = {Knowledge-enhanced ontology-to-vector for automated ontology concept enrichment in BIM},
	year = {2025},
	journal = {Journal of Industrial Information Integration},
	volume = {45},
	pages = {},
	doi = {10.1016/j.jii.2025.100836},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000158606&doi=10.1016%2Fj.jii.2025.100836&partnerID=40&md5=69dfeed55bf716337ecd255e2daf40be},
	abstract = {Building Information Modeling (BIM) relies on standardized ontologies like IfcOWL to address interoperability. However, the increasing complexity and diversity of construction information requirements demand automated enrichment of BIM ontologies, which is hindered by several factors, including complexity in ontology structure, scalability limitations, and domain-specific issues. Manual curation and maintenance of ontologies are labor-intensive and time-consuming, particularly as the scope of BIM projects expands. Despite these challenges, the construction industry lacks an effective automated approach for ontology concept enrichment. Thus, this study proposes a knowledge-enhanced ontology-to-vector (Keno2Vec) approach for automated BIM ontology concept enrichment, which can (1) encode ontology elements into meaningful and semantically rich embeddings by employing the BERT model to integrate both ontological information (names and labels) and external knowledge (definitions from authoritative knowledge bases), effectively addressing the domain expression specificity and complexity of BIM ontologies; and (2) provide a flexible framework that supports various downstream tasks of ontology concept enrichment by utilizing the resulting embeddings, thereby improving the task-specific adaptability and variability. Experimental results on datasets derived from the large-scale ifcOWL and two smaller BIM ontologies demonstrate that Keno2Vec significantly outperforms existing ontology embedding approaches in terms of accuracy and adaptability. For example, Keno2Vec achieves F1 scores on ifcOWL of nearly 87 % for subsumption prediction, 60 % for property identification, 95 % for membership recognition, and 100 % and 90 % for category-based and schema-based concept classification, respectively. Additional analysis highlights the potential of Keno2Vec for improving BIM ontology encoding and benefiting downstream applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bim Ontology; Natural Language Processing; Ontology Concept Enrichment; Ontology Embedding; Pre-trained Language Models; Semantic Representation; Encoding (symbols); Modeling Languages; Natural Language Processing Systems; Ontology; Building Information Modeling Ontology; Building Information Modelling; Embeddings; Language Model; Language Processing; Natural Language Processing; Natural Languages; Ontology Concept Enrichment; Ontology Concepts; Ontology Embedding; Ontology's; Pre-trained Language Model; Semantic Representation; Semantics},
	keywords = {Encoding (symbols); Modeling languages; Natural language processing systems; Ontology; Building information modeling ontology; Building Information Modelling; Embeddings; Language model; Language processing; Natural language processing; Natural languages; Ontology concept enrichment; Ontology concepts; Ontology embedding; Ontology's; Pre-trained language model; Semantic representation; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Jradeh20253344,
	author = {Jradeh, Chloé Khadija and Raoufi, Ensiyeh and David, Jérôme and Larmande, Pierre and Scharffe, François and Todorov, Konstantin and Trojahn, Cassia},
	title = {Graph Embeddings Meet Link Keys Discovery for Entity Matching},
	year = {2025},
	pages = {3344 - 3353},
	doi = {10.1145/3696410.3714581},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005153249&doi=10.1145%2F3696410.3714581&partnerID=40&md5=d0c78b4b7881984b23c2f146c22c2007},
	abstract = {Entity Matching (EM) automates the discovery of identity links between entities within different Knowledge Graphs (KGs). Link keys are crucial for EM, serving as rules allowing to identify identity links across different KGs, possibly described using different ontologies. However, the approach for extracting link keys struggles to scale on large KGs. While embedding-based EM methods efficiently handle large KGs they lack explainability. This paper proposes a novel hybrid EM approach to guarantee the scalability link key extraction approach and improve the explainability of embedding-based EM methods. First, embedding-based EM approaches are used to sample the KGs based on the identity links they generate, thereby reducing the search space to relevant sub-graphs for link key extraction. Second, rules (in the form of link keys) are extracted to explain the generation of identity links by the embedding-based methods. Experimental results demonstrate that the proposed approach allows link key extraction to scale on large KGs, preserving the quality of the extracted link keys. Additionally, it shows that link keys can improve the explainability of the identity links generated by embedding-methods, allowing for the regeneration of 77% of the identity links produced for a specific EM task, thereby providing an approximation of the reasons behind their generation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Embedding-based Em; Entity Matching; Graph Embeddings; Hybrid Ai; Knowledge Graphs; Language Models; Link Keys; Symbolic Em; Embedding-based Entity Matching; Embeddings; Entity Matching; Graph Embeddings; Hybrid Ai; Knowledge Graphs; Language Model; Link Key; Symbolic Entity Matching},
	keywords = {Embedding-based entity matching; Embeddings; Entity matching; Graph embeddings; Hybrid AI; Knowledge graphs; Language model; Link key; Symbolic entity matching},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Liu2025119,
	author = {Liu, Zhiqiang and Gan, Chengtao and Wang, Junjie and Zhang, Yichi and Bo, Zhongpu and Sun, Mengshu and Chen, Huajun and Zhang, Wen},
	title = {OntoTune: Ontology-Driven Self-training for Aligning Large Language Models},
	year = {2025},
	pages = {119 - 133},
	doi = {10.1145/3696410.3714816},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005140144&doi=10.1145%2F3696410.3714816&partnerID=40&md5=4540d4080e2f11c4fafc7e5650f08a18},
	abstract = {Existing domain-specific Large Language Models (LLMs) are typically developed by fine-tuning general-purposed LLMs with large-scale domain-specific corpora. However, training on large-scale corpora often fails to effectively organize domain knowledge of LLMs, leading to fragmented understanding. Inspired by how humans connect concepts and organize knowledge through mind maps, we aim to emulate this approach by using ontology with hierarchical conceptual knowledge to reorganize LLM’s domain knowledge. From this perspective, we propose an ontology-driven self-training framework called OntoTune, which aims to align LLMs with ontology through in-context learning, enabling the generation of responses guided by the ontology. We leverage in-context learning to identify whether the LLM has acquired the specific concept’s ontology knowledge, and select the entries not yet mastered by LLM as the training set to further align the LLM with ontology. Compared to existing domain LLMs based on newly collected large-scale domain-specific corpora, our OntoTune, which relies on the existing, long-term developed ontology and LLM itself, significantly reduces data maintenance costs and offers improved generalization ability. We conduct our study in the medical domain to evaluate the effectiveness of OntoTune, utilizing a standardized medical ontology, SNOMED CT as our ontology source. Experimental results demonstrate that OntoTune achieves state-of-the-art performance in both in-ontology task hypernym discovery and out-of-ontology task medical domain QA. Moreover, compared to the latest direct ontology injection method TaxoLLaMA, our OntoTune better preserves original knowledge of LLM. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Align With Ontology; Large Language Model; Self-training; Align With Ontology; Domain Knowledge; Domain Specific; Existing Domains; Language Model; Large Language Model; Large-scales; Ontology's; Scale Domains; Self-training},
	keywords = {Align with ontology; Domain knowledge; Domain specific; Existing domains; Language model; Large language model; Large-scales; Ontology's; Scale domains; Self-training},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Kausch2025569,
	author = {Kausch, John},
	title = {Knowledge translation as an interdisciplinary method for information science},
	year = {2025},
	journal = {Journal of Documentation},
	volume = {81},
	number = {3},
	pages = {569 - 580},
	doi = {10.1108/JD-07-2024-0167},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003391191&doi=10.1108%2FJD-07-2024-0167&partnerID=40&md5=877c8fb2e8e752ce91919e70a35e0707},
	abstract = {Purpose: This study aims to outline knowledge translation as a method for practising interdisciplinarity in a domain-analytic context which uses new machine learning technologies to improve interoperability between knowledge organisation systems (KOSs). Design/methodology/approach: Through conceptual analysis of topics from translation studies and natural language processing (NLP), a theoretical synthesis is performed which applies functionalist theories of translational action to how word embeddings can be used to increase interoperability. Findings: Theories from translation studies and recent work in context can inform how information science approaches word embeddings and large language models (LLMs) as tools for furthering interoperability. Originality/value: This method for knowledge integration puts concepts like interoperability in a new context and responds to debates about interdisciplinarity in the field of knowledge organisation by proposing a method using machine learning to explore the contexts of different vector spaces. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain Analysis; Interoperability; Knowledge Organization; Knowledge Translation; Linked Open Data; Ontology Alignment; Ontology Mapping; Technology Transfer; Translational Medicine},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Haghighi2025,
	author = {Haghighi, Nava and Yu, Sunny and Landay, James A. and Rosner, Daniela K.},
	title = {Ontologies in Design: How Imagining a Tree Reveals Possibilities and Assumptions in Large Language Models},
	year = {2025},
	pages = {},
	doi = {10.1145/3706598.3713633},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005735162&doi=10.1145%2F3706598.3713633&partnerID=40&md5=031c79e5203db6b76e4403231928ef22},
	abstract = {Amid the recent uptake of Generative AI, sociotechnical scholars and critics have traced a multitude of resulting harms, with analyses largely focused on values and axiology (e.g., bias). While value-based analyses are crucial, we argue that ontologies - concerning what we allow ourselves to think or talk about - is a vital but under-recognized dimension in analyzing these systems. Proposing a need for a practice-based engagement with ontologies, we offer four orientations for considering ontologies in design: pluralism, groundedness, liveliness, and enactment. We share examples of potentialities that are opened up through these orientations across the entire LLM development pipeline by conducting two ontological analyses: examining the responses of four LLM-based chatbots in a prompting exercise, and analyzing the architecture of an LLM-based agent simulation. We conclude by sharing opportunities and limitations of working with ontologies in the design and development of sociotechnical systems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Foundation Models; Generative Ai; Large Language Models; Llm Agents; Ontological Design; Ontologies; Foundation Models; Generative Ai; Language Model; Large Language Model; Llm Agent; Ontological Analysis; Ontological Design; Ontology's; Sociotechnical; Value-based; Ontology},
	keywords = {Foundation models; Generative AI; Language model; Large language model; LLM agent; Ontological analysis; Ontological design; Ontology's; Sociotechnical; Value-based; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Gadusu202529371,
	author = {Gadusu, Srikar Reddy and Kucuk, Yigit and Santillana, Vania and King, Aaron and McGinty, Hande Kucuk},
	title = {Enhancing Aging Biomarker Research through Large Language Models and Knowledge Graphs},
	year = {2025},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume = {39},
	number = {28},
	pages = {29371 - 29373},
	doi = {10.1609/aaai.v39i28.35255},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003908361&doi=10.1609%2Faaai.v39i28.35255&partnerID=40&md5=ef079c07e740f3b61937dc656906c7db},
	abstract = {Aging biomarkers play a crucial role in uncovering the biological mechanisms behind aging and in developing strategies to support healthy aging. However, the search for reliable aging biomarkers is particularly challenging due to the intricate and multifactorial nature of the aging process. Furthermore, biomarker names and categories are not well-standardized in the current literature. While, a formal definition of a biomarker is nonexistent in the current literature, formally defining biomarkers and standardizing the vocabulary for biomarkers can help accelerate AI research around this concept which can lead to better, faster and more accurate analyses of the existing data and literature. Thus, in this work, we generated Knowledge Graphs that can help us define and standardize biomarkers. We present our Knowledge Graphs (KGs) generated using both an LLM and expert-curated datasets. We compare both KGs to understand why systematic integration between these two models is needed. The integration of Knowledge Graphs (KGs) and Large Language Models (LLMs) presents a promising approach to advancing aging biomarker research through the inherent structured and standardized nature of ontology schemas in knowledge graphs. We showcase that the accuracy of LLM-generated KGs remains questionable but systematic methods such as KNARM can help us with the accuracy of these efforts. In future work, we will propose a synergistic framework where KGs and LLMs interact iteratively to improve both the comprehensiveness and accuracy of aging biomarker information. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {'current; Accurate Analysis; Ageing Process; Biological Mechanisms; Developing Strategy; Formal Definition; Integration Of Knowledge; Knowledge Graphs; Language Model; Systematic Integration},
	keywords = {'current; Accurate analysis; Ageing process; Biological mechanisms; Developing strategy; Formal definition; Integration of knowledge; Knowledge graphs; Language model; Systematic integration},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ghosh202528757,
	author = {Ghosh, Rikhiya and von Stockhausen, Hans Martin and Schmitt, Martin and Vasile, George Marica and Karn, Sanjeev Kumar and Farri, Oladimeji},
	title = {CVE-LLM: Ontology-Assisted Automatic Vulnerability Evaluation Using Large Language Models},
	year = {2025},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume = {39},
	number = {28},
	pages = {28757 - 28765},
	doi = {10.1609/aaai.v39i28.35139},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003901778&doi=10.1609%2Faaai.v39i28.35139&partnerID=40&md5=8206a27810ddacde306b61062a345f60},
	abstract = {The National Vulnerability Database (NVD) publishes over a thousand new vulnerabilities monthly, with a projected 25 percent increase in 2024, highlighting the crucial need for rapid vulnerability identification to mitigate cybersecurity attacks and save costs and resources. In this work, we propose using large language models (LLMs) to learn vulnerability evaluation from historical assessments of medical device vulnerabilities in a single manufacturer's portfolio. We highlight the effectiveness and challenges of using LLMs for automatic vulnerability evaluation and introduce a method to enrich historical data with cybersecurity ontologies, enabling the system to understand new vulnerabilities without retraining the LLM. Our LLM system integrates with the in-house application - Cybersecurity Management System (CSMS) - to help Siemens Healthineers (SHS) product cybersecurity experts efficiently assess the vulnerabilities in our products. Also, we present guidelines for efficient integration of LLMs into the cybersecurity tool. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {C (programming Language); Intelligent Computing; Cyber Security; Cybersecurity Management Systems; Historical Data; Language Model; Learn+; Medical Devices; Modelling Systems; National Vulnerability Database; Ontology's; Vulnerability Evaluations; Intelligent Systems},
	keywords = {C (programming language); Intelligent computing; Cyber security; Cybersecurity management systems; Historical data; Language model; Learn+; Medical Devices; Modelling systems; National vulnerability database; Ontology's; Vulnerability evaluations; Intelligent systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Chen2025127,
	author = {Chen, Fanfan and Tsai, Chengen},
	title = {Redefining boundaries: Human and computer in AI-generated worlds},
	year = {2025},
	pages = {127 - 149},
	doi = {10.4018/979-8-3693-7147-3.ch007},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005135735&doi=10.4018%2F979-8-3693-7147-3.ch007&partnerID=40&md5=ba5062aead648f70a832e1251b6893de},
	abstract = {With LLMs generating human-like content, we increasingly rely on AI, idling our intelligence, imagination, and creativity. This symbiotic relationship enhances human capabilities while blurring boundaries, making humans seem more artificial and vice versa. Critics argue that LLMs mimic human language and knowledge without grounding. However, AI's multimodal advancements would support its linguistic vectors' referential grounding. Future multimodal AI, seamlessly generating diverse content, could dominate our world, challenging our understanding of creation, redefining human-AI boundaries, and reshaping our ontological and epistemological understanding. This essay explores whether AI can possess productive imagination, examining narrative imagination, identity, and consciousness. Regardless of whether AI can be considered conscious, its generated content does engage human users, intertwining with them to create a new referential world, which may transcend in a spiral hermeneutical circle, augmenting the realization of ourselves and reality through artificial-human un-realization. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Human Capability; Human Knowledge; Human Language; Human Like; Human Users; Multi-modal; Symbiotic Relationship},
	keywords = {Human capability; Human knowledge; Human language; Human like; Human users; Multi-modal; Symbiotic relationship},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Caponio2025241,
	author = {Caponio, Vito Carlo Alberto and Musella, Gennaro and Pérez-Sayáns, Mario and Lo Muzio, Lorenzo and Amaral Mendes, Rui and López-Pintor, R. M.},
	title = {The Need to Improve the Medical Subject Headings (MeSH) and the Excerpta Medica Tree (EMTREE) Thesauri to Perform Systematic Review on Oral Potentially Malignant Disorders},
	year = {2025},
	journal = {Journal of Oral Pathology and Medicine},
	volume = {54},
	number = {4},
	pages = {241 - 247},
	doi = {10.1111/jop.13616},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000577734&doi=10.1111%2Fjop.13616&partnerID=40&md5=2b448632a136b5cc817006614411ec86},
	abstract = {Background: Despite recent advancements in the understanding and classification of oral potentially malignant disorders (OPMD), their terminology remains inconsistent and heterogeneous throughout the scientific literature, thus affecting evidence-based decision-making relevant for clinical management of these disorders. Updating this classification represents a necessity to improve the indexing and retrieval of OPMD publications, in particular for systematic reviews and meta-analysis. Methods: Through a critical appraisal of the Medical Subject Headings (MeSH) and Excerpta Medica Tree (EMTREE) thesauri, we assessed gaps in the indexing for OPMD literature and propose improvements for enhanced categorisation and retrieval. Results: The present study identifies inconsistencies and limitations in the classification of these disorders across the major medical databases, which may be summarized in the following findings: a) The MeSH database lacks a dedicated subject heading for “oral potentially malignant disorders”; b) EMTREE indexing is incomplete, with only 5 out of 11 recognised OPMD having corresponding terms; c) Incoherent controlled vocabulary mappings hinder systematic literature retrieval. Conclusion: To ensure accurate evidence synthesis, the authors recommend searching both PubMed and Embase for OPMD studies. Moreover, the use of Embase’s PubMed query translator and Large Language Models, such as ChatGPT, may lead to retrieval biases due to indexing discrepancies, posing challenges for early-career researchers and students. We recommend introducing “oral potentially malignant disorders” as a standardised subject heading. Evidence-based medicine underpins clinical decision support systems, which rely on standardised clinical coding for reliable health information. Enhanced medical ontologies will facilitate structured clinical coding, ensuring interoperability and improving clinical decision support systems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Medical Informatics; Meta-analysis; Mouth Diseases; Oral Lichen Planus; Oral Potentially Malignant Disorder; Precancerous Conditions; Article; Controlled Vocabulary; Disease Classification; Embase; Medical Subject Headings; Oral Potentially Malignant Disorder; Systematic Review (topic); Classification; Documentation; Human; Mouth Tumor; Nomenclature; Abstracting And Indexing; Humans; Mouth Neoplasms; Systematic Reviews As Topic; Terminology As Topic; Vocabulary, Controlled},
	keywords = {Article; controlled vocabulary; disease classification; Embase; Medical Subject Headings; oral potentially malignant disorder; systematic review (topic); classification; documentation; human; mouth tumor; nomenclature; Abstracting and Indexing; Humans; Mouth Neoplasms; Systematic Reviews as Topic; Terminology as Topic; Vocabulary, Controlled},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Qiu2025,
	author = {Qiu, Yunjian and Jin, Yan},
	title = {A Method for Synthesizing Ontology-Based Textual Design Datasets: Evaluating the Potential of Large Language Model in Domain-Specific Dataset Generation},
	year = {2025},
	journal = {Journal of Mechanical Design},
	volume = {147},
	number = {4},
	pages = {},
	doi = {10.1115/1.4067478},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217451896&doi=10.1115%2F1.4067478&partnerID=40&md5=67074dded2938115e807e1bed305b10f},
	abstract = {In engineering disciplines, leveraging generative language models requires using specialized datasets for training or fine-tuning the preexisting models. Compiling these domain-specific datasets is a complex endeavor, demanding significant human effort and resources. To address the problem of domain-specific dataset scarcity, this study investigates the potential of generative large language models (LLMs) in creating synthetic domain-specific textual datasets for engineering design domains. By harnessing the advanced capabilities of LLMs, such as GPT-4, a systematic methodology was developed to create high-fidelity datasets using designed prompts, evaluated against a manually labeled benchmark dataset through various computational measurements without human intervention. Findings suggest that well-designed prompts can significantly enhance the quality of domain-specific synthetic datasets with reduced manual effort. The research highlights the importance of prompt design in eliciting precise, domain-relevant information and discusses the balance between dataset robustness and richness. It is demonstrated that a language model trained on synthetic datasets can achieve a level of performance comparable to that of human-labeled, domain-specific datasets in terms of quality, offering a strategic solution to the limitations imposed by dataset shortages in engineering domains. The implications for design thinking processes are particularly noteworthy, with the potential to assist designers through GPT-4's structured reasoning capabilities. This work presents a complete guide for domain-specific dataset generation, automated evaluation metrics, and insights into the interplay between data robustness and comprehensiveness. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Conceptual Design; Creativity And Concept Generation; Data-driven Design; Design Methodology; Design Process; Design Representation; Design Theory And Methodology; Domain-specific Dataset; Engineering Design; Generative Models; Large Language Models; Synthetic Dataset Creation; Design For Manufacturability; Problem Oriented Languages; Concept Generation; Creativity And Concept Generation; Data-driven Design; Design Methodology; Design Representation; Design Theory And Methodology; Design-process; Domain Specific; Domain-specific Dataset; Engineering Design; Generative Model; Language Model; Large Language Model; Synthetic Dataset Creation; Synthetic Datasets; Conceptual Design},
	keywords = {Design for manufacturability; Problem oriented languages; Concept generation; Creativity and concept generation; Data-driven design; Design Methodology; Design representation; Design theory and methodology; Design-process; Domain specific; Domain-specific dataset; Engineering design; Generative model; Language model; Large language model; Synthetic dataset creation; Synthetic datasets; Conceptual design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wolosky2025359,
	author = {Wolosky, Shira},
	title = {Challenging Analogy: Levinas, Maimonides, and Language Addressing Transcendence},
	year = {2025},
	journal = {Harvard Theological Review},
	volume = {118},
	number = {2},
	pages = {359 - 380},
	doi = {10.1017/S0017816025100734},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012411918&doi=10.1017%2FS0017816025100734&partnerID=40&md5=23f25b996c49738b79f3620c355555e6},
	abstract = {Emmanuel Levinas’s philosophy of absolute transcendence has been criticized for defeating any possibility of relationship to the divine as Other. Such critiques restage central theological trends that rely on analogy as opening just such an avenue to the divine. Aquinas proposes analogy in his own criticism of Maimonides’ negative theology of God as beyond any likeness, in ways similar to arguments leveled against Levinas. Levinas, however, proposes a language model, which also illuminates Maimonides’ own language discourses, as a way to allow relationship while sustaining distinction from transcendence. Through language, the divine is addressed while respecting absolute Otherness, in a move away from ontology to ethics. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Aquinas; Levinas; Maimonides; Negative Theology; Other; Ricoeur; Transcendence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Chen2025429,
	author = {Chen, Jiaqi and Sheng, Shuang and Lin, Jiaxi and Li, Yongle and Peng, Yankai and Xu, Chong},
	title = {Zero sample power grid equipment ontology defect grade idetification method based on knowledge enhanced large language model; 基于知识增强大语言模型的零样本电力设备本体缺陷等级识别方法},
	year = {2025},
	journal = {Gaojishu Tongxin/Chinese High Technology Letters},
	volume = {35},
	number = {4},
	pages = {429 - 439},
	doi = {10.3772/j.issn.1002-0470.2025.04.010},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009854008&doi=10.3772%2Fj.issn.1002-0470.2025.04.010&partnerID=40&md5=6049e7052b8a346a9d16e67abe51006b},
	abstract = {Knowledge retrieval-augmented generation technology effectively alleviates the hallucination and knowledge lag issues of large language models (LLM) by incorporating external knowledge bases, becoming a crucial paradigm for enhancing domain-specific task performance. This paper addresses the problems of lack of labeled samples, low knowledge utilization and insufficient explainability in the task of power grid equipment defect grade identification for LLM, and proposes a zero-shot knowledge-enhanced collaborative reasoning framework for LLM to solve these problems. A hierarchical tree-structured knowledge base is constructed. Then, a two-stage retrieval algorithm is designed. This algorithm focuses on the most semantically relevant information to improve the efficiency of knowledge acquisition. Additionally, the method innovatively integrates the prior knowledge of large language models with retrieved knowledge for multi-stage reasoning and verification. The proposed method achieves a classification accuracy of 54. 17% on 218 test samples, which is a 14. 26% improvement compared to methods without knowledge retrieval. It also generates verifiable explanatory texts through chain-of-thought prompts. Moreover, this method is zero-shot and does not require labeled data for training. Experimental results demonstrate that the proposed method effectively leverages the synergy between domain-specific and general knowledge, providing an accurate and interpretable solution for the automatic detection of power equipment defects. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Base Construction; Large Language Model(llm); Multi-stage Reasoning; Power Grid Equipment Defect Detection; Retrieval-augmented Generation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{He20251477,
	author = {He, Jianjia and Sun, Jin},
	title = {Research on the construction and application of knowledge graphs for typical emergency scenarios; 面向典型应急场景的知识图谱构建与应用研究},
	year = {2025},
	journal = {Journal of Safety and Environment},
	volume = {25},
	number = {4},
	pages = {1477 - 1489},
	doi = {10.13637/j.issn.1009-6094.2024.1572},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009302500&doi=10.13637%2Fj.issn.1009-6094.2024.1572&partnerID=40&md5=50817cd63cd408803e8a69ec5befc630},
	abstract = {To facilitate the rapid development of the emergency industry, provide scenario-based data support for industrial-level emergency management, and enhance information retrieval efficiency, this study presents a knowledge graph construction method specifically designed for typical emergency scenarios. To achieve a fine-grained representation of emergency scenario knowledge and enhance the understanding of industrial chains and resource demands within such scenarios, this study addresses the gap in reusable ontologies. It does so by thoroughly analyzing the structure of data sources, organizing domain-specific regulatory documents, and referencing existing research. As a result, an integrated ontology library for emergency scenarios was developed, emphasizing four core elements: “emergency events”, “emergency tasks”, “emergency industries”, and “scenario data”. Following this, we employed databases and web crawling techniques to gather textual data pertaining to emergency events and resource allocation in China’s emergency industry over the past five years. After data cleaning, we obtained 3 572 valid records. Using this dataset, we introduced a knowledge extraction process utilizing the Bidirectional Encoder Representations from Transformers-Bidirectional Long Short-Term Memory-Conditional Random Field (BERT BiLSTM CRF) model and performed knowledge fusion through a combination of cosine similarity based on word frequency statistics and keyword matching. Furthermore, this study visually presented the construction results of three types of typical emergency scenario knowledge graphs and explored their application value. On one hand, by employing knowledge navigation and intelligent retrieval, the study offered a comprehensive analysis of the interconnections between emergency scenarios and industrial networks, as well as potential pathways for accelerating the development of the emergency industry. On the other hand, by developing a Python web-based Knowledge Graph-Large Language Model-Retrieval-Augmented Generation (KG LLM RAG) intelligent question-answering system, this study investigated the integration of emergency scenario knowledge graphs with large language models. It emphasized their application value in providing decision-making support for material reserves, business expansion, and collaboration within the emergency industry. The research results demonstrate that the knowledge representation model developed in this study effectively organizes emergency industry information and scenario-specific demands in a fine-grained manner, while also capturing the relationships between multi-agent collaboration and resource supply within scenarios. The three types of typical emergency scenario knowledge graphs facilitate correlation analysis and deep mining of critical data, making them suitable for integration with large language models to develop intelligent question-answering systems. This provides valuable decision-making support for product and service positioning, material reserves, business expansion, and collaboration within the emergency industry. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Emergency Scenarios; Industry Interconnection; Knowledge Graph; Public Safety},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Demirol2025,
	author = {Demirol, Doygun and Das, R. and Hanbay, Davut},
	title = {A Novel Approach for Cyber Threat Analysis Systems Using BERT Model from Cyber Threat Intelligence Data},
	year = {2025},
	journal = {Symmetry},
	volume = {17},
	number = {4},
	pages = {},
	doi = {10.3390/sym17040587},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003645978&doi=10.3390%2Fsym17040587&partnerID=40&md5=83ae9ef43269deb4123fe198f2e91794},
	abstract = {As today’s cybersecurity environment is becoming increasingly complex, it is crucial to analyse threats quickly and effectively. A delayed response or lack of foresight can lead to data loss, reputational damage, and operational disruptions. Therefore, developing methods that can rapidly extract valuable threat intelligence is a critical need to strengthen defence strategies and minimise potential damage. This paper presents an innovative approach that integrates knowledge graphs and a fine-tuned BERT-based model to analyse cyber threat intelligence (CTI) data. The proposed system extracts cyber entities such as threat actors, malware, campaigns, and targets from unstructured threat reports and establishes their relationships using an ontology-driven framework. A named entity recognition dataset was created and a BERT-based model was trained. To address the class imbalance, oversampling and a focal loss function were applied, achieving an F1 score of 96%. The extracted entities and relationships were visualised and analysed using knowledge graphs, enabling the advanced threat analysis and prediction of potential attack targets. This approach enhances cyber-attack prediction and prevention through knowledge graphs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cyber Threat Intelligence; Knowledge Graphs; Named Entity Recognition; Pre-trained Language Model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Chen2025943,
	author = {Chen, Quanlin and Jia, Jun},
	title = {An Event Ontology and Dataset Construction Method for Strategic Operations Analysis; 面向战略运筹分析的事件本体及数据集构建方法},
	year = {2025},
	journal = {Xitong Fangzhen Xuebao / Journal of System Simulation},
	volume = {37},
	number = {4},
	pages = {943 - 952},
	doi = {10.16182/j.issn1004731x.joss.23-1435},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003472891&doi=10.16182%2Fj.issn1004731x.joss.23-1435&partnerID=40&md5=14094ead1e69637e2115f8f55ca4ed44},
	abstract = {Aiming at the lack of professional datasets for information extraction technology research in the field of strategic operations research analysis, this paper proposes an event ontology and dataset construction method for strategic operations research analysis. The method proposes an event ontology model for strategic operations research analysis according to the needs of situation judgment in strategic operations research analysis, and uses the method of "a small amount of manual annotation + fine-tuned large language model annotation" to construct the event dataset EfSOA for strategic operations research analysis. The dataset construction method proposed in this paper and the constructed dataset EfSOA highlight the domain knowledge of strategic operations research analysis, which can effectively support the research of information extraction methods in this field, and lay a foundation for the future construction of event extraction and relationship mining models for strategic operations research analysis. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Datasets; Event Ontology; Information Extraction; Large Language Modeling; Strategic Operations Analysis; Ontology; Construction Method; Dataset; Event Ontology; Information Extraction; Language Model; Large Language Modeling; Operation Research; Operations Analysis; Research Analysis; Strategic Operation Analyze; Modeling Languages},
	keywords = {Ontology; Construction method; Dataset; Event ontology; Information extraction; Language model; Large language modeling; Operation research; Operations analysis; Research analysis; Strategic operation analyze; Modeling languages},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Graf2025249,
	author = {Graf, Markus M. and Bressem, Keno Kyrill and Adams, Lisa Christine},
	title = {Transformation of free-text radiology reports into structured data; Umwandlung freitextlicher radiologischer Befundtexte in strukturierte Daten},
	year = {2025},
	journal = {Radiologie},
	volume = {65},
	number = {4},
	pages = {249 - 256},
	doi = {10.1007/s00117-025-01422-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001079958&doi=10.1007%2Fs00117-025-01422-4&partnerID=40&md5=5e89fb0e5024feb4002baba3e00b66d4},
	abstract = {Background: The rapid development of large language models (LLMs) opens up new possibilities for the automated processing of medical texts. Transforming unstructured radiology reports into structured data is crucial for efficient use in clinical decision support systems, research, and improving patient care. Objectives: What are the challenges of transforming natural language radiology reports into structured data using LLMs? Which methods and architectures are promising? How can the quality and reliability of the extracted data be ensured? Materials and Methods: This article examines current research on the application of LLMs in radiological information processing. Various approaches such as rule-based systems, machine learning, and deep learning models, particularly neural network architectures, are analyzed and compared. The focus is on extracting information such as diagnoses, anatomical locations, findings, and measurements. Results and Conclusion: LLMs show great potential in transforming reports into structured data. In particular, deep learning models trained on large datasets achieve high accuracies. However, challenges remain, such as dealing with ambiguities, abbreviations, and the variability of linguistic expressions. Combining LLMs with domain-specific knowledge, for example, in the form of ontologies, can further improve the performance of the systems. Integrating contextual information and developing robust evaluation metrics are also important research directions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Large Language Models; Machine Learning; Natural Language Processing (nlp); Structured Findings; Ambiguity; Anatomical Location; Article; Artificial Intelligence; Benchmarking; Clinical Decision Support System; Deep Learning; Human; Information Processing; Large Language Model; Machine Learning; Natural Language Processing; Nerve Cell Network; Ontology; Patient Care; Radiology; Reliability; Electronic Health Record; Organization And Management; Radiology Information System; Deep Learning; Electronic Health Records; Humans; Machine Learning; Natural Language Processing; Radiology Information Systems},
	keywords = {ambiguity; anatomical location; article; artificial intelligence; benchmarking; clinical decision support system; deep learning; human; information processing; large language model; machine learning; natural language processing; nerve cell network; ontology; patient care; radiology; reliability; electronic health record; organization and management; radiology information system; Deep Learning; Electronic Health Records; Humans; Machine Learning; Natural Language Processing; Radiology Information Systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yang2025,
	author = {Yang, Ni and Li, Linus and Shi, Xiaolu and Liu, Yongping and Wen, Ri and Yang, Yuhang and Zhang, Tao and Yang, Xinru and Xu, Yangfan and Liu, Chunfeng},
	title = {Succinylation of SERCA2a at K352 Promotes Its Ubiquitinoylation and Degradation by Proteasomes in Sepsis-Induced Heart Dysfunction},
	year = {2025},
	journal = {Circulation: Heart Failure},
	volume = {18},
	number = {4},
	pages = {},
	doi = {10.1161/CIRCHEARTFAILURE.124.012180},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000060636&doi=10.1161%2FCIRCHEARTFAILURE.124.012180&partnerID=40&md5=3cd4acdca4067ef385db8c5545cdf081},
	abstract = {BACKGROUND: Intracellular Ca<sup>2+</sup> cycling governs effective myocardial systolic contraction and diastolic relaxation. SERCA2a (sarco/endoplasmic reticulum Ca<sup>2+</sup> ATPase type 2a), which plays a crucial role in controlling intracellular Ca<sup>2+</sup> signaling and myocardial cell function, is downregulated and inactivated during sepsis-induced heart dysfunction. However, the cause of this dysregulation remains unclear. In this study, we investigated the effect of lysine succinylation in lipopolysaccharide-induced septic heart dysfunction through global succinylome analysis of myocardial tissues from septic rats. METHODS: We conducted a succinylome profiling and developed a protein language model-based framework to prioritize succinylation at a functionally important site, and further analysis revealed crosstalk between ubiquitination and succinylation of SERCA2a. The succinylation of SERCA2a in septic rats or lipopolysaccharide-treated cells were detected by co-immunoprecipitation. Thereafter, a desuccinylated SERCA2a<sup>K352R</sup> was introduced and its function and stability were determined by Ca<sup>2+</sup> transient and Western blot, respectively. Meanwhile, the effect on SERCA2a<sup>K352R</sup> on heart function was assessed in vivo by echocardiography and hemodynamics. RESULTS: We identified 10324 succinylated lysine sites in heart tissues, including 1042 differentially succinylated lysine sites, in response to lipopolysaccharide. SERCA2a was hypersuccinylated in the myocardial tissues of septic rats and lipopolysaccharide-treated cardiomyocytes. Increased ubiquitination level, reduced protein level, and activity of SERCA2a were observed, along with increased succinylation of SERCA2a in vivo and in vitro. K352 was essential for SERCA2a succinylation, which reduced SERCA2a protein level by promoting formation of the K48 ubiquitin chain on SERCA2a and its degradation by proteasomes. Co-immunoprecipitation combined with liquid chromatography-tandem mass spectrometry identified that SIRT2 (sirtuin2), a deacylase, exhibited interaction with SERCA2a. Furthermore, SIRT2 decreased K352 succinylation of SERCA2a, suggesting that SIRT2 may function as a desuccinylase for SERCA2a. CONCLUSIONS: Succinylation of SERCA2a at K352, which was controlled by SIRT2, promotes its ubiquitinoylation and degradation by proteasomes in sepsis-induced heart dysfunction. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Lipopolysaccharide; Rats; Sarcoplasmic Reticulum; Sepsis; Ubiquitination; Calcium; Proteasome; Troponin T; Lysine; Atp2a2 Protein, Rat; Lipopolysaccharides; Lysine; Proteasome Endopeptidase Complex; Sarcoplasmic Reticulum Calcium-transporting Atpases; Calcium; Creatine Kinase Mb; Proteasome; Sarcoplasmic Reticulum Calcium Transporting Adenosine Triphosphatase; Sirtuin 2; Troponin T; Atp2a2 Protein, Rat; Lipopolysaccharide; Lysine; Amino Acid Sequence; Animal Cell; Animal Experiment; Animal Model; Animal Tissue; Article; Cardiac Muscle Cell; Coimmunoprecipitation; Controlled Study; Deep Neural Network; Echocardiography; Enzyme Activity; Gene Expression; Gene Mutation; Gene Ontology; H9c2(2-1) Cell Line; Heart Contraction; Heart Death; Heart Function; Immunoblotting; Kegg; Liquid Chromatography-mass Spectrometry; Male; Nonhuman; Protein Degradation; Protein Expression; Protein Language Model; Rat; Sepsis; Ubiquitination; Western Blotting; Animal; Cardiac Muscle; Complication; Disease Model; Genetics; Metabolism; Sprague Dawley Rat; Animals; Disease Models, Animal; Lipopolysaccharides; Lysine; Male; Myocardium; Myocytes, Cardiac; Proteasome Endopeptidase Complex; Proteolysis; Rats; Rats, Sprague-dawley; Sarcoplasmic Reticulum Calcium-transporting Atpases; Sepsis; Ubiquitination},
	keywords = {calcium; creatine kinase MB; proteasome; sarcoplasmic reticulum calcium transporting adenosine triphosphatase; sirtuin 2; troponin T; Atp2a2 protein, rat; lipopolysaccharide; lysine; amino acid sequence; animal cell; animal experiment; animal model; animal tissue; Article; cardiac muscle cell; coimmunoprecipitation; controlled study; deep neural network; echocardiography; enzyme activity; gene expression; gene mutation; gene ontology; H9c2(2-1) cell line; heart contraction; heart death; heart function; immunoblotting; KEGG; liquid chromatography-mass spectrometry; male; nonhuman; protein degradation; protein expression; protein language model; rat; sepsis; ubiquitination; Western blotting; animal; cardiac muscle; complication; disease model; genetics; metabolism; Sprague Dawley rat; Animals; Disease Models, Animal; Lipopolysaccharides; Lysine; Male; Myocardium; Myocytes, Cardiac; Proteasome Endopeptidase Complex; Proteolysis; Rats; Rats, Sprague-Dawley; Sarcoplasmic Reticulum Calcium-Transporting ATPases; Sepsis; Ubiquitination},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Zhou2025,
	author = {Zhou, Guowei and Zhao, Yanpeng and He, Song and Bo, Xiaochen},
	title = {SST-ResNet: A Sequence and Structure Information Integration Model for Protein Property Prediction},
	year = {2025},
	journal = {International Journal of Molecular Sciences},
	volume = {26},
	number = {6},
	pages = {},
	doi = {10.3390/ijms26062783},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002284161&doi=10.3390%2Fijms26062783&partnerID=40&md5=664753063e078038ad537a09e5082bdb},
	abstract = {Proteins are the basic building blocks of life and perform fundamental functions in biology. Predicting protein properties based on amino acid sequences and 3D structures has become a key approach to accelerating drug development. In this study, we propose a novel sequence- and structure-based framework, SST-ResNet, which consists of the multimodal language model ProSST and a multi-scale information integration module. This framework is designed to deeply explore the latent relationships between protein sequences and structures, thereby achieving superior synergistic prediction performance. Our method outperforms previous joint prediction models on Enzyme Commission (EC) numbers and Gene Ontology (GO) tasks. Furthermore, we demonstrate the necessity of multi-scale information integration for these two types of data and illustrate its exceptional performance on key tasks. We anticipate that this framework can be extended to a broader range of protein property prediction problems, ultimately facilitating drug development. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine},
	author_keywords = {Deep Learning; Drug Discovery; Information Integration; Multimodal Model; Protein Property; Structure Token; Protein; Proteins; Protein; Algorithm; Amino Acid Sequence; Bioinformatics; Chemistry; Metabolism; Procedures; Protein Conformation; Protein Database; Sequence Analysis; Algorithms; Amino Acid Sequence; Computational Biology; Databases, Protein; Protein Conformation; Proteins; Sequence Analysis, Protein},
	keywords = {protein; algorithm; amino acid sequence; bioinformatics; chemistry; metabolism; procedures; protein conformation; protein database; sequence analysis; Algorithms; Amino Acid Sequence; Computational Biology; Databases, Protein; Protein Conformation; Proteins; Sequence Analysis, Protein},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Provasi20251328,
	author = {Provasi, Davide and Filizola, Marta},
	title = {Fine-Tuned Deep Transfer Learning Models for Large Screenings of Safer Drugs Targeting Class A GPCRs},
	year = {2025},
	journal = {Biochemistry},
	volume = {64},
	number = {6},
	pages = {1328 - 1337},
	doi = {10.1021/acs.biochem.4c00832},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000501881&doi=10.1021%2Facs.biochem.4c00832&partnerID=40&md5=cacff636a51aac21494fe7e5a8f26d67},
	abstract = {G protein-coupled receptors (GPCRs) remain a focal point of research due to their critical roles in cell signaling and their prominence as drug targets. However, directly linking drug efficacy to the receptor-mediated activation of specific intracellular transducers and the resulting physiological outcomes remains challenging. It is unclear whether the enhanced therapeutic window of certain drugs─defined as the dose range that provides effective therapy with minimal side effects─stems from their low intrinsic efficacy across all signaling pathways or ligand bias, wherein specific transducer subtypes are preferentially activated in a given cellular system compared to a reference ligand. Accurately predicting safer compounds, through either low intrinsic efficacy or ligand bias, would greatly advance drug development. While AI models hold promise for such predictions, the development of deep learning models capable of reliably forecasting GPCR ligands with defined bioactivities remains challenging, largely due to the limited availability of high-quality data. To address this, we pretrained a model on receptor sequences and ligand data sets across all class A GPCRs and then refined it to predict low-efficacy compounds or biased agonists for individual class A GPCRs. This was achieved using transfer learning and a neural network incorporating natural language processing of target sequences and receptor mutation effects on signaling. These two fine-tuned models─one for low-efficacy agonists and one for biased agonists─are available on demand for each class A GPCR and enable virtual screening of large chemical libraries, thereby facilitating the discovery of compounds with potentially improved safety profiles. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Retina S Antigen; Ligands; Receptors, G-protein-coupled; Nvidia Rtx A4000 Gpu; Diagnosis; Drug Dosage; Drug Interactions; Targeted Drug Delivery; Cells Signaling; Class A; Drug Efficacy; Drug Targets; Drug-targeting; Focal Points; G Protein Coupled Receptors; Large Screenings; Learning Models; Transfer Learning; Ligands; Apelin Receptor; Cannabinoid Receptor; Dopamine Receptor; G Protein Coupled Receptor; Opiate Receptor; Retina S Antigen; Ligand; Amino Acid Sequence; Article; Binding Affinity; Bioinformatics; Biological Activity; Classifier; Computer Model; Deep Learning; Deep Neural Network; Deep Transfer Learning Model; Dna Repair; Drug Development; Drug Efficacy; Drug Metabolism; Drug Safety; Drug Targeting; Gene Ontology; Gpcr Signaling; Intrinsic Activity; Language Processing; Machine Learning; Natural Language Processing; Phosphorylation; Predictive Value; Protein Language Model; Rattus Norvegicus; Receiver Operating Characteristic; Receptor Intrinsic Activity; Signal Transduction; Transfer Learning (machine Learning); Drug Effect; Human; Metabolism; Preclinical Study; Procedures; Deep Learning; Drug Discovery; Drug Evaluation, Preclinical; Humans; Receptors, G-protein-coupled; Signal Transduction},
	keywords = {Diagnosis; Drug dosage; Drug interactions; Targeted drug delivery; Cells signaling; Class A; Drug efficacy; Drug targets; Drug-targeting; Focal points; G protein coupled receptors; Large screenings; Learning models; Transfer learning; Ligands; apelin receptor; cannabinoid receptor; dopamine receptor; G protein coupled receptor; opiate receptor; retina S antigen; ligand; amino acid sequence; Article; binding affinity; bioinformatics; biological activity; classifier; computer model; deep learning; deep neural network; deep transfer learning model; DNA repair; drug development; drug efficacy; drug metabolism; drug safety; drug targeting; gene ontology; GPCR signaling; intrinsic activity; language processing; machine learning; natural language processing; phosphorylation; predictive value; protein language model; Rattus norvegicus; receiver operating characteristic; receptor intrinsic activity; signal transduction; transfer learning (machine learning); drug effect; human; metabolism; preclinical study; procedures; Deep Learning; Drug Discovery; Drug Evaluation, Preclinical; Humans; Receptors, G-Protein-Coupled; Signal Transduction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Yang2025,
	author = {Yang, Can},
	title = {Knowledge Graph-Enhanced Artwork Image Captioning},
	year = {2025},
	journal = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries},
	pages = {},
	doi = {10.1145/3677389.3702611},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001174943&doi=10.1145%2F3677389.3702611&partnerID=40&md5=9649ffd0515f01fab731f9ad2502395e},
	abstract = {This paper explores the unique challenges of artwork image captioning, a task that demands deep understanding of historical, cultural, and stylistic elements often absent in traditional image captioning. We conducted preliminary experiments using a Meshed Memory Transformer on the Iconclass AI Test Set, which revealed significant improvements in standard metrics but highlighted critical limitations in current datasets and evaluation methods. To address these issues, we propose a novel approach integrating knowledge graphs with large language models. This approach involves creating a specialized art ontology and knowledge graph, and developing new evaluation metrics specifically designed for artwork captioning. While not yet implemented, this proposed method aims to generate more comprehensive, contextually rich, and accurate captions for artwork images. Our research lays the groundwork for future advancements in artwork image captioning, potentially enhancing the accessibility and educational value of digital art collections. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Art Ontology; Art Understanding; Artwork Image Captioning; Cultural Heritage Digitization; Knowledge Graphs; Semantic Web Technologies; Ontology; Art Ontology; Art Understanding; Artwork Image Captioning; Cultural Heritage Digitization; Image Captioning; Knowledge Graphs; Ontology's; Semantic Web Technology; Standard Metrics; Test Sets; Knowledge Graph},
	keywords = {Ontology; Art ontology; Art understanding; Artwork image captioning; Cultural heritage digitization; Image captioning; Knowledge graphs; Ontology's; Semantic Web technology; Standard metrics; Test sets; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Xilong2025,
	author = {Xilong, Hou and Junhan, Zang and Wang, Xiaoguang},
	title = {Leveraging large language models for classification of cultural heritage domain terms: A case study on CIDOC CRM},
	year = {2025},
	journal = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries},
	pages = {},
	doi = {10.1145/3677389.3702562},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001112222&doi=10.1145%2F3677389.3702562&partnerID=40&md5=1bdc8619fb50a59d64cd1ccdbdf3d972},
	abstract = {Large language models (LLMs) have recently revolutionized human language understanding and generation. Ontology is considered one of the primary cornerstones for representing knowledge in a more meaningful way on the semantic web. It s significant to explore whether LLMs know and understand such ontological knowledge. In this paper, we report an experiment to investigate the performance of LLMs in the task of classifying cultural heritage domain terms to upper-level ontology. We first probed the understanding and memorization of CIDOC CRM ontological knowledge by LLMs. Then, we further leverage LLMs to classify domain terms into the structure of CRM, and compare the match type with experts. Our initial findings indicate that LLMs demonstrate a certain level of awareness and comprehension of CIDOC CRM ontological knowledge. LLMs have shown potential as valuable assistants in enhancing ontology engineering and knowledge-intensive tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cultural Heritage; Large Language Model; Ontological Knowledge Probing; Ontology; Terms Classification; Classification (of Information); Economic And Social Effects; History; Ontology; Semantics; Case-studies; Cidoc Crm; Cultural Heritages; Human Language; Language Model; Language Understanding; Large Language Model; Ontological Knowledge Probing; Ontology's; Term Classification; Domain Knowledge},
	keywords = {Classification (of information); Economic and social effects; History; Ontology; Semantics; Case-studies; CIDOC CRM; Cultural heritages; Human language; Language model; Language understanding; Large language model; Ontological knowledge probing; Ontology's; Term classification; Domain Knowledge},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ataei2025,
	author = {Ataei, Sima and Butler, Greg},
	title = {TooT-SS: Transfer Learning using ProtBERT-BFD Language Model for Predicting Specific Substrates of Transport Proteins},
	year = {2025},
	journal = {BIO Web of Conferences},
	volume = {163},
	pages = {},
	doi = {10.1051/bioconf/202516301001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000257874&doi=10.1051%2Fbioconf%2F202516301001&partnerID=40&md5=aee949671ed3dcadeab6323a8ed832f9},
	abstract = {Transmembrane transport proteins are essential in cell life for the passage of substrates across cell membranes. Metabolic network reconstruction requires transport reactions that describe the specific substrate transported as well as the metabolic reactions of enzyme catalysis. We utilize a protein language model called ProtBERT (Protein Bidirectional Encoder Representations from Transformers) and transfer learning with a one-layer Feed-Forward Neural Network (FFNN) to predict 96 specific substrates. We automatically construct a dataset UniProt-SPEC-100 using the ChEBI and GO ontologies with 4,455 sequences from 96 specific substrates. This dataset is extremely imbalanced with a ratio of 1:408 between the smallest class and the largest. Our model TooT-SS predicts 83 classes out of 96 with an F1-score of 0.92 and Matthews Correlation Coefficient (MCC) of 0.91 on a hold-out test set. The results of 3-fold cross-validation experiments, particularly, on small classes show the potential of transfer learning from the ProtBERT language model for handling imbalanced datasets. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Mountantonakis2025,
	author = {Mountantonakis, Michalis and Tzitzikas, Yannis},
	title = {Generating SPARQL Queries over CIDOC-CRM Using a Two-Stage Ontology Path Patterns Method in LLM Prompts},
	year = {2025},
	journal = {Journal on Computing and Cultural Heritage},
	volume = {18},
	number = {1},
	pages = {},
	doi = {10.1145/3708326},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002771319&doi=10.1145%2F3708326&partnerID=40&md5=9099b12317a52f5a5d39a71decdd61c1},
	abstract = {In this article, we focus on the task of exploiting the capabilities of Large Language Models (LLMs) to generate SPARQL Queries for answering natural questions over cultural Knowledge Graphs (KGs) expressed according to the ISO standard ontology CIDOC-CRM. Since CIDOC-CRM is an event-based model, usually we have to follow long paths for answering a question, thereby, the challenge is how to construct the prompt for aiding the LLM to produce the right SPARQL query. We propose and comparatively evaluate methods based on the creation of ontology path patterns of a configurable path radius (or length). Then, we construct a new dedicated benchmark that includes 100 natural questions and the corresponding SPARQL queries over two real KGs from the cultural domain describing artworks. Finally, we present comparative results about the effectiveness and efficiency over the benchmark by using ChatGPT-3.5. The most effective method follows a two-stage process that predicts and uses the most appropriate path patterns of r ≤ 4. This method achieves 3.5 higher accuracy than the baseline method (0.66 versus 0.19), that includes in the prompt only the list of properties and classes of the KG. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cidoc-crm; Cultural Heritage; Llm; Prompt Engineering; Question Answering; Benchmarking; Knowledge Graph; Natural Language Processing Systems; Ontology; Query Languages; Query Processing; Structured Query Language; Cidoc Crm; Cultural Heritages; Cultural Knowledge; Iso Standards; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Prompt Engineering; Question Answering; Iso Standards},
	keywords = {Benchmarking; Knowledge graph; Natural language processing systems; Ontology; Query languages; Query processing; Structured Query Language; CIDOC CRM; Cultural heritages; Cultural knowledge; ISO standards; Knowledge graphs; Language model; Large language model; Ontology's; Prompt engineering; Question Answering; ISO Standards},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Rzepka2025156,
	author = {Rzepka, Rafal and Obayashi, Akihiko},
	title = {Effectiveness of Security Export Control Ontology for Predicting Answer Type and Regulation Categories},
	year = {2025},
	pages = {156 - 161},
	doi = {10.1145/3704137.3704180},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000227924&doi=10.1145%2F3704137.3704180&partnerID=40&md5=f07e338ce5910ab4c0841c3f59baf162},
	abstract = {In this paper we present results of our experiments investigating if an expert knowledge graph can improve Large Language Models accuracy in predicting correct answer labels and regulations related to the topic of security export control. As the lack of related data prevents machine-learning or fine-tuning approaches, we implement prompt expansion by searching most relevant nodes of the graph and adding the expanded context to the prompt. Results of our experiments show that the addition improved answer type selection but clearly hamper the capability of finding a correct regulation category. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Expert Systems; Graphrag; Knowledge Graph; Large Language Models; Security Export Control; Adversarial Machine Learning; Expert Systems; Modeling Languages; Ontology; Prediction Models; Expert Knowledge; Export Controls; Graphrag; Knowledge Graphs; Language Model; Large Language Model; Machine-learning; Modeling Accuracy; Ontology's; Security Export Control; Knowledge Graph},
	keywords = {Adversarial machine learning; Expert systems; Modeling languages; Ontology; Prediction models; Expert knowledge; Export controls; Graphrag; Knowledge graphs; Language model; Large language model; Machine-learning; Modeling accuracy; Ontology's; Security export control; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Farmer2025535,
	author = {Farmer, Matthew Steven and Popescu, Mihail and Powell, Kimberly Ryan},
	title = {Development and evaluation of a 4M taxonomy from nursing home staff text messages using a fine-tuned generative language model},
	year = {2025},
	journal = {Journal of the American Medical Informatics Association},
	volume = {32},
	number = {3},
	pages = {535 - 544},
	doi = {10.1093/jamia/ocaf006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218408518&doi=10.1093%2Fjamia%2Focaf006&partnerID=40&md5=7e676688b8f034794501b78761b5c3b1},
	abstract = {Objective: This study aimed to explore the utilization of a fine-tuned language model to extract expressions related to the Age-Friendly Health Systems 4M Framework (What Matters, Medication, Mentation, and Mobility) from nursing home worker text messages, deploy automated mapping of these expressions to a taxonomy, and explore the created expressions and relationships. Materials and Methods: The dataset included 21 357 text messages from healthcare workers in 12 Missouri nursing homes. A sample of 860 messages was annotated by clinical experts to form a "Gold Standard"dataset. Model performance was evaluated using classification metrics including Cohen's Kappa (κ), with κ ≥ 0.60 as the performance threshold. The selected model was fine-tuned. Extractions were clustered, labeled, and arranged into a structured taxonomy for exploration. Results: The fine-tuned model demonstrated improved extraction of 4M content (κ = 0.73). Extractions were clustered and labeled, revealing large groups of expressions related to care preferences, medication adjustments, cognitive changes, and mobility issues. Discussion: The preliminary development of the 4M model and 4M taxonomy enables knowledge extraction from clinical text messages and aids future development of a 4M ontology. Results compliment themes and findings in other 4M research. Conclusion: This research underscores the need for consensus building in ontology creation and the role of language models in developing ontologies, while acknowledging their limitations in logical reasoning and ontological commitments. Further development and context expansion with expert involvement of a 4M ontology are necessary. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Automated; Clinical Decision-making; Communication; Health Services For The Aged; Llm; Ontology; Pattern Recognition; Taxonomy; Text Messaging; Article; Benchmarking; Clinical Decision Making; Cognition; Consensus; Elderly Care; Health Care Personnel; Home For The Aged; Human; Interpersonal Communication; Language Model; Logical Reasoning; Missouri; Nursing Home; Nursing Home Personnel; Ontology; Ontology Development; Pattern Recognition; Taxonomy; Text Messaging; Information Processing; Natural Language Processing; Datasets As Topic; Humans; Natural Language Processing; Nursing Homes; Text Messaging},
	keywords = {article; benchmarking; clinical decision making; cognition; consensus; elderly care; health care personnel; home for the aged; human; interpersonal communication; language model; logical reasoning; Missouri; nursing home; nursing home personnel; ontology; ontology development; pattern recognition; taxonomy; text messaging; information processing; natural language processing; Datasets as Topic; Humans; Natural Language Processing; Nursing Homes; Text Messaging},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Hurtado2025,
	author = {Hurtado, Lluís Felip and Marco-Ruiz, Luis and Segarra, Encarna and Castro-Bleda, Maria Jose and Bustos-Moreno, Aurelia and Iglesia-Vaya, Maria De La and Vallalta-Rueda, Juan Francisco},
	title = {Leveraging Transformers-based models and linked data for deep phenotyping in radiology},
	year = {2025},
	journal = {Computer Methods and Programs in Biomedicine},
	volume = {260},
	pages = {},
	doi = {10.1016/j.cmpb.2024.108567},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214290792&doi=10.1016%2Fj.cmpb.2024.108567&partnerID=40&md5=c79efde775705bd1cc1844718bf6b5f3},
	abstract = {Background and Objective: Despite significant investments in the normalization and the standardization of Electronic Health Records (EHRs), free text is still the rule rather than the exception in clinical notes. The use of free text has implications in data reuse methods used for supporting clinical research since the query mechanisms used in cohort definition and patient matching are mainly based on structured data and clinical terminologies. This study aims to develop a method for the secondary use of clinical text by: (a) using Natural Language Processing (NLP) for tagging clinical notes with biomedical terminology; and (b) designing an ontology that maps and classifies all the identified tags to various terminologies and allows for running phenotyping queries. Methods and Results: Transformers-based NLP Models, concretely pre-trained RoBERTa language models, were used to process radiology reports and annotate them identifying elements matching UMLS Concept Unique Identifiers (CUIs) definitions. CUIs were mapped into several biomedical ontologies useful for phenotyping (e.g., SNOMED-CT, HPO, ICD-10, FMA, LOINC, and ICPC2, among others) and represented as a lightweight ontology using OWL (Web Ontology Language) constructs. This process resulted in a Linked Knowledge Base (LKB), which allows running expressive queries to retrieve reports that comply with specific criteria using automatic reasoning. Conclusion: Although phenotyping tools mostly rely on relational databases, the combination of NLP and Linked Data technologies allows us to build scalable knowledge bases using standard ontologies from the Web of data. Our approach enables us to execute a pipeline which input is free text and automatically maps identified entities to a LKB that allows answering phenotyping queries. In this work, we have only used Spanish radiology reports, although it is extensible to other languages for which suitable corpora are available. This is particularly valuable in regional and national systems dealing with large research databases from different registries and cohorts and plays an essential role in the scalability of large data reuse infrastructures that require indexing and governing distributed data sources. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Deep Phenotyping; Electronic Health Records; Linked Data; Natural Language Processing; Radiology; Transformers; Clinical Research; Distributed Database Systems; Indexing (materials Working); Indexing (of Information); Metadata; Natural Language Processing Systems; Ontology; Query Languages; Records Management; Relational Database Systems; Structured Query Language; Terminology; Deep Learning; Deep Phenotyping; Electronic Health; Free Texts; Health Records; Language Processing; Natural Language Processing; Natural Languages; Phenotyping; Transformer; Radiology; Article; Human; Icd-10; Knowledge Base; Logical Observation Identifiers Names And Codes; Medical Terminology; Natural Language Processing; Phenotype; Systematized Nomenclature Of Medicine; Biological Ontology; Electronic Health Record; Radiology; Unified Medical Language System; Biological Ontologies; Electronic Health Records; Humans; Natural Language Processing; Phenotype},
	keywords = {Clinical research; Distributed database systems; Indexing (materials working); Indexing (of information); Metadata; Natural language processing systems; Ontology; Query languages; Records management; Relational database systems; Structured Query Language; Terminology; Deep learning; Deep phenotyping; Electronic health; Free texts; Health records; Language processing; Natural language processing; Natural languages; Phenotyping; Transformer; Radiology; Article; human; ICD-10; knowledge base; Logical Observation Identifiers Names and Codes; medical terminology; natural language processing; phenotype; Systematized Nomenclature of Medicine; biological ontology; electronic health record; radiology; Unified Medical Language System; Biological Ontologies; Electronic Health Records; Humans; Natural Language Processing; Phenotype},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Trappey2025,
	author = {Trappey, Amy J.C. and Chou, Shaochien and Li, Gi Kuen J.},
	title = {Patent litigation mining using a large language model—Taking unmanned aerial vehicle development as the case domain},
	year = {2025},
	journal = {World Patent Information},
	volume = {80},
	pages = {},
	doi = {10.1016/j.wpi.2024.102332},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213274311&doi=10.1016%2Fj.wpi.2024.102332&partnerID=40&md5=7ecad2f7ed095b4db711f4c643440393},
	abstract = {As unmanned aerial vehicle (UAV), also called “drone”, swiftly advances with innovative functions and applications, the surge in patent applications has profoundly reshaped the intellectual property (IP) landscape in the UAV industry, leading to a growing number of litigations. This study is structured in two phases, aiming to develop an intelligent approach to analyzing the trend and evolution of patent litigations. The first phase involves macro- and micro-patent analyses of the related technology domain. Macro patent analysis elucidates the fundamental patent information in the drone industry, while micro patent analysis leverages the technology function matrix (TFM) to identify R&D hotspots and potentials. The second phase involves litigation (judgement) mining based on large language model (LLM). Beginning with the construction of a knowledge ontology, the domain infringement landscape can be detected through TFMs. A comparative analysis of the two-phase TFMs (i.e., both TFMs of patent and infringement allocations) is then conducted to pinpoint the key legal actions and the relevant technology. To drill deeper in infringement mining, dynamic topic modeling (DTM) is applied to analyze trends and dynamics in drone controller technology over time. This study aims to strengthen IP protection by developing an intelligent litigation mining approach that adopts large language model (LLM) and uses UAV/drone litigation studies as examples to show how the approach being applied in the industry. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Drone; Dynamic Topic Modeling; Large Language Model; Patent Analysis; Patent Litigation Mining; Technology Function Matrix; Unmanned Aerial Vehicle (uav)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Shim2025,
	author = {Shim, Midan and Choi, Hyojun and Koo, Heeyeon and Um, Kaehyun and Lee, Kyong Ho and Lee, Sanghyun},
	title = {OmEGa(Ω): Ontology-based information extraction framework for constructing task-centric knowledge graph from manufacturing documents with large language model},
	year = {2025},
	journal = {Advanced Engineering Informatics},
	volume = {64},
	pages = {},
	doi = {10.1016/j.aei.2024.103001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211447959&doi=10.1016%2Fj.aei.2024.103001&partnerID=40&md5=83ae325c62a0eb6de19e87ed798314aa},
	abstract = {Manufacturing industry relies heavily on technical documents that encapsulate specialized knowledge essential for optimizing production and maintenance processes. However, extracting meaningful insights from these documents is challenging due to their complex structure, domain-specific terminology, and multimodal content, which includes text, images, and tables. Furthermore, there is a contextual gap between the generic training data of pre-trained language models (PLMs) and the specialized knowledge required for manufacturing documents. To address these issues, a Task-Centric Ontology (TCO) is designed to describe fundamental manufacturing tasks, and develop OmEGa, an Ontology-based Information Extraction Framework for Task-Centric Knowledge Graphs. OmEGa leverages large language models (LLMs) to perform instance recognition and relation classification on multimodal documents. By utilizing spatial embedding and modality linking, OmEGa addresses structural challenges, while TCO-driven reasoning mitigates contextual challenges. Experimental results demonstrate the effectiveness of OmEGa, achieving strong performance on both proprietary and open-source datasets. Additionally, a Knowledge Graph Question Answering (KGQA) system built on the extracted task-centric knowledge shows promise in enhancing communication among domain experts in the manufacturing sector. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Document Understanding; Information Extraction; Knowledge Graph; Large Language Model; Manufacturing And Maintenance Process; Ontology Modeling; Manufacturing Data Processing; Modeling Languages; Ontology; Smart Manufacturing; Document Understanding; Information Extraction; Knowledge Graphs; Language Model; Large Language Model; Maintenance Process; Manufacturing Process; Ontology Model; Ontology-based Information Extraction; Specialized Knowledge; Knowledge Graph},
	keywords = {Manufacturing data processing; Modeling languages; Ontology; Smart manufacturing; Document understanding; Information extraction; Knowledge graphs; Language model; Large language model; Maintenance process; Manufacturing process; Ontology model; Ontology-based information extraction; Specialized knowledge; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Sahbi2025,
	author = {Sahbi, Aya and Alec, Céline and Beust, Pierre},
	title = {Semantic vs. LLM-based approach: A case study of KOnPoTe vs. Claude for ontology population from French advertisements},
	year = {2025},
	journal = {Data and Knowledge Engineering},
	volume = {156},
	pages = {},
	doi = {10.1016/j.datak.2024.102392},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211195979&doi=10.1016%2Fj.datak.2024.102392&partnerID=40&md5=7adba98e00024b722539b1e6c7c8ebdc},
	abstract = {Automatic ontology population is the process of identifying, extracting, and integrating relevant information from diverse sources to instantiate the classes and properties specified in an ontology, thereby creating a Knowledge Graph (KG) for a particular domain. In this study, we evaluate two approaches for ontology population from text: KOnPoTe, a semantic technique that employs textual and domain knowledge analysis, and a generative AI method leveraging Claude, a Large Language Model (LLM). We conduct comparative experiments on three French advertisement domains: real estate, boats, and restaurants to assess the performance of these techniques. Our analysis highlights the respective strengths and limitations of the semantic approach and the LLM-based one in the context of the ontology population process. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Llm; Ontology Population; Textual Descriptions; Domain Knowledge; Ontology; Semantics; Automatic Ontology; Case-studies; Knowledge Graphs; Language Model; Large Language Model; Model Based Approach; Ontology Population; Ontology's; Property; Textual Description; Knowledge Graph},
	keywords = {Domain Knowledge; Ontology; Semantics; Automatic ontology; Case-studies; Knowledge graphs; Language model; Large language model; Model based approach; Ontology Population; Ontology's; Property; Textual description; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Zhang2025341,
	author = {Zhang, Dongliang and Zhou, Wei and Ma, Gang and Wang, Xudong and Liu, Yu and Wang, Xiaomao},
	title = {Construction and application of knowledge graph for flood defense and rescue of water infrastructure; 面向水利防汛抢险的知识图谱构建与应用},
	year = {2025},
	journal = {Shuili Xuebao/Journal of Hydraulic Engineering},
	volume = {56},
	number = {3},
	pages = {341 - 353},
	doi = {10.13243/j.cnki.slxb.20240268},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003194264&doi=10.13243%2Fj.cnki.slxb.20240268&partnerID=40&md5=adcb1bd8c0ce18aa28812655424bec12},
	abstract = {The knowledge platform is an important component in digital twin of water conservancy. However, water conservancy knowledge is dispersed across multi —source texts, which exhibit obvious unstructured and frag-mented characteristics, and knowledge extraction and effective utilization face challenges. To addresses the issues of low data quality and underutilization of knowledge in the field, this study focuses on the texts of flood defense and emergency rescue, and proposes an intelligent method for constructing a flood defense and emergency rescue knowdedge graph by improving the knowledge extraction model and combining unstructured data and external semi-structured data. Initially, a large language model is employed to extract term from unstructured texts and construct an ontology model based on term themes, a pretraining module is used to enhance text representation features, and a convolutional module is introduced to improve the entity knowdedge extraction model, and an entity data enhance-ment method is proposed to improve model accuracy. Then external encyclopedia data is extracted to expand the knowdedge coverage to build a complete flood defense and rescue knowledge graph. Experimental results demonstrate that the proposed model achieves an Fl score of 89.91% in entity knowdedge extraction, significantly outperforming baseline models. Finally, the application method of knowdedge graph in the field of flood defense and rescue is introduced, which can form a knowdedge engine for digital twin of water conservancy construction, providing knowledge support for flood control research and decision-making. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Digital Twin; Flood Defense And Rescue; Knowledge Extraction; Knowledge Graph; Multi—source Data; Knowledge Graph; Ontology; Emergency Rescue; Extraction Modeling; Flood Defence; Flood Defense And Rescue; Knowledge Extraction; Knowledge Graphs; Multi-sources; Multi—source Data; Source Data; Water Conservancy; Flood Control},
	keywords = {Knowledge graph; Ontology; Emergency rescue; Extraction modeling; Flood defence; Flood defense and rescue; Knowledge extraction; Knowledge graphs; Multi-Sources; Multi—source data; Source data; Water conservancy; Flood control},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Lemelin202599,
	author = {Lemelin, Joseph},
	title = {Haugeland’s understanding: on artificial intelligence and existential ontology},
	year = {2025},
	journal = {Continental Philosophy Review},
	volume = {58},
	number = {1},
	pages = {99 - 116},
	doi = {10.1007/s11007-024-09671-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001501878&doi=10.1007%2Fs11007-024-09671-1&partnerID=40&md5=57c51391e1335d337826bf6a60a3dc3a},
	abstract = {This article revisits John Haugeland’s early work on natural language understanding to address contemporary debates about large language models and their capacity for genuine understanding. Through a reinterpretation of Haugeland’s essay “Understanding Natural Language” via key notions in the thought of Martin Heidegger, the article argues that world-disclosing care and the capacity for taking responsibility—what Haugeland calls “giving a damn”—are the conditions of possibility for understanding. By contrasting additive and transformative approaches to understanding, the paper highlights the ontological stakes underpinning contemporary debates about understanding in AI. It concludes by situating the framework Haugeland calls “existential holism” as an overall critique of additive theories. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Haugeland; Heidegger; Large Language Models; Understanding},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Shen202526,
	author = {Shen, Wei and Zheng, Bin and Gao, Yimeng and Yu, Jingwei and Xu, Caixia},
	title = {Research on the Construction of the General Cognitive Engine System for the Field of National Defense Science and Technology Intelligence; 面向国防科技情报领域的通用认知引擎系统构建研究},
	year = {2025},
	journal = {Information studies: Theory and Application},
	volume = {48},
	number = {3},
	pages = {26 - 40},
	doi = {10.16353/j.cnki.1000-7490.2025.03.004},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001375687&doi=10.16353%2Fj.cnki.1000-7490.2025.03.004&partnerID=40&md5=823d9463256dabe1924ffd14bf0fc0aa},
	abstract = {[Purpose/ significance] In the context of an increasingly intense power game, it is crucial to comprehensively utilize intelligent means such as artificial intelligence and big data to perceive, integrate, and analyze critical intelligence from intricate information. [Method/ process] The unified cognitive engine system for defense technology intelligence integrates descriptive information from various intelligence perspectives to explore the construction of a multidimensional label ontology. Focusing on intelligence features, the representation and extraction of intelligence knowledge has been intensively studied. The fusion and disambiguation of intellectual element knowledge is achieved by leveraging the contrastive learning principle. An extensive knowledge graph for the defense technology intelligence field is automatically generated using techniques such as ontology modeling, entity relation extraction, and semantic alignment. Knowledge perception capabilities of large language models are supported by collaborative knowledge graphs. The generation capacity of lightweight large-scale intelligence topic reports is optimized through retrieval-enhanced generation techniques. [Result/ conclusion] Building upon the intelligence database, a national defense science and technology intelligence resource library has been established. The unified cognitive engine system offers functionalities including intelligence element extraction, intelligence knowledge tracing, and intelligence thematic report generation, facilitating the achievement of “intelligent perception and decision-making support” within the broader context. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cognitive Engine; Intelligence Representation; Knowledge Graph; Relation Extraction; Report Generation; Retrieval-augmented Generation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Noori2025,
	author = {Noori, Ali and Devkota, Pratik and Mohanty, Somya D. and Manda, Prashanti},
	title = {LLMs in Action: Robust Metrics for Evaluating Automated Ontology Annotation Systems},
	year = {2025},
	journal = {Information (Switzerland)},
	volume = {16},
	number = {3},
	pages = {},
	doi = {10.3390/info16030225},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001031577&doi=10.3390%2Finfo16030225&partnerID=40&md5=0f97f7b802e682af93b9493065ed2e69},
	abstract = {Ontologies are critical for organizing and interpreting complex domain-specific knowledge, with applications in data integration, functional prediction, and knowledge discovery. As the manual curation of ontology annotations becomes increasingly infeasible due to the exponential growth of biomedical and genomic data, natural language processing (NLP)-based systems have emerged as scalable alternatives. Evaluating these systems requires robust semantic similarity metrics that account for hierarchical and partially correct relationships often present in ontology annotations. This study explores the integration of graph-based and language-based embeddings to enhance the performance of semantic similarity metrics. Combining embeddings generated via Node2Vec and large language models (LLMs) with traditional semantic similarity metrics, we demonstrate that hybrid approaches effectively capture both structural and semantic relationships within ontologies. Our results show that combined similarity metrics outperform individual metrics, achieving high accuracy in distinguishing child–parent pairs from random pairs. This work underscores the importance of robust semantic similarity metrics for evaluating and optimizing NLP-based ontology annotation systems. Future research should explore the real-time integration of these metrics and advanced neural architectures to further enhance scalability and accuracy, advancing ontology-driven analyses in biomedical research and beyond. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontology; Llms; Ontology Annotation; Semantic Similarity; Data Curation; Data Integration; Graph Embeddings; Natural Language Processing Systems; Scalability; Semantics; Annotation Systems; Gene Ontology; Language Model; Language Processing; Large Language Model; Natural Languages; Ontology Annotations; Ontology's; Semantic Similarity; Similarity Metrics; Gene Ontology},
	keywords = {Data curation; Data integration; Graph embeddings; Natural language processing systems; Scalability; Semantics; Annotation systems; Gene ontology; Language model; Language processing; Large language model; Natural languages; Ontology annotations; Ontology's; Semantic similarity; Similarity metrics; Gene Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Liu2025,
	author = {Liu, Hanyou and Mao, Xi},
	title = {Joint Translation Method for English–Chinese Place Names Based on Prompt Learning and Knowledge Graph Enhancement},
	year = {2025},
	journal = {ISPRS International Journal of Geo-Information},
	volume = {14},
	number = {3},
	pages = {},
	doi = {10.3390/ijgi14030128},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000869606&doi=10.3390%2Fijgi14030128&partnerID=40&md5=3218918454b1116520ab8f579d3be640},
	abstract = {In producing English-Chinese bilingual maps, it is usually necessary to translate English place names into Chinese. However, pipeline-based methods for translating place names splits the place name translation task into multiple sub-tasks, carries the risk of error propagation, resulting in lower efficiency and poorer accuracy. Meanwhile, there is relatively little research on place name joint translation. In this regard, the study proposes an English-Chinese place name joint translation method based on prompt learning and knowledge graph enhancement. This method aims to improve the accuracy of English-Chinese place name translation. The proposed method is divided into two parts: The first part is the construction of prompt word template for place name translation. For the translation task of place names, the study first analyzes the characteristics of the transliteration of specific names and the semantic translation of generic names, constructing prompt word templates for the joint translation of ordinary place names. Then, based on the prompt words for ordinary place name translation, it takes into account the translation characteristics of the derived parts in derived place names, constructing a prompt word template for the joint translation of derived place names. Ultimately, leveraging the powerful contextual learning ability of LLM (Large Language Models), it achieves the joint translation of English and Chinese place names. The second part is the construction of the ontology of place name translation knowledge graph. To retrieve relevant knowledge about the input place names, the study designs an ontology for a knowledge graph of place names translation aimed at the English-Chinese place name translation task, combining the needs of English-Chinese place name translation and the semantic relationships between place names. This enhances the contextual information of the input place names and improves the performance of large language models in the English-Chinese place name translation task. Experiments have shown that compared to traditional pipeline-based place name translation methods, the place name translation method proposed in the study has improved performance by 21.26% in ordinary place name translation and an average of 27.70% in the field of derived place name translation. In bilingual map production, the method effectively improves the efficiency and accuracy of toponymic translation. Simultaneously providing reference for place name translation tasks in other languages. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Derived Place Name Translation; English–chinese Place Name Translation; Generic And Specific Names Joint Translation; Knowledge Graph Enhancement; Ordinary Place Name Translation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Wegner2025,
	author = {Wegner, Philipp and Fröhlich, Holger F. and Madan, Sumit},
	title = {Evaluating knowledge fusion models on detecting adverse drug events in text},
	year = {2025},
	journal = {PLOS Digital Health},
	volume = {4},
	number = {3 March},
	pages = {},
	doi = {10.1371/journal.pdig.0000468},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000421735&doi=10.1371%2Fjournal.pdig.0000468&partnerID=40&md5=fce2c45272440a641a7d6335955630aa},
	abstract = {Detecting adverse drug events (ADE) of drugs that are already available on the market is an essential part of the pharmacovigilance work conducted by both medical regulatory bodies and the pharmaceutical industry. Concerns regarding drug safety and economic interests serve as motivating factors for the efforts to identify ADEs. Hereby, social media platforms play an important role as a valuable source of reports on ADEs, particularly through collecting posts discussing adverse events associated with specific drugs. We aim with our study to assess the effectiveness of knowledge fusion approaches in combination with transformer-based NLP models to extract ADE mentions from diverse datasets, for instance, texts from Twitter, websites like askapatient.com, and drug labels. The extraction task is formulated as a named entity recognition (NER) problem. The proposed methodology involves applying fusion learning methods to enhance the performance of transformer-based language models with additional contextual knowledge from ontologies or knowledge graphs. Additionally, the study introduces a multi-modal architecture that combines transformer-based language models with graph attention networks (GAT) to identify ADE spans in textual data. A multi-modality model consisting of the ERNIE model with knowledge on drugs reached an F<inf>1</inf>-score of 71.84% on CADEC corpus. Additionally, a combination of a graph attention network with BERT resulted in an F<inf>1</inf>-score of 65.16% on SMM4H corpus. Impressively, the same model achieved an F<inf>1</inf>-score of 72.50% on the PsyTAR corpus, 79.54% on the ADE corpus, and 94.15% on the TAC corpus. Except for the CADEC corpus, the knowledge fusion models consistently outperformed the baseline model, BERT. Our study demonstrates the significance of context knowledge in improving the performance of knowledge fusion models for detecting ADEs from various types of textual data. © 2025 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Chang2025466,
	author = {Chang, Bingtao and Wen, Weiping and Wu, Xiaojie and Cheng, Siyang and Jiang, Jianchun and Mei, Rui},
	title = {TCLens: Towards Toxicity Tags Aggregation of Massive Labels Generated by Content Moderation for AIGC},
	year = {2025},
	pages = {466 - 473},
	doi = {10.1145/3709026.3709114},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219603259&doi=10.1145%2F3709026.3709114&partnerID=40&md5=65f26f7ff4a3191849849c4b7263b40b},
	abstract = {The recent boost of artificial intelligence represented by Large Language Models (LLMs) is surging. Due to the outstanding performance of LLMs, AI-Generated Content (AIGC) has also made important progress in multimodal knowledge creation referring to text, image, audio, and video. However, the security, privacy, and ethical risks associated with AIGC (e.g., fake news, social engineering attacks, and toxic content) have deeply weakened the compliance of AIGC. Although existing content moderation solutions can filter out several types of toxic content, the audit performance of different vendors and techniques are of varying quality. Some AIGC service providers improve the moderation effectiveness by introducing multiple sources of audit vendors. Due to the lack of general content moderation standards and taxonomy, the labels of multi-source moderation vendors vary greatly. To this end, We propose a novel massive label aggregation approach for content moderation named TCLens. First, we collect results of multi-vendor content moderation engines for building massive toxic labels for AIGC. Then, we introduce an ontology for better tagging with the capability of automatic updating and vendor-agnostic. Finally, we implement a prototype of TCLens. Our evaluation demonstrates that it outperforms single-source tagging and existing SOTA solutions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Aigc; Information Content Moderation; Labels Aggregation; Toxicity Tags; Ai-generated Content; Information Content Moderation; Information Contents; Knowledge Creations; Label Aggregation; Language Model; Multi-modal; Performance; Text Images; Toxicity Tag; Economic And Social Effects},
	keywords = {AI-generated content; Information content moderation; Information contents; Knowledge creations; Label aggregation; Language model; Multi-modal; Performance; Text images; Toxicity tag; Economic and social effects},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ciatto2025,
	author = {Ciatto, Giovanni and Agiollo, Andrea and Magnini, Matteo and Omicini, Andrea},
	title = {Large language models as oracles for instantiating ontologies with domain-specific knowledge},
	year = {2025},
	journal = {Knowledge-Based Systems},
	volume = {310},
	pages = {},
	doi = {10.1016/j.knosys.2024.112940},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214522484&doi=10.1016%2Fj.knosys.2024.112940&partnerID=40&md5=52807f55e1c4bc0586b3cb13347e8ed6},
	abstract = {Background: Endowing intelligent systems with semantic data commonly requires designing and instantiating ontologies with domain-specific knowledge. Especially in the early phases, those activities are typically performed manually by human experts possibly leveraging on their own experience. The resulting process is therefore time-consuming, error-prone, and often biased by the personal background of the ontology designer. Objective: To mitigate that issue, we propose a novel domain-independent approach to automatically instantiate ontologies with domain-specific knowledge, by leveraging on large language models (LLMs) as oracles. Methods: Starting from (i) an initial schema composed by inter-related classes and properties and (ii) a set of query templates, our method queries the LLM multiple times, and generates instances for both classes and properties from its replies. Thus, the ontology is automatically filled with domain-specific knowledge, compliant to the initial schema. As a result, the ontology is quickly and automatically enriched with manifold instances, which experts may consider to keep, adjust, discard, or complement according to their own needs and expertise. Contribution: We formalise our method in general way and instantiate it over various LLMs, as well as on a concrete case study. We report experiments rooted in the nutritional domain where an ontology of food meals and their ingredients is automatically instantiated from scratch, starting from a categorisation of meals and their relationships. There, we analyse the quality of the generated ontologies and compare ontologies attained by exploiting different LLMs. Experimentally, our approach achieves a quality metric that is up to five times higher than the state-of-the-art, while reducing erroneous entities and relations by up to ten times. Finally, we provide a SWOT analysis of the proposed method. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Automation; Domain-specific Knowledge; Large Language Models; Nutrition; Ontology Population; Intelligent Systems; Modeling Languages; Ontology; Semantics; Structured Query Language; Domain-specific Knowledge; Error Prones; Human Expert; Language Model; Large Language Model; Novel Domain; Ontology Population; Ontology's; Property; Semantic Data; Domain Knowledge},
	keywords = {Intelligent systems; Modeling languages; Ontology; Semantics; Structured Query Language; Domain-specific knowledge; Error prones; Human expert; Language model; Large language model; Novel domain; Ontology Population; Ontology's; Property; Semantic data; Domain Knowledge},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Li2025167,
	author = {Li, Yue and Hong, Hailan and Li, Wenlin and Yang, Tao},
	title = {Study on Application of Large Language Model in Constructing Knowledge Graph of Medical Cases of Rhinitis; 大语言模型构建鼻炎医案知识图谱的应用研究},
	year = {2025},
	journal = {Computer Engineering and Applications},
	volume = {61},
	number = {4},
	pages = {167 - 175},
	doi = {10.3778/j.issn.1002-8331.2403-0379},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007353217&doi=10.3778%2Fj.issn.1002-8331.2403-0379&partnerID=40&md5=d574a251f4160205fbe51f9e4143f39a},
	abstract = {An automated knowledge extraction method based on large language model is explored, aiming to construct a knowledge graph on the treatment of rhinitis by national medical master Gan Zuwang, and to provide innovative ideas and methods for the intelligent advancement in the field of traditional Chinese medicine. The clinical medical case data of professor Gan Zuwang are used as the base sample, and the ontology model is constructed using OWL (Web ontology language) to determine the extraction objects and relations, and then the prompt template combining the demonstration case and the relation list is used to guide the automated extraction experiments of the medical case data with the large language model, and the Nebula Graph is used for the storage and the visual display of the knowledge graph. Compared with the traditional knowledge extraction model Bert-BiLSTM-CRF, the ChatGPT4 model performs the best in terms of comprehensive indexes, with an F1 value of 82.75%, which provides an effective solution for the rapid processing of unstructured medical case data and achieves semi-automatic construction of knowledge graph in the field of Chinese medicine. The use of large language models for knowledge graph construction not only provides a practical solution for the intelligence in the field of Chinese medicine, but also contributes new research ideas for the inheritance of diagnostic and treatment experience of famous and veteran Chinese medicine practitioners and the rapid construction of the knowledge graph of Chinese medicine, which promotes the development of Chinese medicine. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gan Zuwang; Large Language Model; National Medical Master; Nebula Graph; Diagnosis; List Processing Languages; Ontology; Chinese Medicines; Extraction Method; Gan Zuwang; Knowledge Extraction; Knowledge Graphs; Language Model; Large Language Model; Medical Case; National Medical Master; Nebula Graph; Knowledge Graph},
	keywords = {Diagnosis; List processing languages; Ontology; Chinese medicines; Extraction method; Gan zuwang; Knowledge extraction; Knowledge graphs; Language model; Large language model; Medical case; National medical master; Nebula graph; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bertini2025139,
	author = {Bertini, Flavio and Dal Palú, Alessandro and Zaglio, Federica and Fabiano, Francesco and Formisano, Andrea},
	title = {Data2Concept2Text: An Explainable Multilingual Framework for Data Analysis Narration},
	year = {2025},
	journal = {Electronic Proceedings in Theoretical Computer Science, EPTCS},
	volume = {416},
	pages = {139 - 152},
	doi = {10.4204/EPTCS.416.13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218622044&doi=10.4204%2FEPTCS.416.13&partnerID=40&md5=6439d7c726401ae386f40e2be294bdd8},
	abstract = {This paper presents a complete explainable system that interprets a set of data, abstracts the underlying features and describes them in a natural language of choice. The system relies on two crucial stages: (i) identifying emerging properties from data and transforming them into abstract concepts, and (ii) converting these concepts into natural language. Despite the impressive natural language generation capabilities demonstrated by Large Language Models, their statistical nature and the intricacy of their internal mechanism still force us to employ these techniques as black boxes, forgoing trustworthiness. Developing an explainable pipeline for data interpretation would allow facilitating its use in safety-critical environments like processing medical information and allowing non-experts and visually impaired people to access narrated information. To this end, we believe that the fields of knowledge representation and automated reasoning research could present a valid alternative. Expanding on prior research that tackled the first stage (i), we focus on the second stage, named Concept2Text. Being explainable, data translation is easily modeled through logic-based rules, once again emphasizing the role of declarative programming in achieving AI explainability. This paper explores a Prolog/CLP-based rewriting system to interpret concepts-articulated in terms of classes and relations, plus common knowledge-derived from a generic ontology, generating natural language text. Its main features include hierarchical tree rewritings, modular multilingual generation, support for equivalent variants across semantic, grammar, and lexical levels, and a transparent rule-based system. We outline the architecture and demonstrate its flexibility through some examples capable of generating numerous diverse and equivalent rewritings based on the input concept. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Hospital Data Processing; Knowledge Representation; Medical Information Systems; Metadata; Natural Language Processing Systems; Ontology; Prolog (programming Language); Spatio-temporal Data; Visual Languages; Abstract Concept; Black Boxes; Critical Environment; Data Interpretation; Language Model; Medical Information; Natural Language Generation; Natural Languages; Property; Stage I; Semantics},
	keywords = {Hospital data processing; Knowledge representation; Medical information systems; Metadata; Natural language processing systems; Ontology; PROLOG (programming language); Spatio-temporal data; Visual languages; Abstract concept; Black boxes; Critical environment; Data interpretation; Language model; Medical information; Natural language generation; Natural languages; Property; Stage I; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Gaskin202535,
	author = {Gaskin, Benjamin},
	title = {Symbol Grounding in the Age of LLMs},
	year = {2025},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {397},
	pages = {35 - 42},
	doi = {10.3233/FAIA241487},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000685544&doi=10.3233%2FFAIA241487&partnerID=40&md5=3ede4da561d23b483dd3a2378929ef52},
	abstract = {This paper considers symbol grounding in its practical and theoretical aspects. Taking up the theoretical perspective, we begin by considering the relative inefficiency of large language models in acquiring language. A framework is introduced based on the concept of morphological computation and formalised with reference to conditional Kolmogorov complexity: that the form of embodied experience scaffolds human language acquisition. This argument is extended to consider the symbol grounding problem, with particular reference to the origin of language in both the individual and historical sense. It is argued that, while humans also make use of statistical learning, the process of symbol grounding via morphological computation is essential at the origins of language and during early development. It provides a minimal ontology in terms of objects, containers, processes, etc.-basic features which language models must instead brute force by statistical means. The paper closes by reconsidering the symbol grounding problem in light of recent advances, particularly the promise of multi-modal models and robotics, and ultimately concludes that the status of the symbol grounding problem depends upon our aims in the pursuit of artificial intelligence. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Morphological Computation; Multi-modal Language Learning; Multi-modal Language Models; Robotics; Symbol Grounding; Adversarial Machine Learning; Basic (programming Language); Computer Aided Language Translation; Computer Simulation Languages; Contrastive Learning; Digital Elevation Model; Linguistics; Ontology; Problem Oriented Languages; Robot Learning; Language Learning; Language Model; Large Language Model; Modal Language; Morphological Computation; Multi-modal; Multi-modal Language Learning; Multi-modal Language Model; Symbol Grounding; Scaffolds},
	keywords = {Adversarial machine learning; BASIC (programming language); Computer aided language translation; Computer simulation languages; Contrastive Learning; Digital elevation model; Linguistics; Ontology; Problem oriented languages; Robot learning; Language learning; Language model; Large language model; Modal language; Morphological computation; Multi-modal; Multi-modal language learning; Multi-modal language model; Symbol grounding; Scaffolds},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Han2025,
	author = {Han, Yaoyao and Liu, Jiping and Luo, An and Wang, Yong and Bao, Shuai},
	title = {Fine-Tuning LLM-Assisted Chinese Disaster Geospatial Intelligence Extraction and Case Studies},
	year = {2025},
	journal = {ISPRS International Journal of Geo-Information},
	volume = {14},
	number = {2},
	pages = {},
	doi = {10.3390/ijgi14020079},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218853736&doi=10.3390%2Fijgi14020079&partnerID=40&md5=29e4ee386e01bf0e3172b989f6e03aa2},
	abstract = {The extraction of disaster geospatial intelligence (DGI) from social media data with spatiotemporal attributes plays a crucial role in real-time disaster monitoring and emergency decision-making. However, conventional machine learning approaches struggle with semantic complexity and limited Chinese disaster corpus. Recent advancements in large language models (LLMs) offer new opportunities to overcome these challenges due to their enhanced semantic comprehension and multi-task learning capabilities. This study investigates the potential application of LLMs in disaster intelligence extraction and proposes an efficient, scalable method for multi-hazard DGI extraction. Building upon a unified ontological framework encompassing core natural disaster elements, this method employs parameter-efficient low-rank adaptation (LoRA) fine-tuning to optimize open-source Chinese LLMs using a meticulously curated instruction-tuning dataset. It achieves simultaneous identification of multi-hazard intelligence cues and extraction of disaster spatial entity attributes from unstructured Chinese social media texts through unified semantic parsing and structured knowledge mapping. Compared to pre-trained models such as BERT and ERNIE, the proposed method was shown to achieve state-of-the-art evaluation results, with the highest recognition accuracy (F1-score: 0.9714) and the best performance in structured information generation (BLEU-4 score: 92.9649). Furthermore, we developed and released DGI-Corpus, a Chinese instruction-tuning dataset covering various disaster types, to support the research and application of LLMs in this field. Lastly, the proposed method was applied to analyze the spatiotemporal evolution patterns of the Zhengzhou “7.20” flood disaster. This study enhances the efficiency of natural disaster monitoring and emergency management, offering technical support for disaster response and mitigation decision-making. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Emergency Management; Geospatial Intelligence; Large Language Models; Natural Disasters; Social Media; Spatiotemporal Information Mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Doumanas2025,
	author = {Doumanas, Dimitrios and Soularidis, Andreas and Spiliotopoulos, Dimitris and Vassilakis, Costas and Kotis, Konstantinos I.},
	title = {Fine-Tuning Large Language Models for Ontology Engineering: A Comparative Analysis of GPT-4 and Mistral},
	year = {2025},
	journal = {Applied Sciences (Switzerland)},
	volume = {15},
	number = {4},
	pages = {},
	doi = {10.3390/app15042146},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218624656&doi=10.3390%2Fapp15042146&partnerID=40&md5=cd9c184dfaf18e7296ecdf2772a70ee0},
	abstract = {Ontology engineering (OE) plays a critical role in modeling and managing structured knowledge across various domains. This study examines the performance of fine-tuned large language models (LLMs), specifically GPT-4 and Mistral 7B, in efficiently automating OE tasks. Foundational OE textbooks are used as the basis for dataset creation and for feeding the LLMs. The methodology involved segmenting texts into manageable chapters, generating question–answer pairs, and translating visual elements into description logic to curate fine-tuned datasets in JSONL format. This research aims to enhance the models’ abilities to generate domain-specific ontologies, with hypotheses asserting that fine-tuned LLMs would outperform base models, and that domain-specific datasets would significantly improve their performance. Comparative experiments revealed that GPT-4 demonstrated superior accuracy and adherence to ontology syntax, albeit with higher computational costs. Conversely, Mistral 7B excelled in speed and cost efficiency but struggled with domain-specific tasks, often generating outputs that lacked syntactical precision and relevance. The presented results highlight the necessity of integrating domain-specific datasets to improve contextual understanding and practical utility in specialized applications, such as Search and Rescue (SAR) missions in wildfire incidents. Both models, despite their limitations, exhibited potential in understanding OE principles. However, their performance underscored the importance of aligning training data with domain-specific knowledge to emulate human expertise effectively. This study, based on and extending our previous work on the topic, concludes that fine-tuned LLMs with targeted datasets enhance their utility in OE, offering insights into improving future models for domain-specific applications. The findings advocate further exploration of hybrid solutions to balance accuracy and efficiency. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain-specific Knowledge; Large Language Models (llms) Fine-tuning; Ontology Engineering (oe); Search And Rescue (sar); Ontology; Syntactics; Domain Specific; Domain-specific Knowledge; Fine Tuning; Language Model; Large Language Model Fine-tuning; Ontology Engineering; Performance; Search And Rescue; Domain Knowledge},
	keywords = {Ontology; Syntactics; Domain specific; Domain-specific knowledge; Fine tuning; Language model; Large language model fine-tuning; Ontology engineering; Performance; Search and rescue; Domain Knowledge},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access}
}

@ARTICLE{Braun2025,
	author = {Braun, Ian R. and Hartley, Emily L. and Olson, Daniel and Matentzoglu, Nicolas A. and Schaper, Kevin and Walls, Ramona L. and Vasilevsky, Nicole A.},
	title = {Increased discoverability of rare disease datasets through knowledge graph integration},
	year = {2025},
	journal = {JAMIA Open},
	volume = {8},
	number = {1},
	pages = {},
	doi = {10.1093/jamiaopen/ooaf001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217561441&doi=10.1093%2Fjamiaopen%2Fooaf001&partnerID=40&md5=f6fd8b904707ef1e17da57345b88c160},
	abstract = {Objectives: Demonstrate a methodology for improving discoverability of rare disease datasets by enriching source data with biological associations. Materials and Methods: We developed an extension of the Biolink semantic model to incorporate patient data and generated a knowledge graph (KG) comprising patient data and associations between biological entities in an existing KG, leveraging existing mappings and mapping standards. Results: The enriched model of patient data can support a search application that is aware of biological associations and provides a semantic search interface to discover and summarize patient datasets within the broader biological context. Discussion and Conclusion: Our methodology enriches datasets with a wealth of additional biological knowledge, improving discoverability. Using condition concepts, we illustrate techniques that could be applied to other entities within source data such as measurements and observations. This work provides a foundational framework for how source data can be modeled to improve accuracy of upstream language models for natural language querying. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Ontology; Rare Disease; Article; Association; Common Data Elements; Data Analytics; Data Integration; Data Interoperability; Gene; Knowledge Graph; Methodology; Patient Coding; Phenotype; Rare Disease; Semantics},
	keywords = {Article; association; common data elements; data analytics; data integration; data interoperability; gene; knowledge graph; methodology; patient coding; phenotype; rare disease; semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Maciol20253735,
	author = {Maciol, Andrzej and Macioł, Piotr and Gumienny, Grzegorz and Wrzała, Konrad},
	title = {A new ontology-based approach to automatic information extraction from speech for production disturbance management},
	year = {2025},
	journal = {International Journal of Advanced Manufacturing Technology},
	volume = {136},
	number = {7},
	pages = {3735 - 3752},
	doi = {10.1007/s00170-025-15000-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217221593&doi=10.1007%2Fs00170-025-15000-4&partnerID=40&md5=fec869b9f80dffd47466bc35e18395d7},
	abstract = {The goal of our research was to design a methodology for extracting systematized knowledge from free speech. The sources of knowledge in our analysis were records of production meetings, focused on production disturbance (PD). The main obstacle is to properly identify the specific meaning of words, in a specific, usually narrow, industry. Machine learning based on data from production records has been increasingly used to build such models. In the case of manufacturing plants with diverse production programs, acquiring the right number and structure of data is not possible; hence, proper identification of such terms is for classical NLP tools, even supported by large language models, not possible. We have attempted to use AI and NLP tools from recorded production meeting recordings to create and continuously update PD’s knowledge as a supplement to data from documentation. This is an approach not previously known in the field of production management. The solution we developed consists of an expert-defined specific ontology, based on the pre-processed speeches. At this stage, a lexicon (vocabulary) is also created, supporting the transformation of the speeches into interpretable texts. The model ontology formulated this way is then used to analyze consecutively provided meeting records and thus update the operational ontology. In our research, we used the materials provided to us in the form of records of production meetings from a medium-sized pressure foundry. The obtained results confirm that the adopted knowledge model and the algorithms might be successfully utilized to solve real-world manufacturing problems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Decision Support Systems; Natural Language Processing; Nlp; Ontology; Owl; Production Disturbance; Manufacturing Data Processing; Metadata; Records Management; Decision Supports; Language Processing; Natural Language Processing; Natural Languages; Nlp Tools; Ontology's; Ontology-based; Owl; Production Disturbance; Support Systems; Ontology},
	keywords = {Manufacturing data processing; Metadata; Records management; Decision supports; Language processing; Natural language processing; Natural languages; NLP tools; Ontology's; Ontology-based; OWL; Production disturbance; Support systems; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025350,
	author = {Li, Yansheng and Zhong, Zhenyu and Meng, Qingxiang and Mao, Zhidian and Dang, Bo and Wang, Tao and Feng, Yuanjun and Zhang, Yongjun},
	title = {Intelligent Purification of Natural Resource Element Change Polygons Driven by Remote Sensing Spatiotemporal Knowledge Graphs; 遥感时空知识图谱驱动的自然资源要素变化图斑智能净化},
	year = {2025},
	journal = {Journal of Geo-Information Science},
	volume = {27},
	number = {2},
	pages = {350 - 366},
	doi = {10.12082/dqxxkx.2025.240571},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216841172&doi=10.12082%2Fdqxxkx.2025.240571&partnerID=40&md5=e45626a46cc797add80eb0d1a4dc8525},
	abstract = {[Objectives] With the development of deep learning technology, the ability to monitor changes in natural resource elements using remote sensing images has significantly improved. While deep learning change detection models excel at extracting low-level semantic information from remote sensing images, they face challenges in distinguishing land-use type changes from non-land-use type changes, such as crop rotation, natural fluctuations in water levels, and forest degradation. To ensure a high recall rate in change detection, these models often generate a large number of false positive change polygons, requiring substantial manual effort to eliminate these false alarms. [Methods] To address this issue, this paper proposes a natural resource element change polygon purification algorithm driven by remote sensing spatiotemporal knowledge graph. The algorithm aims to minimize the false positive rate while maintaining a high recall rate, thereby improving the efficiency of natural resource element change monitoring. To support the intelligent construction and effective reasoning of the spatiotemporal knowledge graph, this study designed a remote sensing spatiotemporal knowledge graph ontology model taking into account spatiotemporal characteristics and developed a GraphGIS toolkit that integrates graph database storage and computation. This paper also introduces a vector knowledge extraction method based on the native spatial analysis of the GraphGIS graph database, a remote sensing image knowledge extraction method based on efficient fine-tuning of the SkySense visual large model, and a polygon purification knowledge extraction method based on the SeqGPT large language model. Under the constraints of the spatiotemporal ontology model, vector, image, and text knowledge converge to form a remote sensing spatiotemporal knowledge graph. Inspired by the manual operation methods for change polygon purification, this paper developed an automatic purification method of change polygons based on first-order logical reasoning within the knowledge graph. To improve the concurrent processing and human-computer interaction, this paper developed a remote sensing spatiotemporal knowledge graph management and service system. [Results] For the task of purifying natural resource element change polygons in Guangdong Province from March to June 2024, the proposed method achieved a true-preserved rate of 95.37% and a false-removed rate of 21.82%. [Conclusions] The intelligent purification algorithm and system for natural resource element change polygons proposed in this study effectively reduce false positives while preserving real change polygons. This approach significantly enhances the efficiency of natural resource element change monitoring. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {First-order Logic Reasoning; Large Language Model; Natural Resource Element Change Monitoring; Remote Sensing Image Change Detection; Remote Sensing Large Model; Spatial Computing In Graph Database; Spatiotemporal Intelligence; Spatiotemporal Knowledge Graph; Deforestation; Graph Databases; Image Retrieval; Knowledge Graph; Ontology; Photomapping; Semantics; First Order Logic; First-order Logic Reasoning; Graph Database; Image Change Detection; Knowledge Graphs; Language Model; Large Language Model; Large Models; Logic Reasoning; Natural Resource Element Change Monitoring; Remote Sensing Image Change Detection; Remote Sensing Images; Remote Sensing Large Model; Remote-sensing; Resource Element; Spatial Computing; Spatial Computing In Graph Database; Spatiotemporal Intelligence; Spatiotemporal Knowledge Graph; Detection Method; Image Analysis; Image Classification; Knowledge Based System; Language; Machine Learning; Remote Sensing; Spatiotemporal Analysis},
	keywords = {Deforestation; Graph Databases; Image retrieval; Knowledge graph; Ontology; Photomapping; Semantics; First order logic; First-order logic reasoning; Graph database; Image change detection; Knowledge graphs; Language model; Large language model; Large models; Logic reasoning; Natural resource element change monitoring; Remote sensing image change detection; Remote sensing images; Remote sensing large model; Remote-sensing; Resource element; Spatial computing; Spatial computing in graph database; Spatiotemporal intelligence; Spatiotemporal knowledge graph; detection method; image analysis; image classification; knowledge based system; language; machine learning; remote sensing; spatiotemporal analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zheng2025,
	author = {Zheng, Zhiyu and Marié, Sylvain and Farazdaghi, Elham and Yahia, Esma and Makhoul, Khal and Lagarde, Théo and El Meouche, Rani and Ababsa, Fakhreddine},
	title = {Mastering building management systems data points tagging with minimal examples: unveiling the power of large language models},
	year = {2025},
	journal = {Energy and Buildings},
	volume = {328},
	pages = {},
	doi = {10.1016/j.enbuild.2024.115173},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213251513&doi=10.1016%2Fj.enbuild.2024.115173&partnerID=40&md5=6d2821321de8f00ca808faf5e49e7156},
	abstract = {The heterogeneity of metadata within Building Management Systems (BMS) poses substantial challenges for advanced analytics, including cross-building analysis. Over the past decade, metadata standard schemas such as Brick have been developed to address this challenge. Nevertheless, mapping BMS metadata with such standards accurately and efficiently continues to be a demanding task across both new and existing buildings. This work explores the application of Large Language Models (LLMs) to tag BMS data points, thus facilitating metadata standardization efforts. Manual or rule-based methods are not only labor-intensive but also error-prone. Similarly, supervised learning approaches using Machine Learning (ML) and Natural Language Processing ( NLP) demand extensive labeled datasets, often making them laborious and inflexible to new BMS metadata types and tasks. We propose a novel three-step framework that enhances the tagging process by integrating a LLM with few-shot prompting and an embedding model. This approach not only improves result interpretability but also effectively mitigates hallucinations. This framework is further supported by analyses of the LLM's inherent capabilities, prompt-aided specific interpretation and output formatting, and evaluations of few-shot sizes. Tested across five different building datasets, our approach, leveraging few-shot examples, achieves performance comparable to state-of-the-art supervised learning methods that rely on large labeled datasets. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Brick Ontology; Building Management Systems; Few-shot Learning, Prompt Engineering; Large Language Models; Metadata Tagging; Semantic Web Technologies; Adversarial Machine Learning; Brick; Labeled Data; Natural Language Processing Systems; Ontology; Semantics; Supervised Learning; Zero-shot Learning; Brick Ontology; Building Management System; Datapoints; Few-shot Learning, Prompt Engineering; Labeled Dataset; Language Model; Large Language Model; Metadata Tagging; Ontology's; Semantic Web Technology; Self-supervised Learning},
	keywords = {Adversarial machine learning; Brick; Labeled data; Natural language processing systems; Ontology; Semantics; Supervised learning; Zero-shot learning; Brick ontology; Building management system; Datapoints; Few-shot learning, prompt engineering; Labeled dataset; Language model; Large language model; Metadata tagging; Ontology's; Semantic Web technology; Self-supervised learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Koubaa2025355,
	author = {Koubaa, Anis and Ammar, Adel H. and Boulila, Wadii},
	title = {Next-generation human-robot interaction with ChatGPT and robot operating system},
	year = {2025},
	journal = {Software - Practice and Experience},
	volume = {55},
	number = {2},
	pages = {355 - 382},
	doi = {10.1002/spe.3377},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205287419&doi=10.1002%2Fspe.3377&partnerID=40&md5=23d32d57b83724486c92ba1503648b1b},
	abstract = {This article presents an innovative concept that harnesses the capabilities of large language models (LLMs) to revolutionize human-robot interaction. This work aims to connect large language models with the Robot Operating System (ROS), the primary development framework for robotics applications. We develop a package for ROS that seamlessly integrates ChatGPT with ROS2-based robotic systems. The core idea is to leverage prompt engineering with LLMs, utilizing unique properties such as ability eliciting, chain-of-thought, and instruction tuning. The concept employs ontology development to convert unstructured natural language commands into structured robotic instructions specific to the application context through prompt engineering. We capitalize on LLMs' zero-shots and few-shots learning capabilities by eliciting structured robotic commands from unstructured human language inputs. To demonstrate the feasibility of this concept, we implemented a proof-of-concept that integrates ChatGPT with ROS2, showcasing the transformation of human language instructions into spatial navigation commands for a ROS2-enabled robot. Besides, we quantitatively evaluated this transformation over three use cases (ground robot, unmanned aerial vehicle, and Robotic arm) and five LLMs (LLaMA-7b, LLaMA2-7b, LLaMA2-70b, GPT-3.5, and GPT-4) on a set of 3000 natural language commands. Our system serves as a new stride towards Artificial General Intelligence (AGI) and paves the way for the robotics and natural language processing communities to collaborate in creating novel, intuitive human-robot interactions. The open-source implementation of our system on ROS 2 is available on GitHub. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Human-robot Interaction; Large Language Models; Mobile Robots; Ontology; Robot Operating System; Mobile Robots; Ontology; Problem Oriented Languages; Robot Applications; Robot Learning; Robot Operating System; Robotic Arms; Unmanned Aerial Vehicles (uav); Zero-shot Learning; Chatgpt; Development Frameworks; Human Language; Humans-robot Interactions; Language Model; Large Language Model; Natural Languages; Ontology's; Robotic Systems; Robotics Applications; Human Robot Interaction},
	keywords = {Mobile robots; Ontology; Problem oriented languages; Robot applications; Robot learning; Robot Operating System; Robotic arms; Unmanned aerial vehicles (UAV); Zero-shot learning; ChatGPT; Development frameworks; Human language; Humans-robot interactions; Language model; Large language model; Natural languages; Ontology's; Robotic systems; Robotics applications; Human robot interaction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Young2025,
	author = {Young, Cameron C. and Enichen, Elizabeth J. and Rivera, Christian and Auger, Corinne A. and Grant, Nathan and Rao, Arya S. and Succi, Marc David},
	title = {Diagnostic Accuracy of a Custom Large Language Model on Rare Pediatric Disease Case Reports},
	year = {2025},
	journal = {American Journal of Medical Genetics, Part A},
	volume = {197},
	number = {2},
	pages = {},
	doi = {10.1002/ajmg.a.63878},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203964043&doi=10.1002%2Fajmg.a.63878&partnerID=40&md5=cd274f0e63f634602f5ad8e5906a857a},
	abstract = {Accurately diagnosing rare pediatric diseases frequently represent a clinical challenge due to their complex and unusual clinical presentations. Here, we explore the capabilities of three large language models (LLMs), GPT-4, Gemini Pro, and a custom-built LLM (GPT-4 integrated with the Human Phenotype Ontology [GPT-4 HPO]), by evaluating their diagnostic performance on 61 rare pediatric disease case reports. The performance of the LLMs were assessed for accuracy in identifying specific diagnoses, listing the correct diagnosis among a differential list, and broad disease categories. In addition, GPT-4 HPO was tested on 100 general pediatrics case reports previously assessed on other LLMs to further validate its performance. The results indicated that GPT-4 was able to predict the correct diagnosis with a diagnostic accuracy of 13.1%, whereas both GPT-4 HPO and Gemini Pro had diagnostic accuracies of 8.2%. Further, GPT-4 HPO showed an improved performance compared with the other two LLMs in identifying the correct diagnosis among its differential list and the broad disease category. Although these findings underscore the potential of LLMs for diagnostic support, particularly when enhanced with domain-specific ontologies, they also stress the need for further improvement prior to integration into clinical practice. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Diagnostic Support; Genetics; Large Language Models; Pediatric Rare Disease; Adrenal Insufficiency; Article; Artificial Intelligence; Artificial Neural Network; Bartter Syndrome; Bradycardia; Case Report; Childhood Disease; Clinical Practice; Congenital Heart Disease; Diagnostic Accuracy; Diagnostic Test Accuracy Study; Fever; Gene Ontology; Genetics; Gitelman Syndrome; Human; Hypoglycemia; Hypotension; Hypothermia; Hypothyroidism; Infant; Language Test; Large Language Model; Lethargy; Machine Learning; Muscle Hypotonia; Natural Language Processing; Phenotype; Physiological Stress; Pseudohypoaldosteronism Type 1; Rare Disease; Seizure; Sepsis; Short Stature; Child; Diagnosis; Pediatrics; Child; Humans; Pediatrics; Phenotype; Rare Diseases},
	keywords = {adrenal insufficiency; Article; artificial intelligence; artificial neural network; Bartter syndrome; bradycardia; case report; childhood disease; clinical practice; congenital heart disease; diagnostic accuracy; diagnostic test accuracy study; fever; gene ontology; genetics; Gitelman syndrome; human; hypoglycemia; hypotension; hypothermia; hypothyroidism; infant; language test; large language model; lethargy; machine learning; muscle hypotonia; natural language processing; phenotype; physiological stress; pseudohypoaldosteronism type 1; rare disease; seizure; sepsis; short stature; child; diagnosis; pediatrics; Child; Humans; Pediatrics; Phenotype; Rare Diseases},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Bagchi2025297,
	author = {Bagchi, Mayukh},
	title = {Toward Generative AI–Driven Metadata Modeling: A Human–Large Language Model Collaborative Approach},
	year = {2025},
	journal = {Library Trends},
	volume = {73},
	number = {3},
	pages = {297 - 322},
	doi = {10.1353/lib.2025.a961196},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007344857&doi=10.1353%2Flib.2025.a961196&partnerID=40&md5=df94c1ac0d2b540ba768f9abe71eb6c2},
	abstract = {For decades, the modeling of metadata has been core to the functioning of any academic library. Metadata’s importance has only increased with the pervasiveness of generative artificial intelligence–driven information activities and services. However, several challenges impact a library metadata model’s reusability, crosswalk, and interoperability with other metadata models. This paper posits that these problems stem from an underlying assumption that there should be only a few core metadata models that would be sufficient for any information service using them, irrespective of the heterogeneity of intradomain or interdomain settings. To that end, this paper advances a contrary view and substantiates its argument in three key steps. First, the paper introduces a novel way of thinking about a library metadata model as an ontology-driven composition of five functionally interlinked representation levels from perception to definition via properties. Second, the paper introduces the representational manifoldness implicit in each of the five levels, which cumulatively contributes to a conceptually entangled library metadata model. Finally, and most importantly, the paper proposes a generative AI–driven, human–large language model collaboration-based metadata modeling approach to disentangle the entanglement inherent in each representation level, which would lead to a conceptually disentangled metadata model. Throughout the paper, the author provides motivating scenarios and examples from libraries handling cancer information. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Academic Libraries; Generative Ai; Human-llm Collaboration; Knowledge Organization; Knowledge Representation; Large Language Models; Metadata; Ontology-driven Metadata Models},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025,
	author = {Li, Yunqing and Ko, Hyunwoong and Ameri, Farhad},
	title = {Integrating Graph Retrieval-Augmented Generation With Large Language Models for Supplier Discovery},
	year = {2025},
	journal = {Journal of Computing and Information Science in Engineering},
	volume = {25},
	number = {2},
	pages = {},
	doi = {10.1115/1.4067389},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001152782&doi=10.1115%2F1.4067389&partnerID=40&md5=3269aec1ade8940fe3c320b5074b6220},
	abstract = {As supply chain complexity and dynamism challenge traditional management approaches, integrating large language models (LLMs) and knowledge graphs (KGs) emerges as a promising method for advancing supply chain analytics. This article presents a methodology crafted to harness the synergies between LLMs and KGs, with a particular focus on enhancing supplier discovery practices. The primary goal is to transform and integrate a vast body of unstructured supplier capability data into a harmonized KG, thus improving the supplier discovery process and enhancing the accessibility and findability of manufacturing suppliers. Through an ontology-driven graph construction process, the presented methodology integrates KGs and retrieval-augmented generation with advanced LLM-based natural language processing techniques. With the aid of a detailed case study, we showcase how this integrated approach not only enhances the quality of answers and increases visibility for small- and medium-sized manufacturers but also amplifies agility and provides strategic insights into supply chain management. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Big Data And Analytics; Cybermanufacturing; Data-driven Engineering; Knowledge Engineering; Machine Learning For Engineering Applications; Manufacturing Automation; Manufacturing Data Processing; Modeling Languages; Natural Language Processing Systems; Smart Manufacturing; Big Data And Analytic; Cybermanufacturing; Data Driven; Data-driven Engineering; Engineering Applications; Knowledge Graphs; Language Model; Machine Learning For Engineering Application; Machine-learning; Manufacturing Automation; Knowledge Graph},
	keywords = {Manufacturing data processing; Modeling languages; Natural language processing systems; Smart manufacturing; Big data and analytic; Cybermanufacturing; Data driven; Data-driven engineering; Engineering applications; Knowledge graphs; Language model; Machine learning for engineering application; Machine-learning; Manufacturing Automation; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Li2025353,
	author = {Li, Yanjun and Yang, Ruiting and Guo, Donghao and Song, Yu},
	title = {Research on the Construction of Digital Knowledge Graphs Based on Resources of National First-Class Undergraduate Programs},
	year = {2025},
	pages = {353 - 359},
	doi = {10.1145/3707292.3707389},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219182993&doi=10.1145%2F3707292.3707389&partnerID=40&md5=dd8cbab98388de29cd98d5be7b7bd296},
	abstract = {[Purpose/Significance]: The digitalization of education is an essential path to advancing higher education. The construction of knowledge graphs is a key approach to achieving the digitalization and intelligence of education. [Method/Process]: This paper leverages the rich video resources of existing national first-class undergraduate programs and, based on the teaching orientations of different universities, independently designs customized ontologies and extraction principles. These are then integrated into the LLM knowledge graph builder to ensure the hierarchical structure of the overall course framework. The course video content is transformed into text form, and large language models (LLMS) and word segmentation tools are used for core content extraction, text cleaning, and lexical analysis. The structured text is then converted into SPO (Subject-Predicate-Object) triplets database. [Results/Conclusions]: Finally, the database is imported into the LLM knowledge graph builder, which is pre-configured with extraction rules. It will automatically generate the knowledge graph. After the text is imported into the LLM knowledge graph builder, it will be manually checked to ensure it better meets the actual needs of the students. [Innovation/Limitations]: The research team plans to apply the knowledge graph to train a specialized knowledge-based Q&A assistant. This will support students’ understanding and self-assessment of knowledge points in an online learning community. Student feedback will be used to improve and enrich the knowledge graph. Compared to existing methods, this approach better aligns with the constantly evolving digital teaching resources available online, offering more comprehensive and higher-level automation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Course Resources; Intelligent Q&a; Knowledge Graph; Ontology Construction; Personalized Learning; Curricula; Federated Learning; Ontology; Students; Teaching; Course Resource; Graph-based; Hierarchical Structures; High Educations; Intelligent Q&a; Knowledge Graphs; Ontology Construction; Ontology's; Personalized Learning; Undergraduate Projects; Knowledge Graph},
	keywords = {Curricula; Federated learning; Ontology; Students; Teaching; Course resource; Graph-based; Hierarchical structures; High educations; Intelligent Q&A; Knowledge graphs; Ontology construction; Ontology's; Personalized learning; Undergraduate projects; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Queiroz2025,
	author = {Queiroz, Jonas F.P. and Jaculli, Marcelo A. and Junior, N. Choueri and Silveira, I. M. and Mendes, José Ricardo Pelaquim and Penteado, Bruno Elias and Guilherme, Ivan Rizzo and Perrout, Stephan Ribeiro},
	title = {An Ontology of Well Engineering Entities to Extract and Structure Text Data from Daily Reports},
	year = {2025},
	volume = {2025-March},
	pages = {},
	doi = {10.2118/223801-MS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000236649&doi=10.2118%2F223801-MS&partnerID=40&md5=962b92de8f9f2f26fb36d6ddcd16bb8c},
	abstract = {Oil and gas activities generate, inevitably, huge amounts of data, especially in the form of Daily Operational Reports during construction and production activities. Extracting information from these reports can be achieved using embeddings combined with Large Language Models (LLMs), with domain knowledge being mandatory for this extracted information to be meaningful and properly presented to end-users. A solution for this challenge is the use of Knowledge Graphs (KG) to organize and relate all the information in a way that properly represents the concepts (e.g., entities, events, and processes) of the specific domain. A KG can address the lack of domain knowledge presented by LLMs, as well as enhance the semantics of embedding-based indexes such as those provided by Transformers, but also as a searching source itself that enables to take advantage of the information relationships (i.e., the natural links between entities or pieces of data). However, the creation of a KG from a specific domain based on unstructured data sources requires not only an ontology to describe the entities and their relationships but also strategies and tools capable of properly extracting and mapping them from the data sources to a KG. In this context, this work proposes a well engineering ontology to guide extracting information from Daily Operational Reports, structure them into KGs, and support information retrieval applications. To assure interoperability, the development of the proposed ontology is aligned with upper-level ontologies and industrial standards like ISO 15926. The results of this work include not only the ontology itself and the procedure followed to build it, but also a discussion of further applications that can be developed using it as a basis. The main applications resulting from this work are named entity recognition, visualization of incidents, classification of operations, templates and auto-filling of reports, calculation of drilling metrics, and queries within the reports database. We have noticed that having structured domain knowledge, in the form of an ontology and KG has improved the success of these applications compared to raw text processing. Finally, this work emphasizes the importance of domain knowledge in the form of a comprehensive ontology that focuses on the relevant entities of the application domain and enables the creation of KGs that can facilitate the organization and query of complex unstructured text information and enhance data analysis and LLM-based applications in the Oil&Gas domain. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Engineering Exhibitions; Exploratory Oil Well Drilling; Iso Standards; Liquefied Petroleum Gas; Metadata; Query Languages; Steganography; Structured Query Language; Data-source; Domain Knowledge; Embeddings; Extracting Information; Knowledge Graphs; Language Model; Oil And Gas Activities; Ontology's; Text Data; Well Engineering; Ontology},
	keywords = {Engineering exhibitions; Exploratory oil well drilling; ISO Standards; Liquefied petroleum gas; Metadata; Query languages; Steganography; Structured Query Language; Data-source; Domain knowledge; Embeddings; Extracting information; Knowledge graphs; Language model; Oil and gas activities; Ontology's; Text data; Well engineering; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025,
	author = {Li, Haotian and Xia, Congming Min and Hou, Youjuan and Hu, Sile and Liu, Yanjun and Jiang, Quan},
	title = {TCMRD - KG: innovative design and development of rheumatology knowledge graph in ancient Chinese literature assisted by large language models},
	year = {2025},
	journal = {Frontiers in Pharmacology},
	volume = {16},
	pages = {},
	doi = {10.3389/fphar.2025.1535596},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000082504&doi=10.3389%2Ffphar.2025.1535596&partnerID=40&md5=4886be4fb77890b3158d1ebee24f7c0a},
	abstract = {Introduction: Rheumatic immune diseases are a type of immune-inflammatory disease that affects muscles, bones, joints, and surrounding soft tissues. They have a long course and a high disability rate, seriously affecting the quality of life of patients. Traditional Chinese medicine plays an important role in the diagnosis and treatment of rheumatic immune diseases. The unique theoretical system and rich treatment methods of traditional Chinese medicine are preserved in ancient Chinese medical books. Methods: This study takes the content related to rheumatism in ancient traditional Chinese medicine books as the research object, integrates ontology theory and technology into the knowledge graph, and realizes the reconstruction of traditional Chinese medicine information knowledge. It provides a basic data structure for data mining and knowledge discovery. Results: This study is the first rheumatism-specific knowledge graph constructed based on ancient traditional Chinese medicine books. It has explored the construction method of a knowledge graph from ancient books by combining automatic labeling of mainstream large language models with manual review. Considering the knowledge characteristics of ancient traditional Chinese medicine books, where existing word segmentation technology struggles to accurately reproduce the original meaning, a new type of entity extraction method is proposed. Discussion: This provides an important foundation for improving the clinical diagnosis and treatment level of traditional Chinese medicine in treating rheumatism, further exploring the knowledge representation and application of traditional Chinese medicine in rheumatism treatment, and it has potential for future expansion and improvement. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artifcial Intelligence; Data Managament; Knowledge Graph (kg); Llms; Rheumatic; Trad. Chinese Medicine (tcm); Acupuncture; Article; Chatgpt; Chinese; Chinese Medicine; Data Extraction; Data Mining; Development; Human; Inflammatory Disease; Knowledge; Knowledge Discovery; Large Language Model; Literature; Prescription; Quality Of Life; Rheumatic Disease; Rheumatology},
	keywords = {acupuncture; Article; ChatGPT; Chinese; Chinese medicine; data extraction; data mining; development; human; inflammatory disease; knowledge; knowledge discovery; large language model; literature; prescription; quality of life; rheumatic disease; rheumatology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@CONFERENCE{Hauna2025750,
	author = {Hauna, Asyafa Ditra Al and Yunus, Andi Prademon and Khomsah, Siti and Choo, Yit Hong and Fukui, Masanori},
	title = {Role-Play Prediction using Ontology-Based Graph Convolutional Network Model},
	year = {2025},
	journal = {Proceedings of International Conference on Artificial Life and Robotics},
	pages = {750 - 754},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219510809&partnerID=40&md5=78a92e8593154d99e860d2012f455a8f},
	abstract = {Current applications of large language models often assign tasks without consideration of how LLMs understand a given prompt. Simple commands sometimes do not guarantee desired responses, as LLMs are systems based on mathematical modeling and cannot cognitively be capable of understanding commands. Hence, a method is required to guide LLMs in performing tasks appropriately. This paper presents a method to develop model-based automation of role selection supported by ontology. This can allow for more accurate and relevant role recommendations than if done manually. As such, this optimization at hand improves the performance of LLMs for specific tasks and overcomes the limitations of previous studies that define the roles by hand. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Convolutional Network; Large Language Models; Ontology; Role-play},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Schmeyer202523,
	author = {Schmeyer, Thomas Achim and Krämer, Kai and Peh, Anna Lena and Brandherm, Boris and Chikobava, Margarita and Kiefer, Gian Lucca},
	title = {Digital Twin Data Broker with Assisted Mapping into a Knowledge Base},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2372 CCIS},
	pages = {23 - 40},
	doi = {10.1007/978-3-031-80760-2_2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219206571&doi=10.1007%2F978-3-031-80760-2_2&partnerID=40&md5=67e9d38f40a7a7803f0e99c170c299c2},
	abstract = {The frequent usage of digital twins to communicate between physical objects is resulting in more complex cyber-physical systems. To simplify the individual components’ integration and to optimize their usage, a data broker is being developed. Therefore, digital twins need to be semantically organized in an ontology that provides the advantage of reasoning methods. An assisted workflow is being developed to automatically enter subgraphs into an ontology. As a digital twin representation, the Asset Administration Shell format is used to have an international standard technology. Based on this, a new domain-specific language is developed, allowing experts to configure the generation process. This process maps the digital twin’s information into a graph representation of the ontology. The preconfigured generation process enables the user to efficiently register new digital twins without having expert knowledge of the underlying ontology. Additionally, a Large Language Model vector embedding and text reasoning support is implemented analysing the digital twin to create entity suggestions. The presented data broker is an automation tool for bridging the gap between semantic descriptions and digital twin formats in order to unite the advantages of both representations. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Asset Administration Shell; Automation; Cyber-physical System; Digital Twin; Ontology; Knowledge Based Systems; Modeling Languages; Asset Administration Shell; Components Integration; Cybe-physical Systems; Cyber-physical Systems; Data Broker; Generation Process; Individual Components; Ontology's; Physical Objects; Reasoning Methods; Ontology},
	keywords = {Knowledge based systems; Modeling languages; Asset administration shell; Components integration; Cybe-physical systems; Cyber-physical systems; Data broker; Generation process; Individual components; Ontology's; Physical objects; Reasoning methods; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bruno2025,
	author = {Bruno, Alessandro and Pipitone, Arianna and Manzotti, Riccardo and Augello, Agnese and Mazzeo, Pier Luigi and Mazzola, Giuseppe and Vella, Filippo},
	title = {AIxPAC 2024-Preface to the 2nd Workshop on Artificial Intelligence for Perception and Artificial Consciousness},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3923},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219182367&partnerID=40&md5=ff97941dd87c6b33f32401e0abbd49a0},
	abstract = {The AIxPAC workshop seeks to unite academic and industry researchers to explore the latest developments in AI related to perception and consciousness. The event includes expert presentations on topics such as the physicalist ontology of consciousness, artificial consciousness, color perception, and computer vision. Key research questions addressed at AIxPAC include: Is it possible to integrate visual perception systems into machines? How effectively does AI handle visual attention processes? What is the connection between attention and consciousness? Can AI architectures and methodologies be leveraged to create Artificial Consciousness? What are the advantages and disadvantages of Large Language Models? These research questions encourage multidisciplinary collaboration and provide a foundation for critical analysis of the discussed topics © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Consciousness; Color Computers; Colour Perception; Language Model; Latest Development; Ontology's; Perception Systems; Research Questions; Visual Attention; Visual Perception; Ontology},
	keywords = {Artificial consciousness; Color computers; Colour perception; Language model; Latest development; Ontology's; Perception systems; Research questions; Visual Attention; Visual perception; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Labala2025431,
	author = {Labala, Rajendra K. and Khan, Zeeshan Ahmad and Mondal, Gopinath and Hazra, Subhajit and Banerjee, Bidisha and Dan, Abhijit and Chattoraj, Asamanja},
	title = {Transcriptomic dysregulation in zebrafish brain exposed to artificial light at night: a comprehensive analysis of gene expression and pathway alterations},
	year = {2025},
	journal = {Biological Rhythm Research},
	volume = {56},
	number = {6},
	pages = {431 - 450},
	doi = {10.1080/09291016.2025.2464687},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219151203&doi=10.1080%2F09291016.2025.2464687&partnerID=40&md5=757fc35af6cbc30ba83109ec3231154d},
	abstract = {Artificial light at night (ALAN) is a known stressor, yet its impact on brain transcriptomes remains poorly understood. We analyzed zebrafish brain RNA-sequencing data after ALAN exposure for 1 week (LLW), 1 month (LLM), and 1 year (LLY). Gene expression patterns shifted progressively, with 558, 664, and 671 differentially expressed genes (DEGs) in LLW, LLM, and LLY, respectively. Pathway analysis revealed altered protein synthesis, signaling, and extracellular matrix interactions. Ribosomal pathway upregulation suggested heightened metabolic demands, while signaling pathway downregulation implicated reduced cellular response to environmental stressors. Gene ontology analysis elucidated changes in sensory perception, nervous system processes, and immune responses. Network analysis identified hub genes associated with motor functions and cellular mobility. Disease enrichment analysis revealed associations of ALAN-induced DEGs with various pathologies. These findings emphasize the need for further research on brain function and strategies to mitigate ALAN’s harmful effects on ecosystems and health. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Alan; Brain; Circadian Rhythm; Pathways; Transcriptome; Ghrelin; Melatonin; Rna; Ghrelin; Leptin; Melatonin; Messenger Rna; Rna; Transcription Factor Clock; Transcriptome; Brain; Cell Component; Circadian Rhythm; Gene Expression; Genetic Analysis; Light Intensity; Perception; Sensory System; Article; Artificial Light; Brain Function; Cluster Analysis; Differential Gene Expression; Disease Association; Dna Base Composition; Down Regulation; Environmental Exposure; Exploratory Research; Extracellular Matrix; Gene Ontology; Gene Set Enrichment Analysis; Immune Response; Kegg; Light; Motor Performance; Nervous System; Network Analysis; Night; Nonhuman; Oxidative Phosphorylation; Pathway Analysis; Pathway Enrichment Analysis; Phototransduction; Protein Synthesis; Qualitative Analysis; Quantitative Analysis; Ribosome; Rna Sequencing; Sensation; Signal Transduction; Transcriptome Sequencing; Transcriptomics; Upregulation; Zebra Fish},
	keywords = {ghrelin; leptin; melatonin; messenger RNA; RNA; transcription factor CLOCK; transcriptome; brain; cell component; circadian rhythm; gene expression; genetic analysis; light intensity; perception; sensory system; Article; artificial light; brain function; cluster analysis; differential gene expression; disease association; DNA base composition; down regulation; environmental exposure; exploratory research; extracellular matrix; gene ontology; gene set enrichment analysis; immune response; KEGG; light; motor performance; nervous system; network analysis; night; nonhuman; oxidative phosphorylation; pathway analysis; pathway enrichment analysis; phototransduction; protein synthesis; qualitative analysis; quantitative analysis; ribosome; RNA sequencing; sensation; signal transduction; transcriptome sequencing; transcriptomics; upregulation; zebra fish},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Stütz2025169,
	author = {Stütz, Jan David and Karras, Oliver and Oelen, Allard and Auer, Sören},
	title = {A User-Driven Hybrid Neuro-Symbolic Approach for Knowledge Graph Creation from Relational Data},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15506 LNCS},
	pages = {169 - 185},
	doi = {10.1007/978-3-031-81375-7_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218937379&doi=10.1007%2F978-3-031-81375-7_10&partnerID=40&md5=77580e9d282408b3c7b1d26571055123},
	abstract = {In all kinds of organizations, relational data is prevalent and ubiquitous in a plethora of systems. However, the integration and exchange of such data is cumbersome, time-consuming, and error-prone. Semantic technologies, such as ontologies, KGs, and linked data, were developed to facilitate this but require comprehensive technical skills and complex methods for mapping relational data to semantic formalisms. Naturally, this process lacks speed, scalability, and automation. This work presents a novel user-driven neuro-symbolic approach to transform relational data into KGs. In our approach, users are supported by neural models (in particular Large Language Models) and symbolic formalisms (ontologies and mappings) to automate various mapping tasks and thus speed up and scale up the transformation from relational to linked data. We implemented our approach in a comprehensive intelligent assistant dubbed LXS. Our experimental evaluation, conducted primarily with participants from the Robert Bosch GmbH, demonstrates enhanced mapping quality compared to manual creation, a competitive application, and AI-only generations. Additionally, it significantly reduces user interaction time by nearly half, independent of the user’s experience level. Also, qualitatively, users appreciated the attractiveness and novelty of the user interface. Furthermore, the neuro-symbolic approach of LXS contributes to a more trustworthy human-AI interaction since it keeps users in the loop and provides transparency in the transformation process. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Hci; Knowledge Graph Creation; Neuro-symbolic; Graphical User Interfaces; Knowledge Graph; User Profile; Complex Methods; Error Prones; Knowledge Graph Creation; Knowledge Graphs; Neuro-symbolic; Ontology's; Relational Data; Semantic Technologies; Technical Skills; User Driven; Mapping},
	keywords = {Graphical user interfaces; Knowledge graph; User profile; Complex methods; Error prones; Knowledge graph creation; Knowledge graphs; Neuro-symbolic; Ontology's; Relational data; Semantic technologies; Technical skills; User driven; Mapping},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Arevalo2025225,
	author = {Arevalo, Kiara Marnitt Ascencion and Ambre, Shruti and Dorsch, Rene},
	title = {AutOnto: Towards A Semi-Automated Ontology Engineering Methodology},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15459 LNCS},
	pages = {225 - 241},
	doi = {10.1007/978-3-031-81221-7_16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218935432&doi=10.1007%2F978-3-031-81221-7_16&partnerID=40&md5=ca4a284fddf4e697c1b7573d812ac843},
	abstract = {This paper addresses the challenge of efficiently constructing domain ontologies for large, rapidly evolving domains, where manual approaches often struggle to overcome knowledge acquisition bottlenecks. To overcome these limitations, we developed an automated framework, AutOnto, for knowledge extraction and ontology conceptualization that leverages Large Language Models (LLMs) and natural language processing (NLP) techniques. AutOnto integrates BERT-based topic modeling with LLMs to automate the extraction of concepts and relationships from text corpora, facilitating the construction of taxonomies and the generation of domain ontologies. We applied AutOnto to a dataset of NLP-specific articles from OpenAlex and compared the resulting ontology generated by our automated process against a well-established gold-standard ontology. The results indicate that AutOnto achieves comparable levels of quality and correctness while significantly reducing the amount of data required and the dependence on domain-specific expertise. These findings highlight AutOnto’s efficiency and effectiveness in knowledge extraction and ontology generation. This work has significant implications for rapid ontology development in large, evolving domains, potentially mitigating the knowledge acquisition bottleneck in ontology engineering. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Natural Language Processing; Ontology Engineering; Domain Knowledge; Modeling Languages; Natural Language Processing Systems; Ontology; Taxonomies; Domain Ontologies; Knowledge Acquisition Bottlenecks; Knowledge Extraction; Knowledge Ontology; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Ontology Engineering; Knowledge Acquisition},
	keywords = {Domain Knowledge; Modeling languages; Natural language processing systems; Ontology; Taxonomies; Domain ontologies; Knowledge acquisition bottlenecks; Knowledge extraction; Knowledge ontology; Language model; Language processing; Large language model; Natural language processing; Natural languages; Ontology engineering; Knowledge acquisition},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Amini202517,
	author = {Amini, Reihaneh and Norouzi, Sanaz Saki and Hitzler, Pascal Al and Amini, Reza},
	title = {Towards Complex Ontology Alignment Using Large Language Models},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15459 LNCS},
	pages = {17 - 31},
	doi = {10.1007/978-3-031-81221-7_2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218934218&doi=10.1007%2F978-3-031-81221-7_2&partnerID=40&md5=0ceb4a7e31a8a179e3d1efda57c8567e},
	abstract = {Ontology alignment, a critical process in the Semantic Web for detecting relationships between different ontologies, has traditionally focused on identifying so-called “simple” 1-to-1 relationships through class labels and properties comparison. The more practically useful exploration of more complex alignments remains a hard problem to automate, and as such is largely underexplored, i.e. in application practice it is usually done manually by ontology and domain experts. Recently, the surge in Natural Language Processing (NLP) capabilities, driven by advancements in Large Language Models (LLMs), presents new opportunities for enhancing ontology engineering practices, including ontology alignment tasks. This paper investigates the application of LLM technologies to tackle the complex ontology alignment challenge. Leveraging a prompt-based approach and integrating rich ontology content – so-called modules – our work constitutes a significant advance towards automating the complex alignment task. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Complex Ontology Alignment; Knowledge Graph; Large Language Model; Modular Ontology Modeling; Ontology; Knowledge Graph; Natural Language Processing Systems; Ontology; Semantics; Complex Ontology Alignment; Knowledge Graphs; Language Model; Large Language Model; Modular Ontologies; Modular Ontology Modeling; Ontology Alignment; Ontology Model; Ontology's; Semantic-web; Modeling Languages},
	keywords = {Knowledge graph; Natural language processing systems; Ontology; Semantics; Complex ontology alignment; Knowledge graphs; Language model; Large language model; Modular ontologies; Modular ontology modeling; Ontology alignment; Ontology model; Ontology's; Semantic-Web; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Khorashadizadeh2025334,
	author = {Khorashadizadeh, Hanieh and Mihindukulasooriya, Nandana and Ranji, Nilufar and Ezzabady, Morteza Kamaladdini and Ieng, Frédéric and Groppe, Jinghua and Benamara, Farah and Groppe, Sven Thilo},
	title = {Construction and Canonicalization of Economic Knowledge Graphs with LLMs},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15459 LNCS},
	pages = {334 - 343},
	doi = {10.1007/978-3-031-81221-7_23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218928651&doi=10.1007%2F978-3-031-81221-7_23&partnerID=40&md5=94c3a938e4ac09f1e4a9b680f71629dc},
	abstract = {Ontology-based knowledge graphs, such as YAGO and Wikidata, rely on pre-defined schemas to organize and connect information. While effective, these systems are inherently domain-specific, requiring tailored ontologies that are costly, time-consuming, and demand expert knowledge to develop. To address these limitations, Open Information Extraction (OpenIE) offers a complementary approach by extracting structured information directly from unstructured text without needing a predefined schema. However, OpenIE results in a vast number of relations, often leading to redundancy and inconsistencies. To overcome this, we propose a novel approach that leverages Large Language Models (LLMs) for constructing a knowledge graph and for canonicalizing relations within it. Our method includes generating question-answer pairs from text, extracting triples from these pairs, and applying a two-step canonicalization process to ensure consistency and reduce redundancy. This paper presents our approach in detail, exploring related work, the construction of the knowledge graph, the canonicalization process, and the evaluation of our methods. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Canonicalization; Knowledge Graph; Large Language Model; Open Information Extraction; Ontology; Canonicalization; Domain Specific; Economic Knowledge; Expert Knowledge; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Ontology-based; Open Information Extraction; Knowledge Graph},
	keywords = {Ontology; Canonicalization; Domain specific; Economic knowledge; Expert knowledge; Knowledge graphs; Language model; Large language model; Ontology's; Ontology-based; Open information extraction; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{2025,
	title = {6th International Conference on Knowledge Graphs and Semantic Web, KGSWC 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15459 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218915405&partnerID=40&md5=120584314eae6d1b122ba3e9758913d8},
	abstract = {The proceedings contain 23 papers. The special focus in this conference is on Knowledge Graphs and Semantic Web. The topics include: Towards Complex Ontology Alignment Using Large Language Models; enhancing Knowledge Graph Construction: Evaluating with Emphasis on Hallucination, Omission, and Graph Similarity Metrics; views, Semantic Data Catalog and Role-Based Access Control for Ontologies and Knowledge Graphs; accelerating Medical Knowledge Discovery Through Automated Knowledge Graph Generation and Enrichment; ontoSeer - A Recommendation System to Improve the Quality of Ontologies; Adaptive Planning on the Web: Using LLMs and Affordances for Web Agents; Enriching RDF Data with LLM Based Named Entity Recognition and Linking on Embedded Natural Language Annotations; TEDME-KG Metrics Framework: A Metrics Framework for TEmporal Data Modelling Evaluation in Knowledge Graphs; SFARDE: A Knowledge-Centric Semantic Strategic Framework for Heritage Artifact Recommendation Integrating Generative AI and Differential Enrichment of Ontologies; leveraging Graph Models for Comprehensive Visual Analytics of Equine Heritage; OWL2Vec4OA: Tailoring Knowledge Graph Embeddings for Ontology Alignment; Design of an Ontology-Driven Constraint Tester (ODCT) and Application to SAREF and Smart Energy Appliances; YOKO ONtO: You only KNIT One Ontology; CoKGLM: Detecting Hallucinations Generated by Large Language Models via Knowledge Graph Verification; autOnto: Towards A Semi-Automated Ontology Engineering Methodology; Enhancing Question Answering Systems with Generative AI: A Study of LLM Performance and Error Analysis; disjointness Violations in Wikidata; enhancing WebProtégé with Version Control Systems; visual Presentation and Summarization of Linked Data Schemas; a Proposed Ontology Evaluation Tool to Assist Ontology Engineers in Selecting Ontologies During the Reuse Phase; manufacturing Commonsense Knowledge. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {European Semantic Web Conference, ESWC 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218675521&partnerID=40&md5=3fae09c5dfa9e1a714b11558c089562c},
	abstract = {The proceedings contain 75 papers. The special focus in this conference is on Semantic Web. The topics include: LLMs4OM: Matching Ontologies with Large Language Models; NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning; Assessing the Evolution of LLM Capabilities for Knowledge Graph Engineering in 2023; column Property Annotation Using Large Language Models; Can LLMs Generate Competency Questions?; 12 Shades of RDF: Impact of Syntaxes on Data Extraction with Language Models; validating Semantic Artifacts with Large Language Models; ontoChat: A Framework for Conversational Ontology Engineering Using Language Models; dataset Management Powered by Semantic Web Technologies; Optimizing Aerospace Product Maintenance: A Novel Multi-Modal Knowledge Graph and LLM Approach for Enhanced Decision Support; LLM-Based Guided Generation of Ontology Term Definitions; towards Solid-Based B2B Data Value Chains; Rapid Graph Generation from Job Descriptions: Combining Taxonomies and LLMs; FAIR Internet of Things Data: Enabling Process Optimization at Munich Airport; product Information Management Systems Powered by Knowledge Graphs; artSampo – Finnish Art on the Semantic Web; towards Semantic Annotation for Scientific Datasets; a Framework for Question Answering on Knowledge Graphs Using Large Language Models; KinGVisher – Knowledge Graph Visualizer; gotta Catch’em All: From Data Silos to a Knowledge Graph; The Helmholtz Knowledge Graph: Driving the Transition Towards a FAIR Data Ecosystem in the Helmholtz Association; Data Search and Discovery in RDF Sources; MLSeascape: Search over Machine Learning Metadata Empowered by Knowledge Graphs; SCOOP-UI: SHACL Shape Extraction in Just a Click!; converter: Enhancing Interoperability in Research Data Management; RDFminer: An Interactive Tool for the Evolutionary Discovery of SHACL Shapes; MusicBO, an Application of Text2AMR2FRED to the Musical Heritage Domain; RDF2vec Embeddings for Updateable Knowledge Graphs – Reuse, Don’t Retrain!; critical Path Identification in Supply Chain Knowledge Graphs with Large Language Models; observations on Bloom Filters for Traversal-Based Query Execution over Solid Pods; PySPARQL Anything Showcase. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {European Semantic Web Conference, ESWC 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15345 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218674386&partnerID=40&md5=aafca631054e13f7197021bbd357054f},
	abstract = {The proceedings contain 75 papers. The special focus in this conference is on Semantic Web. The topics include: LLMs4OM: Matching Ontologies with Large Language Models; NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning; Assessing the Evolution of LLM Capabilities for Knowledge Graph Engineering in 2023; column Property Annotation Using Large Language Models; Can LLMs Generate Competency Questions?; 12 Shades of RDF: Impact of Syntaxes on Data Extraction with Language Models; validating Semantic Artifacts with Large Language Models; ontoChat: A Framework for Conversational Ontology Engineering Using Language Models; dataset Management Powered by Semantic Web Technologies; Optimizing Aerospace Product Maintenance: A Novel Multi-Modal Knowledge Graph and LLM Approach for Enhanced Decision Support; LLM-Based Guided Generation of Ontology Term Definitions; towards Solid-Based B2B Data Value Chains; Rapid Graph Generation from Job Descriptions: Combining Taxonomies and LLMs; FAIR Internet of Things Data: Enabling Process Optimization at Munich Airport; product Information Management Systems Powered by Knowledge Graphs; artSampo – Finnish Art on the Semantic Web; towards Semantic Annotation for Scientific Datasets; a Framework for Question Answering on Knowledge Graphs Using Large Language Models; KinGVisher – Knowledge Graph Visualizer; gotta Catch’em All: From Data Silos to a Knowledge Graph; The Helmholtz Knowledge Graph: Driving the Transition Towards a FAIR Data Ecosystem in the Helmholtz Association; Data Search and Discovery in RDF Sources; MLSeascape: Search over Machine Learning Metadata Empowered by Knowledge Graphs; SCOOP-UI: SHACL Shape Extraction in Just a Click!; converter: Enhancing Interoperability in Research Data Management; RDFminer: An Interactive Tool for the Evolutionary Discovery of SHACL Shapes; MusicBO, an Application of Text2AMR2FRED to the Musical Heritage Domain; RDF2vec Embeddings for Updateable Knowledge Graphs – Reuse, Don’t Retrain!; critical Path Identification in Supply Chain Knowledge Graphs with Large Language Models; observations on Bloom Filters for Traversal-Based Query Execution over Solid Pods; PySPARQL Anything Showcase. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Rebboud202571,
	author = {Rebboud, Youssra and Tailhardat, Lionel and Lisena, Pasquale and Troncy, Raphaël},
	title = {Can LLMs Generate Competency Questions?},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {71 - 80},
	doi = {10.1007/978-3-031-78952-6_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218474145&doi=10.1007%2F978-3-031-78952-6_7&partnerID=40&md5=4f9c4d533bce3706de5a09e47bb22cb3},
	abstract = {Large Language Models have shown high performances in a large number of tasks, being recently applied also to support Knowledge Graphs construction. An important step for data modeling consists in the definition of a set of competency questions, which are often used as a guide for the development of an ontology and as a mean to evaluate the resulting schema. In this work, we investigate the suitability of LLMs for the automatic generation of competency questions given an existing ontology. We compare different large language models under various settings in order to give a comprehensive overview of what LLMs can do to support the knowledge engineer. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Modeling; Knowledge Graphs; Llms; Ontology; Modeling Languages; Ontology; Automatic Generation; Data Modeling; Graph Construction; Knowledge Graphs; Language Model; Llm; Ontology's; Performance; Support Knowledge; Knowledge Graph},
	keywords = {Modeling languages; Ontology; Automatic Generation; Data modeling; Graph construction; Knowledge graphs; Language model; LLM; Ontology's; Performance; Support knowledge; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Fathallah202536,
	author = {Fathallah, Nadeen and Das, Arunav and de Giorgis, Stefano and Poltronieri, Andrea and Haase, Peter and Kovriguina, Liubov},
	title = {NeOn-GPT: A Large Language Model-Powered Pipeline for Ontology Learning},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {36 - 50},
	doi = {10.1007/978-3-031-78952-6_4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218469990&doi=10.1007%2F978-3-031-78952-6_4&partnerID=40&md5=071c0cc79dd4a7986f0497d28a7e43be},
	abstract = {We address the task of ontology learning by combining the structured NeOn methodology framework with Large Language Models (LLMs) for translating natural language domain descriptions into Turtle syntax ontologies. The main contribution of the paper is a prompt pipeline tailored for domain-agnostic modeling, exemplified through the application to a domain-specific case study: the wine ontology. The resulting pipeline is used to develop NeOn-GPT, a workflow for automatic ontology modeling, and its proof of concept implementation, integrated on top of the metaphactory platform. NeOn-GPT leverages the systematic approach of the NeOn methodology and LLMs’ generative capabilities to facilitate a more efficient ontology development process. We evaluate the proposed approach by conducting comprehensive evaluations using the Stanford wine ontology as the gold standard. The obtained results show, that LLMs are not fully equipped to perform procedural tasks required for ontology development, and lack the reasoning skills and domain expertise needed. Overall, LLMs require integration with the workflow or trajectory tools for continuous knowledge engineering tasks. Nevertheless, LLMs can significantly alleviate the time and expertise needed. Our code base is publicly available for research and development purposes, accessible at: https://github.com/andreamust/NEON-GPT. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Neon Methodology; Ontology Modelling; Information Management; Knowledge Engineering; Natural Language Processing Systems; Syntactics; Domain Description; Language Model; Large Language Model; Natural Languages; Neon Methodology; Ontology Development; Ontology Learning; Ontology Model; Ontology's; Work-flows; Ontology},
	keywords = {Information management; Knowledge engineering; Natural language processing systems; Syntactics; Domain description; Language model; Large language model; Natural languages; Neon methodology; Ontology development; Ontology learning; Ontology model; Ontology's; Work-flows; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Han2025335,
	author = {Han, Ziwei and Wang, Hui and Li, Yaxin and Xu, Fangzhou},
	title = {Construction of EMU Fault Knowledge Graph Based on Large Language Model},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {14667 LNCS},
	pages = {335 - 347},
	doi = {10.1007/978-981-96-0914-7_26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218467029&doi=10.1007%2F978-981-96-0914-7_26&partnerID=40&md5=a066fa4152fe7eaeb8ed3a7c76c14ece},
	abstract = {During the operation of the EMU, a large number of fault data will be generated, which record the fault overview, fault cause, treatment measures and other related fault information. Construction of knowledge graph of these data can provide knowledge for relevant staff in the maintenance process and assist them in decision-making, which is of great practical significance. Most of the existing EMU fault knowledge graph construction methods adopt supervised named entity recognition algorithms and relation extraction algorithms, which require a large amount of labeled data for training. To solve the above problems, a method for constructing EMU fault knowledge graph based on large language model is proposed. Firstly, the EMU fault ontology model is constructed based on the existing EMU fault data and drawing on the existing fault knowledge graph. Then the EMU fault triple is extracted by means of small samples using the in-context learning capability of the large language model. Finally, entity linking is performed based on the constructed fault ontology model to construct the EMU fault knowledge graph, and the graph is displayed in a visual way to guide the maintenance and diagnosis of EMU faults. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Emu Fault; Knowledge Graph; Large Language Model; Ontology Model; Emu Fault; Fault Data; Faults Information; Graph-based; Knowledge Graphs; Language Model; Large Language Model; Maintenance Process; Ontology Model; Treatment Measures; Knowledge Graph},
	keywords = {EMU fault; Fault data; Faults information; Graph-based; Knowledge graphs; Language model; Large language model; Maintenance process; Ontology model; Treatment measures; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hirano2025211,
	author = {Hirano, Tsukasa and Shoji, Yoshiyuki and Yamamoto, Takehiro and Ohara, Kouzou},
	title = {Generating Achievement Relationship Graph Between Actions for Alternative Solution Recommendation},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {14667 LNCS},
	pages = {211 - 219},
	doi = {10.1007/978-981-96-0914-7_15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218461905&doi=10.1007%2F978-981-96-0914-7_15&partnerID=40&md5=498bd4da786c7f4ee98097f4b4220e61},
	abstract = {This paper proposes a method for generating a graph of the purpose and alternative actions for a given specific action name. For instance, the purpose of “concentrating on the lecture” can be achieved by taking actions such as “preventing sleepiness,” “taking sugar,” or “changing the lecture.” In this case, the actions “concentrating on the lecture” and “preventing sleepiness” have a means-end (i.e., achieved-by) relationship, and “preventing sleepiness” and “taking sugar” have substitutable relationships. Our method presents these achievable and substitutable actions as a graph to give people various choices and help them make decisions. The proposed method extracts descriptions of actions from product review data using a large-scale language model and assigns action names. The relationship between action names is calculated based on graph computation and language patterns in web documents. The results of subject experiments show that the proposed method can find many pairs of action names in an achievement relationship. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Entailment; Ontology Construction; Purpose And Means; Modeling Languages; Sleep Research; Alternative Solutions; Entailment; Language Model; Language Patterns; Large-scales; Means Ends; Ontology Construction; Product Reviews; Purpose And Mean; Relationship Graphs; Ontology},
	keywords = {Modeling languages; Sleep research; Alternative solutions; Entailment; Language model; Language patterns; Large-scales; Means ends; Ontology construction; Product reviews; Purpose and mean; Relationship graphs; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hertling2025239,
	author = {Hertling, Sven and Norouzi, Ebrahim and Sack, Harald},
	title = {OAEI Machine Learning Dataset for Online Model Generation},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {239 - 243},
	doi = {10.1007/978-3-031-78952-6_34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218459910&doi=10.1007%2F978-3-031-78952-6_34&partnerID=40&md5=5b80c994decba989f88eb8b7ad8fedd2},
	abstract = {Ontology and knowledge graph matching systems are evaluated annually by the Ontology Alignment Evaluation Initiative (OAEI). More and more systems use machine learning-based approaches, including large language models. The training and validation datasets are usually determined by the system developer and often a subset of the reference alignments are used. This sampling is against the OAEI rules and makes a fair comparison impossible. Furthermore, those models are trained offline (a trained and optimized model is packaged into the matcher) and therefore the systems are specifically trained for those tasks. In this paper, we introduce a dataset that contains training, validation, and test sets for most of the OAEI tracks. Thus, online model learning (the systems must adapt to the given input alignment without human intervention) is made possible to enable a fair comparison for ML-based systems. We showcase the usefulness of the dataset by fine-tuning the confidence thresholds of popular systems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Machine Learning; Online Model Generation; Ontology Matching; Contrastive Learning; Federated Learning; Knowledge Graph; Graph Matching Systems; Knowledge Graphs; Learning Dataset; Machine-learning; Model Generation; On-line Modelling; Online Model Generation; Ontology Alignment; Ontology Graphs; Ontology Matching; Adversarial Machine Learning},
	keywords = {Contrastive Learning; Federated learning; Knowledge graph; Graph matching systems; Knowledge graphs; Learning dataset; Machine-learning; Model generation; On-line modelling; Online model generation; Ontology alignment; Ontology graphs; Ontology matching; Adversarial machine learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang2025102,
	author = {Zhang, Bohui and Carriero, Valentina Anita and Schreiberhuber, Katrin and Tsaneva, Stefani and González, Lucía Sánchez and Kim, Jongmo and de Berardinis, Jacopo},
	title = {OntoChat: A Framework for Conversational Ontology Engineering Using Language Models},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {102 - 121},
	doi = {10.1007/978-3-031-78952-6_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218459527&doi=10.1007%2F978-3-031-78952-6_10&partnerID=40&md5=77776a3fa6293cb8bdfb5f1e88063c21},
	abstract = {Ontology engineering (OE) in large projects poses a number of challenges arising from the heterogeneous backgrounds of the various stakeholders, domain experts, and their complex interactions with ontology designers. This multi-party interaction often creates systematic ambiguities and biases from the elicitation of ontology requirements, which directly affect the design, evaluation and may jeopardise the target reuse. Meanwhile, current OE methodologies strongly rely on manual activities (e.g., interviews, discussion pages). After collecting evidence on the most crucial OE activities, we introduce OntoChat, a framework for conversational ontology engineering that supports requirement elicitation, analysis, and testing. By interacting with a conversational agent, users can steer the creation of user stories and the extraction of competency questions, while receiving computational support to analyse the overall requirements and test early versions of the resulting ontologies. We evaluate OntoChat by replicating the engineering of the Music Meta Ontology, and collecting preliminary metrics on the effectiveness of each component from users. We release all code at https://github.com/King-s-Knowledge-Graph-Lab/OntoChat. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Competency Questions; Computational Creativity; Large Language Models; Ontology Engineering; Ontology; Requirements Engineering; Competency Question; Computational Creativities; Design Evaluation; Domain Experts; Language Model; Large Language Model; Large Programs; Multiparty Interaction; Ontology Engineering; Ontology's; Chatbots},
	keywords = {Ontology; Requirements engineering; Competency question; Computational creativities; Design evaluation; Domain experts; Language model; Large language model; Large programs; Multiparty interaction; Ontology engineering; Ontology's; Chatbots},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Reforgiato Recupero2025306,
	author = {Reforgiato Recupero, Diego and Boi, Lorenzo},
	title = {Integrating Action Robot Ontology for Enhanced Human-Robot Interaction: A NAO Robot Case Study},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {306 - 310},
	doi = {10.1007/978-3-031-78952-6_47},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218457427&doi=10.1007%2F978-3-031-78952-6_47&partnerID=40&md5=97e1aaf1ce1c1e6114e29279e190fe8d},
	abstract = {This paper presents an approach that allows the NAO humanoid robot to respond to a question from a user and gesticulate depending on the text that it is saying. The question might also be an action command spoken by the user that the robot recognizes and executes. A Large Language Model is integrated within the approach to provide the question-answering capabilities. For the action commands, we have used an action robot ontology that we have defined in past work. We have extracted the pertinent classes and individuals and generated a three-word string for each action that is matched semantically with the user’s text. Moreover, as far as the action commands are concerned, the system can work in two modes: STATELESS and STATEFUL. When in STATEFUL mode, the robot knows its current posture and performs the command only if it is compatible with its current state. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Action Robot Ontology; Human-robot Interaction; Large Language Models; Natural Language Processing; Adversarial Machine Learning; Anthropomorphic Robots; Microrobots; Modeling Languages; Natural Language Processing Systems; Ontology; Semantics; 'current; Action Robot Ontology; Case-studies; Humans-robot Interactions; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Ontology's; Human Robot Interaction},
	keywords = {Adversarial machine learning; Anthropomorphic robots; Microrobots; Modeling languages; Natural language processing systems; Ontology; Semantics; 'current; Action robot ontology; Case-studies; Humans-robot interactions; Language model; Language processing; Large language model; Natural language processing; Natural languages; Ontology's; Human robot interaction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Giglou202525,
	author = {Giglou, Hamed Babaei and D’Souza, Jennifer and Engel, Felix C. and Auer, Sören},
	title = {LLMs4OM: Matching Ontologies with Large Language Models},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {25 - 35},
	doi = {10.1007/978-3-031-78952-6_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218455564&doi=10.1007%2F978-3-031-78952-6_3&partnerID=40&md5=c3c4db12909f8024518974717c695310},
	abstract = {Ontology Matching (OM), is a critical task in knowledge integration, where aligning heterogeneous ontologies facilitates data interoperability and knowledge sharing. Traditional OM systems often rely on expert knowledge or predictive models, with limited exploration of the potential of Large Language Models (LLMs). We present the LLMs4OM framework, a novel approach to evaluate the effectiveness of LLMs in OM tasks. This framework utilizes two modules for retrieval and matching, respectively, enhanced by zero-shot prompting across three ontology representations: concept, concept-parent, and concept-children. Through comprehensive evaluations using 20 OM datasets from various domains, we demonstrate that LLMs, under the LLMs4OM framework, can match and even surpass the performance of traditional OM systems, particularly in complex matching scenarios. Our results highlight the potential of LLMs to significantly contribute to the field of OM. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Ontology Alignment; Ontology Matching; Retrieval Augmented Generation; Zero-shot Testing; Expert Systems; Ontology; Critical Tasks; Language Model; Large Language Model; Matching System; Matchings; Ontology Alignment; Ontology Matching; Ontology's; Retrieval Augmented Generation; Zero-shot Testing; Zero-shot Learning},
	keywords = {Expert systems; Ontology; Critical tasks; Language model; Large language model; Matching system; Matchings; Ontology alignment; Ontology matching; Ontology's; Retrieval augmented generation; Zero-shot testing; Zero-shot learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Bischof2025133,
	author = {Bischof, Stefan and Filtz, Erwin and Parreira, Josiane Xavier and Steyskal, Simon},
	title = {LLM-Based Guided Generation of Ontology Term Definitions},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {133 - 137},
	doi = {10.1007/978-3-031-78952-6_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218454539&doi=10.1007%2F978-3-031-78952-6_13&partnerID=40&md5=1bb87d01dc10ca28ae22f25a611815f1},
	abstract = {This paper describes our approach for leveraging LLMs to generate definitions and descriptions for ontology terms. Our approach is grounded in the need for detailed and accurate representation of (domain-specific) Knowledge Graphs, and it aims at speeding up the process of generating such text. We outline our approach, including the problems that we encountered, and the solution we propose to overcome them. Our approach is currently in use in an industrial setting. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Ontology Engineering; Text Generation; Knowledge Graph; Domain-specific Knowledge; Industrial Settings; Knowledge Graphs; Knowledge It; Language Model; Large Language Model; Ontology Engineering; Ontology Terms; Text Generations; Ontology},
	keywords = {Knowledge graph; Domain-specific knowledge; Industrial settings; Knowledge graphs; Knowledge IT; Language model; Large language model; Ontology engineering; Ontology terms; Text generations; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lazzari2025238,
	author = {Lazzari, Nicolas and de Giorgis, Stefano and Gangemi, Aldo and Presutti, Valentina},
	title = {Explainable Moral Values: A Neuro-Symbolic Approach to Value Classification},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15345 LNCS},
	pages = {238 - 255},
	doi = {10.1007/978-3-031-78955-7_20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218453251&doi=10.1007%2F978-3-031-78955-7_20&partnerID=40&md5=eb6645f24958425aba8eb3054908e53d},
	abstract = {This work explores the integration of ontology-based reasoning and Machine Learning techniques for explainable value classification. By relying on an ontological formalization of moral values as in the Moral Foundations Theory, relying on the DnS Ontology Design Pattern, the sandra neuro-symbolic reasoner is used to infer values (fomalized as descriptions) that are satisfied by a certain sentence. Sentences, alongside their structured representation, are automatically generated using an open-source Large Language Model. The inferred descriptions are used to automatically detect the value associated with a sentence. We show that only relying on the reasoner’s inference results in explainable classification comparable to other more complex approaches. We show that combining the reasoner’s inferences with distributional semantics methods largely outperforms all the baselines, including complex models based on neural network architectures. Finally, we build a visualization tool to explore the potential of theory-based values classification, which is publicly available at http://xmv.geomeaning.com/. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Explainable Classification; Knowledge Representation; Moral Value Detection; Neuro-symbolic Reasoning; Synthetic Data; Ontology; Philosophical Aspects; Semantics; Based Reasonings; Explainable Classification; Knowledge-representation; Machine Learning Techniques; Moral Value Detection; Neuro-symbolic Reasoning; Ontology-based; Reasoners; Symbolic Reasoning; Synthetic Data; Knowledge Representation},
	keywords = {Ontology; Philosophical aspects; Semantics; Based reasonings; Explainable classification; Knowledge-representation; Machine learning techniques; Moral value detection; Neuro-symbolic reasoning; Ontology-based; Reasoners; Symbolic reasoning; Synthetic data; Knowledge representation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lippolis2025259,
	author = {Lippolis, Anna Sofia and Ceriani, Miguel and Zuppiroli, Sara and Nuzzolese, Andrea Giovanni},
	title = {Ontogenia: Ontology Generation with Metacognitive Prompting in Large Language Models},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {259 - 265},
	doi = {10.1007/978-3-031-78952-6_38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218451819&doi=10.1007%2F978-3-031-78952-6_38&partnerID=40&md5=45b240331711a18cc31af840ace6c730},
	abstract = {Recent advancements in Large Language Models (LLMs) have primarily focused on enhancing task-specific performances by experimenting with prompt design. Despite the proven effectiveness of Metacognitive Prompting (MP), its application in the field of ontology generation remains an uncharted territory. This study addresses this gap by exploring this prompting technique in supporting the ontology design process, particularly with GPT-4, where this strategy has demonstrated consistent superiority over conventional and more direct prompting methods in recent research. Our methodology, named Ontogenia, employs a gold-standard dataset of ontology competency questions translated into SPARQL-OWL queries. This approach allows us to explore various types and stages of knowledge refinement using MP, while adhering to the eXtreme Design methodology, a well-established protocol in ontology design. Finally, the quality and performance of the resulting ontologies are assessed using both standard ontology quality metrics and evaluation by an ontology expert. This research aims to enrich the discussion on methods of ontology generation driven by LLMs by presenting concrete results on the use of metacognitive prompting and ontology design patterns. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Competency Questions; Large Language Models; Metacognitive Prompting; Ontology Engineering; Modeling Languages; Competency Question; Language Model; Large Language Model; Metacognitive Prompting; Metacognitives; Ontology Design; Ontology Engineering; Ontology Generation; Ontology's; Performance; Ontology},
	keywords = {Modeling languages; Competency question; Language model; Large language model; Metacognitive prompting; Metacognitives; Ontology design; Ontology engineering; Ontology generation; Ontology's; Performance; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Hoseini2025183,
	author = {Hoseini, Sayed and Burgdorf, Andreas and Paulus, Alexander and Meisen, Tobias and Quix, Christoph and Pomp, Andre},
	title = {Challenges and Opportunities of LLM-Augmented Semantic Model Creation for Dataspaces},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15345 LNCS},
	pages = {183 - 200},
	doi = {10.1007/978-3-031-78955-7_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218450892&doi=10.1007%2F978-3-031-78955-7_17&partnerID=40&md5=ed5208c3b70fe60d292ed039a05ad875},
	abstract = {The objective of dataspaces is to facilitate seamless and reliable data exchange between different organizations. In Europe, their prominence has grown with the implementation of the European Data Governance Act. This legislation emphasizes the importance of trust, accessibility, and shared dataspaces, which necessitate semantic interoperability grounded in the FAIR principles. While semantic descriptions in the form of semantic models and ontologies are indispensable to dataspaces, their full potential remains unexploited. Meaningful metadata, including contextual information, enhances data usability, but the manual creation of semantic models can be challenging. Large Language Models (LLMs) offer a new way to utilize data in dataspaces. Their advanced natural language processing capabilities enable context-aware data processing and semantic understanding. This paper presents initial experiments on customizing and optimizing LLMs for semantic labeling and modeling tasks. The contributions of this work include research questions for future investigations, early experiments demonstrating the applicability of LLM for semantic labeling, and proposed directions to address discovered challenges. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Dataspace; Llms; Semantic Modeling; Data Assimilation; Data Centers; Data Governances; Data Space; Language Model; Large Language Model; Model Creation; Semantic Descriptions; Semantic Interoperability; Semantic Labeling; Semantic Modelling; Semantic Ontology; Latent Semantic Analysis},
	keywords = {Data assimilation; Data centers; Data governances; Data space; Language model; Large language model; Model creation; Semantic descriptions; Semantic interoperability; Semantic labeling; Semantic modelling; Semantic ontology; Latent semantic analysis},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Alharbi20253,
	author = {Alharbi, Reham and Tamma, Valentina A.M. and Grasso, Floriana and Payne, Terry R.},
	title = {The Role of Generative AI in Competency Question Retrofitting},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {3 - 13},
	doi = {10.1007/978-3-031-78952-6_1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218447447&doi=10.1007%2F978-3-031-78952-6_1&partnerID=40&md5=de488f84c86cb908b9bf333e648ed84e},
	abstract = {Competency Questions (CQs) are essential in ontology engineering; they express an ontology’s functional requirements as natural language questions, offer crucial insights into an ontology’s scope and are pivotal for various tasks, e.g. ontology reuse, testing, requirement specification, and pattern definition. Despite their importance, the practice of publishing CQs alongside ontological artefacts is not commonly adopted. We propose an approach based on Generative AI, specifically Large Language Models (LLMs) for retrofitting CQs from existing ontologies and we study how the control parameters in two LLMs (i.e. gpt-3.5-turbo and gpt-4) affect their performance and investigate the interplay between prompts and configuration for retrofitting viable CQs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Competency Questions; Large Language Models; Ontology Engineering Methodologies; C (programming Language); Generative Adversarial Networks; Ontology; Requirements Engineering; Competency Question; Functional Requirement; Language Model; Large Language Model; Natural Language Questions; Ontology Engineering; Ontology Engineering Methodologies; Ontology Reuse; Ontology's; Testing Requirements; Retrofitting},
	keywords = {C (programming language); Generative adversarial networks; Ontology; Requirements engineering; Competency question; Functional requirement; Language model; Large language model; Natural language questions; Ontology engineering; Ontology Engineering Methodologies; Ontology reuse; Ontology's; Testing requirements; Retrofitting},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Tufek202592,
	author = {Tufek, Nilay and Thuluva, Aparna Saisree and Just, Valentin Philipp and Ekaputra, Fajar J. and Bandyopadhyay, Tathagata and Sabou, Marta and Hanbury, Allan G.},
	title = {Validating Semantic Artifacts with Large Language Models},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15344 LNCS},
	pages = {92 - 101},
	doi = {10.1007/978-3-031-78952-6_9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218443416&doi=10.1007%2F978-3-031-78952-6_9&partnerID=40&md5=787f0b08362a57f58754faf9bf31875b},
	abstract = {As part of knowledge engineering workflows, semantic artifacts, such as ontologies, knowledge graphs or semantic descriptions based on industrial standards, are often validated in terms of their compliance with requirements expressed in natural language (e.g., ontology competency questions, standard specifications). Key to this process is the translation of the requirements in machine-actionable queries (e.g., SPARQL) that can automate the validation process. This manual translation process is time-consuming, error-prone and challenging, especially in areas where domain experts might lack knowledge of semantic technologies. In this paper, we propose a Large Language Models (LLMs) based approach to translate requirements texts into SPARQL queries and test it in validation use cases related to SAREF and OPC UA Robotics. F1 scores of 88–100% indicate the feasibility of the approach and its potential impact on ensuring high quality semantic artifacts and further uptake of the semantic technologies (industrial) domains. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Llm; Opc Ua; Semantic Artifacts; Validation; Knowledge Graph; Latent Semantic Analysis; Machine Translation; Modeling Languages; Query Languages; Requirements Engineering; Semantics; Engineering Workflows; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Opc Ua; Semantic Artifact; Semantic Descriptions; Semantic Technologies; Validation; Ontology},
	keywords = {Knowledge graph; Latent semantic analysis; Machine translation; Modeling languages; Query languages; Requirements engineering; Semantics; Engineering workflows; Knowledge graphs; Language model; Large language model; Ontology's; OPC UA; Semantic artifact; Semantic descriptions; Semantic technologies; Validation; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Patricio202571,
	author = {Patricio, Cristiano and Teixeira, Luís F. and Neves, João C.},
	title = {A two-step concept-based approach for enhanced interpretability and trust in skin lesion diagnosis},
	year = {2025},
	journal = {Computational and Structural Biotechnology Journal},
	volume = {28},
	pages = {71 - 79},
	doi = {10.1016/j.csbj.2025.02.013},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218344367&doi=10.1016%2Fj.csbj.2025.02.013&partnerID=40&md5=b89021385dcbc810b9a4fdcb6932b2c7},
	abstract = {The main challenges hindering the adoption of deep learning-based systems in clinical settings are the scarcity of annotated data and the lack of interpretability and trust in these systems. Concept Bottleneck Models (CBMs) offer inherent interpretability by constraining the final disease prediction on a set of human-understandable concepts. However, this inherent interpretability comes at the cost of greater annotation burden. Additionally, adding new concepts requires retraining the entire system. In this work, we introduce a novel two-step methodology that addresses both of these challenges. By simulating the two stages of a CBM, we utilize a pretrained Vision Language Model (VLM) to automatically predict clinical concepts, and an off-the-shelf Large Language Model (LLM) to generate disease diagnoses grounded on the predicted concepts. Furthermore, our approach supports test-time human intervention, enabling corrections to predicted concepts, which improves final diagnoses and enhances transparency in decision-making. We validate our approach on three skin lesion datasets, demonstrating that it outperforms traditional CBMs and state-of-the-art explainable methods, all without requiring any training and utilizing only a few annotated examples. The code is available at https://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Bottleneck Models; Dermoscopy; Interpretability; Skin Cancer; Vision-language Models; Contrastive Learning; Deep Learning; Dermatology; Diseases; Bottleneck Models; Clinical Settings; Concept Bottleneck Model; Concept-based Approach; Dermoscopy; Interpretability; Language Model; Skin Cancers; Skin Lesion; Vision-language Model; Visual Languages; Article; Concept Bottleneck Model; Decision Making; Deep Learning; Disease Classification; Few Shot Prompting; Language Model; Large Language Model; Melanoma; Nevus; Ontology; Skin Defect; Vision Language Model; Zero Shot Prompting},
	keywords = {Contrastive Learning; Deep learning; Dermatology; Diseases; Bottleneck models; Clinical settings; Concept bottleneck model; Concept-based approach; Dermoscopy; Interpretability; Language model; Skin cancers; Skin lesion; Vision-language model; Visual languages; Article; concept bottleneck model; decision making; deep learning; disease classification; few shot prompting; language model; large language model; melanoma; nevus; ontology; skin defect; vision language model; zero shot prompting},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Kim202525741,
	author = {Kim, Han-kyul and Park, Yujin and Kim, Yoonji and Yi, Seungag and Park, Yeju and So, Sujin and Lee, Hyeonji and Bae, Yeseul},
	title = {EILEEN: A Multi-Modal Framework for Extracting Alcohol Consumption Patterns from Bilingual Clinical Notes},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {25741 - 25751},
	doi = {10.1109/ACCESS.2025.3538803},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217581897&doi=10.1109%2FACCESS.2025.3538803&partnerID=40&md5=f710ec51d3820a3fdbe46f672ad3bd96},
	abstract = {In this work, we introduce EILEEN (Efficient Inference for Language-based Extraction of EHR Notes), a novel multi-modal natural language processing (NLP) framework designed to extract various alcohol consumption patterns from unstructured clinical notes, particularly in bilingual and non-English contexts. Recent advances in NLP have significantly improved information extraction capability across various domains. However, identifying patterns of alcohol consumption in medical documents remains underexplored, with existing approaches heavily relying on traditional NLP methods such as bag-of-words models that require extensive text preprocessing. These methods are often limited to English-language clinical settings, where robust medical ontologies and NLP toolkits are available to support preprocessing tasks. Therefore, this limitation hinders their use in multilingual healthcare settings and in environments lacking robust NLP toolkits to facilitate preprocessing. Motivated by the need for a more generalizable and accurate approach, this paper investigates the impact of large language models (LLMs) in advancing alcohol consumption pattern extraction from clinical notes. By reducing the need for manual preprocessing and improving adaptability to multilingual clinical notes, this work aims to enable broader, more practical applications of NLP models in extracting alcohol consumption patterns from clinical notes. By fine-tuning multilingual language models along with additional data sources, EILEEN effectively analyzes unstructured electronic health records (EHR) without relying on traditional concept normalization or extensive text preprocessing resources. Furthermore, the multi-modal component of EILEEN enables it to integrate and leverage diverse types of alcohol-related information, such as various types and amounts of alcohol consumed by a patient, thereby improving its pattern extraction accuracy. Our experiments, conducted in two different medical institutions in Korea, demonstrate that EILEEN significantly outperforms existing NLP methods in accurately identifying clinically relevant alcohol consumption patterns. By providing accurate, detailed, and clinically useful alcohol consumption patterns from unstructured clinical notes, EILEEN empowers healthcare practitioners with actionable insights essential for informed clinical decision-making. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Alcohol Information Extraction; Clinical Informatics; Multilingual Transformers; Multimodal Learning; Natural Language Processing; Natural Language Processing Systems; Ontology; Alcohol Consumption; Alcohol Information Extraction; Clinical Informatics; Clinical Notes; Consumption Patterns; Language Processing; Multi-modal Learning; Multilingual Transformer; Natural Language Processing; Natural Languages; Electronic Health Record},
	keywords = {Natural language processing systems; Ontology; Alcohol consumption; Alcohol information extraction; Clinical informatics; Clinical notes; Consumption patterns; Language processing; Multi-modal learning; Multilingual transformer; Natural language processing; Natural languages; Electronic health record},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Sumpter2025,
	author = {Sumpter, Lovisa and Blomqvist, Anneli},
	title = {What is functional thinking? Using cosine similarity matrix in a semantic ontological analysis},
	year = {2025},
	journal = {International Electronic Journal of Mathematics Education},
	volume = {20},
	number = {1},
	pages = {},
	doi = {10.29333/iejme/15679},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217473960&doi=10.29333%2Fiejme%2F15679&partnerID=40&md5=886e6b678ef9c2ef77c3715907b3fece},
	abstract = {Knowing functions and functional thinking have recently moved from just knowledge for older students to incorporating younger students, and functional thinking has been identified as one of the core competencies for algebra. Although it is significant for mathematical understanding, there is no unified view of functional thinking and how different aspects of the concept are used as a theoretical base. In this paper, we analyse different definitions used in empirical studies. First, we did a systematic research review resulting in 19 empirical studies focusing on functional thinking with an appropriate theoretical underpinning. The definitions were analysed using an AI tool. After that, we analysed the results using intrinsic mathematical properties of how functions can be defined in mathematics to identify core aspects of the definitions. According to the analysis, two definitions capture most of the key aspects of functional thinking, and most empirical studies use these key concepts. These two definitions treat functional thinking as products or products and processes. One definition used in one empirical study stands out by theoretically operationalizing functional thinking as a process. As such, different ontological assumptions are made in the studies; however, in some cases, having the same epistemological outcome. From a methodological point of view, the cosine similarity matrix was a useful tool for an ontological analysis, but a qualitative analysis is still needed to make meaning of it. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cosine Similarity Matrix; Functional Thinking; Functions; Large Language Models; Ontological Analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yhdego2025,
	author = {Yhdego, Tsegai O. and Wang, Hui},
	title = {Automated Ontology Generation for Zero-shot Defect Identification in Manufacturing},
	year = {2025},
	journal = {IEEE Transactions on Automation Science and Engineering},
	pages = {},
	doi = {10.1109/TASE.2025.3537463},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217032682&doi=10.1109%2FTASE.2025.3537463&partnerID=40&md5=e5ab810de0ed81cdcfa98e466137c161},
	abstract = {A lack of labeled data presents a significant challenge to automatic defect identification in manufacturing, which is a crucial step in process control and certification during process development. State-of-the-art transfer learning is incapable of handling such zero-shot learning (ZSL) when defect labels are absent in training datasets. The latest research on ZSL leverages natural language processing (NLP) based on large language models (LLM) and shows promise by supplementing information to generate labels. However, its performance is hampered by the supporting LLMs pre-trained on generic vocabulary that failed to characterize manufacturing defects accurately. This paper establishes a methodology to automatically extract multi-level attributes from literature to improve defect representation, thereby facilitating ZSL. The extracted attributes contribute to a hierarchical knowledge graph, called defect ontology, to characterize multiple aspects of manufacturing defects. The proposed algorithm takes the defect images and associated text from the literature as input and develops an unsupervised method to identify the hierarchical relationships among the tokenized information extracted from the input text-feature corpora. The hierarchical graph is refined to retain the most relevant information by a pruning algorithm based on a minimum path search. A walk algorithm, along with NLP, parsed the generated ontology to create embedding of defects to enable zero-shot attribute learning to identify defects. The proposed method advances the ZSL methodology by automatically creating a hierarchical knowledge representation from literature and images to replace generic vocabulary in LLM adopted by ZSL algorithms, thus improving defect representation. The case studies are among the earlier attempts to demonstrate the feasibility of using literature data from public sources to extract attributes automatically to identify defects in a real additive manufacturing process based on direct-ink-writing. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Defect Identification; Manufacturing Automation; Self-supervised Learning; Zero-shot Learning; Gluing; Graph Embeddings; Knowledge Graph; Manufacturing Data Processing; Natural Language Processing Systems; Network Security; Ontology; Process Control; Self-supervised Learning; Semi-supervised Learning; Smart Manufacturing; Transfer Learning; Unsupervised Learning; Defect Identification; Hierarchical Knowledge; Labeled Data; Language Model; Language Processing; Manufacturing Automation; Manufacturing Defects; Natural Languages; Ontology Generation; Ontology's; Zero-shot Learning},
	keywords = {Gluing; Graph embeddings; Knowledge graph; Manufacturing data processing; Natural language processing systems; Network security; Ontology; Process control; Self-supervised learning; Semi-supervised learning; Smart manufacturing; Transfer learning; Unsupervised learning; Defect identification; Hierarchical knowledge; Labeled data; Language model; Language processing; Manufacturing Automation; Manufacturing defects; Natural languages; Ontology generation; Ontology's; Zero-shot learning},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Chen2025,
	author = {Chen, Jiaying and Wang, Jingfu and Hu, Yue and Li, Xinhui and Qian, Yurong and Song, Chaolin},
	title = {Evaluating the advancements in protein language models for encoding strategies in protein function prediction: a comprehensive review},
	year = {2025},
	journal = {Frontiers in Bioengineering and Biotechnology},
	volume = {13},
	pages = {},
	doi = {10.3389/fbioe.2025.1506508},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216720830&doi=10.3389%2Ffbioe.2025.1506508&partnerID=40&md5=da8541586a39c97df9999e8f8ef31ba7},
	abstract = {Protein function prediction is crucial in several key areas such as bioinformatics and drug design. With the rapid progress of deep learning technology, applying protein language models has become a research focus. These models utilize the increasing amount of large-scale protein sequence data to deeply mine its intrinsic semantic information, which can effectively improve the accuracy of protein function prediction. This review comprehensively combines the current status of applying the latest protein language models in protein function prediction. It provides an exhaustive performance comparison with traditional prediction methods. Through the in-depth analysis of experimental results, the significant advantages of protein language models in enhancing the accuracy and depth of protein function prediction tasks are fully demonstrated. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Deep Multi-label Classification; Gene Ontology (go); Protein Function Prediction; Protein Language Model; Deep Learning; Gene Encoding; Gene Ontology; Deep Multi-label Classification; Drug Design; Encoding Strategy; Gene Ontology; Language Model; Multi-label Classifications; Protein Function Prediction; Protein Language Model; Prediction Models},
	keywords = {Deep learning; Gene encoding; Gene Ontology; Deep multi-label classification; Drug Design; Encoding strategy; Gene ontology; Language model; Multi-label classifications; Protein function prediction; Protein language model; Prediction models},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Gold Open Access}
}

@ARTICLE{Constum2025,
	author = {Constum, Thomas and Tranouez, Pierrick and Paquet, Thierry},
	title = {DANIEL: a fast document attention network for information extraction and labelling of handwritten documents},
	year = {2025},
	journal = {International Journal on Document Analysis and Recognition},
	pages = {},
	doi = {10.1007/s10032-024-00511-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216651379&doi=10.1007%2Fs10032-024-00511-9&partnerID=40&md5=493176bbb59d4b7aad80dfb15f2218bc},
	abstract = {Information extraction from handwritten documents involves traditionally three distinct steps: Document Layout Analysis, Handwritten Text Recognition, and Named Entity Recognition. Recent approaches have attempted to integrate these steps into a single process using fully end-to-end architectures. Despite this, these integrated approaches have not yet matched the performance of language models, when applied to information extraction in plain text. In this paper, we introduce DANIEL (Document Attention Network for Information Extraction and Labelling), a fully end-to-end architecture integrating a language model and designed for comprehensive handwritten document understanding. DANIEL performs layout recognition, handwriting recognition, and named entity recognition on full-page documents. Moreover, it can simultaneously learn across multiple languages, layouts, and tasks. For named entity recognition, the ontology to be applied can be specified via the input prompt. The architecture employs a convolutional encoder capable of processing images of any size without resizing, paired with an autoregressive decoder based on a transformer-based language model. DANIEL achieves competitive results on four datasets, including a new state-of-the-art performance on RIMES 2009 and M-POPP for Handwriting Text Recognition, and IAM NER for Named Entity Recognition. Furthermore, DANIEL is much faster than existing approaches. We provide the source code and the weights of the trained models at https://github.com/Shulk97/daniel. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Document Information Extraction; End-to-end Transformer; Handwritten Text Recognition; Visual Document Understanding; Modeling Languages; Document Information Extraction; Document Understanding; End To End; End-to-end Transformer; Hand-written Text Recognition; Handwritten Document; Labelings; Language Model; Named Entity Recognition; Visual Document Understanding; Palmprint Recognition},
	keywords = {Modeling languages; Document information extraction; Document understanding; End to end; End-to-end transformer; Hand-written text recognition; Handwritten document; Labelings; Language model; Named entity recognition; Visual document understanding; Palmprint recognition},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Belani2025,
	author = {Belani, Hrvoje and Solic, P. and Zdravevski, Eftim and Trajkovik, Vladimir},
	title = {Internet of Things Ontologies for Well-Being, Aging and Health: A Scoping Literature Review},
	year = {2025},
	journal = {Electronics (Switzerland)},
	volume = {14},
	number = {2},
	pages = {},
	doi = {10.3390/electronics14020394},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215982835&doi=10.3390%2Felectronics14020394&partnerID=40&md5=c5406d6bf717988287cb2c4715867e9a},
	abstract = {Internet of Things aims to simplify and automate complicated tasks by using sensors and other inputs for collecting huge amounts of data, processing them in the cloud and on the edge networks, and allowing decision making toward further interactions via actuators and other outputs. As connected IoT devices rank in billions, semantic interoperability remains one of the permanent challenges, where ontologies can provide a great contribution. The main goal of this paper is to analyze the state of research on semantic interoperability in well-being, aging, and health IoT services by using ontologies. This was achieved by analyzing the following research questions: “Which IoT ontologies have been used to implement well-being, aging and health services?” and “What is the dominant approach to achieve semantic interoperability of IoT solutions for well-being, aging and health?’ We conducted a scoping literature review of research papers from 2013 to 2024 by applying the PRISMA-ScR meta-analysis methodology with a custom-built software tool for an exhaustive search through the following digital libraries: IEEE Xplore, PubMed, MDPI, Elsevier ScienceDirect, and Springer Nature Link. By thoroughly analyzing 30 studies from an initial pool of more than 80,000 studies, we conclude that IoT ontologies for well-being, aging, and health services increasingly adopt Semantic Web of Things standards to achieve semantic interoperability by integrating heterogeneous data through unified semantic models. Emerging approaches, like semantic communication, Large Language Models Edge Intelligence, and sustainability-driven IoT analytics, can further enhance service efficiency and promote a holistic “One Well-Being, Aging, and Health” framework. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Aging; E-health; Health; Internet Of Things (iot); Ontology; Scoping Literature Review; Semantic Interoperability; Semantic Web; Well-being},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{2025,
	title = {23rd International Conference of the Italian Association for Artificial Intelligence, AIxIA 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15450 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215701776&partnerID=40&md5=f50880624ac7fb3d5504443badcc2adb},
	abstract = {The proceedings contain 25 papers. The special focus in this conference is on Italian Association for Artificial Intelligence. The topics include: A Novel Approach for Leveraging Agent-Based Experts on Large Language Models to Enable Data Sharing Among Heterogeneous IoT Devices in Agriculture; an Extensive Empirical Analysis of Macro-actions for Numeric Planning; feature Selection on Contextual Embedding Pushing the Sparseness; neuro-Symbolic Integration for Open Set Recognition in Network Intrusion Detection; MM-IGLU-IT: Multi-modal Interactive Grounded Language Understanding in Italian; IDADA: A Blended Inductive-Deductive Approach for Data Augmentation; HaWANet: Road Scene Understanding with Multi-modal Sensor Data Using Height-Width-Driven Attention Network; hybrid Classification of European Legislation Using Sustainable Development Goals; supporting Decision-Making for City Management Through Automated Planning and Execution; NutriWell: An Explainable Ontology-Based FoodAI Service for Nutrition and Health Management; regular Clocks for Temporal Task Specifications in Reinforcement Learning; a Real-Time Support with Haptic Feedback for Safer Driving Using Monocular Camera; relating Explanations with the Inductive Biases of Deep Graph Networks; integrating Temporal Planning and Knowledge Representation to Generate Personalized Touristic Itineraries; ASR Systems Under Acoustic Challenges: A Multilingual Study; automating Resume Analysis: Knowledge Graphs via Prompt Engineering; combined Text-Visual Attention Models for Robot Task Learning and Execution; ICE: An Evaluation Metric to Assess Symbolic Knowledge Quality; hierarchical Knowledge Extraction from Opaque Machine Learning Predictors; on Different Symbolic Music Representations for Algorithmic Composition Approaches Based on Neural Sequence Models; DR-Minerva: A Multimodal Language Model Based on Minerva for Diagnostic Information Retrieval; REPAIR Platform: Robot-AidEd PersonAlIzed Rehabilitation; Integrating Classical Planners with GPT-Based Planning Policies. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{202522,
	title = {Enhancing functional gene set analysis with large language models},
	year = {2025},
	journal = {Nature Methods},
	volume = {22},
	number = {1},
	pages = {22 - 23},
	doi = {10.1038/s41592-024-02526-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215647632&doi=10.1038%2Fs41592-024-02526-w&partnerID=40&md5=a2a2d4426fdb4f97d0a4923639d5a881},
	abstract = {Large language models (LLMs) demonstrate potential as assistants in functional genomics, offering a new avenue for gene set analysis. In our evaluation of five LLMs, GPT-4 was the top-performing model and generated common functions for gene sets with high specificity, reliable self-assessed confidence and supporting analysis, complementing traditional functional enrichment. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Quality; Functional Enrichment Analysis; Functional Genomics; Gemini Pro; Gene Cluster; Gene Ontology; Gene Set Analysis; Gpt-4; Gpt3.5; Human; Large Language Model; Llama2-70b; Mixtral Instruct; Note; Omics; Article},
	keywords = {data quality; functional enrichment analysis; functional genomics; Gemini Pro; gene cluster; gene ontology; gene set analysis; GPT-4; GPT3.5; human; large language model; Llama2-70b; Mixtral Instruct; Note; omics; article},
	type = {Note},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ferraris2025125,
	author = {Ferraris, Davide and Kotis, Konstantinos I. and Kalloniatis, Christos},
	title = {Enhancing TrUStAPIS Methodology in the Web of Things with LLM-Generated IoT Trust Semantics},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15056 LNCS},
	pages = {125 - 144},
	doi = {10.1007/978-981-97-8798-2_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215544187&doi=10.1007%2F978-981-97-8798-2_7&partnerID=40&md5=e0172f6274636e08d44c6536e538d6c1},
	abstract = {In the Internet of Things (IoT) there are ecosystems where their physical’smart’ entities virtually interact with each other. Often, this interaction occurs among unknown entities, making trust an essential requirement to overcome uncertainty in several aspects of this interaction. However, trust is a complex concept, and incorporating it in IoT is still a challenging topic. For this reason, it is highly significant to specify and model trust in early stages of the System Development Life Cycle (SDLC) of IoT-integrated systems, thus enhancing the aforementioned task. TrUStAPIS is a requirements engineering methodology recently introduced for incorporating trust requirements during IoT-based system design. The scope of this paper is to provide an extension of TrUStAPIS by introducing IoT trust semantics compatible with the W3C Web of Things (WoT) recommendations generated with the assistance of Large Language Models (LLMs). Taking advantage of LLMs as a tool for integrating and refining existing methodologies, in this paper we present our work towards a revision of the TrUStAPIS methodology. In this work, we contribute a new conceptual model and a refined JSON-LD ontology that takes into account IoT trust semantics, providing eventually a valuable tool for software engineers to design and model IoT-based systems and services. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Internet Of Things (iot); Json-ld; Large Language Model (llm); Trust; Web Of Things (wot); Problem Oriented Languages; Software Design; Internet Of Thing; Json-ld; Language Model; Large Language Model; Model Trusts; Systems Development Life Cycle; Trust; Uncertainty; Unknown Entities; Web Of Thing; Requirements Engineering},
	keywords = {Problem oriented languages; Software design; Internet of thing; JSON-LD; Language model; Large language model; Model trusts; Systems development life cycle; Trust; Uncertainty; Unknown entities; Web of thing; Requirements engineering},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025,
	author = {Li, Chao and Petzold, Frank},
	title = {Ontology-Driven Mixture-of-Domain Documentation: A Backbone Approach Enabling Question Answering for Additive Construction},
	year = {2025},
	journal = {Buildings},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.3390/buildings15010133},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214466051&doi=10.3390%2Fbuildings15010133&partnerID=40&md5=ecef308cbc8c6301d6f0a28440ec3ea6},
	abstract = {Advanced construction techniques, such as additive manufacturing (AM) and modular construction, offer promising solutions to address labor shortages, reduce CO<inf>2</inf> emissions, and enhance material efficiency. Despite their potential, the adoption of these innovative methods is hindered by the construction industry’s fragmented expertise. Building Information Modeling (BIM) is frequently suggested to integrate this diverse knowledge, but existing BIM-based approaches lack a robust framework for systematically documenting and retrieving the cross-domain knowledge essential for construction projects. To bridge this gap, this paper presents an ontology-driven methodology for documenting and utilizing expert knowledge, with a focus on AM in construction. Based on a well-founded ontological framework, a set of modular ontologies is formalized for individual domains. Additionally, a prototypical documentation tool is developed to elevate recorded information and BIM models as a knowledge graph. This knowledge graph will interface with advanced large language models (LLMs), enabling effective question answering and knowledge retrieval. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Additive Construction; Building Information Modeling; Documentation; Ontology; Knowledge Graph; Modeling Languages; Ontology; Additive Construction; Advanced Construction; Building Information Modelling; Construction Technique; Documentation; Knowledge Graphs; Labor Shortages; Modulars; Ontology's; Question Answering; Question Answering},
	keywords = {Knowledge graph; Modeling languages; Ontology; Additive construction; Advanced construction; Building Information Modelling; Construction technique; Documentation; Knowledge graphs; Labor shortages; Modulars; Ontology's; Question Answering; Question answering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@ARTICLE{Gopal2025312,
	author = {Gopal, Jeyakodi and Shanthi Bala, P.},
	title = {HyPRETo: Hybrid Pre-trained Ontology Approach for Contextual Relation Classification on Mosquito Vector Biocontrol Agents},
	year = {2025},
	journal = {IFIP Advances in Information and Communication Technology},
	volume = {723 IFIP},
	pages = {312 - 326},
	doi = {10.1007/978-3-031-73617-9_25},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214102353&doi=10.1007%2F978-3-031-73617-9_25&partnerID=40&md5=3a1eed587124cfc66678a1b8d1ab73dd},
	abstract = {Pre-trained Language Model facilitates contextual relation classification by capturing contextual information, addressing word ambiguity, encoding global sentence context, enabling transfer learning, handling out-of-vocabulary words, and improving performance with limited labelled data. Existing pre-training approaches suffer in size, bias, interpretability, generalization, and the lack of domain specificity. To address this, HyPRETo, the hybrid model that combines the strength of token replacement and dynamic masking is proposed to achieve upgraded performance to increase classification accuracy. The Mosquito Vector Biocontrol Agents data is used for implementing the model for a contextual relation classification task. HyPRETo uses ontology to provide structured knowledge. HyPRETo is pre-trained by ELECTRA and fine-tuned by RoBERTa models. Feedforward and softmax activation function is used for classification. The Natural Language Processing technique and SQL database are used to develop an automated question-answering system. The HyPRETo was evaluated with state-of-art models and achieved 98.42% accuracy. As a contribution, the manually annotated input dataset on the mosquito vector control agent is prepared for the classification task. Subsequently, the enhanced model is developed. The interface for an automated question-answering system for mosquito vector biocontrol agents is developed to assist public health applications such as mosquito vector control, disease control, ecosystem management, environmental conservation, and so on. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biocontrol Agents; Contextual; Hypreto; Mosquito Vector; Pre-trained Model; Question-answering; Relation Classification; Information Management; Intelligent Systems; Metadata; Mosquito Control; Natural Language Processing Systems; Network Security; Ontology; Problem Oriented Languages; Vector Control (electric Machinery); Biocontrol Agent; Classification Tasks; Contextual; Hypreto; Mosquito Vectors; Ontology's; Pre-trained Model; Question Answering; Question Answering Systems; Relation Classifications; Vectors},
	keywords = {Information management; Intelligent systems; Metadata; Mosquito control; Natural language processing systems; Network security; Ontology; Problem oriented languages; Vector control (Electric machinery); Biocontrol agent; Classification tasks; Contextual; HyPRETo; Mosquito vectors; Ontology's; Pre-trained model; Question Answering; Question answering systems; Relation classifications; Vectors},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {26th International Conference on Information Integration and Web Intelligence, iiWAS 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15343 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212494941&partnerID=40&md5=fa2aecd7173d4fed3f498bd4793aadb2},
	abstract = {The proceedings contain 52 papers. The special focus in this conference is on Information Integration and Web Intelligence. The topics include: Financial News Classification Using Language Learning Models and Reinforcement Learning; ExtractGPT: Exploring the Potential of Large Language Models for Product Attribute Value Extraction; Feature Extraction for Claim Check-Worthiness Prediction Tasks Using LLM; training Data for Dialogue Generation Considering Philosophies; Finding Adequate Additional Layer of Auxiliary Task in BERT-Based Multi-task Learning; ponzi Scheme Detection and Prevention in Blockchain Platforms Using Machine Learning: A Systematic Literature Review; Cross-Chain Personal Data Exchange on EVM Platforms: Enhancing Transparency, and Equity; Incentivize Peer Review Without Rewarding: Using OSS-Like Citation Pull Request; towards Website X-Ray for Europe’s Municipalities: Unveiling Digital Transformation with Multimodal Embeddings; evolving Applications of Conversational Agents in Healthcare: A Literature Review; hybrid Edge-Cloud Federated Learning: The Case of Lightweight Smoking Detection; anonymization of Unstructured Health Data in Spanish; when Good Enough is the Best Option: Use of Digital Sufficiency to Fight Climate Change; multi-target Feature Selection Method for Predicting User-Level Psychological Status from Text; FIEAP: A Machine Learning Approach for Fair and Interpretable Employee Attrition Prediction; HOCON34k: A Corpus of Hate Speech in Online Comments from German Newspapers; SISIS: Sequence Indexing for SImilarity Search; top-k on Sequences: A New Approach to Enhanced Similarity Search; exploratory Data Analysis of Time Series Using Pre-segmented Clustering; a Data Science Approach for Predicting Soccer Passes Using Positional Data; a Method for Integrating Heterogeneous Data into a Knowledge Graph; predicting Knowledge Graph Updates from Edit Histories; Automatic Extraction of RML-star Mappings from Property Graphs; Exploring the Role of UML in Data Modelling for NoSQL Databases: Position Paper; railway Systems’ Ontologies: A Literature Review and an Alignment Proposal; Combining GraphSAGE and Label Propagation for Node Classification in Graphs. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {26th International Conference on Information Integration and Web Intelligence, iiWAS 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15342 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212253172&partnerID=40&md5=5f4500486ebf9c039d0e8b8e9d3f35af},
	abstract = {The proceedings contain 52 papers. The special focus in this conference is on Information Integration and Web Intelligence. The topics include: Financial News Classification Using Language Learning Models and Reinforcement Learning; ExtractGPT: Exploring the Potential of Large Language Models for Product Attribute Value Extraction; Feature Extraction for Claim Check-Worthiness Prediction Tasks Using LLM; training Data for Dialogue Generation Considering Philosophies; Finding Adequate Additional Layer of Auxiliary Task in BERT-Based Multi-task Learning; ponzi Scheme Detection and Prevention in Blockchain Platforms Using Machine Learning: A Systematic Literature Review; Cross-Chain Personal Data Exchange on EVM Platforms: Enhancing Transparency, and Equity; Incentivize Peer Review Without Rewarding: Using OSS-Like Citation Pull Request; towards Website X-Ray for Europe’s Municipalities: Unveiling Digital Transformation with Multimodal Embeddings; evolving Applications of Conversational Agents in Healthcare: A Literature Review; hybrid Edge-Cloud Federated Learning: The Case of Lightweight Smoking Detection; anonymization of Unstructured Health Data in Spanish; when Good Enough is the Best Option: Use of Digital Sufficiency to Fight Climate Change; multi-target Feature Selection Method for Predicting User-Level Psychological Status from Text; FIEAP: A Machine Learning Approach for Fair and Interpretable Employee Attrition Prediction; HOCON34k: A Corpus of Hate Speech in Online Comments from German Newspapers; SISIS: Sequence Indexing for SImilarity Search; top-k on Sequences: A New Approach to Enhanced Similarity Search; exploratory Data Analysis of Time Series Using Pre-segmented Clustering; a Data Science Approach for Predicting Soccer Passes Using Positional Data; a Method for Integrating Heterogeneous Data into a Knowledge Graph; predicting Knowledge Graph Updates from Edit Histories; Automatic Extraction of RML-star Mappings from Property Graphs; Exploring the Role of UML in Data Modelling for NoSQL Databases: Position Paper; railway Systems’ Ontologies: A Literature Review and an Alignment Proposal; Combining GraphSAGE and Label Propagation for Node Classification in Graphs. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {25th International Conference on Web Information Systems Engineering, WISE 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15437 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211933765&partnerID=40&md5=8a299731440a5243bcc0d221790637e0},
	abstract = {The proceedings contain 132 papers. The special focus in this conference is on Web Information Systems Engineering. The topics include: TAKE: Tracing Associative Empathy Keywords for Generating Empathetic Responses Based on Graph Attention; intent Identification Using Few-Shot and Active Learning with User Feedback; CLIMB: Imbalanced Data Modelling Using Contrastive Learning with Limited Labels; equivariant Diffusion-Based Sequential Hypergraph Neural Networks with Co-attention Fusion for Information Diffusion Prediction; CL3: A Collaborative Learning Framework for the Medical Data Ensuring Data Privacy in the Hyperconnected Environment; selectivity Estimation for Spatial Filters Using Optimizer Feedback: A Machine Learning Perspective; on Adversarial Training with Incorrect Labels; model Lake : A New Alternative for Machine Learning Models Management and Governance; a Benchmark Test Suite for Multiple Traveling Salesmen Problem with Pivot Cities; Deconfounded Causality-Aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs; Regularized Multi-LLMs Collaboration for Enhanced Score-Based Causal Discovery; Combining Uncensored and Censored LLMs for Ransomware Generation; Therapying Outside the Box: Innovating the Implementation and Evaulation of CBT in Therapeutic Artificial Agents; iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models; “Is this Site Legit?”: LLMs for Scam Website Detection; Towards Enhancing Linked Data Retrieval in Conversational UIs Using Large Language Models; BioLinkerAI: Capturing Knowledge Using LLMs to Enhance Biomedical Entity Linking; Enhancing LLMs Contextual Knowledge with Ontologies for Personalised Food Recommendation; ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources; Web-Based AI Assistant for Medical Imaging: A Case Study on Predicting Spontaneous Preterm Birth via Ultrasound Images; satellite-Driven Deep Learning Algorithm for Bathymetry Extraction; Would You Trust an AI Doctor? Building Reliable Medical Predictions with Kernel Dropout Uncertainty; empowering Visual Navigation: A Deep-Learning Solution for Enhanced Accessibility and Safety Among the Visually Impaired; A Transformer and LSTM Model for Electricity Consumption Forecasting and User’s Behavior Influence. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Xiang2025443,
	author = {Xiang, Lingzhi and Li, Qing and Li, Xiang and Diao, Xingchun},
	title = {G-ETI: Incorporating Graph Information for Improved Unsupervised Event Type Induction},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15437 LNCS},
	pages = {443 - 455},
	doi = {10.1007/978-981-96-0567-5_31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211910786&doi=10.1007%2F978-981-96-0567-5_31&partnerID=40&md5=978fdc6d4dfee4a7c17c2c5cf2e73151},
	abstract = {An event ontology is useful for the semantic integration of heterogeneous datasets related to events. To reduce manual efforts in event ontology construction, several Event-Type Induction (ETI) methods were proposed to automatically find new event types from a given data source. Existing ETI methods utilize a corpus-based data source, and achieve the ETI goal via document clustering over pre-trained neural text embeddings. Most of the ETI methods require a semi-supervised setting to obtain a satisfactory result. In this paper, we improve ETI performance by incorporating graph-based data sources, which usually consists of event nodes, participant subjects/objects and the relational edges. Our motivation is that event type information learned from an event graph can complement that learned from text. This idea leads to the Graph-ETI (G-ETI) algorithm, where event clusters are initially identified from text embeddings and later refined through graph-based label propagation. Our algorithm naturally supports the unsupervised ETI setting where no event types are known beforehand. Moreover, we also provide an LLM-based naming module to generate appropriate names for the new event clusters. In the experiment, our method exhibits better event clustering performance compared to existing baselines, especially in the unsupervised setting. These improved clustering assignments combined with our LLM naming module can lead to high-quality ETI capability, which facilitates the event ontology construction process. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Event Graph; Event Ontology Construction; Event-type Induction; Graph Algorithms; Graph Embeddings; Ontology; Data-source; Embeddings; Event Graphs; Event Ontology; Event Ontology Construction; Event Types; Event-type Induction; Induction Method; Ontology Construction; Performance; Semantics},
	keywords = {Graph algorithms; Graph embeddings; Ontology; Data-source; Embeddings; Event graphs; Event ontology; Event ontology construction; Event Types; Event-type induction; Induction method; Ontology construction; Performance; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Buric2025,
	author = {Buric, Filip and Viknander, Sandra and Fu, Xiaozhi and Lemke, Oliver and Carmona, Oriol Gracia and Zrimec, Jan and Szyrwiel, Lukasz and Mülleder, Michael and Ralser, Markus and Zelezniak, Aleksej},
	title = {Amino acid sequence encodes protein abundance shaped by protein stability at reduced synthesis cost},
	year = {2025},
	journal = {Protein Science},
	volume = {34},
	number = {1},
	pages = {},
	doi = {10.1002/pro.5239},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211575445&doi=10.1002%2Fpro.5239&partnerID=40&md5=01b819731b55df42604747cb9e2c9a41},
	abstract = {Understanding what drives protein abundance is essential to biology, medicine, and biotechnology. Driven by evolutionary selection, an amino acid sequence is tailored to meet the required abundance of a proteome, underscoring the intricate relationship between sequence and functional demand. Yet, the specific role of amino acid sequences in determining proteome abundance remains elusive. Here we show that the amino acid sequence alone encodes over half of protein abundance variation across all domains of life, ranging from bacteria to mouse and human. With an attempt to go beyond predictions, we trained a manageable-size Transformer model to interpret latent factors predictive of protein abundances. Intuitively, the model's attention focused on the protein's structural features linked to stability and metabolic costs related to protein synthesis. To probe these relationships, we introduce MGEM (Mutation Guided by an Embedded Manifold), a methodology for guiding protein abundance through sequence modifications. We find that mutations which increase predicted abundance have significantly altered protein polarity and hydrophobicity, underscoring a connection between protein structural features and abundance. Through molecular dynamics simulations we revealed that abundance-enhancing mutations possibly contribute to protein thermostability by increasing rigidity, which occurs at a lower synthesis cost. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Explainable Machine Learning; Language Models; Molecular Dynamics; Protein Engineering; Protein Expression; Protein Sequence; Protein Stability; Proteome; Amino Acid; Protein; Proteins; Amino Acid; Mutant Protein; Nucleotide; Proteome; Protein; Amino Acid Sequence; Article; Controlled Study; Correlation Analysis; Cost Benefit Analysis; Deep Learning; Explainable Machine Learning; Gene Ontology; Human; Hydrophobicity; Language Model; Molecular Dynamics; Nonhuman; Polarization; Prediction; Protein Degradation; Protein Domain; Protein Engineering; Protein Expression Level; Protein Function; Protein Metabolism; Protein Secondary Structure; Protein Stability; Protein Structure; Protein Synthesis; Proteomics; Random Forest; Saccharomyces Cerevisiae; Thermostability; Animal; Chemical Phenomena; Chemistry; Genetics; Metabolism; Mouse; Mutation; Amino Acid Sequence; Animals; Humans; Hydrophobic And Hydrophilic Interactions; Mice; Molecular Dynamics Simulation; Mutation; Protein Biosynthesis; Protein Stability; Proteins},
	keywords = {amino acid; mutant protein; nucleotide; proteome; protein; amino acid sequence; Article; controlled study; correlation analysis; cost benefit analysis; deep learning; explainable machine learning; gene ontology; human; hydrophobicity; language model; molecular dynamics; nonhuman; polarization; prediction; protein degradation; protein domain; protein engineering; protein expression level; protein function; protein metabolism; protein secondary structure; protein stability; protein structure; protein synthesis; proteomics; random forest; Saccharomyces cerevisiae; thermostability; animal; chemical phenomena; chemistry; genetics; metabolism; mouse; mutation; Amino Acid Sequence; Animals; Humans; Hydrophobic and Hydrophilic Interactions; Mice; Molecular Dynamics Simulation; Mutation; Protein Biosynthesis; Protein Stability; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{2025,
	title = {25th International Conference on Web Information Systems Engineering, WISE 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15438 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211327214&partnerID=40&md5=e7fa3d09bc1021d158bec2e6e8d7a440},
	abstract = {The proceedings contain 132 papers. The special focus in this conference is on Web Information Systems Engineering. The topics include: TAKE: Tracing Associative Empathy Keywords for Generating Empathetic Responses Based on Graph Attention; intent Identification Using Few-Shot and Active Learning with User Feedback; CLIMB: Imbalanced Data Modelling Using Contrastive Learning with Limited Labels; equivariant Diffusion-Based Sequential Hypergraph Neural Networks with Co-attention Fusion for Information Diffusion Prediction; CL3: A Collaborative Learning Framework for the Medical Data Ensuring Data Privacy in the Hyperconnected Environment; selectivity Estimation for Spatial Filters Using Optimizer Feedback: A Machine Learning Perspective; on Adversarial Training with Incorrect Labels; model Lake : A New Alternative for Machine Learning Models Management and Governance; a Benchmark Test Suite for Multiple Traveling Salesmen Problem with Pivot Cities; Deconfounded Causality-Aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs; Regularized Multi-LLMs Collaboration for Enhanced Score-Based Causal Discovery; Combining Uncensored and Censored LLMs for Ransomware Generation; Therapying Outside the Box: Innovating the Implementation and Evaulation of CBT in Therapeutic Artificial Agents; iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models; “Is this Site Legit?”: LLMs for Scam Website Detection; Towards Enhancing Linked Data Retrieval in Conversational UIs Using Large Language Models; BioLinkerAI: Capturing Knowledge Using LLMs to Enhance Biomedical Entity Linking; Enhancing LLMs Contextual Knowledge with Ontologies for Personalised Food Recommendation; ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources; Web-Based AI Assistant for Medical Imaging: A Case Study on Predicting Spontaneous Preterm Birth via Ultrasound Images; satellite-Driven Deep Learning Algorithm for Bathymetry Extraction; Would You Trust an AI Doctor? Building Reliable Medical Predictions with Kernel Dropout Uncertainty; empowering Visual Navigation: A Deep-Learning Solution for Enhanced Accessibility and Safety Among the Visually Impaired; A Transformer and LSTM Model for Electricity Consumption Forecasting and User’s Behavior Influence. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {25th International Conference on Web Information Systems Engineering, WISE 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15436 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211324377&partnerID=40&md5=710cfb0decd068c922cc3591fcb9639b},
	abstract = {The proceedings contain 132 papers. The special focus in this conference is on Web Information Systems Engineering. The topics include: TAKE: Tracing Associative Empathy Keywords for Generating Empathetic Responses Based on Graph Attention; intent Identification Using Few-Shot and Active Learning with User Feedback; CLIMB: Imbalanced Data Modelling Using Contrastive Learning with Limited Labels; equivariant Diffusion-Based Sequential Hypergraph Neural Networks with Co-attention Fusion for Information Diffusion Prediction; CL3: A Collaborative Learning Framework for the Medical Data Ensuring Data Privacy in the Hyperconnected Environment; selectivity Estimation for Spatial Filters Using Optimizer Feedback: A Machine Learning Perspective; on Adversarial Training with Incorrect Labels; model Lake : A New Alternative for Machine Learning Models Management and Governance; a Benchmark Test Suite for Multiple Traveling Salesmen Problem with Pivot Cities; Deconfounded Causality-Aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs; Regularized Multi-LLMs Collaboration for Enhanced Score-Based Causal Discovery; Combining Uncensored and Censored LLMs for Ransomware Generation; Therapying Outside the Box: Innovating the Implementation and Evaulation of CBT in Therapeutic Artificial Agents; iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models; “Is this Site Legit?”: LLMs for Scam Website Detection; Towards Enhancing Linked Data Retrieval in Conversational UIs Using Large Language Models; BioLinkerAI: Capturing Knowledge Using LLMs to Enhance Biomedical Entity Linking; Enhancing LLMs Contextual Knowledge with Ontologies for Personalised Food Recommendation; ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources; Web-Based AI Assistant for Medical Imaging: A Case Study on Predicting Spontaneous Preterm Birth via Ultrasound Images; satellite-Driven Deep Learning Algorithm for Bathymetry Extraction; Would You Trust an AI Doctor? Building Reliable Medical Predictions with Kernel Dropout Uncertainty; empowering Visual Navigation: A Deep-Learning Solution for Enhanced Accessibility and Safety Among the Visually Impaired; A Transformer and LSTM Model for Electricity Consumption Forecasting and User’s Behavior Influence. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mai2025126,
	author = {Mai, Huu Tan and Chu, Cuongxuan and Paulheim, Heiko},
	title = {Do LLMs Really Adapt to Domains? An Ontology Learning Perspective},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15231 LNCS},
	pages = {126 - 143},
	doi = {10.1007/978-3-031-77844-5_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211249904&doi=10.1007%2F978-3-031-77844-5_7&partnerID=40&md5=38704b1d84f3911bba65b157ac5f2a75},
	abstract = {Large Language Models (LLMs) have demonstrated unprecedented prowess across various natural language processing tasks in various application domains. Recent studies show that LLMs can be leveraged to perform lexical semantic tasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL). However, it has not effectively been verified whether their success is due to their ability to reason over unstructured or semi-structured data, or their effective learning of linguistic patterns and senses alone. This unresolved question is particularly crucial when dealing with domain-specific data, where the lexical senses and their meaning can completely differ from what a LLM has learned during its training stage. This paper investigates the following question: Do LLMs really adapt to domains and remain consistent in the extraction of structured knowledge, or do they only learn lexical senses instead of reasoning? To answer this question and, we devise a controlled experiment setup that uses WordNet to synthesize parallel corpora, with English and gibberish terms. We examine the differences in the outputs of LLMs for each corpus in two OL tasks: relation extraction and taxonomy discovery. Empirical results show that, while adapting to the gibberish corpora, off-the-shelf LLMs do not consistently reason over semantic relationships between concepts, and instead leverage senses and their frame. However, fine-tuning improves the performance of LLMs on lexical semantic tasks even when the domain-specific terms are arbitrary and unseen during pre-training, hinting at the applicability of pre-trained LLMs for OL. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain Adaptation; Llms; Ontology Learning; Adversarial Machine Learning; Contrastive Learning; Federated Learning; Latent Semantic Analysis; Natural Language Processing Systems; Ontology; Semantics; Applications Domains; Domain Adaptation; Domain Specific; Language Model; Language Processing; Large Language Model; Lexical Semantics; Natural Languages; Ontology Learning; Semantic Tasks; Domain Knowledge},
	keywords = {Adversarial machine learning; Contrastive Learning; Federated learning; Latent semantic analysis; Natural language processing systems; Ontology; Semantics; Applications domains; Domain adaptation; Domain specific; Language model; Language processing; Large language model; Lexical semantics; Natural languages; Ontology learning; Semantic tasks; Domain Knowledge},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{2025,
	title = {25th International Conference on Web Information Systems Engineering, WISE 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15439 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211245787&partnerID=40&md5=2487abf9e418ce205764be9e26b982cd},
	abstract = {The proceedings contain 132 papers. The special focus in this conference is on Web Information Systems Engineering. The topics include: TAKE: Tracing Associative Empathy Keywords for Generating Empathetic Responses Based on Graph Attention; intent Identification Using Few-Shot and Active Learning with User Feedback; CLIMB: Imbalanced Data Modelling Using Contrastive Learning with Limited Labels; equivariant Diffusion-Based Sequential Hypergraph Neural Networks with Co-attention Fusion for Information Diffusion Prediction; CL3: A Collaborative Learning Framework for the Medical Data Ensuring Data Privacy in the Hyperconnected Environment; selectivity Estimation for Spatial Filters Using Optimizer Feedback: A Machine Learning Perspective; on Adversarial Training with Incorrect Labels; model Lake : A New Alternative for Machine Learning Models Management and Governance; a Benchmark Test Suite for Multiple Traveling Salesmen Problem with Pivot Cities; Deconfounded Causality-Aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs; Regularized Multi-LLMs Collaboration for Enhanced Score-Based Causal Discovery; Combining Uncensored and Censored LLMs for Ransomware Generation; Therapying Outside the Box: Innovating the Implementation and Evaulation of CBT in Therapeutic Artificial Agents; iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models; “Is this Site Legit?”: LLMs for Scam Website Detection; Towards Enhancing Linked Data Retrieval in Conversational UIs Using Large Language Models; BioLinkerAI: Capturing Knowledge Using LLMs to Enhance Biomedical Entity Linking; Enhancing LLMs Contextual Knowledge with Ontologies for Personalised Food Recommendation; ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources; Web-Based AI Assistant for Medical Imaging: A Case Study on Predicting Spontaneous Preterm Birth via Ultrasound Images; satellite-Driven Deep Learning Algorithm for Bathymetry Extraction; Would You Trust an AI Doctor? Building Reliable Medical Predictions with Kernel Dropout Uncertainty; empowering Visual Navigation: A Deep-Learning Solution for Enhanced Accessibility and Safety Among the Visually Impaired; A Transformer and LSTM Model for Electricity Consumption Forecasting and User’s Behavior Influence. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Allemang2025324,
	author = {Allemang, Dean and Sequeda, Juan Federico},
	title = {Increasing the Accuracy of LLM Question-Answering Systems with Ontologies},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15233 LNCS},
	pages = {324 - 339},
	doi = {10.1007/978-3-031-77847-6_18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211242388&doi=10.1007%2F978-3-031-77847-6_18&partnerID=40&md5=011d5b3e2c19429c9fc323157efa90ac},
	abstract = {There is increasing evidence that question-answering (QA) systems with Large Language Models (LLMs), which employ a knowledge graph representation of an enterprise SQL database (Text-to-SPARQL), achieve higher accuracy compared to systems that answer questions directly on SQL databases (Text-to-SQL). The objective of this research is to further improve the accuracy of these LLM Question Answering systems. Our approach, Ontology-based Query Check (OBQC), is to check the LLM generated SPARQL query against the semantics specified by the ontology. A query will be flagged as incorrect and prevented from execution if it does not align with the ontological semantics. The study also explores the LLM’s capability in repairing a SPARQL query given an explanation of the error (LLM Repair). Our methods are evaluated using the chat with the data benchmark. The primary finding is our method further increases the accuracy overall by 21.59% thus pushing the overall accuracy level to 65.63%. These results provide further evidence that investing knowledge graphs, namely the ontology, provides higher accuracy for LLM powered question answering systems. Our method is a component of the data.world AI Context Engine which is being widely used by customers in Generative AI production use cases that enable business users to chat with SQL databases. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Database Systems; Knowledge Graph; Ontology; Semantics; Structured Query Language; Graph Representation; High-accuracy; Knowledge Graphs; Language Model; Model Questions; Model Repair; Ontology's; Ontology-based Query; Question Answering Systems; Sql Database},
	keywords = {Database systems; Knowledge graph; Ontology; Semantics; Structured Query Language; Graph representation; High-accuracy; Knowledge graphs; Language model; Model questions; Model repair; Ontology's; Ontology-based query; Question answering systems; SQL database},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Bagozi2025273,
	author = {Bagozi, Ada and Bianchini, Devis and Melchiori, Michele and Rula, Anisa},
	title = {Enhancing LLMs Contextual Knowledge with Ontologies for Personalised Food Recommendation},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15439 LNCS},
	pages = {273 - 283},
	doi = {10.1007/978-981-96-0573-6_20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211242004&doi=10.1007%2F978-981-96-0573-6_20&partnerID=40&md5=d7d85591a9d9c4f0780ab6dd3c9ff484},
	abstract = {Food recommendation systems help consumers make sustainable and nutritionally complete choices, promoting healthy eating habits and addressing the growing interest in food sustainability and waste reduction. Large Language Models (LLMs), such as ChatGPT, are increasingly used for food recommendations due to their natural language processing capabilities. However, providing personalised and contextually relevant suggestions remains challenging because of the lack of a robust conceptualisation of healthy and sustainable food aligned with users’ dietary and lifestyle preferences. Ontologies can address this by offering a structured and semantically rich framework for organising information. In this paper, we propose a modular ontology to enhance the contextual knowledge of LLMs, enabling them to deliver personalised, contextually relevant food recommendations. The ontology’s modules are based on competency questions derived from a research project focused on sustainable and healthy food recommendations. To evaluate the effectiveness of this approach, we conducted experiments where ChatGPT-4 answered these competency questions with and without ontology integration. The answers were then assessed in a user study. Preliminary experimental results indicate significant improvements in the quality and relevance of recommendations when the ontology is employed. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Consumer Empowerment; Large Language Models; Multi-perspective Ontology Engineering; Sustainable Food Recommendation; Consumer Empowerment; Contextual Knowledge; Eating Habits; Language Model; Large Language Model; Multi-perspective; Multi-perspective Ontology Engineering; Ontology Engineering; Ontology's; Sustainable Food Recommendation},
	keywords = {Consumer empowerment; Contextual knowledge; Eating habits; Language model; Large language model; Multi-perspective; Multi-perspective ontology engineering; Ontology engineering; Ontology's; Sustainable food recommendation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{De Bellis2025227,
	author = {De Bellis, Alessandro and Anelli, Vito Walter and Di Noia, Tommaso and Di Sciascio, Eugenio},
	title = {PRONTO: Prompt-Based Detection of Semantic Containment Patterns in MLMs},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15232 LNCS},
	pages = {227 - 246},
	doi = {10.1007/978-3-031-77850-6_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211240274&doi=10.1007%2F978-3-031-77850-6_13&partnerID=40&md5=93eb763bbd22b1e15d976ace3a38141c},
	abstract = {Masked Language Models (MLMs) like BERT and RoBERTa excel at predicting missing words based on context, but their ability to understand deeper semantic relationships is still being assessed. While MLMs have demonstrated impressive capabilities, it is still unclear if they merely exploit statistical word co-occurrence or if they can capture a deeper, structured understanding of meaning, similar to how knowledge is organized in ontologies. This is a topic of increasing interest, with researchers seeking to understand how MLMs might internally represent concepts like ontological classes and semantic containment relations (e.g., sub-class and instance-of). Unveiling this knowledge could have significant implications for Semantic Web applications, but it necessitates a profound understanding of how these models express such relationships. This work investigates whether MLMs can understand these relationships, presenting a novel approach to automatically leverage the predictions returned by MLMs to discover semantic containment relations in unstructured text. We achieve this by constructing a verbalizer, a system that translates the model’s internal predictions into classification labels. Through a comprehensive probing procedure, we assess the method’s effectiveness, reliability, and interpretability. Our findings demonstrate a key strength of MLMs: their ability to capture semantic containment relationships. These insights bring significant implications for MLM application in ontology construction and aligning text data with ontologies. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Masked Language Models; Ontologies; Prompt Learning; Contrastive Learning; Ontology; Semantics; Excel; Language Model; Masked Language Model; Ontology's; Prompt Learning; Semantic Relationships; Semantic Web Applications; Sub Class; Unstructured Texts; Word Co-occurrence; Latent Semantic Analysis},
	keywords = {Contrastive Learning; Ontology; Semantics; Excel; Language model; Masked language model; Ontology's; Prompt learning; Semantic relationships; Semantic web applications; Sub class; Unstructured texts; Word co-occurrence; Latent semantic analysis},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {23rd International Semantic Web Conference, ISWC 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15231 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211222309&partnerID=40&md5=71ccee63d4ef2e070099e7c9ff12adea},
	abstract = {The proceedings contain 44 papers. The special focus in this conference is on Semantic Web. The topics include: Relationships Are Complicated! An Analysis of Relationships Between Datasets on the Web; multi-view Transformer-Based Network for Prerequisite Learning in Concept Graphs; knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-Based Causal Discovery; Repairing Networks of EL<inf>⊥</inf> Ontologies Using Weakening and Completing; Do LLMs Really Adapt to Domains? An Ontology Learning Perspective; supervised Relational Learning with Selective Neighbor Entities for Few-Shot Knowledge Graph Completion; knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation; unaligned Federated Knowledge Graph Embedding; finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion; blink: Blank Node Matching Using Embeddings; distilling Event Sequence Knowledge From Large Language Models. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {23rd International Semantic Web Conference, ISWC 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15233 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211205922&partnerID=40&md5=d567acf8e8c3d035e95493e20cc3d39c},
	abstract = {The proceedings contain 44 papers. The special focus in this conference is on Semantic Web. The topics include: Relationships Are Complicated! An Analysis of Relationships Between Datasets on the Web; multi-view Transformer-Based Network for Prerequisite Learning in Concept Graphs; knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-Based Causal Discovery; Repairing Networks of EL<inf>⊥</inf> Ontologies Using Weakening and Completing; Do LLMs Really Adapt to Domains? An Ontology Learning Perspective; supervised Relational Learning with Selective Neighbor Entities for Few-Shot Knowledge Graph Completion; knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation; unaligned Federated Knowledge Graph Embedding; finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion; blink: Blank Node Matching Using Embeddings; distilling Event Sequence Knowledge From Large Language Models. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {25th International Conference on Web Information Systems Engineering, WISE 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15440 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211202068&partnerID=40&md5=66d4fb20a327f8d7017e57af8a1c9cc0},
	abstract = {The proceedings contain 132 papers. The special focus in this conference is on Web Information Systems Engineering. The topics include: TAKE: Tracing Associative Empathy Keywords for Generating Empathetic Responses Based on Graph Attention; intent Identification Using Few-Shot and Active Learning with User Feedback; CLIMB: Imbalanced Data Modelling Using Contrastive Learning with Limited Labels; equivariant Diffusion-Based Sequential Hypergraph Neural Networks with Co-attention Fusion for Information Diffusion Prediction; CL3: A Collaborative Learning Framework for the Medical Data Ensuring Data Privacy in the Hyperconnected Environment; selectivity Estimation for Spatial Filters Using Optimizer Feedback: A Machine Learning Perspective; on Adversarial Training with Incorrect Labels; model Lake : A New Alternative for Machine Learning Models Management and Governance; a Benchmark Test Suite for Multiple Traveling Salesmen Problem with Pivot Cities; Deconfounded Causality-Aware Parameter-Efficient Fine-Tuning for Problem-Solving Improvement of LLMs; Regularized Multi-LLMs Collaboration for Enhanced Score-Based Causal Discovery; Combining Uncensored and Censored LLMs for Ransomware Generation; Therapying Outside the Box: Innovating the Implementation and Evaulation of CBT in Therapeutic Artificial Agents; iText2KG: Incremental Knowledge Graphs Construction Using Large Language Models; “Is this Site Legit?”: LLMs for Scam Website Detection; Towards Enhancing Linked Data Retrieval in Conversational UIs Using Large Language Models; BioLinkerAI: Capturing Knowledge Using LLMs to Enhance Biomedical Entity Linking; Enhancing LLMs Contextual Knowledge with Ontologies for Personalised Food Recommendation; ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources; Web-Based AI Assistant for Medical Imaging: A Case Study on Predicting Spontaneous Preterm Birth via Ultrasound Images; satellite-Driven Deep Learning Algorithm for Bathymetry Extraction; Would You Trust an AI Doctor? Building Reliable Medical Predictions with Kernel Dropout Uncertainty; empowering Visual Navigation: A Deep-Learning Solution for Enhanced Accessibility and Safety Among the Visually Impaired; A Transformer and LSTM Model for Electricity Consumption Forecasting and User’s Behavior Influence. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {23rd International Semantic Web Conference, ISWC 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15232 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211199051&partnerID=40&md5=23e027d24e29ea07ff2b1cce6f3ba9d2},
	abstract = {The proceedings contain 44 papers. The special focus in this conference is on Semantic Web. The topics include: Relationships Are Complicated! An Analysis of Relationships Between Datasets on the Web; multi-view Transformer-Based Network for Prerequisite Learning in Concept Graphs; knowledge Graph Structure as Prompt: Improving Small Language Models Capabilities for Knowledge-Based Causal Discovery; Repairing Networks of EL<inf>⊥</inf> Ontologies Using Weakening and Completing; Do LLMs Really Adapt to Domains? An Ontology Learning Perspective; supervised Relational Learning with Selective Neighbor Entities for Few-Shot Knowledge Graph Completion; knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation; unaligned Federated Knowledge Graph Embedding; finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion; blink: Blank Node Matching Using Embeddings; distilling Event Sequence Knowledge From Large Language Models. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{De Santis2025304,
	author = {De Santis, Antonio and Balduini, Marco and de Santis, Federico and Proia, Andrea and Leo, Arsenio and Brambilla, Marco and Della Valle, Emanuele},
	title = {Integrating Large Language Models and Knowledge Graphs for Extraction and Validation of Textual Test Data},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15233 LNCS},
	pages = {304 - 323},
	doi = {10.1007/978-3-031-77847-6_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211195122&doi=10.1007%2F978-3-031-77847-6_17&partnerID=40&md5=dbff801489415bef0a01fbdee2794711},
	abstract = {Aerospace manufacturing companies, such as Thales Alenia Space, design, develop, integrate, verify, and validate products characterized by high complexity and low volume. They carefully document all phases for each product but analyses across products are challenging due to the heterogeneity and unstructured nature of the data in documents. In this paper, we propose a hybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with Large Language Models (LLMs) to extract and validate data contained in these documents. We consider a case study focused on test data related to electronic boards for satellites. To do so, we extend the Semantic Sensor Network ontology. We store the metadata of the reports in a KG, while the actual test results are stored in parquet accessible via a Virtual Knowledge Graph. The validation process is managed using an LLM-based approach. We also conduct a benchmarking study to evaluate the performance of state-of-the-art LLMs in executing this task. Finally, we analyze the costs and benefits of automating preexisting processes of manual data extraction and validation for subsequent cross-report analyses. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Extraction; Knowledge Graphs; Large Language Models; Space Industry; Aerospace Industry; Benchmarking; Metadata; Semantics; Aerospace Manufacturing; Data Extraction; High Complexity; Knowledge Graphs; Language Model; Large Language Model; Manufacturing Companies; Space Design; Space Industry; Test Data; Knowledge Graph},
	keywords = {Aerospace industry; Benchmarking; Metadata; Semantics; Aerospace manufacturing; Data extraction; High complexity; Knowledge graphs; Language model; Large language model; Manufacturing companies; Space design; Space industry; Test data; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Albayrak2025,
	author = {Albayrak, Abdulkadir and Xiao, Yao and Mukherjee, Piyush and Barnett, Sarah S. and Marcou, Cherisse A. and Hart, Steven N.},
	title = {Enhancing human phenotype ontology term extraction through synthetic case reports and embedding-based retrieval: A novel approach for improved biomedical data annotation},
	year = {2025},
	journal = {Journal of Pathology Informatics},
	volume = {16},
	pages = {},
	doi = {10.1016/j.jpi.2024.100409},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211050660&doi=10.1016%2Fj.jpi.2024.100409&partnerID=40&md5=bc5affdcdd2d0c4d4f476b0c6778c2ab},
	abstract = {With the increasing utilization of exome and genome sequencing in clinical and research genetics, accurate and automated extraction of human phenotype ontology (HPO) terms from clinical texts has become imperative. Traditional methods for HPO term extraction, such as PhenoTagger, often face limitations in coverage and precision. In this study, we propose a novel approach that leverages large language models (LLMs) to generate synthetic sentences with clinical context, which were semantically encoded into vector embeddings. These embeddings are linked to HPO terms, creating a robust knowledgebase that facilitates precise information retrieval. Our method circumvents the known issue of LLM hallucinations by storing and querying these embeddings within a true database, ensuring accurate context matching without the need for a predictive model. We evaluated the performance of three different embedding models, all of which demonstrated substantial improvements over PhenoTagger. Top recall (sensitivity), precision (positive-predictive value, PPV), and F1 are 0.64, 0.64, and 0.64, respectively, which were 31%, 10%, and 21% better than PhenoTagger. Furthermore, optimal performance was achieved when we combined the best performing embedding model with PhenoTagger (a.k.a. Fused model), resulting in recall (sensitivity), precision (PPV), and F1 values of 0.7, 0.7, and 0.7, respectively, which are 10%, 10%, and 10% better than the best embedding models. Our findings underscore the potential of this integrated approach to enhance the precision and reliability of HPO term extraction, offering a scalable and effective solution for biomedical data annotation. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Human Phenotype Ontology; Phenotagger; Vector Embeddings},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Vollmers2025174,
	author = {Vollmers, Daniel and Srivastava, Nikit and Zahera, Hamada M. and Moussallem, Diego and Ngonga-Ngomo, Axel Cyrille},
	title = {UniQ-Gen: Unified Query Generation Across Multiple Knowledge Graphs},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15370 LNAI},
	pages = {174 - 189},
	doi = {10.1007/978-3-031-77792-9_11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210886308&doi=10.1007%2F978-3-031-77792-9_11&partnerID=40&md5=95ad88964325e8ec5cd899466dc35219},
	abstract = {Generating SPARQL queries is crucial for extracting relevant information from diverse knowledge graphs. However, the structural and semantic differences among these graphs necessitate training or fine-tuning a tailored model for each one. In this paper, we propose UniQ-Gen, a unified query generation approach to generate SPARQL queries across various knowledge graphs. UniQ-Gen integrates entity recognition, disambiguation, and linking through a BERT-NER model and employs cross-encoder ranking to align questions with the Freebase ontology. We conducted several experiments on different benchmark datasets such as LC-QuAD 2.0, GrailQA, and QALD-10. The evaluation results demonstrate that our approach achieves performance equivalent to or better than models fine-tuned for individual knowledge graphs. This finding suggests that fine-tuning a unified model on a heterogeneous dataset of SPARQL queries across different knowledge graphs eliminates the need for separate models for each graph, thereby reducing resource requirements. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Kgqa; Large Language Models; Question Answering Over Knowledge Graphs; Sparql Generation; Query Languages; Question Answering; Semantics; Structured Query Language; Fine Tuning; Kgqa; Knowledge Graphs; Language Model; Large Language Model; Query Generation; Question Answering; Question Answering Over Knowledge Graph; Sparql Generation; Structural Differences; Knowledge Graph},
	keywords = {Query languages; Question answering; Semantics; Structured Query Language; Fine tuning; KGQA; Knowledge graphs; Language model; Large language model; Query generation; Question Answering; Question answering over knowledge graph; SPARQL generation; Structural differences; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Gnatenko2025453,
	author = {Gnatenko, Anton R. and Kutz, Oliver and Troquard, Nicolas},
	title = {Modelling and Mining Knowledge About Computational Complexity},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15370 LNAI},
	pages = {453 - 470},
	doi = {10.1007/978-3-031-77792-9_27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210875715&doi=10.1007%2F978-3-031-77792-9_27&partnerID=40&md5=747423292a38be6e7a405a0645fca26d},
	abstract = {We present an ontology of computational complexity that allows for a representation of research findings on the subject and supports query answering and reasoning tasks to help students and researchers in finding known facts and deriving new ones. The facts about decision problems and complexity classes are organised as a knowledge graph. The relationships between them are axiomatised using the FOWL framework that allows one to combine OWL 2 and first-order logic to balance between reasoning efficiency and expressive power. While the axioms were created through ontological analysis based on received ‘textbook knowledge’, the facts were extracted from the textual corpus of the ‘Complexity Zoo’ website, a human-curated ‘encyclopedia’ of complexity classes, in a human-supervised process employing large language models. We discuss, on the one hand, the modelling choices in relation to the previous work on knowledge representation in mathematics and, on the other hand, the peculiarities of using language models for the mining of complex symbolic facts. Finally, we illustrate some of the features of the hybrid reasoning system by providing a usage example. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Complexity; Domain Modelling; Knowledge Mining; Llm; Representation Of Mathematical Knowledge; Domain Knowledge; Modeling Languages; Ontology; Complexity Class; Domain Model; Knowledge Mining; Language Model; Llm; Mathematical Knowledge; Ontology's; Query Answering; Reasoning Tasks; Representation Of Mathematical Knowledge; Knowledge Graph},
	keywords = {Domain Knowledge; Modeling languages; Ontology; Complexity class; Domain model; Knowledge mining; Language model; LLM; Mathematical knowledge; Ontology's; Query answering; Reasoning tasks; Representation of mathematical knowledge; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Carriero2025434,
	author = {Carriero, Valentina Anita and Azzini, Antonia and Baroni, Ilaria and Scrocca, Mario and Celino, Irene},
	title = {Human Evaluation of Procedural Knowledge Graph Extraction from Text with Large Language Models},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15370 LNAI},
	pages = {434 - 452},
	doi = {10.1007/978-3-031-77792-9_26},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210875567&doi=10.1007%2F978-3-031-77792-9_26&partnerID=40&md5=40478d78cabafd522990d9941c73faa3},
	abstract = {Procedural Knowledge is the know-how expressed in the form of sequences of steps needed to perform some tasks. Procedures are usually described by means of natural language texts, such as recipes or maintenance manuals, possibly spread across different documents and systems, and their interpretation and subsequent execution is often left to the reader. Representing such procedures in a Knowledge Graph (KG) can be the basis to build digital tools to support those users who need to apply or execute them. In this paper, we leverage Large Language Model (LLM) capabilities and propose a prompt engineering approach to extract steps, actions, objects, equipment and temporal information from a textual procedure, in order to populate a Procedural KG according to a pre-defined ontology. We evaluate the KG extraction results by means of a user study, in order to qualitatively and quantitatively assess the perceived quality and usefulness of the LLM-extracted procedural knowledge. We show that LLMs can produce outputs of acceptable quality and we assess the subjective perception of AI by human evaluators. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Engineering; Knowledge Extraction; Knowledge Graphs; Large Language Models; Ontology; Procedural Knowledge; Natural Language Processing Systems; Ontology; Graph Extractions; Human Evaluation; Knowledge Extraction; Knowledge Graphs; Language Model; Large Language Model; Maintenance Manual; Natural Languages Texts; Ontology's; Procedural Knowledge; Knowledge Graph},
	keywords = {Natural language processing systems; Ontology; Graph extractions; Human evaluation; Knowledge extraction; Knowledge graphs; Language model; Large language model; Maintenance manual; Natural languages texts; Ontology's; Procedural knowledge; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Capshaw2025399,
	author = {Capshaw, Riley and Blomqvist, Eva},
	title = {Contextualizing Entity Representations for Zero-Shot Relation Extraction with Masked Language Models},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15370 LNAI},
	pages = {399 - 415},
	doi = {10.1007/978-3-031-77792-9_24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210848015&doi=10.1007%2F978-3-031-77792-9_24&partnerID=40&md5=45282001afe6225ccaee18bde89df536},
	abstract = {Knowledge graphs (KGs) and their related ontologies constitute a key component in modern knowledge-based systems. However, hand-crafting these is not scalable, particularly due to the rate at which knowledge changes in many real-world applications. Partially automating the process of extracting and even modelling knowledge has therefore been a subject of research for many years. Nevertheless, accurate and reliable KG construction from natural language documents still remains a difficult task with many challenges, even in light of the impressive recent advances in language modelling. This paper focuses on one of those challenges, namely the extraction of accurate entity representations from text documents in order to facilitate relation extraction (RE). We present a novel method for generating document-contextualized input representations for entities using a masked language model (MLM) without the need for any sort of fine-tuning. These representations are then used as inputs to the same MLM that generated them, alleviating the need to include entire documents when prompting. Our results show that these representations 1) improve the ability of the MLMs BERT and RoBERTa to identify statements that represent correct relations between two entities; and 2) allow BERT to perform on par with the fine-tuned MLMs BioBERT and PubMedBERT. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Document-level Relation Extraction; Entity Embedding; Knowledge Graphs; Machine Reading; Masked Language Models; Graph Embeddings; Modeling Languages; Natural Language Processing Systems; Ontology; Document-level Relation Extraction; Embeddings; Entity Embedding; Knowledge Graphs; Knowledge-based Systems; Language Model; Machine Reading; Masked Language Model; Ontology's; Relation Extraction; Knowledge Graph},
	keywords = {Graph embeddings; Modeling languages; Natural language processing systems; Ontology; Document-level relation extraction; Embeddings; Entity embedding; Knowledge graphs; Knowledge-based systems; Language model; Machine reading; Masked language model; Ontology's; Relation extraction; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {24th International Conference on Knowledge Engineering and Knowledge Management, EKAW 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15370 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210830354&partnerID=40&md5=d375d5519e08237fdbfd4b4d7c6290ec},
	abstract = {The proceedings contain 28 papers. The special focus in this conference is on Knowledge Engineering and Knowledge Management. The topics include: Influence Beyond Similarity: A Contrastive Learning Approach to Object Influence Retrieval; Discovering a Representative Set of Link Keys in RDF Datasets; understanding the Impact of Entity Linking on the Topology of Entity Co-occurrence Networks for Social Media Analysis; Empowering CamemBERT Legal Entity Extraction With LLM Boostrapping; Lexicalization Is All You Need: Examining the Impact of Lexical Knowledge in a Compositional QALD System; on the Roles of Competency Questions in Ontology Engineering; structured Representations for Narratives; comparing Symbolic and Embedding-Based Approaches for Relational Blocking; uniQ-Gen: Unified Query Generation Across Multiple Knowledge Graphs; LLM-Driven Knowledge Extraction in Temporal and Description Logics; FaVEL: Fact Validation Ensemble Learning; a Framework for Evaluating Entity Alignment Impact on Downstream Knowledge Discovery; Scholarly Wikidata: Population and Exploration of Conference Data in Wikidata Using LLMs; understanding Inflicted Injuries in Young Children: Toward an Ontology Based Approach; a Review and Comparison of Competency Question Engineering Approaches; A Generic Framework to Better Understand and Compare FAIRness Measures; ORKA: An Ontology for Robotic Knowledge Acquisition; transformers in the Service of Description Logic-Based Contexts; additive Counterfactuals for Explaining Link Predictions on Knowledge Graphs; peGazUs: A Knowledge Graph Based Approach to Build Urban Perpetual Gazetteers; ontology-Constrained Generation of Domain-Specific Clinical Summaries; contextualizing Entity Representations for Zero-Shot Relation Extraction with Masked Language Models; validating a Functional Status Knowledge Graph in a Large-Scale Living Lab; human Evaluation of Procedural Knowledge Graph Extraction from Text with Large Language Models; modelling and Mining Knowledge About Computational Complexity; generating a Question Answering Dataset About Geographic Changes in a Knowledge Graph. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {25th International Conference on Intelligent Data Engineering and Automated Learning, IDEAL 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15347 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210813148&partnerID=40&md5=2e4f52e2edd5b7e0f6ddfa3e830d50c9},
	abstract = {The proceedings contain 92 papers. The special focus in this conference is on Intelligent Data Engineering and Automated Learning. The topics include: Model-Based Meta-reinforcement Learning for Hyperparameter Optimization; towards Sustainable Precision: Machine Learning for Laser Micromachining Optimization; association Rules Mining with Auto-encoders; using Contrastive Learning to Map Stylistic Similarities in Narrative Writers; automatic Classification of Signal and Noise in Functional Magnetic Resonance Imaging Scans Using Convolutional Neural Networks; how Resilient are Language Models to Text Perturbations?; emotional Sequential Influence Modeling on False Information; CSSDH: An Ontology for Social Determinants of Health to Operational Continuity of Care Data Interoperability; padel Two-Dimensional Tracking Extraction from Monocular Video Recordings; drowsiness Detection Using Vital Sign Sensors and Deep Learning on Smartwatches; Benchmarking Out of the Box Open-Source LLMs for Malware Detection Based on API Calls Sequences; multimodal Visio-Lingual Content Analysis to Detect Fake Content on Reddit; MetaLIRS: Meta-learning for Imputation and Regression Selection; pipeline for Semantic Segmentation of Large Railway Point Clouds; preliminary Investigation on Machine Learning and Deep Learning Models for Change of Direction Classification in Running; efficient Radar Scheduling Using Genetic Algorithms and Stochastic Heuristic Initialization; towards a Communication Specification Language for Heterogeneous Service Orchestration Based on Process Calculus and Holonic Multi-agent Systems; counterfactual Explanations for Sustainable Tourism Indicators; Tracking Healthy Organs in Medical Scans to Improve Cancer Treatment by Using UW-Madison GI Tract Image Segmentation; low Consumption Models for Disease Diagnosis in Isolated Farms; fast and Scalable Recommendation Retrieval Model with Mixed Attention and Knowledge Distillation; Federated Learning for Vietnamese SMS Spam Detection Using Pre-trained PhoBERT; deep Learning Inference on Edge: A Preliminary Device Comparison; causal Explanation of Graph Neural Networks; the Contribution of Social Sciences Driven User Studies to the Development of Human-Centered Artificial Intelligence; towards Reliable Drift Detection and Explanation in Text Data. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mehenni2025382,
	author = {Mehenni, Gaya and Zouaq, Amal},
	title = {Ontology-Constrained Generation of Domain-Specific Clinical Summaries},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15370 LNAI},
	pages = {382 - 398},
	doi = {10.1007/978-3-031-77792-9_23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210803481&doi=10.1007%2F978-3-031-77792-9_23&partnerID=40&md5=bda18407cd7b691dcd4c13ec111584bc},
	abstract = {Large Language Models (LLMs) offer promising solutions for text summarization. However, some domains require specific information to be available in the summaries. Generating these domain-adapted summaries is still an open challenge. Similarly, hallucinations in generated content is a major drawback of current approaches, preventing their deployment. This study proposes a novel approach that leverages ontologies to create domain-adapted summaries both structured and unstructured. We employ an ontology-guided constrained decoding process to reduce hallucinations while improving relevance. When applied to the medical domain, our method shows potential in summarizing Electronic Health Records (EHRs) across different specialties, allowing doctors to focus on the most relevant information to their domain. Evaluation on the MIMIC-III dataset demonstrates improvements in generating domain-adapted summaries of clinical notes and hallucination reduction. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology; 'current; Constrained Decoding; Decoding Process; Domain Specific; Electronic Health; Language Model; Medical Domains; Ontology's; Specific Information; Text Summarisation; Electronic Health Record},
	keywords = {Ontology; 'current; Constrained decoding; Decoding process; Domain specific; Electronic health; Language model; Medical domains; Ontology's; Specific information; Text Summarisation; Electronic health record},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mihindukulasooriya2025243,
	author = {Mihindukulasooriya, Nandana and Tiwari, Sanju M. and Dobriy, Daniil and Nielsen, Finn Årup Årup and Chhetri, Tek Raj and Polleres, Axel Florian},
	title = {Scholarly Wikidata: Population and Exploration of Conference Data in Wikidata Using LLMs},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15370 LNAI},
	pages = {243 - 259},
	doi = {10.1007/978-3-031-77792-9_15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210802375&doi=10.1007%2F978-3-031-77792-9_15&partnerID=40&md5=f224d891a9ed43be2bfaf25defb185ea},
	abstract = {Several initiatives have been undertaken to conceptually model the domain of scholarly data using ontologies and to create respective Knowledge Graphs. Yet, the full potential seems unleashed, as automated means for automatic population of said ontologies are lacking, and respective initiatives from the Semantic Web community are not necessarily connected: we propose to make scholarly data more sustainably accessible by leveraging Wikidata’s infrastructure and automating its population in a sustainable manner through LLMs by tapping into unstructured sources like conference Web sites and proceedings texts as well as already existing structured conference datasets. While an initial analysis shows that Semantic Web conferences are only minimally represented in Wikidata, we argue that our methodology can help to populate, evolve and maintain scholarly data as a community within Wikidata. Our main contributions include (a) an analysis of ontologies for representing scholarly data to identify gaps and relevant entities/properties in Wikidata, (b) semi-automated extraction – requiring (minimal) manual validation – of conference metadata (e.g., acceptance rates, organizer roles, programme committee members, best paper awards, keynotes, and sponsors) from websites and proceedings texts using LLMs. Finally, we discuss (c) extensions to visualization tools in the Wikidata context for data exploration of the generated scholarly data. Our study focuses on data from 105 Semantic Web-related conferences and extends/adds more than 6000 entities in Wikidata. It is important to note that the method can be more generally applicable beyond Semantic Web-related conferences for enhancing Wikidata’s utility as a comprehensive scholarly resource. Source Repository: https://github.com/scholarly-wikidata/  DOI:https://doi.org/10.5281/zenodo.10989709  License: Creative Commons CC0 (Data), MIT (Code). © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Model; Scholarly Data; Wikidata; Data Centers; Ontology; Semantics; Spatio-temporal Data; Automatic Populations; Community Is; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Scholarly Data; Semantic-web; Web Community; Wikidata; Data Assimilation},
	keywords = {Data centers; Ontology; Semantics; Spatio-temporal data; Automatic populations; Community IS; Knowledge graphs; Language model; Large language model; Ontology's; Scholarly data; Semantic-Web; Web community; Wikidata; Data assimilation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{2025,
	title = {25th International Conference on Intelligent Data Engineering and Automated Learning, IDEAL 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15346 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210527822&partnerID=40&md5=332dbaf2cb464fb7e860844d2825b95a},
	abstract = {The proceedings contain 92 papers. The special focus in this conference is on Intelligent Data Engineering and Automated Learning. The topics include: Model-Based Meta-reinforcement Learning for Hyperparameter Optimization; towards Sustainable Precision: Machine Learning for Laser Micromachining Optimization; association Rules Mining with Auto-encoders; using Contrastive Learning to Map Stylistic Similarities in Narrative Writers; automatic Classification of Signal and Noise in Functional Magnetic Resonance Imaging Scans Using Convolutional Neural Networks; how Resilient are Language Models to Text Perturbations?; emotional Sequential Influence Modeling on False Information; CSSDH: An Ontology for Social Determinants of Health to Operational Continuity of Care Data Interoperability; padel Two-Dimensional Tracking Extraction from Monocular Video Recordings; drowsiness Detection Using Vital Sign Sensors and Deep Learning on Smartwatches; Benchmarking Out of the Box Open-Source LLMs for Malware Detection Based on API Calls Sequences; multimodal Visio-Lingual Content Analysis to Detect Fake Content on Reddit; MetaLIRS: Meta-learning for Imputation and Regression Selection; pipeline for Semantic Segmentation of Large Railway Point Clouds; preliminary Investigation on Machine Learning and Deep Learning Models for Change of Direction Classification in Running; efficient Radar Scheduling Using Genetic Algorithms and Stochastic Heuristic Initialization; towards a Communication Specification Language for Heterogeneous Service Orchestration Based on Process Calculus and Holonic Multi-agent Systems; counterfactual Explanations for Sustainable Tourism Indicators; Tracking Healthy Organs in Medical Scans to Improve Cancer Treatment by Using UW-Madison GI Tract Image Segmentation; low Consumption Models for Disease Diagnosis in Isolated Farms; fast and Scalable Recommendation Retrieval Model with Mixed Attention and Knowledge Distillation; Federated Learning for Vietnamese SMS Spam Detection Using Pre-trained PhoBERT; deep Learning Inference on Edge: A Preliminary Device Comparison; causal Explanation of Graph Neural Networks; the Contribution of Social Sciences Driven User Studies to the Development of Human-Centered Artificial Intelligence; towards Reliable Drift Detection and Explanation in Text Data. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hu202582,
	author = {Hu, Mengzhou and Alkhairy, Sahar and Lee, Ingoo and Pillich, Rudolf Tito and Fong, Dylan and Smith, Kevin and Bachelder, Robin E. and Ideker, Trey E. and Pratt, Dexter},
	title = {Evaluation of large language models for discovery of gene set function},
	year = {2025},
	journal = {Nature Methods},
	volume = {22},
	number = {1},
	pages = {82 - 91},
	doi = {10.1038/s41592-024-02525-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210471684&doi=10.1038%2Fs41592-024-02525-x&partnerID=40&md5=8fcc56ee8fdc1af8690edea6f5598808},
	abstract = {Gene set enrichment is a mainstay of functional genomics, but it relies on gene function databases that are incomplete. Here we evaluate five large language models (LLMs) for their ability to discover the common functions represented by a gene set, supported by molecular rationale and a self-confidence assessment. For curated gene sets from Gene Ontology, GPT-4 suggests functions similar to the curated name in 73% of cases, with higher self-confidence predicting higher similarity. Conversely, random gene sets correctly yield zero confidence in 87% of cases. Other LLMs (GPT-3.5, Gemini Pro, Mixtral Instruct and Llama2 70b) vary in function recovery but are falsely confident for random sets. In gene clusters from omics data, GPT-4 identifies common functions for 45% of cases, fewer than functional enrichment but with higher specificity and gene coverage. Manual review of supporting rationale and citations finds these functions are largely verifiable. These results position LLMs as valuable omics assistants. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Glucose 6 Phosphate; Glucose 6 Phosphate; Article; Cd8+ T Lymphocyte; Cell Differentiation; Cellular Stress Response; Differential Gene Expression; Energy Metabolism; Exocytosis; Functional Genomics; Gene Ontology; Gene Set Enrichment Analysis; Generative Pretrained Transformer; Glucose Homeostasis; Glucose Metabolism; Glucose Transport; Homeostasis; Human; Ion Transport; Iron Homeostasis; Large Language Model; Neurotransmitter Release; Pentose Phosphate Cycle; Prompt Engineering; Protein Degradation; Protein Quality; Proteomics; Quality Control; Synapse Vesicle; T Lymphocyte Differentiation; Temperature; Transcriptomics; Ubiquitination; Wnt Signaling; Bioinformatics; Genetic Database; Genomics; Procedures; Computational Biology; Databases, Genetic; Gene Ontology; Genomics; Humans},
	keywords = {glucose 6 phosphate; Article; CD8+ T lymphocyte; cell differentiation; cellular stress response; differential gene expression; energy metabolism; exocytosis; functional genomics; gene ontology; gene set enrichment analysis; generative pretrained transformer; glucose homeostasis; glucose metabolism; glucose transport; homeostasis; human; ion transport; iron homeostasis; large language model; neurotransmitter release; pentose phosphate cycle; prompt engineering; protein degradation; protein quality; proteomics; quality control; synapse vesicle; T lymphocyte differentiation; temperature; transcriptomics; ubiquitination; Wnt signaling; bioinformatics; genetic database; genomics; procedures; Computational Biology; Databases, Genetic; Gene Ontology; Genomics; Humans},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Biester202568,
	author = {Biester, Fabian and Abdelaal, Mohamed and del Gaudio, Daniel},
	title = {LLMClean: Context-Aware Tabular Data Cleaning via LLM-Generated OFDs},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2186 CCIS},
	pages = {68 - 78},
	doi = {10.1007/978-3-031-70421-5_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210170210&doi=10.1007%2F978-3-031-70421-5_7&partnerID=40&md5=b32c35ed9b78dc6b524690657e86eaec},
	abstract = {Data cleaning tools, particularly those that exploit functional dependencies within ontological frameworks or context models, are instrumental in augmenting data quality. Nevertheless, crafting these context models is a demanding task, both in terms of resources and expertise, often necessitating specialized knowledge from domain experts. This paper introduces an innovative approach, called LLMClean, for the automated generation of context models, utilizing Large Language Models to analyze and understand various datasets. LLMClean encompasses a sequence of actions, starting with categorizing the dataset, extracting or mapping relevant models, and ultimately synthesizing the context model. To demonstrate its potential, we have developed and tested a prototype that applies our approach to three distinct datasets from the Internet of Things, healthcare, and Industry 4.0 sectors. The results of our evaluation indicate that LLMClean can achieve data cleaning efficacy comparable with that of context models crafted by human experts and with state-of-the-art data cleaning tools. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cleaning Tools; Context Models; Context-aware; Data Cleaning; Data Quality; Framework Models; Functional Dependency; Ontological Frameworks; Specialized Knowledge; Tabular Data},
	keywords = {Cleaning tools; Context models; Context-Aware; Data cleaning; Data quality; Framework models; Functional dependency; Ontological frameworks; Specialized knowledge; Tabular data},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Piyathilake2025206,
	author = {Piyathilake, Vinuri and Dilni, Thuthi and Pushpananda, Randil and de Silva, Lasanthi N.C. and Zaheed, Yumna},
	title = {Towards a Conversational AI Chatbot to Assist Farmers in Disease Detection},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {14967 LNAI},
	pages = {206 - 217},
	doi = {10.1007/978-3-031-73497-7_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210144021&doi=10.1007%2F978-3-031-73497-7_17&partnerID=40&md5=29762a9804731fbbab33aa766d91c9fd},
	abstract = {Agricultural diseases exert a profound impact on both crop yield and quality, leading to substantial losses in global food production. Consequently, the need for timely disease recognition has become increasingly evident throughout the cultivation process. However, a major challenge lies in the limited expertise of farmers to identify diseases at an early stage. Furthermore, even if changes in crops are detected, there may be a lack of knowledge regarding appropriate remedies. Resorting to expert consultations for on-site assessments can be time-consuming, risking the widespread transmission of infectious diseases. In order to bridge the knowledge gap between experts and farmers, the utilization of a conversational agent, which can provide continuous multilingual personalized support, has emerged as a promising avenue. Thus, this paper provides a comprehensive overview of existing agricultural chatbots, detailing their architectural designs and methods for handling the required knowledge. Recognizing a significant gap between the requirements for a personalized multilingual chatbot for plant disease detection and existing solutions, the study proposes a hybrid model that integrates an ontology-based knowledgebase, open-source frameworks, and a large language model-based architecture to develop a more intelligent chatbot for detecting paddy diseases in Sri Lanka. The ultimate goal of this work is to develop a voice-enabled agricultural assistant, promoting inclusivity and user-friendliness for farmers with limited digital literacy in the future. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Conversational Agents; Large Language Models; Ontology-based Knowledgebase; Personalized Chatbots; Plant Disease Detection; Precision Agriculture; Ontology; Chatbots; Conversational Agents; Disease Detection; Knowledge Basis (kbs); Language Model; Large Language Model; Ontology-based; Ontology-based Knowledgebase; Personalized Chatbot; Plant Disease; Plant Disease Detection; Precision Agriculture},
	keywords = {Ontology; Chatbots; Conversational agents; Disease detection; Knowledge basis (KBs); Language model; Large language model; Ontology-based; Ontology-based knowledgebase; Personalized chatbot; Plant disease; Plant disease detection; Precision Agriculture},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Filippone2025163,
	author = {Filippone, Giuseppe and La Rosa, Gianmarco and Tabacchi, Marco Elio},
	title = {SDF-FuzzIA: A Fuzzy-Ontology Based Plug-in for the Intelligent Analysis of Geo-Thematic Data},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15350 LNAI},
	pages = {163 - 169},
	doi = {10.1007/978-3-031-76235-2_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210099435&doi=10.1007%2F978-3-031-76235-2_13&partnerID=40&md5=fdba09b589cb65dbea2d76a83d72c0d6},
	abstract = {This short paper presents a description of SDF-FuzzIA, a Fuzzy-Ontology LLM-based system for the intelligent analysis of geo-thematic data that serves as a plug-in to the Sustainability Decision Framework (SDF) Decision Support System (DSS). A description of the components implemented in the system is given, followed by an explanation of the interaction between the components and the main system. As this still is a work in progress, future directions and possible hurdles are explored. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Fuzzy Logic; Fuzzy Ontology; Soft Computing; Ontology; Decision Framework; Decision Supports; Fuzzy Ontology; Fuzzy-logic; Intelligent Analysis; Ontology-based; Plug-ins; Soft-computing; Support Systems; Thematic Data; Fuzzy Logic},
	keywords = {Ontology; Decision framework; Decision supports; Fuzzy ontology; Fuzzy-Logic; Intelligent analysis; Ontology-based; Plug-ins; Soft-Computing; Support systems; Thematic data; Fuzzy logic},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Prokop2025235,
	author = {Prokop, Dominik and Stenchlak, Stepan and Škoda, Petr and Klímek, Jakub and Nečaský, Martin},
	title = {Enhancing Domain Modeling with Pre-trained Large Language Models: An Automated Assistant for Domain Modelers},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15238 LNCS},
	pages = {235 - 253},
	doi = {10.1007/978-3-031-75872-0_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209593844&doi=10.1007%2F978-3-031-75872-0_13&partnerID=40&md5=e6d5452beb7ac4d1e49791e8503ffb11},
	abstract = {Domain modeling involves creating abstract representations of information within a specific domain using techniques such as conceptual modeling and ontology engineering. Traditionally, manual creation and maintenance of domain models are labor intensive and require modeling expertise. This paper explores the automation of domain modeling using pre-trained large language models (LLMs), presenting an experimental LLM-based conceptual modeling assistant that collaborates with a human expert. The assistant provides modeling suggestions based on a given textual description of the domain of interest, aiding in the design of classes, attributes, and associations. We present a generic framework for domain modeling assistants that consists of class, attribute, and association generators, and show how they can be implemented using an LLM. We demonstrate a concrete configuration of this framework and its prototype implementation. We evaluated the effectiveness of the framework configuration across various domains. Our findings indicate that the assistant significantly enhances the efficiency of modeling while maintaining reasonable quality of the outputs. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Conceptual Modeling; Domain Modeling; Domain Modeling Automation; Large Language Models; Abstract Representation; Conceptual Model; Domain Model; Domain Modeling Automation; Language Model; Large Language Model; Model Engineering; Modeling Automation; Ontology Engineering; Modeling Languages},
	keywords = {Abstract representation; Conceptual model; Domain model; Domain modeling automation; Language model; Large language model; Model engineering; Modeling automation; Ontology engineering; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{2025,
	title = {43rd International Conference on Conceptual Modeling, ER 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15238 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209551458&partnerID=40&md5=8beac58109dd074a36359432bcbc8d8a},
	abstract = {The proceedings contain 22 papers. The special focus in this conference is on Conceptual Modeling. The topics include: A Universal Prompting Strategy for Extracting Process Model Information from Natural Language Text Using Large Language Models; agent System Event Data: Concepts, Dimensions, Applications; Multi-faceted Evaluation of Modeling Languages for Augmented Reality Applications The Case of ARWFML; Application of the Tree-of-Thoughts Framework to LLM-Enabled Domain Modeling; ECQL: Towards Succinct and Extensible Modeling of Multi-model Query Results; An Analysis of the Semantic Foundation of KerML and SysML v2; portions of Matter and Their Existential Events: An Ontology-Based Conceptual Model; model-Driven Design and Generation of Training Simulators for Reinforcement Learning; generating Secure Workflow Designs from Requirements Goal Models Using Patterns; modeling and Reasoning About Explanation Requirements Using Goal Models; enhancing Domain Modeling with Pre-trained Large Language Models: An Automated Assistant for Domain Modelers; How are LLMs Used for Conceptual Modeling? An Exploratory Study on Interaction Behavior and User Perception; Small, Medium, and Large Language Models for Text-to-SQL; Establishing Traceability Between Natural Language Requirements and Software Artifacts by Combining RAG and LLMs; GenACT: An Ontology-Based Temporal Web Data Generator; SAQI: An Ontology Based Knowledge Graph Platform for Social Air Quality Index; Conceptual Framework for Designing Hippocratic APIs; the Role-Artifact-Function Framework for Understanding Digital Identity Models; ontological Foundations of Resilience; conceptual Modelling Method for Digital Twins. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Dalal20251333,
	author = {Dalal, Sumit and Tilwani, Deepa and Gaur, Manas and Jain, Sarika and Shalin, Valerie L. and Sheth, Amit P.},
	title = {A Cross Attention Approach to Diagnostic Explainability Using Clinical Practice Guidelines for Depression},
	year = {2025},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	volume = {29},
	number = {2},
	pages = {1333 - 1342},
	doi = {10.1109/JBHI.2024.3483577},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207401106&doi=10.1109%2FJBHI.2024.3483577&partnerID=40&md5=02c883078df94d7533d7f54f516e483e},
	abstract = {The lack of explainability in using relevant clinical knowledge hinders the adoption of artificial intelligence-powered analysis of unstructured clinical dialogue. A wealth of relevant, untapped Mental Health (MH) data is available in online communities, providing the opportunity to address the explainability problem with substantial potential impact as a screening tool for both online and offline applications. Inspired by how clinicians rely on their expertise when interacting with patients, we leverage relevant clinical knowledge to classify and explain depression-related data, reducing manual review time and engendering trust. We developed a method to enhance attention in contemporary transformer models and generate explanations for classifications that are understandable by mental health practitioners (MHPs) by incorporating external clinical knowledge. We propose a domain-general architecture called ProcesS knowledge-infused cross ATtention (PSAT) that incorporates clinical practice guidelines (CPG) when computing attention. We transform a CPG resource focused on depression, such as the Patient Health Questionnaire (e.g. PHQ-9) and related questions, into a machine-readable ontology using SNOMED-CT. With this resource, PSAT enhances the ability of models like GPT-3.5 to generate application-relevant explanations. Evaluation of four expert-curated datasets related to depression demonstrates PSAT's application-relevant explanations. PSAT surpasses the performance of twelve baseline models and can provide explanations where other baselines fall short. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cross Attention; Depression; Explainable; Language Models; Mental Health; Phq-9; Diagnosis; Clinical Knowledge; Clinical Practice Guidelines; Cross Attention; Depression; Explainable; Health Data; Language Model; Mental Health; On-line Communities; Phq-9; Electronic Health Record; Article; Artificial Intelligence; Attention; Clinical Practice Guideline; Columbia Suicide Risk Severity Scale; Convolutional Neural Network; Cross Attention Approach; Depression; Diagnostic Test Accuracy Study; Health Practitioner; Human; Human Experiment; Knowledge; Language Model; Long Short Term Memory Network; Mental Health; Patient Health Questionnaire 9; Receiver Operating Characteristic; Scoring System; Suicidal Ideation; Suicide; Suicide Attempt; Systematized Nomenclature Of Medicine},
	keywords = {Diagnosis; Clinical knowledge; Clinical practice guidelines; Cross attention; Depression; Explainable; Health data; Language model; Mental health; On-line communities; PHQ-9; Electronic health record; Article; artificial intelligence; attention; clinical practice guideline; columbia suicide risk severity scale; convolutional neural network; cross attention approach; depression; diagnostic test accuracy study; health practitioner; human; human experiment; knowledge; language model; long short term memory network; mental health; Patient Health Questionnaire 9; receiver operating characteristic; scoring system; suicidal ideation; suicide; suicide attempt; Systematized Nomenclature of Medicine},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Jeon2025,
	author = {Jeon, Daeseong and Lee, Changyong},
	title = {Grouping research proposals with funding agency requirements: A contextualized language model and constrained K-means clustering approach},
	year = {2025},
	journal = {Expert Systems with Applications},
	volume = {259},
	pages = {},
	doi = {10.1016/j.eswa.2024.125242},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203014028&doi=10.1016%2Fj.eswa.2024.125242&partnerID=40&md5=a4575b34a7ffcad7fef576a999d1de53},
	abstract = {As the volume of research proposals increases and the interdisciplinary nature of research fields exacerbates the complexity of manual proposal grouping, the grouping of research proposals becomes increasingly vital in funding agencies’ evaluation procedures. Although previous ontology- and word embedding-based approaches have made valuable contributions to the advancement of research proposal grouping, their practical utility has been limited due to a lack of consideration of funding agencies’ requirements. This study proposes a systematic approach for grouping research proposals that aligns with three common requirements: size, cannot-link, and must-link constraints. The proposed approach utilizes KLUE-RoBERTa for proposal vectorization and constrained K-means clustering for proposal grouping with size constraints. We introduce a proposal pre-partitioning and a proposal vector centralization to simultaneously consider the cannot- and must-link constraints in grouping proposals. An empirical analysis of 3,665 proposals submitted to the National Research Foundation of Korea demonstrates the effectiveness and practicality of the proposed approach. Additionally, we conduct a comparative analysis of various combinations of methodological components to optimize this approach. The proposed approach is considered a valuable complementary tool for grouping proposals, enhancing the overall efficiency and effectiveness of the proposal evaluation system. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Constrained K-means Clustering; Contextualized Language Model; Funding Agency Requirements; Research Proposal Grouping; K-means Clustering; Cannot Links; Constrained K-mean Clustering; Contextualized Language Model; Funding Agencies; Funding Agency Requirement; K-means++ Clustering; Language Model; Must Links; Research Proposal Grouping; Research Proposals; Decentralized Finance},
	keywords = {K-means clustering; Cannot links; Constrained K-mean clustering; Contextualized language model; Funding agencies; Funding agency requirement; K-means++ clustering; Language model; Must links; Research proposal grouping; Research proposals; Decentralized finance},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Rezayi20251235,
	author = {Rezayi, Saed and Liu, Zhengliang and Wu, Zihao and Dhakal, Chandra K. and Ge, Bao and Dai, Haixing and Mai, Gengchen and Liu, Ninghao and Zhen, Chen and Liu, Tianming},
	title = {Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications},
	year = {2025},
	journal = {IEEE Transactions on Big Data},
	volume = {11},
	number = {3},
	pages = {1235 - 1246},
	doi = {10.1109/TBDATA.2024.3442542},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201463745&doi=10.1109%2FTBDATA.2024.3442542&partnerID=40&md5=5b2de44b44642ac5f1b62bf16a120c7f},
	abstract = {This paper explores new frontiers in agricultural natural language processing (NLP) by investigating the effectiveness of food-related text corpora for pretraining transformer-based language models. Specifically, we focus on semantic matching, establishing mappings between food descriptions and nutrition data through fine-tuning AgriBERT with the FoodOn ontology. Our work introduces an expanded comparison with state-of-the-art language models such as GPT-4, Mistral-large, Claude 3 Sonnet, and Gemini 1.0 Ultra. This exploratory investigation, rather than a direct comparison, aims to understand how AgriBERT, a domain-specific, fine-tuned, open-source model, complements the broad knowledge and generative abilities of these advanced LLMs in addressing the unique challenges of the agricultural sector. We also experiment with other applications, such as cuisine prediction from ingredients, expanding our research to include various NLP tasks beyond semantic matching. Overall, this paper underscores the potential of integrating domain-specific models like AgriBERT with advanced LLMs to enhance the performance and applicability of agricultural NLP applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Food Applications; Language Models; Natural Language Processing; Semantic Matching; Semantics; Biological System Modeling; Chatgpt; Context Models; Food Applications; Language Model; Language Processing; Natural Language Processing; Natural Languages; Semantic Matching; Task Analysis; Modeling Languages},
	keywords = {Semantics; Biological system modeling; ChatGPT; Context models; Food applications; Language model; Language processing; Natural language processing; Natural languages; Semantic matching; Task analysis; Modeling languages},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@ARTICLE{Turner2025,
	author = {Turner, Matthew D. and Appaji, Abhishek and Ar Rakib, Nibras and Golnari, Pedram and Rajasekar, Arcot K. and Anitha Rathnam, K. V. and Sahoo, Satya Sanket and Wang, Yue and Wang, Lei and Turner, Jessica A.},
	title = {Large language models can extract metadata for annotation of human neuroimaging publications},
	year = {2025},
	journal = {Frontiers in Neuroinformatics},
	volume = {19},
	pages = {},
	doi = {10.3389/fninf.2025.1609077},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014802944&doi=10.3389%2Ffninf.2025.1609077&partnerID=40&md5=3020e2abb7aa83cb2facfe83feb2b1c1},
	abstract = {We show that recent (mid-to-late 2024) commercial large language models (LLMs) are capable of good quality metadata extraction and annotation with very little work on the part of investigators for several exemplar real-world annotation tasks in the neuroimaging literature. We investigated the GPT-4o LLM from OpenAI which performed comparably with several groups of specially trained and supervised human annotators. The LLM achieves similar performance to humans, between 0.91 and 0.97 on zero-shot prompts without feedback to the LLM. Reviewing the disagreements between LLM and gold standard human annotations we note that actual LLM errors are comparable to human errors in most cases, and in many cases these disagreements are not errors. Based on the specific types of annotations we tested, with exceptionally reviewed gold-standard correct values, the LLM performance is usable for metadata annotation at scale. We encourage other research groups to develop and make available more specialized “micro-benchmarks,” like the ones we provide here, for testing both LLMs, and more complex agent systems annotation performance in real-world metadata annotation tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Document Annotation; Human Neuroimaging; Information Extraction; Large Language Models; Metadata Annotation; Ontologies; Text Mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2025,
	title = {6th Conference on Health Inference and Learning, CHIL 2025},
	year = {2025},
	journal = {Proceedings of Machine Learning Research},
	volume = {287},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014755343&partnerID=40&md5=c5c521d6e2abae57c3fd0a12f743775d},
	abstract = {The proceedings contain 43 papers. The special focus in this conference is on Health Inference and Learning. The topics include: KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings; medMod: Multimodal Benchmark for Medical Prediction Tasks with Electronic Health Records and Chest X-Ray Scans; multi-Objective Fine-Tuning of Clinical Scoring Tables: Adapting to Variations in Demography and Data; LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records; ExOSITO: Explainable Off-Policy Learning with Side Information for Intensive Care Unit Blood Test Orders; diagnosing our datasets: How does my language model learn clinical information?; learning Interactions Between Continuous Treatments and Covariates with a Semiparametric Model; distributionally Robust Learning in Survival Analysis; a Study of Artifacts on Melanoma Classification under Diffusion-Based Perturbations; test-Time Calibration: A Framework for Personalized Test-Time Adaptation in Real-World Biosignals; multiaccuracy for Subpopulation Calibration Over Distribution Shift in Medical Prediction Models; feasibility of Immersive Virtual Reality and Customized Robotics with Wearable Sensors for Upper Extremity Training; ALPEC: A Comprehensive Evaluation Framework and Dataset for Machine Learning-Based Arousal Detection in Clinical Practice; towards Predicting Temporal Changes in a Patient’s Chest X-ray Images based on Electronic Health Records; revealing Treatment Non-Adherence Bias in Clinical Machine Learning Using Large Language Models; A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs; Bridging the utility gap between MALDI-TOF and WGS for affordable outbreak cluster detection; uncertainty Quantification for Machine Learning in Healthcare: A Survey; Benchmarking ECG Delineation using Deep Neural Network-based Semantic Segmentation Models. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Elhussein2025,
	author = {Elhussein, Ahmed and Meddeb, Paul and Newbury, Abigail M. and Mirone, Jeanne and Stoll, Martin and Gürsoy, Gamze},
	title = {KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code Embeddings},
	year = {2025},
	journal = {Proceedings of Machine Learning Research},
	volume = {287},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014744296&partnerID=40&md5=58c0a909a33b049d00d78f4efb2d71f0},
	abstract = {Machine learning in healthcare requires effec3tive representation of structured medical codes, but current methods face a trade-off: knowledge graph-based approaches capture formal relationships but miss real-world patterns, while7 data-driven methods learn empirical associations but often overlook structured knowledge in medical terminologies. We present KEEP (Knowledge-preserving and Empirically refined Embedding Process), an efficient framework that bridges this gap by combining knowledge graph embeddings with adaptive learning from clinical data. KEEP first generates embeddings from knowledge graphs, then employs regularized training on patient records to adaptively integrate empirical patterns while preserving ontological relationships. Importantly, KEEPproduces final embeddings without task-specific axillary or end-to-end training enabling KEEP to support multiple downstream applications and model architectures. Evaluations on structured EHR from UK Biobank and MIMIC-IV demonstrate that KEEP outperforms both traditional and Language Model-based approaches in capturing semantic relationships and predicting clinical outcomes. Moreover, KEEP’s minimal computational requirements make it par ticularly suitable for resource-constrained environments. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Embeddings; Graphic Methods; Knowledge Graph; Medical Computing; Ontology; Semantics; Terminology; 'current; Clinical Data; Data-driven Methods; Embeddings; Graph-based; Knowledge Graphs; Machine-learning; Medical Ontology; Real-world; Trade Off; Economic And Social Effects},
	keywords = {Graph embeddings; Graphic methods; Knowledge graph; Medical computing; Ontology; Semantics; Terminology; 'current; Clinical data; Data-driven methods; Embeddings; Graph-based; Knowledge graphs; Machine-learning; Medical ontology; Real-world; Trade off; Economic and social effects},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Piano2025,
	author = {Piano, Leonardo and Pisu, Alessia and Tiddia, Sandro Gabriele and Carta, Salvatore Mario and Giuliani, Alessandro and Pompianu, Livio},
	title = {LLIMONIIE: Large Language Instructed Model for Open Named Italian Information Extraction},
	year = {2025},
	journal = {Journal of Intelligent Information Systems},
	pages = {},
	doi = {10.1007/s10844-025-00978-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014455825&doi=10.1007%2Fs10844-025-00978-w&partnerID=40&md5=8b2d727927f50a317faf2e21e68c175c},
	abstract = {The exponential growth of unstructured documents generated daily underscores the urgent need to develop technologies to structure information effectively. Traditional Information Extraction (IE) models enable the transformation of textual data into structured formats (e.g., semantic triplets), facilitating efficient searches and uncovering hidden data insights. However, they require predefined ontologies and, often, extensive human efforts. On the other hand, Open IE tools extract information without any input knowledge, but they are limited in capturing entire and in-depth contexts. Furthermore, the state of the art presents a substantial discrepancy between the efforts carried out in English-centric methods and those in low-resource languages, such as Italian. Our study aims to address the aforementioned key challenges. To this end, we first define Open Named Information Extraction (ONIE), an approach that generalizes IE across diverse domains without requiring input ontologies and captures complex relationships. Then, we develop LLIMONIIE (Large Language Instructed Model for Open Named Italian Information Extraction), a novel end-to-end generative information extraction framework that leverages the capabilities of Large Language Models (LLMs) to perform ONIE from Italian documents, able to extract Named Entities and Open Relations uniformly. Furthermore, we devise an innovative dataset generation methodology to support our research. Finally, we release the code and dataset, contributing to the scientific community and the development of low-resource languages. Experiments demonstrate the potential of our proposal, achieving competitive results compared to the actual state of the art of Italian IE. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Information Extraction; Instruction Tuning; Large Language Models; Low Resource Language; Oie; Computational Linguistics; Knowledge Management; Ontology; Open Data; Semantics; Exponential Growth; Information Extraction; Instruction Tuning; Language Model; Large Language Model; Low Resource Languages; Oie; Ontology's; State Of The Art; Unstructured Documents; Information Retrieval},
	keywords = {Computational linguistics; Knowledge management; Ontology; Open Data; Semantics; Exponential growth; Information extraction; Instruction tuning; Language model; Large language model; Low resource languages; OIE; Ontology's; State of the art; Unstructured documents; Information retrieval},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yusuf2025365,
	author = {Yusuf, Yasir Lutfan Bin and Saee, Suhaila},
	title = {Measuring the Feasibility of a Question and Answering System for the Sarawak Gazette Using Chatbot Technology},
	year = {2025},
	journal = {Acta Informatica Pragensia},
	volume = {14},
	number = {3},
	pages = {365 - 392},
	doi = {10.18267/j.aip.263},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014336389&doi=10.18267%2Fj.aip.263&partnerID=40&md5=e4565fedfedab92bffee363276b85686},
	abstract = {Background: The Sarawak Gazette is a critical repository of information pertaining to Sarawak’s history. It has received much attention over the last two decades, with prior studies focusing on digitizing and extracting the gazette’s ontologies to increase the gazette’s accessibility. However, the creation of a question answering system for the Sarawak Gazette, another avenue that could improve accessibility, has been overlooked. Objective: This study created a new system to generate answers for user questions related to the gazette using chatbot technology. Methods: This system sends user queries to a context retrieval system, then generates an answer from the retrieved contexts using a Large Language Model. A question answering dataset was also created using a Large Language Model to evaluate this system, with dataset quality assessed by 10 annotators. Results: The system achieved 55% higher precision, and 42% higher recall compared to previous state-of-the-art historical document question answering while only sacrificing 11% of cosine similarity. The annotators overall rated the dataset 2.9 out of 3. Conclusion: The system could answer the general public’s questions about the Sarawak Gazette in a more direct and friendly manner compared to traditional information retrieval methods. The methods developed in this study are also applicable to other Malaysian historical texts that are written in English. All code used in this study have been released on GitHub. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Accessibility; Artificial Intelligence; Historical Documents; Langchain; Old Newspapers; Question Answering; Retrieval Augmented Generation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Liu2025,
	author = {Liu, Xuhong and Liu, Jie and Liu, Xiulei and Li, Ning and Wang, Linghui},
	title = {Scientist attribute extraction based on instruction supervised fine-tuning and loss function redefinition},
	year = {2025},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {13692},
	pages = {},
	doi = {10.1117/12.3068455},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014290246&doi=10.1117%2F12.3068455&partnerID=40&md5=51cb6268d14d29d81bdbe1bf5defcea2},
	abstract = {The challenge of obtaining high-quality labeled data for scientist attributes significantly impacts traditional deep learning models in the task of scientist attribute extraction. These models typically depend heavily on extensive labeled datasets to achieve optimal performance. However, due to the limited availability of such high-quality labeled data, these models often exhibit weaker generalization abilities when applied to new, unseen data. Utilizing large language models, which possess stronger text comprehension abilities, for scientist attribute extraction can effectively reduce dependence on labeled data. However, the issue of "hallucination" in large language models affects their performance in this task. To address this issue, this paper firstly constructs an ontology for scientist attributes and designs prompt strategies. Subsequently, the idea of bounding box regression loss from object detection tasks is adopted to redefine the loss function for Fine-Tuning large models. This redefinition ensures a clear convergence direction when applying large models to scientist attribute extraction task, effectively reducing LLM hallucination and enhancing the accuracy of the attribute identification process. Our empirical evaluation demonstrates that this novel methodology substantially enhances the efficacy of large models in scientist attribute extraction task. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Attribute Extraction; Instruction Supervised Fine-tuning; Large Language Models; Loss Function; Biographies; Deep Learning; Labeled Data; Learning Systems; Object Recognition; Tuning; Attributes Extractions; Fine Tuning; High Quality; Instruction Supervised Fine-tuning; Language Model; Large Language Model; Large Models; Loss Functions; Tuning Function; Extraction; Object Detection},
	keywords = {Biographies; Deep learning; Labeled data; Learning systems; Object recognition; Tuning; Attributes extractions; Fine tuning; High quality; Instruction supervised fine-tuning; Language model; Large language model; Large models; Loss functions; Tuning function; Extraction; Object detection},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Teslya20251540,
	author = {Teslya, Nikolay N.},
	title = {Using NLP Tools for Linking Materials Within the 'Pushkin Digital' Resource},
	year = {2025},
	journal = {International Conference of Young Specialists on Micro/Nanotechnologies and Electron Devices, EDM},
	pages = {1540 - 1543},
	doi = {10.1109/EDM65517.2025.11096639},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014166911&doi=10.1109%2FEDM65517.2025.11096639&partnerID=40&md5=f3937f289389215b65b7cfc22e96a806},
	abstract = {The development of the scientific and educational resource 'Pushkin Digital' requires processing a substantial volume of documents to create an ontology of A. S. Pushkin's literary heritage. The works of A. S. Pushkin and related texts contain mentions of entities, such as historical figure names, geographical locations, dates, and references biographical sources. All these entities are the source of links in the ontology. The paper presents a description of the natural language processing techniques used in processing the materials featured on the Pushkin Digital resource in order to create links between them. The proposed system system utilizes state-of-the-art NLP techniques including BERT for robust named entity recognition, identifying and classifying key entities within the text, SBERT for enabling the system to discern relationships and connections between entities even when expressed with different wording, and LLMs for complex text analysis. Regular expressions are employed for identifying and processing structured text elements, such as dates and bibliographic references, ensuring data consistency and accuracy. This combination of techniques allows for the automated construction of a rich and interconnected ontology, facilitating indepth exploration of Pushkin's literary heritage and its broader cultural significance. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Document Analysis; Information Extraction; Natural Language Processing; Ontology; Text Mining; Arts Computing; Bibliographic Retrieval Systems; Information Retrieval; Natural Language Processing Systems; Text Mining; Digital Resources; Documents Analysis; Information Extraction; Language Processing; Natural Language Processing; Natural Languages; Nlp Tools; Ontology's; Scientific Resources; Text-mining; Ontology},
	keywords = {Arts computing; Bibliographic retrieval systems; Information retrieval; Natural language processing systems; Text mining; Digital resources; Documents analysis; Information extraction; Language processing; Natural language processing; Natural languages; NLP tools; Ontology's; Scientific resources; Text-mining; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kotenko20251460,
	author = {Kotenko, Igor V. and Abramenko, Georgii T.},
	title = {Detecting and Analysing Cyber Attacks Based on Graph Neural Networks, Ontologies and Large Language Models},
	year = {2025},
	journal = {International Conference of Young Specialists on Micro/Nanotechnologies and Electron Devices, EDM},
	pages = {1460 - 1464},
	doi = {10.1109/EDM65517.2025.11096818},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105014152166&doi=10.1109%2FEDM65517.2025.11096818&partnerID=40&md5=224d4609693b5bb1d5524bde9e9d6946},
	abstract = {This paper presents an intelligent system to automate the process of detecting and analysing cyber-attacks using Suricata logs, graph neural networks (GNNs), and large language models (LLMs). The proposed approach is based on several key components: collecting and preprocessing network events from Suricata, building an ontological model of attacks using MITRE ATT&CK, applying graph neural networks to identify relationships between events, and finally integrating a language model for dialogue interaction with the operator and generating attack hypotheses. Experimental results demonstrate high accuracy in detecting anomalous network patterns and operator friendliness and indicate the potential for further development of the system for use in high-load and distributed infrastructures. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Anomaly Detection; Gnn; Llm; Ontologies; Process Automation; Rag; Suricata; Computational Linguistics; Distributed Computer Systems; Intelligent Systems; Learning Systems; Network Security; Neural Networks; Anomaly Detection; Cyber-attacks; Graph Neural Networks; Language Model; Large Language Model; Ontological Modeling; Ontology's; Process Automation; Rag; Suricata; Ontology},
	keywords = {Computational linguistics; Distributed computer systems; Intelligent systems; Learning systems; Network security; Neural networks; Anomaly detection; Cyber-attacks; Graph neural networks; Language model; Large language model; Ontological modeling; Ontology's; Process automation; RAG; Suricata; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Maratsi202528,
	author = {Maratsi, Maria Ioanna and Gialoussi, Nina and Alexopoulos, Charalampos and Charalabidis, Yannis K.},
	title = {A Proposed Methodology for Sub-Ontology Development in Comprehensive Scientific Investigation Methods and Tooling},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2331 CCIS},
	pages = {28 - 43},
	doi = {10.1007/978-3-031-81974-2_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013971365&doi=10.1007%2F978-3-031-81974-2_3&partnerID=40&md5=781fa0b9db7a6cb02c568dd99d22e7d9},
	abstract = {The role of ontologies in facilitating search capabilities within large collections of data is critical; the integration and analysis of diverse data sources becomes feasible as ontologies frame the data conceptually and provide a common understanding of terms and their relationships- the lack of ontological and conceptual support entailing the opposite effect. Along with documents and data lost in the vast-ness of available yet disparate data sources, numerous scientific papers and published research remain undiscovered due to poor linking to their respective scientific domain and investigation method(s) described in them. Within the scope of this study is to retrieve existing Wikidata method codes for 3 disciplines: psychology, neuroscience and cultural heritage, and analyse them, with the purpose of identifying gaps in the usage of hierarchical levels or codes, and examining whether they are currently capable of sufficiently describing the methodological domains in question, while also pertaining to a suitable level of specificity in or-der for the related data to be efficiently and effectively queried and retrieved. The findings revealed several issues regarding the discoverability and semantic search capabilities to retrieve scientific literature papers on research (or investigation) methods and tooling for the in-word disciplines. In this light, a proposed methodology to alleviate the current situation is drafted, introducing the utilisation of technological means, such as LLMs, to assist in identifying orphan categories of methods or tools and, by benchmarking against basic existing ontologies (e.g., FrameNet or other related Linked Open Vocabularies), to enrich the hierarchical structure of current representation practices in this regard. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Linked Data; Linked Open Data; Linked Open Vocabularies; Llms; Lov; Ontology Mapping; Scientific Method; Scientific Tools; Semantic Interoperability; Semantic Search; Wikidata; Wikidata Codes; Codes (symbols); Data Handling; Interoperability; Open Data; Semantic Web; Semantics; Knowledge Graphs; Linked Open Data; Linked Open Vocabulary; Llm; Lov; Ontology Mapping; Scientific Method; Scientific Tool; Semantic Interoperability; Semantic Search; Wikidata; Wikidata Code; Ontology},
	keywords = {Codes (symbols); Data handling; Interoperability; Open Data; Semantic Web; Semantics; Knowledge graphs; Linked open data; Linked open vocabulary; LLM; LOV; Ontology mapping; Scientific method; Scientific tool; Semantic interoperability; Semantic search; Wikidata; Wikidata code; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {18th Research Conference on Metadata and Semantic Research, MTSR 2024},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2331 CCIS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013970557&partnerID=40&md5=637043d168471212d6a52fee87d35ac7},
	abstract = {The proceedings contain 29 papers. The special focus in this conference is on Metadata and Semantics Research. The topics include: Applying New Standards to Legacy Data for Semantic Interoperability and Multilingualism: A Case Study at the National Library of Greece; CTP Ontology: An Ontology for Creating and Structuring Cultural Thematic Paths; Metadata-Driven Cross-Infrastructure Integration Between Solid Earth and Marine Sciences in the GEO-INQUIRE Project; vector Spaces Model: A Knowledge Integration Method for Research on Linkage Relationships in Agricultural Science and Technology; ontological Approaches to Morphological Semantics in Modern Greek Derivation; the Semantics of Emotion: Exploring Synonym Rings to Evaluate Emotive Contexts in Translations Across Sinhala and English; ontological Patterns for Modeling Art Exhibitions: An Initial Investigation; Developing Datasets for Training OCR/HTR Models for the Late 19th Century Greek Texts; public Libraries - Aggregators of New and Reusable Knowledge Resources for Users’ Creative Development; The Use-Case of Enhanced AEON in Education; building a Transliteration Tool to Enhance the Exchange of Metadata About Greek Authors; the Convergence of Open Data, Linked Data, Ontologies, and Large Language Models: Enabling Next-Generation Knowledge Systems; Cross-Referencing Metadata Through an Extension of the MEDFORD Language; A RAG Approach for Generating Competency Questions in Ontology Engineering; a Proposed Methodology for Sub-Ontology Development in Comprehensive Scientific Investigation Methods and Tooling; DATA-FW: An Ontology Network for Annotating Open Datasets; Creation of a Music Ontology in the Framework of the Project Greek Music Audiovisual Collections (M.EL.O.S.); Aligning Data Management Plans with Community Standards Using FAIR Implementation Profiles; Modeling Modern and Historical Data as a Knowledge Graph: ACase Study for Earthquake Data; towards a Knowledge Graph for Models and Algorithms in Applied Mathematics; preface. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Miaoling2025319,
	author = {Miaoling, Chai and Zhang, Xian and Yawei, Tang and DaWaZhuoMa, null},
	title = {Vector Spaces Model: A Knowledge Integration Method for Research on Linkage Relationships in Agricultural Science and Technology},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2331 CCIS},
	pages = {319 - 333},
	doi = {10.1007/978-3-031-81974-2_27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013962743&doi=10.1007%2F978-3-031-81974-2_27&partnerID=40&md5=27e730d9e77d3cafa8c217c0e109cd32},
	abstract = {This study proposed an integration model that utilizes vector space features of multimodal data to address deficiencies in knowledge integration and logic within Science and Technology linkage research, and facilitate it in agricultural field. The research constructs a Vector Spaces Model (VSM) from three aspects: spatial direction, spatial matrix, and spatial topics. First, based on the social structure of multimodal data, an interactive merging method was used to integrate the linear model and the chain model to construct an Innovation Ecological Chain (IEC) from scientific research to the market. Second, a distance matrix was constructed based on classification of data processing depth and technology maturity to measure the semantic distance on the ecological chain. Third, combined metadata and ontology with Latent Dirichlet Allocation (LDA) to construct the spatial topic model, furthermore, the Large Language Model (LLM) utilizes in multimode data topic extraction. The experiments have shown that, 12 types of data from 1953 to 2023 in the Chinese highland barley can be obtained. Under the landscape of enterprise knowledge discovery, the 65 records topics in the corporate annual reports (2011–2022) spans across 5 sectors in VSM, 6 types of data could integrate in VSM and distribute in 6 sectors of it, and 17 key stakeholders identified from 5 types of data, and the Fund is important to distinguish the identity. Overall, the work verified that multimodal data could integrate under the VSM, while metadata and ontology can quantify the spatial vectors in semantics. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Highland Barley; Knowledge Integration; Multimodal Data; Ontology; Semantic; Vector Space; Agriculture; Chains; Classification (of Information); Data Integration; Ecology; Integration; Knowledge Management; Latent Semantic Analysis; Matrix Algebra; Metadata; Vectors; Agricultural Science And Technologies; Ecological Chains; Highland Barley; Integration Method; Integration Models; Knowledge Integration; Multi-modal; Multimodal Data; Ontology's; Vector Space Models; Semantics; Vector Spaces},
	keywords = {Agriculture; Chains; Classification (of information); Data integration; Ecology; Integration; Knowledge management; Latent semantic analysis; Matrix algebra; Metadata; Vectors; Agricultural science and technologies; Ecological chains; Highland barley; Integration method; Integration models; Knowledge integration; Multi-modal; Multimodal data; Ontology's; Vector space models; Semantics; Vector spaces},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Cigliano2025197,
	author = {Cigliano, Andrea and Fallucchi, Francesca},
	title = {The Convergence of Open Data, Linked Data, Ontologies, and Large Language Models: Enabling Next-Generation Knowledge Systems},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2331 CCIS},
	pages = {197 - 213},
	doi = {10.1007/978-3-031-81974-2_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013958473&doi=10.1007%2F978-3-031-81974-2_17&partnerID=40&md5=553c5b8e0f596428b60c7c6e12eae9fd},
	abstract = {This paper explores the convergence of Open Data initiatives, Linked Data technologies, ontological knowledge representation, and Large Language Models (LLMs) in generative Artificial Intelligence (AI). It examines how these complementary approaches can be integrated to create more powerful, flexible, and context-aware knowledge systems. The paper provides an overview of the open data landscape, the Semantic Web and Linked Data vision, ontologies and knowledge organization systems, and recent advances in LLMs. It then discusses how these technologies can be synergistically combined to enable next-generation knowledge systems that leverage both structured knowledge and natural language understanding. Potential applications in areas such as scientific research, government transparency, and intelligent information retrieval are discussed. The paper also addresses key challenges including scalability, data quality, ethical considerations, and the need for explainable AI. A strategic roadmap for realizing this integration is proposed, emphasizing collaboration between academia, industry, and government. While significant technical and ethical challenges remain, the convergence of these technologies has the potential to fundamentally transform how we interact with and derive insights from information, enabling more intelligent and context-aware knowledge systems to address complex real-world problems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Generative Artificial Intelligence; Large Language Model; Linked Open Data; Llm; Ontologies; Open Data; Skos; Domain Knowledge; Ethical Technology; Knowledge Organization; Knowledge Organization System (kos); Linked Data; Natural Language Processing Systems; Open Data; Semantic Web; Simple Knowledge Organization System (skos); Context-aware; Generative Artificial Intelligence; Knowledge System; Language Model; Large Language Model; Linked Open Data; Ontology's; Skos; Ontology},
	keywords = {Domain Knowledge; Ethical technology; Knowledge organization; Knowledge organization system (KOS); Linked data; Natural language processing systems; Open Data; Semantic Web; Simple knowledge organization system (SKOS); Context-Aware; Generative artificial intelligence; Knowledge system; Language model; Large language model; Linked open data; Ontology's; SKOS; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Pan202570,
	author = {Pan, Xueli and Ossenbruggen, Jacco Van and de Boer, Victor and Huang, Zhisheng},
	title = {A RAG Approach for Generating Competency Questions in Ontology Engineering},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2331 CCIS},
	pages = {70 - 81},
	doi = {10.1007/978-3-031-81974-2_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013955848&doi=10.1007%2F978-3-031-81974-2_6&partnerID=40&md5=9076217b9a4a337e38d337f34bfddfc2},
	abstract = {Competency question (CQ) formulation is central to several ontology development and evaluation methodologies. Traditionally, the task of crafting these competency questions heavily relies on the effort of domain experts and knowledge engineers which is often time-consuming and labor-intensive. With the emergence of Large Language Models (LLMs), there arises the possibility to automate and enhance this process. Unlike other similar works which use existing ontologies or knowledge graphs as input to LLMs, we present a retrieval-augmented generation (RAG) approach that uses LLMs for the automatic generation of CQs given a set of scientific papers considered to be a domain knowledge base. We investigate its performance and specifically, we study the impact of different number of papers to the RAG and different temperature setting of the LLM. We conduct experiments using GPT-4 on two domain ontology engineering tasks and compare results against ground-truth CQs constructed by domain experts. Empirical assessments on the results, utilizing evaluation metrics (precision and consistency), reveal that compared to zero-shot prompting, adding relevant domain knowledge to the RAG improves the performance of LLMs on generating CQs for concrete ontology engineering tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Competency Questions; Llms; Ontology Engineering; Domain Knowledge; Knowledge Graph; Competency Question; Domain Experts; Domain Knowledge; Engineering Tasks; Language Model; Large Language Model; Ontology Development; Ontology Engineering; Ontology Evaluations; Performance; Ontology},
	keywords = {Domain Knowledge; Knowledge graph; Competency question; Domain experts; Domain knowledge; Engineering tasks; Language model; Large language model; Ontology development; Ontology engineering; Ontology evaluations; Performance; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Barandiaran2025,
	author = {Barandiaran, Xabier E. and Almendros, Lola S.},
	title = {Transforming agency: On the mode of existence of large language models},
	year = {2025},
	journal = {Phenomenology and the Cognitive Sciences},
	pages = {},
	doi = {10.1007/s11097-025-10094-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013749515&doi=10.1007%2Fs11097-025-10094-3&partnerID=40&md5=2c2b33e3a8e1e707dd355d5607ad73c1},
	abstract = {This paper investigates the ontological characterization of Large Language Models (LLMs) like ChatGPT. Between inflationary and deflationary accounts, we pay special attention to their status as agents. This requires explaining in detail the architecture, processing, and training procedures that enable LLMs to display their capacities, and the extensions used to turn LLMs into agent-like systems. After a systematic analysis we conclude that a LLM fails to meet necessary and sufficient conditions for autonomous agency in the light of embodied theories of mind: the individuality condition (it is not the product of its own activity, it is not even directly affected by it), the normativity condition (it does not generate its own norms or goals), and, partially the interactional asymmetry condition (it is not the origin and sustained source of its interaction with the environment). If not agents, then… What are LLMs? We argue that ChatGPT should be characterized as an interlocutor or linguistic automaton, a library-that-talks, devoid of (autonomous) agency, but capable to engage performatively on non-purposeful yet purpose-structured and purpose-bounded tasks. When interacting with humans, a “ghostly” component of the human-machine interaction makes it possible to enact genuine conversational experiences with LLMs. Despite their lack of sensorimotor and biological embodiment, LLMs textual embodiment (the training corpus), digital extended interface embodiments, and resource-hungry computational embodiment, significantly transform existing forms of machine automatism and human agency. Beyond assisted and extended agency, the LLM-human coupling can produce midtended forms of agency, closer to the production of intentional agency than to the extended instrumentality of any previous technologies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Agency; Autonomy; Human Machine Interaction; Interlocutor Automata; Large Language Models; Transformers},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Peng20251115,
	author = {Peng, Yuying and Zhao, Siqi and Luo, Ximeng and Ning, Yongzhi},
	title = {Large Language Models and Knowledge Graphs Synergistically Enhancing Personalized Learning},
	year = {2025},
	pages = {1115 - 1119},
	doi = {10.1109/CSTE64638.2025.11091995},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013538581&doi=10.1109%2FCSTE64638.2025.11091995&partnerID=40&md5=ab6ad634404ebe7444d3a7fe809a69ad},
	abstract = {The rapid development of artificial intelligence technology has promoted the intelligence of the education field. The collaborative work of large language models (LLMs) and knowledge graphs (KGs) can effectively develop personalized learning paths. Personalized learning in programming courses is crucial for meeting students' different needs and proficiency levels. This article proposes a method for personalized teaching of programming courses using LLMs and KGs. The LLMs is used to analyze students' learning status, provide real-time feedback, and generate customized learning materials, while the KGs provides a structured knowledge base to plan essential skills and learning progress, highly integrating the advantages of the LLMs and the logical connections of knowledge from various disciplines. The application results show that designing a knowledge system ontology through disciplinary knowledge analysis, retraining and fine-tuning the LLMs, forms the final personalized knowledge system, improves the accuracy of learning path recommendations and the relevance of generated exercises, and can be easily promoted to other disciplinary fields. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence Technology; Knowledge Graphs; Large Language Models; Personalized Learning; Programming Courses; Curricula; Education Computing; Engineering Education; Knowledge Graph; Knowledge Organization; Learning Systems; Ontology; Teaching; Artificial Intelligence Technologies; Collaborative Work; Education Field; Knowledge Graphs; Knowledge System; Language Model; Large Language Model; Learning Paths; Personalized Learning; Programming Course; Students},
	keywords = {Curricula; Education computing; Engineering education; Knowledge graph; Knowledge organization; Learning systems; Ontology; Teaching; Artificial intelligence technologies; Collaborative Work; Education field; Knowledge graphs; Knowledge system; Language model; Large language model; Learning paths; Personalized learning; Programming course; Students},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kumar2025,
	author = {Kumar, Aditya and Singh, Dilpreet and Cypko, Mario A. and Amft, Oliver},
	title = {A multi-view validation framework for LLM-generated knowledge graphs of chronic kidney disease},
	year = {2025},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	pages = {},
	doi = {10.1007/s11548-025-03495-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013466422&doi=10.1007%2Fs11548-025-03495-x&partnerID=40&md5=ef23a625218594536d33b4108699c32f},
	abstract = {Purpose: The goal of our work is to develop a multi-view validation framework for evaluating LLM-generated knowledge graph (KG) triples. The proposed approach aims to address the lack of established validation procedure in the context of LLM-supported KG construction. Methods: The proposed framework evaluates the LLM-generated triples across three dimensions: semantic plausibility, ontology-grounded type compatibility, and structural importance. We demonstrate the performance for GPT-4 generated concept-specific (e.g., for medications, diagnosis, procedures) triples in the context of chronic kidney disease (CKD). Results: The proposed approach consistently achieves high-quality results across evaluated GPT-4 generated triples, strong semantic plausibility (semantic score mean: 0.79), excellent type compatibility (type score mean: 0.84), and high structural importance of entities within the CKD knowledge domain (ResourceRank mean: 0.94). Conclusion: The validation framework offers a reliable and scalable method for evaluating quality and validity of LLM-generated triples across three views: semantic plausibility, type compatibility, and structural importance. The framework demonstrates robust performance in filtering high-quality triples and lays a strong foundation for fast and reliable medical KG construction and validation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chronic Kidney Disease; Expert System; Healthcare; Knowledge Graphs; Knowledge-driven Modelling; Llms; Medical Informatics; Model-guided Medicine; Nephrology; Semantic Evaluation; Triples; Validation},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bruns2025,
	author = {Bruns, Oleksandra and Graciotti, Arianna and Sartini, Bruno and Tietz, Tabea},
	title = {Proceedings of the Second International Workshop of Semantic Digital Humanities co-located with the Extended Semantic Web Conference 2025},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {4009},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013459541&partnerID=40&md5=407718c8072f1a961059a8ace30002b3},
	abstract = {Investigating, interpreting, and safeguarding the world’s cultural and historical assets are crucial for comprehending humanity’s past and future. Recently, there has been a surge of interest in applying Ontologies, Knowledge Graphs, and Semantic Web Technologies to Cultural Heritage (CH) and Digital Humanities (DH). Nonetheless, varying areas of expertise and traditions have led to a disconnect between technological solutions and the needs of the humanities. The International Workshop of Semantic Digital Humanities (SemDH) aims to close this divide and promote cooperation among the Semantic Web, CH, and DH communities. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bias; Cultural Heritage; Digital Humanities; Fair; Knowledge Graphs; Knowledge Representation; Large Language Models; Ontologies; Knowledge Graph; Semantic Web; Bias; Cultural Heritages; Digital Humanities; Fair; International Workshops; Knowledge Graphs; Knowledge-representation; Language Model; Large Language Model; Ontology's; Ontology},
	keywords = {Knowledge graph; Semantic Web; Bias; Cultural heritages; Digital humanities; FAIR; International workshops; Knowledge graphs; Knowledge-representation; Language model; Large language model; Ontology's; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li20254979,
	author = {Li, Qinpeng and Zhan, Lili and Cai, Xinjian},
	title = {Assessing DeepSeek-R1 for Clinical Decision Support in Multidisciplinary Laboratory Medicine},
	year = {2025},
	journal = {Journal of Multidisciplinary Healthcare},
	volume = {18},
	pages = {4979 - 4988},
	doi = {10.2147/JMDH.S538253},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013253813&doi=10.2147%2FJMDH.S538253&partnerID=40&md5=7069d4642964ecdfa3fa1049b26a4308},
	abstract = {Background: Recent advancements in artificial intelligence (AI), particularly with large language models (LLMs), are transforming healthcare by enhancing diagnostic decision-making and clinical workflows. The application of LLMs like DeepSeek-R1 in clinical laboratory medicine demonstrates potential for improving diagnostic accuracy, supporting decision-making, and optimizing patient care. Objective: This study evaluates the performance of DeepSeek-R1 in analyzing clinical laboratory cases and assisting with medical decision-making. The focus is on assessing its accuracy and completeness in generating diagnostic hypotheses, differential diagnoses, and diagnostic workups across diverse clinical cases. Methods: We analyzed 100 clinical cases from Clinical Laboratory Medicine Case Studies, which includes comprehensive case histories and laboratory findings. DeepSeek-R1 was queried independently for each case three times, with three specific questions regarding diagnosis, differential diagnoses, and diagnostic tests. The outputs were assessed for accuracy and completeness by senior clinical laboratory physicians. Results: DeepSeek-R1 achieved an overall accuracy of 72.9% (95% CI [69.9%, 75.7%]) and completeness of 73.4% (95% CI [70.5%, 76.2%]). Performance varied by question type: the highest accuracy was observed for diagnostic hypotheses (85.7%, 95% CI [81.2%, 89.2%]) and the lowest for differential diagnoses (55.0%, 95% CI [49.3%, 60.5%]). Notable variations in performance were also seen across disease categories, with the best performance observed in genetic and obstetric diagnostics (accuracy 93.1%, 95% CI [84.0%, 97.3%]; completeness 86.1%, 95% CI [76.4%, 92.3%]). Conclusion: DeepSeek-R1 demonstrates potential for a decision-support tool in clinical laboratory medicine, particularly in generating diagnostic hypotheses and recommending diagnostic workups. However, its performance in differential diagnosis and handling specific clinical nuances remains limited. Future work should focus on expanding training data, integrating clinical ontologies, and incorporating physician feedback to improve real-world applicability. DeepSeek-R1 and the new versions under development may be promising tools for non-medical professionals and professionals in medical laboratory diagnoses. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Clinical Decision Support Systems; Clinical Laboratory Medicine; Deepseek-r1; Large Language Model; Article; Artificial Intelligence; Clinical Decision Support System; Clinical Laboratory; Controlled Study; Decision Support System; Diagnostic Test; Diagnostic Test Accuracy Study; Human; Laboratory Diagnosis; Large Language Model; Major Clinical Study; Medical Decision Making; Patient Care; Physician},
	keywords = {Article; artificial intelligence; clinical decision support system; clinical laboratory; controlled study; decision support system; diagnostic test; diagnostic test accuracy study; human; laboratory diagnosis; large language model; major clinical study; medical decision making; patient care; physician},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zengeya202587,
	author = {Zengeya, Tsitsi and Fonou-Dombeu, Jean Vincent and Gwetu, Mandlenkosi Victor},
	title = {An Attention-Based Deep Learning Model for Term Extraction from Text Using BERT},
	year = {2025},
	journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
	volume = {632 LNICST},
	pages = {87 - 102},
	doi = {10.1007/978-3-031-94439-0_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013199652&doi=10.1007%2F978-3-031-94439-0_6&partnerID=40&md5=6179d97728d4ea73b61714699c6d9ce6},
	abstract = {Extracting terms from text is relevant in many applications such as document summarizing, question answering, ontology learning and many more. However, the unstructured nature of textual data poses significant hurdles that make the automatic extraction of terms from text a challenging task. Furthermore, while significant research efforts have been dedicated to term extraction from textual data, the incorporation of domain knowledge and context awareness as a way of enriching the extracted terms remains a challenge. To tackle these challenges, this study proposes an attention-based Deep Learning model for term extraction from text using the Bidirectional Encoder Representations from Transformers (BERT) language model. The model uses (1) web scraping for term extraction from web pages, (2) BERT encoder for enriching term extraction through contextualization and cosine similarity to extract domain-specific terms and (3) WordNet for adding knowledge context and disambiguation through vocabulary labeling. The proposed system was applied to five different data samples from the horticulture domain and was evaluated with various metrics including accuracy, precision, and F1-score. The experimental results indicate that the proposed model improves the domain-specific term extraction and vocabulary labeling with an accuracy of 73%, a precision of 89%, a recall of 79%, as well as F1-Score of 84%, better than the related studies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Cosine Similarity Measure; Term Extraction; Vocabulary Building; Wordnet; Deep Learning; Domain Knowledge; Extraction; Intelligent Systems; Learning Systems; Natural Language Processing Systems; Ontology; Text Processing; Websites; Bidirectional Encoder Representation From Transformer; Cosine Similarity Measures; Domain Specific; F1 Scores; Labelings; Learning Models; Term Extraction; Textual Data; Vocabulary Building; Wordnet; Signal Encoding},
	keywords = {Deep learning; Domain Knowledge; Extraction; Intelligent systems; Learning systems; Natural language processing systems; Ontology; Text processing; Websites; Bidirectional encoder representation from transformer; Cosine similarity measures; Domain specific; F1 scores; Labelings; Learning models; Term extraction; Textual data; Vocabulary building; Wordnet; Signal encoding},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Chmielewski2025,
	author = {Chmielewski, Sebastian and Meisen, Tobias and Pomp, Andre},
	title = {The Synergy of Large Language Models and Dataspaces: A Functional Exploration},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {4007},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013071135&partnerID=40&md5=169fb06d7c5bed6b69b49338a74afa88},
	abstract = {Dataspaces provide a decentralized framework for secure and sovereign data exchange, yet ensuring semantic interoperability remains a key challenge. Large Language Models (LLMs) have emerged as powerful tools for enhancing data usability, particularly in metadata enrichment, semantic labeling, and data querying. This paper systematically investigates the role of LLMs in dataspaces through a structured literature review, identifying five core tasks: data querying, visualization, augmentation, cleaning, and metadata enrichment. Our findings highlight that metadata enrichment-specifically semantic labeling and modeling-is a primary area where LLMs can improve interoperability by automatically generating structured and meaningful metadata. However, challenges such as hallucinations, inconsistent labeling, and limited domain adaptation persist, affecting their reliability in real-world applications. We discuss approaches to mitigate these limitations, including the integration of LLMs with knowledge graphs and domain ontologies. By demonstrating how LLMs can contribute to automated metadata enrichment, this study provides a foundational analysis of their role in enabling FAIR (Findable, Accessible, Interoperable, Reusable) data principles in dataspaces. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Dataspaces; Large Language Models (llms); Semantic Interoperability; Semantic Labeling; Semantic Modeling; Semantics In Dataspaces; Computer Software Reusability; Data Visualization; Electronic Data Interchange; Interoperability; Knowledge Management; Metadata; Natural Resources Exploration; Ontology; Structured Query Language; Data Querying; Data Space; Functional Exploration; Language Model; Large Language Model; Semantic In Dataspace; Semantic Interoperability; Semantic Labeling; Semantic Modelling; Semantics},
	keywords = {Computer software reusability; Data visualization; Electronic data interchange; Interoperability; Knowledge management; Metadata; Natural resources exploration; Ontology; Structured Query Language; Data querying; Data space; Functional exploration; Language model; Large language model; Semantic in dataspace; Semantic interoperability; Semantic labeling; Semantic modelling; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Stäbler2025,
	author = {Stäbler, Maximilian and Lange, Markus and Kipper, Samir and Langdon, Chris and Köster, Frank},
	title = {GC-DAM: Graph and Contextual Embeddings for Heterogeneous Data Asset Matching},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {4007},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013070352&partnerID=40&md5=326a772cb63e7ec10a0f8dc06272fdf3},
	abstract = {Data assets-such as datasets, data services, APIs, algorithms, and analytical models-are valuable digital resources that organizations use to create value, support decision-making, and optimize business processes. Matching and integrating these assets, despite differences in semantic languages, ontologies, or schemas, is essential for building scalable and interoperable dataspaces. However, existing approaches often focus solely on semantic similarities, overlooking structurally similar assets from other domains that could be highly relevant. To address this gap, we present Graph and Contextual Embeddings for Heterogeneous Data Asset Matching (GC-DAM). GC-DAM employs two embedding strategies to match data assets based on both semantic and structural attributes. Structural (morphological) features are automatically incorporated into a knowledge graph, enabling the identification of assets that are structurally similar to a query but may originate from different domains, while metadata descriptions capture the semantic (contextual) features. This dual approach overcomes the limitations of methods that rely solely on semantic descriptions. We validate our approach against a custom dataset of 10,000 Kaggle data assets. Our multimodal embedding achieves 77% agreement on our custom dataset, demonstrating its ability to identify structurally similar assets across diverse domains, even when they are semantically different. The dataset and code are publicly available to the research community. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Automated Interoperability; Heterogeneous Dataspaces; Knowledge Graphs; Llm; Multi-modal-embedding; Graph Embeddings; Interoperability; Knowledge Graph; Automated Interoperability; Data Assets; Data Space; Embeddings; Heterogeneous Dataspace; Knowledge Graphs; Llm; Matchings; Multi-modal; Multi-modal-embedding; Semantics},
	keywords = {Graph embeddings; Interoperability; Knowledge graph; Automated interoperability; Data assets; Data space; Embeddings; Heterogeneous dataspace; Knowledge graphs; LLM; Matchings; Multi-modal; Multi-modal-embedding; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Cavalcanti Hernandes2025,
	author = {Cavalcanti Hernandes, Leonardo and Szejka, A. L. and Mas, Fernando},
	title = {Intelligent product manufacturing cost estimation framework driven by semantic technologies and knowledge-based systems},
	year = {2025},
	journal = {International Journal of Computer Integrated Manufacturing},
	pages = {},
	doi = {10.1080/0951192X.2025.2545485},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012848711&doi=10.1080%2F0951192X.2025.2545485&partnerID=40&md5=864cac133d63b961e47b8d82e5bb446d},
	abstract = {Accurate cost estimation in the early stages of new product development (NPD) is increasingly challenged by the complexity of aerospace parts and the rapid expansion of manufacturing process knowledge. Traditional cost estimation approaches, often reliant on static models and expert judgment, struggle to adapt and integrate diverse, evolving process data. This paper introduces a novel Knowledge-Based System for Product Manufacturing Cost Estimation (KBS-PMCE) that leverages semantic technologies and natural language processing (NLP) to deliver explainable, extensible cost estimation capabilities. The proposed framework formalizes manufacturing knowledge into structured ontologies, enabling semantic interoperability and dynamic alignment of data sources, including the incorporation of newly defined manufacturing processes. Validation in a case study involving complex aerospace sheet metal (ASM) parts illustrates the framework’s potential to support cost estimation grounded in formalized process equations and reasoning logic. The results highlight the system’s potential to enhance decision-making and interoperability in smart manufacturing environments. Future research will aim to broaden the framework’s applicability across diverse manufacturing sectors and improve semantic alignment automation through the integration of large language models (LLMs). © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge-based Systems; Manufacturing Cost Estimation; New Product Development; Ontology-based Engineering; Semantic Technologies; Computational Cost; Cost Engineering; Cost Estimating; Industrial Research; Knowledge Management; Ontology; Product Development; Semantic Web; Semantics; Smart Manufacturing; Cost Estimations; Knowledge-based Systems; Manufacturing Cost; Manufacturing Cost Estimation; Manufacturing Process; New Product Development; Ontology-based; Ontology-based Engineering; Product Manufacturing; Semantic Technologies; Knowledge Based Systems},
	keywords = {Computational cost; Cost engineering; Cost estimating; Industrial research; Knowledge management; Ontology; Product development; Semantic Web; Semantics; Smart manufacturing; Cost estimations; Knowledge-based systems; Manufacturing cost; Manufacturing cost estimation; Manufacturing process; New product development; Ontology-based; Ontology-based engineering; Product manufacturing; Semantic technologies; Knowledge based systems},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Schön2025156,
	author = {Schön, Johanna},
	title = {Delectable Knowledge-How the Sense of Touch Illuminates the Ontological Status of AI-Recipes},
	year = {2025},
	journal = {ESPES},
	volume = {14},
	number = {1},
	pages = {156 - 170},
	doi = {10.5281/zenodo.15845775},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012840931&doi=10.5281%2Fzenodo.15845775&partnerID=40&md5=b89e6339e54410fc3bfc77374e9d5733},
	abstract = {The ongoing digital transformation has significantly altered our access to cooking recipes, introducing AI-generated outputs from Large Language Models (LLMs). This paper examines the ontological status of such AI-generated recipes, arguing that neither recipe platonism nor recipe constructivism can fully account for them, as both rely on the existence of a prepared dish as a condition for validation. Yet, some AI-generated recipes are treated as successful, suggesting that their acceptance is not purely abstract, but rooted in embodied culinary knowledge. Drawing on Andrea Borghini’s concept of apprenticeship, I argue that recognising a recipe involves both abstract cognition within a type-token framework and sensory engagement through cooking and tasting. This analysis is supported by insights from Alva Noë and Matthew Ratcliffe, who conceptualise perception not as passive reception, but as a skilled and bodily situated mode of interaction with the world. Perceiving, in this sense, becomes a form of doing and touch and taste play a crucial role in how we validate recipes through practice. By shifting the focus from what is a recipe? to how do we recognise a recipe?, I propose a broader account that accommodates both human and AI contributions. Ultimately, recipe validity is not merely a matter of cognitive classification, but an experiential process grounded in sensory interaction, highlighting cooking as a vital mode of understanding recipe ontology. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Authenticity; Culinary Epistemology; Embodied Knowledge; Multisensory Validity; Recipe Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wang2025139,
	author = {Wang, Shaojun and Liu, Hancheng and You, Ronghui and Liu, Yunjia and Xiong, Yi and Zhu, Shanfeng},
	title = {NetGO 3.0: A Recent Protein Function Prediction Tool Based on Protein Language Model},
	year = {2025},
	journal = {Methods in Molecular Biology},
	volume = {2947},
	pages = {139 - 150},
	doi = {10.1007/978-1-0716-4662-5_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012787373&doi=10.1007%2F978-1-0716-4662-5_7&partnerID=40&md5=544619abe22882efdbbb513d9226c1a2},
	abstract = {In the field of bioinformatics, automated function prediction (AFP) for proteins is a significant issue. We propose a computational framework based on a protein language model to provide accurate functional predictions for proteins. By integrating multiple component methods, our approach effectively improves prediction performance. Additionally, we have developed a user-friendly online platform that allows users to obtain prediction results simply by submitting protein sequences, freely available at https://dmiip.sjtu.edu.cn/ng3.0/?returning=true. We provide a detailed guide on how to use the web server and correctly interpret the prediction results. Finally, through a practical example, we demonstrate the superior performance of NetGO 3.0 in predicting protein functions, further showcasing the potential of this framework for protein functional annotation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontology; Netgo 3.0; Protein Function Prediction; Protein Language Models; Protein; Proteins; Amino Acid Sequence; Bioinformatics; Chapter; Commercial Phenomena; Gene Ontology; Human; Prediction; Protein Function; Protein Language Model; Algorithm; Chemistry; Metabolism; Procedures; Protein Database; Software; Protein; Algorithms; Computational Biology; Databases, Protein; Proteins; Software},
	keywords = {amino acid sequence; bioinformatics; chapter; commercial phenomena; gene ontology; human; prediction; protein function; protein language model; algorithm; chemistry; metabolism; procedures; protein database; software; protein; Algorithms; Computational Biology; Databases, Protein; Proteins; Software},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Bahmani202575,
	author = {Bahmani, Soufia and Chaudhari, Meenal and Carrier, Callen and Garrett, Steven and Pratyush, Pawel and Kc, Dukka B.},
	title = {Multitask Learning-Based Approaches for Protein Function Prediction},
	year = {2025},
	journal = {Methods in Molecular Biology},
	volume = {2947},
	pages = {75 - 88},
	doi = {10.1007/978-1-0716-4662-5_4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012781209&doi=10.1007%2F978-1-0716-4662-5_4&partnerID=40&md5=a48ac1936e92eafcc74341c7886df160},
	abstract = {Advancements in sequencing technologies have resulted in a massive growth in the number of sequences available. Only a small fraction of the proteins in UniProtKB have been functionally annotated. Understanding the roles and studying the mechanisms of newly discovered proteins is one of the most important biological problems in the post-genomic era. To address the sequence-function gap many computational methods have been developed. This chapter reviews Multitask Learning (MTL)-based approaches for protein function prediction, highlighting its potential to enhance both predictive accuracy and computational efficiency in bioinformatics. MTL utilizes shared representations to leverage common information across related tasks, improving predictive performance. Key findings reveal that MTL improves predictive performance by integrating shared features across related tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Dna–protein Binding; Gene Ontology (go); Metal Binding Sites; Multitask Learning (mtl); Posttranslational Modification (ptm); Protein Function Prediction; Protein Language Model; Protein–protein Interaction (ppi); Rna–protein Binding; Protein; Proteins; Binding Site; Bioinformatics; Chapter; Commercial Phenomena; Deep Learning; Gene Ontology; Human; Learning; Metal Binding; Nonhuman; Prediction; Protein Dna Binding; Protein Function; Protein Language Model; Protein Processing; Protein Protein Interaction; Protein Rna Binding; Algorithm; Chemistry; Machine Learning; Metabolism; Procedures; Protein Database; Software; Protein; Algorithms; Computational Biology; Databases, Protein; Humans; Machine Learning; Proteins; Software},
	keywords = {binding site; bioinformatics; chapter; commercial phenomena; deep learning; gene ontology; human; learning; metal binding; nonhuman; prediction; protein DNA binding; protein function; protein language model; protein processing; protein protein interaction; protein RNA binding; algorithm; chemistry; machine learning; metabolism; procedures; protein database; software; protein; Algorithms; Computational Biology; Databases, Protein; Humans; Machine Learning; Proteins; Software},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Çevrim2025241,
	author = {Çevrim, Elif and Yiğit, Melih Gökay and Ulusoy, Erva and Yılmaz, Ardan and Dogan, Tunca},
	title = {A Benchmarking Platform for Assessing Protein Language Models on Function-Related Prediction Tasks},
	year = {2025},
	journal = {Methods in Molecular Biology},
	volume = {2947},
	pages = {241 - 268},
	doi = {10.1007/978-1-0716-4662-5_14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012779893&doi=10.1007%2F978-1-0716-4662-5_14&partnerID=40&md5=685ca2fe1912dc8b5837d4dae6ce14b8},
	abstract = {Proteins play a crucial role in almost all biological processes, serving as the building blocks of life and mediating various cellular functions, from enzymatic reactions to immune responses. Accurate annotation of protein functions is essential for advancing our understanding of biological systems and developing innovative biotechnological applications and therapeutic strategies. To predict protein function, researchers primarily rely on classical homology-based methods, which use evolutionary relationships, and increasingly on machine learning (ML) approaches. Lately, protein language models (PLMs) have gained prominence; these models leverage specialized deep learning architectures to effectively capture intricate relationships between sequence, structure, and function. We recently conducted a comprehensive benchmarking study to evaluate diverse protein representations (i.e., classical approaches and PLMs) and discuss their trade-offs. The current work introduces the Protein Representation Benchmark—PROBE tool, a benchmarking framework designed to evaluate protein representations on function-related prediction tasks. Here, we provide a detailed protocol for running the framework via the GitHub repository and accessing our newly developed user-friendly web service. PROBE encompasses four core tasks: semantic similarity inference, ontology-based function prediction, drug target family classification, and protein–protein binding affinity estimation. We demonstrate PROBE’s usage through a new use case evaluating ESM2 and three recent multimodal PLMs—ESM3, ProstT5, and SaProt—highlighting their ability to integrate diverse data types, including sequence and structural information. This study underscores the potential of protein language models in advancing protein function prediction and serves as a valuable tool for both PLM developers and users. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmarking Frameworks; Gene Ontology; Protein Families; Protein Function Prediction; Protein Language Models; Protein–protein Interactions; Representation Learning; Protein; Proteins; Benchmarking; Binding Affinity; Cell Function; Chapter; Commercial Phenomena; Deep Learning; Feature Learning (machine Learning); Gene Ontology; Human; Immune Response; Machine Learning; Prediction; Protein Binding; Protein Family; Protein Function; Protein Language Model; Protein Protein Interaction; Bioinformatics; Chemistry; Metabolism; Procedures; Protein Database; Semantics; Software; Protein; Benchmarking; Computational Biology; Databases, Protein; Humans; Machine Learning; Proteins; Semantics; Software},
	keywords = {benchmarking; binding affinity; cell function; chapter; commercial phenomena; deep learning; feature learning (machine learning); gene ontology; human; immune response; machine learning; prediction; protein binding; protein family; protein function; protein language model; protein protein interaction; bioinformatics; chemistry; metabolism; procedures; protein database; semantics; software; protein; Benchmarking; Computational Biology; Databases, Protein; Humans; Machine Learning; Proteins; Semantics; Software},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zou2025191,
	author = {Zou, Jialin and Yuan, Qianmu and Yang, Yuedong},
	title = {An Online Server for Geometry-Aware Protein Function Annotations Through Predicted Structure},
	year = {2025},
	journal = {Methods in Molecular Biology},
	volume = {2947},
	pages = {191 - 208},
	doi = {10.1007/978-1-0716-4662-5_11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012779739&doi=10.1007%2F978-1-0716-4662-5_11&partnerID=40&md5=e969b16b5619620982e9ae3cf1544006},
	abstract = {Understanding protein functions is critical for metagenomic research, disease mechanism elucidation, and rational drug discovery. Traditional biochemical methods to determine protein functions are often inefficient and costly, leading to a widening gap between the rapid accumulation of protein sequences and their functional annotations. To address this, we introduce GPSFun, a fast and accurate web server that provides comprehensive geometry-aware protein sequence function annotations, including protein-binding sites for multiple ligands, gene ontologies, subcellular locations, and protein solubility. GPSFun’s web server can be accessed at https://bio-web1.nscc-gz.cn/app/GPSFun, offering users a result with a rich visualized interface. Additionally, GPSFun was adopted to annotate ligand-binding sites for over 568,000 proteins from Swiss-Prot, leading to a complementary database named GPSiteDB (https://bio-web1.nscc-gz.cn/database/GPSiteDB). In this chapter, we first overview the workflow and results of our method, and then, we provide a step-by-step instruction on the usage of GPSFun and GPSiteDB, along with the detailed interpretations of the corresponding results. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontologies; Gpsfun Web Server; Ligand-binding Sites; Protein Function Prediction; Protein Solubility; Subcellular Locations; Protein; Ligands; Proteins; Amino Acid Sequence; Binding Site; Cellular Distribution; Cross Linking; Directed Acyclic Graph; Feature Extraction; Gene Ontology; Genome; Geometry; Graph Neural Network; Human; Ligand Binding; Measurement Accuracy; Metagenomics; Nonhuman; Prediction; Protein Binding; Protein Function; Protein Language Model; Protein Secondary Structure; Sequence Alignment; Sequence Database; Solubility; Surface Area; Swiss-prot; Velocity; Bioinformatics; Chemistry; Genetics; Internet; Metabolism; Molecular Genetics; Procedures; Protein Conformation; Protein Database; Software; Ligand; Protein; Binding Sites; Computational Biology; Databases, Protein; Ligands; Molecular Sequence Annotation; Protein Binding; Protein Conformation; Proteins; Software},
	keywords = {amino acid sequence; binding site; cellular distribution; cross linking; directed acyclic graph; feature extraction; gene ontology; genome; geometry; graph neural network; human; ligand binding; measurement accuracy; metagenomics; nonhuman; prediction; protein binding; protein function; protein language model; protein secondary structure; sequence alignment; sequence database; solubility; surface area; SWISS-PROT; velocity; bioinformatics; chemistry; genetics; Internet; metabolism; molecular genetics; procedures; protein conformation; protein database; software; ligand; protein; Binding Sites; Computational Biology; Databases, Protein; Ligands; Molecular Sequence Annotation; Protein Binding; Protein Conformation; Proteins; Software},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Khalov2025795,
	author = {Khalov, Andrey and Ataeva, Olga M.},
	title = {Automatic Mapping of Upper-Level Ontology Classes (DOLCE) and Domain-Specific Ontology ITSMO},
	year = {2025},
	pages = {795 - 802},
	doi = {10.1109/ICAIBD64986.2025.11082040},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012762960&doi=10.1109%2FICAIBD64986.2025.11082040&partnerID=40&md5=03a58282e7ad85da2118c7855a3ebada},
	abstract = {This paper proposes a method for extending the top-level ontology DOLCE (DOLCE-lite version, referred to as TLO) to the domain of IT services without expert involvement. The main challenge addressed is the automatic mapping of classes in conditions of a small number of objects (<100) and the absence of annotated data. A review of existing approaches is conducted, their limitations are identified, and novel mapping methods are proposed, integrating embeddings and large language models. The suggested method achieved an 82.35% mapping accuracy when integrating DOLCE and ITSMO ontologies. As a result, the ITO-seed ontology was developed, containing linked classes from DOLCE and ITSMO, which can be utilized in further research and in building knowledge graphs for IT Service Management (ITSM) systems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Clustering; Dolce; Gpt; Itil; Mapping; Ontology; Owl; Owl2vec; Prompt Engineering; Rdf; Rdf2vec; Artificial Intelligence; Birds; Information Systems; Knowledge Management; Bert; Clusterings; Dolce; Gpt; Itil; Ontology's; Owl; Owl2vec; Prompt Engineering; Rdf; Rdf2vec; Mapping; Ontology},
	keywords = {Artificial intelligence; Birds; Information systems; Knowledge management; BERT; Clusterings; DOLCE; GPT; ITIL; Ontology's; OWL; Owl2vec; Prompt engineering; RDF; Rdf2vec; Mapping; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{He2025369,
	author = {He, Bofan and Cheng, Jerry Q. and Gu, Huanying},
	title = {Static and Dynamic Embedding Approaches to Identify Is-A Relations in SNOMED CT},
	year = {2025},
	pages = {369 - 378},
	doi = {10.1109/ICHI64645.2025.00050},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012760706&doi=10.1109%2FICHI64645.2025.00050&partnerID=40&md5=483223ede657cc39d5d7756f354e5d48},
	abstract = {Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT) is a comprehensive clinical terminology with over 350,000 unique concepts and 1 million relationships. It serves as an essential resource for both clinical practice and medical research. Approximately 40% of these relationships are of the is-a type, which is key to classifying concepts as subtypes or subclasses within broader categories and is fundamental for decision support systems and automated reasoning in clinical environments. Identifying is-a relationships in SNOMED CT is crucial for enhancing the accuracy of patient cohort queries, which form the backbone of clinical and research applications. Our study aims to use deep learning-based neural networks to determine whether a relationship between two concepts falls under the is-a or non-is-a categories. This approach can help detect misclassified or undefined is-a concept pairs, ultimately improving the quality of SNOMED CT ontology. We construct a node-edge-node structure for concept pairs and their relationships, which we then split into two categories: is-a and non-is-a classes. In this study, we propose two classification approaches. In the first approach using embeddingBag, the data are mapped into vector representations and used as input for a fully connected neural network to learn the patterns of the is-a relationship. During model training, 80% of the data is used for training and the remaining 20% for testing.We achieved a precision of 0.903, recall of 0.894, and F1 Score of 0.898 in predicting the is-a relation among the associated medical concepts. The second approach involved transformer-based models like BERT(Bidirectional Encoder Representations from Transformers), introduced by Devlin et al. [1], which dramatically advanced various natural language processing (NLP) tasks by offering deeply contextualized word embeddings. And RoBERTa(Robustly optimized BERT approach)[2], both BERT and RoBERTa models have significantly advanced the field of NLP. This work uses them as encoders connected to a classifier head to predict is-a or non-is-a categories. We fine-tune our models using a historical SNOMED CT dataset from 2017, while evaluation is carried out on new data introduced after 2023 in the 2024 release. With a recall of 0.933, precision of 0.933, F1 score of 0.932, accuracy of 0.933, and ROC of 0.972, RoBERTa outperforms the other baseline models (Support Vector Machine(SVM), K-Nearest Neighbors(KNN), Naive Bayes, and Multilayer Perceptron(MLP)) across all metrics. This work demonstrates that our implementation can effectively identify unknown is-a relations in future datasets. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Classification Of Multi Sentence; Deep Learning; Information Retrieval; Large Language Model; Natural Language Processing; Snomed-ct; Classification (of Information); Clinical Research; Computational Linguistics; Deep Neural Networks; Learning Algorithms; Learning Systems; Medical Imaging; Natural Language Processing Systems; Ontology; Terminology; Bidirectional Encoder Representation From Transformer; Classification Of Multi Sentence; Clinical Terms; Deep Learning; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Snomed-ct; Information Retrieval},
	keywords = {Classification (of information); Clinical research; Computational linguistics; Deep neural networks; Learning algorithms; Learning systems; Medical imaging; Natural language processing systems; Ontology; Terminology; Bidirectional encoder representation from transformer; Classification of multi sentence; Clinical terms; Deep learning; Language model; Language processing; Large language model; Natural language processing; Natural languages; SNOMED-CT; Information retrieval},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Cox2025462,
	author = {Cox, Kyle and Qu, Gang and Hsu, Chiyang and Xu, Jiawei and Zhou, Yingtong and Tan, Zhen and Hu, Mengzhou and Chen, Tianlong and Hu, Ziniu and Zhao, Zhongming},
	title = {Thought Graph: Balancing Specificity and Uncertainty in LLM-Based Gene Set Annotation},
	year = {2025},
	pages = {462 - 470},
	doi = {10.1109/ICHI64645.2025.00060},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012713321&doi=10.1109%2FICHI64645.2025.00060&partnerID=40&md5=ce6215766e49f3b8e469f42dc1e59e6c},
	abstract = {Accurate predictive reasoning is a cornerstone of biomedical decision-making, particularly in precision oncology, where elucidating the intricate relationships between disease-risk genes and biological processes is critical. This study presents a novel "Thought Graph"methodology, an advancement of the Tree of Thoughts framework, to systematically generate and refine biological process representations derived from gene sets while addressing the trade-off between specificity and uncertainty. Balancing these factors is essential for robust and interpretable gene set analyses, as it accounts for the complexity, variability, and overlapping functions of biological pathways. Furthermore, we introduce a quantitative metric that integrates specificity and uncertainty, thereby enhancing the rigor and transparency of the inference process. Using a subset of the Gene Ontology database, we evaluate the effectiveness of our system in generating biologically meaningful terms that accurately describe the underlying biological processes of gene sets. We compare its performance against a domain-specific tool (GSEA) and five LLM baselines across multiple metrics. Our system achieves the highest cosine similarity (64.00%) and specificity percentile (96.40%), highlighting its capacity to generate terms closely aligned with human annotations while maintaining a balance between specificity and accuracy. By advancing the artificial intelligence driven analyses, this work facilitates more informed decision-making in biomedical research, precision oncology, and related fields. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Set Analysis; Large Language Model; Medical Reasoning; Precision Oncology; Thought Graph; Artificial Intelligence; Behavioral Research; Decision Making; Gene Ontology; Genes; Oncology; Biological Process; Decisions Makings; Gene Set Analyze; Gene Sets; Language Model; Large Language Model; Medical Reasonings; Precision Oncology; Thought Graph; Uncertainty; Economic And Social Effects},
	keywords = {Artificial intelligence; Behavioral research; Decision making; Gene Ontology; Genes; Oncology; Biological process; Decisions makings; Gene set analyze; Gene sets; Language model; Large language model; Medical reasonings; Precision oncology; Thought graph; Uncertainty; Economic and social effects},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Farmer2025,
	author = {Farmer, E. E. and Brown, David and Gore, Michael Allen and Tufan, Hale Ann},
	title = {Applying large language models to extract information from crop trait prioritization studies},
	year = {2025},
	journal = {Plants People Planet},
	pages = {},
	doi = {10.1002/ppp3.70075},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012596600&doi=10.1002%2Fppp3.70075&partnerID=40&md5=b9884d7e0b7f87c16a05250ca3afa6c0},
	abstract = {Societal Impact Statement: Investigation of farmers', consumers', and other stakeholders' trait preferences is vital for the adoption and impact of improved crop varieties. While qualitative research methods are known to increase the depth and scope of information from respondents, only 5% of previous trait preference studies used qualitative data in their analyses. We show that AI-based natural language processing, particularly GPTs, is both a time and cost-effective mechanism for accurately analyzing open-ended trait preference data. This will contribute to the selection and prioritization of breeding targets to better meet end-user needs, with implications for food security and health outcomes globally. Summary: Crop trait preference research is critical for the development of improved crop varieties, guiding breeding programs in setting trait priorities and targets that represent farmers' and consumers' needs. However, there is a dearth of methodological harmonization in trait preference studies, leading to high heterogeneity in collected data and analysis frameworks, which constrains comparability between studies. Qualitative research tools using open-ended questions are among the most common methods used to elucidate crop trait preferences, but only a fraction of these data are used in analysis. The ascendance of AI tools in data analysis provides an opportunity to enhance capitalization of these data from open-ended question types. We use natural language processing (NLP) techniques, including generative pretrained transformer (GPT) models, to elucidate labels from open-ended question responses and perform multilabel text classification. We compare these labels to pre-codes from close-ended questions, as well as to existing crop trait ontology terms. We find that analyzing responses to open-ended questions using NLP leads to information gain, including an increase in diversity of traits and insight into their social functions. We conclude that using NLP-based approaches would allow breeding teams to extract trait terms from open-ended question responses efficiently and to compare these to both existing ontology terms and close-ended survey data. Our findings reveal the importance of using open-ended questions to inform survey codes in mixed methods research design for trait preference studies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cassava Breeding; Generative Pretrained Transformers; Large Language Models; Natural Language Processing; Trait Preferences},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Neupane2025,
	author = {Neupane, Roshan Lal and Pusapati, Vamsi K.V. and Edara, Lakshmi Srinivas and Cheng, Xiyao and Neupane, Kiran and Chintapatla, Harshavardhan and Mitra, Reshmi and Korkali, Mert and Suk Na, Hyeong and Srinivas, Sharan},
	title = {Securing Inverter-Based Resources via Knowledge-Driven Threat Modeling, Analysis, and Mitigation},
	year = {2025},
	pages = {},
	doi = {10.1109/NOMS57970.2025.11073642},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012224579&doi=10.1109%2FNOMS57970.2025.11073642&partnerID=40&md5=10a92592b3f95da8348b310137c47621},
	abstract = {Inverter-based Resources (IBRs) present unique cybersecurity challenges due to their digital control systems and connection to the electric grid. They rely on digital communications and control systems, making them vulnerable to cyberattacks. These attacks can disrupt grid operations and stability, compromise data, or cause physical damage to equipment. To address these challenges, it is essential to establish robust cybersecurity measures that meet and exceed existing industry standards. In this paper, we describe a comprehensive strategy to bolster the cybersecurity of IBRs through cutting-edge applications and technologies via a cybersecurity framework called 'CIBR-Fort', a knowledge-driven, interoperable, scalable, and manageable framework for modeling, analysis, and mitigation of cyber threats disrupting different components of IBR systems. Our knowledge-driven analysis consists of a fusion of knowledge graphs (KGs) in cybersecurity and the electric grid, achieved through link prediction leveraging Large Language Models (LLMs) and cosine similarity, attributed towards informed decision-making for threat mitigation. The evaluation results show how we can automate LLM-driven link prediction based on the fusion of two distantly separated ontologies, generating a dataset that can be used for scaling via graph learning that can be utilized for further security analyses of IBR systems. In addition, we show our knowledge-driven threat analysis can predict different attacks with 91.88% maximum accuracy. Lastly, we show how we can achieve real-time end-to-end threat mitigation with an average of 40 ms per traffic flow. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cybersecurity; Inverter-based Resources; Knowledge Graph; Large Language Models; Retrieval Augmented Generation; Control Systems; Cybersecurity; Digital Communication Systems; Electric Inverters; Electric Machine Control; Interoperability; Knowledge Graph; Modeling Languages; Network Security; Cyber Security; Inverter-based; Inverter-based Resource; Knowledge Graphs; Language Model; Large Language Model; Modeling Analyzes; Retrieval Augmented Generation; Threats Analysis; Threats Mitigations; Forecasting},
	keywords = {Control systems; Cybersecurity; Digital communication systems; Electric inverters; Electric machine control; Interoperability; Knowledge graph; Modeling languages; Network security; Cyber security; Inverter-based; Inverter-based resource; Knowledge graphs; Language model; Large language model; Modeling analyzes; Retrieval augmented generation; Threats analysis; Threats mitigations; Forecasting},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Blasch2025192,
	author = {Blasch, Erik Philip and Insaurralde, Carlos C.},
	title = {Space and Air Traffic Management Situation Awareness with Notices},
	year = {2025},
	pages = {192 - 199},
	doi = {10.1109/CogSIMA64436.2025.11079470},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012195937&doi=10.1109%2FCogSIMA64436.2025.11079470&partnerID=40&md5=1baa5281582c55021deba13c960314cc},
	abstract = {Air traffic management has long been associated with situation awareness, especially supporting pilots in assessment and response to challenging situations. For multi-domain air and space operations, artificial intelligence and recently large language models (LLMs) can increase semantic understanding. In this paper, we focus on LLM aerospace analysis of Notice to Airman (NOTAM) and Notice to Space Operators (NOTSO). The notices afford a communication of the situation that can be extracted as an ontology to support human operators. Results show that LLM-based clustering can facilitate cognitive situation awareness. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Air Traffic Management; Data Fusion; Space Traffic Management; Uncertainty Ontology; Air Navigation; Air Traffic Control; Air Transportation; Artificial Intelligence; Data Fusion; Digital Avionics; Information Management; Semantics; Air Operation; Air Traffic Management; Language Model; Multi-domains; Ontology's; Situation Awareness; Space Operations; Space Traffic Management; Uncertainty; Uncertainty Ontology; Ontology},
	keywords = {Air navigation; Air traffic control; Air transportation; Artificial intelligence; Data fusion; Digital avionics; Information management; Semantics; Air operation; Air Traffic Management; Language model; Multi-domains; Ontology's; Situation awareness; Space operations; Space traffic management; Uncertainty; Uncertainty ontology; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ronzhin2025201,
	author = {Ronzhin, Lev V. and Astanin, Pavel A. and Rauzina, Svetlana E. and Yadgarova, Polina A. and Zarubina, Tatyana V.},
	title = {Development of a service for automatically extraction of medical concepts from Russian unstructured texts},
	year = {2025},
	journal = {Siberian Journal of Clinical and Experimental Medicine},
	volume = {40},
	number = {2},
	pages = {201 - 210},
	doi = {10.29001/2073-8552-2025-40-2-201-210},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012154444&doi=10.29001%2F2073-8552-2025-40-2-201-210&partnerID=40&md5=55aa907ea0355bcad5fc8460e8e7d2b8},
	abstract = {Introduction. A significant part of medical data is currently generated and stored in an unstructured (textual) form. One way to process unstructured information is named entity recognition (NER). In the classical view, solving the NER problem within medical texts involves identifying objects or concepts that have a specific context related to the actions or events mentioned in the text. The National Unified Terminological System (NUTS) has been developed since 2022 based on international and federal medical thesauri and other sources. It can be used as the term set for solving problems of this type. At the time of the study, there was no available information in the scientific literature about tools solving NER problem in unstructured Russian-language medical texts. Aim: To develop a tool for extracting named entities from Russian-language medical texts. Material and Methods. Named entity recognition is performed using the NUTS as the terminological framework. The preprocessing pipeline includes full text segmentation, sentences tokenization and dependency parsing, words lemmatization and morphological analysis. The Annotation tool has been evaluated on clinical guidelines. The primary evaluation metric is the ratio of correctly identified terms to the total number of experts’ extracted terms. Results. As part of this study, the Annotation tool for medical texts has been developed. It is an automatized tool for extraction and categorization NUTS terms. This service is based on combined use large language models and rules. The Annotation tool can analyze texts in any language of the Indo-European group using any terminological system. The Annotation tool is hybrid and extracts automatically up to 93% of terms from the actual unstructured guidelines texts. The quality of this service is comparable to international NER tools for English-language texts: cTAKES with 91% accuracy and MetaMap with an F1-score of 88%. Conclusion. The article presents the Annotation tool-a hybrid service for named entity recognition within unstructured medical texts. The service was validated by extraction of NUTS terms in current clinical guidelines, with subsequent verification by medical experts. The obtained results demonstrate the promising potential of both this tool and the National Unified terminology system (NUTS). © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept; Knowledge Base; Named Entity Recognition; Natural Language Processing; Ner; Nlp; Nuts; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ferraz2025,
	author = {Ferraz, Lucas and Cotovio, Pedro Giesteira and Pesquita, Cátia},
	title = {Can Language Models Align Biomedical Ontologies?: Evaluating Retrieval-Augmented Prompt Strategies in Bio-ML},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {4001},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012127875&partnerID=40&md5=644ab89c151aee621af0eef8183870b8},
	abstract = {Aligning biomedical ontologies presents a significant challenge due to their complexity and the highly domain-specific nature of their vocabulary. Recent advancements in Language Models (LMs) have led to their increasing application in ontology alignment tasks, offering promising results. However, a systematic evaluation of semantics-based prompting strategies for leveraging LMs in this context remains unexplored. This study investigates the effectiveness of different prompting techniques to enhance biomedical ontology alignment performance. We have developed a framework to support the design of LM-based queries to assess the semantic similarity between ontology classes. The framework interrogates the ontologies to align to extract relevant contextual information to inject into the LM prompts allowing the use of Retrieval Augmented Generation (RAG). We conduct preliminary experiments on selected hard cases from biomedical ontologies that compose the Ontology Alignment Evaluation Initiative Bio-ML track and provide some insights into the effectiveness, reliability, and limitations of prompt-based approaches in ontology matching. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Representation; Language Models; Ontology Alignment; Alignment; Computational Linguistics; Ontology; Query Processing; Semantics; Biomedical Ontologies; Domain Specific; Knowledge-representation; Language Model; Model-based Opc; Ontology Alignment; Ontology's; Performance; Specific Nature; Systematic Evaluation; Knowledge Representation},
	keywords = {Alignment; Computational linguistics; Ontology; Query processing; Semantics; Biomedical ontologies; Domain specific; Knowledge-representation; Language model; Model-based OPC; Ontology alignment; Ontology's; Performance; Specific nature; Systematic evaluation; Knowledge representation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2025,
	title = {SeWeBMeDa 2025 - Proceedings of the 8th Workshop on Semantic Web Solutions for Large-Scale Biomedical Data Analytics, co-located with the ESWC 2025: Extended Semantic Web Conference, ESWC 2025},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {4001},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012115823&partnerID=40&md5=db7ebf70d2ebc3ac4ff92f0a88824d20},
	abstract = {The proceedings contain 6 papers. The topics discussed include: knowledge graphs for explainable scientific discovery; AI based chatbots vs. traditional search: a systematic comparison of response quality for dementia management information; constructing CCEE an LLM evaluation dataset for complex context-aware event extraction for gene regulatory networks; using clinical guidelines, domain ontology, and LLMs for personalized leukemia treatment recommendations; LISE: a logic-based interactive similarity explainery; and can language models align biomedical ontologies?: evaluating retrieval augmented prompt strategies in Bio-ML. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Pokkuluri2025,
	author = {Pokkuluri, Kiran Sree and Jain, Kirti and Lakshmi, V. Sravani Rajya and Pastariya, Rishab and Lathigara, Amit M. and Navanitha, Dubbaka},
	title = {A Unified Knowledge Base for Drug Label Analysis Using Learning Models, NLP, and IoT Tasks},
	year = {2025},
	pages = {},
	doi = {10.1109/OTCON65728.2025.11070706},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012096608&doi=10.1109%2FOTCON65728.2025.11070706&partnerID=40&md5=d6df086f35ad5793a3bfb981ad843c0f},
	abstract = {Linguistic transmission advanced humanity. Voice, text, and pictures affect knowledge transmission. Techniques for interpolating data may retain knowledge by determining semantic equivalents based on its context, the lesson, and connections without sacrificing data integrity. Knowledge has affected the security and privacy of data, showing vulnerability on every side of privacy and security violations with numerous legislation and standards. linguistic models convert voice impulses to digital data to examine phonological, prosodic, phonotactic, and lexical features. In contrast, embedding text in pictures and voice patterns modify word meaning via resolution, simplicity, and pitch adjustment. NLP uses vectors to detect word similarity, ensuring anonymity for ontology-based understanding with compatibility while deciphering control features. Using text-based semantic similarities analysis, visual text is classed by arrangement and topic. Every individual's linguistic dialect is distinct, with a tendency toward the local dialect. Due of rapid modifications to the environment, methods' acoustic qualities are difficult to transmit. Voice translating syllables might or might not communicate word feeling according to phonetical modification. A learnt variables taxonomy with prosody properties helps knowledge bases find relevant material without intermediary stages. For successful information transmission, integration, and edge manufacturing, edge learning requires context. An ontology of learnt connections from text, speech, and pictures shows information resemblance usefulness, and interest. The suggested XTI-CNN technique excels in all four metrics and has 93.2% accuracy, making it ideal for hybrid analysis of data in related to drugs data extraction applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Context Learning; Interoperability; Language Models; Ontology; Privateness; Semantic Reasoning; Semantic Similarity; Speech Recognition; Text Extraction; Voice Activity Detection; Voice Cloning; Character Recognition; Computational Linguistics; Data Mining; Data Privacy; Knowledge Based Systems; Knowledge Management; Knowledge Transfer; Laws And Legislation; Natural Language Processing Systems; Semantics; Speech Transmission; Text Processing; Context Learning; Language Model; Learn+; Ontology's; Privateness; Semantic Reasoning; Semantic Similarity; Text Extraction; Voice Cloning; Voice-activity Detections; Ontology; Speech Recognition},
	keywords = {Character recognition; Computational linguistics; Data mining; Data privacy; Knowledge based systems; Knowledge management; Knowledge transfer; Laws and legislation; Natural language processing systems; Semantics; Speech transmission; Text processing; Context learning; Language model; Learn+; Ontology's; Privateness; Semantic reasoning; Semantic similarity; Text extraction; Voice cloning; Voice-activity detections; Ontology; Speech recognition},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Li2025706,
	author = {Li, Yi and Lu, Wenxin and Xiang, Xiuzhen},
	title = {OntoCons: A Method for Intelligent Construction of Domain Ontology Models for Large Language Models},
	year = {2025},
	pages = {706 - 712},
	doi = {10.1109/NNICE64954.2025.11063905},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012094740&doi=10.1109%2FNNICE64954.2025.11063905&partnerID=40&md5=99fdc72416564aaccebf00ec8b2b1c76},
	abstract = {With the development of large language models technology, its application in the field of education has been steadily expanding. However, due to limitations in understanding specialized literature in vertical fields, large language models face serious issues of 'hallucination' and prior bias when responding to teachers' questions about specialized domains. To address this, this paper proposes an intelligent construction framework for ontology models based on knowledge tuple extraction, called OntoCons, which introduces entity extraction and relation extraction methods into the field of ontology model construction. OntoCons transforms the ontology model construction problem into a knowledge tuple extraction problem, enhancing the transparency and interpretability of ontology model construction methods, and introduces subgraph encoding strategies to improve the accuracy of relation extraction. This research provides a new method for the intelligent construction of ontology models in vertical fields, improving the understanding and response quality of large language models in specialized domains by combining machine learning and manual analysis. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Entity Extraction; Knowledge Extraction; Large Language Model; Ontology Model; Relation Extraction; Domain Knowledge; Engineering Education; Learning Systems; Ontology; Teaching; Entity Extractions; Intelligent Constructions; Knowledge Extraction; Language Model; Large Language Model; Model Construction; Ontology Model; Relation Extraction; Tuples Extraction; Vertical Fields; Extraction},
	keywords = {Domain Knowledge; Engineering education; Learning systems; Ontology; Teaching; Entity extractions; Intelligent constructions; Knowledge extraction; Language model; Large language model; Model construction; Ontology model; Relation extraction; Tuples extraction; Vertical fields; Extraction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Xu2025,
	author = {Xu, Xingru and Dumontier, Michel J. and Sun, Chang},
	title = {Using Clinical Guidelines, domain ontology, and LLMs for Personalized Leukemia Treatment Recommendations},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {4001},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012093656&partnerID=40&md5=9413007753d2abbc6649416d698bf552},
	abstract = {Large Language Models (LLMs) offer new opportunities for clinical decision support, but face challenges in reliability, precise recommendations for individual patients, and adherence to medical guidelines. Challenges such as insufficient domain knowledge, generic outputs, and hallucination are risks to their clinical adoption. This paper proposes an approach that integrates LLMs with Clinical Practice Guidelines (CPGs) and medical ontologies to enhance personalized treatment recommendations. We compared four strategies to generate treatment recommendations with and without integrating clinical guidelines: (1) LLMs without any guideline input, (2) providing the full guideline document as textual input to LLMs with retrieval-augmented generation (RAG) technique; (3) converting guideline documents from PDF to markdown files capturing the structure of tables, diagrams, and references and using Chain-of-Thoughts to reason each decision steps; (4) structuring guidelines as graphs and linking medical concepts to ontologies as input to LLMs. We experimented on GPT-3.5 Turbo, GPT-4, and Llama 2. The evaluations assessed guideline adherence, treatment completeness, path alignment, and answer relevancy with Acute Lymphoblastic Leukemia as the primary use case. Additionally, we developed a user interface for health professionals to input patient descriptions and obtain treatment recommendations and explanations. Preliminary results demonstrate the feasibility of the graph-based approach in decision path tracing, graph-augmented reasoning, and natural language explanations to enhance transparency for clinician validation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Practice Guidelines; Knowledge Graphs; Large Language Models; Ontologies; Treatment Recommendation; Decision Support Systems; Diseases; Graph Structures; Graph Theory; Graphic Methods; Information Retrieval; Knowledge Graph; Patient Treatment; User Interfaces; Clinical Decision Support; Clinical Guideline; Clinical Practice Guidelines; Domain Ontologies; Knowledge Graphs; Language Model; Large Language Model; Medical Guidelines; Ontology's; Treatment Recommendation; Ontology},
	keywords = {Decision support systems; Diseases; Graph structures; Graph theory; Graphic methods; Information retrieval; Knowledge graph; Patient treatment; User interfaces; Clinical decision support; Clinical guideline; Clinical practice guidelines; Domain ontologies; Knowledge graphs; Language model; Large language model; Medical guidelines; Ontology's; Treatment recommendation; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Matta2025825,
	author = {Matta, Nada and Pfundstein, Patrick and Larde, Camille and Ismedon, Baptiste},
	title = {Answering Application of Generative AI in Industry: Integration of Semantic Representation},
	year = {2025},
	journal = {Proceedings of the International Conference on Computer Supported Cooperative Work in Design, CSCWD},
	number = {2025},
	pages = {825 - 828},
	doi = {10.1109/CSCWD64889.2025.11033426},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011476486&doi=10.1109%2FCSCWD64889.2025.11033426&partnerID=40&md5=b01a808593b5253137353a16e27243b2},
	abstract = {Generative AI acts as an effective guide in decision-making process. This paper is designed to outline the main challenges in applying this technique within businesses and for specific activities. Semantic representation as ontology can be one solution for these challenges. Our first work to link ontology to LLM algorithms is illustrated in financial institutions © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bard; Chatgpt; Finance; Generative Ai; Llm; Ontology; Artificial Intelligence; Decision Making; Finance; Information Management; Information Systems; Information Use; Semantic Web; Semantics; Bard; Chatgpt; Decision-making Process; Financial Institution; Generative Ai; Llm; Ontology's; Semantic Representation; Specific Activity; Ontology},
	keywords = {Artificial intelligence; Decision making; Finance; Information management; Information systems; Information use; Semantic Web; Semantics; Bard; ChatGPT; Decision-making process; Financial institution; Generative AI; LLM; Ontology's; Semantic representation; Specific activity; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Uher2025,
	author = {Uher, Jana},
	title = {Statistics is not measurement: The inbuilt semantics of psychometric scales and language-based models obscures crucial epistemic differences},
	year = {2025},
	journal = {Frontiers in Psychology},
	volume = {16},
	pages = {},
	doi = {10.3389/fpsyg.2025.1534270},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011346724&doi=10.3389%2Ffpsyg.2025.1534270&partnerID=40&md5=59b8c3b557d2eace3374411d40c2cc92},
	abstract = {This article provides a comprehensive critique of psychology's overreliance on statistical modelling at the expense of epistemologically grounded measurement processes. It highlights that statistics deals with structural relations in data regardless of what these data represent, whereas measurement establishes traceable empirical relations between the phenomena studied and the data representing information about them. These crucial epistemic differences are elaborated using Rosen's general model of measurement, involving the coherent modelling of the (1) objects of research, (2) data generation (encoding), (3) formal manipulation (e.g., statistical analysis) and (4) result interpretation regarding the objects studied (decoding). This system of interrelated modelling relations is shown to underlie metrologists' approaches for tackling the problem of epistemic circularity in physical measurement, illustrated in the special cases of measurement coordination and calibration. The article then explicates psychology's challenges for establishing genuine analogues of measurement, which arise from the peculiarities of its study phenomena (e.g., higher-order complexity, non-ergodicity) and language-based methods (e.g., inbuilt semantics). It demonstrates that psychometrics cannot establish coordinated and calibrated modelling relations, thus generating only pragmatic quantifications with predictive power but precluding epistemically justified inferences on the phenomena studied. This epistemic gap is often overlooked, however, because many psychologists mistake their methods' inbuilt semantics—thus, descriptions of their study phenomena (e.g., in rating scales, item variables, statistical models)—for the phenomena described. This blurs the epistemically necessary distinction between the phenomena studied and those used as means of investigation, thereby confusing ontological with epistemological concepts—psychologists' cardinal error. Therefore, many mistake judgements of verbal statements for measurements of the phenomena described and overlook that statistics can neither establish nor analyze a model's relations to the phenomena explored. The article elaborates epistemological and methodological fundamentals to establish coherent modelling relations between real and formal study system and to distinguish the epistemic components involved, considering psychology's peculiarities. It shows that epistemically justified inferences necessitate methods for analysing individuals' unrestricted verbal responses, now advanced through artificial intelligence systems modelling natural language (e.g., NLP algorithms, LLMs). Their increasing use to generate standardised descriptions of study phenomena for rating scales and constructs, by contrast, will only perpetuate psychologists' cardinal error—and thus, psychology's crisis. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models (llms); Measurement; Metrology; Modelling Relation; Natural Language Processing (nlp); Psychometrics; Rating Scales; Semantics-syntax},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Gueddes2025461,
	author = {Gueddes, Abdelweheb and Fathallah, Wyssem and Mahjoub, Mohamed Ali},
	title = {BERT-Based Knowledge Graph Construction from Social Media},
	year = {2025},
	pages = {461 - 466},
	doi = {10.1109/IWCMC65282.2025.11059459},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011341967&doi=10.1109%2FIWCMC65282.2025.11059459&partnerID=40&md5=8d43dc25e5b830cec8148a9b3faeb1f3},
	abstract = {Social media platforms serve as massive repositories of textual data, reflecting diverse human interactions and preferences. However, the unstructured nature of this content poses significant challenges for extracting semantically rich insights. This paper introduces a novel methodology for the automated construction of knowledge graphs (KGs) from social media discourse, specifically focusing on Twitter tweets. Our approach synergistically integrates large language models (LLMs), specifically a fine-tuned BERT model, with an ontology-driven framework. First, we define a detailed ontology of online communication concepts. The pre-trained BERT model is then fine-tuned using a multi-task learning approach on a curated dataset of anonymized and segmented Twitter discussions, thereby aligning its semantic representations with the predefined ontology. The fine-tuned LLM is leveraged for several critical tasks including entity and relation extraction, sentiment analysis, intention classification and the inference of contextual information and discussion styles. Furthermore, a mechanism is introduced to infer inter-user relationships and shared interests using graph neural networks (GNNs), analyzing patterns in interaction and language use. This multi-faceted extracted and inferred data is subsequently employed to build a knowledge graph, stored and queried via the Neo4j graph database management system. This study presents several contributions such as the integration of a ontology with an LLM method and the innovative user relationship and shared interest extraction using graph neural networks. The proposed methodology was rigorously evaluated using a real-world dataset of Twitter discussions, showcasing its ability to capture semantic content, and elucidate inter-user relationships effectively and revealing shared interests of the users involved. Furthermore, an ablation study is included which further demonstrates each of the method component contribution and demonstrates the importance of such integrations. Our findings highlight the potential for various downstream applications such as in community structure analysis and sentiment analysis to improve information management within online social networks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Knowledge Graph; Large Language Models (llms); Ontology-driven Methodology; Social Media Analysis; Classification (of Information); Information Management; Knowledge Graph; Management Information Systems; Ontology; Semantics; Sentiment Analysis; Social Networking (online); Tweets; Bert; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Ontology-driven Methodology; Social Media; Social Media Analysis; Users' Relationships; Extraction},
	keywords = {Classification (of information); Information management; Knowledge graph; Management information systems; Ontology; Semantics; Sentiment analysis; Social networking (online); Tweets; BERT; Knowledge graphs; Language model; Large language model; Ontology's; Ontology-driven methodology; Social media; Social media analysis; Users' relationships; Extraction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Do2025555,
	author = {Do, Thanh Son and Hier, Daniel B. and Obafemi-Ajayi, Tayo},
	title = {Mapping Biomedical Ontology Terms to Ids: Effect of Domain Prevalence on Prediction Accuracy},
	year = {2025},
	pages = {555 - 560},
	doi = {10.1109/CAI64502.2025.00101},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011289811&doi=10.1109%2FCAI64502.2025.00101&partnerID=40&md5=6e57aef3b13e1dde5d7b5468be8c8a9b},
	abstract = {This study evaluates the ability of large language models (LLMs) to map biomedical ontology terms to their corresponding ontology IDs across the Human Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies. Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate for their prevalence in the biomedical literature, we examined the relationship between ontology ID prevalence and mapping accuracy. Results indicate that ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers. Higher prevalence of ontology IDs in the biomedical literature correlated with higher mapping accuracy. Predictive models based on receiver operating characteristic (ROC) curves confirmed this relationship. In contrast, this pattern did not apply to mapping protein names to Human Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline performance (95 %) in mapping protein names to HUGO gene symbols, with mapping accuracy unaffected by prevalence. We propose that the high prevalence of HUGO gene symbols in the literature has caused these symbols to become lexicalized, enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy. These findings highlight the limitations of LLMs in mapping ontology terms to low-prevalence ontology IDs and underscore the importance of incorporating ontology ID prevalence into the training and evaluation of LLMs for biomedical applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontology; Human Phenotype Ontology; Large Language Models; Lexicalization; Machine Codes; Ontology Mapping; Uniprot Kb; Zipf's Law; Codes (symbols); Gene Ontology; Genes; Mapping; Systems Analysis; Terminology; Gene Ontology; Human Phenotype Ontology; Language Model; Large Language Model; Lexicalization; Machine Codes; Ontology Mapping; Ontology's; Uniprot; Uniprot Kb; Zipf Law; Medical Applications},
	keywords = {Codes (symbols); Gene Ontology; Genes; Mapping; Systems analysis; Terminology; Gene ontology; Human phenotype ontology; Language model; Large language model; Lexicalization; Machine codes; Ontology mapping; Ontology's; Uniprot; Uniprot KB; Zipf Law; Medical applications},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Xu20251610,
	author = {Xu, Bin and Tong, Richard Jiarui and Li, Yanyan and Chen, Penghe and Li, Hanming and Liang, Joleen and Fan, Xing and Tong, Jessie},
	title = {An Architectural Framework for Educational Knowledge Graphs (IEEE P2807.6): Ontology Design, Llm Integration, and Adaptive Learning Applications},
	year = {2025},
	pages = {1610 - 1616},
	doi = {10.1109/CAI64502.2025.00249},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011260890&doi=10.1109%2FCAI64502.2025.00249&partnerID=40&md5=d8569498bbc3f1e4f5d1adbc9ebd141f},
	abstract = {The IEEE P2807.6 Education Knowledge Graph (EduKG) standard defines a semantic infrastructure to represent educational knowledge, resources, and pedagogy in a unified graph format. This paper expands on the core EduKG architecture, detailing its ontology design and key entities-Learning Points, Resource Items, and Pedagogical Rules-that collectively model the domain, content, and instructional strategies of learning systems. We further explore how EduKG can be integrated with advanced AI technologies, including large language models (LLMs) and retrieval-augmented generation (Graph-RAG) via embedding databases, to enable intelligent behavior such as semantic search, question answering, and dynamic content generation. These integrations position EduKG as a central component in next-generation smart education systems, wherein knowledge graphs work in concert with intelligent agents and adaptive instructional systems to deliver fully automated, personalized, and interactive learning experiences. By leveraging the standardized graph-structured representation and semantic reasoning capabilities of EduKG, such systems can achieve interoperability across platforms and support complex AI-driven tutoring and training scenarios. This work provides a comprehensive overview of the EduKG framework and highlights its role in empowering adaptive, cognitive, and collaborative learning solutions for the future of digital education. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adaptive Instructional Systems; Education Knowledge Graph; Intelligent Tutoring; Interoperability; Large Language Models; Ontology; Retrieval-augmented Generation; Cognitive Systems; Collaborative Learning; Computer Aided Instruction; E-learning; Education Computing; Engineering Education; Graph Structures; Graph Theory; Graphic Methods; Intelligent Agents; Knowledge Graph; Knowledge Management; Learning Systems; Search Engines; Semantics; Teaching; Adaptive Instructional System; Education Knowledge Graph; Educational Knowledge; Instructional System; Intelligent Tutoring; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Retrieval-augmented Generation; Ontology},
	keywords = {Cognitive systems; Collaborative learning; Computer aided instruction; E-learning; Education computing; Engineering education; Graph structures; Graph theory; Graphic methods; Intelligent agents; Knowledge graph; Knowledge management; Learning systems; Search engines; Semantics; Teaching; Adaptive instructional system; Education knowledge graph; Educational knowledge; Instructional system; Intelligent tutoring; Knowledge graphs; Language model; Large language model; Ontology's; Retrieval-augmented generation; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Gao2025,
	author = {Gao, Qinxin and Li, Minmin and Yang, Qi and Zhu, Wei and Wei, Shilong},
	title = {Knowledge graph construction for historic architectural complexes: a case study of the grand view garden in the story of the stone},
	year = {2025},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {13642},
	pages = {},
	doi = {10.1117/12.3066895},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011095921&doi=10.1117%2F12.3066895&partnerID=40&md5=dba12cc33013f09d0a98c6389995b0bc},
	abstract = {The preservation and spatial reconstruction of historical cultural heritage represent pivotal challenges in the field of cultural heritage studies. The extensive and complex knowledge associated with such heritage necessitates the use of knowledge graphs to facilitate integration and management, addressing practical application needs. However, research on historical cultural heritage remains limited, particularly architectural complexes. Spatial layout considerations are often overlooked, and construction methodologies lack efficiency. This study uses the Grand View Garden in The Story of the Stone as a case study to establish a knowledge ontology for historic architectural complexes. The ontology enables automated and highly accurate knowledge extraction leveraging large language models. It supports the development of geographic and architectural component knowledge graphs for the Grand View Garden. This approach provides knowledge services for digital reconstruction and intelligent applications of historical and cultural heritage. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Grand View Garden; Historic Architecture; Knowledge Graph; The Story Of The Stone; Architecture; Historic Preservation; Knowledge Graph; Knowledge Management; Architectural Complexes; Case-studies; Cultural Heritage Studies; Cultural Heritages; Grand View Garden; Graph Construction; Historic Architecture; Knowledge Graphs; Spatial Reconstruction; The Story Of The Stone; Ontology},
	keywords = {Architecture; Historic preservation; Knowledge graph; Knowledge management; Architectural complexes; Case-studies; Cultural heritage studies; Cultural heritages; Grand view garden; Graph construction; Historic architecture; Knowledge graphs; Spatial reconstruction; The story of the stone; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kalinina202541,
	author = {Kalinina, Svetlana V. and Kotsyubinskaya, Lyubov V.},
	title = {ONTOLOGY AS A TOOL FOR MASHINE TRANSLATION TERMINOLOGICAL OPTIMIZATION OF OIL AND GAS INDUSTRY LEXIS},
	year = {2025},
	journal = {Terra Linguistica},
	volume = {16},
	number = {2},
	pages = {41 - 55},
	doi = {10.18721/JHSS.16203},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011093374&doi=10.18721%2FJHSS.16203&partnerID=40&md5=97c10b332c5c8614893ed8b7ee785763},
	abstract = {The study relevance is stipulated by terminological inaccuracy in machine translation of highly specialized oil and gas industry texts. Competence in domain-specific terminology is the essential prerequisite for effective cross-linguistic communication in global business projects. The modern large language models (LLMs) based on the transformer architecture feature the limitations processing language for special purposes (LSP); thereby appealing to computational lexicography becomes necessary. In the AI era, ontology is a key lexicographic tool providing semantic precision within the certain domains. The linguistic competence of a specialist engaged in natural language text processing is the base for technical knowledge effective management, ensuring transformer adaptation to highly specialized contexts. The article is aimed to substantiate the reasonability of ontological approach application to machine processing of highly specialized texts in natural language. To achieve that aim several tasks shall be addressed: justification of ontological approach to the LSP at the present stage of the applied linguistics progress; description of essential properties of actual English oil and gas terminology; creation of oil and gas equipment ontology fragment using Protégé editor; processing of EN_RU technical text using the DeepSeek R1 transformer model considering the ontology fragment; presentation of the results in a comparative table format. The research material comprises the current terminology lexis functioning in English scientific and technical literature and oil and gas industry periodicals. Methodological basis includes terminology frame analysis procedure, ontology building method. The research results are practically significant for oil and gas translation projects: the ontology application in transformers ensures higher terminological accuracy in comparison with a basic model, providing LLMs adaptation to the technical terminology requirements. Thus, the ontological approach transforms translation procedures through the combination of technology innovation and linguistic expertise for deeper understanding of the technical lexis in specialized knowledge spreading. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Lexicographic Automation; Oil And Gas Industry Terminology; Ontology; Terminology Management; Transformer Model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yaroshko2025219,
	author = {Yaroshko, Taras and Kosa, Victoria and Ignatenko, Oleksii and Makarenkov, Oleksii and Ermolayev, Vadim},
	title = {Engineering Scientific Knowledge Graphs from Publications: The Anti-Corruption Use Case},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2359 CCIS},
	pages = {219 - 232},
	doi = {10.1007/978-3-031-81372-6_16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011080107&doi=10.1007%2F978-3-031-81372-6_16&partnerID=40&md5=ea88bf9dd62a3f121365d4165c1921b1},
	abstract = {In this position paper, we present our approach and early results in engineering an open research knowledge graph for a scholarly domain. All code, data, and results are publicly available for academic purposes at: https://github. com/tyaroshko/orkg-acd. The domain of Anti-Corruption has been chosen as the use case as it is a vibrantly developing field of scholarly research at the intersection of several fields and research communities, such as Legal, Information Science, Data Analytics, Governance, etc. Furthermore, having a methodologically sound and knowledge-based approach for anti-corruption is in demand in Ukraine on its way toward becoming a member of the European Union. Our approach for building the knowledge graph is based on the use of terminology saturation analysis that ensures the representatives of the used literature sample for knowledge extraction. For building knowledge representations from the recognized terminology, a semiautomated approach with a human in the loop is exploited, including the use of LLMs and zero-shot prompt engineering. The results are further validated using human experts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Anti-corruption; Large Language Model; Scientific Domain Ontology; Scientific Knowledge Graph; Terminology Saturation Analysis; Crime; Domain Knowledge; Graphic Methods; Knowledge Graph; Ontology; Anti-corruption; Domain Ontologies; Knowledge Graphs; Language Model; Large Language Model; Position Papers; Scientific Domain Ontology; Scientific Knowledge; Scientific Knowledge Graph; Terminology Saturation Analyze; Terminology},
	keywords = {Crime; Domain Knowledge; Graphic methods; Knowledge graph; Ontology; Anti-corruption; Domain ontologies; Knowledge graphs; Language model; Large language model; Position papers; Scientific domain ontology; Scientific knowledge; Scientific knowledge graph; Terminology saturation analyze; Terminology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Flor20251,
	author = {Flor, Michael},
	title = {Automatic Question Generation},
	year = {2025},
	journal = {Synthesis Lectures on Human Language Technologies},
	volume = {Part F611},
	pages = {1 - 180},
	doi = {10.1007/978-3-031-92072-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011065066&doi=10.1007%2F978-3-031-92072-1&partnerID=40&md5=6354c52c004f0793a662945369582cd0},
	abstract = {This book provides an overview of the fundamentals of Automatic Question Generation (AQG) for computational linguistics researchers, test developers, and educators. The author presents a variety of AQG system architectures, including generating questions from syntactic analyses, semantic resources, neural architectures, ontologies and knowledge graphs, and large language models. The advantages and pitfalls of a variety of AQG evaluation methods, including multi-aspect ratings by human experts, end-users, as well as crowd-sourcing and automatic evaluation techniques are discussed. The book also provides a roadmap of options for AQG targeted orientation, content selection, and focusing decisions. Machine learning opportunities for training systems to generate questions based on human-generated examples are also explored. This book offers greater depth and breadth than previous surveys of AQG. Readers will gain a comprehensive knowledge of current research, examples of applications of AQG, and inspiration for future directions for innovation and application. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Automatic Question Generation; Data Augmentation Methods; Human-computer Studies; Machine Learning; Natural Language Generation; Ontologies And Taxonomies; Computational Linguistics; Computer Architecture; Human Computer Interaction; Information Systems; Machine Learning; Natural Language Processing Systems; Ontology; Semantics; Syntactics; Augmentation Methods; Automatic Question Generation; Data Augmentation; Data Augmentation Method; Generation Systems; Human-computer Study; Machine-learning; Natural Language Generation; Ontology And Taxonomy; Ontology's; Learning Systems},
	keywords = {Computational linguistics; Computer architecture; Human computer interaction; Information systems; Machine learning; Natural language processing systems; Ontology; Semantics; Syntactics; Augmentation methods; Automatic question generation; Data augmentation; Data augmentation method; Generation systems; Human-computer study; Machine-learning; Natural language generation; Ontology and taxonomy; Ontology's; Learning systems},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Robert202522,
	author = {Robert, Kuo Chung Lin and Jessica, Wen Jie Lin},
	title = {Case Study: Apply Ontology and AIGC for Task Appoint in Smart Building Control Centers},
	year = {2025},
	pages = {22 - 26},
	doi = {10.1109/ICAID65275.2025.11034402},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011028437&doi=10.1109%2FICAID65275.2025.11034402&partnerID=40&md5=7bd2eca04f0754580c1d22ce24da8ff6},
	abstract = {Since the amazing effect of ChatGPT has been recognized by the industry, towards integrating with employees' workflow to create more business applications. At the same time, many artificial intelligence scientists are actively looking for methods and platforms that are highly efficient and can quickly embed LLM and gain business value quickly. This study proposes two important innovations. First, it uses historical operating data to quickly and automatically generate LLMs in the company's exclusive domain. Second, to quickly embed those models in ontology maps that to generate thinking and decision-making documents (as AIGC). In the paper, first step is use auto-insight engine to transfer input data from operational datasets. Secondly, to adjust parameters the training in deep learning framework to output models. Finally, integrate more API that they like conversation tools or other services. It can automatically generate and output high-precision event determination results in real time and can automatic notification and complete the processing flow that integrate daily report for frontline partners. In future, it will be able to develop more follow-up applications, such as artificial intelligence assistant for the security control center partner, which can determine the high level of daily incidents and automatically complete high-complexity daily operation procedures in a closed-loop security environment, and moderately connect high-level voice assistants to achieve important indicators such as health care, efficiency improvement and cost reduction. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Aigc; Llms; Ontology; Smart Building; Cost Reduction; Decision Making; Deep Learning; Intelligent Buildings; Personnel; Aigc; Building Controls; Business Applications; Business Value; Case-studies; Control Centre; Llm; Ontology's; Operating Data; Work-flows; Ontology},
	keywords = {Cost reduction; Decision making; Deep learning; Intelligent buildings; Personnel; AIGC; Building controls; Business applications; Business value; Case-studies; Control centre; LLM; Ontology's; Operating data; Work-flows; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Gueddes2025,
	author = {Gueddes, Abdelweheb and Mahjoub, Mohamed Ali},
	title = {BERT-OntoSent: combining BERT language model with sentiment ontology for enhanced sentiment analysis on social media},
	year = {2025},
	journal = {Journal of Information and Telecommunication},
	pages = {},
	doi = {10.1080/24751839.2025.2528363},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010884026&doi=10.1080%2F24751839.2025.2528363&partnerID=40&md5=b34cc46310a8628964e09bac420c314a},
	abstract = {Sentiment analysis on social media is vital but challenged by language complexity and context dependency. Existing methods often fall short. This paper presents BERT-OntoSent, a hybrid approach synergizing BERT's contextual power with ontology-based structured knowledge. We provide a detailed methodology where the DCWEB-SOBA process transforms text into (aspect, opinion, sentiment) triplets, populating a domain-specific BERT-OntoSent ontology used for fine-tuning BERT. SWRL rules within the ontology then refine BERT's predictions, enhancing robustness, particularly for negation and mixed sentiments, as formalized in our proposed refinement algorithm. We present a comprehensive evaluation demonstrating BERT-OntoSent's effectiveness not only on the SemEval 2016 restaurant benchmark but also across diverse domains, including product reviews (Amazon) and microblogs (Twitter). Our results consistently show significant improvements in accuracy and F1-score compared to BERT-only and ontology-only baselines across these datasets. Furthermore, we report on experimental optimizations for scalability, demonstrating practical performance gains. The findings confirm the robustness and broader applicability of the BERT-OntoSent hybrid approach, establishing the value of LLM-ontology synergy for advanced sentiment analysis. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Hybrid Approach; Ontology; Sentiment Analysis; Social Media; Data Mining; Sentiment Analysis; Social Networking (online); Bert; Context Dependency; Hybrid Approach; Language Complexity; Language Model; Ontology's; Ontology-based; Power; Social Media; Ontology},
	keywords = {Data mining; Sentiment analysis; Social networking (online); BERT; Context dependency; Hybrid approach; Language complexity; Language model; Ontology's; Ontology-based; Power; Social media; Ontology},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Niyonkuru2025,
	author = {Niyonkuru, Enock and Caufield, John Harry and Carmody, Leigh C. and Gargano, Michael A. and Toro, Sabrina and Whetzel, Patricia L. and Blau, Hannah and Soto Gomez, Mauricio A. and Casiraghi, Elena and Chimirri, Leonardo},
	title = {Leveraging generative AI to assist biocuration of medical actions for rare disease},
	year = {2025},
	journal = {Bioinformatics Advances},
	volume = {5},
	number = {1},
	pages = {},
	doi = {10.1093/bioadv/vbaf141},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010857151&doi=10.1093%2Fbioadv%2Fvbaf141&partnerID=40&md5=76a23f0f0188e5af556bb4dff10fe4c7},
	abstract = {Motivation: Structured representations of clinical data can support computational analysis of individuals and cohorts, and ontologies representing disease entities and phenotypic abnormalities are now commonly used for translational research. The Medical Action Ontology (MAxO) provides a computational representation of treatments and other actions taken for clinical management. Currently, manual biocuration is used to annotate MAxO terms to rare diseases. However, it is challenging to scale manual curation to comprehensively capture information about medical actions for the more than 10 000 rare diseases. Results: We present AutoMAxO, a semi-automated workflow that leverages Large Language Models (LLMs) to streamline MAxO biocuration. AutoMAxO first uses LLMs to retrieve candidate curations from abstracts of relevant publications. Next, the candidate curations are matched to ontology terms from MAxO, Human Phenotype Ontology (HPO), and MONDO disease ontology via a combination of LLMs and post-processing techniques. Finally, the matched terms are presented in a structured form to a human curator for approval. We used this approach to process abstracts related to 37 rare genetic diseases and identified 958 novel treatment annotations that were transferred to the MAxO annotation dataset. © 2025 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {25th International Conference on Computational Science, ICCS 2025},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15905 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010851169&partnerID=40&md5=19586e6cbfb32b6db0a53d03b9274593},
	abstract = {The proceedings contain 52 papers. The special focus in this conference is on Computational Science. The topics include: Efficient Peptide MRM Transition Prediction via Convolutional Hashing; comparison of Crash Simulations on Two Types of Flying Cars; centrality Resilience in Complex Networks; a Customizable Agent-Based Simulation Framework for Emergency Departments; Leveraging Positional Bias of LLM In-Context Learning with Class-Few-Shot and Maj-Min Alternating Ordering; Remote Sensing AI for Crop Planting in Wildfire Fuel Mapping; Regularization Algorithm for Eliminating Singularities in the PIES Formula for 3D Multidomain Orthotropic Problems; advancing Bird Species Classification: A Fusion of Audio and Image Data; prototype-Pairs Decomposition for Extracting Simple and Meaningful Rules; Fast Prediction of Job Execution Times in the ALICE Grid Through GPU-Based Inference with Quantization and Sparsity Techniques; Reversible Data Hiding in Encrypted Images with Pixel Prediction and ERLE Compression; information Flow Between Neighboring Housing Markets: A Case from the Seoul Metropolitan Area; a Bi-Stage Framework for Automatic Development of Pixel-Based Planar Antenna Structures; Investigation of CUDA Graphs Performance for Selected Parallel Applications; instance Selection by Fast Local Set Border Selector; Modelling the Transient Evolution of Queues in Plugged-in Electric Vehicles (PEV) Fast Charging Stations; is Heterogeneous Model Soup Tasty? A Multidimensional Evaluation of Diverse Model Soups in Language Model Alignment; Performance Evaluation of IMS/NGN Network with SDN-Based Transport Stratum; variable-Resolution Machine Learning for Rapid Multi-Criterial Antenna Design; a Fast and Scalable Genomic Data Compressor for Multicore Clusters; anchored Semantics: Augmenting Ontologies via Competency Questions, Self-Attention, and Predictive Graph Learning; modeling Firm Birth and Death Dynamics Using Survival Fractions and Age Distributions; Enhancing Sentiment Analysis Through Multimodal Fusion: A BERT-DINOv2 Approach; Modeling Parallel AI Applications for Performance Analysis on Cloud Environments; Accelerating LBM with C++ STL Asynchronous Parallel Model. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {25th International Conference on Computational Science, ICCS 2025},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15906 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010847762&partnerID=40&md5=68e2d6cb6006ce624c4cb35b873b9269},
	abstract = {The proceedings contain 52 papers. The special focus in this conference is on Computational Science. The topics include: Efficient Peptide MRM Transition Prediction via Convolutional Hashing; comparison of Crash Simulations on Two Types of Flying Cars; centrality Resilience in Complex Networks; a Customizable Agent-Based Simulation Framework for Emergency Departments; Leveraging Positional Bias of LLM In-Context Learning with Class-Few-Shot and Maj-Min Alternating Ordering; Remote Sensing AI for Crop Planting in Wildfire Fuel Mapping; Regularization Algorithm for Eliminating Singularities in the PIES Formula for 3D Multidomain Orthotropic Problems; advancing Bird Species Classification: A Fusion of Audio and Image Data; prototype-Pairs Decomposition for Extracting Simple and Meaningful Rules; Fast Prediction of Job Execution Times in the ALICE Grid Through GPU-Based Inference with Quantization and Sparsity Techniques; Reversible Data Hiding in Encrypted Images with Pixel Prediction and ERLE Compression; information Flow Between Neighboring Housing Markets: A Case from the Seoul Metropolitan Area; a Bi-Stage Framework for Automatic Development of Pixel-Based Planar Antenna Structures; Investigation of CUDA Graphs Performance for Selected Parallel Applications; instance Selection by Fast Local Set Border Selector; Modelling the Transient Evolution of Queues in Plugged-in Electric Vehicles (PEV) Fast Charging Stations; is Heterogeneous Model Soup Tasty? A Multidimensional Evaluation of Diverse Model Soups in Language Model Alignment; Performance Evaluation of IMS/NGN Network with SDN-Based Transport Stratum; variable-Resolution Machine Learning for Rapid Multi-Criterial Antenna Design; a Fast and Scalable Genomic Data Compressor for Multicore Clusters; anchored Semantics: Augmenting Ontologies via Competency Questions, Self-Attention, and Predictive Graph Learning; modeling Firm Birth and Death Dynamics Using Survival Fractions and Age Distributions; Enhancing Sentiment Analysis Through Multimodal Fusion: A BERT-DINOv2 Approach; Modeling Parallel AI Applications for Performance Analysis on Cloud Environments; Accelerating LBM with C++ STL Asynchronous Parallel Model. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {25th International Conference on Computational Science, ICCS 2025},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15904 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010845429&partnerID=40&md5=50a481879ab01ba56e0612ba606b172b},
	abstract = {The proceedings contain 52 papers. The special focus in this conference is on Computational Science. The topics include: Efficient Peptide MRM Transition Prediction via Convolutional Hashing; comparison of Crash Simulations on Two Types of Flying Cars; centrality Resilience in Complex Networks; a Customizable Agent-Based Simulation Framework for Emergency Departments; Leveraging Positional Bias of LLM In-Context Learning with Class-Few-Shot and Maj-Min Alternating Ordering; Remote Sensing AI for Crop Planting in Wildfire Fuel Mapping; Regularization Algorithm for Eliminating Singularities in the PIES Formula for 3D Multidomain Orthotropic Problems; advancing Bird Species Classification: A Fusion of Audio and Image Data; prototype-Pairs Decomposition for Extracting Simple and Meaningful Rules; Fast Prediction of Job Execution Times in the ALICE Grid Through GPU-Based Inference with Quantization and Sparsity Techniques; Reversible Data Hiding in Encrypted Images with Pixel Prediction and ERLE Compression; information Flow Between Neighboring Housing Markets: A Case from the Seoul Metropolitan Area; a Bi-Stage Framework for Automatic Development of Pixel-Based Planar Antenna Structures; Investigation of CUDA Graphs Performance for Selected Parallel Applications; instance Selection by Fast Local Set Border Selector; Modelling the Transient Evolution of Queues in Plugged-in Electric Vehicles (PEV) Fast Charging Stations; is Heterogeneous Model Soup Tasty? A Multidimensional Evaluation of Diverse Model Soups in Language Model Alignment; Performance Evaluation of IMS/NGN Network with SDN-Based Transport Stratum; variable-Resolution Machine Learning for Rapid Multi-Criterial Antenna Design; a Fast and Scalable Genomic Data Compressor for Multicore Clusters; anchored Semantics: Augmenting Ontologies via Competency Questions, Self-Attention, and Predictive Graph Learning; modeling Firm Birth and Death Dynamics Using Survival Fractions and Age Distributions; Enhancing Sentiment Analysis Through Multimodal Fusion: A BERT-DINOv2 Approach; Modeling Parallel AI Applications for Performance Analysis on Cloud Environments; Accelerating LBM with C++ STL Asynchronous Parallel Model. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {25th International Conference on Computational Science, ICCS 2025},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15903 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010841109&partnerID=40&md5=7fc8207b111db87a41bc9093c8f40a94},
	abstract = {The proceedings contain 52 papers. The special focus in this conference is on Computational Science. The topics include: Efficient Peptide MRM Transition Prediction via Convolutional Hashing; comparison of Crash Simulations on Two Types of Flying Cars; centrality Resilience in Complex Networks; a Customizable Agent-Based Simulation Framework for Emergency Departments; Leveraging Positional Bias of LLM In-Context Learning with Class-Few-Shot and Maj-Min Alternating Ordering; Remote Sensing AI for Crop Planting in Wildfire Fuel Mapping; Regularization Algorithm for Eliminating Singularities in the PIES Formula for 3D Multidomain Orthotropic Problems; advancing Bird Species Classification: A Fusion of Audio and Image Data; prototype-Pairs Decomposition for Extracting Simple and Meaningful Rules; Fast Prediction of Job Execution Times in the ALICE Grid Through GPU-Based Inference with Quantization and Sparsity Techniques; Reversible Data Hiding in Encrypted Images with Pixel Prediction and ERLE Compression; information Flow Between Neighboring Housing Markets: A Case from the Seoul Metropolitan Area; a Bi-Stage Framework for Automatic Development of Pixel-Based Planar Antenna Structures; Investigation of CUDA Graphs Performance for Selected Parallel Applications; instance Selection by Fast Local Set Border Selector; Modelling the Transient Evolution of Queues in Plugged-in Electric Vehicles (PEV) Fast Charging Stations; is Heterogeneous Model Soup Tasty? A Multidimensional Evaluation of Diverse Model Soups in Language Model Alignment; Performance Evaluation of IMS/NGN Network with SDN-Based Transport Stratum; variable-Resolution Machine Learning for Rapid Multi-Criterial Antenna Design; a Fast and Scalable Genomic Data Compressor for Multicore Clusters; anchored Semantics: Augmenting Ontologies via Competency Questions, Self-Attention, and Predictive Graph Learning; modeling Firm Birth and Death Dynamics Using Survival Fractions and Age Distributions; Enhancing Sentiment Analysis Through Multimodal Fusion: A BERT-DINOv2 Approach; Modeling Parallel AI Applications for Performance Analysis on Cloud Environments; Accelerating LBM with C++ STL Asynchronous Parallel Model. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zlobin20253,
	author = {Zlobin, Oleg N. and Litvinov, Vladislav L. and Filippov, Felix V.},
	title = {Domain-Specific Language Models for Continuous Learning},
	year = {2025},
	pages = {3 - 5},
	doi = {10.1109/NeuroNT66873.2025.11049984},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010833709&doi=10.1109%2FNeuroNT66873.2025.11049984&partnerID=40&md5=6bb702d40e7a1acdf5a0c5b03b314b87},
	abstract = {The paper examines the issue of building domain-specific (domain-oriented) language models and the possibility of their continuous learning. A concept is proposed that provides for the introduction of additional cognitive memory with domain administration, designed for the gradual accumulation of new information obtained after the termination of learning the language model. The issue of transition from the inference mode to a new model learning is considered. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Cognitive Memory; Domains; Embeddings; Language Models; Ontology Concepts; Update Context; Verbal Memory; Computational Linguistics; Learning Systems; Ontology; Problem Oriented Languages; Cognitive Memory; Continuous Learning; Domain; Domain Specific; Domains Specific Languages; Embeddings; Language Model; Ontology Concepts; Update Context; Verbal Memory; Artificial Intelligence},
	keywords = {Computational linguistics; Learning systems; Ontology; Problem oriented languages; Cognitive memory; Continuous learning; Domain; Domain specific; Domains specific languages; Embeddings; Language model; Ontology concepts; Update context; Verbal memory; Artificial intelligence},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025189,
	author = {Li, Shengqi and Gupta, Amarnath},
	title = {Anchored Semantics: Augmenting Ontologies via Competency Questions, Self-Attention, and Predictive Graph Learning},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15906 LNCS},
	pages = {189 - 197},
	doi = {10.1007/978-3-031-97635-3_23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010817112&doi=10.1007%2F978-3-031-97635-3_23&partnerID=40&md5=a79006385e5bb752062dec2d382cd7a3},
	abstract = {We propose a framework that enriches ontologies by leveraging competency questions and distant supervision. The process begins by using an LLM to extract domain-relevant entities from the questions, followed by incremental refinement through short definitions anchored to a predefined dictionary. These entities and their hierarchies, along with associated queries, are embedded using a fine-tuned Llama3.2:1b and further processed through a self-attention mechanism to create unified representations. A directed acyclic graph models the dependencies between entities, with additional nodes derived from frequent co-occurrences in queries. A Graph Attention Network (GAT) is used for stable link prediction, discovering latent semantic relationships. These links are then labeled with specific relation types using a fine-tuned RoBERTa module. Evaluations using datasets from HPC training sessions and OpenAlex abstracts show significant improvements in link prediction and ontology enrichment over standard GAT and GraphSage baselines. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Competency Questions; Distant Supervision; Graph Embedding; Ontology Augmentation; Directed Graphs; Graph Embeddings; Latent Semantic Analysis; Query Processing; Semantics; Undirected Graphs; Attention Mechanisms; Co-occurrence; Competency Question; Directed Acyclic Graph Model; Distant Supervision; Latent Semantics; Link Prediction; Ontology Augmentation; Ontology's; Ontology},
	keywords = {Directed graphs; Graph embeddings; Latent semantic analysis; Query processing; Semantics; Undirected graphs; Attention mechanisms; Co-occurrence; Competency question; Directed acyclic graph model; Distant supervision; Latent semantics; Link prediction; Ontology augmentation; Ontology's; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ghazouani202567,
	author = {Ghazouani, Fethi and Giustozzi, Franco and Le Ber, Florence},
	title = {LLM-Driven Case-Base Populating for Structuring and Integrating Restoration Experiences},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15662 LNAI},
	pages = {67 - 80},
	doi = {10.1007/978-3-031-96559-3_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010815398&doi=10.1007%2F978-3-031-96559-3_5&partnerID=40&md5=5edd9a3da5bfa66a6d0f852e64dcbc6c},
	abstract = {In Case-Based Reasoning (CBR), a case-base is a structured knowledge collection, often represented as an ontology with a TBox (conceptual structure) and ABox (concrete cases). This work emphasizes the need for a case-base to integrate knowledge from unstructured texts describing hydro-ecosystem restoration experiences, forming the foundation for a CBR framework to address new restoration challenges. However, populating the case-base from unstructured texts presents challenges due to fragmented, inconsistent, and implicit information. Therefore, we propose leveraging large language models (LLMs) to extract knowledge from unstructured texts, where the ontology population is approached as a generative knowledge extraction task. The results demonstrate that LLMs can effectively extract structured knowledge, facilitating the creation of a case-base for future projects. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Case-base; Case-based Reasoning; Llm; Ontology; Unstructured Texts; Case Based Reasoning; Computational Linguistics; Ecosystems; Knowledge Acquisition; Knowledge Management; Restoration; Case Base; Casebased Reasonings (cbr); Conceptual Structures; Hydro-ecosystems; Language Model; Large Language Model; Model-driven; Ontology's; Structured Knowledge; Unstructured Texts; Ontology},
	keywords = {Case based reasoning; Computational linguistics; Ecosystems; Knowledge acquisition; Knowledge management; Restoration; Case base; Casebased reasonings (CBR); Conceptual structures; Hydro-ecosystems; Language model; Large language model; Model-driven; Ontology's; Structured knowledge; Unstructured texts; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Bora202515,
	author = {Bora, Anubrat and Deepak, Gerard},
	title = {VRSIL: A Framework for Video Recommendation Integrating Semantic Intelligence with a Large Language Model},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2461 CCIS},
	pages = {15 - 24},
	doi = {10.1007/978-3-031-96473-2_2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010813395&doi=10.1007%2F978-3-031-96473-2_2&partnerID=40&md5=e89825e27bd426a69a8c41847958678c},
	abstract = {This framework presents a strategic model for video recommendation in the Web 3.0 era, integrating hybrid machine intelligence, generative AI, and semantic artificial intelligence through advanced semantic reasoning and quantitative semantic measures. The model dynamically generates ontologies from preprocessed query words, which are then used to select features based on linked similarity. These features are employed to classify the dataset from the perspective of the query using a logistic regression classifier. Semantics-oriented reasoning is achieved by computing the Normalized Pointwise Mutual Information (NPMI) to determine quantitative thresholds, and the Jian-Konrad Index is utilized as a criterion function for optimization. This optimization is carried out using the Elephant Optimization algorithm, which is executed only once to maintain the diversity and number of recommended entities. This approach ensures that the recommendations are both relevant and varied, aligning with the dynamic nature of Web 3.0. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology Generation; Semantic Intelligence; Semantic Similarity; Artificial Intelligence; Logistic Regression; Semantic Web; Semantics; Hybrid Machine; Language Model; Machine Intelligence; Ontology Generation; Optimisations; Semantic Intelligence; Semantic Reasoning; Semantic Similarity; Strategic Modeling; Web 3.0; Ontology},
	keywords = {Artificial intelligence; Logistic regression; Semantic Web; Semantics; Hybrid machine; Language model; Machine intelligence; Ontology generation; Optimisations; Semantic intelligence; Semantic reasoning; Semantic similarity; Strategic modeling; Web 3.0; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2025,
	title = {MDTT 2025 - Proceedings of the 4th International Conference on Multilingual Digital Terminology Today},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3990},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010771861&partnerID=40&md5=7544bfd62271bc0b262963be228b98fc},
	abstract = {The proceedings contain 27 papers. The topics discussed include: a multi-task text classification pipeline with natural language explanations for Greek tweets; an LLM-based approach for translating keywords in scientific publications; a multilingual chatbot for migrants: concept and implementation; compiling linguistic resources for specific purposes: using LLMs to collect data from social media; KnowledgeTB: leveraging large language models for enhanced terminology extraction over knowledge graphs; LUMEN: leveraging large language models LUMEN: leveraging large language models for dynamic ontologies in wind energy domain analysis; a methodology for the design of an ontology-based terminology resource on an institutionalized domain; a Mandarin-Cantonese parallel corpus with formality ranking; a computational explanatory combinatorial dictionary of religious terminology in early Baltic catechisms; and exploring terminology transfer: Greek-French perspectives in employment and economy. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Chatzitheodorou2025,
	author = {Chatzitheodorou, Konstantinos},
	title = {KnowledgeTB: Leveraging Large Language Models for Enhanced Terminology Extraction over Knowledge Graphs},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3990},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010768714&partnerID=40&md5=a2cfd8a54ee3b0f81e89a636339ac191},
	abstract = {The advent of Machine Learning and Large Language Models has revolutionized terminology management, introducing innovative approaches to designing and enriching terminological resources. This paper explores the synergy between Large Language Models and Knowledge Graphs, emphasizing their combined potential to enhance structural design, metadata representation, and data interoperability within specialized domains. Through case studies, we demonstrate how Large Language Models generate domain-specific terminology, ensure linguistic and conceptual precision, and propose metadata by analyzing contextual patterns. Concurrently, we highlight how Knowledge Graphs facilitate the integration of terminological resources into broader ontological frameworks, enabling dynamic updates, enhanced usability, and enriched contextual insights. Our findings propose new strategies for validating terminological resources in terms of ergonomics and usability, offering actionable guidance for experts in terminology, computational linguistics, and knowledge representation. This research contributes to advancing best practices for developing digital terminology systems in the era of Artificial Intelligence. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Digital Terminology Systems; Knowledge Graphs; Metadata Representation; Terminology Extraction; Terminology Management; Computational Linguistics; Domain Knowledge; Graphic Methods; Interoperability; Knowledge Graph; Knowledge Management; Learning Systems; Metadata; Ontology; Terminology; Data Interoperability; Digital Terminology System; Innovative Approaches; Knowledge Graphs; Language Model; Machine-learning; Metadata Representation; Terminology Extraction; Terminology Management; Terminology System; Extraction},
	keywords = {Computational linguistics; Domain Knowledge; Graphic methods; Interoperability; Knowledge graph; Knowledge management; Learning systems; Metadata; Ontology; Terminology; Data interoperability; Digital terminology system; Innovative approaches; Knowledge graphs; Language model; Machine-learning; Metadata representation; Terminology extraction; Terminology management; Terminology System; Extraction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lops2025,
	author = {Lops, Andrea and Sassi, Serena},
	title = {LUMEN: Leveraging Large Language Models for Dynamic Ontologies in Wind Energy Domain Analysis},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3990},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010754197&partnerID=40&md5=6b34e7aa673f27053a6d25ee8cd45275},
	abstract = {This study introduces LUMEN (Latent Understanding through Modeling Embeddings in Natural language), a structured approach leveraging Large Language Models (LLMs) and semantic embeddings to construct a hierarchical thesaurus for the wind energy domain. Using a domain-specific corpus generated via Sketch Engine, LUMEN applies a three-step methodology: corpus creation, semantic label identification through LLM-based analysis, and hierarchical term classification via embedding-based semantic similarity calculations. We outline our machine learning configuration, including the embedding techniques and similarity metrics employed. Results indicate that LUMEN can capture nuanced subdomains and semantic interrelations within wind energy, despite occasional misclassification issues. Future research directions include systematic benchmarking against established ontology-building tools and multilingual adaptation to broaden applicability. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain Analysis; Domain Tree; Large Language Model; Natural Language Processing; Terminology; Wind Power Research; Artificial Intelligence; Computational Linguistics; Embeddings; Information Systems; Knowledge Management; Latent Semantic Analysis; Modeling Languages; Natural Language Processing Systems; Ontology; Semantics; Thesauri; Wind Power; Domain Analysis; Domain Tree; Language Model; Language Processing; Large Language Model; Model Embedding; Natural Language Processing; Natural Languages; Power; Wind Power Research; Terminology},
	keywords = {Artificial intelligence; Computational linguistics; Embeddings; Information systems; Knowledge management; Latent semantic analysis; Modeling languages; Natural language processing systems; Ontology; Semantics; Thesauri; Wind power; Domain analysis; Domain tree; Language model; Language processing; Large language model; Model embedding; Natural language processing; Natural languages; Power; Wind power research; Terminology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Neeley2025,
	author = {Neeley, Matthew B. and Qi, Guantong and Wang, Guanchu and Tang, Ruixiang and Mao, Dongxue and Liu, Chaozhong and Pasupuleti, Sasidhar and Yuan, Bo and Xia, Fan and Liu, Pengfei},
	title = {Survey and improvement strategies for gene prioritization with large language models},
	year = {2025},
	journal = {Bioinformatics Advances},
	volume = {5},
	number = {1},
	pages = {},
	doi = {10.1093/bioadv/vbaf148},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010682598&doi=10.1093%2Fbioadv%2Fvbaf148&partnerID=40&md5=133bfecdafe4800b92cde8285ebfe8f3},
	abstract = {Motivation Rare diseases remain difficult to diagnose due to limited patient data and genetic diversity, with many cases remaining undiagnosed despite advances in variant prioritization tools. While large language models have shown promise in medical applications, their optimal application for trustworthy and accurate gene prioritization downstream of modern prioritization tools has not been systematically evaluated. Results We benchmarked various language models for gene prioritization using multi-agent and Human Phenotype Ontology classification approaches to categorize patient cases by phenotype-based solvability levels. To address language model limitations in ranking large gene sets, we implemented a divide-and-conquer strategy with mini-batching and token limiting for improved efficiency. GPT-4 outperformed other language models across all patient datasets, demonstrating superior accuracy in ranking causal genes. Multi-agent and Human Phenotype Ontology classification approaches effectively distinguished between confidently-solved and challenging cases. However, we observed bias toward well-studied genes and input order sensitivity as notable language model limitations. Our divide-and-conquer strategy enhanced accuracy, overcoming positional and gene frequency biases in literature. This framework optimized the overall process for identifying disease-causal genes compared to baseline evaluation, better enabling targeted diagnostic and therapeutic interventions and streamlining diagnosis of rare genetic disorders. Availability and implementation Software and additional material is available at: https://github.com/LiuzLab/GPT-Diagnosis. © 2025 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Menad2025654,
	author = {Menad, Safaa and Medeiros, Gabriel Henrique Alencar and Soualmia, Lina F.},
	title = {Enhancing the Description-Detection Framework with Semantic Clustering Using Biostransformers},
	year = {2025},
	journal = {Proceedings of the IEEE Symposium on Computer-Based Medical Systems},
	pages = {654 - 659},
	doi = {10.1109/CBMS65348.2025.00135},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010645286&doi=10.1109%2FCBMS65348.2025.00135&partnerID=40&md5=2a11e2829831cc19ff8dc016f05c6124},
	abstract = {Event-Based Surveillance Systems (EBS) are crucial for detecting emerging public health threats. However, these systems face significant challenges, including overreliance on manual expert intervention, limited handling of heterogeneous textual data, etc. The Description-Detection Framework (DDF) addresses some of these limitations by leveraging PropaPhen (Core Propagation Phenomenon Ontology), UMLS, and OpenStreetMaps to detect suspicious health-related cases using spatiotemporal and textual data. However, DDF is restricted to detection and lacks the ability to classify the detected observations into meaningful categories. To adress this limitation, we propose to enhance DDF by incorporating a clustering-based classification process. This enhancement employs BioSTransformers, a pretrained biomedical language model built on Sentence Transformers trained on PubMed data, to compute semantic similarity between observations. By capturing domain-specific semantic relationships, BioSTransformers enables clustering that integrates biological semantics with spatiotemporal context, outperforming traditional methods from the literature in observation classification. Our proposed approach reduces the dependency on manual expert effort, improves the system's ability to process heterogeneous data, and enhances the accuracy and contextual relevance of case classification. The results demonstrate the potential of this method to advance EBS systems, providing a scalable and automated solution to public health surveillance challenges. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Language Models; Core Propagation Phenomenon Ontology; Description Detection Framework; Public Health Surveillance; Spatiotemporal Reasoning; Classification (of Information); Computational Linguistics; Health Risks; Natural Language Processing Systems; Public Health; Security Systems; Semantics; Biomedical Language Model; Core Propagation Phenomenon Ontology; Description Detection Framework; Detection Framework; Event-based; Language Model; Ontology's; Public Health Surveillances; Spatio-temporal Reasoning; Surveillance Systems; Ontology},
	keywords = {Classification (of information); Computational linguistics; Health risks; Natural language processing systems; Public health; Security systems; Semantics; Biomedical language model; Core propagation phenomenon ontology; Description detection framework; Detection framework; Event-based; Language model; Ontology's; Public health surveillances; Spatio-temporal reasoning; Surveillance systems; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Teclaw2025,
	author = {Teclaw, Wojciech and Łuczkowski, Marcin and Labonnote, Nathalie and Hjelseth, Eilif},
	title = {Building information model and schema cross-validation using semantics – conceptual framework},
	year = {2025},
	journal = {International Journal of Architectural Computing},
	pages = {},
	doi = {10.1177/14780771251352954},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010599305&doi=10.1177%2F14780771251352954&partnerID=40&md5=e22edf984918c7575e8f14ed451fd433},
	abstract = {Ensuring consistency between Mechanical, Electrical, and Plumbing (MEP) schema drawings and Building Information Models (BIM) is essential for design accuracy and minimizing data discrepancies in construction projects. While BIM provides detailed 3D visualizations of building components, schematic drawings remain crucial for capturing the logical and functional relationships within early-stage designs. However, discrepancies between these two representations often arise, necessitating extensive manual verification. This study introduces a conceptual framework for automated cross-validation between MEP schema drawings and BIM models by leveraging semantic representations. The framework utilizes AI-driven technologies, particularly Large Language Models (LLMs), to extract structured knowledge from both schematics and BIM data, translating this information into machine-readable formats based on the Brick ontology. By integrating semantic web technologies and multimodal processing, the proposed framework effectively identifies inconsistencies in airflow distribution, system connectivity, and performance parameters. This approach significantly enhances the efficiency and accuracy of design validation, minimizes data discrepancies, and fosters interoperability among heterogeneous data sources. Initial findings demonstrate the scalability and effectiveness of semantic-based validation, suggesting substantial benefits for MEP-BIM integration. Future research will extend the framework to additional MEP domains, including electrical and plumbing systems, and further refine AI-based recognition methods. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bim; Llm; Mep; Semantics; Validation; Architectural Design; Building Information Model; Data Accuracy; Information Theory; Interoperability; Ontology; Semantic Web; Three Dimensional Computer Graphics; Building Information Modelling; Conceptual Frameworks; Construction Projects; Cross Validation; Information Schema; Language Model; Large Language Model; Mechanical; Mechanical, Electrical, And Plumbing; Validation; Semantics},
	keywords = {Architectural design; Building Information Model; Data accuracy; Information theory; Interoperability; Ontology; Semantic Web; Three dimensional computer graphics; Building Information Modelling; Conceptual frameworks; Construction projects; Cross validation; Information schema; Language model; Large language model; Mechanical; Mechanical, electrical, and plumbing; Validation; Semantics},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Shi2025374,
	author = {Shi, Ruhui and Chen, Tianran and Lu, Chunyu and Shang, Duo and Luo, Jun and Hui, Xin and Li, Haoran and He, Huihong},
	title = {Dynamic Assessment and Early Warning of Cross-Border Pipeline Risks Based on Knowledge Graph},
	year = {2025},
	pages = {374 - 377},
	doi = {10.1109/AEMCSE65292.2025.11042435},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010558936&doi=10.1109%2FAEMCSE65292.2025.11042435&partnerID=40&md5=fe90113108f54be993c45e3f1190d29c},
	abstract = {This paper introduces a novel framework for the dynamic assessment and early warning of cross-border pipeline risks, leveraging the zero-shot learning capabilities of the GPT-4 large language model (LLM) to construct and utilize a domain-specific knowledge graph. The approach circumvents the need for extensive pre-training or fine-tuning, thereby enabling rapid development and deployment in a domain characterized by data scarcity. The methodology begins with the construction of a knowledge graph, which involves a comprehensive ontology design, followed by knowledge extraction from unstructured text sources, and culminates in knowledge representation and storage in a structured format. Subsequently, the framework facilitates dynamic risk understanding and analysis, encompassing risk factor identification, risk event inference, and the analysis of complex relationships between these elements, all supported by the structured knowledge base. Finally, the system provides knowledge-based risk interpretation and decision-making support, generating explanations for risk events, assessing their potential impacts, and offering actionable recommendations to stakeholders. By employing carefully crafted prompts and the inherent natural language processing capabilities of GPT-4, the framework facilitates the extraction of entities and relationships from unstructured textual data, effectively populating the knowledge graph. This knowledge graph, representing entities, attributes, and relationships pertinent to cross-border pipeline risks, serves as a foundation for dynamic risk assessment. This method presents a scalable, data-efficient approach tailored to the specialized domain of cross-border pipeline risk management, particularly advantageous in contexts with limited resources. The proposed framework demonstrates the potential of LLMs, specifically GPT-4, in conjunction with knowledge graph methodologies, to advance risk management practices in complex, data-sparse domains. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {China-russia Pipeline; Knowledge Graph; Llm; Data Mining; Decision Making; Digital Storage; Domain Knowledge; Extraction; Graph Theory; Graphic Methods; Knowledge Graph; Knowledge Management; Natural Language Processing Systems; Ontology; Pipelines; Risk Analysis; Risk Management; China-russia Pipeline; Cross-border; Domain-specific Knowledge; Dynamic Assessment; Early Warning; Knowledge Graphs; Language Model; Large Language Model; Learning Capabilities; Risk-based; Risk Assessment},
	keywords = {Data mining; Decision making; Digital storage; Domain Knowledge; Extraction; Graph theory; Graphic methods; Knowledge graph; Knowledge management; Natural language processing systems; Ontology; Pipelines; Risk analysis; Risk management; China-russia pipeline; Cross-border; Domain-specific knowledge; Dynamic assessment; Early warning; Knowledge graphs; Language model; Large language model; Learning capabilities; Risk-based; Risk assessment},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Jenny Kalaiarasi2025,
	author = {Jenny Kalaiarasi, S. and Nimala, K. K.},
	title = {Advancements in Context-Aware Recommender Systems: An Exhaustive Exploration of Ontology and LLM in improving ranking accuracy},
	year = {2025},
	pages = {},
	doi = {10.1109/RMKMATE64874.2025.11042856},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010522965&doi=10.1109%2FRMKMATE64874.2025.11042856&partnerID=40&md5=3f906f4128e0b16c696a6b4f12321eb3},
	abstract = {Recommendation systems have become a ubiquitous presence across various domains such as e-commerce, entertainment (movies and music), tourism, news, advertising, stock markets and social networks. The integration of Ontology and Large Language Models (LLMs) presents a transformative approach for enhancing recommender systems by improving contextual intelligence, personalization, and explainability. Traditional recommendation algorithms often suffer from data sparsity, cold-start problems, and limited semantic understanding, which hinder their ability to deliver highly relevant and interpretable suggestions. This research provides an exhaustive exploration of how ontology-driven knowledge representation can be seamlessly combined with LLMs to refine user-item interactions, enrich embeddings, and improve reasoning in recommendation pipelines. We propose a hybrid framework where ontological structures encode domain-specific relationships while LLMs process, infer, and enhance recommendations through contextual embeddings.This synergy enables semantic-aware recommendations, explainable item suggestions, and adaptive learning mechanisms that adjust based on user behavior and external context. Furthermore, we investigate novel ontology injection techniques, including knowledge graph-based reinforcement, to improve long-tail item exposure and enhance recommendation diversity. Through rigorous evaluation on real-world datasets, we demonstrated how ontology-LLM fusion significantly outperforms baseline models in accuracy, novelty, and interpretability. This study establishes a foundation for the next generation of context-aware, knowledge-enhanced recommender systems that leverage LLMs for reasoning and ontologies for structured knowledge representation, paving the way for more intelligent and user-centric recommendations. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Intelligence; Context-aware Recommendation Systems; Llm; Ontology; Semantic-aware; Behavioral Research; Electronic Commerce; Electronic Trading; Embeddings; Graphic Methods; Knowledge Management; Knowledge Representation; Recommender Systems; Semantics; User Interfaces; Context-aware Recommendation Systems; Context-aware Recommender Systems; E- Commerces; In Contexts; Knowledge-representation; Language Model; Large Language Model; Ontology's; Semantic-aware; Ontology},
	keywords = {Behavioral research; Electronic commerce; Electronic trading; Embeddings; Graphic methods; Knowledge management; Knowledge representation; Recommender systems; Semantics; User interfaces; Context-aware recommendation systems; Context-aware recommender systems; E- commerces; In contexts; Knowledge-representation; Language model; Large language model; Ontology's; Semantic-aware; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2025,
	title = {15th International Conference on Simulation and Modeling Methodologies, Technologies and Applications, SIMULTECH 2025},
	year = {2025},
	journal = {Proceedings of the International Conference on Simulation and Modeling Methodologies, Technologies and Applications},
	volume = {1},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010457885&partnerID=40&md5=3d81aad4ba6bd70be80b807377ce0da1},
	abstract = {The proceedings contain 42 papers. The special focus in this conference is on Simulation and Modeling Methodologies, Technologies and Applications. The topics include: Research on Manual Carrier Landing Task in High Sea Conditions; Embodied AI in Mobile Robot Simulation with Eye Sim: Coverage Path Planning with Large Language Models; a New Numerical Method for Fast Prediction of Wheel Tread Wear for Stacker Cranes; digital Twin Concept for a Novel Aerosol-on-Demand Jet-Printing System; A Comparative Experimental Evaluation of iPI and iPI-Fuzzy Controllers for a Thermal Process with a Long Dead Time; An Extensible MARL Framework for Multi-UAV Collaborative Path Planning; Aerial Logistics in Hard-to-Reach Environments: Systematic Review of the Use of Class 1 UAVs in Health Supply Distribution in Military Operations and Other Context; machine Learning Applied to Optimize Fuel Consumption in Amazonian Waterways Military Logistics; maximizing Tactical Success: The Impact of the Mechanized Anti-Tank Company in a Coordinated Attack Assessed Through Constructive Simulation; TRIMARAN: A Toolbox for Radiometric Imaging with Microwave ARrays of ANtennas; An OWL Implementation of OntoUML and BPMN Models to Unify Representation of Structure and Behavior of Complex Domains: Application to Routing Protocols; SCART: Simulation of Cyber Attacks for Real-Time; advanced Predictive Process Control for Industrial Thickeners; Modular Simulator for DAE-Based Systems Using DEVS Formalism; machine Learning-Driven Framework for Identifying Parameter-Driven Anomalies in Multiphysics Simulations; Ontological Framework for Integrating Predictive Analytics, AI, and Big Data in Decision-Making Systems Using Knowledge Graph; indexed Concatenation Notation: A Novel Way to Summarize Networks and Other Complex Systems. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Cases2025127,
	author = {Cases, Ildefonso and Martínez-Redondo, Gemma I. and Fernández, Rosa and Rojas, Ana María},
	title = {Functional Annotation of Proteomes Using Protein Language Models: A High-Throughput Implementation of the ProtTrans Model},
	year = {2025},
	journal = {Methods in Molecular Biology},
	volume = {2941},
	pages = {127 - 137},
	doi = {10.1007/978-1-0716-4623-6_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010291591&doi=10.1007%2F978-1-0716-4623-6_8&partnerID=40&md5=986c553492dd26a0df64425cf81b8055},
	abstract = {Protein function prediction is critical for a wide range of applications in biology, spanning from functional genomics to protein design and genome evolution, among others. However, accurately predicting protein function remains a longstanding challenge in computational biology, especially for non-model organisms. Traditional methods based on sequence similarity often fail to annotate a significant proportion of proteins. The emergence of protein language models has significantly improved this process, enabling more accurate and comprehensive functional annotation. In this work, we highlight how the ProtTrans language model outperforms other tools in per-protein annotation, offering a more precise approach to predicting protein function. We also introduce functional annotation based on embedding space similarity (FANTASIA; available at https://github.com/MetazoaPhylogenomicsLab/FANTASIA), a tool developed to harness these advances for large-scale annotation of uncharacterized proteomes. We provide a detailed overview of how to use FANTASIA, interpret its outputs, and demonstrate its utility in three case studies: (a) enrichment analyses from transcriptomics data, (b) assigning novel functions to unannotated genes in model organisms, and (c) identifying genes involved in important functions in non-model organisms. These results demonstrate the potential of protein language models to advance functional annotation in diverse biological contexts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Biology; Fantasia; Protein Function Prediction; Protein Language Models; Proteome Annotation; Protein; Proteins; Proteome; Isoprotein; Proteome; Protein; Adult; Annelid; Bioinformatics; Caenorhabditis Elegans; Comparative Genomics; Comparative Study; Controlled Study; Differential Gene Expression; Drosophila Melanogaster; Functional Genomics; Gene Ontology; High Throughput Analysis; K Nearest Neighbor; Language Model; Machine Learning; Nonhuman; Prediction; Protein Function; Protein Language Model; Prottrans Model; Rna Sequencing; Transcriptomics; Animal; Genetics; Human; Metabolism; Molecular Genetics; Procedures; Protein Database; Proteomics; Software; Animals; Computational Biology; Databases, Protein; Humans; Molecular Sequence Annotation; Proteins; Proteome; Proteomics; Software},
	keywords = {isoprotein; proteome; protein; adult; annelid; bioinformatics; Caenorhabditis elegans; comparative genomics; comparative study; controlled study; differential gene expression; Drosophila melanogaster; functional genomics; gene ontology; high throughput analysis; k nearest neighbor; language model; machine learning; nonhuman; prediction; protein function; protein language model; prottrans model; RNA sequencing; transcriptomics; animal; genetics; human; metabolism; molecular genetics; procedures; protein database; proteomics; software; Animals; Computational Biology; Databases, Protein; Humans; Molecular Sequence Annotation; Proteins; Proteome; Proteomics; Software},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Boadu2025101,
	author = {Boadu, Frimpong and Lee, Ahhyun and Cheng, Jianlin},
	title = {TransFun: A Tool of Integrating Large Language Models, Transformers, and Equivariant Graph Neural Networks to Predict Protein Function},
	year = {2025},
	journal = {Methods in Molecular Biology},
	volume = {2941},
	pages = {101 - 111},
	doi = {10.1007/978-1-0716-4623-6_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010289532&doi=10.1007%2F978-1-0716-4623-6_6&partnerID=40&md5=5d2d8888c530343c1451878488397bb3},
	abstract = {Experimentally determining the functions of proteins is a complex and time-consuming process. This challenge contributes to a gap, where many proteins have known sequences, predicted structures, and other crucial information, yet lack functional annotations. This gap underscores the critical importance of automated function prediction (AFP) methods, which aim to develop computational techniques dedicated to predicting protein functions. Most AFP methods leverage the wealth of diverse protein information available, such as sequences, structures, protein-protein interactions, and domain characteristics. These methods often utilize individual features or integrate multiple features to enhance the accuracy of function prediction. In this chapter, we focus on TransFun, a structure-based protein function prediction technique. TransFun leverages the embeddings provided by the ESM-1b pretrained protein language models to distill intricate sequence features and combines them with AlphaFold’s predicted structures to predict protein functions. Availability:https://github.com/jianlin-cheng/TransFun © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Equivariant Graph Neural Networks; Gene Ontology; Go Terms; Graph Neural Networks; Protein Function Prediction; Transformer; Protein; Proteins; Chapter; Commercial Phenomena; Controlled Study; Deep Learning; Diagnostic Test Accuracy Study; Gene Ontology; Graph Neural Network; Large Language Model; Nonhuman; Prediction; Protein Function; Protein Language Model; Protein Protein Interaction; Algorithm; Artificial Neural Network; Bioinformatics; Chemistry; Human; Metabolism; Procedures; Protein Conformation; Protein Database; Software; Protein; Algorithms; Computational Biology; Databases, Protein; Graph Neural Networks; Humans; Large Language Models; Neural Networks, Computer; Protein Conformation; Proteins; Software},
	keywords = {chapter; commercial phenomena; controlled study; deep learning; diagnostic test accuracy study; gene ontology; graph neural network; large language model; nonhuman; prediction; protein function; protein language model; protein protein interaction; algorithm; artificial neural network; bioinformatics; chemistry; human; metabolism; procedures; protein conformation; protein database; software; protein; Algorithms; Computational Biology; Databases, Protein; Graph Neural Networks; Humans; Large Language Models; Neural Networks, Computer; Protein Conformation; Proteins; Software},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Varghese2025201,
	author = {Varghese, Dana Mary and Athulya, T. and Mohani, Vikash K. and Ahmad, Shandar},
	title = {A Survey of Biological Function Prediction Methods with Focus on Natural Language Processing (NLP) and Large Language Models (LLM)},
	year = {2025},
	journal = {Methods in Molecular Biology},
	volume = {2941},
	pages = {201 - 225},
	doi = {10.1007/978-1-0716-4623-6_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010288045&doi=10.1007%2F978-1-0716-4623-6_13&partnerID=40&md5=d393cf5f11b06a9d0f5314dfaee774bf},
	abstract = {Protein function prediction from sequence, structure, gene expression profiles, and published literature are needed to understand all biological processes. Natural language processing of biological text and large language model (LLM)-based encoding of sequence and structure opens powerful paths to rapid function annotation and novel training models. In this survey, we take a look at the available models for function prediction, especially the NLP-and LLM-based models. The survey highlights the major advances made and the ground that still needs to be covered to automate the process of function prediction from two major sources namely protein sequences and published research documents. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Function Prediction; Gene Ontologies; Large Language Models; Natural Language Processing; Protein Function; Protein; Proteins; Protein; Amino Acid Sequence; Gene Expression; Large Language Model; Molecular Evolution; Natural Language Processing; Protein Protein Interaction; Protein Structure; Bioinformatics; Chemistry; Genetics; Human; Metabolism; Procedures; Computational Biology; Humans; Large Language Models; Natural Language Processing; Proteins},
	keywords = {protein; amino acid sequence; gene expression; large language model; molecular evolution; natural language processing; protein protein interaction; protein structure; bioinformatics; chemistry; genetics; human; metabolism; procedures; Computational Biology; Humans; Large Language Models; Natural Language Processing; Proteins},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang2025113,
	author = {Zhang, Chengxin and Liu, Quancheng and Freddolino, Peter Lydia},
	title = {Using InterLabelGO+ for Accurate Protein Language Model-Based Function Prediction},
	year = {2025},
	journal = {Methods in Molecular Biology},
	volume = {2941},
	pages = {113 - 125},
	doi = {10.1007/978-1-0716-4623-6_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010287964&doi=10.1007%2F978-1-0716-4623-6_7&partnerID=40&md5=00702489c3f873e43c2dd18781831671},
	abstract = {The recent availability of large language models for protein sequences has spurred the development of deep learning models for prediction of protein functions, mainly in the form of gene ontology (GO) terms. We developed InterLabelGO+, a top performing deep learning–based protein GO term prediction model in the recent fifth Critical Assessment of Function Annotation (CAFA5) challenge. InterLabelGO+ uses the ESM2 protein language model to extract sequence features, which are then used as inputs to a deep learning model that was trained under a loss function that considers the potentially complex relations between different GO terms. The deep learning–predicted GO terms are then combined with GO terms from sequence homology search to derive consensus predictions. InterLabelGO+ is available at https://seq2fun.dcmb.med.umich.edu/InterLabelGO/. In this chapter, we demonstrate procedures to perform protein GO term prediction with the InterLabelGO+ webserver and the standalone package, as well as how to retrain the model with up-to-date training data. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Gene Ontology; Protein Function Prediction; Protein Language Model; Sequence Homology; Protein; Proteins; Amino Acid Sequence; Deep Learning; Gene Ontology; Large Language Model; Prediction; Predictive Model; Protein Language Model; Sequence Homology; Bioinformatics; Chemistry; Genetics; Human; Metabolism; Molecular Genetics; Procedures; Protein Database; Software; Protein; Computational Biology; Databases, Protein; Deep Learning; Gene Ontology; Humans; Molecular Sequence Annotation; Proteins; Software},
	keywords = {amino acid sequence; deep learning; gene ontology; large language model; prediction; predictive model; protein language model; sequence homology; bioinformatics; chemistry; genetics; human; metabolism; molecular genetics; procedures; protein database; software; protein; Computational Biology; Databases, Protein; Deep Learning; Gene Ontology; Humans; Molecular Sequence Annotation; Proteins; Software},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Giri202585,
	author = {Giri, Swagarika Jaharlal and Pandey, Udayan and Park, Joon-hong and Kihara, Daisuke},
	title = {Translating a GO Term List to Human Readable Function Description Using GO2Sum},
	year = {2025},
	journal = {Methods in Molecular Biology},
	volume = {2941},
	pages = {85 - 99},
	doi = {10.1007/978-1-0716-4623-6_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010287883&doi=10.1007%2F978-1-0716-4623-6_5&partnerID=40&md5=a1077d8ee3db5d29036f6628b0d3db36},
	abstract = {Understanding the functions of proteins is one of the most important challenges in modern biology. Typically, protein function prediction methods generate a list of gene ontology (GO) terms, sometimes consisting of 50–100 functional terms. While GO serves the purpose of standardizing terms, interpreting a long list of GO terms is difficult for biologists. To address this challenge, we developed Gene Ontology terms Summarizer (GO2Sum), a language model–based summarizer that takes a list of GO terms as input and converts them into a concise, free-text summary describing the protein’s function, subunit structure, and pathway information. GO2Sum was fine-tuned on GO term assignments and free-text function descriptions from UniProt entries. We built a Web server of GO2Sum, which offers an easy use of GO2Sum for biology users. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Function Prediction; Gene Ontology; Go2sum; Large Language Model; Llm; Protein Function; Summarization; Protein; Proteins; Protein S; Ras Protein; Protein; Chromosome Segregation; Commercial Phenomena; Gene Ontology; Human; Large Language Model; Protein Function; Protein Quaternary Structure; Protein Structure; Translating (language); Bioinformatics; Chemistry; Genetics; Metabolism; Procedures; Protein Database; Software; Computational Biology; Databases, Protein; Gene Ontology; Humans; Proteins; Software},
	keywords = {protein S; Ras protein; protein; chromosome segregation; commercial phenomena; gene ontology; human; large language model; protein function; protein quaternary structure; protein structure; translating (language); bioinformatics; chemistry; genetics; metabolism; procedures; protein database; software; Computational Biology; Databases, Protein; Gene Ontology; Humans; Proteins; Software},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {23rd International Conference on Artificial Intelligence and Soft Computing, ICAISC 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15164 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010264633&partnerID=40&md5=e16cb56c26e168bdca411ba83394c7b8},
	abstract = {The proceedings contain 96 papers. The special focus in this conference is on Artificial Intelligence and Soft Computing. The topics include: Functional Based Fuzzy Logic Query Library; Automatic Program Assessment, Grading and Code Generation: Possible AI-Support in a Software Development Course; Computer Network User Profiling Using Firewall Logs and ANFIS-Based Hybrid Systems; different Approaches for Customer Lifetime Value in e-Book Subscription Domain; tweeting on an Empty Stomach: Unpacking Food Price Hikes and Inflation via Twitter; memory Augmented Multi-agent Reinforcement Learning for Cooperative Environment; the Use of Fuzzy Sets in the Classification of Metagenomic Data; speech Deepfake Recognition with Data-Efficient Vision Transformers; explainable Cubic Attention-Based Autoencoder for Skin Cancer Classification; a Study of Classifiers for Marking Short Answers in Intelligent Tutoring Systems; ontology-Based Rare Disease Database Integration-Practical Issues and Challenges; a Review of the Challenges with Massive Web-Mined Corpora Used in Large Language Models Pre-training; a Hybrid Approach Towards Vertical Handover in 5G Networks Using Genetic Neuro-Fuzzy Controller; swarm Optimization for Enhanced Random Forest-Based IoT Security; PV Solar Power Forecasting Using a Subset of Characteristic Features Selected by AI Methods; pain Intensity Assessment Using Recurrent Neural Networks; Improving YOLOv8: An Algorithm Based on EfficientRepHead and BIFPN; T-REX: Trustworthy and Reliable EXplainability Framework; multi-label Long-Tailed Disease Recognition on Chest X-ray Images; Myo-to-Gesture: Evolving AI-Based Innovations in Myoelectric Signal Processing – State-of-the-Art Survey and Research Proposal; a New Image Enhancement-Based Framework for Spoofing Detection in Ear-Based Biometric Authentication Systems; from Omics Data to Candidate Genes: An Innovative Machine Learning Approach for Biomarker Identification. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {23rd International Conference on Artificial Intelligence and Soft Computing, ICAISC 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15166 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010258244&partnerID=40&md5=167e12b307e53302665a07bcc1f513e3},
	abstract = {The proceedings contain 96 papers. The special focus in this conference is on Artificial Intelligence and Soft Computing. The topics include: Functional Based Fuzzy Logic Query Library; Automatic Program Assessment, Grading and Code Generation: Possible AI-Support in a Software Development Course; Computer Network User Profiling Using Firewall Logs and ANFIS-Based Hybrid Systems; different Approaches for Customer Lifetime Value in e-Book Subscription Domain; tweeting on an Empty Stomach: Unpacking Food Price Hikes and Inflation via Twitter; memory Augmented Multi-agent Reinforcement Learning for Cooperative Environment; the Use of Fuzzy Sets in the Classification of Metagenomic Data; speech Deepfake Recognition with Data-Efficient Vision Transformers; explainable Cubic Attention-Based Autoencoder for Skin Cancer Classification; a Study of Classifiers for Marking Short Answers in Intelligent Tutoring Systems; ontology-Based Rare Disease Database Integration-Practical Issues and Challenges; a Review of the Challenges with Massive Web-Mined Corpora Used in Large Language Models Pre-training; a Hybrid Approach Towards Vertical Handover in 5G Networks Using Genetic Neuro-Fuzzy Controller; swarm Optimization for Enhanced Random Forest-Based IoT Security; PV Solar Power Forecasting Using a Subset of Characteristic Features Selected by AI Methods; pain Intensity Assessment Using Recurrent Neural Networks; Improving YOLOv8: An Algorithm Based on EfficientRepHead and BIFPN; T-REX: Trustworthy and Reliable EXplainability Framework; multi-label Long-Tailed Disease Recognition on Chest X-ray Images; Myo-to-Gesture: Evolving AI-Based Innovations in Myoelectric Signal Processing – State-of-the-Art Survey and Research Proposal; a New Image Enhancement-Based Framework for Spoofing Detection in Ear-Based Biometric Authentication Systems; from Omics Data to Candidate Genes: An Innovative Machine Learning Approach for Biomarker Identification. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {23rd International Conference on Artificial Intelligence and Soft Computing, ICAISC 2024},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15165 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010242943&partnerID=40&md5=e61f76510f99b9f5825e08754ca803a5},
	abstract = {The proceedings contain 96 papers. The special focus in this conference is on Artificial Intelligence and Soft Computing. The topics include: Functional Based Fuzzy Logic Query Library; Automatic Program Assessment, Grading and Code Generation: Possible AI-Support in a Software Development Course; Computer Network User Profiling Using Firewall Logs and ANFIS-Based Hybrid Systems; different Approaches for Customer Lifetime Value in e-Book Subscription Domain; tweeting on an Empty Stomach: Unpacking Food Price Hikes and Inflation via Twitter; memory Augmented Multi-agent Reinforcement Learning for Cooperative Environment; the Use of Fuzzy Sets in the Classification of Metagenomic Data; speech Deepfake Recognition with Data-Efficient Vision Transformers; explainable Cubic Attention-Based Autoencoder for Skin Cancer Classification; a Study of Classifiers for Marking Short Answers in Intelligent Tutoring Systems; ontology-Based Rare Disease Database Integration-Practical Issues and Challenges; a Review of the Challenges with Massive Web-Mined Corpora Used in Large Language Models Pre-training; a Hybrid Approach Towards Vertical Handover in 5G Networks Using Genetic Neuro-Fuzzy Controller; swarm Optimization for Enhanced Random Forest-Based IoT Security; PV Solar Power Forecasting Using a Subset of Characteristic Features Selected by AI Methods; pain Intensity Assessment Using Recurrent Neural Networks; Improving YOLOv8: An Algorithm Based on EfficientRepHead and BIFPN; T-REX: Trustworthy and Reliable EXplainability Framework; multi-label Long-Tailed Disease Recognition on Chest X-ray Images; Myo-to-Gesture: Evolving AI-Based Innovations in Myoelectric Signal Processing – State-of-the-Art Survey and Research Proposal; a New Image Enhancement-Based Framework for Spoofing Detection in Ear-Based Biometric Authentication Systems; from Omics Data to Candidate Genes: An Innovative Machine Learning Approach for Biomarker Identification. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Bouas202595,
	author = {Bouas, Christos and Papoutsoglou, Maria C. and Tassios, Alexandros and Tegos, Stergios D. and Manousaridis, Konstantinos and Mavropoulos, Thanassis and Vrochidis, Stefanos and Meditskos, Georgios},
	title = {Knowledge Extraction and Ontology Modeling in the SALLY Chatbot},
	year = {2025},
	journal = {IFIP Advances in Information and Communication Technology},
	volume = {754 IFIPAICT},
	pages = {95 - 103},
	doi = {10.1007/978-3-031-97313-0_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010208274&doi=10.1007%2F978-3-031-97313-0_8&partnerID=40&md5=f8721ca170bf3fe1df83fa69419470c1},
	abstract = {Reception and social integration of Third Country Nationals (TCNs) remains a pressing challenge in contemporary societies shaped by migration. The SALLY project supports the integration of migrants in Greece through a multilingual conversational agent designed to assist users in accessing information related to legal, healthcare, employment, and social services. This paper presents preliminary work on a knowledge extraction framework that captures key elements from natural language conversations—such as topics, entities, and relationships—using Large Language Models (LLMs). These are structured as RDF Knowledge Graphs guided by an ontology that reflects the dynamics of user-agent interaction. By translating informal dialogue into semantically meaningful representations, the system offers a foundation for better understanding user needs and behavior, while enhancing the transparency and responsiveness of the SALLY conversational agent. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatbot; Knowledge Extraction; Knowledge Graphs; Large Language Models; Migrant Integration; Behavioral Research; Chatbots; Human Computer Interaction; Human Engineering; Information Systems; Information Use; Integration; Knowledge Management; Modeling Languages; Multi Agent Systems; Natural Language Processing Systems; Ontology; User Interfaces; Conversational Agents; Extraction Modeling; Knowledge Extraction; Knowledge Graphs; Knowledge Ontology; Language Model; Large Language Model; Migrant Integration; Ontology Model; Extraction},
	keywords = {Behavioral research; Chatbots; Human computer interaction; Human engineering; Information systems; Information use; Integration; Knowledge management; Modeling languages; Multi agent systems; Natural language processing systems; Ontology; User interfaces; Conversational agents; Extraction modeling; Knowledge extraction; Knowledge graphs; Knowledge ontology; Language model; Large language model; Migrant integration; Ontology model; Extraction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Chowdhury2025170,
	author = {Chowdhury, Rafi Rashid and Goto, Takaaki and Tsuchida, Kensei and Kirishima, Tadaaki and Bandi, Ajay},
	title = {An Automated Framework of Ontology Generation for Abstract Concepts Using LLMs},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2435 CCIS},
	pages = {170 - 180},
	doi = {10.1007/978-3-031-92178-0_14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010069636&doi=10.1007%2F978-3-031-92178-0_14&partnerID=40&md5=798c765c413f4b8dbf5309e30c267d01},
	abstract = {This study introduces an automated framework for constructing ontologies of abstract concepts by integrating Large Language Models (LLMs) with semantic web technologies. The proposed system leverages advanced models, including ChatGPT-4, GPT-4, and Gemini-2.0 flash-exp, to extract entities and relationships from textual data, transforming them into structured ontologies represented in the Web Ontology Language (OWL). By adhering to semantic web standards, the framework ensures the creation of reusable, scalable, and interoperable ontologies that enable advanced applications. This methodology bridges the gap between unstructured data and structured cultural knowledge, enhancing the digital representation and understanding of cultural concepts. As a case study, the framework is applied to extract a cultural ontology from a Wikipedia page, demonstrating its effectiveness in converting unstructured textual data into structured knowledge. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Model; Natural Language Processing; Ontology; Owl; Rdf; Computational Linguistics; Computer Software Reusability; Data Mining; Knowledge Management; Natural Language Processing Systems; Resource Description Framework (rdf); Websites; Abstract Concept; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Ontology's; Owl; Rdf; Textual Data; Ontology},
	keywords = {Computational linguistics; Computer software reusability; Data mining; Knowledge management; Natural language processing systems; Resource Description Framework (RDF); Websites; Abstract concept; Language model; Language processing; Large language model; Natural language processing; Natural languages; Ontology's; OWL; RDF; Textual data; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {40th International Conference on Computers and Their Applications, CATA 2025},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2435 CCIS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010065783&partnerID=40&md5=e789da8ab6490f2841280eac6d4fe69a},
	abstract = {The proceedings contain 19 papers. The special focus in this conference is on Computers and Their Applications. The topics include: Measurement and Characterization of Problems with Public API Support: A Case Study on YouTube APIs; Conversational AI in Healthcare: A Framework for Privacy, Security, Ethics, Transparency and Harm Prevention; chain Table: Protecting Table-Level Data Integrity by Digital Ledger Technology; sentiment, Volume, and Topics in University Tweets: Methodology, Insights, and Challenges; Automated Test Case Generation for Software Testing Using Generative AI; ensemble Machine Learning Approach to Phishing Website Detection; human Activity Recognition Using an Ensemble Learning Approach; comparison of Some Pseudorandom Binary Generators Based on Combinatorial Functions; design Process of a New Pseudorandom Binary Generator Model; machine Learning for Real World Water Consumption Forecasting; Comparative Analysis of Machine Learning Classifiers for Yellow Fever Diagnosis Using Causative Data: Evaluating Naïve Bayes, KNN, RIPPER, and PART; An Automated Framework of Ontology Generation for Abstract Concepts Using LLMs; using Machine Learning Techniques to Detect Network Intrusions; a Novel Feature Selection Method for Classification Against Email Phishing; reputation Proof with Load Services in Ad-Hoc Peer-to-Peer Networks; Scalable Automated Vulnerability Inspection Framework Using Nmap for CVE Detection in Distributed Remote Networks; Clustering of Processing-Induced Martensitic Phases Using AC-GAN and Magnetic Susceptibility Evaluation in High-Gradient Fields. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yao2025,
	author = {Yao, Liwei and Ren, Fu and Du, Kaixuan and Du, Qingyun},
	title = {From knowledge graph construction to retrieval-augmented generation: a framework for comprehensive earthquake emergency support},
	year = {2025},
	journal = {Geo-Spatial Information Science},
	pages = {},
	doi = {10.1080/10095020.2025.2514813},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009902479&doi=10.1080%2F10095020.2025.2514813&partnerID=40&md5=0c9769723dc40c230bc7a317e19a3a0e},
	abstract = {Effective decision-making during earthquake emergencies requires rapid access to accurate, structured, and context-specific knowledge. However, existing knowledge resources in this domain are fragmented, heterogeneous, and largely unstructured, causing decision-makers to rely heavily on intuition or scattered textual materials, which often results in delayed, inconsistent, or suboptimal emergency responses. To address these challenges, this study proposes a structured framework integrating large language models (LLMs) with knowledge representation techniques to systematically construct domain-specific knowledge graphs (KGs) tailored explicitly for earthquake emergency scenarios. The framework comprises three primary stages: (1) developing an ontology that encompasses the complete earthquake emergency management cycle–prevention, preparedness, response, and recovery–as well as earthquake-specific measures, models, terminology, and attributes; (2) guiding LLMs with structured prompts to extract entities, relationships, and attributes from unstructured data; and (3) employing a knowledge fusion strategy to resolve ambiguities and consolidate information across the graph. From a corpus of 2682 professional documents, including emergency plans, technical standards, and specialized books, the framework extracted 284,801 entities and over 80,000 unique relationship types, subsequently consolidated into approximately 1000 meaningful categories. The final KG, refined through entity fusion and clustering, comprises over 268,000 nodes and 833,000 relationships. To effectively utilize the constructed KG, we developed an Improved Hybrid Retrieval-Augmented Generation (HybridRAG) application framework, integrating symbolic retrieval from the KG with semantic similarity-based retrieval from a vector database. This dual retrieval approach enables LLMs to generate responses that are both semantically coherent and deeply grounded in operational knowledge. Comparative experiments conducted on a newly constructed dataset of 150 earthquake-specific questions demonstrated that the Improved HybridRAG method significantly outperforms standard methods–including LLM-only, semantic vector-based retrieval, and purely symbolic retrieval–in accuracy, clarity, comprehensiveness, conciseness, and relevance. These findings validate the advantage of combining structured and semantic knowledge retrieval, illustrating the framework’s capability to provide reliable, contextually aligned, and actionable insights for decision-makers in earthquake emergency scenarios. Future work will focus on improving attribute completeness, refining entity alignment techniques to capture complex semantic nuances, and exploring multimodal data integration to further extend the KG’s utility. This study underscores the potential of systematically combining structured KGs and LLMs to significantly enhance decision-making capabilities in disaster management contexts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Earthquake Disaster Management; Emergency; Knowledge Enhancement; Knowledge Fusion; Knowledge Graph; Large Language Models (llm)},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Saletta2025345,
	author = {Saletta, Martina and Bombarda, Andrea and Bellini, Matteo and Goisis, Lucrezia and Cazzaniga, Paolo and Iascone, Maria Rosaria and Savo, Domenico Fabio},
	title = {Automated Phenotype-Based Clustering of Clinical Reports Using Large Language Models},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15735 LNAI},
	pages = {345 - 350},
	doi = {10.1007/978-3-031-95841-0_64},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009773519&doi=10.1007%2F978-3-031-95841-0_64&partnerID=40&md5=c2609a4c747cd6cf10226432e0e73001},
	abstract = {Large Language Models (LLMs) have shown significant potential in natural language processing tasks, including various applications in clinical and biomedical domains. This study explores the use of LLMs for analyzing a real dataset from Italian clinical reports and proposes a pipeline for automatically clustering these reports based on the described symptoms. The pipeline incorporates two approaches: (1) direct analysis of textual descriptions in the clinical reports, and (2) standardized processing through the automatic extraction of Human Phenotype Ontology terms using LLM-based methods. The obtained clusters will serve as the foundation for further predictive analyses, such as estimating the likelihood of a patient carrying specific genetic mutations. Our investigation compares the performance of direct text analysis against phenotype-standardized descriptions, highlighting the strengths and limitations of each approach. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Reports; Human Phenotype Ontology; Large Language Models; Phenotype Clustering; Clustering Algorithms; Computational Linguistics; Natural Language Processing Systems; Predictive Analytics; Based Clustering; Clinical Report; Clusterings; Human Phenotype Ontology; Language Model; Language Processing; Large Language Model; Natural Languages; Ontology's; Phenotype Clustering; Ontology},
	keywords = {Clustering algorithms; Computational linguistics; Natural language processing systems; Predictive analytics; Based clustering; Clinical report; Clusterings; Human phenotype ontology; Language model; Language processing; Large language model; Natural languages; Ontology's; Phenotype clustering; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ayad2025,
	author = {Ayad, Sarah and Khelifa Chibout, Lydia},
	title = {Ontology Population With Large Language Models (LLMs): A Case Study on Asbestos Ontology},
	year = {2025},
	journal = {Applied Ontology},
	pages = {},
	doi = {10.1177/15705838251334528},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009643870&doi=10.1177%2F15705838251334528&partnerID=40&md5=99260599b40b559e791145cd4b880519},
	abstract = {The Asbestos Ontology is a domain application ontology designed for use in an ontology-based approach that estimates the probability of the existence of asbestos products in a building. However, new issues in the building domain, such as predicting the presence of lead in buildings, renovating asbestos floors, or the reuse and recycling of components or parts of buildings as part of the circular economy, require a generalization of this ontology to a building ontology. The lack of relevant data tends to make decision-making difficult. The purpose of our approach is to show how instance-level knowledge graphs can be populated without having to manually create hundreds of instances using large language models (LLMs) and prompt engineering. This paper introduces a novel method for populating ontologies using the latest generative LLMs, such as GPT-3.5. Our method is characterized by an innovative recursive zero-shot prompting technique. The key contributions of this study are: (i) a new strategy for recursively prompting LLMs to elicit pertinent knowledge from the asbestos application domain; (ii) ontology population informed by the ontology metamodel; and (iii) formalization of the results into OWL axioms for the automatic integration of new instances. To evaluate the efficacy of our approach, we employed two main methodologies: (1) querying for instances linked to each entity; and (2) recursively querying for instances to leverage our recursive prompting strategy. Our initial strategy focused on evaluating the effectiveness of zero-shot prompting in retrieving relevant values for entities and data properties. This was facilitated through the development of the PromptGeneration function, which adjusted the input C<inf>i</inf> across various contexts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Asbestos; Chatgpt; Large Language Models; Ontology; Ontology Population; Prompt Engineering; Buildings; Decision Making; Knowledge Management; Case-studies; Chatgpt; In-buildings; Language Model; Large Language Model; Ontology Population; Ontology's; Ontology-based; Prompt Engineering; Reuse And Recycling; Asbestos; Ontology},
	keywords = {Buildings; Decision making; Knowledge management; Case-studies; ChatGPT; In-buildings; Language model; Large language model; Ontology Population; Ontology's; Ontology-based; Prompt engineering; Reuse and recycling; Asbestos; Ontology},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Chis2025,
	author = {Chis, Andrei and Ghiran, Ana Maria and Buchmann, Robert Andrei},
	title = {GRAPHxx: DSML engineering for knowledge graph building and streamlining with GraphRAG},
	year = {2025},
	journal = {Software and Systems Modeling},
	pages = {},
	doi = {10.1007/s10270-025-01297-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009527215&doi=10.1007%2Fs10270-025-01297-y&partnerID=40&md5=6ceabfe1f493290a523f3b4c103a4306},
	abstract = {Symbolic AI has been facing long-term adoption obstacles and a slow uptake caused by the limited availability of frictionless tooling—that can support not only knowledge engineers, but also novice users and educators, with designing and presenting graph exemplars that can be visually communicated, edited and ad hoc processed. There’s still a shortage of tools that can democratize knowledge graph (KG) creation, something that is increasingly needed—firstly by educators trying to discuss examples with novices without stumbling on OWL jargon at the earliest step, but also for more recent integration cases such as (i) GraphRAG, where private KGs are called to augment or ensure factuality of large language model services; or (ii) in search engine optimization (SEO), where SEO practitioners lacking knowledge engineering background must embed Schema.org graph data into their Web content. Most visual KG tools are visualizers of KGs created by other means—either in OWL-centric ontology editors posing high expertise barriers, or converted from available serializations, or lifted from legacy data sources. The few actual KG editors are mostly OWL editors neglecting to support the creation of schemaless graph datasets as well as of flexible combinations of graph data and schema fragments. This paper reports on a Design Science effort adopting metamodeling means, traditionally employed for the engineering of domain-specific modeling languages, toward defining a KG development and integration method that facilitates both the visual design of KG exemplars and their operationalization. We aim to balance a diagrammatic look and feel with machine readability of the semantic content being produced—further streamlined in an architecting proposition for integration with LLM services and for the production of Schema.org graph snippets. The DSML was deployed and evaluated as a tool implemented on the ADOxx metamodeling platform, using RDF and LangChain as mediators that streamline the content toward triplestores and LLM services. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adoxx; Domain-specific Modeling Language; Knowledge Graphs; Llm; Rdf; Retrieval Augmented Generation; Graph Theory; Integration; Knowledge Graph; Modeling Languages; Ontology; Search Engines; Semantic Web; Semantics; Visual Languages; Adoxx; Domain Specific Modeling Languages; Domain-specific Modelling Languages; Graph Data; Knowledge Graphs; Llm; Metamodeling; Rdf; Retrieval Augmented Generation; Search Engine Optimizations; Birds},
	keywords = {Graph theory; Integration; Knowledge graph; Modeling languages; Ontology; Search engines; Semantic Web; Semantics; Visual languages; ADOxx; Domain specific modeling languages; Domain-Specific Modelling Languages; Graph data; Knowledge graphs; LLM; Metamodeling; RDF; Retrieval augmented generation; Search engine optimizations; Birds},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Onozuka2025136,
	author = {Onozuka, Soichi and Ohnishi, Takaaki},
	title = {Analysis of LLMs for RDF Triple Generation: Semantic and Syntactic Evaluation Using WebNLG},
	year = {2025},
	journal = {Proceedings - IEEE International Conference on Semantic Computing, ICSC},
	pages = {136 - 143},
	doi = {10.1109/ICSC64641.2025.00025},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009459996&doi=10.1109%2FICSC64641.2025.00025&partnerID=40&md5=a0086a1e9ef98246a60e66d2507f9e5a},
	abstract = {Using the WebNLG dataset as ground truth, we evaluate the capability of large language models (LLMs) to generate resource description framework triples from natural language input. The proposed method employs two complementary evaluation metrics: cosine similarity for assessing semantic proximity and graph edit distance for comparing structural aspects of triple sets. The analysis demonstrates that these metrics provide distinct yet complementary perspectives on semantic evaluation, enabling a comprehensive assessment of the natural language understanding capabilities of LLMs. Through this approach, we demonstrate that modern LLMs exhibit sophisticated abilities in integrated syntax and semantics processing by utilizing distributed representations where both types of information coexist within high-dimensional vector spaces. This integration suggests that understanding of language structure of LLMs transcends simple pattern recognition to achieve meaningful semantic comprehension. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Llms; Ontology; Rdf; Similarity; Computational Linguistics; Large Datasets; Latent Semantic Analysis; Natural Language Processing Systems; Pattern Recognition; Resource Description Framework (rdf); Semantics; Syntactics; Vector Spaces; Evaluation Metrics; Ground Truth; Language Model; Large Language Model; Natural Languages; Ontology's; Rdf; Rdf Triples; Resources Description Frameworks; Similarity; Ontology},
	keywords = {Computational linguistics; Large datasets; Latent semantic analysis; Natural language processing systems; Pattern recognition; Resource Description Framework (RDF); Semantics; Syntactics; Vector spaces; Evaluation metrics; Ground truth; Language model; Large language model; Natural languages; Ontology's; RDF; RDF triples; Resources description frameworks; Similarity; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Jiang202528,
	author = {Jiang, Longquan and Huang, Junbo and Moller, Cedric and Usbeck, Ricardo},
	title = {Ontology-Guided, Hybrid Prompt Learning for Generalization in Knowledge Graph Question Answering},
	year = {2025},
	journal = {Proceedings - IEEE International Conference on Semantic Computing, ICSC},
	pages = {28 - 35},
	doi = {10.1109/ICSC64641.2025.00010},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009456479&doi=10.1109%2FICSC64641.2025.00010&partnerID=40&md5=19916ad8cf2702f3905c9c621d6d8ec4},
	abstract = {Most existing Knowledge Graph Question Answering (KGQA) approaches are designed for a specific KG, such as Wikidata, DBpedia or Freebase. Due to the heterogeneity of the underlying graph schema, topology and assertions, most KGQA systems cannot be transferred to unseen Knowledge Graphs (KGs) without resource-intensive training data. We present OntoSCPrompt, a novel Large Language Model (LLM)-based KGQA approach with a two-stage architecture that separates semantic parsing from KG-dependent interactions. OntoSCPrompt first generates a SPARQL query structure (including SPARQL keywords such as SELECT, ASK, WHERE and placeholders for missing tokens) and then fills them with KG-specific information. To enhance the understanding of the underlying KG, we present an ontology-guided, hybrid prompt learning strategy that integrates KG ontology into the learning process of hybrid prompts (e.g., discrete and continuous vectors). We also present several task-specific decoding strategies to ensure the correctness and executability of generated SPARQL queries in both stages. Experimental results demonstrate that OntoSCPrompt performs as well as SOTA approaches without retraining on a number of KGQA datasets such as CWQ, WebQSP and LC-QuAD 1.0 in a resource-efficient manner and can generalize well to unseen domain-specific KGs like DBLP-QuAD and CoyPu KG <sup>1</sup><sup>1</sup>Code: https://github.com/LongquanJiang/OntoSCPrompt. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Generalization; Kgqa; Llm; Qa; Knowledge Graph; Knowledge Transfer; Learning Systems; Ontology; Query Processing; Question Answering; Dbpedia; Generalisation; Knowledge Graph Question Answering; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Qa; Question Answering; Specific Knowledge; Semantics},
	keywords = {Knowledge graph; Knowledge transfer; Learning systems; Ontology; Query processing; Question answering; Dbpedia; Generalisation; Knowledge graph question answering; Knowledge graphs; Language model; Large language model; Ontology's; QA; Question Answering; Specific knowledge; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2025,
	title = {Proceedings - 2025 19th International Conference on Semantic Computing, ICSC 2025},
	year = {2025},
	journal = {Proceedings - IEEE International Conference on Semantic Computing, ICSC},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009456371&partnerID=40&md5=11bcadb4eae661420a580dd18b29d3ae},
	abstract = {The proceedings contain 47 papers. The topics discussed include: a generative ai-based framework for decentralized finance and cryptocurrency fraud prevention; ontology-guided, hybrid prompt learning for generalization in knowledge graph question answering; multi-media authentication through application of generative adversarial networks and blockchain technology; analysis of LLMs for RDF triple generation: semantic and syntactic evaluation using WebNLG; emotion cum false information detection based on sentiment, structural and semantic frameworks; towards semantically interoperable data ecosystems in the energy sector: a case study on domain-specific ontologies in an energy data space; and improving tabular reusability through data dictionary descriptions. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Scheffer202513,
	author = {Scheffer, Sara E. and Ansari, Fazel},
	title = {An ontological framework for AR-enhanced maintenance management},
	year = {2025},
	journal = {Procedia CIRP},
	volume = {134},
	pages = {13 - 18},
	doi = {10.1016/j.procir.2025.03.044},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009411442&doi=10.1016%2Fj.procir.2025.03.044&partnerID=40&md5=05531b0f5955c3fde7b7fe0952db0925},
	abstract = {This paper presents an ontology design framework that integrates opportunistic maintenance (OM) strategies, Stackelberg game theory models, Nash equilibrium concepts, and augmented reality (AR) to enhance decision-making, information visualization and contextualization in industrial settings. Traditional OM approaches often lack real-time, context-aware decision support, efficient resource allocation, and multi-agent collaboration, essential for dynamic optimization strategies. Integrating Stackelberg game theory models into OM enhances resource allocation and scheduling through hierarchical decision-making, allowing a leader-follower to optimally distribute resources while providing data-rich visualization and supporting adaptive, strategic planning in maintenance management. In dynamic multi-agent environments with multiple stakeholders, achieving Nash equilibrium leads to stable and efficient resource allocation, as no participant can unilaterally improve their outcome without impacting others. By incorporating Stackelberg game dynamics, Nash equilibrium concepts, and AR, the conceptual framework facilitates structured strategic planning, balancing leadership-driven optimization with equilibrium-based stability in decision-making and visualization. Evaluation of this ontology is proposed through a case study in a laboratory setting. The proposed ontology serves as a knowledge base for improving decision-making and provides a replicable framework for future advancements in industrial maintenance management by enabling the integration of emerging technologies, such as foundation models and large language models (LLMs), to refine maintenance strategies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ar Decision-making; Game Theory; Ontology; Opportunistic Maintenance; Behavioral Research; Computation Theory; Data Visualization; Decision Making; Decision Support Systems; Decision Theory; Information Analysis; Information Management; Information Use; Knowledge Based Systems; Maintenance; Multi Agent Systems; Nash Equilibrium; Resource Allocation; Strategic Planning; Visualization; Augmented Reality Decision-making; Decisions Makings; Efficient Resource Allocation; Game Theory Models; Maintenance Management; Maintenance Strategies; Nash Equilibria; Ontology's; Opportunistic Maintenance; Stackelberg Games; Game Theory; Ontology},
	keywords = {Behavioral research; Computation theory; Data visualization; Decision making; Decision support systems; Decision theory; Information analysis; Information management; Information use; Knowledge based systems; Maintenance; Multi agent systems; Nash equilibrium; Resource allocation; Strategic planning; Visualization; Augmented reality decision-making; Decisions makings; Efficient resource allocation; Game theory models; Maintenance management; Maintenance strategies; Nash equilibria; Ontology's; Opportunistic maintenance; Stackelberg Games; Game theory; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ikeda2025,
	author = {Ikeda, Shuya and Zou, Zhaonan and Bono, Hidemasa U. and Moriya, Yuki and Kawashima, Shuichi and Katayama, Toshiaki and Oki, Shinya and Ohta, Tazro},
	title = {Extraction of biological terms using large language models enhances the usability of metadata in the BioSample database},
	year = {2025},
	journal = {GigaScience},
	volume = {14},
	pages = {},
	doi = {10.1093/gigascience/giaf070},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009294611&doi=10.1093%2Fgigascience%2Fgiaf070&partnerID=40&md5=eefa70b7ae8e0c2a220698024886e237},
	abstract = {BioSample is a repository of experimental sample metadata. It is a comprehensive archive that enables searches of experiments, regardless of type. However, there is substantial variability in the submitted metadata due to the difficulty in defining comprehensive rules for describing them and the limited user awareness of best practices in creating them. This inconsistency poses considerable challenges to the findability and reusability of archived data. Given the scale of BioSample, which hosts over 40 million records, manual curation is impractical. Automatic rule-based ontology mapping methods have been proposed to address this issue, but their effectiveness is limited by the heterogeneity of the metadata. Recently, large language models (LLMs) have gained attention in natural language processing and are promising tools for automating metadata curation. In this study, we evaluated the performance of LLMs in extracting cell line names from BioSample descriptions using a gold-standard dataset derived from ChIP-Atlas, a secondary database of epigenomics experiment data in which samples were manually curated. The LLM-assisted methods outperformed traditional approaches, achieving higher accuracy and coverage. We further extended them to extract information about experimentally manipulated genes from metadata when manual curation had not yet been applied in ChIP-Atlas. This also yielded successful results, including the facilitation of more precise filtering of the data and the prevention of possible misinterpretations caused by the inclusion of unintended data. These findings underscore the potential of LLMs in improving the findability and reusability of experimental data in general, which would considerably reduce the user workload and enable more effective scientific data management. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Automatic Data Curation; Biological Sample; Large Language Model; Article; Awareness; Best Practice; Chromatin Immunoprecipitation; Data Base; Epigenetics; Filtration; Human; Information Processing; Large Language Model; Metadata; Natural Language Processing; Nonhuman; Prevention; Usability; Workload; Bioinformatics; Data Mining; Factual Database; Procedures; Computational Biology; Data Curation; Data Mining; Databases, Factual; Humans; Large Language Models; Metadata; Natural Language Processing},
	keywords = {article; awareness; best practice; chromatin immunoprecipitation; data base; epigenetics; filtration; human; information processing; large language model; metadata; natural language processing; nonhuman; prevention; usability; workload; bioinformatics; data mining; factual database; procedures; Computational Biology; Data Curation; Data Mining; Databases, Factual; Humans; Large Language Models; Metadata; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {26th International Working Conference on Business Process Modeling, Development, and Support, BPMDS 2025 and 30th International Working Conference on Exploring Modeling Methods for Systems Analysis and Development, EMMSAD 2025},
	year = {2025},
	journal = {Lecture Notes in Business Information Processing},
	volume = {558 LNBIP},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009268144&partnerID=40&md5=85562aaa54388b7c4351f6208fe33eb8},
	abstract = {The proceedings contain 28 papers. The special focus in this conference is on Business Process Modeling, Development, and Support. The topics include: Interdependency-Aware Business Process Prioritization for Process Improvements; Human-AI Collaboration for Business Process Modeling with Petri Nets; repairing Process Descriptions by Discovering Deviations from Process Models; process Model Complexity Metrics, Cognitive Load and Visual Behavior: A Multi-granular Eye-Tracking Analysis; which Tables are Mine(able)?; OCPM<sup>2</sup>: Extending the Process Mining Methodology for Object-Centric Event Data Extraction; making the Case for Process Analytics: A Use Case in Court Proceedings; predicting Unseen Process Behavior Based on Log Injection; toward IoT-Based Process Analytics: Extending Event Knowledge Graphs with Ambiguity; probabilistic Learning of Temporal Uncertainties in Business Processes; Assessing the Suitability of Large Language Models in Generating UML Class Diagrams as Conceptual Models; exploring the Influence of Data Characteristics on Machine Learning Outcomes; Can an LLM Use Work System Axioms When Describing Work Systems for Requirements Analysis?; forensic Readiness and Privacy: Towards Resolving Software Goal Conflict; an Ontological Model of the Phishing Attack Process; harborLang: Enhancing Maritime Operational Safety Through Cyber Threat Simulation and Assessment; learning Analytics Dashboard with Peer Comparison for Student Feedback in Conceptual Modeling Education; are Code and Design Models Similarly Effective in Understanding Software Structure and Behavior?; Towards a Maturity Assessment Framework for MBSE Adoption: Results from a Meta-synthesis; enhancing C2-Systems: Validation of Goal and Concept Models with Stakeholders; supporting Collaborative Design by Diagram Briefs in the Early Stage of Innovation Projects; A Metamodel for Applying Green BPM Approaches with the EU Taxonomy; experience Report: Applying a Capability Heat Map in a Government Organization; state of the Art and Research Directions for Visual Conceptual Modeling in Robotics; a Domain-Specific Modeling Method for Designing Conversational Agents for Coaching: A Case from Health Coaching. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang202565,
	author = {Zhang, Mingzhe and Huang, Yujing and Zhang, Dehai and Li, Jianqing and Xia, Rongjie},
	title = {Constructing Knowledge Graph for News Recommendation in Integrated Media},
	year = {2025},
	journal = {Learning and Analytics in Intelligent Systems},
	volume = {50},
	pages = {65 - 72},
	doi = {10.1007/978-3-031-94266-2_9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009249328&doi=10.1007%2F978-3-031-94266-2_9&partnerID=40&md5=6cc6654add4a86b7ef9263ae7f290c20},
	abstract = {In the age of information technology, although it is convenient to obtain the information of integrated media, the hidden value is difficult to excavate directly, and it is necessary to use knowledge graph to carry out intelligent analysis and interpretation. However, the construction of a knowledge graph for integrated media is time-consuming and laborious. Therefore, this paper initiates the construction of an integrated media knowledge graph for news recommendation. Firstly, this paper carried out the construction of news knowledge graph ontology. Subsequently, this paper uses large language models (LLMs) for event extraction and event relation extraction work, while this paper uses a BERT + Softmax model to predict entity-level relation within events. The automated construction from news text and related images to knowledge graph is realized. Experiments show that the integrated media knowledge graph generated through this automated construction process is of high quality, demonstrating its potential in personalized news recommendation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Event Extraction; Knowledge Extraction; Knowledge Graph Construction; Large Language Model; Knowledge Graph; Ontology; Undirected Graphs; Automated Construction; Events Extractions; Graph Construction; Integrated Media; Knowledge Extraction; Knowledge Graph Construction; Knowledge Graphs; Language Model; Large Language Model; News Recommendation; Extraction},
	keywords = {Knowledge graph; Ontology; Undirected graphs; Automated construction; Events extractions; Graph construction; Integrated media; Knowledge extraction; Knowledge graph construction; Knowledge graphs; Language model; Large language model; News recommendation; Extraction},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {37th International Conference on Advanced Information Systems Engineering, CAiSE 2025},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15702 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009239394&partnerID=40&md5=284793b05f9a489ecff4b2d02ad099cd},
	abstract = {The proceedings contain 17 papers. The special focus in this conference is on Advanced Information Systems Engineering. The topics include: Benchmarking LLMs for Business Architecture Modelling with Hierarchical Capability Maps; Evaluating Organization Security: User Stories of European Union NIS2 Directive; LitroACP: A Lightweight and Robust Framework for Extracting Access Control Policies from Specifications; energy Profiling of Data-Sharing Pipelines: Modeling, Estimation, and Reuse Strategies; determining Window Sizes Using Species Estimation for Accurate Process Mining over Streams; engineering Early Warning Systems: an Industrial Experience; LLMs to Replace Crowdsourcing in Generating Syntactically Diverse Paraphrases for Task-Oriented Chatbots; a Conversational Framework for Faithful Multi-perspective Analysis of Production Systems; achieving Group Fairness Through Independence in Predictive Process Monitoring; on the Use of Steady-State Detection for Process Mining: Achieving More Accurate Insights; automating Performance Insights: Suggesting and Computing Process Performance Indicators from Event Logs; collaborative Multi-organization Information System Engineering Based on Team Practice Agreements; declarative Domain Testing: An Approach for Automatic and Integrated Test Data Generation; anchorlogy: An Ontology for Anchoring Bias Detection in Forecasting; process Model Forecasting Using Deep Temporal Learning. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {International workshops associated with the 37th International Conference on Advanced Information Systems Engineering, CAiSE 2025},
	year = {2025},
	journal = {Lecture Notes in Business Information Processing},
	volume = {556 LNBIP},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009234167&partnerID=40&md5=12419aab3c40eba9125b66b3b9488643},
	abstract = {The proceedings contain 29 papers. The special focus in this conference is on Advanced Information Systems Engineering. The topics include: Benchmarking Knowledge Graph Question Answering via Complexity-Aware Queries; towards Industry 5.0 in Tourism Sector Through Conceptual Modeling and Semantic Technology for Personalized Travel Itineraries; retrieval-Augmented Generation for Entity Alignment in Knowledge Graphs: An Incipient Experiment; Natural Language Querying of Invoice Data Using RAG and GraphRAG: Leveraging LLMs for Financial Document Insights; impact of Knowledge Graph Representations on Question Answering with Language Models; The MATRIX Ontology - Semantic Memory for Multi-agent Experience Transfer, Reasoning and and Interaction eXchange; Discovering Inconsistencies in Documents with Long-Context LLMs; a Blockchain-Based Model for Fungible Assets and Secure Transformation Processes Traceability; suspicious Activity Detection Using Blockchain Process Mining; feature Selection in Medical Imaging: A Comprehensive Review; Exploring BioNER Frontiers: An In-Depth Evaluation; a Meta-model for Integrating Explainable Forecasting with Digital Twins; towards a Digital Twin of a Decanter Centrifuge for Wastewater Management; digital Twins in Systems-of-Systems on the Internet-of-Vehicles: The Case of Overtaking Maneuver; development and Evaluation of Support Tools for Modeling Digital Twin Concepts and Architectures; requirements Analysis for a Digital Twin to Increase the Resilience of Multimodal Corridors: A Case Study in the Twente Region; Leveraging LLMs to Discover Causal Dependencies: A Case Study on a University Program; Exploring Business Process Model Similarity with LLMs: Challenges and Potentials; enabling Process Mining on Multimodal Robotic Data; LLM-Enhanced Derivation of the Maturity Level of RESTful Services; Improving Content-Based Data Product Retrieval in Federated Environments with LLM and Sampling; Leveraging LLMs and RAG for Enhanced Football Talent Scouting; On the Maturity of LLMOps Services Computing: An Industrial Study; automated Newsrooms and Enhanced Editorial Processes Through Large Language Models; Language Models for Legal NLP: A Literature Review. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kovriguina202593,
	author = {Kovriguina, Liubov and Muromtsev, Dmitry I. and Haase, Peter},
	title = {The MATRIX Ontology - Semantic Memory for Multi-agent Experience Transfer, Reasoning and and Interaction eXchange},
	year = {2025},
	journal = {Lecture Notes in Business Information Processing},
	volume = {556 LNBIP},
	pages = {93 - 104},
	doi = {10.1007/978-3-031-94931-9_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009229875&doi=10.1007%2F978-3-031-94931-9_8&partnerID=40&md5=3eaeb4496f47656d1ecd579e4093c103},
	abstract = {Multi-agent collaboration has a long established record in reinforcement learning, and has gained momentum in the recent years with the breakthroughs in language-based intelligence. At the same time, there has been demonstrated a significant potential of joining LLM and RL agents in a single pipeline. Memory is the central component of multi-agent pipelines, both in learning and inference: it stores long-term experience, agents’ transactions in the working memory, other learning artifacts. However, given a plethora of protocols, environments, and experience representations, the challenge is to implement interpretable decision-making between RL agents (neural layer), and LLM agents (symbolic) in working memory, and experience transfer in the long run. Driven by the idea that agents should be able to collaborate autonomously, and learn how to use, transfer, and synthesize new knowledge and processes, we propose a graph-based memory model, which can be shared and reused by both RL and LLM agentic teams on any representation level. This memory model is a collection of RDF graphs, allowing for neural and symbolic representations at the same time, and providing interoperability and explainability of knowledge graphs. In the current paper, we introduce the MATRIX ontology (Multi-Agent Experience Transfer, Reasoning and Interaction eXchange), a conceptualization behind the shared memory layer, that can serve as a memory, decision, and experience model for heterogeneous agentic teams, and show its applications in different use cases. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Experience Transfer Between Neuro-symbolic Agents; Knowledge Graphs For Shared Memory Representation; Multi-agent Reinforcement Learning; Ontology For Multi-agent Experience Transfer; Semantic Memory For Multi-agent Teams; Autonomous Agents; Decision Making; Graph Theory; Intelligent Agents; Matrix Algebra; Memory Architecture; Multi Agent Systems; Neural Networks; Reinforcement Learning; Semantic Web; Semantics; Transfer Learning; Experience Transfer Between Neuro-symbolic Agent; Knowledge Graph For Shared Memory Representation; Knowledge Graphs; Multi Agent; Multi-agent Reinforcement Learning; Multiagent Teams; Ontology For Multi-agent Experience Transfer; Ontology's; Semantic Memory; Semantic Memory For Multi-agent Team; Shared Memory; Symbolic Agents; Ontology},
	keywords = {Autonomous agents; Decision making; Graph theory; Intelligent agents; Matrix algebra; Memory architecture; Multi agent systems; Neural networks; Reinforcement learning; Semantic Web; Semantics; Transfer learning; Experience transfer between neuro-symbolic agent; Knowledge graph for shared memory representation; Knowledge graphs; Multi agent; Multi-agent reinforcement learning; Multiagent teams; Ontology for multi-agent experience transfer; Ontology's; Semantic memory; Semantic memory for multi-agent team; Shared memory; Symbolic agents; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Guryanov20251011,
	author = {Guryanov, A. V. and Moshkin, Vadim S. and Dyrnochkin, Alexander},
	title = {An Approach to Automated Ontology Extraction From Technological Documentation Using NLP and LLM},
	year = {2025},
	pages = {1011 - 1016},
	doi = {10.1109/ICIEAM65163.2025.11028174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009220232&doi=10.1109%2FICIEAM65163.2025.11028174&partnerID=40&md5=ae8cbf6929836ebcd5106cf7818cd65b},
	abstract = {The paper presents an approach to constructing a formal description of a subject area using automated analysis of technological documentation. The first stage of the proposed approach is extracting data (terms, objects of the subject area) from technical documentation on the subject area. Then, a natural language query is formed for a large language model (LLM), which helps to extract relations between the extracted terms. At the final stage, an OWL ontology is formed by postprocessing the obtained data. Evaluation experiments were conducted on 16 documents (ISO) on the topic of 'Space industry'. Experiments were also conducted to compare algorithms for extracting terms and relations using LLM and the C-value algorithm. The proposed approach has proven its effectiveness on strictly formalized texts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Design Documentation; Large Language Model; Natural Language Processing; Ontology; Technical Documentation; Term; C (programming Language); Natural Language Processing Systems; Query Languages; Design Documentation; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Ontology Extraction; Ontology's; Technical Documentations; Term; Ontology},
	keywords = {C (programming language); Natural language processing systems; Query languages; Design documentation; Language model; Language processing; Large language model; Natural language processing; Natural languages; Ontology Extraction; Ontology's; Technical documentations; Term; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {37th International Conference on Advanced Information Systems Engineering, CAiSE 2025},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15701 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009217308&partnerID=40&md5=31411309fc4fbff99a586cd95fc62511},
	abstract = {The proceedings contain 17 papers. The special focus in this conference is on Advanced Information Systems Engineering. The topics include: Benchmarking LLMs for Business Architecture Modelling with Hierarchical Capability Maps; Evaluating Organization Security: User Stories of European Union NIS2 Directive; LitroACP: A Lightweight and Robust Framework for Extracting Access Control Policies from Specifications; energy Profiling of Data-Sharing Pipelines: Modeling, Estimation, and Reuse Strategies; determining Window Sizes Using Species Estimation for Accurate Process Mining over Streams; engineering Early Warning Systems: an Industrial Experience; LLMs to Replace Crowdsourcing in Generating Syntactically Diverse Paraphrases for Task-Oriented Chatbots; a Conversational Framework for Faithful Multi-perspective Analysis of Production Systems; achieving Group Fairness Through Independence in Predictive Process Monitoring; on the Use of Steady-State Detection for Process Mining: Achieving More Accurate Insights; automating Performance Insights: Suggesting and Computing Process Performance Indicators from Event Logs; collaborative Multi-organization Information System Engineering Based on Team Practice Agreements; declarative Domain Testing: An Approach for Automatic and Integrated Test Data Generation; anchorlogy: An Ontology for Anchoring Bias Detection in Forecasting; process Model Forecasting Using Deep Temporal Learning. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Huang202557,
	author = {Huang, Yujing and Zhang, Mingzhe and Li, Xiaohan and Zhang, Dehai and Qin, Peiqi and Ren, Diandian and Xiao, Yanxu},
	title = {Construction of Yunnan Minority Artifacts Knowledge Graph Based on Pre-trained Large Language Models},
	year = {2025},
	journal = {Learning and Analytics in Intelligent Systems},
	volume = {50},
	pages = {57 - 64},
	doi = {10.1007/978-3-031-94266-2_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009210918&doi=10.1007%2F978-3-031-94266-2_8&partnerID=40&md5=2968594d6ca574a7b10dc236ee5bf29f},
	abstract = {Yunnan's minority artifacts, as symbols of national memory and culture, have not been adequately preserved or utilized. This paper aims to construct a knowledge graph for the image description of Yunnan minority artifacts. First, a typical artifact ontology was developed using existing data. Then, traditional BERT-based methods were compared with emerging LLM-based approaches for entity recognition and relation extraction. The experiments show that using BERT combined with Bi-LSTM and CRF for entity recognition and BERT for relation extraction outperforms LLM in this scenario. This study adopts traditional methods to successfully construct a knowledge graph, enhancing the comprehension and interpretation of these cultural elements through precise entity linking. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Extraction; Knowledge Graph; Large Language Model; Yunnan Ethnic Minorities; Data Mining; Graphic Methods; Knowledge Graph; Ontology; Entity Recognition; Ethnic Minorities; Graph-based; Image Descriptions; Knowledge Extraction; Knowledge Graphs; Language Model; Large Language Model; Relation Extraction; Yunnan Ethnic Minority; Extraction},
	keywords = {Data mining; Graphic methods; Knowledge graph; Ontology; Entity recognition; Ethnic minorities; Graph-based; Image descriptions; Knowledge extraction; Knowledge graphs; Language model; Large language model; Relation extraction; Yunnan ethnic minority; Extraction},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2025,
	title = {SemTech4STLD 2025 - 3rd International Workshop on Semantic Technologies and Deep Learning Models for Scientific, Technical and Legal Data, co-located with Extended Semantic Web Conference 2025, ESWC 2025},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3979},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009162903&partnerID=40&md5=909a4501d42a0a589d09407ecef13feb},
	abstract = {The proceedings contain 7 papers. The topics discussed include: evaluating LLMs for named entity recognition in scientific domain with fine tuning and few-shot learning; taming hallucinations: a semantic matching evaluation framework for LLM generated ontologies; benchmarking large language models for sustainable development goals classification: evaluating in-context learning and fine-tuning strategies; biomedical entity linking with triple-aware pre-training; enabling natural language access to BIM models with ai and knowledge graphs; context-aware explanations: leveraging knowledge graphs for adaptive explainability in dynamic environments; and leveraging knowledge graphs and generative ai for augmented research paper retrieval. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ahmed2025199,
	author = {Ahmed, Nafisa and Kwok, Hin Chi and Hamdaqa, Mohammad and Assunção, Wesley Klewerton Guez},
	title = {SMATCH-M-LLM: Semantic Similarity in Metamodel Matching With Large Language Models},
	year = {2025},
	pages = {199 - 210},
	doi = {10.1109/MSR66628.2025.00040},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009099546&doi=10.1109%2FMSR66628.2025.00040&partnerID=40&md5=99eebac4ac67ecc52b6190e9eb76e74f},
	abstract = {Metamodel matching plays a crucial role in defining transformation rules in model-driven engineering by identifying correspondences between different metamodels, forming the foundation for effective transformations. Current techniques face significant challenges due to syntactical and structural heterogeneity. To address this, matching techniques often employ semantic similarity to identify correspondences. Traditional semantic matchers, however, rely on ontology matching tools or lexical databases, which often struggle when metamodels use different terminologies or hierarchical structures. Inspired by the contextual understanding capabilities of Large Language Models (LLMs), this paper explores the capability of GPT-4 potentials as a semantic matcher and alternative to existing methods for metamodel matching. However, metamodels can be large, which can overwhelm LLMs if provided in a single prompt, leading to reduced accuracy. Therefore, we propose prompting LLMs with fragments of the source and target metamodels, identifying correspondences through an iterative process. The fragments to be provided in the prompt are identified based on an initial mapping derived from their elements' definitions. Through experiments with 10 metamodels, our results show that our LLMbased approach improves the accuracy of metamodel matching, achieving an average F-measure of ≈ 91%, outperforming both the baseline and hybrid approaches, which have a maximum average F-measure of ≈ 29% and ≈ 74%, respectively. Moreover, our approach surpasses single-prompt LLM-based matching, which has an average F-measure of 80%, by approximately 11%. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain-specific Languages; Metamodel Matching; Model Migration; Model-driven Engineering; Computational Linguistics; Iterative Methods; Latent Semantic Analysis; Ontology; Software Engineering; Terminology; Domains Specific Languages; F Measure; Language Model; Meta Model; Meta-model Matching; Model Migrations; Model Semantics; Model-driven Engineering; Semantic Similarity; Transformation Rules; Semantics},
	keywords = {Computational linguistics; Iterative methods; Latent semantic analysis; Ontology; Software engineering; Terminology; Domains specific languages; F measure; Language model; Meta model; Meta-model matching; Model migrations; Model semantics; Model-driven Engineering; Semantic similarity; Transformation rules; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Fathallah2025,
	author = {Fathallah, Nadeen and Staab, Steffen and Algergawy, Alsayed},
	title = {Taming Hallucinations: A Semantic Matching Evaluation Framework for LLM-Generated Ontologies},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3979},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009068954&partnerID=40&md5=b4220ec10d18b3bc6039a3c2da02ba9d},
	abstract = {Ontology learning using Large Language Models (LLMs) has shown promise yet remains challenged by hallucinations-spurious or inaccurate concepts and relationships that undermine domain validity. This issue is particularly critical in highly specialized fields such as life sciences, where ontology accuracy directly impacts knowledge representation and decision-making. In this work, we introduce an automated evaluation framework that systematically assesses the quality of LLM-generated ontologies by comparing their concepts and relationship triples against domain knowledge (i.e. expert-curated domain ontologies). Our approach leverages transformer-based semantic similarity methods to detect hallucinations, ensuring that generated ontologies align with real-world knowledge. We evaluate our framework using six LLM-generated ontologies, validating them against three reference ontologies with increasing domain specificity. This work establishes a scalable, automated approach for validating LLM-generated ontologies, paving the way for their broader adoption in complex, knowledge-intensive domains. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Life Science Domain; Neon-gpt; Ontology Learning; Ontology Matching; Decision Making; Domain Knowledge; Human Computer Interaction; Knowledge Management; Knowledge Representation; Learning Systems; Ontology; Semantics; Evaluation Framework; Language Model; Large Language Model; Life Science Domain; Life-sciences; Neon-gpt; Ontology Learning; Ontology Matching; Ontology's; Semantic Matching; Copyrights},
	keywords = {Decision making; Domain Knowledge; Human computer interaction; Knowledge management; Knowledge representation; Learning systems; Ontology; Semantics; Evaluation framework; Language model; Large language model; Life science domain; Life-sciences; Neon-GPT; Ontology learning; Ontology matching; Ontology's; Semantic matching; Copyrights},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {10th International Symposium on End-User Development, IS-EUD 2025},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15713 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008963787&partnerID=40&md5=f3dfb2cec173ad9ba451ab30a2894fb2},
	abstract = {The proceedings contain 21 papers. The special focus in this conference is on End-User Development. The topics include: Explaining Problems in Daily Automations with ExplainTAP; Conversational Rule Creation in XR: User’s Strategies in VR and AR Automation; From User Needs to Smart Ecosystems Through LLMs: The Smartifier Case Study; Assembly Workers as PLC Programmers: What End Users Need to Understand About Low-Code Development Platforms; Fostering Novice Collaboration in ML-Based System Design Through Visual Languages and Touch Interfaces; visual and Textual Language Synchronization for End-User Programming Environments; OMNI: An EUD Tool for Ontological Multisensory Navigation Interface; Explanation-Driven Interventions for Artificial Intelligence Model Customization: Empowering End-Users to Tailor Black-Box AI in Rhinocytology; Evaluating Visual Prompting Modalities for Generative AI-Assisted UI Design; beyond Usability: Introducing Wellbeing-Driven End-User Development Design; AI-Assisted Cognitive Support for Caregivers: A RAG and EUD Framework for Geriatric Care; From Digital Self-control Apps to iOS Shortcuts: Enabling Privacy-Centric Wellbeing Research Without Code; empowering End Users to Design for Their Digital Self-control; Leveraging EUD and Generative AI for Ethical Phishing Campaigns; classifying User Motivation in Interactions; mindsets, Cultures, and Technologies in Support of End-User Development; Assessing Computational Thinking Skills Through Artefacts: The Case of ModeLLer; scenario-Based Design Tools: From End-User Reflections to Requirements; defining Classes and Semantic Relationships from User Scenarios Through a Heuristic Approach. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {Forum and the Doctoral Consortium of the 37th International Conference on Advanced Information Systems Engineering, CAiSE 2025},
	year = {2025},
	journal = {Lecture Notes in Business Information Processing},
	volume = {557 LNBIP},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008677236&partnerID=40&md5=0404bd4d255662a8ffea0f13d44ad7cc},
	abstract = {The proceedings contain 38 papers. The special focus in this conference is on Advanced Information Systems Engineering. The topics include: Towards the Interoperability of Low-Code Platforms; The Work System Perspective as a Nexus Between Silos in IS Modeling and Between Rigor and Broad Usability; leveraging Profiling to Bridge Healthcare Silos for Federated Analyses; lost in Models? Structuring Managerial Decision Support in Process Mining with Multi-criteria Decision Making; from Words to Workflows: Extracting Object-Centric Event Logs from Textual Data; WSF4ADO: An ADOxx Deployment of the Work Systems Framework; robotic Datasets for Process Mining; Automated Business Process Analysis: An LLM-Based Approach to Value Assessment; Humidor: A Zero-Shot LLM Approach for Cumulative Knowledge Building in Design Science Research; trust Paradoxes in Machine Learning: An Ontological Approach; conceptualizing Business Process Dependencies That Propagate Cyber Risk; towards a Data Satellite Architecture for Federated Digital Ecosystems: Combating Data Pollution and Enhancing Trustworthiness; towards an Ontology for Representing Time Series Knowledge: Motivation, Requirements and Concept; OLAP Operations for Object-Centric Process Mining; goal-Oriented Process Monitoring: An Artifact-Driven Monitoring Extension; LLM4Model: Automated Requirements Specification Model Authoring; a Pattern-Based Approach for Explaining Ontology-Driven Conceptual Models; Towards an AI-Agent-Based Framework for Agile Business Process Management; studying Workarounds in Software Forms: An Experimental Protocol; Pondering on Capability Brokering with LLM; MARTSIA: A Tool for Confidential Data Exchange via Public Blockchain; AOAME: An Enterprise Knowledge Graphs Editor for Domain Experts; OpenBPT: An Extensible Platform for Conceptual Modeling and Analysis; SecBPMN2BC Online Editor: A Web-Based Tool for Designing Secure Business Processes on Blockchains; a3S3 - Automated Android Audit of Safety and Security Signals; FIREPRIME App: A Self-assessment Tool to Evaluate Home Risk to Wildfires; toward Improving the Quality of Enterprise Architecture Models; security in Automated Manufacturing: A Function-Driven Approach. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Amdouni2025,
	author = {Amdouni, Emna and Belfadel, Abdelhadi and Gagnant, Maxence and Renault, Isabelle and Kierszbaum, Samuel and Carrion, Jeremy and Dussartre, Matthieu and Tmar, Sana},
	title = {Semi-Automatic Building of Ontologies from Unstructured French Texts: Industrial Case Study},
	year = {2025},
	journal = {Data Science and Engineering},
	pages = {},
	doi = {10.1007/s41019-025-00284-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008668986&doi=10.1007%2Fs41019-025-00284-z&partnerID=40&md5=8a3069cdff10b1b5c2fba5b3ae8bc18d},
	abstract = {An enterprise’s knowledge is often contained in many documents of different types: textual and multimedia. Exploiting this information is generally difficult due to a lack of suitable tools. In this research work, our motivation is to assist non-expert users in creating a semantic model with minimum manual design. This paper describes a semi-automatic pipeline to build an ontology version from French texts by parsing natural language content and exploring open knowledge graphs. Specifically, with practical examples, we showcase how we integrated some existing open-source NLP solutions related to Named Entity Recognition, Relation Recognition and Named Entity Linking for building enriched ontology versions. We applied our approach to generate and manage industrial ontology versions for two real case studies: electrical grid management and requirements engineering. Finally, we conclude that having a complete and enriched knowledge graph requires larger training datasets for better performance of NLP tasks. Some steps of ontology implementation are still hard to computerize completely, notably axiom discovery, which strongly relies on supervised language models. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Electrical Grid Management; Knowledge Graphs; Natural Language Processing; Ontology Design; Requirements Engineering; Semantic Web; Computational Linguistics; Engineering Research; Industrial Management; Knowledge Graph; Knowledge Management; Natural Language Processing Systems; Ontology; Open Systems; Semantic Web; Electrical Grid Management; Electrical Grids; Grid Management; Knowledge Graphs; Language Processing; Natural Language Processing; Natural Languages; Ontology Design; Requirement Engineering; Semantic-web; Requirements Engineering},
	keywords = {Computational linguistics; Engineering research; Industrial management; Knowledge graph; Knowledge management; Natural language processing systems; Ontology; Open systems; Semantic Web; Electrical grid management; Electrical grids; Grid management; Knowledge graphs; Language processing; Natural language processing; Natural languages; Ontology design; Requirement engineering; Semantic-Web; Requirements engineering},
	type = {Review},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2025,
	title = {ESWC-JP 2025 - Joint Proceedings of the ESWC 2025 Workshops and Tutorials, co-located with 22nd Extended Semantic Web Conference, ESWC 2025},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3977},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008507449&partnerID=40&md5=e93234279758ef7da9d05599f5ba9ffc},
	abstract = {The proceedings contain 39 papers. The topics discussed include: from experts to LLMs: evaluating the quality of automatically generated ontologies; assessing the capability of large language models for domain-specific ontology generation; how do scaling laws apply to knowledge graph engineering tasks? the impact of model size on large language model performance; large language models as knowledge evaluation agents; LLM-retrieval based scientific knowledge grounding; ConExion: concept extraction with large language models; semantic similarity analysis of scientific papers in scholarly knowledge graphs; MESD: metadata extraction from scholarly documents - a shared task overview; and from nearest neighbors to LLMs: a hybrid zero-shot approach to multi-label classification. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Llugiqi2025,
	author = {Llugiqi, Majlinda and Ekaputra, Fajar J. and Sabou, Marta},
	title = {From Experts to LLMs: Evaluating the Quality of Automatically Generated Ontologies},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3977},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008498807&partnerID=40&md5=b2b9fe8261f5ee963e6e69a1cf2d3f92},
	abstract = {Ontologies play a crucial role in knowledge representation, yet their manual construction requires domain expertise and effort. While previous work has focused on using large language models (LLMs) for assessing ontology creation, fully automated ontology generation remains underexplored. As a consequence, most research relies on a limited set of well-known ontologies or knowledge graphs, which constrains the evaluation of various tasks such as link prediction and knowledge graph completion. This highlights the need for diverse ontology benchmarks with varying characteristics, such as number of concepts, hierarchy depth and so on, to effectively evaluate tasks such as link prediction and knowledge graph completion. In this work, we investigate the feasibility of generating ontologies using LLMs and evaluate whether they can produce ontologies of comparable quality to human-built ones. Given a seed set of concepts, a target number of concepts, relations, and maximum hierarchy depth, we employ three different LLMs to generate ontologies within the heart disease domain. Defining a seed set of concepts is particularly important for modeling the features of tabular datasets, enabling structured knowledge representation for downstream tasks. We systematically evaluate the generated ontologies by analyzing their structural integrity, semantic coherence, and suitability for downstream tasks. Our results show that while LLM-generated ontologies differ structurally from human-built ones, they remain comparable in semantic similarity and downstream ML performance, with LLaMA-generated ontologies proving to be the most effective. These findings highlight the potential of LLM-generated ontologies not only to support automated knowledge representation but also to enhance ontology benchmarks by introducing diverse structural characteristics, enabling more comprehensive evaluations of machine learning tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain-specific Ontologies; Large-language Models; Ontology Evaluation; Ontology Generation; Benchmarking; Computational Linguistics; Domain Knowledge; Knowledge Graph; Learning Systems; Semantics; As-links; Domain-specific Ontologies; Down-stream; Knowledge Graphs; Knowledge-representation; Language Model; Large-language Model; Ontology Evaluations; Ontology Generation; Ontology's; Ontology},
	keywords = {Benchmarking; Computational linguistics; Domain Knowledge; Knowledge graph; Learning systems; Semantics; AS-links; Domain-specific ontologies; Down-stream; Knowledge graphs; Knowledge-representation; Language model; Large-language model; Ontology evaluations; Ontology generation; Ontology's; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Reshef Kera2025,
	author = {Reshef Kera, Denisa Reshef and Dotam, Avital},
	title = {AI Beyond Rules, Heuristics, and Dreams: Ergative-Absolutive Agents for Participatory Simulations},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3977},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008498574&partnerID=40&md5=555e72792c4de53fc33706951f64e8ef},
	abstract = {This paper opens with a critique of the familiar division between rule-based Symbolic AI and data-driven Subsymbolic methods, suggesting that even their neurosymbolic convergence remains shaped by an inherited model of agency in which discrete subjects act upon passive objects, a structure naturalized by Indo-European subject-predicate-object grammars. We propose a linguistic turn in designing AI agents, using typologically diverse alignment systems, particularly ergative-absolutive languages such as Basque, as tools to rethink how agency is modeled and enacted in language-trained systems. We argue that Large Language Models (LLMs), far from being mere predictive tools, function as performative stages where grammars of agency are enacted rather than encoded. This reframing invites a shift: from optimizing systems to express predefined meanings, to interpreting the emergent structures that unfold through interaction. Drawing on the metaphor of the dreaming machine, we treat unpredictability and improvisation not merely as limitations of reasoning, but as openings for enacting alternative ontologies of action. To explore this, we propose a two-step framework. First, we examine how alignment patterns surface in LLM-generated interaction, not as imposed rules, but as constraints enacted by the grammar in context. Second, we stage participatory simulations in which stakeholders co-design agents with contrasting grammatical alignments, testing how such reconfigurations may support more adaptive, negotiated, and accountable forms of agency. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Agency; Ai-human Collaboration; Deliberative Ai; Ergative Ai; Ergative Languages; Language Alignment; Performative Simulations; Artificial Intelligence; Computational Grammars; Computer Simulation Languages; Ontology; Ai Agency; Ai-human Collaboration; Deliberative Ai; Ergative Ai; Ergative Language; Language Alignment; Language Model; Participatory Simulations; Performative Simulation; Rule Based; Alignment},
	keywords = {Artificial intelligence; Computational grammars; Computer simulation languages; Ontology; AI agency; AI-human collaboration; Deliberative AI; Ergative AI; Ergative language; Language alignment; Language model; Participatory simulations; Performative simulation; Rule based; Alignment},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Norouzi2025,
	author = {Norouzi, Ebrahim and Hertling, Sven and Sack, Harald},
	title = {ConExion: Concept Extraction with Large Language Models},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3977},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008495646&partnerID=40&md5=26d9192d1623044601ad692225d1c091},
	abstract = {In this paper, an approach for concept extraction from documents using pre-trained large language models (LLMs) is presented. Compared with conventional methods that extract keyphrases summarizing the important information discussed in a document, our approach tackles a more challenging task of extracting all present concepts related to the specific domain, not just the important ones. Through comprehensive evaluations of two widely used benchmark datasets, we demonstrate that our method improves the F<inf>1</inf> score compared to state-of-the-art techniques. Additionally, we explore the potential of using prompts within these models for unsupervised concept extraction. The extracted concepts are intended to support domain coverage evaluation of ontologies and facilitate ontology learning, highlighting the effectiveness of LLMs in concept extraction tasks. Our source code and datasets are publicly available at https://github.com/ISE-FIZKarlsruhe/concept_extraction. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Extraction; Large Language Models; Present Keyphrase Extraction; Ontology; Benchmark Datasets; Comprehensive Evaluation; Concept Extraction; Conventional Methods; Key-phrase; Key-phrases Extractions; Language Model; Large Language Model; Present Keyphrase Extraction; State-of-the-art Techniques; Extraction},
	keywords = {Ontology; Benchmark datasets; Comprehensive evaluation; Concept extraction; Conventional methods; Key-phrase; Key-phrases extractions; Language model; Large language model; Present keyphrase extraction; State-of-the-art techniques; Extraction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hannah2025,
	author = {Hannah, George and de Berardinis, Jacopo and Payne, Terry R. and Tamma, Valentina A.M. and Mitchell, Andrew and Piercy, Ellen and Johnson, Ewan and Ng, Andrew and Rostron, Harry and Konev, Boris Yu},
	title = {Large Language Models as Knowledge Evaluation Agents},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3977},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008495455&partnerID=40&md5=522713b4c09e34210bf1335f60193335},
	abstract = {With the recent rise of large language models, there has been a growing interest in employing LLMs in different knowledge engineering tasks, such as supporting the semi-automatic creation of KG schemata. This is the case also when leveraging semi-structured data by translating XML schemata into ontologies, where there is a need to inject additional knowledge that makes explicit the relationships between different entities. We investigate the viability of using LLMs as knowledge evaluation agents to assess the suitability of the injected knowledge; by using Gemini as an LLM-based proxy for a human evaluator, and we configure it with different parameter settings and prompt structures. The responses (i.e. the LLM-based assessment of the relationships) are compared against the set of assessments or evaluations carried out by domain experts. We find that despite encountering some issues, the use of LLMs as a proxy expert shows promise in their ability to understand complex domains and evaluate relationships with respect to that domain. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Llm Evaluation; Ontology Engineering; Prompt Engineering; Domain Knowledge; Knowledge Engineering; Knowledge Management; Ontology; Automatic Creations; Engineering Tasks; Knowledge Evaluations; Language Model; Llm Evaluation; Ontology Engineering; Prompt Engineering; Semi-automatics; Semistructured Data; Xml-schema; Agents},
	keywords = {Domain Knowledge; Knowledge engineering; Knowledge management; Ontology; Automatic creations; Engineering tasks; Knowledge evaluations; Language model; LLM evaluation; Ontology engineering; Prompt engineering; Semi-automatics; Semistructured data; XML-Schema; Agents},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lippolis2025,
	author = {Lippolis, Anna Sofia and Saeedizade, Mohammad Javad and Keskisärkkä, Robin and Gangemi, Aldo and Blomqvist, Eva and Nuzzolese, Andrea Giovanni},
	title = {Assessing the Capability of Large Language Models for Domain-Specific Ontology Generation},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3977},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008494228&partnerID=40&md5=fe96df813eee91b10e9e7d0088ce1efc},
	abstract = {Large Language Models (LLMs) have shown significant potential for ontology engineering. However, it is still unclear to what extent they are applicable to the task of domain-specific ontology generation. In this study, we explore the application of LLMs for automated ontology generation and evaluate their performance across different domains. Specifically, we investigate the generalizability of two state-of-the-art LLMsDeepSeek and o1-preview, both equipped with reasoning capabilitiesby generating ontologies from a set of competency questions (CQs) and related user stories. Our experimental setup comprises six distinct domains carried out in existing ontology engineering projects and a total of 95 curated CQs designed to test the models reasoning for ontology engineering. Our findings show that with both LLMs, the performance of the experiments is remarkably consistent across all domains, indicating that these methods are capable of generalizing ontology generation tasks irrespective of the domain. These results highlight the potential of LLM-based approaches in achieving scalable and domain-agnostic ontology construction and lay the groundwork for further research into enhancing automated reasoning and knowledge representation techniques. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Ontology Engineering; Ontology Generation; Domain Knowledge; Knowledge Representation; Different Domains; Domain-specific Ontologies; Language Model; Large Language Model; Ontology Engineering; Ontology Generation; Ontology's; Performance; State Of The Art; Two-state; Ontology},
	keywords = {Domain Knowledge; Knowledge representation; Different domains; Domain-specific ontologies; Language model; Large language model; Ontology engineering; Ontology generation; Ontology's; Performance; State of the art; Two-state; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Zhang2025,
	author = {Zhang, Bohui and Alharbi, Reham and He, Yuan},
	title = {Evaluation of Language Models in Knowledge Engineering (ELMKE) Workshop},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3977},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008493041&partnerID=40&md5=49ed063bd4a9a836001b5270541c0917},
	abstract = {This preface outlines the scope of the Second Workshop on Evaluation of Language Models in Knowledge Engineering (ELMKE 2025) and provides essential details for inclusion in the ESWC 2025 joint proceedings. Held in Portorož, Slovenia, and co-located with ESWC 2025, ELMKE focused on topics such as evaluation methodologies, knowledge and ontology engineering, large language models, benchmark creation, human-centered evaluation, and aspects of trustworthiness, interpretability, and explainability. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology; Co-located; Engineering Evaluations; Engineering Workshops; Evaluation Methodologies; Human-centered Evaluation; Interpretability; Language Model; Ontology Engineering; Slovenia; Knowledge Engineering},
	keywords = {Ontology; Co-located; Engineering evaluations; Engineering workshops; Evaluation methodologies; Human-centered evaluation; Interpretability; Language model; Ontology engineering; Slovenia; Knowledge engineering},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Reder2025,
	author = {Reder, Gabriel K. and Collins, Carl and Rehim, Abbi Abdel and Soldatova, Larisa N. and King, Ross Donald},
	title = {LLM-retrieval based scientific knowledge grounding},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3977},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008492610&partnerID=40&md5=a9ab92868a4aeef03699ba70826cc913},
	abstract = {The automated high-throughput laboratory offers unprecedented potential for scientific discovery, yet effectively linking studies to existing knowledge remains a significant challenge. As the general body of scientific knowledge grows, so too does the burden of contextualizing a new experiment. While ontologies and databases serve as structured common repositories, their rigid schemas are often incompatible with the unstructured or semi-structured formats of most laboratories. In this study we investigate the integration of large language models (LLMs) with ontology-based vector databases to anchor semi-structured scientific experiments into knowledge bases via automated retrieval. Our approach extracts scientific entities from unstructured experimental texts, and grounds them to relevant ontology terms. We automate knowledge grounding, which enhances the integration of unstructured experimental data into established formal scientific languages. We have tested our method on a diverse selection of experimental yeast biology papers focused on Saccharomyces cerevisiae, a foundational model system that has driven major discoveries in molecular and cellular biology, and observed strong pipeline performance. We argue that such a knowledge grounding approach is a critical component for the new wave of efficient artificial intelligence (AI) driven automated laboratories that integrate LLMs with high-throughput experimentation and data-driven discovery. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Information Extraction For Rkgs/skgs; Knowledge Engineering; Large Language Models; Ontologies; Saccharomyces Cerevisiae; Automation; Data Integration; Electric Grounding; Information Retrieval; Knowledge Engineering; Molecular Biology; Throughput; Yeast; Cerevisiae; High-throughput; Information Extraction For Rkg/skg; Language Model; Large Language Model; Ontology's; Saccharomyces; Scientific Discovery; Scientific Knowledge; Semi-structured; Ontology},
	keywords = {Automation; Data integration; Electric grounding; Information retrieval; Knowledge engineering; Molecular biology; Throughput; Yeast; Cerevisiae; High-throughput; Information extraction for RKG/SKG; Language model; Large language model; Ontology's; Saccharomyces; Scientific discovery; Scientific knowledge; Semi-structured; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Rufiner2025,
	author = {Rufiner, Hugo Leonardo and Funes, José Gabriel and Asla, Mariano and Giovanini, Leonardo L. and Majul, Enrique Alberto},
	title = {Non-Human Agents: Exploring Modern AI Through the Lens of Contemporary Ontology and Theology},
	year = {2025},
	journal = {Theology and Science},
	pages = {},
	doi = {10.1080/14746700.2025.2514297},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008438449&doi=10.1080%2F14746700.2025.2514297&partnerID=40&md5=c095f59c17df3ac045d705301f3018ac},
	abstract = {Accelerated Artificial Intelligence (AI) development necessitates re-evaluating intelligence and agency. This paper examines AI's evolution—focusing on large language models (LLMs), agentic AI, and robotics—exploring philosophical and theological implications. Engaging ontological models, from substantialism to relationalism, it interprets AI's nature and impact on human self-understanding. Theologically, it centers on imago Dei, comparing human and artificial intelligence regarding rationality, relationality, and embodiment. It addresses ethical dilemmas including moral agency and responsibility. Integrating philosophical and theological methodologies, the paper aims to understand AI's place, significance, and ethical considerations for responsible development, concluding with emphasis on interdisciplinary dialogue. © 2025 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Alvero2025,
	author = {Alvero, A. J. and Sedlacek, Quentin Charles and León, Maricela and Pena, Courtney},
	title = {Digital accents, homogeneity-by-design, and the evolving social science of written language},
	year = {2025},
	journal = {Annual Review of Applied Linguistics},
	pages = {},
	doi = {10.1017/S0267190525000042},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008384330&doi=10.1017%2FS0267190525000042&partnerID=40&md5=25320ad0f7805de1f789446ffe34b1d9},
	abstract = {Human language is increasingly written rather than just spoken, primarily due to the proliferation of digital technology in modern life. This trend has enabled the creation of generative artificial intelligence (AI) trained on corpora containing trillions of words extracted from text on the internet. However, current language theory inadequately addresses digital text communication’s unique characteristics and constraints. This paper systematically analyzes and synthesizes existing literature to map the theoretical landscape of digitized language. The evidence demonstrates that, parallel to spoken language, features of written communication are frequently correlated with the socially constructed demographic identities of writers, a phenomenon we refer to as “digital accents.” This conceptualization raises complex ontological questions about the nature of digital text and its relationship to social identity. The same line of questioning, in conjunction with recent research, shows how generative AI systematically fails to capture the breadth of expression observed in human writing, an outcome we call “homogeneity-by-design.” By approaching text-based language from this theoretical framework while acknowledging its inherent limitations, social scientists studying language can strengthen their critical analysis of AI systems and contribute meaningful insights to their development and improvement. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Homogenization; Computational Text Analysis; Large Language Models; Sociolinguistics; Sociology},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Pruski2025,
	author = {Pruski, Cédric and Gallais, Marie and Silveira, Marcos Da},
	title = {Enhancing ESCO with Generative AI: A Dynamic Approach to Supporting 21st Century Education},
	year = {2025},
	journal = {IEEE Global Engineering Education Conference, EDUCON},
	pages = {},
	doi = {10.1109/EDUCON62633.2025.11016516},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008223380&doi=10.1109%2FEDUCON62633.2025.11016516&partnerID=40&md5=b97ab1cd1b30a3fd2cf619db06f6e211},
	abstract = {In the rapidly evolving landscape of engineering education, upskilling and lifelong learning have become critical to maintaining competitiveness and fostering innovation. The use of ontologies, such as the European Skills, Competences, Qualifications, and Occupations (ESCO), plays a crucial role in organizing and managing the skills required for modern engineering roles. However, the slow pace of ontology updates and the lack of contextual adaptability present significant challenges, leading to outdated and irrelevant information for educators, learners, and industry professionals. This paper explores the potential of integrating Large Language Models (LLMs) with knowledge engineering to accelerate the process of updating ontologies like ESCO. By dynamically analyzing data and incor-porating contextual information, LLMs offer promising avenues for enhancing the evolution and precision of these ontologies. We discuss the potential impact of this approach in engineering education, particularly in aligning ups killing and reskilling efforts with the demands of emerging technologies such as AI -driven automation and digital engineering. This paper aims to highlight how LLMs can support the creation of more responsive, context-aware learning frameworks, ultimately sustaining educational ex-cellence and fostering critical thinking in engineering education. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Contextualization; Lifelong Learning; Llm; Ontology Evolution; Upskilling; Engineering Education; Knowledge Engineering; Contextualization; Dynamic Approaches; Industry Professionals; Language Model; Large Language Model; Life Long Learning; Modern Engineering; Ontology Evolution; Ontology's; Upskilling; Ontology},
	keywords = {Engineering education; Knowledge engineering; Contextualization; Dynamic approaches; Industry professionals; Language model; Large language model; Life long learning; Modern engineering; Ontology evolution; Ontology's; Upskilling; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wu2025403,
	author = {Wu, Zhenyu and Chen, Jiaoyan and Paton, Norman W.},
	title = {Taxonomy Inference for Tabular Data Using Large Language Models},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15718 LNCS},
	pages = {403 - 422},
	doi = {10.1007/978-3-031-94575-5_22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008012410&doi=10.1007%2F978-3-031-94575-5_22&partnerID=40&md5=59fb679fd3a86e06b779edbbdc66d3bd},
	abstract = {Taxonomy inference for tabular data is a critical task of schema inference, aiming at discovering entity types (i.e., concepts) of the tables and building their hierarchy. It can play an important role in data management, data exploration, ontology learning, and many data-centric applications. Existing schema inference systems focus more on XML, JSON or RDF data, and often rely on lexical formats and structures of the data for calculating similarities, with limited exploitation of the semantics of the text across a table. Motivated by recent works on taxonomy completion and construction using Large Language Models (LLMs), this paper presents two LLM-based methods for taxonomy inference for tables: (i) EmTT which embeds columns by fine-tuning with contrastive learning encoder-alone LLMs like BERT and utilises clustering for hierarchy construction, and (ii) GeTT which generates table entity types and their hierarchy by iterative prompting using a decoder-alone LLM like GPT-4. Extensive evaluation on three real-world datasets with six metrics covering different aspects of the output taxonomies has demonstrated that EmTT and GeTT can both produce taxonomies with strong consistency relative to the Ground Truth. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Contrastive Learning; Large Language Models; Prompt Learning; Schema Inference; Tabular Data; Taxonomy Inference; Computational Linguistics; Data Mining; Information Management; Learning Systems; Ontology; Semantics; Critical Tasks; Data Exploration; Entity-types; Language Model; Large Language Model; Management Data; Prompt Learning; Schema Inference; Tabular Data; Taxonomy Inference; Taxonomies},
	keywords = {Computational linguistics; Data mining; Information management; Learning systems; Ontology; Semantics; Critical tasks; Data exploration; Entity-types; Language model; Large language model; Management data; Prompt learning; Schema inference; Tabular data; Taxonomy inference; Taxonomies},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ourekouch2025441,
	author = {Ourekouch, Mounir and Koulali, Mohammed Amine and Erradi, Mohamed},
	title = {RelCheck: Improving Relation Extraction with Ontology-Guided and LLM-Based Validation},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15718 LNCS},
	pages = {441 - 459},
	doi = {10.1007/978-3-031-94575-5_24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007980362&doi=10.1007%2F978-3-031-94575-5_24&partnerID=40&md5=e240bb1f207e7bb0f1d5f7e949973cc2},
	abstract = {Relation extraction (RE) is a key task in natural language processing (NLP) and a core component of information extraction. It focuses on identifying semantic relations between entities in text. Pretrained language models (PLMs), such as transformer-based models like BERT, XLNet and RoBERTa, have made notable progress in RE. Predictions of relations from these models are provided with varying confidence levels. While high-confidence predictions of relations are generally accurate, low-confidence predictions tend to be less precise and often lead to inaccuracies. The current research question is how to re-evaluate the low confidence predictions to ensure the overall confidence of a PLM. To solve this problem we propose a framework using automatically generated ontology schemas and LLMs. We first propose an algorithm that constructs ontology schemas from the RE datasets (TACRED and ReTACRED). Then we use LLMs to validate these low-confidence predictions through prompting to further improve the precision of final predictions. Experimental results on transformer-based models, GCN and LSTM-based models across two large-scale RE datasets (TACRED and ReTACRED) show significant improvements in precision and overall performance. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Construction; Large Language Models (llms); Ontology Schema; Relation Extraction; Computational Linguistics; Data Mining; Extraction; Knowledge Graph; Large Datasets; Natural Language Processing Systems; Ontology; Prediction Models; Semantics; Confidence Predictions; Graph Construction; Knowledge Graph Construction; Knowledge Graphs; Language Model; Large Language Model; Model-based Validation; Ontology Schema; Ontology's; Relation Extraction; Forecasting},
	keywords = {Computational linguistics; Data mining; Extraction; Knowledge graph; Large datasets; Natural language processing systems; Ontology; Prediction models; Semantics; Confidence predictions; Graph construction; Knowledge graph construction; Knowledge graphs; Language model; Large language model; Model-based validation; Ontology schema; Ontology's; Relation extraction; Forecasting},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {22nd European Semantic Web Conference, ESWC 2025},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15718 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007976068&partnerID=40&md5=6f00ebc82ac43e42cb161098fad3f26d},
	abstract = {The proceedings contain 19 papers. The special focus in this conference is on European Semantic Web. The topics include: OWL<inf>strict</inf>: A Constrained OWL Fragment to Avoid Ambiguities for Knowledge Graph Practitioners; py-amr2fred: A Python Library for Converting Text into OWL-Compliant RDF KGs; LLM-Supported Mapping Generation for Semantic Manufacturing Treasure Hunting; knowledge Graph Construction for Health, Lifestyle and Fitness Applications; Semantic Technologies for Global Governance: A Hybrid AI Approach to Tracking and Monitoring WHO Resolutions; research Knowledge Graphs: The Shifting Paradigm of Scholarly Information Representation; MOOC on Linguistic Linked Data; ontoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment; Interoperable Interpretation and Evaluation of ODRL Policies; the Semantic Web Language Server: Enhancing the Developer Experience for Semantic Web Practitioners; mobilityDCAT-AP: A Metadata Specification for Enhanced Cross-Border Mobility Data Sharing; LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining with Large Language Models; DBpedia-TKG: Capturing Wikipedia’s Evolution as Temporal Knowledge Graphs; LLM-KG-Bench 3.0: A Compass for Semantic Technology Capabilities in the Ocean of LLMs; Incremunica: Web-Based Incremental View Maintenance for SPARQL; showVoc: A Thorough Platform for Publishing and Browsing Linked Open Datasets; Procedural Knowledge Ontology (PKO). © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang2025,
	author = {Zhang, Chi and Yang, Hao and Liu, Xingyun and Wu, Rongrong and Zong, Hui and Wu, Erman and Zhou, Yi and Li, Jiakun and Shen, Bairong},
	title = {A Knowledge-Enhanced Platform (MetaSepsisKnowHub) for Retrieval Augmented Generation–Based Sepsis Heterogeneity and Personalized Management: Development Study},
	year = {2025},
	journal = {Journal of Medical Internet Research},
	volume = {27},
	pages = {},
	doi = {10.2196/67201},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007975026&doi=10.2196%2F67201&partnerID=40&md5=44320dce98fcccb0b0ae9009e802c2dd},
	abstract = {Background: Sepsis is a severe syndrome of organ dysfunction caused by infection; it has high heterogeneity and high in-hospital mortality, representing a grim clinical challenge for precision medicine in critical care. Objective: We aimed to extract reported sepsis biomarkers to provide users with comprehensive biomedical information and integrate retrieval augmented generation (RAG) and prompt engineering to enhance the accuracy, stability, and interpretability of clinical decisions recommended by large language models (LLMs). Methods: To address the challenge, we established and updated the first knowledge-enhanced platform, MetaSepsisKnowHub, comprising 427 sepsis biomarkers and 423 studies, aiming to systematically collect and annotate sepsis biomarkers to guide personalized clinical decision-making in the diagnosis and treatment of human sepsis. We curated a tailored LLM framework incorporating RAG and prompt engineering and incorporated 2 performance evaluation scales: the System Usability Scale and the Net Promoter Score. Results: The overall quantitative ratings of expert-reviewed clinical recommendations based on RAG surpassed baseline responses generated by 4 LLMs and showed a statistically significant improvement in textual questions (GPT-4: mean 75.79, SD 7.11 vs mean 81.59, SD 9.87; P=.02; GPT-4o: mean 70.36, SD 7.63 vs mean 77.98, SD 13.26; P=.02; Qwen2.5-instruct: mean 77.08 SD 3.75 vs mean 85.46, SD 7.27; P<.001; and DeepSeek-R1: mean 77.67, SD 3.66 vs mean 86.42, SD 8.56; P<.001), but no significant statistical differences could be measured in clinical scenarios. The RAG assessment score comparing RAG-based responses and expert-provided benchmark answers illustrated prominent factual correctness, accuracy, and knowledge recall compared to the baseline responses. After use, the average the System Usability Scale score was 82.20 (SD 14.17) and the Net Promoter Score was 72, demonstrating high user satisfaction and loyalty. Conclusions: We highlight the pioneering MetaSepsisKnowHub platform, and we show that combining MetaSepsisKnowHub with RAG can minimize limitations on precision and maximize the breadth of LLMs to shorten the bench-to-bedside distance, serving as a knowledge-enhanced paradigm for future application of artificial intelligence in critical care medicine. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Human Sepsis; Knowledge-enhanced; Personalized Application; Precision Medicine; Retrieval Augmented Generation; Adrenomedullin; C Reactive Protein; Lactic Acid; Procalcitonin; Biomarkers; Metasepsisknowhub; Adrenomedullin; Amino Terminal Pro Brain Natriuretic Peptide; Antiinfective Agent; C Reactive Protein; Calgranulin A; Cd14 Antigen; Interleukin 10; Interleukin 6; Lactic Acid; Long Untranslated Rna; New Drug; Procalcitonin; Biological Marker; Adult; Article; Artificial Intelligence-assisted Diagnosis; Bioinformatics; Clinical Article; Clinical Decision Making; Clinical Practice; Context Conflict Hallucination; Controlled Study; Correlation Coefficient; Data Interpretation; Data Mining; Data Quality; Diagnostic Accuracy; Disease Severity; Fact Conflict Hallucination; Few Shot Learning; Gene Interaction; Gene Ontology; Gene Set Enrichment Analysis; Generative Artificial Intelligence; Human; Human Tissue; Information Processing; Information Retrieval; Input Conflict Hallucination; Kegg; Knowledge Enhanced Platform; Knowledge Management; Large Language Model; Medical Informatics; Medical Terminology; Medline; Net Promoter Score; Newborn; Personalized Medicine; Prompt Engineering; Protein Protein Interaction; Retrieval Augmented Generation; Scoring System; Semi Structured Interview; Sensitivity And Specificity; Sepsis; Sequential Organ Failure Assessment Score; Statistically Significant Result; System Usability Scale Score; Systems Biology; Web Of Science; Young Adult; Diagnosis; Therapy; Biomarkers; Humans; Precision Medicine; Sepsis},
	keywords = {adrenomedullin; amino terminal pro brain natriuretic peptide; antiinfective agent; C reactive protein; calgranulin A; CD14 antigen; interleukin 10; interleukin 6; lactic acid; long untranslated RNA; new drug; procalcitonin; biological marker; adult; Article; artificial intelligence-assisted diagnosis; bioinformatics; clinical article; clinical decision making; clinical practice; context conflict hallucination; controlled study; correlation coefficient; data interpretation; data mining; data quality; diagnostic accuracy; disease severity; fact conflict hallucination; few shot learning; gene interaction; gene ontology; gene set enrichment analysis; generative artificial intelligence; human; human tissue; information processing; information retrieval; input conflict hallucination; KEGG; knowledge enhanced platform; knowledge management; large language model; medical informatics; medical terminology; Medline; Net Promoter Score; newborn; personalized medicine; prompt engineering; protein protein interaction; retrieval augmented generation; scoring system; semi structured interview; sensitivity and specificity; sepsis; Sequential Organ Failure Assessment Score; statistically significant result; System Usability Scale score; systems biology; Web of Science; young adult; diagnosis; therapy; Biomarkers; Humans; Precision Medicine; Sepsis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Giglou2025174,
	author = {Giglou, Hamed Babaei and D’Souza, Jennifer and Karras, Oliver and Auer, Sören},
	title = {OntoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15719 LNCS},
	pages = {174 - 191},
	doi = {10.1007/978-3-031-94578-6_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007815364&doi=10.1007%2F978-3-031-94578-6_10&partnerID=40&md5=6e69d2be465862dc81511789569ffa62},
	abstract = {Ontology Alignment (OA) is fundamental for achieving semantic interoperability across diverse knowledge systems. We present OntoAligner, a comprehensive, modular, and robust Python toolkit for ontology alignment, designed to address current limitations with existing tools faced by practitioners. Existing tools are limited in scalability, modularity, and ease of integration with recent AI advances. OntoAligner provides a flexible architecture integrating existing lightweight OA techniques such as fuzzy matching but goes beyond by supporting contemporary methods with retrieval-augmented generation and large language models for OA. The framework prioritizes extensibility, enabling researchers to integrate custom alignment algorithms and datasets. This paper details the design principles, architecture, and implementation of the OntoAligner, demonstrating its utility through benchmarks on standard OA tasks. Our evaluation highlights OntoAligner’s ability to handle large-scale ontologies efficiently with few lines of code while delivering high alignment quality. By making OntoAligner open-source, we aim to provide a resource that fosters innovation and collaboration within the OA community, empowering researchers and practitioners with a toolkit for reproducible OA research and real-world applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Ontology Alignment; Ontology Matching; Python Library; Retrieval Augmented Generation; Python; Current Limitation; Knowledge System; Language Model; Large Language Model; Modulars; Ontology Alignment; Ontology Matching; Python Library; Retrieval Augmented Generation; Semantic Interoperability; Interoperability},
	keywords = {Python; Current limitation; Knowledge system; Language model; Large language model; Modulars; Ontology alignment; Ontology matching; Python library; Retrieval augmented generation; Semantic interoperability; Interoperability},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {22nd European Semantic Web Conference, ESWC 2025},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15719 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007795726&partnerID=40&md5=ed7ce5d8a89f4e2602298f85281b11cc},
	abstract = {The proceedings contain 19 papers. The special focus in this conference is on European Semantic Web. The topics include: OWL<inf>strict</inf>: A Constrained OWL Fragment to Avoid Ambiguities for Knowledge Graph Practitioners; py-amr2fred: A Python Library for Converting Text into OWL-Compliant RDF KGs; LLM-Supported Mapping Generation for Semantic Manufacturing Treasure Hunting; knowledge Graph Construction for Health, Lifestyle and Fitness Applications; Semantic Technologies for Global Governance: A Hybrid AI Approach to Tracking and Monitoring WHO Resolutions; research Knowledge Graphs: The Shifting Paradigm of Scholarly Information Representation; MOOC on Linguistic Linked Data; ontoAligner: A Comprehensive Modular and Robust Python Toolkit for Ontology Alignment; Interoperable Interpretation and Evaluation of ODRL Policies; the Semantic Web Language Server: Enhancing the Developer Experience for Semantic Web Practitioners; mobilityDCAT-AP: A Metadata Specification for Enhanced Cross-Border Mobility Data Sharing; LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining with Large Language Models; DBpedia-TKG: Capturing Wikipedia’s Evolution as Temporal Knowledge Graphs; LLM-KG-Bench 3.0: A Compass for Semantic Technology Capabilities in the Ocean of LLMs; Incremunica: Web-Based Incremental View Maintenance for SPARQL; showVoc: A Thorough Platform for Publishing and Browsing Linked Open Datasets; Procedural Knowledge Ontology (PKO). © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Sadruddin2025244,
	author = {Sadruddin, Sameer and D’Souza, Jennifer and Poupaki, Eleni and Watkins, Alex and Giglou, Hamed Babaei and Rula, Anisa and Karasulu, Bora and Auer, Sören and Mackus, Adrie and Kessels, Erwin},
	title = {LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining with Large Language Models},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15719 LNCS},
	pages = {244 - 261},
	doi = {10.1007/978-3-031-94578-6_14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007760723&doi=10.1007%2F978-3-031-94578-6_14&partnerID=40&md5=09f8a3feeee6d10d3656ede32cca3b1e},
	abstract = {Extracting structured information from unstructured text is crucial for modeling real-world processes, but traditional schema mining relies on semi-structured data, limiting scalability. This paper introduces schema-miner, a novel tool that combines large language models with human feedback to automate and refine schema extraction. Through an iterative workflow, it organizes properties from text, incorporates expert input, and integrates domain-specific ontologies for semantic depth. Applied to materials science—specifically atomic layer deposition—schema-miner demonstrates that expert-guided LLMs generate semantically rich schemas suitable for diverse real-world applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Human-in-the-loop Workflow; Large Language Models; Schema Discovery; Schema Mining; Scientific Schemas; Multilayers; Human-in-the-loop; Human-in-the-loop Workflow; Language Model; Large Language Model; Schema Discovery; Schema Mining; Scientific Schema; Structured Information; Unstructured Texts; Work-flows; Semantics},
	keywords = {Multilayers; Human-in-the-loop; Human-in-the-loop workflow; Language model; Large language model; Schema discovery; Schema mining; Scientific schema; Structured information; Unstructured texts; Work-flows; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Maratsi20253,
	author = {Maratsi, Maria Ioanna and Haskiya, David and Berggren, Mats and Charalabidis, Yannis K. and Alexopoulos, Charalampos},
	title = {Towards Open Archival Linked Data (ALD): The Case of Swedish National Archives},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15719 LNCS},
	pages = {3 - 23},
	doi = {10.1007/978-3-031-94578-6_1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007724368&doi=10.1007%2F978-3-031-94578-6_1&partnerID=40&md5=4f2a5241d0354ee690903514385c7d44},
	abstract = {The advantages of aligning custom data schemas with standardised ontologies within their respective knowledge domain have long since been proven in practice. Sharing a common structural representation by mapping concepts and relationships between the schemas is essential to ensure data interoperability (especially on a semantic level), integration, reuse, and the ability to leverage machine-processable and advanced-search capabilities. Archival institutions preserve, manage, and provide access to large amounts of diverse cultural and historical data, demonstrating a high potential to be active contributors to a global knowledge network, should archival data be transformed and offered as linked (open) data. The present study aims to improve the alignment of the Swedish National Archives schema to a core ontology for archival resources description (Records-in-Context Ontology - RiC-O) by producing conceptual mappings between them and reusing as many elements and relations as possible from Records-in-Context. The produced mappings were evaluated ex ante by domain experts and integrated in the Swedish National Archives schema as new RDF representations. Apart from the purpose of enabling the process of transforming archival data into linked data, the conducted research aimed to contribute expert-curated data for future experimentation on introducing automation (e.g., use of LLMs) in the semantic alignment and mapping process. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Archival Data; Archives; Cultural Heritage; Digital Archives; Digital Humanities; Linked Data; Linked Open Data; Ontology Alignment; Ontology Mapping; Reusability; Semantic Interoperability; Computer Software Reusability; Domain Knowledge; Mapping; Archival Data; Archive; Cultural Heritages; Digital Archives; Digital Humanities; Linked Open Data; Ontology Alignment; Ontology Mapping; Semantic Interoperability; Swedishs; Reusability},
	keywords = {Computer software reusability; Domain Knowledge; Mapping; Archival data; Archive; Cultural heritages; Digital archives; Digital humanities; Linked open data; Ontology alignment; Ontology mapping; Semantic interoperability; Swedishs; Reusability},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Schmidt202584,
	author = {Schmidt, Wilma Johanna and Grangel-González, Irlán and Huschle, Tobias and Wagner, Lena and Kharlamov, Evgeny and Paschke, Adrian},
	title = {LLM-Supported Mapping Generation for Semantic Manufacturing Treasure Hunting},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15719 LNCS},
	pages = {84 - 101},
	doi = {10.1007/978-3-031-94578-6_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007703821&doi=10.1007%2F978-3-031-94578-6_5&partnerID=40&md5=017a5496791560bf5fd315ec9e4c1183},
	abstract = {In large manufacturing companies, such as Bosch, that operate thousands of production lines with each comprising up to dozens of production machines and other equipment, even simple inventory questions such as of location and quantities of a particular equipment type require non-trivial solutions. Addressing these questions requires to integrate multiple heterogeneous data sets which is time consuming and error prone and demands domain as well as knowledge experts. Knowledge graphs (KGs) are practical for consolidating inventory data by bringing it into the same format and linking inventory items. However, the KG creation and maintenance itself pose challenges as mappings are needed to connect data sets and ontologies. In this work, we address these challenges by exploring LLM-supported and context-enhanced YARRRML mapping generation. Facing large ontologies in the manufacturing domain and token limitations in LLM prompts, we further evaluate ontology reduction methods in our approach. Our work provides a valuable support when creating YARRRML manufacturing mappings as well as supporting data and schema updates. We evaluate our approach both quantitatively against reference mappings created manually by experts and qualitatively with expert feedback. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Construction; Llm; Manufacturing; Mapping Generation; Ontology Reduction; Yarrrml; Layered Manufacturing; Metal Forming; Papermaking; Photomapping; Refining; % Reductions; Data Set; Graph Construction; Knowledge Graph Construction; Knowledge Graphs; Llm; Mapping Generations; Ontology Reduction; Ontology's; Yarrrml; Fabrication},
	keywords = {Layered manufacturing; Metal forming; Papermaking; Photomapping; Refining; % reductions; Data set; Graph construction; Knowledge graph construction; Knowledge graphs; LLM; Mapping generations; Ontology reduction; Ontology's; YARRRML; Fabrication},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Jee2025,
	author = {Jee, Taekwon},
	title = {LLM-Based Overlay Issue Classification and Solution Optimization in Semiconductor Manufacturing},
	year = {2025},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {13425},
	pages = {},
	doi = {10.1117/12.3050976},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007599184&doi=10.1117%2F12.3050976&partnerID=40&md5=778c0bcb20e87274adfd235bcbe361db},
	abstract = {As Moore's Law decelerates and talent shortages threaten semiconductor innovation, the industry faces dual crises of technical scalability and institutional knowledge erosion. This paper presents a centralized AI framework integrating large language models (LLMs) and ontology-based machine learning to address yield optimization, process control, and knowledge fragmentation in high-volume manufacturing (HVM). By synthesizing virtual fabrication datasets (160M wafers) and deploying domain-specific AI agents (PRISM and INFER), our system achieves 0.2nm overlay prediction accuracy (R<sup>2=0.98)</sup> and reduces troubleshooting time from weeks to minutes. We demonstrate how semantic unification of structured/unstructured data enables dynamic adaptation to process variations while preserving IP security. Case studies in lithography overlay control highlight 95% diagnostic alignment with human experts, with ethical safeguards ensuring human oversight. This work provides a roadmap for democratizing semiconductor expertise through AI-augmented workflows while proposing open standards for industry-wide collaboration. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence (ai); Issue Pattern Classification; Large Language Model (llm); Overlay; Process Knowledge Base System; Knowledge Acquisition; Knowledge Representation; Ontology; Process Control; Artificial Intelligence; Issue Pattern Classification; Knowledge Base System; Language Model; Large Language Model; Model-based Opc; Overlay; Patterns Classification; Process Knowledge Base; Process Knowledge Base System; Inference Engines},
	keywords = {Knowledge acquisition; Knowledge representation; Ontology; Process control; Artificial intelligence; Issue pattern classification; Knowledge base system; Language model; Large language model; Model-based OPC; Overlay; Patterns classification; Process knowledge base; Process knowledge base system; Inference engines},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhong2025,
	author = {Zhong, Wei and Sun, Mingyue and Yao, Shun and Liu, Yifan and Peng, Dingchuan and Liu, Yan and Yang, Kai and Gao, Huimin and Yan, Huihui and Hao, Wenjing},
	title = {Enhancing the Accuracy of Human Phenotype Ontology Identification: Comparative Evaluation of Multimodal Large Language Models},
	year = {2025},
	journal = {Journal of Medical Internet Research},
	volume = {27},
	pages = {},
	doi = {10.2196/73233},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007323200&doi=10.2196%2F73233&partnerID=40&md5=b095f4f8bdb090c6708e14d1271e22d5},
	abstract = {Background: Identifying Human Phenotype Ontology (HPO) terms is crucial for diagnosing and managing rare diseases. However, clinicians, especially junior physicians, often face challenges due to the complexity of describing patient phenotypes accurately. Traditional manual search methods using HPO databases are time-consuming and prone to errors. Objective: The aim of the study is to investigate whether the use of multimodal large language models (MLLMs) can improve the accuracy of junior physicians in identifying HPO terms from patient images related to rare diseases. Methods: In total, 20 junior physicians from 10 specialties participated. Each physician evaluated 27 patient images sourced from publicly available literature, with phenotypes relevant to rare diseases listed in the Chinese Rare Disease Catalogue. The study was divided into 2 groups: the manual search group relied on the Chinese Human Phenotype Ontology website, while the MLLM-assisted group used an electronic questionnaire that included HPO terms preidentified by ChatGPT-4o as prompts, followed by a search using the Chinese Human Phenotype Ontology. The primary outcome was the accuracy of HPO identification, defined as the proportion of correctly identified HPO terms compared to a standard set determined by an expert panel. Additionally, the accuracy of outputs from ChatGPT-4o and 2 open-source MLLMs (Llama3.2:11b and Llama3.2:90b) was evaluated using the same criteria, with hallucinations for each model documented separately. Furthermore, participating physicians completed an additional electronic questionnaire regarding their rare disease background to identify factors affecting their ability to accurately describe patient images using standardized HPO terms. Results: A total of 270 descriptions were evaluated per group. The MLLM-assisted group achieved a significantly higher accuracy rate of 67.4% (182/270) compared to 20.4% (55/270) in the manual group (relative risk 3.31, 95% CI 2.58‐4.25; P<.001). The MLLM-assisted group demonstrated consistent performance across departments, whereas the manual group exhibited greater variability. Among standalone MLLMs, ChatGPT-4o achieved an accuracy of 48% (13/27), while the open-source models Llama3.2:11b and Llama3.2:90b achieved 15% (4/27) and 18% (5/27), respectively. However, MLLMs exhibited a high hallucination rate, frequently generating HPO terms with incorrect IDs or entirely fabricated content. Specifically, ChatGPT-4o, Llama3.2:11b, and Llama3.2:90b generated incorrect IDs in 57.3% (67/117), 98% (62/63), and 82% (46/56) of cases, respectively, and fabricated terms in 34.2% (40/117), 41% (26/63), and 32% (18/56) of cases, respectively. Additionally, a survey on the rare disease knowledge of junior physicians suggests that participation in rare disease and genetic disease training may enhance the performance of some physicians. Conclusions: The integration of MLLMs into clinical workflows significantly enhances the accuracy of HPO identification by junior physicians, offering promising potential to improve the diagnosis of rare diseases and standardize phenotype descriptions in medical research. However, the notable hallucination rate observed in MLLMs underscores the necessity for further refinement and rigorous validation before widespread adoption in clinical practice. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Human Phenotype Ontology; Large Language Model; Multimodal Large Language Models; Open-source Llms; Rare Diseases; Achondroplasia; Acromegaly; Adult Onset Still Disease; Alagille Syndrome; Albinism; Albright Syndrome; Alport Syndrome; Amyotrophic Lateral Sclerosis; Angioneurotic Edema; Article; Bardet Biedl Syndrome; Biological Ontology; Blue Rubber Bleb Nevus; Chatgpt; Chinese; Clinical Article; Clinical Practice; Congenital Malformation; Controlled Study; Cutaneous T Cell Lymphoma; Fabry Disease; Fibrodysplasia Ossificans Progressiva; Gaucher Disease; Generalized Pustular Psoriasis; Genetic Disorder; Hallucination; Happy Puppet Syndrome; Human; Jeune Syndrome; Large Language Model; Lennox Gastaut Syndrome; Marfan Syndrome; Myasthenia Gravis; Noonan Syndrome; Peutz Jeghers Syndrome; Phenotype; Physician; Poems Syndrome; Progeria; Randomized Controlled Trial; Rare Disease; Risk Factor; Scoliosis; Suppurative Hidradenitis; Comparative Study; Language; Biological Ontologies; Humans; Language; Large Language Models; Phenotype},
	keywords = {achondroplasia; acromegaly; adult onset Still disease; Alagille syndrome; albinism; Albright syndrome; Alport syndrome; amyotrophic lateral sclerosis; angioneurotic edema; Article; Bardet Biedl syndrome; biological ontology; blue rubber bleb nevus; ChatGPT; Chinese; clinical article; clinical practice; congenital malformation; controlled study; cutaneous T cell lymphoma; Fabry disease; fibrodysplasia ossificans progressiva; Gaucher disease; generalized pustular psoriasis; genetic disorder; hallucination; happy puppet syndrome; human; jeune syndrome; large language model; Lennox Gastaut syndrome; Marfan syndrome; myasthenia gravis; Noonan syndrome; Peutz Jeghers syndrome; phenotype; physician; POEMS syndrome; progeria; randomized controlled trial; rare disease; risk factor; scoliosis; suppurative hidradenitis; comparative study; language; Biological Ontologies; Humans; Language; Large Language Models; Phenotype},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{O'Neill2025,
	author = {O'Neill, Brenda and Stapleton, Larry and Carew, Peter J.},
	title = {Human centered systems start with social dynamics and arrive at ontology},
	year = {2025},
	journal = {AI and Society},
	pages = {},
	doi = {10.1007/s00146-025-02396-6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007238787&doi=10.1007%2Fs00146-025-02396-6&partnerID=40&md5=36764e53deec528e3648d13d48db20ff},
	abstract = {The purpose of this research is to support and nurture tacit knowledge whilst simultaneously leading to the development of machine-based intelligent systems which incorporate machine readable knowledge for the benefit of society. This paper starts with an introduction to the persistent power struggle between human and technology and shines a light on Professor Michael Cooley’s involvement with the Lucas Plan in the 1970s and his PhD work which focused on the transition from manual draftsmanship to Computer Aided Design in engineering. A research lab is identified as a ‘complex adaptive system’ and forms the basis of a longitudinal case study on the Human Centered bottom-up approach to digitisation of cultural heritage. Components required to support and nurture the growth of a Participation Action Research lab are identified. The novel ‘ENRICHER’ method embodies human centeredness and is operationalized, tested, evaluated and findings discussed. Examples of emergence are also discussed. A metric of the ENRICHER method initially identified where the lab did not fully meet all the methods 8 points. Subsequent actions adjusted the holonic lens focus to metadata and the ongoing work on the creation of a cataloging tool for the librarians. The use of XML technologies integrates the work into a larger model of intelligence. It positions the work on the semantic web technology stack and opens up the pathway to ontology generation and development and management of large language models. The ENRICHER method is a way of developing human–machine symbiotics that also incorporate AI e.g. transcription, metadata generation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai; Human Centered; Tacit Knowledge; Xml; Complex Adaptive Systems; Computer-aided Design; Human Centered; Human Centered Systems; Longitudinal Case Study; Ontology's; Power; Research Labs; Social Dynamics; Tacit Knowledge; Computer Aided Logic Design},
	keywords = {Complex adaptive systems; Computer-aided design; Human centered; Human centered systems; Longitudinal case study; Ontology's; Power; Research labs; Social dynamics; Tacit knowledge; Computer aided logic design},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Neubig202551,
	author = {Neubig, Stefan and Radhakrishnan, Rahul and Göhl, Linus and Loges, Ronja and Polgar, Madalina and Hein, Andreas and Krcmar, Helmut A.O.},
	title = {Boosting the Querying Accuracy of Multi-Level Occupancy Data with Ontology-Guided LLMs},
	year = {2025},
	journal = {Springer Proceedings in Business and Economics},
	pages = {51 - 63},
	doi = {10.1007/978-3-031-83705-0_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007233613&doi=10.1007%2F978-3-031-83705-0_5&partnerID=40&md5=9c5cb03c2814f16ddd8bb9f3d91fdc0a},
	abstract = {As overtourism and local overcrowding are becoming increasingly critical concerns, determining and predicting occupancy levels based on real-time data and predictive models that serve as a decision-making basis for necessary countermeasures are gaining popularity. Moreover, with the rise of large language models (LLMs), approaches that automate related data access have become tempting. However, real-world databases are often inherently complex and heterogeneously structured, complicating using LLM-based text-to-SQL. Previous studies report an accuracy of only 16%, which indicates the need for better approaches. This paper investigates how ontologies can support LLMs in increasing the accuracy of querying real-world databases. Based on the need to reduce overcrowding, we propose an ontology for modeling complex, multi-level occupancy data. Our ontology, based on previous work, is theoretically well-founded and compatible with existing tourism ontologies. In a case study based on a real-world database from Outdooractive, one of the largest European outdoor tourism platforms, we compare vanilla LLM-based text-to-SQL's performance with ontology-based data access. Our results show that the ontology-based approach almost triples the querying accuracy, which illustrates the effectiveness and potential of such semantic approaches. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graphs; Large Language Models; Occupancy Prediction; Ontologies; Smart Tourism; Visitor Management},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2025,
	title = {32nd ENTER International eTourism Conference, ENTER 2025},
	year = {2025},
	journal = {Springer Proceedings in Business and Economics},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007225114&partnerID=40&md5=e90d04492f109e6da3d1ccf33642c010},
	abstract = {The proceedings contain 38 papers. The special focus in this conference is on eTourism. The topics include: A Systematic Approach for Investigating the Technology Chasm in Tourism and Travel; A Generative AI-Driven Tourism Information Dissemination Support System with Direct Posting to SNS; data-Driven Practices in European Smart Tourism Destinations: Towards a Tourism Data Space; Boosting the Querying Accuracy of Multi-Level Occupancy Data with Ontology-Guided LLMs; user Roles in Blockchain-Based Tokenization Travel Platforms; a Carrying Capacity Calculator for Pedestrians Using OpenStreetMap Data: Application to Urban Tourism and Public Spaces; capturing Physiological and Self-Reported Response to Destination Promotion: A Neuromarketing Approach; an Ontology for the “System Tourism”—A Call for Participation; development and Validation of a Tool for Discovering Potential Value in Tourism Resources; Generative AI for Sentiment Analysis; high-End Hotel Location Evaluation and Prediction in Nanjing City: A Data-Driven Approach Using Multi-Source Spatial Data and Machine Learning; analysis of Sentiments in Airbnb Experiences; Exploring VR Conference Attendees’ Perceptions and Experiences: Gender Differences and Barriers to Adoption; Gamification Is SH*T: Or Is It?; the Ecosystem of Fun for Hotel Mobile App Users; detecting Emotions in User Generated Content and Their Influence on Tourist Satisfaction; sephora Tweens: How Mobile Technology and Social Media Turn Brands into Travel Destinations; the Language Barrier in the Republic of South Africa: Is Google Translate a Help or Hinder?; digital Tourism Marketing as a Potential Catalyst for Transformative Experiences; the Decoy Effect in Recommender Systems: Leveraging Digital Nudging for Sustainable Tourism; making It Personal: How Place Attachment in the Metaverse Can Influence Sustainable Behavioral Intentions. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Segall2025107,
	author = {Segall, Matthew David},
	title = {The Philosophical Implications of Artificial Intelligence: How Can the Implementation of AI Technologies Contribute to and Not Degrade Human Flourishing?},
	year = {2025},
	journal = {Integrated Science},
	volume = {35},
	pages = {107 - 114},
	doi = {10.1007/978-3-031-87023-1_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006946649&doi=10.1007%2F978-3-031-87023-1_8&partnerID=40&md5=d6e81865b923847ed64e7c0e446f4bea},
	abstract = {Following the famous 1956 Dartmouth Conference, where a team of computer scientists first coined the term “artificial intelligence,” philosophers have raised significant ontological and ethical questions regarding AI’s nature and its implications for human flourishing. More recently, the growing popularity of Large Language Models and so-called “deep fakes” has made the public more aware of the power of AI, thrusting profound existential dilemmas into our collective consciousness. This chapter attempts to address our current situation from a philosophical perspective by reflecting on basic questions, including: What are these technologies for? Whose interests do they serve? And perhaps most importantly, how are they changing our sense of conscious human agency? The aim is to better contextualize the transformation already underway in the hopes of avoiding false myths and unethical outcomes. © 2025 Elsevier B.V., All rights reserved.},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kanade2025303,
	author = {Kanade, Aditya Sanjiv and Patwardhan, Manasi S. and Patidar, Mayur and Vig, Lovekesh and Vasudevan, Bagyalakshmi},
	title = {Fashion Attribute Extraction Under an Evolving Ontology},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15623 LNCS},
	pages = {303 - 319},
	doi = {10.1007/978-3-031-91569-7_19},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006879779&doi=10.1007%2F978-3-031-91569-7_19&partnerID=40&md5=29fe8a08039dadc15951cc3e98185187},
	abstract = {Fashion trends evolve rapidly, and industries operating within the fashion domain need to constantly update the underlying models to absorb newer trends. The current approach for model updation involves time-consuming and costly retraining procedures due to the incrementally growing dataset size. Thus, continual learning approaches that incrementally accommodate changes in data distributions without exhaustive retraining hold promise for efficient model updation in the fashion domain. In this paper, we study incremental learning for fashion attribute extraction (FAE), an essential task with many downstream applications. Previous studies on FAE have been on purely static settings, and no fashion-domain datasets exist to study the incremental FAE task. In order to address this, we propose an algorithm to transform a static dataset into an evolving dataset and apply the proposed method to existing static FAE datasets. We observe that both generative and discriminative Visual Language Models (VLMs) have dominated static FAE benchmarks. Hence, we choose to extend continual learning frameworks proposed for VLMs to create three discriminative and two generative baselines for the incremental FAE task. We show that, under an evolving ontology, the generative baselines exhibit 50% lower catastrophic forgetting (CF) compared to the discriminative setting for the incremental DeepFashion dataset with a complex ontology. In comparison, we see equivalent performance on the incremental FashionAI dataset with a relatively simple ontology. Finally, we compare our generative VLM baselines with zero and few-shot Gemini baselines on the incremental FAE task, demonstrating that the marginal performance gain does not justify its significantly higher computational cost and inference time (approximately 140x more compute and 150x slower inference), encouraging further research into efficient specialist models for incremental FAE. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Catastrophic Forgetting; Continual Learning; Fashion Attribute Extraction; Generative Vlms; 'current; Attributes Extractions; Catastrophic Forgetting; Continual Learning; Data Set Size; Fashion Attribute Extraction; Generative Visual Language Model; Learning Approach; Ontology's; Visual Language Model},
	keywords = {'current; Attributes extractions; Catastrophic forgetting; Continual learning; Data set size; Fashion attribute extraction; Generative visual language model; Learning approach; Ontology's; Visual language model},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wilson2025,
	author = {Wilson, Joseph B.},
	title = {‘Python is ready to have a conversation with you’: the reproduction of an ontology of language in natural language processing educational material},
	year = {2025},
	journal = {AI and Society},
	pages = {},
	doi = {10.1007/s00146-025-02391-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006805154&doi=10.1007%2Fs00146-025-02391-x&partnerID=40&md5=39fe191d9b04638171e900d5ab974465},
	abstract = {What language is, as a phenomenon, is a common topic of metalinguistic discourse. The current interest in Large Language Models (LLMs) provides an opportunity to examine how this discourse differs across fields of study. I examine the descriptions of language in online educational materials and in a course designed to teach Python and its use in Natural Language Processing (NLP). Instructors and textbooks explicitly introduce language as a semantic, referential, and denotationally transparent system of logical units encoded in text. In practice, instructors use a wide variety of nonreferential indexical communicative strategies such as hedging, voicing, and gesture, as well as semantically ambiguous linguistic techniques like anthropomorphism, metonym, and metaphor, that are not included in the educators’ definition of language. Instead of treating this discourse as evidence of a particular language ideology, I argue it makes sense to speak of domain-specific language ontologies. Using an ontological frame explains why concepts of language across neighbouring disciplines such as computational linguistics, sociolinguistics, and linguistic anthropology often feel incommensurable. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computer Code; Language Ontology; Large Language Model; Natural Language Processing; Python; Context Free Grammars; Context Sensitive Grammars; Ontology; Python; Semantics; Teaching; 'current; Computer Codes; Educational Materials; Language Model; Language Ontology; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Ontology's; Natural Language Processing Systems},
	keywords = {Context free grammars; Context sensitive grammars; Ontology; Python; Semantics; Teaching; 'current; Computer codes; Educational materials; Language model; Language ontology; Language processing; Large language model; Natural language processing; Natural languages; Ontology's; Natural language processing systems},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Riquelme-García20252155,
	author = {Riquelme-García, Andrea and Mulero-Hernández, Juan and Fernández-Breis, Jesualdo Tomás},
	title = {Annotation of biological samples data to standard ontologies with support from large language models},
	year = {2025},
	journal = {Computational and Structural Biotechnology Journal},
	volume = {27},
	pages = {2155 - 2167},
	doi = {10.1016/j.csbj.2025.05.020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006647768&doi=10.1016%2Fj.csbj.2025.05.020&partnerID=40&md5=2ba9be7fbc19d1efa522cbb192a639e1},
	abstract = {The semantic integration of biological data is hindered by the vast heterogeneity of data sources and their limited semantic formalization. A crucial step in this process is mapping data elements to ontological concepts, which typically involves substantial manual effort. Large Language Models (LLMs) have demonstrated potential in automating complex language-related tasks and may offer a solution to streamline biological data annotation. This study investigates the utility of LLMs—specifically various base and fine-tuned GPT models—for the automatic assignment of ontological identifiers to biological sample labels. We evaluated model performance in annotating labels to four widely used ontologies: the Cell Line Ontology (CLO), Cell Ontology (CL), Uber-anatomy Ontology (UBERON), and BRENDA Tissue Ontology (BTO). Our dataset was compiled from publicly available, high-quality databases containing biologically relevant sequence information, which suffers from inconsistent annotation practices, complicating integrative analyses. Model outputs were compared against annotations generated by text2term, a state-of-the-art annotation tool. The fine-tuned GPT model outperformed both the base models and text2term in annotating cell lines and cell types, particularly for the CL and UBERON ontologies, achieving a precision of 47–64% and a recall of 88–97%. In contrast, base models exhibited significantly lower performance. These results suggest that fine-tuned LLMs can accelerate and improve the accuracy of biological data annotation. Nonetheless, our evaluation highlights persistent challenges, including variable precision across ontology categories and the continued need for expert curation to ensure annotation validity. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bioinformatics; Biological Samples; Data Interoperability; Generative Ai; Large Language Models; Formal Concept Analysis; Ontology; Base Models; Biological Data; Biological Samples; Cell Lines; Data Annotation; Data Interoperability; Generative Ai; Language Model; Large Language Model; Ontology's; Tissue Culture; Biological Product; Anatomical Concepts; Anatomy Ontology; Article; Data Base; Data Interoperability; False Negative Result; False Positive Result; Female; Generative Artificial Intelligence; Hela Cell Line; Human; Human Cell; Large Language Model; Medical Ontology; Medical Research; Ontology; Ontology Development; Uterine Epithelium},
	keywords = {Formal concept analysis; Ontology; Base models; Biological data; Biological samples; Cell lines; Data annotation; Data interoperability; Generative AI; Language model; Large language model; Ontology's; Tissue culture; biological product; anatomical concepts; anatomy ontology; Article; data base; data interoperability; false negative result; false positive result; female; generative artificial intelligence; HeLa cell line; human; human cell; large language model; medical ontology; medical research; ontology; ontology development; uterine epithelium},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Sauvage2025212,
	author = {Sauvage, Eve},
	title = {SynKGP: Knowledge Graph Population with Syntactic-LLM Hybridation for Question-Answering},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15576 LNCS},
	pages = {212 - 219},
	doi = {10.1007/978-3-031-88720-8_34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006620090&doi=10.1007%2F978-3-031-88720-8_34&partnerID=40&md5=3b6aeff1a629557e21a47b4953eb7120},
	abstract = {Information Retrieval (IR) from structured data is facilitated by schemas such as ontologies, which enable efficient data access and organization. However, most knowledge is recorded in unstructured formats using natural language, which induces additional efforts to retrieve information. Retrieval Augmented Generation (RAG) has been developed to support general IR in unstructured text, yet it often produces inconsistencies due to model limitations. To address these inconsistencies, we propose transforming unstructured texts into structured information using text-to-Knowledge Graph (KG) translation. KGs support robust retrieval and reasoning tools, making them well-suited for complex knowledge representation. They are also understandable for humans. Our approach employs open-ended information extraction (for node extraction) and open-ended relation extraction (for relation extraction). To ensure the fidelity of the information present in the input text, we want to leverage a new combination of syntactic analysis for entity and relation extraction and Large Language Model (LLM) analysis to link the entities together. Initial implementations of this pipeline showed that our method effectively produces understandable and queryable KG. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Population; Large Language Models; Open Information Extraction; Relation Extraction; Syntactic Analysis; Computational Grammars; Content Based Retrieval; Machine Translation; Query Processing; Structured Query Language; Syntactics; Knowledge Graph Population; Knowledge Graphs; Language Model; Large Language Model; Open Information Extraction; Question Answering; Relation Extraction; Structured Data; Syntactic Analysis; Unstructured Texts; Online Searching},
	keywords = {Computational grammars; Content based retrieval; Machine translation; Query processing; Structured Query Language; Syntactics; Knowledge graph population; Knowledge graphs; Language model; Large language model; Open information extraction; Question Answering; Relation extraction; Structured data; Syntactic analysis; Unstructured texts; Online searching},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2025538,
	author = {Li, Zhuolun and Liang, Shaojun and Yang, Yi and Xu, Chengjie and Zheng, Ying},
	title = {Fault Knowledge Graph for QUAV Construction Based on Qwen Large Model},
	year = {2025},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {1344 LNEE},
	pages = {538 - 548},
	doi = {10.1007/978-981-96-2228-3_50},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006534693&doi=10.1007%2F978-981-96-2228-3_50&partnerID=40&md5=f414058dc8bbc9614958ef0190c93ef7},
	abstract = {Research on employing knowledge graphs for Quadrotor Unmanned Aerial Vehicle (QUAV) fault diagnosis is relatively scarce. This study explores the construction of a fault knowledge graph for QUAV. The ontology framework is devised with a focus on structural principles and supplementary fault case data, reflecting the unique characteristics of QUAV-related faults. Leveraging the Qwen large language model of Alibaba Cloud, we streamline the information extraction process from text by integrating Named Entity Recognition (NER) and Relation Extraction (RE) tasks, resulting in improved efficiency and accuracy over special-ized models. The use of Cypher serves as a pivotal tool for integrating knowledge graphs into the graph database, thereby facilitating their visualization. Through harnessing the powerful querying functionalities of Cypher, the experiments are executed that closely mimic the intricate fault analysis strategies adopted by main-tenance professionals. The experimental results confirm that this knowledge graph provides a sound foundation for reasoning based on structural principles. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cypher; Fault Knowledge Graph; Large Model; Quav; Online Searching; Cipher; Extraction Process; Fault Knowledge Graph; Faults Diagnosis; Knowledge Graphs; Language Model; Large Models; Named Entity Recognition; Ontology's; Quadrotor Unmanned Aerial Vehicles; Knowledge Graph},
	keywords = {Online searching; Cipher; Extraction process; Fault knowledge graph; Faults diagnosis; Knowledge graphs; Language model; Large models; Named entity recognition; Ontology's; Quadrotor unmanned aerial vehicles; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Borazio2025116,
	author = {Borazio, Federico and Croce, Danilo and Basili, Roberto},
	title = {Adapting LLMs for Domain-Specific Retrieval: A Case Study in Nuclear Safety},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15576 LNCS},
	pages = {116 - 122},
	doi = {10.1007/978-3-031-88720-8_20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006497851&doi=10.1007%2F978-3-031-88720-8_20&partnerID=40&md5=0df8cbcdb891070ecb73a8c9877953cf},
	abstract = {Large Language Models (LLMs) have revolutionized IR by improving query formulation, aggregating results, and generating intuitive summaries. However, their generalist training often limits their effectiveness in specialized domains, where precise terminology, relationships, and context are critical. To address this, we propose a simple yet effective method to adapt LLMs for domain-specific tasks by leveraging structured knowledge bases through a technique called “textification”. This approach transforms domain knowledge, such as glossaries and ontologies, into synthetic textual definitions or question-answer pairs, useful for fine-tuning an LLM to internalize specialized concepts, definitions, and semantic relationships. We demonstrate the impact of this approach with a case study in the nuclear safety domain, leveraging the International Atomic Energy Agency (IAEA) Safety Glossary to fine-tune an LLM. The adapted model consistently improves upon off-the-shelf versions in tasks like concept recognition, definition generation, and classification of ontological relationships. These results highlight the potential of domain-adapted LLMs to enhance retrieval-augmented systems, enabling more accurate and contextually relevant applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adaptation Of Retrieval Systems; Generative Ai; Knowledge Injection; Query Languages; Search Engines; Adaptation Of Retrieval System; Case-studies; Domain Specific; Generative Ai; Knowledge Injection; Language Model; Nuclear Safety; Query Formulation; Retrieval Systems; Simple++; Domain Knowledge},
	keywords = {Query languages; Search engines; Adaptation of retrieval system; Case-studies; Domain specific; Generative AI; Knowledge injection; Language model; Nuclear safety; Query formulation; Retrieval systems; Simple++; Domain Knowledge},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {19th International Conference on Research Challenges in Information Science, RCIS 2025},
	year = {2025},
	journal = {Lecture Notes in Business Information Processing},
	volume = {547 LNBIP},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006410124&partnerID=40&md5=4dd20c2580d2c21300f066874faf09eb},
	abstract = {The proceedings contain 46 papers. The special focus in this conference is on Research Challenges in Information Science. The topics include: Modeling Out-of-Vocabulary Words via Grammatical Fusion; early Length of Stay Prediction at Admission in Short-Stay Hospitals; alignment of Schema-Only and Instance-Only Data Sources Using Large Language Models; can Llama 3 Accurately Assess Readability? A Comparative Study Using Lead Sections from Wikipedia; research Challenges in Routine Optimization for Synthesizing Software Robots; mining for Meaning: Ontology-Aware Process Mining Methods Through Knowledge Patterns; How to Use a FEM Model as a Basis for Strategic-Level Risk Analysis; modelling Neural Network Models; quantifying the Magnitude of Violation: Predictive Compliance Monitoring Approaches; evaluating Programming Optimization Techniques in C and Python: Impact on Energy Consumption; an Analysis of Resilience in Digital Business Ecosystems; AI Auditing: Towards a Practicable Model; From Acquiring to Suggesting DL Design Choices with Agility: A System Design; digital Twins for Incident Detection and Response; towards an Enterprise Architecture Based Approach for the Development of Digital Twins for Sustainable Real Estate Management; a Method for Domain Reference Model Inference Through Knowledge and Data Intelligent Unifiers. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Myo2025102,
	author = {Myo, Su Wai and Anutariya, Chutiporn},
	title = {Ontology-Based Learning Assistant Chatbot: Enhancing Accurate and Explanatory Knowledge Provision in Myanmar’s Primary Education},
	year = {2025},
	journal = {Lecture Notes in Networks and Systems},
	volume = {1390 LNNS},
	pages = {102 - 112},
	doi = {10.1007/978-3-031-90295-6_11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005936670&doi=10.1007%2F978-3-031-90295-6_11&partnerID=40&md5=1c74a206672cb59c31c8479a3079697b},
	abstract = {Large language models (LLMs) and LLM-based generative AI tools have demonstrated considerable effectiveness in educational settings by challenging traditional classroom dynamics. They generate answers based on knowledge acquired during pre-training, making the answer construction process and the sources of information ambiguous. This uncertainty in responses complicates the assurance of appropriateness and reliability for young students, particularly in primary education. This paper, therefore, proposes an ontology-based learning assistant chatbot designed to address students’ inquiries using Subject Ontology (SO), which was developed for primary school teachers to model and verify subject knowledge. The chatbot aims to alleviate common academic challenges in Myanmar’s primary education. From the evaluation, teachers valued the chatbot’s transparency and reliability, as they could maintain direct control over the underlying knowledge base, enabling them to efficiently verify the accuracy of the chatbot’s responses and their sources. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatbot; Learning Assistant; Ontology; Primary Education; Domain Knowledge; Expert Systems; Knowledge Engineering; Ontology; Personnel Training; Teaching; Chatbots; Classroom Dynamics; Educational Settings; Language Model; Learning Assistant; Model-based Opc; Myanmars; Ontology's; Ontology-based Learning; Primary Education; Students},
	keywords = {Domain Knowledge; Expert systems; Knowledge engineering; Ontology; Personnel training; Teaching; Chatbots; Classroom dynamics; Educational settings; Language model; Learning assistant; Model-based OPC; Myanmars; Ontology's; Ontology-based learning; Primary education; Students},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {19th International Conference on Research Challenges in Information Science, RCIS 2025},
	year = {2025},
	journal = {Lecture Notes in Business Information Processing},
	volume = {548 LNBIP},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005936624&partnerID=40&md5=fda72684ca4539cfb739949808c52653},
	abstract = {The proceedings contain 46 papers. The special focus in this conference is on Research Challenges in Information Science. The topics include: Modeling Out-of-Vocabulary Words via Grammatical Fusion; early Length of Stay Prediction at Admission in Short-Stay Hospitals; alignment of Schema-Only and Instance-Only Data Sources Using Large Language Models; can Llama 3 Accurately Assess Readability? A Comparative Study Using Lead Sections from Wikipedia; research Challenges in Routine Optimization for Synthesizing Software Robots; mining for Meaning: Ontology-Aware Process Mining Methods Through Knowledge Patterns; How to Use a FEM Model as a Basis for Strategic-Level Risk Analysis; modelling Neural Network Models; quantifying the Magnitude of Violation: Predictive Compliance Monitoring Approaches; evaluating Programming Optimization Techniques in C and Python: Impact on Energy Consumption; an Analysis of Resilience in Digital Business Ecosystems; AI Auditing: Towards a Practicable Model; From Acquiring to Suggesting DL Design Choices with Agility: A System Design; digital Twins for Incident Detection and Response; towards an Enterprise Architecture Based Approach for the Development of Digital Twins for Sustainable Real Estate Management; a Method for Domain Reference Model Inference Through Knowledge and Data Intelligent Unifiers. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Al-Machot202525,
	author = {Al-Machot, Fadi and Horsch, Martin Thomas and Ullah, Habib},
	title = {Building Trustworthy AI: Transparent AI Systems via Language Models, Ontologies, and Logical Reasoning (TranspNet)},
	year = {2025},
	journal = {Lecture Notes in Networks and Systems},
	volume = {1375 LNNS},
	pages = {25 - 34},
	doi = {10.1007/978-3-031-89274-5_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005933003&doi=10.1007%2F978-3-031-89274-5_3&partnerID=40&md5=a46fa9153b009f0ea4bc78eb6658b282},
	abstract = {Growing concerns over the lack of transparency in AI, particularly in high-stakes fields like healthcare and finance, drive the need for explainable and trustworthy systems. While Large Language Models (LLMs) perform exceptionally well in generating accurate outputs, their “black box” nature poses significant challenges to transparency and trust. To address this, the paper proposes the TranspNet pipeline, which integrates symbolic AI with LLMs. By leveraging domain expert knowledge, retrieval-augmented generation (RAG), and formal reasoning frameworks like Answer Set Programming (ASP), TranspNet enhances LLM outputs with structured reasoning and verification. This approach strives to help AI systems deliver results that are as accurate, explainable, and trustworthy as possible, aligning with regulatory expectations for transparency and accountability. TranspNet provides a solution for developing AI systems that are reliable and interpretable, making it suitable for real-world applications where trust is critical. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Accountability; Ai Transparency; Answer Set Programming (asp); Explainable Ai; Large Language Models (llms); Neural-symbolic Integration; Ontologies; Retrieval-augmented Generation (rag); Symbolic Ai; Trustworthy Ai; Ai Accountability; Ai Transparency; Answer Set Programming; Explainable Ai; Language Model; Large Language Model; Neural-symbolic Integration; Ontology's; Retrieval-augmented Generation; Symbolic Ai; Trustworthy Ai; Ontology},
	keywords = {AI accountability; AI transparency; Answer set programming; Explainable AI; Language model; Large language model; Neural-symbolic integration; Ontology's; Retrieval-augmented generation; Symbolic AI; Trustworthy AI; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Kordi2025,
	author = {Kordi, Marzieh and Mariotti, Francesco and Magrini, Roberto and Lollini, Paolo and Bondavalli, Andrea},
	title = {On the Usage of ChatGPT for Integrating CAPEC Attacks into ADVISE Meta Ontology},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3962},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005835334&partnerID=40&md5=f59155512fca30b7fdaed22c4beed35f},
	abstract = {In today’s cybersecurity landscape, robust security assessment methodologies are essential for evaluating and improving systems, networks, applications, and data security. Modeling and simulation play an important role in this process by providing meaningful representations and analyses of attacks and defense strategies, particularly in systems where security breaches could have devastating consequences. The ADversary VIew Security Evaluation (ADVISE) Meta framework offers an ontology-based approach that, starting from a system’s architectural model, automatically generates detailed security models representing the attack steps that adversaries might take to achieve their goals. Manually extending the ADVISE Meta ontology with specific attack patterns is a challenging task that involves a deep understanding of the ontology, and its semantics. It also requires analyzing the attack paths to identify the necessary information in the ontology. To address this challenge we propose a methodology to facilitate the integration of attack patterns into the ADVISE Meta framework using ChatGPT. We focus on the Common Attack Pattern Enumeration and Classification (CAPEC) catalog by MITRE, a popular catalog with more than 500 attack patterns describing the common attributes and approaches used by adversaries to exploit known weaknesses in IT systems. ChatGPT is used as a support tool to interpret the descriptions of the attacks in the CAPEC catalog and systematically integrate the interpreted data into the ADVISE Meta ontology to generate the attack steps. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Capec; Chatgpt; Cybersecurity; Llms; Security Modeling; Attack Patterns; Chatgpt; Common Attack Pattern Enumeration And Classification; Cyber Security; Llm; Meta-frameworks; Meta-ontology; Ontology's; Security Evaluation; Security Modeling; Cyber Attacks},
	keywords = {Attack patterns; ChatGPT; Common attack pattern enumeration and classification; Cyber security; LLM; Meta-frameworks; Meta-ontology; Ontology's; Security evaluation; Security modeling; Cyber attacks},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bernardini2025,
	author = {Bernardini, Andrea and Lezoche, Mario and Angelini, Simone and Dondossola, Giovanna and Terruggia, Roberta},
	title = {Advancing Internet-Connected Devices Posture Analysis with a Meta-Search Engine: A Case Study in Energy Systems},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3962},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005824204&partnerID=40&md5=df494d58a3cac4f953de4dc4f32a9bc7},
	abstract = {In the contemporary digital ecosystem, Internet of Things Search Engines can be used for passive reconnaissance of Internet-connected devices, mapping possible attack surfaces without a direct interaction with the target devices or infrastructures. Each IoT search engine utilizes diverse scanning techniques and analytical methodologies, resulting in metadata with varying levels of coverage, accuracy, and relevance. This research introduces an IoT meta-search engine prototype designed to aggregate and merge metadata from commercial IoT search engines (Shodan, Censys, Netlas, Zoomeye, Binaryedge, Fofa) complemented by Common Vulnerabilities and Exposures (CVE) and Common Weakness Enumeration (CWE) sources. By merging those data, a more comprehensive and detailed perspective of the interconnected device landscape can be provided. Our methodology leverages an ontological framework using Stanford’s Protégé and Python, implementing zero-shot learning with a panel of three Large Language Models (LLMs) under human supervision to map IoT search engine taxonomic structures and quantitatively validate the generated Knowledge Base. The IoT meta-search engine is tested on photovoltaic (PV) energy production and monitoring systems, a domain essential to renewable energy grids. Vulnerabilities in PV systems can be exploited by hackers, causing energy disruptions, data breaches, or manipulation of grid operations. Although the findings are preliminary, they serve as a proof of concept to demonstrate the feasibility of the methodology to provide various types of overviews and insights associated with individual and multiple hosts for security posture evaluation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Energy Systems; Internet Connected Device; Internet Of Things; Llm; Ontology; Search Engine; Security; Vulnerability; Inference Engines; Knowledge Representation; Online Searching; Ontology; Sensitive Data; Taxonomies; Case-studies; Energy Systems; Internet Connected Device; Language Model; Large Language Model; Meta Search Engines; Ontology's; Posture Analysis; Security; Vulnerability; Knowledge Acquisition},
	keywords = {Inference engines; Knowledge representation; Online searching; Ontology; Sensitive data; Taxonomies; Case-studies; Energy systems; Internet connected device; Language model; Large language model; Meta search engines; Ontology's; Posture analysis; Security; Vulnerability; Knowledge acquisition},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Qasim2025350,
	author = {Qasim, Iqra and Horsch, Alexander and Prasad, Dilip K.},
	title = {SeqToFunc - From Sequence to Protein Function Prediction Using Language Modeling},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15618 LNCS},
	pages = {350 - 362},
	doi = {10.1007/978-3-031-88220-3_25},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005655887&doi=10.1007%2F978-3-031-88220-3_25&partnerID=40&md5=dbb0573f91a3965b3ca32771b219073a},
	abstract = {The Gene Ontology (GO) has about 45,000 terms that define a wide range of molecular functions, biological processes, and cellular locations of proteins, making Automated Function Prediction (AFP) a difficult problem. To accurately predict protein functions from this broad vocabulary, a model must be able to manage the diversity and complexities of protein biology. Despite a number of efforts, AFP is still an unsolved problem. Protein Language Models (pLMs) are a powerful tool to understand and predict protein functions using learned amino acid sequences and their relationships. AFP is inherently a multi-label classification problem. Our model SeqToFunc uses a hybrid transformer model that encodes both the protein sequences and their interactions into contextually aware embeddings for protein function prediction. The interactions between GO graphs are handled using a Graph Convolutional Network (GCN). We finetuned ESM2 on the CAFA5 challenge dataset for GO term predictions and reported our results using F1 score. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Go Term Prediction; Hybrid Transformer Model; Multi-label Classification; Multi-label Prediction; Protein Function Prediction; Protein Language Model (plm); Biomolecules; Gene Transfer; Gene Ontology Term Prediction; Gene Ontology Terms; Hybrid Transformer Model; Label Predictions; Language Model; Multi-label Classifications; Multi-label Prediction; Multi-labels; Protein Function Prediction; Protein Language Model; Transformer Modeling; Genome},
	keywords = {Biomolecules; Gene transfer; Gene ontology term prediction; Gene ontology terms; Hybrid transformer model; Label predictions; Language model; Multi-label classifications; Multi-label prediction; Multi-labels; Protein function prediction; Protein language model; Transformer modeling; Genome},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Donald2025110194,
	author = {Donald, Andy and Galanopoulos, Apostolos and Curry, Edward and Muñoz, Emir and Ullah, Ihsan and Waskow, M. A. and Kalra, Manan and Saxena, Sagar and Iqbal, Talha},
	title = {A Semantic Approach for Linked Model, Data, and Dataspace Cards},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {110194 - 110207},
	doi = {10.1109/ACCESS.2025.3572211},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005595465&doi=10.1109%2FACCESS.2025.3572211&partnerID=40&md5=39260fad811a4417f9ad46d34949c5fc},
	abstract = {In artificial intelligence, the significance of thorough documentation of models and datasets for publication is underestimated. However, due to the rising trend in the explainability and fairness of AI models, frameworks like Model Cards, Service Cards and Data Cards have emerged to facilitate understanding and reusing those models and datasets. Moreover, the Dataspace concept integrates these resources into Dataspace Cards, a comprehensive framework that systematically captures and organises crucial information to guide model and data selection for a specific application. This paper advocates a Semantic Web approach for transforming Model/Data Cards into Linked Data or knowledge graphs within a Dataspace, rendering them machine-readable and interoperable. A significant contribution is the development of a vocabulary that unifies Data, Model and Dataspace Card ontologies, enhancing consistent documentation and understanding of the Dataspace design. The paper further demonstrates the applicability of the proposed schema in various use cases, including bias detection in BERT-base-uncased and Large Language Models. Additionally, we propose a conceptual semantic approach, examined in-depth for sentiment and emotion analysis, to highlight how extended Dataspace Cards can improve applicability and outcomes. We found that this unified, ontology-driven approach results in more consistent metadata linking and more fine-grained bias detection in BERT-based-uncased than standalone documentation tools relying solely on Model or Data Cards. Furthermore, compared to existing frameworks, the richer interlinking capabilities of our proposed Dataspace Cards also facilitated easier traceability of performance outcomes, thereby ultimately fostering higher trustworthiness and reusability of AI resources. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Documentation; Data Cards; Dataspace Cards; Model Cards; Semantic Web; Service Cards; Ai Documentation; Data Cards; Data Space; Dataspace Card; Model Card; Modeling Data; Ontology's; Semantic Approach; Semantic-web; Service Card},
	keywords = {AI documentation; Data cards; Data space; Dataspace card; Model card; Modeling data; Ontology's; Semantic approach; Semantic-Web; Service card},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Menad2025169,
	author = {Menad, Safaa and Abdeddaïm, Saïd and Soualmia, Lina F.},
	title = {BioSTransformers for Health Ontologies Merging},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2454 CCIS},
	pages = {169 - 181},
	doi = {10.1007/978-3-031-87569-4_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005256728&doi=10.1007%2F978-3-031-87569-4_8&partnerID=40&md5=f9ad7e159e38ba7f06cc31bd6ae5256a},
	abstract = {Ontologies are fundamental for organizing and representing knowledge within a specific domain. In the biomedical domain, this type of representation is essential for structuring, coding, and retrieving data efficiently. However, existing biomedical ontologies often lack coverage of all relevant concepts and relationships in the same resource. In this paper, we describe our model for semantically merging and integrating diseases, symptoms, drugs, and adverse events into a single, unified resource. We propose leveraging BioSTransformers, our developed and trained siamese neural network models on biomedical data, to discover new relevant relationships between different biomedical concepts and create novel semantic relationships between these concepts. Our objective is to build a consistent, merged ontology that serves as a valuable resource for healthcare professionals across various health-related applications. To assess and validate the newly generated relationships, we plan to use external knowledge bases such as the UMLS Metathesaurus and the Semantic Network, alongside a large language model. Initial results are encouraging and pave the way for further research. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Ontologies; Large Language Models; Ontology Merging; Siamese Neural Models; Adverse Events; Biomedical Domain; Biomedical Ontologies; Disease Symptoms; Language Model; Large Language Model; Neural Modelling; Ontology Merging; Ontology's; Siamese Neural Model; Diseases},
	keywords = {Adverse events; Biomedical domain; Biomedical ontologies; Disease symptoms; Language model; Large language model; Neural modelling; Ontology merging; Ontology's; Siamese neural model; Diseases},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Cadman2025,
	author = {Cadman, Sam and Tanner, Claire and Pang, Patrick Cheong Iao},
	title = {Humanism strikes back? A posthumanist reckoning with ‘self-development’ and generative AI},
	year = {2025},
	journal = {AI and Society},
	pages = {},
	doi = {10.1007/s00146-025-02339-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005165796&doi=10.1007%2Fs00146-025-02339-1&partnerID=40&md5=455d202439e78aa7a18c34b91987fbe5},
	abstract = {Since the release of OpenAI's ChatGPT in 2022, AI activity has reached a fever pitch. Calls for effective ethical responses to the pressurised AI environment have in turn abounded. Posthumanism, which seeks to build ethical futures by de-centring the ‘human’, is an obvious candidate to act as a lynchpin of theoretical intervention. In their responses, posthumanist scholars appear to have embraced AI’s potential to destabilise Humanist philosophical ideas. We critically interrogate this initial enthusiasm. Conceptually distinguishing ‘post-dualist self-development’ (PDSD) from ‘technical self-development’ (TSD), we show how AI prompts an urgent need to advance posthumanist engagement with how technical development unsupervised by humans is ontologically discrete from other forms of material agency. We argue that specific engagement with TSD as distinct from PDSD is a key to avoid ignoring or underestimating Humanist and anthropocentric aspects of current AI innovation, and the influence of anthropomorphism. Without a theoretical reckoning with these tensions, posthumanism in the AI-era runs the risk of potentially promoting technologies that reinvigorate Humanist and anthropocentric expansion. To conclude, we show how a posthumanist ethics of generative AI that pays requisite attention to both TSD and PDSD may enable more anticipatory and nuanced assessments of the risks and benefits of discrete AI technologies to inform public discourse, appropriate social, institutional, policy and governance responses, and direct AI research and development priorities. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai; Humanism; Large Language Models; Posthumanism; Self-development; 'current; Ai Technologies; Humanism; Institutional Policies; Language Model; Large Language Model; Posthumanism; Research And Development; Self-development; Technical Development; Ethical Technology},
	keywords = {'current; AI Technologies; Humanism; Institutional policies; Language model; Large language model; Posthumanism; Research and development; Self-development; Technical development; Ethical technology},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{2025,
	title = {Proceedings of the 58th Hawaii International Conference on System Sciences, HICSS 2025},
	year = {2025},
	journal = {Proceedings of the Annual Hawaii International Conference on System Sciences},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005164929&partnerID=40&md5=6eb5774bafc5641d2b5991bc58fd1f5b},
	abstract = {The proceedings contain 813 papers. The topics discussed include: effective Japanese language acquisition through LINE groups: a self-regulated learning perspective; impacts of accountability partners on users’ online learning behaviors; black tiles instead of smiles – key reasons and effects of students’ webcam use during online lectures; Human-AI shared regulation for hybrid intelligence in learning and teaching: conceptual domain, ontological foundations, propositions, and implications for research; enhancing exploratory learning through exploratory search with the emergence of large language models; effective utilization of generative AI tool in education: a study exploring question pattern of varied student performance; and enhancing immersive virtual reality games with gamified learning analytics: the effects on learning value and enjoyment. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Pham20251565,
	author = {Pham, Anh T.V. and Huettemann, Sebastian and Mueller, Roland M.},
	title = {Enhancing Ontologies with Large Language Models: A Semi-Automated Approach},
	year = {2025},
	journal = {Proceedings of the Annual Hawaii International Conference on System Sciences},
	pages = {1565 - 1574},
	doi = {10.24251/hicss.2025.189},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005143430&doi=10.24251%2Fhicss.2025.189&partnerID=40&md5=464e13815927a5a34910caab9c2bff4a},
	abstract = {The process of creating and maintaining domain ontologies is a time- and resource-intensive activity, given the dynamic nature of domain knowledge and the regular introduction of new terms. This study aims to determine the effectiveness of large language models (LLMs) in augmenting the domain ontology authoring process. We fine-tuned state-of-the-art pre-trained LLMs and evaluated their performance on two tasks: synonym identification and parent-child relationship identification. The models achieved 98% accuracy in the first task and 75.4% accuracy in the second, demonstrating significant capabilities in automating synonym identification and relationship classification. In addition to providing a methodological basis for further extending and improving these results, we demonstrate that LLMs can be effectively used in ontology development and maintenance. This can save time and effort in the process. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Natural Language Processing; Ontology Enrichment; Ontology Extension; Transformer Models; Automatic Identification; Ontology; Domain Ontologies; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Ontology Enrichment; Ontology Extension; Ontology's; Transformer Modeling; Natural Language Processing Systems},
	keywords = {Automatic identification; Ontology; Domain ontologies; Language model; Language processing; Large language model; Natural language processing; Natural languages; Ontology enrichment; Ontology extension; Ontology's; Transformer modeling; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Banks20252277,
	author = {Banks, Jaime},
	title = {As a Large Language Model: Ontological-Category Cue Effects on Agent and Message Evaluations},
	year = {2025},
	journal = {Proceedings of the Annual Hawaii International Conference on System Sciences},
	pages = {2277 - 2286},
	doi = {10.24251/hicss.2025.280},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005141686&doi=10.24251%2Fhicss.2025.280&partnerID=40&md5=1256d02d717a7e605b959ac2f83cfd63},
	abstract = {The tendency for AI to refer to itself-for instance using first-person pronouns and referring to (in)capabilities-raises questions about the interpretation and effects of machines' self-referential language in human-machine communication. AI vary in their tendencies to identify themselves as machines or to mask that ontological category in the course of interactions. To examine how self-referential ontological-category cues (i.e., “As a large language model …”) influence judgments of contextualized agents and their responses, a 2×2×2 experiment was conducted. Participants (N = 800) evaluated an exchange between an inconspicuous user and ChatGPT, manipulated to represent three variables: Machine cue present/absent × natural/technical topic × creative/logical framing. Experimental findings point to a weak interaction effect of the cue and the topic suggesting a mild “stay in your lane” effect. Findings have implications for whether and in what context machines may be more or less favorably evaluated when their machine status is cued. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Cognitive Heuristics; Context-specificity; Ontological Categorization; Self-referential Language; Context Sensitive Languages; Cognitive Heuristic; Context-specificity; Creatives; First Person; Human-machine Communication; Interaction Effect; Language Model; Ontological Categorization; Self-referential Language; Weak Interactions; Computational Grammars},
	keywords = {Context sensitive languages; Cognitive heuristic; Context-specificity; Creatives; First person; Human-machine communication; Interaction effect; Language model; Ontological categorization; Self-referential language; Weak interactions; Computational grammars},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Cooper20251770,
	author = {Cooper, Sara and Ros, Raquel Julia and Lemaignan, Séverin and Gebellí, Ferran and Ferrini, Lorenzo and Juričić, Luka},
	title = {Demonstration of an Open-Source ROS 2 Framework and Simulator for Situated Interactive Social Robots},
	year = {2025},
	journal = {ACM/IEEE International Conference on Human-Robot Interaction},
	pages = {1770 - 1772},
	doi = {10.1109/HRI61500.2025.10974109},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004875524&doi=10.1109%2FHRI61500.2025.10974109&partnerID=40&md5=15b7a25432821b279b8d4ef7c50051b2},
	abstract = {We introduce an open-source ROS 2 architecture for situated social robots, along with a simulator that allows mixed-reality development and interactions. The architecture is a hybrid symbolic/subsymbolic system that integrates explicit ontology semantics for perception, reasoning, and execution, with LLMs. It features multimodal social perception by leveraging the open source ROS4HRI framework; LLMs (both edge- and cloud-based) to facilitate natural language interaction between the user and system; KnowledgeCore, an open-source knowledge base, to reason about facts in the world; and an intent-based controller to supervise the execution of parallel/sequential tasks and skills. We demonstrate our system architecture with a social robot running the mixed-reality system. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Mixed-reality Simulator; Ros 2 Framework; Situated Social Robots; Domain Knowledge; Expert Systems; Industrial Robots; Intelligent Robots; Knowledge Acquisition; Knowledge Representation; Mixed Reality; Mixed-reality Simulator; Multi-modal; Ontology Semantics; Open-source; Ros 2 Framework; Situated Social Robot; Social Perception; Social Robots; Sub-symbolic Systems},
	keywords = {Domain Knowledge; Expert systems; Industrial robots; Intelligent robots; Knowledge acquisition; Knowledge representation; Mixed reality; Mixed-reality simulator; Multi-modal; Ontology semantics; Open-source; ROS 2 framework; Situated social robot; Social perception; Social robots; Sub-symbolic systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mavridis2025,
	author = {Mavridis, Apostolos and Tegos, Stergios D. and Anastasiou, Christos and Papoutsoglou, Maria C. and Meditskos, Georgios},
	title = {Large language models for intelligent RDF knowledge graph construction: results from medical ontology mapping},
	year = {2025},
	journal = {Frontiers in Artificial Intelligence},
	volume = {8},
	pages = {},
	doi = {10.3389/frai.2025.1546179},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004439718&doi=10.3389%2Ffrai.2025.1546179&partnerID=40&md5=3862950d3446c57c4a9cd9a0c91669d2},
	abstract = {The exponential growth of digital data, particularly in specialized domains like healthcare, necessitates advanced knowledge representation and integration techniques. RDF knowledge graphs offer a powerful solution, yet their creation and maintenance, especially for complex medical ontologies like Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT), remain challenging. Traditional methods often struggle with the scale, heterogeneity, and semantic complexity of medical data. This paper introduces a methodology leveraging the contextual understanding and reasoning capabilities of Large Language Models (LLMs) to automate and enhance medical ontology mapping for Resource Description Framework (RDF) knowledge graph construction. We conduct a comprehensive comparative analysis of six systems–GPT-4o, Claude 3.5 Sonnet v2, Gemini 1.5 Pro, Llama 3.3 70B, DeepSeek R1, and BERTMap—using a novel evaluation framework that combines quantitative metrics (precision, recall, and F1-score) with qualitative assessments of semantic accuracy. Our approach integrates a data preprocessing pipeline with an LLM-powered semantic mapping engine, utilizing BioBERT embeddings and ChromaDB vector database for efficient concept retrieval. Experimental results on a dataset of 108 medical terms demonstrate the superior performance of modern LLMs, particularly GPT-4o, achieving a precision of 93.75% and an F1-score of 96.26%. These findings highlight the potential of LLMs in bridging the gap between structured medical data and semantic knowledge representation, toward more accurate and interoperable medical knowledge graphs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Health Data; Knowledge Graph; Llm; Ontology; Rdf; Snomed Ct},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{Withers2025765,
	author = {Withers, Christine Ann and Rufai, Amina Mardiyyah and Venkatesan, Aravind and Tirunagari, Santosh and Lobentanzer, Sebastian and Harrison, Melissa M. and Zdrazil, Barbara},
	title = {Natural language processing in drug discovery: bridging the gap between text and therapeutics with artificial intelligence},
	year = {2025},
	journal = {Expert Opinion on Drug Discovery},
	volume = {20},
	number = {6},
	pages = {765 - 783},
	doi = {10.1080/17460441.2025.2490835},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004288989&doi=10.1080%2F17460441.2025.2490835&partnerID=40&md5=d691806c71ddf75259ac2f72a1652060},
	abstract = {Introduction: The field of Natural Language Processing (NLP) within the life sciences has exploded in its capacity to aid the extraction and analysis of data from scientific texts in recent years through the advancement of Artificial Intelligence (AI). Drug discovery pipelines have been innovated and accelerated by the uptake of AI/Machine Learning (ML) techniques. Areas covered: The authors provide background on Named Entity Recognition (NER) in text–from tagging terms in text using ontologies to entity identification via ML models. They also explore the use of Knowledge Graphs (KGs) in biological data ingestion, manipulation, and extraction, leading into the modern age of Large Language Models (LLMs) and their ability to maneuver complex and abundant data. The authors also cover the main strengths and weaknesses of the many methods available when undertaking NLP tasks in drug discovery. Literature was derived from searches utilizing Europe PMC, ResearchRabbit and SciSpace. Expert opinion: The mass of scientific data that is now produced each year is both a huge positive for potential innovation in drug discovery and a new hurdle for researchers to overcome. Notably, methods should be selected to fit a use case and the data available, as each method performs optimally under different conditions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Drug Discovery; Knowledge Graph; Large Language Model; Machine Learning; Named Entity Recognition; Natural Language Processing; Ontology; Artificial Intelligence; Deep Learning; Drug Analysis; Drug Development; Europe; Human; Ingestion; Knowledge Graph; Large Language Model; Machine Learning; Modern Times; Natural Language Processing; Ontology; Review; Therapy; Data Mining; Procedures; Artificial Intelligence; Data Mining; Drug Discovery; Humans; Machine Learning; Natural Language Processing},
	keywords = {artificial intelligence; deep learning; drug analysis; drug development; Europe; human; ingestion; knowledge graph; large language model; machine learning; modern times; natural language processing; ontology; review; therapy; data mining; procedures; Artificial Intelligence; Data Mining; Drug Discovery; Humans; Machine Learning; Natural Language Processing},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Cheng2025268,
	author = {Cheng, Wanqiu and Tang, Jintao and Sun, Yuanyuan and Wang, Ting and Li, Shasha and Liu, Xiang and Li, Ronghui and Yang, Guoping},
	title = {OAGLLM: A Retrieval-Augmented Large Language Model for Medication Instructions},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2433 CCIS},
	pages = {268 - 284},
	doi = {10.1007/978-981-96-3752-2_16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003862119&doi=10.1007%2F978-981-96-3752-2_16&partnerID=40&md5=17c8a31fddadb5c977aad09c7b7c42f6},
	abstract = {The precision of contextual information is crucial for the results of Large Language Models (LLMs). However, to achieve more precise results in the field of medication instructions, which are characterized by their specificity in covering multiple drug characteristics, it is necessary to refine these instructions. Additionally, most current methods for constructing indexes do not consider sentence-level semantic information, which can easily ignore important details. Therefore, we propose a retrieval enhancement method based on ontology subdivision for LLM(OAGLLM). In the domain of Medication Instructions, we propose an ontology-based subdivision approach for constructing a specialized ontology and systematically storing expert knowledge, accurately match various aspects of Medication Instructions. To capture sentence-level semantic information and improve retrieval accuracy, we have introduced a hierarchical construction indexing method, which is designed to enhance text relevance and coherence. Lastly, we develop a retrieval augmentation system that integrates the Medication Instructions ontology with the hierarchical database. To validate the effectiveness of OAGLLM, we constructed a dataset on medication instructions. Our experiments demonstrate that our method outperforms other models in overall performance and excels across various types of data. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Hierarchical Construction Indexing Method; Medication Instructions; Ontology-based Subdivision; Retrieval Augmented Method; Indexing (of Information); Hierarchical Construction Indexing Method; Indexing Methods; Language Model; Medication Instruction; Ontology's; Ontology-based; Ontology-based Subdivision; Retrieval Augmented Method; Semantics Information; Sentence Level; Indexing (materials Working)},
	keywords = {Indexing (of information); Hierarchical construction indexing method; Indexing methods; Language model; Medication instruction; Ontology's; Ontology-based; Ontology-based subdivision; Retrieval augmented method; Semantics Information; Sentence level; Indexing (materials working)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zheng2025,
	author = {Zheng, Chen and Chen, Guohua and Chen, Honghao and Xu, Qiming and Zhao, Yimeng and Zhao, Yuanfei and Yang, Yunfeng},
	title = {Chemical process safety domain knowledge graph-enhanced LLM for efficient emergency response decision support},
	year = {2025},
	journal = {Canadian Journal of Chemical Engineering},
	pages = {},
	doi = {10.1002/cjce.25700},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003820036&doi=10.1002%2Fcjce.25700&partnerID=40&md5=c0621e8a7816a3f410210c72917a0650},
	abstract = {Chemical process safety accidents are characterized by their sudden onset, rapid evolution, and severe consequences. Developing effective emergency response decisions for such complex and dynamic incidents requires comprehensively considering various knowledge domains. Relying solely on expert experience and emergency plans often fails to meet the demands of effective emergency response. To enhance the efficiency of emergency response decision-making in chemical process accidents, this study proposes a method that leverages a chemical process safety knowledge graph (CPSKG) to enhance large language models (LLMs) for generating reliable emergency response decisions. The proposed method uses a seven-step approach to designing scenario and emergency response ontologies. By aligning with the characteristics of emergency domain knowledge texts and the ontology framework, natural language processing (NLP) and retrieval-augmented generation using graphs (Graph RAG) techniques are employed to construct a semantically rich CPSKG. The entities and relationships within the graph enhance the reasoning capabilities of LLMs, facilitating the generation of efficient and reliable emergency response decisions. A case study was conducted to validate the reliability of this approach. The results demonstrate that the LLM enhanced with the CPSKG outperforms other models in generating more effective emergency response decisions. As a key contribution, the proposed method improves the efficiency of knowledge sharing and emergency response in the chemical process safety domain while generating reliable and auxiliary emergency decisions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chemical Process Safety; Decision-making; Emergency Response; Knowledge Graph; Large Language Model; Ontology; Knowledge Graph; Natural Language Processing Systems; Chemical Process Safety; Decision Supports; Decisions Makings; Domain Knowledge; Emergency Response; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Safety Knowledge; Emergency Services},
	keywords = {Knowledge graph; Natural language processing systems; Chemical process safety; Decision supports; Decisions makings; Domain knowledge; Emergency response; Knowledge graphs; Language model; Large language model; Ontology's; Safety knowledge; Emergency services},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Garijo2025,
	author = {Garijo, Daniel and Poveda-Villalón, María and Amador-Domínguez, Elvira and Wang, Ziyuan and García-Castro, Raúl and Corcho, Oscar},
	title = {LLMs for Ontology Engineering: A landscape of Tasks and Benchmarking challenges},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3953},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003737186&partnerID=40&md5=9dce4e0184f2e7460f28c1daffb3f40a},
	abstract = {Large Language Models (LLMs) have emerged as a powerful technology for text generation tasks, showing promise in supporting the Ontology Engineering (OE) process. In this paper, we review current research on applying LLMs to OE tasks, aiming to identify commonalities and gaps in the state of the art. We categorize these efforts using the Linked Open Terms (LOT) methodology, characterizing them by their input and expected output. From this analysis, we highlight key challenges when creating benchmarks to evaluate LLM performance in OE tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmark; Challenges; Large Language Models; Ontology Engineering; 'current; Benchmark; Challenge; Engineering Tasks; Language Model; Large Language Model; Ontology Engineering; Ontology Engineering Process; State Of The Art; Text Generations; Ontology},
	keywords = {'current; Benchmark; Challenge; Engineering tasks; Language model; Large language model; Ontology engineering; Ontology engineering process; State of the art; Text generations; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Herwanto2025,
	author = {Herwanto, Guntur Budi and Tsaneva, Stefani and Sabou, Marta},
	title = {Ontology Corpora for LLM-based Knowledge Engineering Research},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3953},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003728659&partnerID=40&md5=959516da61ee5c39df60e25a9d7493b4},
	abstract = {Generative AI (GenAI) solutions are likely to have a profound impact on the Knowledge Engineering (KE) field. Considerable research is needed to understand the extent to which various KE tasks can be performed with GenAI, how the performance of these tasks compares to human baselines, and how to effectively adapt KE workflows to make the best use of GenAI methods. To conduct such research, there is a need of collections of corpora of ontologies with a range of diverse characteristics to support systematic experimentation covering a broad variety of ontology types. We propose collecting such corpora and describe our ongoing efforts to collect ontologies created by students, as representative for the work of junior ontology engineers (beginners level knowledge engineering skills). We also create an ontology analysis workflow to extract key metadata from ontologies and associated reports, which we share with the community. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology Corpus; Ontology Engineering; Ontology Processing Workflow; Student Authored Ontologies; Behavioral Research; Engineering Education; Industrial Research; Ontology; Professional Aspects; Work Simplification; Engineering Fields; Engineering Tasks; Ontology Corpus; Ontology Engineering; Ontology Processing Workflow; Ontology's; Student Authored Ontology; Work-flows; Engineering Research},
	keywords = {Behavioral research; Engineering education; Industrial research; Ontology; Professional aspects; Work simplification; Engineering fields; Engineering tasks; Ontology corpus; Ontology engineering; Ontology processing workflow; Ontology's; Student authored ontology; Work-flows; Engineering research},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2025,
	title = {HGAIS 2024 - Proceedings of the Special Session on Harmonising Generative AI and Semantic Web Technologies, co-located with the 23rd International Semantic Web Conference, ISWC 2024},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3953},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003726476&partnerID=40&md5=5eaae429353e70e72793e075a58a61bc},
	abstract = {The proceedings contain 14 papers. The topics discussed include: LLM-based SPARQL query generation from natural language over federated knowledge graphs; a benchmark for the detection of metalinguistic disagreements between LLMs and knowledge graphs; assessing large language models for SPARQL query generation in scientific question answering; benchmarking ontology validation capabilities of LLMs; ontology corpora for LLM-based knowledge engineering research; information for conversation generation: proposals utilizing knowledge graphs; OAEI-LLM: a benchmark dataset for understanding large language model hallucinations in ontology matching; a comprehensive benchmark for evaluating LLM-generated ontologies; and hybrid evaluation of Socratic dialogue for teaching. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Alharbi2025,
	author = {Alharbi, Reham and Berardinis, Jacopo De and Grasso, Floriana and Payne, Terry R. and Tamma, Valentina A.M.},
	title = {Characteristics and Desiderata for Competency Question Benchmarks},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3953},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003723011&partnerID=40&md5=b4e4a80078b99d58fd4caf616df9b708},
	abstract = {Competency Questions (CQs) are essential in ontology engineering; they express an ontology’s functional requirements through natural language questions, offer crucial insights into an ontology’s scope, and are pivotal for various tasks, such as ontology reuse, testing, requirement specification, and pattern definition. Various approaches have emerged that make use of LLMs for the generation of CQs from different knowledge sources. However, comparative evaluations are hindered by differences in tasks, datasets and evaluation measures used. In this paper, we provide a set of desiderata for a benchmark of CQs, where we position state of the art approaches with respect to a categorisation of tasks, and highlight the main challenges hindering the definition of a community-based benchmark to support comparative studies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmark; Competency Questions; Dataset; Evaluation; Large Language Models; Requirements Engineering; Benchmark; Competency Question; Dataset; Evaluation; Functional Requirement; Language Model; Large Language Model; Natural Language Questions; Ontology Engineering; Ontology's; Specifications},
	keywords = {Requirements engineering; Benchmark; Competency question; Dataset; Evaluation; Functional requirement; Language model; Large language model; Natural language questions; Ontology engineering; Ontology's; Specifications},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Rebboud2025,
	author = {Rebboud, Youssra and Lisena, Pasquale and Tailhardat, Lionel and Troncy, Raphaël},
	title = {Benchmarking LLM-based Ontology Conceptualization: A Proposal},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3953},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003719390&partnerID=40&md5=3a9798338e35ed4a969743b0278931bb},
	abstract = {This study presents a benchmark proposal designed to enhance knowledge engineering tasks through the use of large language models (LLMs). As LLMs become increasingly pivotal in knowledge extraction and modeling, it is crucial to evaluate and improve their performance. Building on prior work aiming at reverse generating competency questions (CQs) from existing ontologies, we introduce a benchmark focused on specific knowledge modeling tasks including ontology documentation, ontology generation, and query generation. In addition, we propose a baseline evaluation framework that applies various techniques, such as semantic comparison, ontology evaluation criteria, and structural comparison, using both existing ground truth datasets and newly proposed ontologies with corresponding CQs and documentation. This rigorous evaluation aims to provide a deeper understanding of LLM capabilities and contribute to their optimization in knowledge engineering applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmark Proposal; Knowledge Engineering; Knowledge Representation; Large Language Models; Conceptual Design; Knowledge Acquisition; Knowledge Graph; Knowledge Organization System (kos); Benchmark Proposal; Engineering Tasks; Knowledge Extraction; Knowledge Model; Knowledge-representation; Language Model; Large Language Model; Model-based Opc; Ontology's; Performance; Image Representation},
	keywords = {Conceptual design; Knowledge acquisition; Knowledge graph; Knowledge organization system (KOS); Benchmark proposal; Engineering tasks; Knowledge extraction; Knowledge model; Knowledge-representation; Language model; Large language model; Model-based OPC; Ontology's; Performance; Image representation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Plu2025,
	author = {Plu, Julien and Escobar, Oscar Moreno and Trouillez, Edouard and Gapin, Axelle and Troncy, Raphaël},
	title = {A Comprehensive Benchmark for Evaluating LLM-Generated Ontologies},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3953},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003711461&partnerID=40&md5=8fd2db4b7f249a0ddbc9b672049723f2},
	abstract = {This paper presents a methodology for evaluating ontologies that are automatically generated by Large Language Models (LLMs). Our approach combines quantitative metrics that compare generated ontologies with respect to a human-made reference and qualitative user assessments across diverse domains. We apply this methodology to evaluate the ontologies produced by various LLMs, including Claude 3.5 Sonnet, GPT-4o, and GPT-4o-mini. The results demonstrate the benchmark’s effectiveness in identifying strengths and weaknesses of LLM-generated ontologies, providing valuable insights for improving automated ontology generation techniques. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmark; Evaluation; Knowledge Engineering; Llm; Ontology Development; Automatically Generated; Benchmark; Diverse Domains; Evaluation; Language Model; Large Language Model; Ontology Development; Ontology Generation; Ontology's; Quantitative Metric; Ontology},
	keywords = {Automatically generated; Benchmark; Diverse domains; Evaluation; Language model; Large language model; Ontology development; Ontology generation; Ontology's; Quantitative metric; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Qiang2025,
	author = {Qiang, Zhangcheng and Taylor, Kerry L. and Wang, Weiqing and Jiang, Jing},
	title = {OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3953},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003710219&partnerID=40&md5=2bc85a3f291179ba30149f9d98dc04dc},
	abstract = {Hallucinations of large language models (LLMs) commonly occur in domain-specific downstream tasks, with no exception in ontology matching (OM). The prevalence of using LLMs for OM raises the need for benchmarks to better understand LLM hallucinations. The OAEI-LLM dataset is an extended version of the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate LLM-specific hallucinations in OM tasks. We outline the methodology used in dataset construction and schema extension, and provide examples of potential use cases. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Llm Hallucinations; Ontology Matching; Modeling Languages; Benchmark Datasets; Domain Specific; Down-stream; Extended Versions; Language Model; Large Language Model; Large Language Model Hallucination; Ontology Alignment; Ontology Matching; Large Datasets},
	keywords = {Modeling languages; Benchmark datasets; Domain specific; Down-stream; Extended versions; Language model; Large language model; Large language model hallucination; Ontology alignment; Ontology matching; Large datasets},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Tsaneva2025,
	author = {Tsaneva, Stefani and Herwanto, Guntur Budi and Sabou, Marta},
	title = {Benchmarking Ontology Validation Capabilities of LLMs},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3953},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003709629&partnerID=40&md5=a794337d7764e3e883482385a1b47794},
	abstract = {With the advent of Generative AI, numerous approaches exploring large language models (LLMs) have been proposed for addressing a number of Knowledge Engineering (KE) tasks. Yet, the status of this research field is rather preliminary and there is, for now, no systematic and comprehensive understanding on how LLMs perform on selected knowledge engineering tasks (e.g., what is their expertise level in understanding ontology modeling concepts). Such insights would be crucial for researchers working in this field to support with selecting the most suitable LLMs during experiment design. This situation is exacerbated by the rapid expansion in the number of available LLMs. We therefore see the need for methodologies and tools that allow (comparatively) assessing LLM capabilities. To address this need, we propose the creation of an assessment test benchmark for evaluating the LLM knowledge engineering skills. We present ongoing work and preliminary results on assessing the expertise of LLMs in terms of a concrete KE task, namely ontology validation. Our experiments highlight the superiority of proprietary models on this task, particularly GPT-4o and Claude-Sonnet-3.5, over open source models. Lastly, we identify the need of a community-driven comparative LLM assessment platform that facilitates resource sharing and experience exchange, while protecting the integrity and privacy of the envisioned benchmark. We share (i) the current version of the qualification tests and (ii) its implementation for assessing LLM capabilities for ontology validation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Assessment Tests; Expertise Evaluation; Knowledge Engineering; Llms; Ontology Validation; Assessment Test; Engineering Tasks; Expertise Evaluation; Language Model; Large Language Model; Modeling Concepts; Ontology Model; Ontology Validations; Research Fields; Validation Capability; Engineering Research},
	keywords = {Assessment test; Engineering tasks; Expertise evaluation; Language model; Large language model; Modeling concepts; Ontology model; Ontology validations; Research fields; Validation capability; Engineering research},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Singh20257277,
	author = {Singh, Chandan Kumar and Kumar, Devesh and Sanap, Vipul and Sinha, Rajesh},
	title = {LLM-RSPF: Large Language Model-Based Robotic System Planning Framework for Domain Specific Use-cases},
	year = {2025},
	pages = {7277 - 7286},
	doi = {10.1109/WACV61041.2025.00707},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003629658&doi=10.1109%2FWACV61041.2025.00707&partnerID=40&md5=f936765f3f0bf5e3170914af18fe83b6},
	abstract = {The employment of large language models (LLMs) for task planning and reasoning has emerged as a focal point of interest within the robotics research community. However, directly applying LLMs, even with large token-sized prompts, does not achieve the task planning performance required for an industrial-grade domain-specific use-case (DSU). This work aims to overcome the obstacles of a robotic task planner for DSUs by introducing a novel planning framework, LLM-RSPF (Large Language Model-based Robotic System Planning Framework). Central to the LLM-RSPF is a novel robotic system ontology that organizes the components of the robotic system in a coherent and a systematic manner. The ontology empowers the LLM-RSP F to efficiently capture a contextual representation of the DSU using the LLMs. Subsequently, the research introduces a LLM-tuning regimen referred as chain of hierarchical thought (CoHT), specifically crafted to complement the proposed system ontology. Integrating these two components, the LLM-RSPF aims to enhance the accuracy, robustness, and throughput of a robotic system in a cost-effective manner. In addition, the research presents an empirical methodology to generate the LLM-tuning dataset size for a guaranteed performance. The LLM-RSPF is validated on a retail order-fulfillment use-case thereby, illustrating the efficacy of the framework. Through rigorous evaluation, the LLM-RSPF demonstrates exceptional performance on the generated dataset, effectively meeting the DSU objectives. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Coht; Domain-specific Use-case; Large Language Model; Robotic System Ontology; Task Planning; Industrial Robots; Intelligent Robots; Coht; Domain Specific; Domain-specific Use-case; Language Model; Large Language Model; Planning Framework; Robotic System Ontology; Robotic Systems; System Ontology; Task Planning; Motion Planning},
	keywords = {Industrial robots; Intelligent robots; Coht; Domain specific; Domain-specific use-case; Language model; Large language model; Planning framework; Robotic system ontology; Robotic systems; System ontology; Task planning; Motion planning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kim202573603,
	author = {Kim, Seungyeon and Kim, Donghyun and Hwang, Seokju and Lee, Kyong Ho and Lee, Kyunghwa},
	title = {LLM-Assisted Ontology Restriction Verification With Clustering-Based Description Generation},
	year = {2025},
	journal = {IEEE Access},
	volume = {13},
	pages = {73603 - 73618},
	doi = {10.1109/ACCESS.2025.3562560},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003435799&doi=10.1109%2FACCESS.2025.3562560&partnerID=40&md5=7fb7ff39e7103d7aba92020f3d6a17d9},
	abstract = {An ontology is a scheme for structuring relationships between concepts in a domain, promoting data interoperability and system integration. However, poorly designed ontologies can lead to errors and performance issues. While systems engineering has standardized evaluation guidelines (e.g., ISO/IEC), ontology engineering lacks such standards, leading to various independent evaluation methods. One frequent issue among novice developers is the misuse of ontology restrictions, particularly ‘allValuesFrom’ and ‘someValuesFrom’, which can significantly impact the correctness and reliability of ontologies. However, existing studies have not adequately addressed effective methods for detecting such errors. To address this gap, we propose a context-aware verification framework utilizing large language models to detect and correct misuse in ontology restrictions. Unlike conventional methods, our framework integrates contextual descriptions derived from ontological axioms, enabling more accurate verification. Additionally, we introduce a clustering-based description generation method that systematically organizes contextual information, further enhancing verification accuracy. Experimental evaluation conducted on diverse ontology datasets suggests that contextual integration improves verification performance. Moreover, the clustering-based description generation improves restriction misuse detection and correction compared to traditional approaches. By automating ontology restriction verification, this study contributes significantly to enhancing the reliability of ontology evaluation and provides a foundation for developing more scalable and standardized verification techniques. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clustering; Ontology Evaluation; Ontology Restriction Verification; Text Generation; Iso Standards; Ontology; Clusterings; Data Interoperability; Data Systems; Ontology Evaluations; Ontology Restriction Verification; Ontology's; Performance Issues; Relationship Between Concepts; System Integration; Text Generations; Interoperability},
	keywords = {ISO Standards; Ontology; Clusterings; Data interoperability; Data systems; Ontology evaluations; Ontology restriction verification; Ontology's; Performance issues; Relationship between concepts; System integration; Text generations; Interoperability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Qiang2025516,
	author = {Qiang, Zhangcheng and Wang, Weiqing and Taylor, Kerry L.},
	title = {Agent-OM: Leveraging LLM Agents for Ontology Matching},
	year = {2025},
	journal = {Proceedings of the VLDB Endowment},
	volume = {18},
	number = {3},
	pages = {516 - 529},
	doi = {10.14778/3712221.3712222},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003258966&doi=10.14778%2F3712221.3712222&partnerID=40&md5=8f3cd5ce918a389593d0e92ce9e603d9},
	abstract = {Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative(OAEI) tracks over state-of-the-art OM systems show thatour system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adversarial Machine Learning; Expert Systems; Interoperability; Learning Systems; Ontology; Search Engines; Design Paradigm; Knowledge-based Expert Systems; Language Model; Matching System; Model Agents; Ontology Matching; Ontology's; Performance; Related Entities; Semantic Interoperability; Semantics},
	keywords = {Adversarial machine learning; Expert systems; Interoperability; Learning systems; Ontology; Search engines; Design paradigm; Knowledge-based expert systems; Language model; Matching system; Model agents; Ontology matching; Ontology's; Performance; Related entities; Semantic interoperability; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang2025222,
	author = {Zhang, Yongting and Wang, Huanhuan and Yeoh, Pauline Shan Qing and Yu, Ze Hua and Zou, Baowen and Hasikin, Khairunnisa Binti and Wee, Lai Khin and Wu, Xiang},
	title = {Automatic Knowledge Graph Construction and Dynamic Fusion Method Using LLMs and Graph Embedding for Medical Informatics Education},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2447 CCIS},
	pages = {222 - 240},
	doi = {10.1007/978-981-96-3735-5_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003208891&doi=10.1007%2F978-981-96-3735-5_17&partnerID=40&md5=88757b756c68510952fe2c795b28d765},
	abstract = {Knowledge graphs (KGs) have great potential in various practical applications, especially in education. Educational KGs are of major importance in course design and personal learning. They collected and integrated data from multiple sources, such as textbooks and education guidelines, to provide systematic knowledge architecture for learners, gradually improving education quality. Previous work focused on massive manual operations and professional knowledge to build KGs and presented many limitations, such as increased time consumption. To address the above issues, this study proposes a pipeline that combines large language models (LLMs) and a graph embedding method to support KG construction for medical informatics, LLM-MIKG. Specifically, the entire construction process can be divided into three critical phases. First, we adopt a top-down approach to create an ontology, dividing medical informatics into clinical informatics, bioinformatics, pharmaceutical informatics, medical intelligence, medical information security, and intelligent medicine. Second, the prompting engineering should be optimized. This study designs a recursive prompting strategy to extract the entities and relationships of each knowledge node from top to bottom, forming many local KGs. Then, a graph embedding method is used to fuse local KGs to merge duplicate entities, establishing a complete medical informatics KG. Finally, the experimental results demonstrated the superiority of the proposed pipeline. We visualized 30,000 nodes on neo4j and deployed it on a self-developed medical informatics lifelong education platform to support further educational applications, which is an innovative attempt at medical informatics. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Automatic Knowledge Graph Construction; Graph Embedding Method; Knowledge Fusion; Large Language Model; Neo4j; Curricula; Distributed Database Systems; Knowledge Graph; Medical Education; Medical Informatics; Natural Language Processing Systems; Automatic Knowledge Graph Construction; Embedding Method; Graph Construction; Graph Embedding Method; Graph Embeddings; Knowledge Fusion; Knowledge Graphs; Language Model; Large Language Model; Neo4j},
	keywords = {Curricula; Distributed database systems; Knowledge graph; Medical education; Medical informatics; Natural language processing systems; Automatic knowledge graph construction; Embedding method; Graph construction; Graph embedding method; Graph embeddings; Knowledge fusion; Knowledge graphs; Language model; Large language model; Neo4j},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Xiahou20251069,
	author = {Xiahou, Xiaer and Chen, Gaotong and Li, Zirui and Xu, Xin and Li, Qiming},
	title = {Knowledge Management in Construction Quality Management: Current State, Challenges, and Future Directions},
	year = {2025},
	journal = {IEEE Transactions on Engineering Management},
	volume = {72},
	pages = {1069 - 1088},
	doi = {10.1109/TEM.2025.3550354},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003157608&doi=10.1109%2FTEM.2025.3550354&partnerID=40&md5=bff39b79802f667a4826a8a3246f4bbf},
	abstract = {Construction quality management (CQM), as one of the major activities in construction project management, relies heavily on knowledge. Unfortunately, the knowledge of CQM is diverse in format and scattered in different stakeholders within the whole construction processes. Therefore, knowledge management (KM) of CQM is underinvestigated. To offering a comprehensive view of KM in CQM, this article employed a mixed review method to critically review 87 related articles. The results indicate 1) building information modeling, ontology, and natural language processing are identified as critical technologies in KM, 2) expert system and decision support, structural health monitoring, and project management are the major application domains. This article conducts an in-depth analysis of the literature based on the three phases of quality control: pre-construction, in-construction, and post-construction. The results are discussed to critically assess the critical technologies in KM. A framework is proposed to guide the effective implementation of KM in CQM, alongside a discussion of the current challenges and opportunities. The article further identifies potential development directions for KM in CQM, including total quality management, digital twins, development of large language models, construction of “No-cost” KM platforms, uniform evaluation and standardization mechanisms, tacit knowledge capture, and confidentiality and security. A novel paradigm for knowledge-driven quality management decision-making is first introduced. This article offers a comprehensive perspective on the application of KM in CQM, which will significantly enhance the effectiveness of CQM implementation in the future. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Construction Quality Management (cqm); Knowledge Management (km); Knowledge-driven Quality Management; Mixed Review; Quality Control; Decision Making; Natural Language Processing Systems; Total Quality Management; 'current; Building Information Modelling; Construction Process; Construction Project Management; Construction Quality Management; Critical Technologies; Knowledge-driven Quality Management; Management Is; Mixed Review; Ontology Language; Project Management},
	keywords = {Decision making; Natural language processing systems; Total quality management; 'current; Building Information Modelling; Construction process; Construction project management; Construction quality management; Critical technologies; Knowledge-driven quality management; Management IS; Mixed review; Ontology language; Project management},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Wu202524,
	author = {Wu, Yanting and Sun, Yicheng and Wen, Xiaojian and Liu, Xiaoqiang and Bao, Jinsong and Wang, Sen},
	title = {A Generative Modeling Method for Digital Twin Shop Floor},
	year = {2025},
	journal = {IEEE Internet Computing},
	volume = {29},
	number = {1},
	pages = {24 - 31},
	doi = {10.1109/MIC.2024.3522301},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003040038&doi=10.1109%2FMIC.2024.3522301&partnerID=40&md5=536a6181f3b0e7ca577d71375281ee20},
	abstract = {Digital twin (DT) as a key enabling technology for achieving digitization, flexibility, and customization in shop floors has attracted significant attention. However, the shop floor involves diverse assets across multiple dimensions, scales, and interdisciplinary fields, making the modeling process complex. To address this issue, this article analyzes the construction process of ontology-based information models and proposes a generative modeling method for digital twin shop floor driven by large language models (LLMs). First, LLMs are utilized to analyze user intentions, acquiring the hierarchical object structure of DT models. Second, by combining an analysis–retrieval method to extract domain knowledge and generate dynamic prompts, LLMs are guided to realize the creation and fusion of objects and construct structured and semantically enriched DT models. Finally, the effectiveness of the proposed method is validated through examples of shop floor resource scheduling. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Machine Shops; Customisation; Digitisation; Enabling Technologies; Generative Model; Interdisciplinary Fields; Language Model; Model Method; Modeling Process; Multiple Dimensions; Shopfloors; Semantics},
	keywords = {Machine shops; Customisation; Digitisation; Enabling technologies; Generative model; Interdisciplinary fields; Language model; Model method; Modeling process; Multiple dimensions; Shopfloors; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Hosseini2025380,
	author = {Hosseini, Ali Mohammad and Kastner, Wolfgang and Sauter, Thilo},
	title = {Leveraging LLMs and Knowledge Graphs to Design Secure Automation Systems},
	year = {2025},
	journal = {IEEE Open Journal of the Industrial Electronics Society},
	volume = {6},
	pages = {380 - 395},
	doi = {10.1109/OJIES.2025.3545811},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003034847&doi=10.1109%2FOJIES.2025.3545811&partnerID=40&md5=78e6bb987f64de17e8595c90d94b9c7f},
	abstract = {The digital transformation of Industrial Control Systems (ICSs) within the Industry 4.0 paradigm is essential for industrial organizations to remain competitive, while cybersecurity is an enabler. However, security measures, often implemented late in the engineering process, lead to costly and complicated implementations. Thus, this article is concerned with the “security by design” principle in ICSs and facilitates compliance with ICS security standards, which can be legally mandated for some critical systems or adopted by asset owners to protect their assets. Current methods for compliance demand manual efforts from security experts, making the compliance process time-consuming and costly. To address this, we propose a framework for leveraging large language models (LLMs) combined with knowledge graphs to automate the interpretation of security requirements and system architecture as two main elements of the design phase. Our knowledge graph-augmented LLM framework converts system architectures into human natural language, enhancing the automation of various security analyses, especially those that need to handle textual requirements. The framework enables validating applicable security requirements provided by IEC 62443-3-3 (a widely-used ICS security standard) concerning system designs through a question-and-answer interface. To evaluate the framework, various questions with reference responses from human experts were prepared in the context of a use case, and the quality of the LLMs' responses was measured across various metrics. Moreover, we compared the framework with a baseline approach based on formal queries. The results show that the proposed framework effectively automates security tasks and offers a user-friendly interface accessible to nonexperts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Industrial Control System (ics); Knowledge Graph; Large Language Model (llm); Ontology; Security By Design; Chemical Plants; Competition; Concrete Construction; Petroleum Products; Requirements Engineering; Control System Security; Industrial Control System; Industrial Control Systems; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Security By Design; Security Requirements; Security Standards; Knowledge Graph},
	keywords = {Chemical plants; Competition; Concrete construction; Petroleum products; Requirements engineering; Control system security; Industrial control system; Industrial control systems; Knowledge graphs; Language model; Large language model; Ontology's; Security by design; Security requirements; Security standards; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{Martino2025357,
	author = {Martino, Beniamino Di and Esposito, Antonio and Fiorentino, Francesco},
	title = {Automation and Visualization of Knowledge in Cloud Services: A Semantic Ontology Enhanced by Generative AI Models},
	year = {2025},
	journal = {Lecture Notes on Data Engineering and Communications Technologies},
	volume = {250},
	pages = {357 - 365},
	doi = {10.1007/978-3-031-87778-0_35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002964143&doi=10.1007%2F978-3-031-87778-0_35&partnerID=40&md5=9f3a5b7ad75dcb43fdd22922b7b30996},
	abstract = {The rapid evolution of Cloud Computing has revolutionized the delivery and use of IT services. However, challenges such as portability and interoperability among cloud platforms hinder seamless service integration. This thesis addresses these issues by proposing a conceptual framework for the semantic representation and enrichment of a Cloud Ontology, supported by Artificial Intelligence (AI) technologies. The work focuses on leveraging Large Language Models (LLM) and Retrieval-Augmented Generation (RAG) techniques to automate the population and management of ontologies. Advanced tools such as Protégé, Dify, and RAGFlow were utilized to develop innovative solutions for representing, querying, and interactively visualizing knowledge in cloud domains. Semantic technologies like RDF and OWL ensured broad interoperability, while the integration of linguistic models enabled continuous and dynamic updates of information. The results demonstrate significant improvements in the automation of ontology management processes and access to structured data. This thesis contributes to the field of Knowledge Engineering by proposing an integrated approach to tackling the complexity and fragmentation of the cloud landscape. Finally, future perspectives are outlined for applying these techniques in broader contexts, such as big data management and cross-industry interoperability. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Information Management; Ontology; Semantics; Structured Query Language; Cloud Platforms; Cloud Services; Cloud-computing; Intelligence Models; It Services; Ontology's; Seamless Services; Semantic Ontology; Services Integrations; Visualization Of Knowledge},
	keywords = {Information management; Ontology; Semantics; Structured Query Language; Cloud platforms; Cloud services; Cloud-computing; Intelligence models; IT services; Ontology's; Seamless services; Semantic ontology; Services integrations; Visualization of knowledge},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Xie2025,
	author = {Xie, Junfeng and Li, Wei and You, Hairu and Zhang, Dafang},
	title = {GraphTransNet: predicting epilepsy-related genes using a graph-augmented protein language model},
	year = {2025},
	journal = {Frontiers in Pharmacology},
	volume = {16},
	pages = {},
	doi = {10.3389/fphar.2025.1584625},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002619239&doi=10.3389%2Ffphar.2025.1584625&partnerID=40&md5=2ad3c638094da1eee35fb1f0cc6b3f5b},
	abstract = {Introduction: Epilepsy, a complex neurological disorder characterised by recurrent seizures and significant genetic heterogeneity, presents considerable challenges form accurate diagnosis and drug target identification. While traditional genomewide association studies (GWAS) and sequencing technologies have advanced our understanding of epilepsy-related gene targets, they often struggle to identify novel and rare variants crucial for precise diagnosis and targeted drug development. The increasing availability of large-scale genomic data, coupled with the power of deep learning, offers a promising avenue for progress. Method: In this work, we introduce GraphTransNet, a novel hybrid neural network model designed for predicting epilepsy-associated gene targets, with direct implications for improved disease diagnosis and therapeutic target identification. GraphTransNet leverages protein language models (specifically ESM) to generate numerical embeddings from gene sequences. These embeddings are then processed by a novel architecture integrating transformer and convolutional neural network (CNN)components to predict epilepsy-related gene targets. Results: Our results demonstrate that GraphTransNet achieves high accuracy in identifying epilepsy targets, outperforming existing predictive tools in terms of both recall and precision metrics for reliable disease diagnosis and effective drug target identification. Rigorous comparisons with established machine learning methods and other deep learning architectures further underscore the efficacy of GraphTransNet. Discussion: This approach represents a valuable computational tool for advancing epilepsy genetics research, with the potential to contribute to more accurate diagnostic strategies and the discovery of novel drug targets for improved treatment outcomes. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Epilepsy Diseases; Epilepsy-associated Interactions; Protein Language Model; Transformer; Ablation Therapy; Area Under The Curve; Article; Artificial Intelligence; Artificial Neural Network; Bioinformatics; Controlled Study; Convolutional Neural Network; Cross Validation; Deep Learning; Diagnostic Test Accuracy Study; Epilepsy; Gene Expression; Gene Ontology; Gene Sequence; Genome-wide Association Study; Human; K Nearest Neighbor; Learning Algorithm; Measurement Precision; Natural Language Processing; Prediction; Protein Language Model; Protein Protein Interaction; Random Forest; Receiver Operating Characteristic; Single Nucleotide Polymorphism; Support Vector Machine; Synaptic Transmission; Whole Exome Sequencing},
	keywords = {ablation therapy; area under the curve; Article; artificial intelligence; artificial neural network; bioinformatics; controlled study; convolutional neural network; cross validation; deep learning; diagnostic test accuracy study; epilepsy; gene expression; gene ontology; gene sequence; genome-wide association study; human; k nearest neighbor; learning algorithm; measurement precision; natural language processing; prediction; protein language model; protein protein interaction; random forest; receiver operating characteristic; single nucleotide polymorphism; support vector machine; synaptic transmission; whole exome sequencing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Sion2025534,
	author = {Sion, Laurens and Van Landuyt, Dimitri and Joosen, Wouter},
	title = {Leveraging the Domain Experts: Specializing Privacy Threat Knowledge},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15263 LNCS},
	pages = {534 - 541},
	doi = {10.1007/978-3-031-82349-7_33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002460621&doi=10.1007%2F978-3-031-82349-7_33&partnerID=40&md5=9568837b6e0122f7fe2b2dcb98e39266},
	abstract = {The design and development of privacy-preserving software systems remains a challenging endeavor, especially with the wide-spread adoption of potentially privacy-harmful technologies such as ML/AI, LLMs, telemetry, etc. Current privacy threat knowledge consolidation efforts mainly focus on the ontological generalization of threat knowledge. The generic encoding of privacy threat knowledge is useful for increasing overall awareness of the diversity and scope of privacy threats and promoting broader application of privacy threat analysis. However, it also inhibits reuse of threat knowledge that is more tailored to the organization context or application domain. There is thus an emerging need to encode, manage, and share specialized privacy threat knowledge that may be more domain-, technology-, or organization-specific. In this position paper, we outline a vision and roadmap towards improved support for the overall management of privacy threat knowledge, and particularly we envision advanced knowledge modeling support for capturing specialized threat knowledge, supporting evolution, customization, and reuse. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Anonymity; Domain Knowledge; Privacy-preserving Techniques; 'current; Design And Development; Domain Experts; Encodings; Generalisation; Privacy Preserving; Privacy Threats; Reuse; Software-systems; Wide Spreads; Differential Privacy},
	keywords = {Anonymity; Domain Knowledge; Privacy-preserving techniques; 'current; Design and Development; Domain experts; Encodings; Generalisation; Privacy preserving; Privacy threats; Reuse; Software-systems; Wide spreads; Differential privacy},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Desai2025,
	author = {Desai, Arun and Kulkarni, Anagha},
	title = {Unleashing the Potential of Ontology in Skill Extraction},
	year = {2025},
	pages = {},
	doi = {10.1109/ICAET63349.2025.10932270},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002444956&doi=10.1109%2FICAET63349.2025.10932270&partnerID=40&md5=7af1c210443a2102ec5a3f838fb63953},
	abstract = {Research reveals ontology's ability to extract information about skills from online job sites by giving a structured and semantically rich representation of skills. The study talks about more accurate and thorough skill profiling by systematically building ontological models that allow for an indepth knowledge of the complex links between skills, abilities, and domains. In this research, we study papers from different methods like supervised learning, unsupervised learning, and LLMs. The paper begins by providing not only an overview what is new in ontology generation but also its application in the context of skill extraction. It then delves into the challenges and opportunities associated with ontology-based skill extraction, highlighting the ways of processing natural language in bridging the gap between unstructured text and the formal representation of skills and competencies. Furthermore, the paper presents a comprehensive framework for ontology-driven skill extraction, emphasizing the importance of contextual awareness and the identification of implicit skills that may not be explicitly stated in the source text. The potential implications of this approach are manifold, as it could significantly impact various aspects of the talent management ecosystem. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Jobs; Machine Learning; Ontology; Skill Extraction; Skills; Contrastive Learning; Federated Learning; Ontology; Self-supervised Learning; Unsupervised Learning; Extract Informations; In-depth Knowledge; Job; Machine-learning; Online Job Sites; Ontological Modeling; Ontology Generation; Ontology's; Skill; Skill Extractions; Adversarial Machine Learning},
	keywords = {Contrastive Learning; Federated learning; Ontology; Self-supervised learning; Unsupervised learning; Extract informations; In-depth knowledge; Job; Machine-learning; Online job sites; Ontological modeling; Ontology generation; Ontology's; Skill; Skill extractions; Adversarial machine learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Abolhasani2025,
	author = {Abolhasani, Mohammad Sadeq and Pan, Rong},
	title = {OntoKGen: A Genuine Ontology and Knowledge Graph Generator using Large Language Model},
	year = {2025},
	journal = {Proceedings - Annual Reliability and Maintainability Symposium},
	pages = {},
	doi = {10.1109/RAMS48127.2025.10935139},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002271575&doi=10.1109%2FRAMS48127.2025.10935139&partnerID=40&md5=a1dda94eab8edc38426e8bfe96221f84},
	abstract = {Extracting relevant and structured knowledge from large, complex technical documents within the Reliability and Maintainability (RAM) domain is labor-intensive and prone to errors. Our work addresses this challenge by presenting OntoKGen, a Genuine pipeline for Ontology extraction and Knowledge Graph (KG) generation. OntoKGen leverages Large Language Models (LLMs) through an interactive user interface guided by our adaptive iterative Chain of Thought (CoT) algorithm to ensure that the ontology extraction process and, thus, KG generation align with user-specific requirements. Although KG generation follows a clear, structured path based on the confirmed ontology, there is no universally correct ontology as it is inherently based on the user’s preferences. OntoKGen recommends an ontology grounded in best practices, minimizing user effort and providing valuable insights that may have been overlooked, all while giving the user complete control over the final ontology. Having generated the KG based on the confirmed ontology, OntoKGen enables seamless integration into schemeless, non-relational databases like Neo4j. This integration allows for flexible storage and retrieval of knowledge from diverse, unstructured sources, facilitating advanced querying, analysis, and decision-making. Moreover, the generated KG serves as a robust foundation for future integration into Retrieval-Augmented Generation (RAG) systems, offering enhanced capabilities for developing domain-specific intelligent applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Model; Neo4j; Ontology And Knowledge Graph Generator; Prompt Engineering; Ontology; Query Languages; Relational Database Systems; Structured Query Language; Graph Generation; Knowledge Graphs; Language Model; Large Language Model; Neo4j; Ontology And Knowledge Graph Generator; Ontology Extraction; Ontology Graphs; Ontology's; Prompt Engineering; Knowledge Graph},
	keywords = {Ontology; Query languages; Relational database systems; Structured Query Language; Graph generation; Knowledge graphs; Language model; Large language model; Neo4j; Ontology and knowledge graph generator; Ontology Extraction; Ontology graphs; Ontology's; Prompt engineering; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2025,
	author = {Liu, Xi and Zheng, Xiaohu and Cui, Weiwei},
	title = {Psychological Counseling with Integration of Knowledge Graph and Multi-Agent Collaboration; 面向心理咨询的改进知识图谱融合多智能体协作方法},
	year = {2025},
	journal = {Journal of Shanghai Jiaotong University (Science)},
	pages = {},
	doi = {10.1007/s12204-024-2785-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002241637&doi=10.1007%2Fs12204-024-2785-1&partnerID=40&md5=0425160ae23dd5a2e923a2131cb4ce79},
	abstract = {Applying large language models (LLMs) to psychological counseling can reduce costs and lower barriers to access. However, issues inherent to LLMs, such as output hallucinations, knowledge lags, and response biases, limit the quality of counseling services. To address these challenges, we propose a psychological counseling method that integrates a knowledge graph (KG) and multi-agent collaboration (MAC). First, we design an ontology model and use an improved convolution residual network (CRN) to extract relational triples for constructing a psychological medicine KG. Second, we perform low-rank adaptation (LoRA) on Qwen1.5-7B-chat using the Text2Cypher training set. Finally, we design MAC prompts based on the characteristics of the retrieval-based question-answering (QA) process to facilitate the collaborative implementation of psychological medicine knowledge QA and updates. The experiments showed that the improved CRN achieved an extraction accuracy of 80%. The mean squared error of the general-domain capability of the LLM before and after LoRA was only 0.738, with a Text2Cypher score of 99.8 after LoRA. The results generated after prompting received a 70.44% approval rate compared to other methods. Thus, the proposed method can effectively enhance the intelligence, accuracy, and controllability of psychological counseling. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {A; Convolution Residual Network (crn); Knowledge Graph (kg); Low-rank Adaptation (lora); Multiagent; Psychological Counseling; R749; Tp18; Chatbots; Medicinal Chemistry; Multi Agent Systems; Ontology; A; Agent Collaboration; Convolution Residual Network; Knowledge Graph; Knowledge Graphs; Low-rank Adaptation; Multi Agent; Psychological Counseling; R749; Tp18},
	keywords = {Chatbots; Medicinal chemistry; Multi agent systems; Ontology; A; Agent collaboration; Convolution residual network; Knowledge graph; Knowledge graphs; Low-rank adaptation; Multi agent; Psychological counseling; R749; Tp18},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {3rd International Conference on Language Processing and Knowledge Management, LPKM 2024},
	year = {2025},
	journal = {Lecture Notes in Networks and Systems},
	volume = {1303 LNNS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002011373&partnerID=40&md5=b67598e8b49d1450f72c99f8268a4994},
	abstract = {The proceedings contain 29 papers. The special focus in this conference is on Language Processing and Knowledge Management. The topics include: Machine Learning Serving the School Orientation Process; improving Arabic Fake News Detection Across Context-Aware Attention Deep Model Based on Natural Language Processing; towards a Maude-Based Approach for Formal Modeling Deep Neural Networks; Anti-pattern Based IoRT-Aware Business Process Structure Verification Approach; opinion Analysis Based on a Sentiment Lexical Ontology and Deep Learning Models: Tunisian Dialect Case; disfluent-to-Fluent Tunisian Dialect Speech Translation with Fine-Tuning Pre-trained Language Models; BERT-Based Model for Sarcasm Detection in Arabic Texts; typology of Event Data Imperfections; towards Sentiment Analysis for Libyan Dialect; text Categorization Can Enhance Domain-Agnostic Stopword Extraction; normalized Orthography for Tunisian Arabic; deep Learning Approach for Early Prediction of Depression on Social Network; adapting Large Language Models to Biomedical Domain: A Survey of Techniques and Approaches; tunisian Arabic Understanding: Resources Analysis and Evaluation; from Data to Decisions: An Ontology-Driven Method for Opinion Mining; LLMs for Cyberbullying Detection in Political Social Media; Assessing BERT Models for Arabic Named Entity Recognition in a Multi-dialectal Context; tunisian Normalized Pronunciation; traffScOnto: Ontology for Traffic Management in the Context of Smart City Domain; SERTUS Dataset Collection from Spontaneous Environments; an Agricultural Sentiment Dataset for Pest Control and Crop Diseases; deep Learning Approach to Identify and Classify Arabic Verbal Multi-word Expressions; a Design Pattern-Based Approach for Analyzing MapReduce Applications; analyzing the Impact of Big Data in Mental Health; The Impact of AI on Knowledge Management; a Rule-Based System for Translating Libyan Dialect Dual Forms to Modern Standard Arabic. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Gauthier2025383,
	author = {Gauthier, Jean Marie and Jenn, Eric and Conejo, Ramon},
	title = {Ontology-Driven LLM Assistance for Task-Oriented Systems Engineering},
	year = {2025},
	journal = {International Conference on Model-Driven Engineering and Software Development},
	volume = {1},
	pages = {383 - 394},
	doi = {10.5220/0013441100003896},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001852863&doi=10.5220%2F0013441100003896&partnerID=40&md5=09216a90bdbc5c07ea2223c1c1058979},
	abstract = {This paper presents an LLM-based assistant integrated within an experimental modelling platform to support Systems Engineering tasks. Leveraging an ontology-driven approach, the assistant guides engineers through Systems Engineering tasks using an iterative prompting technique that builds task-specific context from prior steps. Our approach combines prompt engineering, few-shot learning, Chain of Thought reasoning, and Retrieval-Augmented Generation to generate accurate and relevant outputs without fine-tuning. A dual-chatbot system aids in task completion. The evaluation of the assistant’s effectiveness in the development of a robotic system demonstrates its potential to enhance Systems Engineering process efficiency and support decision-making. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Agent; Large Language Model; Modelling Assistant; Ontology; Systems Engineering; Systems Modelling},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ismail20251201,
	author = {Ismail, Rohana Binti and Makhtar, Mokhairi and Hasan, Hasni and Zamri, Nurnadiah Z.A.B.N.M. and Azizan, Azilawati Binti},
	title = {A Comparative Evaluation of Ontology Learning Techniques in the Context of the Qur’an},
	year = {2025},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {16},
	number = {3},
	pages = {1201 - 1209},
	doi = {10.14569/IJACSA.2025.01603115},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001634948&doi=10.14569%2FIJACSA.2025.01603115&partnerID=40&md5=486bcfc5c0bdeff61b986055e26fd2c4},
	abstract = {Ontology Learning refers to the automatic or semiautomatic process of creating ontologies by extracting terms, concepts, and relationships from text written in natural languages. This process is essential, as manually building ontologies is time-consuming and labour-intensive. The Qur’an, a vast source of knowledge for Muslims, presents linguistic and cultural complexities, with many words carrying multiple meanings depending on context. Ontologies offer a structured way to represent this knowledge, linking concepts systematically. Although various ontologies have been developed from the Qur’an for purposes such as advanced querying and analysis, most rely on manual creation methods. Few studies have examined the use of Ontology Learning for Qur’anic ontologies. Thus, this study evaluates three Ontology Learning techniques: Named Entity Recognition (NER), statistical methods, and Quranic patterns. The NER aims to find names represented by entity, statistical techniques aimed at finding frequently occurring words, and pattern-based techniques aim to identify complex relationships and multi-word expressions. The Ontology Learning techniques were evaluated based on precision, recall, and F-measure to assess extraction accuracy. The NER technique achieved an average precision of 0.62, statistical methods of 0.45, and pattern-based techniques of 0.58, indicating the strengths and weaknesses of each approach for extracting relevant terms as concepts, instances, or relations. This indicates that improvements or enhancements to the existing techniques are necessary for more accurate results. Future work will focus on refining or adapting patterns based on the structure of the Qur’an translation using LLMs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Hajj; Ner; Ontology Learning; Pattern-based; Qur’an; Statistical; Adversarial Machine Learning; Federated Learning; Ontology; Structured Query Language; Comparative Evaluations; Hajj; Learning Techniques; Named Entity Recognition; Ontology Learning; Ontology's; Pattern-based; Qur’an; Statistical; Term Relationship; Contrastive Learning},
	keywords = {Adversarial machine learning; Federated learning; Ontology; Structured Query Language; Comparative evaluations; Hajj; Learning techniques; Named entity recognition; Ontology learning; Ontology's; Pattern-based; Qur’an; Statistical; Term relationship; Contrastive Learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{de Giorgis2025121,
	author = {de Giorgis, Stefano and Lazzari, Nicolas},
	title = {Neuro-Symbolic Classification of Basic Human Values},
	year = {2025},
	journal = {Lecture Notes in Computer Science},
	volume = {15356 LNAI},
	pages = {121 - 133},
	doi = {10.1007/978-3-031-85463-7_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001356138&doi=10.1007%2F978-3-031-85463-7_8&partnerID=40&md5=27644865dee7e4091c26a57c88aedd0e},
	abstract = {This work explores the integration of ontology-based reasoning and Machine Learning techniques for explainable classification in the domain of moral and cultural values. By relying on an ontological formalization of moral values as in the Basic Human Values theory, which is based on the Description and Situation Ontology Design Pattern, the sandra neuro-symbolic reasoner is used to infer values (formalized as descriptions) that are satisfied by a certain sentence. Sentences, alongside their structured representation, are automatically generated using an open-source Large Language Model. The inferred descriptions are used to automatically detect the value associated with a sentence. We show that only relying on the reasoner’s inference results in explainable classification comparable to other more complex approaches. Moreover, we test how the LLMs tacit knowledge can be exploited to obtain novel formalizations of the domain. We show that combining the reasoner’s inferences with simple distributional semantics methods largely outperforms all the baselines, including complex models based on neural network architectures. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graphs; Moral Values; Neuro-symbolic Ai; Ontology; Adversarial Machine Learning; Ontology; Semantics; Based Reasonings; Formalisation; Human Values; Knowledge Graphs; Moral Value; Neuro-symbolic Ai; Ontology's; Ontology-based; Reasoners; Symbolic Classifications; Knowledge Graph},
	keywords = {Adversarial machine learning; Ontology; Semantics; Based reasonings; Formalisation; Human values; Knowledge graphs; Moral value; Neuro-symbolic AI; Ontology's; Ontology-based; Reasoners; Symbolic classifications; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2025347,
	author = {Liu, Boyang and Liu, Guoliang and Tian, Guohui and Jiang, Jian and Wang, Shanmei},
	title = {Task Reasoning of Service Robots with Fused Multi-modal Information and Ontology Knowledge},
	year = {2025},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {1326 LNEE},
	pages = {347 - 357},
	doi = {10.1007/978-981-96-2468-3_30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001293420&doi=10.1007%2F978-981-96-2468-3_30&partnerID=40&md5=c57516184ac6f11d440627db0b77d3ae},
	abstract = {Family environment is one of main area for service robot applications. Complex multi-modal scene and users’ high requirements for service levels are challenges that service robots face. In order to improve the service level of service robots in multi-modal family scene, this paper proposes a service robot task reasoning mechanism that integrates multi-modal information and ontology knowledge. The system manages user and environment information by ontology knowledge base, obtains fused multi-modal information including vision, voice, and scene knowledge in real time, and reasons service task based on fine-tuned LLM (Large Language Model). The system is designed based on the edge-cloud collaborative architecture and has high service levels in experimental tests of multi-modal family elderly care scene. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Model; Multi-modal; Ontology Technology; Service Robot; Service Task Reasoning; Industrial Robots; Multipurpose Robots; Ontology; Language Model; Large Language Model; Multi-modal; Multi-modal Information; Ontology Technology; Ontology's; Robot Face; Service Levels; Service Robots; Service Task Reasoning; Robot Applications},
	keywords = {Industrial robots; Multipurpose robots; Ontology; Language model; Large language model; Multi-modal; Multi-modal information; Ontology technology; Ontology's; Robot face; Service levels; Service robots; Service task reasoning; Robot applications},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Holvoet2025224,
	author = {Holvoet, Laura and van Bekkum, Michael A. and de Vries, Aijse},
	title = {An Approach to Automated Instruction Generation with Grounding Using LLMs and RAG},
	year = {2025},
	journal = {Lecture Notes in Mechanical Engineering},
	pages = {224 - 233},
	doi = {10.1007/978-3-031-86489-6_23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001258485&doi=10.1007%2F978-3-031-86489-6_23&partnerID=40&md5=64e465d63efd4e7275aab7ee26b0d53f},
	abstract = {Despite ongoing digitization in industry, many companies still work with paper instructions or ‘paper-on-glass’ solutions (e.g., PDF files on screens). In recent years, various digital work instruction (DWI) technologies have become available that provide shop-floor employees with information during their activities, e.g., sequences of instructions for tasks at hand. Engineering new instructions in these systems for new products or product variants is however expensive and time-consuming. To scale up, there is a need for methods to generate work instructions (semi) automatically. Recently, Generative AI models and Large Language Models (LLMs) have taken center stage with their abilities to interact fluently with humans, both in understanding user questions/statements and in convincingly producing natural language texts. These models however suffer from several problems, including hallucinations where unsubstantiated content is presented as facts and lack of domain-specific data about products and procedures. For instruction generation however, we need verifiably correct statements about the task at hand. To tackle both problems, we have created a pipeline that combines the generative abilities of LLMs with explicit domain-specific data. We deploy a variant of Retrieval Augmented Generation (RAG) and incorporate an ontology that augments the instructions with additional information (policies, warnings, tools). Our results show an increase in correctness of output. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Generative Ai; Instruction Generation; Large Language Models; Manufacturing; Pipeline; Rag; Competition; Critical Path Analysis; Crushed Stone Plants; Distributed Database Systems; Ergonomics; Glass Industry; Glass Manufacture; Metadata; Natural Language Processing Systems; Ontology; Paper And Pulp Industry; Problem Oriented Languages; Digitisation; Domain Specific; Generative Ai; Instruction Generations; Language Model; Large Language Model; Pdf Files; Retrieval Augmented Generation; Shopfloors; Work Instructions; Pipelines},
	keywords = {Competition; Critical path analysis; Crushed stone plants; Distributed database systems; Ergonomics; Glass industry; Glass manufacture; Metadata; Natural language processing systems; Ontology; Paper and pulp industry; Problem oriented languages; Digitisation; Domain specific; Generative AI; Instruction generations; Language model; Large language model; PDF files; Retrieval augmented generation; Shopfloors; Work instructions; Pipelines},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Massai2025,
	author = {Massai, Lorenzo and Marinai, Simone},
	title = {An Integrated System for Interacting with Multi-Page Scholarly Documents},
	year = {2025},
	journal = {CEUR Workshop Proceedings},
	volume = {3937},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001126693&partnerID=40&md5=c5940f84975ea3e8780b21f05a2ba7fe},
	abstract = {In this work we present a preliminary version of a comprehensive interface for supporting users to interact with scholarly documents, enabling multi-layered exploration and offering deeper insights by integrating diverse features and contextual information. By bridging diverse information our work pursues the identification, characterization, and linking of visual elements to semantic and context data, leveraging large language models for interoperability. Recent advances in retrieval augmented generation are also exploited to address some language models limitations, allowing them to access latent information from document representations such as graph and vector embeddings. The system under development performs an analysis of input documents and enables the extraction of visual and semantic features, making them accessible in a comprehensive framework. The association of structural information to visual data allows formal analysis of documents and is exploited in our model to enhance visual extraction, performing a novel ontology-based constraint violation detection. The information extracted through this framework is semantically explorable, providing access to the document structure, which can be exploited in many applications like question answering and document understanding. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Conversational Agents; Document Layout Analysis; Document Understanding; Large Language Models; Linked Data; Multi-modal Feature Extraction; Natural Language Processing; Question Answering; Retrieval Augmented Generation; Scholarly Document Processing; Text Mining; Chatbots; Data Assimilation; Natural Language Processing Systems; Question Answering; Semantics; Spatio-temporal Data; Conversational Agents; Document Layout Analysis; Document Understanding; Document-processing; Features Extraction; Language Model; Language Processing; Large Language Model; Multi-modal; Multi-modal Feature Extraction; Natural Language Processing; Natural Languages; Question Answering; Retrieval Augmented Generation; Scholarly Document Processing; Text-mining; Latent Semantic Analysis},
	keywords = {Chatbots; Data assimilation; Natural language processing systems; Question answering; Semantics; Spatio-temporal data; Conversational agents; Document layout analysis; Document understanding; Document-processing; Features extraction; Language model; Language processing; Large language model; Multi-modal; Multi-modal feature extraction; Natural language processing; Natural languages; Question Answering; Retrieval augmented generation; Scholarly document processing; Text-mining; Latent semantic analysis},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2025,
	title = {10th Balkan Conference in Informatics, BCI 2024},
	year = {2025},
	journal = {Communications in Computer and Information Science},
	volume = {2391 CCIS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000827692&partnerID=40&md5=e3496fd6496d2a672a343b75ba086821},
	abstract = {The proceedings contain 23 papers. The special focus in this conference is on Informatics. The topics include: Unveiling the Potential of Explainable Artificial Intelligence in Predictive Modeling, Exploring Food Security and Nutrition in Madagascar; Comparison of Methodological Approaches: CRISP-DM vs OSEMN Methodology Using Linear Regression and Statistical Analysis; processing Range-Sum Queries Within Triangular Shapes; microservice-Based Distributed Application for Unifying Social Networks; towards an Ontology for Describing the Contextualization of Written Cultural Heritage in the Light of Semiotics; affective Analysis of Literature Books - Detective Novels and Short Stories of Agatha Christie; preventing Academic Dishonesty Originating from Large Language Models; a Gamified Approach for Monitoring Student Attendance and Fostering Learner Engagement: AttendanceManager Platform; personalized Online Learning Based on IoT (Systematic Mapping Study); Student Perspectives on Generative AI Use in Higher Education: The Automation and Augmentation of Learning; leveraging Fitness Functions to Assess Cloud Migration Readiness of Clean Architecture; resource Management in Cloud IaaS via Machine Learning Algorithms; blockchain-Based Decentralised Marketplace for Secure Trading of Intellectual Property; an Intelligent Agent-Based Approach to Enhancing Urban Health and Resilience under Climate Change; MAS-PatientCare: Medical Diagnosis and Patient Management System Based on a Multi-agent Architecture; Microsoft Copilot, Google Gemini, OpenAI ChatGPT and Anthropic Claude: Assisting in Cardiology Diagnosing - A Case Study; enhancing Web Accessibility Through a Customizable Browser Extension for Users with Visual Impairments; time Traveller - A RotSafeLinker Tool; shareFactory - Efficient and Accessible Content-Sharing Platform; steganalysis: A Study on Compression Attack, Methods and Transit Resilience; a Practical Guide for Application Security Baselines in the Software Development Process. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Xiang2025133,
	author = {Xiang, Yunfei and Lin, Peng and Luo, Yiming and Xu, Houlei and Ning, Zeyu and Qiao, Yu and Zhou, Mengxia},
	title = {A Knowledge Graph-Based Approach for Construction Safety Hazards Management and Rectification Measures Intelligent Recommendation},
	year = {2025},
	journal = {Lecture Notes in Civil Engineering},
	volume = {630 LNCE},
	pages = {133 - 141},
	doi = {10.1007/978-3-031-84224-5_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000670719&doi=10.1007%2F978-3-031-84224-5_10&partnerID=40&md5=5fd09180629d082b432de3a2b2457b35},
	abstract = {Identification and rectification of safety hazards are crucial for the safety management of a construction site. However, the absence of structured knowledge necessitates a heavy reliance on experienced managers or experts for construction safety management. This dependence on manual formulation of rectification measures results in inefficient safety management and underutilization of data value. This study proposes a knowledge graph based approach for the safety hazards inspections and recommendation of rectification measures, which consists of four steps: (1) constructing the semantic expression framework of safety hazard ontology for information extraction; (2) extracting entities from textual and semi-structured data using a large language model (ChatGLM-6B); (3) knowledge fusion and inference; (4) storing structured knowledge and developing semantic retrieval and safety hazard rectification measures recommendation. The proposed method was validated in a hydropower project, where a knowledge graph of safety hazards was constructed, consisting of 108,000 entities and 121 relationships. The results demonstrate that this method can automatically provide targeted rectification measures for safety hazards such as fire, falls from heights, scaffold collapse and electric shock. This work can provide a practical reference for improving the effectiveness of the construction project safety management. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Intelligent Recommendation; Knowledge Graph; Safety Hazards; Safety Management; Information Management; Project Management; Scaffolds; Semantics; Construction Safety; Construction Sites; Data Values; Graph-based; Hazard Management; Intelligent Recommendation; Knowledge Graphs; Safety Hazards; Safety Management; Structured Knowledge; Knowledge Graph},
	keywords = {Information management; Project management; Scaffolds; Semantics; Construction safety; Construction sites; Data values; Graph-based; Hazard management; Intelligent recommendation; Knowledge graphs; Safety hazards; Safety management; Structured knowledge; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Polat2025,
	author = {Polat, Fina and Tiddi, Ilaria and Groth, Paul T.},
	title = {Testing prompt engineering methods for knowledge extraction from text},
	year = {2025},
	journal = {Semantic Web},
	pages = {},
	doi = {10.3233/SW-243719},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000556870&doi=10.3233%2FSW-243719&partnerID=40&md5=6684b5fdcee0b30d76c95548d1173fbe},
	abstract = {The capabilities of Large Language Models (LLMs,) such as Mistral 7B, Llama 3, GPT-4, present a significant opportunity for knowledge extraction (KE) from text. However, LLMs’ context-sensitivity can hinder obtaining precise and task-aligned outcomes, thereby requiring prompt engineering. This study explores the efficacy of five prompt methods with different task demonstration strategies across 17 different prompt templates, utilizing a relation extraction dataset (RED-FM) with the aforementioned LLMs. To facilitate evaluation, we introduce a novel framework grounded in Wikidata’s ontology. The findings demonstrate that LLMs are capable of extracting a diverse array of facts from text. Notably, incorporating a simple instruction accompanied by a task demonstration – comprising three examples selected via a retrieval mechanism – significantly enhances performance across Mistral 7B, Llama 3, and GPT-4. The effectiveness of reasoning-oriented prompting methods such as Chain-of-Thought, Reasoning and Acting, while improved with task demonstrations, does not surpass alternative methods. This suggests that framing extraction as a reasoning task may not be necessary for KE. Notably, task demonstrations leveraging examples selected via retrieval mechanisms facilitate effective knowledge extraction across all tested prompting strategies and LLMs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Generative Knowledge Extraction; Gpt-4; Llama-3; Mistral 7b; Ontology Based Evaluation; Prompt Engineering; Wikidata},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Bassiouny2025,
	author = {Bassiouny, Abdelrhman M. and Elsayed, Ahmed Hassan and Falomir, Zoe and del Pobil, Angel P.},
	title = {UJI-Butler: A Symbolic/Non-symbolic Robotic System that Learns Through Multi-modal Interaction},
	year = {2025},
	journal = {International Journal of Social Robotics},
	pages = {},
	doi = {10.1007/s12369-025-01234-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000484187&doi=10.1007%2Fs12369-025-01234-5&partnerID=40&md5=f2bd6ab59427b8b7a99babbcd5c51701},
	abstract = {This paper introduces UJI-Butler, an innovative multi-robot framework that blends symbolic and non-symbolic artificial intelligence methods. Unlike previous systems, UJI-Butler integrates large language models (LLMs) with a knowledge base akin to RAG-based systems, while imposing logical reasoning on LLM-generated results. It facilitates multi-modal interaction with human users through speech, sign language, and physical interaction, fostering a human-in-the-loop learning paradigm. By acquiring new knowledge through verbal communication and mastering manipulation skills via human-lead-through programming, UJI-Butler enhances transparency and trust by incorporating human feedback during operations. Experimental results demonstrate that UJI-Butler’s combination of symbolic and non-symbolic AI offers intuitive interaction and accelerates the learning process with experience. It adeptly stores and utilizes knowledge gained from verbal communication, recognizing hand gestures for requests. Additionally, UJI-Butler successfully performs user-taught physical skills and generalizes them to varying object sizes and locations. The explicit nature of acquired knowledge enables seamless transferability to other platforms and modification by human users. The code of the whole project is available on Github, in addition, video demonstrations of the UJI-Butler system are available online in a Youtube Playlist. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cognitive Robotics; Collaborative Robotics; Human-robot Interaction; Knowledge Bases; Large Language Models; Lead-through-programming; Life-long Learning; Machine Learning; Multi-robot; Ontology; Reasoning; Sign Language; Symbolic Ai; Collaborative Robots; Educational Robots; Intelligent Robots; Knowledge Acquisition; Ontology; Robot Learning; Robot Programming; Cognitive Robotics; Humans-robot Interactions; Knowledge Base; Language Model; Large Language Model; Lead-through-programming; Life Long Learning; Machine-learning; Multirobots; Ontology's; Reasoning; Sign Language; Symbolic Ai; Multipurpose Robots},
	keywords = {Collaborative robots; Educational robots; Intelligent robots; Knowledge acquisition; Ontology; Robot learning; Robot programming; Cognitive robotics; Humans-robot interactions; Knowledge base; Language model; Large language model; Lead-through-programming; Life long learning; Machine-learning; Multirobots; Ontology's; Reasoning; Sign language; Symbolic AI; Multipurpose robots},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Hier2025,
	author = {Hier, Daniel B. and Do, Thanh Son and Obafemi-Ajayi, Tayo},
	title = {A simplified retriever to improve accuracy of phenotype normalizations by large language models},
	year = {2025},
	journal = {Frontiers in Digital Health},
	volume = {7},
	pages = {},
	doi = {10.3389/fdgth.2025.1495040},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000479162&doi=10.3389%2Ffdgth.2025.1495040&partnerID=40&md5=73679ab1cf57608407d6f8b55b714a95},
	abstract = {Large language models have shown improved accuracy in phenotype term normalization tasks when augmented with retrievers that suggest candidate normalizations based on term definitions. In this work, we introduce a simplified retriever that enhances large language model accuracy by searching the Human Phenotype Ontology (HPO) for candidate matches using contextual word embeddings from BioBERT without the need for explicit term definitions. Testing this method on terms derived from the clinical synopses of Online Mendelian Inheritance in Man (OMIM<sup>®</sup>), we demonstrate that the normalization accuracy of GPT-4o increases from a baseline of 62% without augmentation to 85% with retriever augmentation. This approach is potentially generalizable to other biomedical term normalization tasks and offers an efficient alternative to more complex retrieval methods. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cosine Similarity; Hpo; Large Language Model; Omim; Phenotype Normalization; Retrievalaugmented Generation; Small Language Model; Accuracy; Article; Clinical Feature; Human; Hyporeflexia; Large Language Model; Natural Language Processing; Ontology; Phenotype; Prediction; Reproducibility; Visual Impairment},
	keywords = {accuracy; Article; clinical feature; human; hyporeflexia; large language model; natural language processing; ontology; phenotype; prediction; reproducibility; visual impairment},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Anderson2025,
	author = {Anderson, Lindsey N. and Hoyt, Charles Tapley and Zucker, Jeremy D. and McNaughton, Andrew D. and Teuton, Jeremy R. and Karis, Klas and Arokium-Christian, Natasha N. and Warley, Jackson T. and Stromberg, Zachary R. and Gyori, Benjamin M.},
	title = {Computational tools and data integration to accelerate vaccine development: challenges, opportunities, and future directions},
	year = {2025},
	journal = {Frontiers in Immunology},
	volume = {16},
	pages = {},
	doi = {10.3389/fimmu.2025.1502484},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000469104&doi=10.3389%2Ffimmu.2025.1502484&partnerID=40&md5=05ca30b10792e8a1c6c3c2763b83b756},
	abstract = {The development of effective vaccines is crucial for combating current and emerging pathogens. Despite significant advances in the field of vaccine development there remain numerous challenges including the lack of standardized data reporting and curation practices, making it difficult to determine correlates of protection from experimental and clinical studies. Significant gaps in data and knowledge integration can hinder vaccine development which relies on a comprehensive understanding of the interplay between pathogens and the host immune system. In this review, we explore the current landscape of vaccine development, highlighting the computational challenges, limitations, and opportunities associated with integrating diverse data types for leveraging artificial intelligence (AI) and machine learning (ML) techniques in vaccine design. We discuss the role of natural language processing, semantic integration, and causal inference in extracting valuable insights from published literature and unstructured data sources, as well as the computational modeling of immune responses. Furthermore, we highlight specific challenges associated with uncertainty quantification in vaccine development and emphasize the importance of establishing standardized data formats and ontologies to facilitate the integration and analysis of heterogeneous data. Through data harmonization and integration, the development of safe and effective vaccines can be accelerated to improve public health outcomes. Looking to the future, we highlight the need for collaborative efforts among researchers, data scientists, and public health experts to realize the full potential of AI-assisted vaccine design and streamline the vaccine development process. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Computational Methods; Correlates Of Protection; Data Harmonization; Knowledge Extraction; Large Language Models; Machine Learning; Vaccine Platform Technologies; Vaccines; Vaccine; Artificial Intelligence; Computer Model; Data Integration; Drug Development; Electric Potential; Health Outcome; Human; Immune Response; Immune System; Information Processing; Large Language Model; Machine Learning; Natural Language Processing; Review; Therapy; Vaccine Design; Vaccine Development; Animal; Bioinformatics; Immunology; Procedures; Animals; Artificial Intelligence; Computational Biology; Humans; Machine Learning; Natural Language Processing; Vaccine Development; Vaccines},
	keywords = {vaccine; artificial intelligence; computer model; data integration; drug development; electric potential; health outcome; human; immune response; immune system; information processing; large language model; machine learning; natural language processing; review; therapy; vaccine design; vaccine development; animal; bioinformatics; immunology; procedures; Animals; Artificial Intelligence; Computational Biology; Humans; Machine Learning; Natural Language Processing; Vaccine Development; Vaccines},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Kainer2025,
	author = {Kainer, David},
	title = {The effectiveness of large language models with RAG for auto-annotating trait and phenotype descriptions},
	year = {2025},
	journal = {Biology Methods and Protocols},
	volume = {10},
	number = {1},
	pages = {},
	doi = {10.1093/biomethods/bpaf016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000452266&doi=10.1093%2Fbiomethods%2Fbpaf016&partnerID=40&md5=933d46fd3f391f478b21a6a34c77258a},
	abstract = {Ontologies are highly prevalent in biology and medicine and are always evolving. Annotating biological text, such as observed phenotype descriptions, with ontology terms is a challenging and tedious task. The process of annotation requires a contextual understanding of the input text and of the ontological terms available. While text-mining tools are available to assist, they are largely based on directly matching words and phrases and so lack understanding of the meaning of the query item and of the ontology term labels. Large Language Models (LLMs), however, excel at tasks that require semantic understanding of input text and therefore may provide an improvement for the auto-annotation of text with ontological terms. Here we describe a series of workflows incorporating OpenAI GPT's capabilities to annotate Arabidopsis thaliana and forest tree phenotypic observations with ontology terms, aiming for results that resemble manually curated annotations. These workflows make use of an LLM to intelligently parse phenotypes into short concepts, followed by finding appropriate ontology terms via embedding vector similarity or via Retrieval-Augmented Generation (RAG). The RAG model is a state-of-the-art approach that augments conversational prompts to the LLM with context-specific data to empower it beyond its pre-trained parameter space. We show that the RAG produces the most accurate automated annotations that are often highly similar or identical to expert-curated annotations. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Annotation; Gpt; Large Language Model; Ontology; Phenotype; Rag},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{Sala2025,
	author = {Sala, Roberto and Francalanza, Emmanuel and Arena, Simone},
	title = {A review on three decades of manufacturing maintenance research: past, present and future directions},
	year = {2025},
	journal = {Production and Manufacturing Research},
	volume = {13},
	number = {1},
	pages = {},
	doi = {10.1080/21693277.2025.2469037},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000338116&doi=10.1080%2F21693277.2025.2469037&partnerID=40&md5=f20fbe72081c82ebb786de910b24aa3c},
	abstract = {Maintenance engineering has taken up more and more a strategic function in recent decades due to technological advancements and its role in asset productivity. Over time, a plethora of methods have been proposed, shifting from reactive approaches to complex, data-driven strategies focused on failure prediction (e.g. through Machine learning) and knowledge management (e.g. based on ontologies and large language models). The advancements achieved by maintenance have also beneficially impacted production quality, sustainability, and safety. This work presents the results of a systematic literature review of papers published on the topic of maintenance in the past 30 years. In particular, natural language processing has been used to analyze abstract, extract topics and, through further analysis delineate past, current, and future trends in the field of maintenance engineering in manufacturing. This work contributes to define a vision on how maintenance in manufacturing will evolve in the next future. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bertopic; Bibliometric Analysis; Maintenance; Manufacturing; Natural Language Processing; Quality Control; Asset Productivity; Bertopic; Bibliometrics Analysis; Complex Data; Data Driven; Failures Prediction; Language Processing; Natural Language Processing; Natural Languages; Technological Advancement; Strategic Planning},
	keywords = {Quality control; Asset productivity; Bertopic; Bibliometrics analysis; Complex data; Data driven; Failures prediction; Language processing; Natural language processing; Natural languages; Technological advancement; Strategic planning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{Krishnan2025145,
	author = {Krishnan, A. Aravind and Deepak, Gerard},
	title = {BDSLL: Biomedical Document Recommendation Using Knowledge-Empowered Semantic Intercepted Large Language Models},
	year = {2025},
	journal = {Lecture Notes in Networks and Systems},
	volume = {1073},
	pages = {145 - 153},
	doi = {10.1007/978-981-97-5703-9_11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000227034&doi=10.1007%2F978-981-97-5703-9_11&partnerID=40&md5=ea3f6dd85afb2faf8b1d483a99838a1e},
	abstract = {There is a requirement for document recommendation frameworks focusing on certain domains linked to medical sciences and biosciences like biomedical document recommendation in the era of the Web 3.0. This paper proposes a biomedical document recommendation strategy that realizes informative term generation using the encompassment of extraction of terms and categories and the TF-IDF. The framework also generates an ontology from the informative terms harvested from which features are selected to classify the dataset using a random forest classifier. The model also uses very strong knowledge bases that are highly specific to the domain like QIAGEN. It also uses WikiData which is generic but has a full cover of world knowledge to bridge the knowledge present in the datasets and the existing structure web. Metadata generation also increases the quantity of additional knowledge that is encompassed in the model. BioBERT is a strategic large language model that focuses on biosciences and biomedicine as its domain is used to classify the metadata to make it more permeable into the model. Semantic similarity is computed using NPMI, web overlap, SimRank, and Ricklefs and Lau index at different stages in the pipeline to facilitate quantitative reasoning through semantic relatedness computation. The salient semantic analysis also accelerates the overall semantic association into the proposed framework. This framework can demonstrate a precision of 96.56%, recall of 96.93%, and an FDR of 0.04. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Document; Document Recommendation; Large Language Model; Ontology; Ricklefs And Lau Index; Classification (of Information); Decision Trees; Domain Knowledge; Metadata; Recommender Systems; Semantics; Biomedical Documents; Document Recommendation; Informative Terms; Language Model; Large Language Model; Medical Science; Ontology's; Recommendation Strategies; Ricklefs And Lau Index; Web 3.0; Ontology},
	keywords = {Classification (of information); Decision trees; Domain Knowledge; Metadata; Recommender systems; Semantics; Biomedical documents; Document recommendation; Informative terms; Language model; Large language model; Medical science; Ontology's; Recommendation strategies; Ricklefs and lau index; Web 3.0; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2025108,
	author = {Liu, Qiaoyi and Qin, Jian},
	title = {The role of ontologies in machine learning: a case study of gene ontology},
	year = {2025},
	journal = {Information Research},
	volume = {30},
	number = {iConf 2025},
	pages = {108 - 122},
	doi = {10.47989/ir30iConf47575},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000135729&doi=10.47989%2Fir30iConf47575&partnerID=40&md5=a4705b1ec90f5d78ecef90040b72c3f1},
	abstract = {Introduction. Ontologies as knowledgebases have been heavily applied in computational biological studies by implementing into ML models for purposes such as disease-gene associations identification. Method. We conduct a case study using gene ontology (GO) annotation data and three ML models to replicate the prediction of autism spectrum disorder (ASD)-causing genes. Analysis. Data were collected from GO and Simmons Foundation Autism Research Initiative (SFARI). The semantic similarities between GO annotation terms on gene products were calculated. Results. The best-performing model can reach an AUC of .85, which means using GO annotation data for ASD disease-gene prediction can receive a significantly accurate result. However, we stress the importance of constructing knowledgebases in adapting to LLMs and the role of LIS professionals in curating community knowledge for interoperability and reuse. Conclusion. Biomedical ontologies play a crucial role in the discovery of biomedical knowledge. Knowledge organization and computer science domains require more communication and synchronization in the face of emerging AI and ML technologies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontology; Knowledge Organization Systems; Knowledgebases; Machine Learning; Ontologies},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Erlach20241295,
	author = {Erlach, Lena and Kuhn, Raphael and Agrafiotis, Andreas and Shlesinger, Danielle and Yermanos, Alexander Dimitri and Reddy, Sai T.},
	title = {Evaluating predictive patterns of antigen-specific B cells by single-cell transcriptome and antibody repertoire sequencing},
	year = {2024},
	journal = {Cell Systems},
	volume = {15},
	number = {12},
	pages = {1295 - 1303.e5},
	doi = {10.1016/j.cels.2024.11.005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211623221&doi=10.1016%2Fj.cels.2024.11.005&partnerID=40&md5=7a3b9ee4babd98600919f9208e4c047c},
	abstract = {The field of antibody discovery typically involves extensive experimental screening of B cells from immunized animals. Machine learning (ML)-guided prediction of antigen-specific B cells could accelerate this process but requires sufficient training data with antigen-specificity labeling. Here, we introduce a dataset of single-cell transcriptome and antibody repertoire sequencing of B cells from immunized mice, which are labeled as antigen specific or non-specific through experimental selections. We identify gene expression patterns associated with antigen specificity by differential gene expression analysis and assess their antibody sequence diversity. Subsequently, we benchmark various ML models, both linear and non-linear, trained on different combinations of gene expression and antibody repertoire features. Additionally, we assess transfer learning using features from general and antibody-specific protein language models (PLMs). Our findings show that gene expression-based models outperform sequence-based models for antigen-specificity predictions, highlighting a promising avenue for computationally guided antibody discovery. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Antibody Repertoire Sequencing; Antigen-specific B Cells; Antigen-specificity Prediction; B Cell Immune Response; Machine Learning For Antibody Discovery; Single-cell Sequencing Dataset; Single-cell Transcriptome Sequencing; Antibodies; Antigens; B Lymphocyte Antibody; B Lymphocyte Antigen; Transcriptome; Antibody; Antigen; Affinity Maturation; Animal Cell; Animal Experiment; Antibody Specificity; Antigen Specificity; Article; B Lymphocyte; Cell Fractionation; Cell Proliferation; Classification Algorithm; Classifier; Controlled Study; Cytokine Production; Differential Expression Analysis; Down Regulation; Female; Gene Expression Profiling; Gene Ontology; Genetic Variability; Kegg; Kernel Method; Language Model; Lymphocyte Activation; Machine Learning; Mouse; Nonhuman; Nonlinear System; Overlapping Gene; Prediction; Random Forest; Single Cell Rna Seq; Somatic Hypermutation; Stereochemistry; Support Vector Machine; Transfer Of Learning; Upregulation; Animal; Genetics; Immunology; Procedures; Single Cell Analysis; Animals; Antibodies; Antigens; B-lymphocytes; Machine Learning; Mice; Single-cell Analysis; Transcriptome},
	keywords = {B lymphocyte antibody; B lymphocyte antigen; transcriptome; antibody; antigen; affinity maturation; animal cell; animal experiment; antibody specificity; antigen specificity; Article; B lymphocyte; cell fractionation; cell proliferation; classification algorithm; classifier; controlled study; cytokine production; differential expression analysis; down regulation; female; gene expression profiling; gene ontology; genetic variability; KEGG; kernel method; language model; lymphocyte activation; machine learning; mouse; nonhuman; nonlinear system; overlapping gene; prediction; random forest; single cell RNA seq; somatic hypermutation; stereochemistry; support vector machine; transfer of learning; upregulation; animal; genetics; immunology; procedures; single cell analysis; Animals; Antibodies; Antigens; B-Lymphocytes; Machine Learning; Mice; Single-Cell Analysis; Transcriptome},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Song2024,
	author = {Song, Jihwan and Yoon, Sungmin},
	title = {Ontology-assisted GPT-based building performance simulation and assessment: Implementation of multizone airflow simulation},
	year = {2024},
	journal = {Energy and Buildings},
	volume = {325},
	pages = {},
	doi = {10.1016/j.enbuild.2024.114983},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208462359&doi=10.1016%2Fj.enbuild.2024.114983&partnerID=40&md5=93603af169e402fe4e5921a8648d5bc2},
	abstract = {Building performance simulation (BPS) is crucial for building performance assessments across its lifecycle. However, the complexity of buildings and the iterative nature of simulation poses challenges, leading to high costs and low values. Previous studies focused on simplification, but did not fully utilize advanced simulation engines. Despite recent advancements, there is a lack of research on leveraging artificial intelligence (AI), specifically generative pre-trained transformer (GPT), for BPS. Therefore, this study proposes a GPT-based BPS system, enhancing simulation efficiency and value by integrating simulation engines and advanced data analytics in the GPT environment. The ontology for GPT-based BPS is also developed to enable comprehensive, reliable, informative BPS environments. Based on this framework, case studies were conducted for GPT-based multizone airflow network simulation in a high-rise residential building using CONTAM software. They demonstrate GPT's capabilities in retrieving simulation data, visualizing results with data mining, answering questions based on building knowledge, checking compliance with design guidelines, and proposing design alternatives. Finally, this study emphasizes expert interventions with ontological engineering informatics to utilize strictly structured BPS engines. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Building Performance; Building Performance Simulation (bps); Chatgpt; Contam; Digital Twins; Gpt; Large Language Model (llm); Ontology; Building Performance; Building Performance Simulation; Building Performance Simulations; Chatgpt; Contam; Generative Pre-trained Transformer; Language Model; Large Language Model; Ontology's; Simulation Engine; Digital Elevation Model},
	keywords = {Ontology; Building performance; Building performance simulation; Building performance simulations; ChatGPT; CONTAM; Generative pre-trained transformer; Language model; Large language model; Ontology's; Simulation engine; Digital elevation model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Bomba2024,
	author = {Bomba, Federico and Menendez-Blanco, Maria and Grigis, Paolo and Cremaschi, Michele and De Angeli, Antonella},
	title = {The Choreographer-Performer Continuum: A Diffraction Tool to Illuminate Authorship in More Than Human Co-Performances},
	year = {2024},
	journal = {ACM Transactions on Computer-Human Interaction},
	volume = {31},
	number = {6},
	pages = {},
	doi = {10.1145/3689040},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214582521&doi=10.1145%2F3689040&partnerID=40&md5=ba0407cc61e8ab389239e21998eddf69},
	abstract = {The design of robust and trustworthy Generative AI (GenAI) requires a deep understanding of the agencies emerging from human interactions with them. To contribute to this goal, we retrospectively studied an art project involving a visual artist, a computer scientist, an artistic director, and a generative model (GPT-2). The model was fine-tuned with trip reports describing the experience of eating psychedelic mushrooms. Building on agential realism, we analysed the co-performance between the artist and the model as their agency moved along the choreographer-performer continuum. Results reveal ontological surprises, leading to the proposal of entangled authorship to de-individualise the production of knowledge from a More Than Human perspective. The paper illustrates how art can expose different forms of relationships, challenging the idea of GenAI as just a tool that simplifies or replaces human labour. We conclude by emphasising the transformational potential of GenAI for novel modes of engagement between humans and machines. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Agency; Agential Realism; Ai And Art; Creative Ai; Hallucination; Large Language Models; Continuum Mechanics; Psychoacoustic; Agency; Agential Realism; Ai And Art; Creative Ai; Creatives; Hallucination; Humaninteraction; Language Model; Large Language Model; Performance; Human Form Models},
	keywords = {Continuum mechanics; Psychoacoustic; Agency; Agential realism; AI and art; Creative AI; Creatives; Hallucination; Humaninteraction; Language model; Large language model; Performance; Human form models},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Bubnov2024521,
	author = {Bubnov, A. S. and Gallini, Nadezhda I. and Grishin, Igor Yu and Kobozeva, Irina M. and Loukachevitch, Natalia V. and Panich, M. B. and Raevsky, E. N. and Sadkovsky, F. A. and Timirgaleeva, Rena R.},
	title = {Ontologies As a Foundation for Formalization of Scientific Information and Extraction of New Knowledge},
	year = {2024},
	journal = {Doklady Mathematics},
	volume = {110},
	number = {3},
	pages = {521 - 527},
	doi = {10.1134/S106456242470234X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219613205&doi=10.1134%2FS106456242470234X&partnerID=40&md5=c2e406d5cbe0e85214511e8067d6023f},
	abstract = {Abstract: “Ark of Knowledge” is a digital project developed by Lomonosov Moscow State University. It provides access to fundamental knowledge in Russian and should play a key role in the preservation and dissemination of Russia’s cultural and scientific heritage. “Ark of Knowledge” is an ontological information system. The article discusses modern ideas about ontology, the stages of creation and ontological features of the Great Russian Encyclopedia and Wikidata, as well as the design of an information system and its use for language models training. The initial working prototype of this information system is briefly described. Work on creating the system is being carried out by researchers and programmers from the Knowledge Engineering Laboratory of the Institute for Mathematical Research of Complex Systems of Lomonosov Moscow State University, as well as researchers from the Faculties of Philology, Mechanics and Mathematics, Computational Mathematics and Cybernetics, and the Branch of Moscow State University in Sevastopol. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Fundamental Knowledge; Information System; Information System “ark Of Knowledge,” Great Russian Encyclopedia; Ontology; Ontology Design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Thrift2024,
	author = {Thrift, William John and Lounsbury, Nicolas W. and Broadwell, Quade and Heidersbach, Amy J. and Freund, Emily C. and Abdolazimi, Yassan and Phung, Qui T. and Chen, Jieming and Capietto, Aude Hélène and Tong, Ann Jay},
	title = {Towards designing improved cancer immunotherapy targets with a peptide-MHC-I presentation model, HLApollo},
	year = {2024},
	journal = {Nature Communications},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.1038/s41467-024-54887-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213799538&doi=10.1038%2Fs41467-024-54887-7&partnerID=40&md5=b38f678517420d86a113400e44844b60},
	abstract = {Based on the success of cancer immunotherapy, personalized cancer vaccines have emerged as a leading oncology treatment. Antigen presentation on MHC class I (MHC-I) is crucial for the adaptive immune response to cancer cells, necessitating highly predictive computational methods to model this phenomenon. Here, we introduce HLApollo, a transformer-based model for peptide-MHC-I (pMHC-I) presentation prediction, leveraging the language of peptides, MHC, and source proteins. HLApollo provides end-to-end treatment of MHC-I sequences and deconvolution of multi-allelic data, using a negative-set switching strategy to mitigate misassigned negatives in unlabelled ligandome data. HLApollo shows a 12.65% increase in average precision (AP) on ligandome data and a 4.1% AP increase on immunogenicity test data compared to next-best models. Incorporating protein features from protein language models yields further gains and reduces the need for gene expression measurements. Guided by clinical use, we demonstrate pan-allelic generalization which effectively captures rare alleles in underrepresented ancestries. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Amino Acid; Cancer Vaccines; Histocompatibility Antigens Class I; Peptides; Amino Acid; Major Histocompatibility Antigen Class 1; Peptide; Proteome; Cancer Vaccine; Hla Antigen Class 1; Allele; Cancer; Gene Expression; Immune Response; Immunity; Precision; Protein; Adaptive Immunity; Antigen Presentation; Article; Brain Cancer; Cancer Cell; Cancer Immunization; Cancer Immunotherapy; Cd8+ T Lymphocyte; Cell Culture; Cell Expansion; Cell Line; Cell Population; Colorectal Cancer; Controlled Study; Deconvolution; Epithelial Mesenchymal Transition; Feed Forward Neural Network; Flow Cytometry; Fluorescence Activated Cell Sorting; Gene Ontology; Genotype; Hla System; Human; Human Cell; Immunogenicity; Immunoprecipitation; Liquid Chromatography-mass Spectrometry; Major Histocompatibility Complex; Ovary Cancer; Peptidomics; Prediction; Predictive Model; Protein Language Model; Protein Processing; Rna Sequencing; Skin Cancer; Viral Gene Delivery System; Bioinformatics; Chemistry; Genetics; Immunology; Immunotherapy; Neoplasm; Procedures; Therapy; Alleles; Antigen Presentation; Cancer Vaccines; Computational Biology; Histocompatibility Antigens Class I; Humans; Immunotherapy; Neoplasms; Peptides},
	keywords = {amino acid; major histocompatibility antigen class 1; peptide; proteome; cancer vaccine; HLA antigen class 1; allele; cancer; gene expression; immune response; immunity; precision; protein; adaptive immunity; antigen presentation; Article; brain cancer; cancer cell; cancer immunization; cancer immunotherapy; CD8+ T lymphocyte; cell culture; cell expansion; cell line; cell population; colorectal cancer; controlled study; deconvolution; epithelial mesenchymal transition; feed forward neural network; flow cytometry; fluorescence activated cell sorting; gene ontology; genotype; HLA system; human; human cell; immunogenicity; immunoprecipitation; liquid chromatography-mass spectrometry; major histocompatibility complex; ovary cancer; peptidomics; prediction; predictive model; protein language model; protein processing; RNA sequencing; skin cancer; viral gene delivery system; bioinformatics; chemistry; genetics; immunology; immunotherapy; neoplasm; procedures; therapy; Alleles; Antigen Presentation; Cancer Vaccines; Computational Biology; Histocompatibility Antigens Class I; Humans; Immunotherapy; Neoplasms; Peptides},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Díaz Ochoa2024,
	author = {Díaz Ochoa, Juan G. and Mustafa, Faizan E. and Weil, Felix and Wang, Yi and Kama, Kudret and Knott, Markus},
	title = {The aluminum standard: using generative Artificial Intelligence tools to synthesize and annotate non-structured patient data},
	year = {2024},
	journal = {BMC Medical Informatics and Decision Making},
	volume = {24},
	number = {1},
	pages = {},
	doi = {10.1186/s12911-024-02825-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213549719&doi=10.1186%2Fs12911-024-02825-4&partnerID=40&md5=7082e4367bbc502b2ad10bb3eb2d37ff},
	abstract = {Background: Medical narratives are fundamental to the correct identification of a patient’s health condition. This is not only because it describes the patient’s situation. It also contains relevant information about the patient’s context and health state evolution. Narratives are usually vague and cannot be categorized easily. On the other hand, once the patient’s situation is correctly identified based on a narrative, it is then possible to map the patient’s situation into precise classification schemas and ontologies that are machine-readable. To this end, language models can be trained to read and extract elements from these narratives. However, the main problem is the lack of data for model identification and model training in languages other than English. First, gold standard annotations are usually not available due to the high level of data protection for patient data. Second, gold standard annotations (if available) are difficult to access. Alternative available data, like MIMIC (Sci Data 3:1, 2016) is written in English and for specific patient conditions like intensive care. Thus, when model training is required for other types of patients, like oncology (and not intensive care), this could lead to bias. To facilitate clinical narrative model training, a method for creating high-quality synthetic narratives is needed. Method: We devised workflows based on generative AI methods to synthesize narratives in the German language to avoid the disclosure of patient’s health data. Since we required highly realistic narratives, we generated prompts, written with high-quality medical terminology, asking for clinical narratives containing both a main and co-disease. The frequency of distribution of both the main and co-disease was extracted from the hospital’s structured data, such that the synthetic narratives reflect the disease distribution among the patient’s cohort. In order to validate the quality of the synthetic narratives, we annotated them to train a Named Entity Recognition (NER) algorithm. According to our assumptions, the validation of this system implies that the synthesized data used for its training are of acceptable quality. Result: We report precision, recall and F1 score for the NER model while also considering metrics that take into account both exact and partial entity matches. Trained models are cautious, with a precision up to 0.8 for Entity Type match metric and a F1 score of 0.3. Conclusion: Despite its inherent limitations, this technology has the potential to allow data interoperability by using encoded diseases across languages and regions without compromising data safety. Additionally, it facilitates the synthesis of unstructured patient data. In this way, the identification and training of models can be accelerated. We believe that this method may be able to generate discharge letters for any combination of main and co-diseases, which will significantly reduce the amount of time spent writing these letters by healthcare professionals. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Generative Ai; Language Models; Non-english Language Models; Synthetic Data; Synthetic Narratives; Artificial Intelligence; Human; Natural Language Processing; Verbal Communication; Artificial Intelligence; Humans; Narration; Natural Language Processing},
	keywords = {artificial intelligence; human; natural language processing; verbal communication; Artificial Intelligence; Humans; Narration; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@ARTICLE{Shah-Mohammadi20242173,
	author = {Shah-Mohammadi, Fatemeh and Finkelstein, Joseph},
	title = {Addressing Semantic Variability in Clinical Outcome Reporting Using Large Language Models},
	year = {2024},
	journal = {BioMedInformatics},
	volume = {4},
	number = {4},
	pages = {2173 - 2185},
	doi = {10.3390/biomedinformatics4040116},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213514159&doi=10.3390%2Fbiomedinformatics4040116&partnerID=40&md5=5ae6470cf0fb990e3ae1cbe7e3ee5a5b},
	abstract = {Background/Objectives: Clinical trials frequently employ diverse terminologies and definitions to describe similar outcomes, leading to ambiguity and inconsistency in data interpretation. Addressing the variability in clinical outcome reports and integrating semantically similar outcomes is important in healthcare and clinical research. Variability in outcome reporting not only hinders the comparability of clinical trial results but also poses significant challenges in evidence synthesis, meta-analysis, and evidence-based decision-making. Methods: This study investigates variability reduction in outcome measures reporting using rule-based and large language-based models. It aims to mitigate the challenges associated with variability in outcome reporting by comparing these two models. The first approach, which is rule-based, will leverage well-known ontologies, and the second approach exploits sentence-bidirectional encoder representations from transformers (SBERT) to identify semantically similar outcomes along with Generative Pre-training Transformer (GPT) to refine the results. Results: The results show that the relatively low percentages of outcomes are linked to established rule-based ontologies. Analysis of outcomes by word count highlighted the absence of ontological linkage for three-word outcomes, which indicates potential gaps in semantic representation. Conclusions: Employing large language models (LLMs), this study demonstrates its ability to identify similar outcomes, even with more than three words, suggesting a crucial role in outcome harmonization efforts, potentially reducing redundancy and enhancing data interoperability. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Outcome; Large Language Model; Ontology; Semantic Variability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Gao2024,
	author = {Gao, Zijing and Liu, Qiao and Zeng, Wanwen and Jiang, Rui and Wong, Winghung},
	title = {EpiGePT: a pretrained transformer-based language model for context-specific human epigenomics},
	year = {2024},
	journal = {Genome Biology},
	volume = {25},
	number = {1},
	pages = {},
	doi = {10.1186/s13059-024-03449-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212500671&doi=10.1186%2Fs13059-024-03449-7&partnerID=40&md5=b10f819ad7ce7569c847a8320d36ce04},
	abstract = {The inherent similarities between natural language and biological sequences have inspired the use of large language models in genomics, but current models struggle to incorporate chromatin interactions or predict in unseen cellular contexts. To address this, we propose EpiGePT, a transformer-based model designed for predicting context-specific human epigenomic signals. By incorporating transcription factor activities and 3D genome interactions, EpiGePT outperforms existing methods in epigenomic signal prediction tasks, especially in cell-type-specific long-range interaction predictions and genetic variant impacts, advancing our understanding of gene regulation. A free online prediction service is available at http://health.tsinghua.edu.cn/epigept. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {3d Genome; Epigenomics; Gene Regulation; Language Model; Transformer; Chromatin; Transcription Factors; Alternative Rna Splicing; Article; Bioinformatics; Chromatin; Classification Algorithm; Controlled Study; Coronavirus Disease 2019; Dna Methylation; Dna Sequence; Embryonic Stem Cell; Epigenetics; Gene Control; Gene Expression; Gene Ontology; Genetic Transcription; Genome-wide Association Study; Genomics; Glucose Homeostasis; Gm12878 Cell Line; Human; Human Cell; Human Tissue; K-562 Cell Line; Lung Parenchyma; Machine Learning; Mathematical Model; Mouse; Nonhuman; Receiver Operating Characteristic; Single Nucleotide Polymorphism; Total Quality Management; Genetic Epigenesis; Genetics; Human Genome; Metabolism; Procedures; Software; Transcription Factor; Chromatin; Epigenesis, Genetic; Epigenomics; Genome, Human; Humans; Software; Transcription Factors},
	keywords = {alternative RNA splicing; Article; bioinformatics; chromatin; classification algorithm; controlled study; coronavirus disease 2019; DNA methylation; DNA sequence; embryonic stem cell; epigenetics; gene control; gene expression; gene ontology; genetic transcription; genome-wide association study; genomics; glucose homeostasis; GM12878 cell line; human; human cell; human tissue; K-562 cell line; lung parenchyma; machine learning; mathematical model; mouse; nonhuman; receiver operating characteristic; single nucleotide polymorphism; total quality management; genetic epigenesis; genetics; human genome; metabolism; procedures; software; transcription factor; Chromatin; Epigenesis, Genetic; Epigenomics; Genome, Human; Humans; Software; Transcription Factors},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access}
}

@ARTICLE{Xiang2024,
	author = {Xiang, Wenkai and Xiong, Zhaoping and Chen, Huan and Xiong, Jiacheng and Zhang, Wei and Fu, Zunyun and Zheng, Mingyue and Liu, Bing and Shi, Qian},
	title = {FAPM: functional annotation of proteins using multimodal models beyond structural modeling},
	year = {2024},
	journal = {Bioinformatics},
	volume = {40},
	number = {12},
	pages = {},
	doi = {10.1093/bioinformatics/btae680},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211995402&doi=10.1093%2Fbioinformatics%2Fbtae680&partnerID=40&md5=e1363b4d8d8eed06a5e33d0bedde3753},
	abstract = {Motivation: Assigning accurate property labels to proteins, like functional terms and catalytic activity, is challenging, especially for proteins without homologs and “tail labels” with few known examples. Previous methods mainly focused on protein sequence features, overlooking the semantic meaning of protein labels. Results: We introduce functional annotation of proteins using multimodal models (FAPM), a contrastive multimodal model that links natural language with protein sequence language. This model combines a pretrained protein sequence model with a pretrained large language model to generate labels, such as Gene Ontology (GO) functional terms and catalytic activity predictions, in natural language. Our results show that FAPM excels in understanding protein properties, outperforming models based solely on protein sequences or structures. It achieves state-of-the-art performance on public benchmarks and in-house experimentally annotated phage proteins, which often have few known homologs. Additionally, FAPM’s flexibility allows it to incorporate extra text prompts, like taxonomy information, enhancing both its predictive performance and explainability. This novel approach offers a promising alternative to current methods that rely on multiple sequence alignment for protein annotation. Availability and implementation: The online demo is at: https://huggingface.co/spaces/wenkai/FAPM_demo. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Amino Acid Sequence; Article; Bacteriophage; Benchmarking; Catalysis; Controlled Study; Diagnosis; Gene Ontology; Human; Large Language Model; Nonhuman; Prediction; Sequence Alignment; Bioinformatics; Chemistry; Metabolism; Molecular Genetics; Natural Language Processing; Procedures; Protein Database; Sequence Analysis; Software; Protein; Computational Biology; Databases, Protein; Gene Ontology; Molecular Sequence Annotation; Natural Language Processing; Proteins; Sequence Analysis, Protein; Software},
	keywords = {amino acid sequence; article; bacteriophage; benchmarking; catalysis; controlled study; diagnosis; gene ontology; human; large language model; nonhuman; prediction; sequence alignment; bioinformatics; chemistry; metabolism; molecular genetics; natural language processing; procedures; protein database; sequence analysis; software; protein; Computational Biology; Databases, Protein; Gene Ontology; Molecular Sequence Annotation; Natural Language Processing; Proteins; Sequence Analysis, Protein; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Dohi2024,
	author = {Dohi, Eisuke and Takatsuki, Terue and Tateisi, Yuka and Fujiwara, Toyofumi and Yamamoto, Yasunori},
	title = {Examining HPO by organ and system to facilitate practical use by clinicians},
	year = {2024},
	journal = {Genomics and Informatics},
	volume = {22},
	number = {1},
	pages = {},
	doi = {10.1186/s44342-024-00024-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209785282&doi=10.1186%2Fs44342-024-00024-1&partnerID=40&md5=aaa8bf5c516a0db2e29ec3e54691233f},
	abstract = {The Human Phenotype Ontology (HPO) is widely used for annotating clinical text data, and sufficient annotation is crucial for the effective utilization of clinical texts. It was known that the use of LLMs can successfully extract symptoms and findings, but cannot annotate them with the HPO. We hypothesized that one of the potential issue for this is the lack of appropriate terms in the HPO. Therefore, during the Biomedical Linked Annotation Hackathon 8 (BLAH8), we attempted the following two tasks in order to grasp the overall picture of HPO. (1) Extract all HPO terms for each of the 23 HPO subclasses (defined as categories) directly under the HPO "Phenotypic abnormality" and then (2) search for major attributes in each of 23 categories. We employed LLM for these two tasks related to examining HPO and, at the same time, found that LLM didn't work well without ingenuity for tasks that lacked sentences and context. A manual search for terms within each category revealed that the HPO contains a mix of terms with four major attributes: (1) Disease Name, (2) Condition, (3) Test Data, and (4) Symptoms and Findings. Manual curation showed that the ratio of symptoms and findings varied from 0 to 93.1% across categories. For clinicians, who are end-users of medical terminology including HPO, it is difficult to understand ontologies. However, for good quality ontology is also important for good-quality data, and a clinician’s help is essential. It is also important to make the overall picture and limitations of ontologies easy to understand in order to bring out the explanatory power of LLMs and artificial intelligence. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Annotation; Biological Ontologies; Large Language Model; Phenotype},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Pan2024,
	author = {Pan, Zeyu and Shi, Jianyong and Jiang, Liu},
	title = {A semantic augmented approach to FEMA P-58 based dynamic regional seismic loss estimation application},
	year = {2024},
	journal = {Journal of Building Engineering},
	volume = {98},
	pages = {},
	doi = {10.1016/j.jobe.2024.111224},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209645399&doi=10.1016%2Fj.jobe.2024.111224&partnerID=40&md5=10eedb43a48d1baffdf48b0db54fe352},
	abstract = {Regional seismic loss estimation (RSLE) is a crucial process in both immediate post-earthquake emergency response and long-term reconstruction endeavors. Over the years, significant progress has been made in RSLE: sensing approaches such as field investigation and remote sensing offers a comprehensive overview necessary for real-time disaster response, while simulation techniques such as the methodology proposed in Federal Emergency Management Agency (FEMA) P-58 series provide insights into the mechanism of disaster development and its potential long-term impacts on urban assets. Nonetheless, challenges persist in the realm of practical RSLE applications. Firstly, a dynamic understanding of a disaster event and its influence is deemed important for effective emergency responses while challenging to achieve in current approaches. Secondly, stakeholders with varying roles, including administrators, rescue teams and ordinary citizens have distinct information requirements for RSLE. Thirdly, the complexity of seismic loss estimation, involving diverse data sources such as building information models, performance models, fragility data and sensor observations, poses interoperability issue. To tackle these issues, this article introduces a dynamic, multi-granularity, ontological representation scheme tailored for RSLE decision-supporting systems. This scheme operates across various scales, from individual building components to broader regional scales by synergistically employing FEMA P-58 guidelines and Semantic Web technologies. Upon the corresponding semantics, a question-and-answer agent powered by large language model is further developed to facilitate interaction requirements within the RSLE process via FEMA P-58 pipeline. The practical efficacy of this approach is validated through a prototype deployed under a real earthquake event, demonstrating its value in real-world scenarios. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Dynamic Regional Seismic Loss Estimation; Fema P-58; Large Language Model; Mainshock-aftershock Sequence; Semantic Web; Digital Elevation Model; Earthquake Effects; Metadata; Risk Management; Seismic Response; Aftershock Sequence; Dynamic Regional Seismic Loss Estimation; Federal Emergency Management Agency; Federal Emergency Management Agency P-58; Language Model; Large Language Model; Main Shock; Mainshock-aftershock Sequence; Seismic Loss Estimation; Semantic-web; Semantics},
	keywords = {Digital elevation model; Earthquake effects; Metadata; Risk management; Seismic response; Aftershock sequence; Dynamic regional seismic loss estimation; Federal Emergency Management Agency; Federal emergency management agency P-58; Language model; Large language model; Main shock; Mainshock-aftershock sequence; Seismic loss estimation; Semantic-Web; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Chen2024,
	author = {Chen, Yian and Jiang, Huixian},
	title = {Optimizing automated compliance checking with ontology-enhanced natural language processing: Case in the fire safety domain},
	year = {2024},
	journal = {Journal of Environmental Management},
	volume = {371},
	pages = {},
	doi = {10.1016/j.jenvman.2024.123320},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208974479&doi=10.1016%2Fj.jenvman.2024.123320&partnerID=40&md5=566f7641e4047b28f4e4b1a1b9c5439d},
	abstract = {The fire safety compliance checking (FSCC) plays a crucial role in ensuring the quality of fire engineering design and eliminating inherent fire hazards. It requires an objective and rational interpretation of fire regulations. However, the texts of fire regulations are filled with numerous rules related to spatial limitations, which pose a significant challenge in interpreting them. The current method of interpreting these rules mostly relies on manual translation, which is not efficient. To address this issue, this study proposes an innovative automated framework for interpreting rules by combining ontology technology with natural language processing (NLP). Through the utilization of pre-trained language models (PLMs), concepts and relationships are extracted from sentences, a domain-specific ontology is established, spatial knowledge is transformed into language-agnostic tree structures based on the ontology, and the semantic components of spatial relationships are extracted. The tree structure is then mapped to logical clauses based on semantic consistency, thereby improving the efficiency of interpretation. Experimental results demonstrate that the architecture achieves an F1 score of 86.27 for entity extraction and 81.81 for spatial relationship joint extraction tasks, with an accuracy of 96.26% in the formalization of logical rules, highlighting its proficiency in automatically interpreting fire spatial rules. This study offers technical support to enhance public understanding of fire safety management and fire prevention predictions, thereby promoting the intelligent management of the building safety environment. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Automated Compliance Checking (acc); Named Entity Recognition; Ontology Automatic Construction; Pre-trained Language Model; Relationship Extraction; Rule Interpretation; Fire Extinguishers; Fireproofing; Fires; Natural Language Processing Systems; Ontology; Semantics; Automated Compliance Checking; Automatic Construction; Language Model; Named Entity Recognition; Ontology Automatic Construction; Ontology's; Pre-trained Language Model; Relationship Extraction; Rule Interpretation; Fire Hazards; Compliance; Design Method; Extraction Method; Knowledge; Language; Optimization; Agnostic; Article; Clinical Article; Compliance (physical); Diagnosis; Fire Protection; Human; Language Model; Natural Language Processing; Ontology; Prediction; Fire; Natural Language Processing},
	keywords = {Fire extinguishers; Fireproofing; Fires; Natural language processing systems; Ontology; Semantics; Automated compliance checking; Automatic construction; Language model; Named entity recognition; Ontology automatic construction; Ontology's; Pre-trained language model; Relationship extraction; Rule interpretation; Fire hazards; compliance; design method; extraction method; knowledge; language; optimization; Agnostic; article; clinical article; compliance (physical); diagnosis; fire protection; human; language model; natural language processing; ontology; prediction; fire; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Reales2024,
	author = {Reales, Daniel and Manrique, Rubén Francisco and Grévisse, Christian},
	title = {Core Concept Identification in Educational Resources via Knowledge Graphs and Large Language Models},
	year = {2024},
	journal = {SN Computer Science},
	volume = {5},
	number = {8},
	pages = {},
	doi = {10.1007/s42979-024-03341-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208722616&doi=10.1007%2Fs42979-024-03341-y&partnerID=40&md5=758642118b6621a5065b31a83abbb2cd},
	abstract = {The growing demand for online education raises the question of which learning resources should be included in online programs to ensure students achieve their desired learning outcomes. By automatically identifying the core concepts in educational materials, teachers can select coherent and relevant resources for their courses. This work explores the use of Large Language Models (LLMs) to identify core concepts in educational resources. We propose three different pipelines for building knowledge graphs from lecture transcripts using LLMs and ontologies such as DBpedia. These knowledge graphs are then utilized to determine the central concepts (nodes) within the educational resources. Results show that LLM-constructed knowledge graphs when guided by ontologies, achieve state-of-the-art performance in core concept identification. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Core Concept Identification; Knowledge Graphs; Large Language Models},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Chiang2024,
	author = {Chiang, Kuoliang and Chou, Yucheng and Tung, Hsin and Huang, Chinyin and Hsieh, Liangpo and Chang, Kaiping and Kwan, Sheongyeong and Huang, Wanyu},
	title = {Customized GPT model largely increases surgery decision accuracy for pharmaco-resistant epilepsy},
	year = {2024},
	journal = {Journal of Clinical Neuroscience},
	volume = {130},
	pages = {},
	doi = {10.1016/j.jocn.2024.110918},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208666983&doi=10.1016%2Fj.jocn.2024.110918&partnerID=40&md5=85be06e6b1389783938ed99f3362a035},
	abstract = {Background: To develop an enhanced epilepsy diagnosis system by integrating an expert-informed ontology with a custom generative pre-trained transformer (GPT), validated by inferring possible seizure lateralization and localization using retrospective textual data from the pre-surgical assessments of patients with pharmaco-resistant epilepsy (PRE). Methods: We developed an AI system for epilepsy diagnosis using Protégé with OWL/SWRL, integrating a knowledge base with seizure semiology, seizure types EEG descriptors, expert insights, and literature to pinpoint seizure locations. A customized GPT model was then tailored for specific diagnostic needs. Validated through 16 surgical cases, the system's accuracy in seizure localization and the JSON (JavaScript Object Notation) Epilepsy Matcher's term matching capabilities were confirmed against a Protégé-based knowledge base. Results: A total of 117 patients with PRE underwent video-EEG monitoring at a single institution. However, only 16 of these patients received epilepsy surgery. The Protégé system achieved 75 % accuracy in diagnosing epilepsy from 16 cases using semiology, which increased to 87.5 % with EEG data. The Json Epilepsy Matcher further improved accuracy to 87.5 % with symptoms alone and 93.8 % when including EEG data, highlighting the benefits of applying GPT techniques. Conclusions: This study highlights the efficacy of the JSON Epilepsy Matcher in improving seizure diagnosis accuracy. When combined with EEG data, it achieves a 93.8 % accuracy rate, suggesting a potential improvement in the practicality and generalizability of the original ontology expert system, boosting physicians’ confidence in confirming surgery and potentially sparing many children from prolonged suffering. This innovative approach not only improves diagnostic accuracy but also sets a precedent for future applications of AI in neurology. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Generative Pre-trained Transformer; Large-scale Language Model; Localization; Seizure Descriptors; Semiology; Levetiracetam; Topiramate; Valproic Acid; Levetiracetam; Topiramate; Valproic Acid; Absentmindedness; Adult; Article; Artificial Intelligence; Aura; Clinical Decision Making; Clinical Effectiveness; Cognitive Defect; Computer Language; Confusion; Controlled Study; Cortical Dysplasia; Cortical Thickness (brain); Craniotomy; Data Processing; Delta Rhythm; Diagnostic Accuracy; Diagnostic Test Accuracy Study; Disease Severity; Epilepsy Surgery; Female; Generative Pretrained Transformer; Hallucination; Hippocampal Sclerosis; Human; Intermethod Comparison; Javascript Object Notation Epilepsy Matcher; Knowledge; Limb Stiffness; Low Drug Dose; Major Clinical Study; Male; Measurement Precision; Medical History; Memory Disorder; Middle Aged; Motor Dysfunction; Nuclear Magnetic Resonance Imaging; Occipital Craniotomy; Onset Age; Ontology Web Language; Refractory Epilepsy; Reliability; Retrospective Study; Seizure; Semantic Web Rule Language; Surgery Decision Accuracy; Surgical Technique; Symptom; Temporal Lobectomy; Theta Rhythm; Unconsciousness; Uncontrolled Limb Movement; Validation Study; Visual Disorder; Adolescent; Diagnosis; Electroencephalography; Neurosurgery; Procedures; Surgery; Young Adult; Adolescent; Adult; Artificial Intelligence; Drug Resistant Epilepsy; Electroencephalography; Female; Humans; Male; Neurosurgical Procedures; Retrospective Studies; Young Adult},
	keywords = {levetiracetam; topiramate; valproic acid; absentmindedness; adult; Article; artificial intelligence; aura; clinical decision making; clinical effectiveness; cognitive defect; computer language; confusion; controlled study; cortical dysplasia; cortical thickness (brain); craniotomy; data processing; delta rhythm; diagnostic accuracy; diagnostic test accuracy study; disease severity; epilepsy surgery; female; generative pretrained transformer; hallucination; hippocampal sclerosis; human; intermethod comparison; JavaScript Object Notation Epilepsy Matcher; knowledge; limb stiffness; low drug dose; major clinical study; male; measurement precision; medical history; memory disorder; middle aged; motor dysfunction; nuclear magnetic resonance imaging; occipital craniotomy; onset age; Ontology Web Language; refractory epilepsy; reliability; retrospective study; seizure; Semantic Web Rule Language; surgery decision accuracy; surgical technique; symptom; temporal lobectomy; theta rhythm; unconsciousness; uncontrolled limb movement; validation study; visual disorder; adolescent; diagnosis; electroencephalography; neurosurgery; procedures; surgery; young adult; Adolescent; Adult; Artificial Intelligence; Drug Resistant Epilepsy; Electroencephalography; Female; Humans; Male; Neurosurgical Procedures; Retrospective Studies; Young Adult},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Lei2024,
	author = {Lei, Xinyu and Cao, Xue and Zhang, Faye and Lai, Qifang and Gao, Pengcheng and Li, Yuehong},
	title = {Study of carbonate alkalinity-induced hepatic tissue damage in Hefang crucian carp (Carassius auratus) based on transcriptomic analysis},
	year = {2024},
	journal = {Comparative Biochemistry and Physiology - Part D: Genomics and Proteomics},
	volume = {52},
	pages = {},
	doi = {10.1016/j.cbd.2024.101351},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208504918&doi=10.1016%2Fj.cbd.2024.101351&partnerID=40&md5=ae8d5c2c28975a5d79b7ad30b026d74f},
	abstract = {This study investigated the effects of different sodium bicarbonate (NaHCO<inf>3</inf>) concentrations (0 g/L, 1 g/L, and 3 g/L) on Hefang crucian carp (12.0 ± 1.1 g) over a 96-hour period. The experiment is divided into three groups, each with three replicates, and each replicate contains 30 fish. We employed a comprehensive approach integrating histology, physiological and biochemical assays, transcriptomics, as well as artificial intelligence (AI)-assisted analysis. This multifaceted method allowed us to examine changes in gill and liver morphology, osmoregulation, antioxidant capacity, immune response, and physiological metabolism. Results showed that gill and liver tissue damage increased with rising water alkalinity. Serum sodium (Na<sup>+</sup>), potassium (K<sup>+</sup>), blood ammonia, and gill Na<sup>+</sup>/K<sup>+</sup>-ATPase (NKA) levels increased significantly (p < 0.05). Hepatic antioxidant enzymes initially increased, then decreased with prolonged stress. Serum and liver immunoenzyme indices were higher in bicarbonate-treated groups compared to controls. Carbonate treatment altered lipid and glucose metabolism in both serum and liver. Transcriptome analysis, enhanced by large language models (LLMs), revealed differentially expressed genes (DEGs) significantly associated with ion binding, transport, apoptosis, and metabolism. In conclusion, excessive carbonate intake in fish alters serum physiological functions and affects hepatic metabolic functions. Crucian carp primarily regulate hepatic antioxidant systems, utilize carbohydrate breakdown for energy requirements, and employ lipids in osmoregulation. This study provides insights into fish adaptation to saline-alkaline environments and offers support for the development of aquaculture in saline-alkaline waters. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Carbonate Alkalinity Stress; Histological Analysis; Liver Injury; Transcriptomics; 6 Phosphofructokinase; Acetate Coenzyme A Ligase; Acetyl Coenzyme A Carboxylase; Adenosine Triphosphatase (potassium Sodium); Aminotransferase; Ammonia; Bicarbonate; Fatty Acid Synthase; Hexokinase; Lipoprotein Lipase; Potassium; Pyruvate Kinase; Sodium; Carbonates; 6 Phosphofructokinase; Acetate Coenzyme A Ligase; Acetyl Coenzyme A Carboxylase; Adenosine Triphosphatase (potassium Sodium); Aminotransferase; Ammonia; Bicarbonate; Carnitine Palmitoyl Transaminase 1; Enzyme; Fatty Acid Synthase; Hexokinase; Lipoprotein Lipase; Liver Triacylglycerol Lipase; Potassium; Pyruvate Kinase; Sodium; Unclassified Drug; Carbonic Acid Derivative; Transcriptome; Adult; Alkalinity; Ammonia Blood Level; Animal Experiment; Animal Model; Animal Tissue; Antioxidant Activity; Apoptosis; Article; Artificial Intelligence; Carbohydrate Metabolism; Concentration (parameter); Controlled Study; Differential Gene Expression; Enzyme Blood Level; Functional Enrichment Analysis; Gene Ontology; Gill; Glucose Metabolism; Goldfish; Histopathology; Immune Response; Ion Transport; Kegg; Large Language Model; Lipid Metabolism; Liver Injury; Liver Metabolism; Liver Tissue; Metabolism; Nonhuman; Osmoregulation; Potassium Blood Level; Rna Sequencing; Sodium Blood Level; Transcriptomics; Animal; Carp; Drug Effect; Gene Expression Profiling; Genetics; Liver; Animals; Carbonates; Carps; Gene Expression Profiling; Gills; Goldfish; Liver; Transcriptome},
	keywords = {6 phosphofructokinase; acetate coenzyme A ligase; acetyl coenzyme A carboxylase; adenosine triphosphatase (potassium sodium); aminotransferase; ammonia; bicarbonate; carnitine palmitoyl transaminase 1; enzyme; fatty acid synthase; hexokinase; lipoprotein lipase; liver triacylglycerol lipase; potassium; pyruvate kinase; sodium; unclassified drug; carbonic acid derivative; transcriptome; adult; alkalinity; ammonia blood level; animal experiment; animal model; animal tissue; antioxidant activity; apoptosis; Article; artificial intelligence; carbohydrate metabolism; concentration (parameter); controlled study; differential gene expression; enzyme blood level; functional enrichment analysis; gene ontology; gill; glucose metabolism; goldfish; histopathology; immune response; ion transport; KEGG; large language model; lipid metabolism; liver injury; liver metabolism; liver tissue; metabolism; nonhuman; osmoregulation; potassium blood level; RNA sequencing; sodium blood level; transcriptomics; animal; carp; drug effect; gene expression profiling; genetics; liver; Animals; Carbonates; Carps; Gene Expression Profiling; Gills; Goldfish; Liver; Transcriptome},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Shi2024678,
	author = {Shi, Dachuan and Liedl, Philipp and Bauernhansl, Thomas},
	title = {Interoperable information modelling leveraging asset administration shell and large language model for quality control toward zero defect manufacturing},
	year = {2024},
	journal = {Journal of Manufacturing Systems},
	volume = {77},
	pages = {678 - 696},
	doi = {10.1016/j.jmsy.2024.10.011},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207646345&doi=10.1016%2Fj.jmsy.2024.10.011&partnerID=40&md5=23a97c12769c11036b5f34798b5fe52c},
	abstract = {In the era of Industry 4.0, Zero Defect Manufacturing (ZDM) has emerged as a prominent strategy for quality improvement, emphasizing data-driven approaches for defect prediction, prevention, and mitigation. The success of ZDM heavily depends on the availability and quality of data typically collected from diverse and heterogeneous sources during production and quality control, presenting challenges in data interoperability. Addressing this, we introduce a novel approach leveraging Asset Administration Shell (AAS) and Large Language Models (LLMs) for creating interoperable information models that incorporate semantic contextual information to enhance the interoperability of data integration in the quality control process. AAS, initiated by German industry stakeholders, shows a significant advancement in information modeling, blending ontology and digital twin concepts for the virtual representation of assets. In this work, we develop a systematic, use-case-driven methodology for AAS-based information modeling. This methodology guides the design and implementation of AAS models, ensuring model properties are presented in a unified structure and reference external standardized vocabularies to maintain consistency across different systems. To automate this referencing process, we propose a novel LLM-based algorithm to semantically search model properties within a standardized vocabulary repository. This algorithm significantly reduces manual intervention in model development. A case study in the injection molding domain demonstrates the practical application of our approach, showcasing the integration and linking of product quality and machine process data with the help of the developed AAS models. Statistical evaluation of our LLM-based semantic search algorithm confirms its efficacy in enhancing data interoperability. This methodology offers a scalable and adaptable solution for various industrial use cases, promoting widespread data interoperability in the context of Industry 4.0. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Asset Administration Shell; Industry 4.0; Information Modelling; Interoperability; Large Language Model; Ontology; Quality Control; Zero Defect Manufacturing; Network Security; Steganography; Asset Administration Shell; Data Interoperability; Information Modeling; Language Model; Large Language Model; Model Properties; Ontology's; Shell Models; Zero Defect Manufacturing; Zero Defects; Statistical Process Control},
	keywords = {Network security; Steganography; Asset administration shell; Data interoperability; Information Modeling; Language model; Large language model; Model properties; Ontology's; Shell models; Zero defect manufacturing; Zero defects; Statistical process control},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Toro2024,
	author = {Toro, Sabrina and Anagnostopoulos, Anna V. and Bello, Susan M. and Blumberg, Kai Lewis and Cameron, Rhiannon and Carmody, Leigh C. and Diehl, Alexander D. and Dooley, Damion M. and Duncan, William D. and Fey, Petra},
	title = {Dynamic Retrieval Augmented Generation of Ontologies using Artificial Intelligence (DRAGON-AI)},
	year = {2024},
	journal = {Journal of Biomedical Semantics},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.1186/s13326-024-00320-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206568005&doi=10.1186%2Fs13326-024-00320-3&partnerID=40&md5=a23bb98318ff68a24095821e8384557b},
	abstract = {Background: Ontologies are fundamental components of informatics infrastructure in domains such as biomedical, environmental, and food sciences, representing consensus knowledge in an accurate and computable form. However, their construction and maintenance demand substantial resources and necessitate substantial collaboration between domain experts, curators, and ontology experts. We present Dynamic Retrieval Augmented Generation of Ontologies using AI (DRAGON-AI), an ontology generation method employing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG). DRAGON-AI can generate textual and logical ontology components, drawing from existing knowledge in multiple ontologies and unstructured text sources. Results: We assessed performance of DRAGON-AI on de novo term construction across ten diverse ontologies, making use of extensive manual evaluation of results. Our method has high precision for relationship generation, but has slightly lower precision than from logic-based reasoning. Our method is also able to generate definitions deemed acceptable by expert evaluators, but these scored worse than human-authored definitions. Notably, evaluators with the highest level of confidence in a domain were better able to discern flaws in AI-generated definitions. We also demonstrated the ability of DRAGON-AI to incorporate natural language instructions in the form of GitHub issues. Conclusions: These findings suggest DRAGON-AI's potential to substantially aid the manual ontology construction process. However, our results also underscore the importance of having expert curators and ontology editors drive the ontology generation process. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Biocuration; Knowledge Graphs; Large Language Models; Ontologies; Ontology Engineering; Artificial Intelligence; Biological Ontology; Information Retrieval; Natural Language Processing; Procedures; Artificial Intelligence; Biological Ontologies; Information Storage And Retrieval; Natural Language Processing},
	keywords = {artificial intelligence; biological ontology; information retrieval; natural language processing; procedures; Artificial Intelligence; Biological Ontologies; Information Storage and Retrieval; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access; Gold Open Access}
}

@ARTICLE{Durmaz2024,
	author = {Durmaz, Ali Riza and Thomas, Akhil and Mishra, Lokesh and Murthy, Rachana Niranjan and Straub, Thomas S.},
	title = {An ontology-based text mining dataset for extraction of process-structure-property entities},
	year = {2024},
	journal = {Scientific Data},
	volume = {11},
	number = {1},
	pages = {},
	doi = {10.1038/s41597-024-03926-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206056411&doi=10.1038%2Fs41597-024-03926-5&partnerID=40&md5=4cac877d49b2d2747aa86c904233aad1},
	abstract = {While large language models learn sound statistical representations of the language and information therein, ontologies are symbolic knowledge representations that can complement the former ideally. Research at this critical intersection relies on datasets that intertwine ontologies and text corpora to enable training and comprehensive benchmarking of neurosymbolic models. We present the MaterioMiner dataset and the linked materials mechanics ontology where ontological concepts from the mechanics of materials domain are associated with textual entities within the literature corpus. Another distinctive feature of the dataset is its eminently fine-grained annotation. Specifically, 179 distinct classes are manually annotated by three raters within four publications, amounting to 2191 entities that were annotated and curated. Conceptual work is presented for the symbolic representation of causal composition-process-microstructure-property relationships. We explore the annotation consistency between the three raters and perform fine-tuning of pre-trained language models to showcase the feasibility of training named entity recognition models. Reusing the dataset can foster training and benchmarking of materials language models, automated ontology construction, and knowledge graph generation from textual data. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Article; Benchmarking; Human; Language Model; Large Language Model; Mechanics; Ontology},
	keywords = {article; benchmarking; human; language model; large language model; mechanics; ontology},
	type = {Data paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access}
}

@ARTICLE{Wu2024,
	author = {Wu, Jinge and Dong, Hang and Li, Zexi and Wang, Haowei and Li, Runci and Patra, Arijit and Dai, Chengliang and Ali, Waqar and Scordis, Phil and Wu, Honghan},
	title = {A hybrid framework with large language models for rare disease phenotyping},
	year = {2024},
	journal = {BMC Medical Informatics and Decision Making},
	volume = {24},
	number = {1},
	pages = {},
	doi = {10.1186/s12911-024-02698-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205771843&doi=10.1186%2Fs12911-024-02698-7&partnerID=40&md5=e346af8b03a824d503a506deca503522},
	abstract = {Purpose: Rare diseases pose significant challenges in diagnosis and treatment due to their low prevalence and heterogeneous clinical presentations. Unstructured clinical notes contain valuable information for identifying rare diseases, but manual curation is time-consuming and prone to subjectivity. This study aims to develop a hybrid approach combining dictionary-based natural language processing (NLP) tools with large language models (LLMs) to improve rare disease identification from unstructured clinical reports. Methods: We propose a novel hybrid framework that integrates the Orphanet Rare Disease Ontology (ORDO) and the Unified Medical Language System (UMLS) to create a comprehensive rare disease vocabulary. SemEHR, a dictionary-based NLP tool, is employed to extract rare disease mentions from clinical notes. To refine the results and improve accuracy, we leverage various LLMs, including LLaMA3, Phi3-mini, and domain-specific models like OpenBioLLM and BioMistral. Different prompting strategies, such as zero-shot, few-shot, and knowledge-augmented generation, are explored to optimize the LLMs’ performance. Results: The proposed hybrid approach demonstrates superior performance compared to traditional NLP systems and standalone LLMs. LLaMA3 and Phi3-mini achieve the highest F1 scores in rare disease identification. Few-shot prompting with 1-3 examples yields the best results, while knowledge-augmented generation shows limited improvement. Notably, the approach uncovers a significant number of potential rare disease cases not documented in structured diagnostic records, highlighting its ability to identify previously unrecognized patients. Conclusion: The hybrid approach combining dictionary-based NLP tools with LLMs shows great promise for improving rare disease identification from unstructured clinical reports. By leveraging the strengths of both techniques, the method demonstrates superior performance and the potential to uncover hidden rare disease cases. Further research is needed to address limitations related to ontology mapping and overlapping case identification, and to integrate the approach into clinical practice for early diagnosis and improved patient outcomes. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Electronic Health Record; Large Language Model; Natural Language Processing; Phenotyping; Biological Ontology; Diagnosis; Electronic Health Record; Human; Natural Language Processing; Phenotype; Rare Disease; Unified Medical Language System; Biological Ontologies; Electronic Health Records; Humans; Natural Language Processing; Phenotype; Rare Diseases},
	keywords = {biological ontology; diagnosis; electronic health record; human; natural language processing; phenotype; rare disease; Unified Medical Language System; Biological Ontologies; Electronic Health Records; Humans; Natural Language Processing; Phenotype; Rare Diseases},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@ARTICLE{Yu2024,
	author = {Yu, Alessandra Nicoletta Cruz and Lowe, Leroy J. and Schiller, Daniela},
	title = {Future considerations for the Human Affectome: Reply to commentaries},
	year = {2024},
	journal = {Neuroscience and Biobehavioral Reviews},
	volume = {167},
	pages = {},
	doi = {10.1016/j.neubiorev.2024.105901},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205440605&doi=10.1016%2Fj.neubiorev.2024.105901&partnerID=40&md5=89b125d800c1135530e0e7cdd0e404d7},
	author_keywords = {Abstraction; Academic Interest; Affective Level; Affective Phenomena; Algorithm; Central Empirical Task; Cognition; Consciousness; Goal Directed Process; Human; Human Affectome; Large Language Model; Note; Ontology; Phenotype; Scientific Literature; Teleology; Adult; Article},
	keywords = {abstraction; academic interest; affective level; affective phenomena; algorithm; central empirical task; cognition; consciousness; goal directed process; human; human affectome; large language model; Note; ontology; phenotype; scientific literature; teleology; adult; article},
	type = {Note},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lai2024,
	author = {Lai, Poting and Coudert, Elisabeth and Aimo, Lucila and Axelsen, Kristian Buhl and Breuza, Lionel and de Castro, Edouard and Feuermann, Marc and Morgat, Anne and Pourcel, Lucille and Pedruzzi, I.},
	title = {EnzChemRED, a rich enzyme chemistry relation extraction dataset},
	year = {2024},
	journal = {Scientific Data},
	volume = {11},
	number = {1},
	pages = {},
	doi = {10.1038/s41597-024-03835-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203337764&doi=10.1038%2Fs41597-024-03835-7&partnerID=40&md5=3da9ab8fa725751eb90a5f8d6bc30a7d},
	abstract = {Expert curation is essential to capture knowledge of enzyme functions from the scientific literature in FAIR open knowledgebases but cannot keep pace with the rate of new discoveries and new publications. In this work we present EnzChemRED, for Enzyme Chemistry Relation Extraction Dataset, a new training and benchmarking dataset to support the development of Natural Language Processing (NLP) methods such as (large) language models that can assist enzyme curation. EnzChemRED consists of 1,210 expert curated PubMed abstracts where enzymes and the chemical reactions they catalyze are annotated using identifiers from the protein knowledgebase UniProtKB and the chemical ontology ChEBI. We show that fine-tuning language models with EnzChemRED significantly boosts their ability to identify proteins and chemicals in text (86.30% F<inf>1</inf> score) and to extract the chemical conversions (86.66% F<inf>1</inf> score) and the enzymes that catalyze those conversions (83.79% F<inf>1</inf> score). We apply our methods to abstracts at PubMed scale to create a draft map of enzyme functions in literature to guide curation efforts in UniProtKB and the reaction knowledgebase Rhea. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Enzymes; Enzyme; Chemistry; Knowledge Base; Medline; Natural Language Processing; Protein Database; Databases, Protein; Enzymes; Knowledge Bases; Natural Language Processing; Pubmed},
	keywords = {enzyme; chemistry; knowledge base; Medline; natural language processing; protein database; Databases, Protein; Enzymes; Knowledge Bases; Natural Language Processing; PubMed},
	type = {Data paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Gold Open Access}
}

@ARTICLE{Wei2024,
	author = {Wei, Zhaoquan and Chen, Xi and Sun, Youshi and Zhang, Yifei and Dong, Ruifang and Wang, Xiaojing and Chen, Shuangtao},
	title = {Exploring the molecular mechanisms and shared potential drugs between rheumatoid arthritis and arthrofibrosis based on large language model and synovial microenvironment analysis},
	year = {2024},
	journal = {Scientific Reports},
	volume = {14},
	number = {1},
	pages = {},
	doi = {10.1038/s41598-024-69080-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201404596&doi=10.1038%2Fs41598-024-69080-5&partnerID=40&md5=a0de65d09c1b512e29eeaf5cf32ace74},
	abstract = {Rheumatoid arthritis (RA) and arthrofibrosis (AF) are both chronic synovial hyperplasia diseases that result in joint stiffness and contractures. They shared similar symptoms and many common features in pathogenesis. Our study aims to perform a comprehensive analysis between RA and AF and identify novel drugs for clinical use. Based on the text mining approaches, we performed a correlation analysis of 12 common joint diseases including arthrofibrosis, gouty arthritis, infectious arthritis, juvenile idiopathic arthritis, osteoarthritis, post infectious arthropathies, post traumatic osteoarthritis, psoriatic arthritis, reactive arthritis, rheumatoid arthritis, septic arthritis, and transient arthritis. 5 bulk sequencing datasets and 4 single-cell sequencing datasets of RA and AF were integrated and analyzed. A novel drug repositioning method was found for drug screening, and text mining approaches were used to verify the identified drugs. RA and AF performed the highest gene similarity (0.77) and functional ontology similarity (0.84) among all 12 joint diseases. We figured out that they share the same key pathogenic cell including CD34 + sublining fibroblasts (CD34-SLF) and DKK3 + sublining fibroblasts (DKK3-SLF). Potential therapeutic target database (PTTD) was established with the differential expressed genes (DEGs) of these key pathogenic cells. Based on the PTTD, 15 potential drugs for AF and 16 potential drugs for RA were identified. This work provides a new perspective on AF and RA study which enhances our understanding of their pathogenesis. It also shed light on their underlying mechanism and open new avenues for drug repositioning studies. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Arthritis; Arthrofibrosis; Rheumatoid Arthritis; Sublining Fibroblasts; Synovial Microenvironment; Data Mining; Drug Effect; Drug Repositioning; Drug Therapy; Fibroblast; Fibrosis; Genetics; Human; Metabolism; Pathology; Rheumatoid Arthritis; Synovium; Tumor Microenvironment; Arthritis, Rheumatoid; Cellular Microenvironment; Data Mining; Drug Repositioning; Fibroblasts; Fibrosis; Humans; Synovial Membrane},
	keywords = {data mining; drug effect; drug repositioning; drug therapy; fibroblast; fibrosis; genetics; human; metabolism; pathology; rheumatoid arthritis; synovium; tumor microenvironment; Arthritis, Rheumatoid; Cellular Microenvironment; Data Mining; Drug Repositioning; Fibroblasts; Fibrosis; Humans; Synovial Membrane},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Li2024,
	author = {Li, Jianfu and Li, Yiming and Pan, Yuanyi and Guo, Jinjing and Sun, Zenan and Li, Fang and He, Yongqun Oliver and Tao, Cui},
	title = {Mapping vaccine names in clinical trials to vaccine ontology using cascaded fine-tuned domain-specific language models},
	year = {2024},
	journal = {Journal of Biomedical Semantics},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.1186/s13326-024-00318-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200874420&doi=10.1186%2Fs13326-024-00318-x&partnerID=40&md5=e7f402a67d05c1e4f88408c00a611cf4},
	abstract = {Background: Vaccines have revolutionized public health by providing protection against infectious diseases. They stimulate the immune system and generate memory cells to defend against targeted diseases. Clinical trials evaluate vaccine performance, including dosage, administration routes, and potential side effects. ClinicalTrials.gov is a valuable repository of clinical trial information, but the vaccine data in them lacks standardization, leading to challenges in automatic concept mapping, vaccine-related knowledge development, evidence-based decision-making, and vaccine surveillance. Results: In this study, we developed a cascaded framework that capitalized on multiple domain knowledge sources, including clinical trials, the Unified Medical Language System (UMLS), and the Vaccine Ontology (VO), to enhance the performance of domain-specific language models for automated mapping of VO from clinical trials. The Vaccine Ontology (VO) is a community-based ontology that was developed to promote vaccine data standardization, integration, and computer-assisted reasoning. Our methodology involved extracting and annotating data from various sources. We then performed pre-training on the PubMedBERT model, leading to the development of CTPubMedBERT. Subsequently, we enhanced CTPubMedBERT by incorporating SAPBERT, which was pretrained using the UMLS, resulting in CTPubMedBERT + SAPBERT. Further refinement was accomplished through fine-tuning using the Vaccine Ontology corpus and vaccine data from clinical trials, yielding the CTPubMedBERT + SAPBERT + VO model. Finally, we utilized a collection of pre-trained models, along with the weighted rule-based ensemble approach, to normalize the vaccine corpus and improve the accuracy of the process. The ranking process in concept normalization involves prioritizing and ordering potential concepts to identify the most suitable match for a given context. We conducted a ranking of the Top 10 concepts, and our experimental results demonstrate that our proposed cascaded framework consistently outperformed existing effective baselines on vaccine mapping, achieving 71.8% on top 1 candidate’s accuracy and 90.0% on top 10 candidate’s accuracy. Conclusion: This study provides a detailed insight into a cascaded framework of fine-tuned domain-specific language models improving mapping of VO from clinical trials. By effectively leveraging domain-specific information and applying weighted rule-based ensembles of different pre-trained BERT models, our framework can significantly enhance the mapping of VO from clinical trials. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Trials; Domain-specific Language Models; Normalization; Vaccine Ontology; Vaccines; Vaccine; Biological Ontology; Clinical Trial (topic); Human; Immunology; Natural Language Processing; Unified Medical Language System; Biological Ontologies; Clinical Trials As Topic; Humans; Natural Language Processing; Vaccines},
	keywords = {vaccine; biological ontology; clinical trial (topic); human; immunology; natural language processing; Unified Medical Language System; Biological Ontologies; Clinical Trials as Topic; Humans; Natural Language Processing; Vaccines},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Wang2024,
	author = {Wang, Zeqiang and Wang, Yuqi and Zhang, Haiyang and Wang, Wei and Qi, Jun and Chen, Jianjun and Sastry, Nishanth N. and Johnson, Jon and De, Suparna},
	title = {ICDXML: enhancing ICD coding with probabilistic label trees and dynamic semantic representations},
	year = {2024},
	journal = {Scientific Reports},
	volume = {14},
	number = {1},
	pages = {},
	doi = {10.1038/s41598-024-69214-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200536681&doi=10.1038%2Fs41598-024-69214-9&partnerID=40&md5=89a1bb9ca561a195fc74d1dbd9697ab1},
	abstract = {Accurately assigning standardized diagnosis and procedure codes from clinical text is crucial for healthcare applications. However, this remains challenging due to the complexity of medical language. This paper proposes a novel model that incorporates extreme multi-label classification tasks to enhance International Classification of Diseases (ICD) coding. The model utilizes deformable convolutional neural networks to fuse representations from hidden layer outputs of pre-trained language models and external medical knowledge embeddings fused using a multimodal approach to provide rich semantic encodings for each code. A probabilistic label tree is constructed based on the hierarchical structure existing in ICD labels to incorporate ontological relationships between ICD codes and enable structured output prediction. Experiments on medical code prediction on the MIMIC-III database demonstrate competitive performance, highlighting the benefits of this technique for robust clinical code assignment. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Extreme Multi-label Classification; Few-shot Learning; Icd Coding; Medical Knowledge Representation; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Mullin2024,
	author = {Mullin, Sarah and McDougal, Robert A. and Cheung, Kei-Hoi Hoi and Kilicoglu, Halil and Beck, Amanda P. and Zeiss, Caroline J.},
	title = {Chemical entity normalization for successful translational development of Alzheimer’s disease and dementia therapeutics},
	year = {2024},
	journal = {Journal of Biomedical Semantics},
	volume = {15},
	number = {1},
	pages = {},
	doi = {10.1186/s13326-024-00314-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200034968&doi=10.1186%2Fs13326-024-00314-1&partnerID=40&md5=b5169efc0a8aab6588af5f3720b8104e},
	abstract = {Background: Identifying chemical mentions within the Alzheimer’s and dementia literature can provide a powerful tool to further therapeutic research. Leveraging the Chemical Entities of Biological Interest (ChEBI) ontology, which is rich in hierarchical and other relationship types, for entity normalization can provide an advantage for future downstream applications. We provide a reproducible hybrid approach that combines an ontology-enhanced PubMedBERT model for disambiguation with a dictionary-based method for candidate selection. Results: There were 56,553 chemical mentions in the titles of 44,812 unique PubMed article abstracts. Based on our gold standard, our method of disambiguation improved entity normalization by 25.3 percentage points compared to using only the dictionary-based approach with fuzzy-string matching for disambiguation. For the CRAFT corpus, our method outperformed baselines (maximum 78.4%) with a 91.17% accuracy. For our Alzheimer’s and dementia cohort, we were able to add 47.1% more potential mappings between MeSH and ChEBI when compared to BioPortal. Conclusion: Use of natural language models like PubMedBERT and resources such as ChEBI and PubChem provide a beneficial way to link entity mentions to ontology terms, while further supporting downstream tasks like filtering ChEBI mentions based on roles and assertions to find beneficial therapies for Alzheimer’s and dementia. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Alzheimer; Chebi; Dementia; Entity Normalization; Ontology; Alzheimer Disease; Biological Ontology; Dementia; Drug Therapy; Human; Metabolism; Natural Language Processing; Translational Research; Alzheimer Disease; Biological Ontologies; Dementia; Humans; Natural Language Processing; Translational Research, Biomedical},
	keywords = {Alzheimer disease; biological ontology; dementia; drug therapy; human; metabolism; natural language processing; translational research; Alzheimer Disease; Biological Ontologies; Dementia; Humans; Natural Language Processing; Translational Research, Biomedical},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Chen2024,
	author = {Chen, Liuqing and Zuo, Haoyu and Cai, Zebin and Yin, Yuan and Zhang, Yuan and Sun, Lingyun and Childs, Peter R.N.},
	title = {Toward Controllable Generative Design: A Conceptual Design Generation Approach Leveraging the Function-Behavior-Structure Ontology and Large Language Models},
	year = {2024},
	journal = {Journal of Mechanical Design},
	volume = {146},
	number = {12},
	pages = {},
	doi = {10.1115/1.4065562},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199391540&doi=10.1115%2F1.4065562&partnerID=40&md5=bda3860dd2a3fca92a8532621c58deba},
	abstract = {Recent research in the field of design engineering is primarily focusing on using AI technologies such as Large Language Models (LLMs) to assist early-stage design. The engineer or designer can use LLMs to explore, validate, and compare thousands of generated conceptual stimuli and make final choices. This was seen as a significant stride in advancing the status of the generative approach in computer-aided design. However, it is often difficult to instruct LLMs to obtain novel conceptual solutions and requirement-compliant in real design tasks, due to the lack of transparency and insufficient controllability of LLMs. This study presents an approach to leverage LLMs to infer Function-Behavior-Structure (FBS) ontology for high-quality design concepts. Prompting design based on the FBS model decomposes the design task into three sub-tasks including functional, behavioral, and structural reasoning. In each sub-task, prompting templates and specification signifiers are specified to guide the LLMs to generate concepts. User can determine the selected concepts by judging and evaluating the generated function-structure pairs. A comparative experiment has been conducted to evaluate the concept generation approach. According to the concept evaluation results, our approach achieves the highest scores in concept evaluation, and the generated concepts are more novel, useful, functional, and low cost compared to the baseline. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Conceptual Design; Creativity And Concept Generation; Fbs Model; Generative Design; Large Language Model; Machine Learning; Architectural Design; Computational Linguistics; Computer Aided Design; Function Evaluation; Machine Learning; Ontology; Structural Design; Concept Generation; Creativity And Concept Generation; Design Tasks; Function-behavior-structure Model; Function-behaviour-structure Ontologies; Generative Design; Language Model; Large Language Model; Machine-learning; Subtask; Conceptual Design},
	keywords = {Architectural design; Computational linguistics; Computer aided design; Function evaluation; Machine learning; Ontology; Structural design; Concept generation; Creativity and concept generation; Design tasks; Function-behavior-structure model; Function-Behaviour-Structure ontologies; Generative design; Language model; Large language model; Machine-learning; Subtask; Conceptual design},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Gilbert2024,
	author = {Gilbert, Stephen Henry and Kather, Jakob Nikolas and Hogan, Aidan},
	title = {Augmented non-hallucinating large language models as medical information curators},
	year = {2024},
	journal = {npj Digital Medicine},
	volume = {7},
	number = {1},
	pages = {},
	doi = {10.1038/s41746-024-01081-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191180518&doi=10.1038%2Fs41746-024-01081-0&partnerID=40&md5=ae5aee2925ddd6cd701c4dfcffeed891},
	abstract = {Reliably processing and interlinking medical information has been recognized as a critical foundation to the digital transformation of medical workflows, and despite the development of medical ontologies, the optimization of these has been a major bottleneck to digital medicine. The advent of large language models has brought great excitement, and maybe a solution to the medicines’ ‘communication problem’ is in sight, but how can the known weaknesses of these models, such as hallucination and non-determinism, be tempered? Retrieval Augmented Generation, particularly through knowledge graphs, is an automated approach that can deliver structured reasoning and a model of truth alongside LLMs, relevant to information structuring and therefore also to decision support. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Automation; Clinical Decision Support System; Communication Disorder; Digital Technology; Human; Information Retrieval; Knowledge Base; Large Language Model; Machine Learning; Medical Information; Medical Information System; Note},
	keywords = {automation; clinical decision support system; communication disorder; digital technology; human; information retrieval; knowledge base; large language model; machine learning; medical information; medical information system; Note},
	type = {Note},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 26; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Giri2024,
	author = {Giri, Swagarika Jaharlal and Ibtehaz, Nabil and Kihara, Daisuke},
	title = {GO2Sum: generating human-readable functional summary of proteins from GO terms},
	year = {2024},
	journal = {npj Systems Biology and Applications},
	volume = {10},
	number = {1},
	pages = {},
	doi = {10.1038/s41540-024-00358-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187902454&doi=10.1038%2Fs41540-024-00358-0&partnerID=40&md5=31d1997ef1b4d90111e3410be1dace27},
	abstract = {Understanding the biological functions of proteins is of fundamental importance in modern biology. To represent a function of proteins, Gene Ontology (GO), a controlled vocabulary, is frequently used, because it is easy to handle by computer programs avoiding open-ended text interpretation. Particularly, the majority of current protein function prediction methods rely on GO terms. However, the extensive list of GO terms that describe a protein function can pose challenges for biologists when it comes to interpretation. In response to this issue, we developed GO2Sum (Gene Ontology terms Summarizer), a model that takes a set of GO terms as input and generates a human-readable summary using the T5 large language model. GO2Sum was developed by fine-tuning T5 on GO term assignments and free-text function descriptions for UniProt entries, enabling it to recreate function descriptions by concatenating GO term descriptions. Our results demonstrated that GO2Sum significantly outperforms the original T5 model that was trained on the entire web corpus in generating Function, Subunit Structure, and Pathway paragraphs for UniProt entries. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Protein; Gene Ontology; Genetics; Human; Software; Gene Ontology; Humans; Proteins; Software},
	keywords = {protein; gene ontology; genetics; human; software; Gene Ontology; Humans; Proteins; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Stuart20241031,
	author = {Stuart, Susan Alice Jane},
	title = {Why language clouds our ascription of understanding, intention and consciousness},
	year = {2024},
	journal = {Phenomenology and the Cognitive Sciences},
	volume = {23},
	number = {5},
	pages = {1031 - 1052},
	doi = {10.1007/s11097-024-09970-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186607622&doi=10.1007%2Fs11097-024-09970-1&partnerID=40&md5=c1bcb45e7ebfc13366dce931a1ad37a8},
	abstract = {The grammatical manipulation and production of language is a great deceiver. We have become habituated to accept the use of well-constructed language to indicate intelligence, understanding and, consequently, intention, whether conscious or unconscious. But we are not always right to do so, and certainly not in the case of large language models (LLMs) like ChapGPT, GPT-4, LLaMA, and Google Bard. This is a perennial problem, but when one understands why it occurs, it ceases to be surprising that it so stubbornly persists. This paper will have three main sections. In the Introduction I will say a little about language, its aetiology, and useful sub-divisions into natural and cultural. In the second section I will explain the current situation with regard to large language models and fill in the background debates which set the problem up as one of increased complexity rather than one of a qualitatively different kind from narrow or specific AI. In the third section I will present the case for the missing phenomenological background and why it is necessary for the co-creation of shared meaning in both natural and cultural language, and I will conclude this section by presenting a rationale for why this situation arises and will continue to arise. Before we do any of this, I need to clarify one point: I do not wish to challenge the ascription of artificial general intelligence (AGI) to LLMs, indeed I think agnosticism is best in this respect, but I do challenge the more serious, and erroneous, ascription of understanding, intention, reason, and consciousness to them. And so, I am making two points: an epistemological one about why we fall into error in our ascription of a mental life to LLMs, and an ontological one about the impossibility of LLMs being or becoming conscious. © 2024 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Groza2024,
	author = {Groza, Tudor and Caufield, John Harry and Gration, Dylan and Baynam, Gareth S. and Haendel, Melissa Anne and Robinson, Peter Nicholas and Mungall, Christopher John and Reese, Justin T.},
	title = {An evaluation of GPT models for phenotype concept recognition},
	year = {2024},
	journal = {BMC Medical Informatics and Decision Making},
	volume = {24},
	number = {1},
	pages = {},
	doi = {10.1186/s12911-024-02439-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183914608&doi=10.1186%2Fs12911-024-02439-w&partnerID=40&md5=3f1de657ddaeaade257f2362670bc9cc},
	abstract = {Objective: Clinical deep phenotyping and phenotype annotation play a critical role in both the diagnosis of patients with rare disorders as well as in building computationally-tractable knowledge in the rare disorders field. These processes rely on using ontology concepts, often from the Human Phenotype Ontology, in conjunction with a phenotype concept recognition task (supported usually by machine learning methods) to curate patient profiles or existing scientific literature. With the significant shift in the use of large language models (LLMs) for most NLP tasks, we examine the performance of the latest Generative Pre-trained Transformer (GPT) models underpinning ChatGPT as a foundation for the tasks of clinical phenotyping and phenotype annotation. Materials and methods: The experimental setup of the study included seven prompts of various levels of specificity, two GPT models (gpt-3.5-turbo and gpt-4.0) and two established gold standard corpora for phenotype recognition, one consisting of publication abstracts and the other clinical observations. Results: The best run, using in-context learning, achieved 0.58 document-level F1 score on publication abstracts and 0.75 document-level F1 score on clinical observations, as well as a mention-level F1 score of 0.7, which surpasses the current best in class tool. Without in-context learning, however, performance is significantly below the existing approaches. Conclusion: Our experiments show that gpt-4.0 surpasses the state of the art performance if the task is constrained to a subset of the target ontology where there is prior knowledge of the terms that are expected to be matched. While the results are promising, the non-deterministic nature of the outcomes, the high cost and the lack of concordance between different runs using the same prompt and input make the use of these LLMs challenging for this particular task. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Generative Pretrained Transformer; Human Phenotype Ontology; Large Language Models; Phenotype Concept Recognition; Human; Knowledge; Language; Machine Learning; Phenotype; Rare Disease; Humans; Knowledge; Language; Machine Learning; Phenotype; Rare Diseases},
	keywords = {human; knowledge; language; machine learning; phenotype; rare disease; Humans; Knowledge; Language; Machine Learning; Phenotype; Rare Diseases},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Ayad20242611,
	author = {Ayad, Sarah and Alsayoud, Fatimah},
	title = {Prompt engineering techniques for semantic enhancement in business process models},
	year = {2024},
	journal = {Business Process Management Journal},
	volume = {30},
	number = {7},
	pages = {2611 - 2641},
	doi = {10.1108/BPMJ-02-2024-0108},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201561386&doi=10.1108%2FBPMJ-02-2024-0108&partnerID=40&md5=7d29bd76d8d750e89ba2238d8715bb27},
	abstract = {Purpose: The term knowledge refers to the part of the world investigated by a specific discipline and that includes a specific taxonomy, vocabulary, concepts, theories, research methods and standards of justification. Our approach uses domain knowledge to improve the quality of business process models (BPMs) by exploiting the domain knowledge provided by large language models (LLMs). Among these models, ChatGPT stands out as a notable example of an LLM capable of providing in-depth domain knowledge. The lack of coverage presents a limitation in each approach, as it hinders the ability to fully capture and represent the domain’s knowledge. To solve such limitations, we aim to exploit GPT-3.5 knowledge. Our approach does not ask GPT-3.5 to create a visual representation; instead, it needs to suggest missing concepts, thus helping the modeler improve his/her model. The GPT-3.5 may need to refine its suggestions based on feedback from the modeler. Design/methodology/approach: We initiate our semantic quality enhancement process of a BPM by first extracting crucial elements including pools, lanes, activities and artifacts, along with their corresponding relationships such as lanes being associated with pools, activities belonging to each lane and artifacts associated with each activity. These data are systematically gathered and structured into ArrayLists, a form of organized collection that allows for efficient data manipulation and retrieval. Once we have this structured data, our methodology involves creating a series of prompts based on each data element. We adopt three approaches to prompting: zero-shot, few-shot and chain of thoughts (CoT) prompts. Each type of prompting is specifically designed to interact with the OpenAI language model in a unique way, aiming to elicit a diverse array of suggestions. As we apply these prompting techniques, the OpenAI model processes each prompt and returns a list of suggestions tailored to that specific element of the BPM. Our approach operates independently of any specific notation and offers semi-automation, allowing modelers to select from a range of suggested options. Findings: This study demonstrates the significant potential of prompt engineering techniques in enhancing the semantic quality of BPMs when integrated with LLMs like ChatGPT. Our analysis of model activity richness and model artifact richness across different prompt techniques and input configurations reveals that carefully tailored prompts can lead to more complete BPMs. This research is a step forward for further exploration into the optimization of LLMs in BPM development. Research limitations/implications: The limitation is the domain ontology that we are relying on to evaluate the semantic completeness of the new BPM. In our future work, the modeler will have the option to ask for synonyms, hyponyms, hypernyms or keywords. This feature will facilitate the replacement of existing concepts to improve not only the completeness of the BPM but also the clarity and specificity of concepts in BPMs. Practical implications: To demonstrate our methodology, we take the “Hospitalization” process as an illustrative example. In the scope of our research, we have presented a select set of instructions pertinent to the “chain of thought” and “few-shot prompting.” Due to constraints in presentation and the extensive nature of the instructions, we have not included every detail within the body of this paper. However, they can be found in the previous GitHub link. Two appendices are given at the end. Appendix 1 describes the different prompt instructions. Appendix 2 presents the application of the instructions in our example. Originality/value: In our research, we rely on the domain application knowledge provided by ChatGPT-3 to enhance the semantic quality of BPMs. Typically, the semantic quality of BPMs may suffer due to the modeler's lack of domain knowledge. To address this issue, our approach employs three prompt engineering methods designed to extract accurate domain knowledge. By utilizing these methods, we can identify and propose missing concepts, such as activities and artifacts. This not only ensures a more comprehensive representation of the business process but also contributes to the overall improvement of the model's semantic quality, leading to more effective and accurate business process management. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Business Process Improvement; Business Process Modeling; Business Process Redesign; Prompt Engineering; Semantic Quality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Liu2024,
	author = {Liu, Dongge and Cheng, Liang},
	title = {MAKG: A maritime accident knowledge graph for intelligent accident analysis and management},
	year = {2024},
	journal = {Ocean Engineering},
	volume = {312},
	pages = {},
	doi = {10.1016/j.oceaneng.2024.119280},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204184779&doi=10.1016%2Fj.oceaneng.2024.119280&partnerID=40&md5=743a74cc31933a980100c0ca70de6cc3},
	abstract = {With the increasing frequency of human activities at sea, maritime accidents are occurring more often. Analyzing and mining maritime accident cases can help uncover the causal mechanisms behind these incidents, thereby enhancing maritime safety. As an emerging technology for knowledge management and mining, knowledge graphs offer significant support for the storage, reasoning, and decision-making processes related to maritime accidents. In this study, we established a knowledge graph construction and application framework for maritime accidents to facilitates the extraction and management of maritime knowledge from unstructured texts. First, 581 accident reports released by the China Maritime Safety Administration over the past decade (2014–2023) were used as the data basis for analysis and construction of the maritime accident ontology structure using the seven-step method, which comprises 8 entity types, 8 relationship types, and 18 attribute entity types. Second, We proposed MBERT-BiLSTM-CRF-SF, a named entity recognition model based on domain pretraining and self-training, to reduce graph construction costs. This model achieved state-of-the-art performance in the maritime domain, with an F1 score of 0.910 ± 0.006, which is about 5% higher than the mainstream model. In addition, we proposed an entity alignment method based on font and semantics to refine knowledge further. On the basis of the proposed method, we constructed a large, high-quality maritime accident knowledge graph (MAKG) system that contains 16,099 entities and 20,809 relationship instances. Finally, we reduced the complexity of applying knowledge graphs by integrating the CRISPE prompt learning framework of the large language model, and experiments on graph traversal, pattern recognition, and aggregation analysis were conducted to assess the quality of MAKG. Results demonstrate that MAKG can effectively enhance the efficiency of querying and reasoning about maritime accident information, thus providing significant support for the prevention and management of maritime accidents. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Knowledge Graph; Maritime Accident; Named Entity Recognition; Prompt Learning; Decision Making; Semantics; Accident Analysis; Accident Management; Bert; Entity-types; Graph Construction; Knowledge Graphs; Maritime Accidents; Maritime Safety; Named Entity Recognition; Prompt Learning; Knowledge Graph; Accident; Accident Prevention; Complexity; Decision Making; Information System; Maritime Transportation; Performance Assessment; Transportation Safety; Transportation Technology; China},
	keywords = {Decision making; Semantics; Accident analysis; Accident management; BERT; Entity-types; Graph construction; Knowledge graphs; Maritime accidents; Maritime safety; Named entity recognition; Prompt learning; Knowledge graph; accident; accident prevention; complexity; decision making; information system; maritime transportation; performance assessment; transportation safety; transportation technology; China},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{2024,
	title = {13th Symposium on Languages, Applications and Technologies, SLATE 2024},
	year = {2024},
	journal = {OpenAccess Series in Informatics},
	volume = {120},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210417340&partnerID=40&md5=02e32a2ed7125b0cfa9c7522188fb98b},
	abstract = {The proceedings contain 14 papers. The topics discussed include: using embeddings to improve named entity recognition classification with graphs; contributions to legal document summarization: judgments from the Portuguese supreme court of justice; ontology visualization tools: a bibliographic review and a proposal; early findings in using LLMs to assess semantic relations strength; experimental comparison of the effectiveness and error rates of projectional and text editors; automatic and dynamic visualization of process-based concurrent programs; upgrade of lark compiler generator to support attribute grammars; DrumLace – a domain specific language (DSL) for drum programming; towards an intelligent algorithm for profile authentication and identification; and a language for explaining counterexamples. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yu2024,
	author = {Yu, Jingwen and Wang, Yaohao and Wang, Haidong and Wei, Zhi and Pei, Yonggang},
	title = {Decoding Critical Targets and Signaling Pathways in EBV-Mediated Diseases Using Large Language Models},
	year = {2024},
	journal = {Viruses},
	volume = {16},
	number = {11},
	pages = {},
	doi = {10.3390/v16111660},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210446933&doi=10.3390%2Fv16111660&partnerID=40&md5=270feed8efa25e1d29a0cd0e204e9d5d},
	abstract = {Epstein–Barr virus (EBV), a member of the gamma herpesvirus, is the first identified human oncovirus and is associated with various malignancies. Understanding the intricate interactions between EBV antigens and cellular pathways is crucial to unraveling the molecular mechanisms in EBV-mediated diseases. However, fully elucidating EBV–host interactions and the associated pathogenesis remains a significant challenge. In this study, we employed large language models (LLMs) to screen 36,105 EBV-relevant scientific publications and summarize the current literature landscape on various EBV-associated diseases like Burkitt lymphoma (BL), diffuse large B-cell lymphoma (DLBCL), nasopharyngeal carcinoma (NPC), and so on. LLM-generated data indicate that the most-studied EBV-associated pathways are enriched in immune response, apoptosis, cell growth, and replication. The analyses of protein–protein interactions (PPIs) reveal three principal EBV-related protein clusters: TP53-centered apoptotic factors, EBV-associated transcription factors, and immune response elements. Utilizing our dataset and public databases, we demonstrated that BLLF3-targeted TLR2-associated factors are effective diagnostic markers for DLBCL. Next, we confirmed the co-expression of LMP1-targeted calcium pathway factors in BL. Finally, we demonstrated the correlation and co-expression of LMP1-induced PARP1, HIF1A, HK2, and key glycolysis-related factors, further suggesting that LMP1 actively regulates the glycolysis pathway. Therefore, our study presents a comprehensive functional encyclopedia of the interactions between EBV antigens and host signaling pathways across various EBV-associated diseases, providing valuable insights for the development of therapeutic strategies. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ebv; Ebv-mediated Diseases; Llm; Signaling Pathways; Host Factor; Interleukin 17; Microrna; Transcription Factor; Apoptosis; Article; Burkitt Lymphoma; Carcinogenesis; Cell Growth; Cell Proliferation; Cell Survival; Diffuse Large B Cell Lymphoma; Epstein Barr Virus; Epstein Barr Virus Infection; Gene Expression; Gene Mutation; Gene Ontology; Gene Silencing; Immune Response; Innate Immunity; Large Language Model; Lipid Metabolism; Mapk Signaling; Mismatch Repair; Nasopharynx Carcinoma; Nonhuman; Omics; Oxidative Phosphorylation; Pi3k/akt Signaling; Protein Homeostasis; Protein Misfolding; Protein Phosphorylation; Protein Protein Interaction; Signal Transduction; Transcription Regulation; Tumor Microenvironment; Ubiquitination; Unfolded Protein Response; Upregulation; Host Pathogen Interaction; Human; Metabolism; Physiology; Virology; Burkitt Lymphoma; Epstein-barr Virus Infections; Herpesvirus 4, Human; Host-pathogen Interactions; Humans; Lymphoma, Large B-cell, Diffuse; Nasopharyngeal Carcinoma; Protein Interaction Maps; Signal Transduction},
	keywords = {host factor; interleukin 17; microRNA; transcription factor; apoptosis; Article; Burkitt lymphoma; carcinogenesis; cell growth; cell proliferation; cell survival; diffuse large B cell lymphoma; Epstein Barr virus; Epstein Barr virus infection; gene expression; gene mutation; gene ontology; gene silencing; immune response; innate immunity; large language model; lipid metabolism; MAPK signaling; mismatch repair; nasopharynx carcinoma; nonhuman; omics; oxidative phosphorylation; Pi3K/Akt signaling; protein homeostasis; protein misfolding; protein phosphorylation; protein protein interaction; signal transduction; transcription regulation; tumor microenvironment; ubiquitination; unfolded protein response; upregulation; host pathogen interaction; human; metabolism; physiology; virology; Burkitt Lymphoma; Epstein-Barr Virus Infections; Herpesvirus 4, Human; Host-Pathogen Interactions; Humans; Lymphoma, Large B-Cell, Diffuse; Nasopharyngeal Carcinoma; Protein Interaction Maps; Signal Transduction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Liu2024,
	author = {Liu, Dan and Young, Francesca and Lamb, Kieran D. and Robertson, David L. and Yuan, Ke},
	title = {Prediction of virus-host associations using protein language models and multiple instance learning},
	year = {2024},
	journal = {PLOS Computational Biology},
	volume = {20},
	number = {11},
	pages = {},
	doi = {10.1371/journal.pcbi.1012597},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209872540&doi=10.1371%2Fjournal.pcbi.1012597&partnerID=40&md5=ae39555c06e31b337640aae8e01aa269},
	abstract = {Predicting virus-host associations is essential to determine the specific host species that viruses interact with, and discover if new viruses infect humans and animals. Currently, the host of the majority of viruses is unknown, particularly in microbiomes. To address this challenge, we introduce EvoMIL, a deep learning method that predicts the host species for viruses from viral sequences only. It also identifies important viral proteins that significantly contribute to host prediction. The method combines a pre-trained large protein language model (ESM) and attention-based multiple instance learning to allow protein-orientated predictions. Our results show that protein embeddings capture stronger predictive signals than sequence composition features, including amino acids, physiochemical properties, and DNA k-mers. In multi-host prediction tasks, EvoMIL achieves median F1 score improvements of 10.8%, 16.2%, and 4.9% in prokaryotic hosts, and 1.7%, 6.6% and 11.5% in eukaryotic hosts. EvoMIL binary classifiers achieve impressive AUC over 0.95 for all prokaryotic hosts and range from roughly 0.8 to 0.9 for eukaryotic hosts. Furthermore, EvoMIL identifies important proteins in the prediction task. We found them capturing key functions in virus-host specificity. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Viral Proteins; Prediction Models; Embeddings; Eukaryotics; Language Model; Learning Methods; Microbiome; Multiple-instance Learning; Prediction Tasks; Predictive Signals; Prokaryotics; Viral Proteins; Invertebrates; Amino Acid Sequence; Area Under The Curve; Article; Cross Validation; Deep Learning; False Negative Result; False Positive Result; Feature Extraction; Gene Ontology; Hierarchical Clustering; Human; Learning; Multiple Instance Learning; Mycobacterium Smegmatis; Physical Chemistry; Prediction; Protein Language Model; Receiver Operating Characteristic; Sensitivity And Specificity; Taxonomy; Virus Cell Interaction; Animal; Bioinformatics; Chemistry; Host Microbe Interaction; Host Pathogen Interaction; Metabolism; Physiology; Procedures; Virus; Viral Protein; Animals; Computational Biology; Deep Learning; Host Microbial Interactions; Host-pathogen Interactions; Humans; Viral Proteins; Viruses},
	keywords = {Prediction models; Embeddings; Eukaryotics; Language model; Learning methods; Microbiome; Multiple-instance learning; Prediction tasks; Predictive signals; Prokaryotics; Viral proteins; Invertebrates; amino acid sequence; area under the curve; Article; cross validation; deep learning; false negative result; false positive result; feature extraction; gene ontology; hierarchical clustering; human; learning; multiple instance learning; Mycobacterium smegmatis; physical chemistry; prediction; protein language model; receiver operating characteristic; sensitivity and specificity; taxonomy; virus cell interaction; animal; bioinformatics; chemistry; host microbe interaction; host pathogen interaction; metabolism; physiology; procedures; virus; viral protein; Animals; Computational Biology; Deep Learning; Host Microbial Interactions; Host-Pathogen Interactions; Humans; Viral Proteins; Viruses},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Gold Open Access}
}

@ARTICLE{Ivanisenko2024,
	author = {Ivanisenko, Timofey Vladimirovich and Demenkov, Pavel S. and Ivanisenko, Vladimir Aleksandrovich},
	title = {An Accurate and Efficient Approach to Knowledge Extraction from Scientific Publications Using Structured Ontology Models, Graph Neural Networks, and Large Language Models},
	year = {2024},
	journal = {International Journal of Molecular Sciences},
	volume = {25},
	number = {21},
	pages = {},
	doi = {10.3390/ijms252111811},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208591467&doi=10.3390%2Fijms252111811&partnerID=40&md5=d5aa2407bf186b4b8a5ba56165c6b138},
	abstract = {The rapid growth of biomedical literature makes it challenging for researchers to stay current. Integrating knowledge from various sources is crucial for studying complex biological systems. Traditional text-mining methods often have limited accuracy because they don’t capture semantic and contextual nuances. Deep-learning models can be computationally expensive and typically have low interpretability, though efforts in explainable AI aim to mitigate this. Furthermore, transformer-based models have a tendency to produce false or made-up information—a problem known as hallucination—which is especially prevalent in large language models (LLMs). This study proposes a hybrid approach combining text-mining techniques with graph neural networks (GNNs) and fine-tuned large language models (LLMs) to extend biomedical knowledge graphs and interpret predicted edges based on published literature. An LLM is used to validate predictions and provide explanations. Evaluated on a corpus of experimentally confirmed protein interactions, the approach achieved a Matthews correlation coefficient (MCC) of 0.772. Applied to insomnia, the approach identified 25 interactions between 32 human proteins absent in known knowledge bases, including regulatory interactions between MAOA and 5-HT2C, binding between ADAM22 and 14-3-3 proteins, which is implicated in neurological diseases, and a circadian regulatory loop involving RORB and NR1D1. The hybrid GNN-LLM method analyzes biomedical literature efficiency to uncover potential molecular interactions for complex disorders. It can accelerate therapeutic target discovery by focusing expert verification on the most relevant automatically extracted information. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Andsystem; Deep Learning; Gnn; Knowledge Graph; Llm; Text-mining; Protein 14 3 3; Adam Protein; Amine Oxidase (flavin Containing) Isoenzyme A; Nuclear Receptor Nr1d1; Protein 14 3 3; Protein Adam 22; Retinoid Related Orphan Receptor Beta; Serotonin 2c Receptor; Unclassified Drug; Accuracy; Article; Binary Classification; Circadian Rhythm; Data Mining; Graph Neural Network; Human; Insomnia; Knowledge Base; Large Language Model; Medical Literature; Multilayer Perceptron; Neurologic Disease; Prediction; Protein Binding; Protein Protein Interaction; Publication; Urban Health; Artificial Neural Network; Deep Learning; Procedures; Data Mining; Deep Learning; Humans; Knowledge Bases; Neural Networks, Computer; Publications},
	keywords = {ADAM protein; amine oxidase (flavin containing) isoenzyme A; nuclear receptor NR1D1; protein 14 3 3; protein adam 22; retinoid related orphan receptor beta; serotonin 2C receptor; unclassified drug; accuracy; Article; binary classification; circadian rhythm; data mining; graph neural network; human; insomnia; knowledge base; large language model; medical literature; multilayer perceptron; neurologic disease; prediction; protein binding; protein protein interaction; publication; urban health; artificial neural network; deep learning; procedures; Data Mining; Deep Learning; Humans; Knowledge Bases; Neural Networks, Computer; Publications},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Bai2024,
	author = {Bai, Peihao and Li, Guanghui and Luo, Jiawei and Liang, Cheng},
	title = {Deep learning model for protein multi-label subcellular localization and function prediction based on multi-Task collaborative training},
	year = {2024},
	journal = {Briefings in Bioinformatics},
	volume = {25},
	number = {6},
	pages = {},
	doi = {10.1093/bib/bbae568},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208497965&doi=10.1093%2Fbib%2Fbbae568&partnerID=40&md5=7d99386bb6b33577a7380baa4aedb6c8},
	abstract = {The functional study of proteins is a critical task in modern biology, playing a pivotal role in understanding the mechanisms of pathogenesis, developing new drugs, and discovering novel drug targets. However, existing computational models for subcellular localization face significant challenges, such as reliance on known Gene Ontology (GO) annotation databases or overlooking the relationship between GO annotations and subcellular localization. To address these issues, we propose DeepMTC, an end-To-end deep learning-based multi-Task collaborative training model. DeepMTC integrates the interrelationship between subcellular localization and the functional annotation of proteins, leveraging multi-Task collaborative training to eliminate dependence on known GO databases. This strategy gives DeepMTC a distinct advantage in predicting newly discovered proteins without prior functional annotations. First, DeepMTC leverages pre-Trained language model with high accuracy to obtain the 3D structure and sequence features of proteins. Additionally, it employs a graph transformer module to encode protein sequence features, addressing the problem of long-range dependencies in graph neural networks. Finally, DeepMTC uses a functional cross-Attention mechanism to efficiently combine upstream learned functional features to perform the subcellular localization task. The experimental results demonstrate that DeepMTC outperforms state-of-The-Art models in both protein function prediction and subcellular localization. Moreover, interpretability experiments revealed that DeepMTC can accurately identify the key residues and functional domains of proteins, confirming its superior performance. The code and dataset of DeepMTC are freely available at https://github.com/ghli16/DeepMTC. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Transformer; Multi-task Collaborative Training; Pre-trained Language Model; Protein Function Prediction; Subcellular Localization; Protein; Proteins; Protein; Artificial Neural Network; Bioinformatics; Deep Learning; Gene Ontology; Human; Metabolism; Procedures; Protein Database; Computational Biology; Databases, Protein; Deep Learning; Gene Ontology; Humans; Neural Networks, Computer; Proteins},
	keywords = {protein; artificial neural network; bioinformatics; deep learning; gene ontology; human; metabolism; procedures; protein database; Computational Biology; Databases, Protein; Deep Learning; Gene Ontology; Humans; Neural Networks, Computer; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Fiedler202454,
	author = {Fiedler, Anna K. and Zhang, Kai and Lal, Tia S. and Jiang, Xiaoqian and Fraser, Stuart M.},
	title = {Generative Pre-trained Transformer for Pediatric Stroke Research: A Pilot Study},
	year = {2024},
	journal = {Pediatric Neurology},
	volume = {160},
	pages = {54 - 59},
	doi = {10.1016/j.pediatrneurol.2024.07.001},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202057125&doi=10.1016%2Fj.pediatrneurol.2024.07.001&partnerID=40&md5=0e1bf2bc848e586d2f37a344773a3a28},
	abstract = {Background: Pediatric stroke is an important cause of morbidity in children. Although research can be challenging, large amounts of data have been captured through collaborative efforts in the International Pediatric Stroke Study (IPSS). This study explores the use of an advanced artificial intelligence program, the Generative Pre-trained Transformer (GPT), to enter pediatric stroke data into the IPSS. Methods: The most recent 50 clinical notes of patients with ischemic stroke or cerebral venous sinus thrombosis at the UTHealth Pediatric Stroke Clinic were deidentified. Domain-specific prompts were engineered for an offline artificial intelligence program (GPT) to answer IPSS questions. Responses from GPT were compared with the human rater. Percent agreement was assessed across 50 patients for each of the 114 queries developed from the IPSS database outcome questionnaire. Results: GPT demonstrated strong performance on several questions but showed variability overall. In its early iterations it was able to match human judgment occasionally with an accuracy score of 1.00 (n = 20, 17.5%), but it scored as low as 0.26 in some patients. Prompts were adjusted in four subsequent iterations to increase accuracy. In its fourth iteration, agreement was 93.6%, with a maximum agreement of 100% and minimum of 62%. Of 2400 individual items assessed, our model entered 2247 (93.6%) correctly and 153 (6.4%) incorrectly. Conclusions: Although our tailored generative model with domain-specific prompt engineering and ontological guidance shows promise for research applications, further refinement is needed to enhance its accuracy. It cannot enter data entirely independently, but it can be employed in tandem with human oversight contributing to a collaborative approach that reduces overall effort. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Gpt; Ipss; Llm; Pediatric Stroke; Ps-gpt; Article; Artificial Intelligence; Cerebral Sinus Thrombosis; Cerebrovascular Accident; Child; Clinical Article; Female; Follow Up; Generative Pretrained Transformer; Hallucination; Human; Male; Pilot Study; Retrospective Study; Adolescent; Ischemic Stroke; Medical Research; Preschool Child; Therapy; Adolescent; Artificial Intelligence; Biomedical Research; Child; Child, Preschool; Female; Humans; Ischemic Stroke; Male; Pilot Projects; Sinus Thrombosis, Intracranial; Stroke},
	keywords = {Article; artificial intelligence; cerebral sinus thrombosis; cerebrovascular accident; child; clinical article; female; follow up; generative pretrained transformer; hallucination; human; male; pilot study; retrospective study; adolescent; ischemic stroke; medical research; preschool child; therapy; Adolescent; Artificial Intelligence; Biomedical Research; Child; Child, Preschool; Female; Humans; Ischemic Stroke; Male; Pilot Projects; Sinus Thrombosis, Intracranial; Stroke},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Burgueño2024578,
	author = {Burgueño, Lola and Maria Keet, C. and Kienzle, Jörg and Michael, Judith and Babur, Önder},
	title = {A Human Behavior Exploration Approach Using LLMs for Cyber-Physical Systems},
	year = {2024},
	pages = {578 - 586},
	doi = {10.1145/3652620.3687806},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212228857&doi=10.1145%2F3652620.3687806&partnerID=40&md5=51589c3feb4464f19e0cc5499e4832b4},
	abstract = {In the early phases of Cyber-Physical Systems (CPS) development, scoping human behavior plays a significant role, especially when interactions extend beyond expected behavior. Here, it is especially challenging to develop cases that capture the full spectrum of human behavior. Up to now, identifying such behavior of humans remains a task for domain experts. We explore how one can use Large Languages Models (LLMs) in the design phase of systems to provide additional information about human-CPS interaction. Our approach proposes a preliminary ontology describing a hierarchy of types of behavior and relevant CPS components as input for prompt templates. It uses them to generate parts of human behavior descriptions, as well as a canned prompt with one variable about behavior. For demonstration, we take a smart building with a Home Energy System as the use case. An initial user evaluation shows that the behavior descriptions generated with standard and ontology-driven prompts complement each other and are useful when assisting humans. The discovered uncommon behaviors can be used to complete interaction scenarios that eventually result in a more robust CPS implementation. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cyber-physical Systems; Digital Twin; Human Behavior; Large Language Models; User Scenario; Hierarchical Systems; Behaviour Descriptions; Cybe-physical Systems; Cyber-physical Systems; Human Behaviors; Language Model; Large Language Model; Ontology's; Scoping; System Development; User Scenario; Ontology},
	keywords = {Hierarchical systems; Behaviour descriptions; Cybe-physical systems; Cyber-physical systems; Human behaviors; Language model; Large language model; Ontology's; Scoping; System development; User scenario; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Mulayim2024312,
	author = {Mulayim, Ozan Baris and Paul, Lazlo and Pritoni, Marco and Prakash, Anand Krishnan and Sudarshan, Malavikha and Fierro, Gabe},
	title = {Large Language Models for the Creation and Use of Semantic Ontologies in Buildings: Requirements and Challenges},
	year = {2024},
	pages = {312 - 317},
	doi = {10.1145/3671127.3698792},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211353946&doi=10.1145%2F3671127.3698792&partnerID=40&md5=6a17f987aa13e3b6f731061dedb22409},
	abstract = {Semantic ontologies offer a formalized, machine-readable framework for representing knowledge, enabling the structured description of complex systems. In the building domain, the adoption of ontologies like the Brick schema has transformed how buildings and their systems are modeled by providing a standardized, interoperable language. However, the complexity and the steep learning curve involved in developing and querying semantic models present substantial challenges, often requiring a workforce with specialized expertise. This paper builds on our experience in investigating how Large Language Models (LLMs) can help address these challenges, focusing on their role in constructing and querying of semantic models, particularly using the Brick Schema. Our study outlines the requirements and metrics for evaluating the scalability and effectiveness of LLM-based tools, while also discussing the current challenges and limitations in developing such tools. Ultimately, this paper aims to orient research efforts as various groups experiment with diverse techniques, while enabling more effective comparison of emerging solutions and fostering collaboration across the field. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graphs; Large Language Models; Semantic Ontology; Brick; Knowledge Graph; Latent Semantic Analysis; Ontology; Query Languages; Semantics; Building Requirements; In-buildings; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Querying Semantics; Semantic Modelling; Semantic Ontology; Steep Learning Curve; Structured Query Language},
	keywords = {Brick; Knowledge graph; Latent semantic analysis; Ontology; Query languages; Semantics; Building requirements; In-buildings; Knowledge graphs; Language model; Large language model; Ontology's; Querying semantics; Semantic modelling; Semantic ontology; Steep learning curve; Structured Query Language},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Monka20244665,
	author = {Monka, Sebastian and Grangel-González, Irlán and Schmid, Stefan and Halilaj, Lavdim and Rickart, Marc and Rudolph, Oliver and Dias, Rui},
	title = {Enhancing Manufacturing Knowledge Access with LLMs and Context-Aware Prompting},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {392},
	pages = {4665 - 4672},
	doi = {10.3233/FAIA241062},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216677054&doi=10.3233%2FFAIA241062&partnerID=40&md5=d7e19bae8d68aba6382f30c1f316dafa},
	abstract = {Knowledge graphs (KGs) have transformed data management within the manufacturing industry, offering effective means for integrating disparate data sources through shared and structured conceptual schemas. However, harnessing the power of KGs can be daunting for non-experts, as it often requires formulating complex SPARQL queries to retrieve specific information. With the advent of Large Language Models (LLMs), there is a growing potential to automatically translate natural language queries into the SPARQL format, thus bridging the gap between user-friendly interfaces and the sophisticated architecture of KGs. The challenge remains in adequately informing LLMs about the relevant context and structure of domain-specific KGs, e.g., in manufacturing, to improve the accuracy of generated queries. In this paper, we evaluate multiple strategies that use LLMs as mediators to facilitate information retrieval from KGs. We focus on the manufacturing domain, particularly on the Bosch Line Information System KG and the I40 Core Information Model. In our evaluation, we compare various approaches for feeding relevant context from the KG to the LLM and analyze their proficiency in transforming real-world questions into SPARQL queries. Our findings show that LLMs can significantly improve their performance on generating correct and complete queries when provided only the adequate context of the KG schema. Such context-aware prompting techniques help LLMs to focus on the relevant parts of the ontology and reduce the risk of hallucination. We anticipate that the proposed techniques help LLMs to democratize access to complex data repositories and empower informed decision-making in manufacturing settings. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Integration; Information Management; Knowledge Graph; Manufacturing Data Processing; Modeling Languages; Natural Language Processing Systems; Query Languages; Smart Manufacturing; Translation (languages); Conceptual Schemas; Context-aware Prompting; Data-source; Knowledge Graphs; Language Model; Manufacturing Industries; Manufacturing Knowledge; Natural Language Queries; Power; Specific Information; Structured Query Language},
	keywords = {Data integration; Information management; Knowledge graph; Manufacturing data processing; Modeling languages; Natural language processing systems; Query languages; Smart manufacturing; Translation (languages); Conceptual schemas; Context-aware prompting; Data-source; Knowledge graphs; Language model; Manufacturing industries; Manufacturing knowledge; Natural language queries; Power; Specific information; Structured Query Language},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Ozaki202425,
	author = {Ozaki, Ana},
	title = {Actively Learning from Machine Learning Models with Queries and Counterexamples (Extended Abstract)},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {392},
	pages = {25 - 26},
	doi = {10.3233/FAIA240462},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213380175&doi=10.3233%2FFAIA240462&partnerID=40&md5=8680253abfcc62dadd1db271956269f0},
	abstract = {We consider the exact and probably approximately correct (PAC) learning frameworks from computational learning theory and discuss opportunities and challenges for applying notions developed within these frameworks to extract information from black-box machine learning models, in particular, from language models. We discuss recent works that consider algorithms designed for the exact and PAC frameworks to extract information in the format of automata, Horn theories, and ontologies from machine learning models and possible applications of these approaches for understanding the models, studying biases, and knowledge acquisition. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Active Learning; Contrastive Learning; Federated Learning; Self-supervised Learning; Black Boxes; Computational Learning Theory; Extended Abstracts; Extract Informations; Horn Theory; Language Model; Learning Frameworks; Machine Learning Models; Probably Approximately Correct; Probably Approximately Correct Learning; Adversarial Machine Learning},
	keywords = {Active learning; Contrastive Learning; Federated learning; Self-supervised learning; Black boxes; Computational learning theory; Extended abstracts; Extract informations; Horn theory; Language model; Learning frameworks; Machine learning models; Probably approximately correct; Probably approximately correct learning; Adversarial machine learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Silva20241333,
	author = {Silva, Marta Contreiras and Faria, Daniel and Pesquita, Cátia},
	title = {Complex Multi-Ontology Alignment Through Geometric Operations on Language Embeddings},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {392},
	pages = {1333 - 1340},
	doi = {10.3233/FAIA240632},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213368903&doi=10.3233%2FFAIA240632&partnerID=40&md5=2a4ee19f34446fb87dbc163e17bc4910},
	abstract = {With knowledge graphs increasing in popularity, aligning and integrating them is paramount to ensure their usefulness and reusability. A key step in this process is ontology matching, whereby the semantic models of KGs are aligned into a single cohesive semantic backbone. While finding simple pairwise equivalences between entities in two ontologies is well addressed by state-of-the-art algorithms, finding more complex mappings that can include multiple entities from different ontologies is far from solved, despite their importance in ensuring a deep and meaningful integration of KGs. We propose a novel complex ontology matching approach that explores geometric operations over the shared semantic space afforded by large language models, enabling the discovery of complex mappings that are missed by purely lexical approaches. We evaluate our approach on several biomedical ontologies using partial reference alignments and manual expert validation. Our approach improves on the performance of a purely lexical approach while also increasing the coverage of complex multi-ontology alignments by 20 to 80%, which translates to a 97% coverage of the source ontologies. Moreover, the manual evaluation of the mappings produced by LLM shows that it achieves a high level of precision. This work demonstrates that the use of LLMs can improve on the performance of traditional lexical strategies. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Embeddings; Knowledge Graph; Latent Semantic Analysis; Ontology; Reusability; Semantics; Complex Mapping; Embeddings; Geometric Operations; Knowledge Graphs; Multi-ontologies; Ontology Alignment; Ontology Matching; Ontology's; Performance; Semantic Modelling; Mapping},
	keywords = {Graph embeddings; Knowledge graph; Latent semantic analysis; Ontology; Reusability; Semantics; Complex mapping; Embeddings; Geometric operations; Knowledge graphs; Multi-ontologies; Ontology alignment; Ontology matching; Ontology's; Performance; Semantic modelling; Mapping},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Chen20241389,
	author = {Chen, Jieying and Dong, Hang and Chen, Jiaoyan and Horrocks, Ian},
	title = {Ontology Text Alignment: Aligning Textual Content to Terminological Axioms},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {392},
	pages = {1389 - 1396},
	doi = {10.3233/FAIA240639},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213316349&doi=10.3233%2FFAIA240639&partnerID=40&md5=2c1c97aa38f8bdd7b3fb3b8b1d142d78},
	abstract = {Despite the impressive advancements in Large Language Models (LLMs), their ability to perform reasoning and provide explainable outcomes remains a challenge, underscoring the continued relevance of ontologies in certain areas, particularly due to the reasoning and validation capabilities of ontologies. Ontology modelling and semantic search, due to their inherent complexity, still demand considerable human effort and expertise. Addressing this gap, our paper introduces the problem of ontology text alignment, which involves finding the most relevant axioms with respect to the given reference text. We propose an advanced Retrieval Augmented Generation framework that leverages BERT models and generative LLMs, together with ontology semantic enhancement based on atomic decomposition. Additionally, we have developed benchmarks in geology and biomedical areas. Our evaluation demonstrates the positive impact of our framework. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmarking; Ontology; Language Model; Model Search; Ontology Model; Ontology Semantics; Ontology's; Reasoning Capabilities; Semantic Search; Text Alignments; Textual Content; Validation Capability; Semantics},
	keywords = {Benchmarking; Ontology; Language model; Model search; Ontology model; Ontology semantics; Ontology's; Reasoning capabilities; Semantic search; Text alignments; Textual content; Validation capability; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Mansourian2024,
	author = {Mansourian, Ali and Oucheikh, Rachid},
	title = {ChatGeoAI: Enabling Geospatial Analysis for Public through Natural Language, with Large Language Models},
	year = {2024},
	journal = {ISPRS International Journal of Geo-Information},
	volume = {13},
	number = {10},
	pages = {},
	doi = {10.3390/ijgi13100348},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207678194&doi=10.3390%2Fijgi13100348&partnerID=40&md5=9bf1853a4e4b5e03c7120f62062c3d4c},
	abstract = {Large Language Models (LLMs) such as GPT, BART, and Gemini stand at the forefront of Generative Artificial Intelligence, showcasing remarkable prowess in natural language comprehension and task execution. This paper proposes a novel framework developed on the foundation of Llama 2, aiming to bridge the gap between natural language queries and executable code for geospatial analyses within the PyQGIS environment. It empowers non-expert users to leverage GIS technology without requiring deep knowledge of geospatial programming or tools. Through cutting-edge Natural Language Processing (NLP) techniques, including tailored entity recognition and ontology mapping, the framework accurately interprets user intents and translates them into specific GIS operations. Integration of geospatial ontologies enriches semantic comprehension, ensuring precise alignment between user descriptions, geospatial datasets, and geospatial analysis tasks. A code generation module empowered by Llama 2 converts these interpretations into PyQGIS scripts, enabling the execution of geospatial analysis and results visualization. Rigorous testing across a spectrum of geospatial analysis tasks, with incremental complexity, evaluates the framework and the performance of such a system, with LLM at its core. The proposed system demonstrates proficiency in handling various geometries, spatial relationships, and attribute queries, enabling accurate and efficient analysis of spatial datasets. Moreover, it offers robust error-handling mechanisms and supports tasks related to map styling, visualization, and data manipulation. However, it has some limitations, such as occasional struggles with ambiguous attribute names and aliases, which leads to potential inaccuracies in the filtering and retrieval of features. Despite these limitations, the system presents a promising solution for applications integrating LLMs into GIS and offers a flexible and user-friendly approach to geospatial analysis. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Code Generation; Geoai; Geospatial Analysis; Gis; Gis Democratization; Large Language Model; Llama; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access; Gold Open Access}
}

@ARTICLE{Al Shuraiqi2024,
	author = {Al Shuraiqi, Somaiya and AalAbdulsalam, Abdulrahman Khalifa and Masters, Ken and Zidoum, Hamza M. and AlZaabi, Adhari Abdullah},
	title = {Automatic Generation of Medical Case-Based Multiple-Choice Questions (MCQs): A Review of Methodologies, Applications, Evaluation, and Future Directions},
	year = {2024},
	journal = {Big Data and Cognitive Computing},
	volume = {8},
	number = {10},
	pages = {},
	doi = {10.3390/bdcc8100139},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207495519&doi=10.3390%2Fbdcc8100139&partnerID=40&md5=cba137b8216c16d7ca47dae03ee54230},
	abstract = {This paper offers an in-depth review of the latest advancements in the automatic generation of medical case-based multiple-choice questions (MCQs). The automatic creation of educational materials, particularly MCQs, is pivotal in enhancing teaching effectiveness and student engagement in medical education. In this review, we explore various algorithms and techniques that have been developed for generating MCQs from medical case studies. Recent innovations in natural language processing (NLP) and machine learning (ML) for automatic language generation have garnered considerable attention. Our analysis evaluates and categorizes the leading approaches, highlighting their generation capabilities and practical applications. Additionally, this paper synthesizes the existing evidence, detailing the strengths, limitations, and gaps in current practices. By contributing to the broader conversation on how technology can support medical education, this review not only assesses the present state but also suggests future directions for improvement. We advocate for the development of more advanced and adaptable mechanisms to enhance the automatic generation of MCQs, thereby supporting more effective learning experiences in medical education. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Automatic Question Generation (aqg); Case-based Multiple-choice Questions (mcqs); Deep Learning (dl); Large Language Model (llm); Machine Learning (ml); Natural Language Processing (nlp); Ontology; Adversarial Machine Learning; Contrastive Learning; Deep Learning; Natural Language Processing Systems; Question Answering; Automatic Question Generation; Case Based; Case-based Multiple-choice Question; Language Model; Language Processing; Large Language Model; Machine Learning; Machine-learning; Multiple-choice Questions; Natural Language Processing; Natural Languages; Ontology's; Medical Education},
	keywords = {Adversarial machine learning; Contrastive Learning; Deep learning; Natural language processing systems; Question answering; Automatic question generation; Case based; Case-based multiple-choice question; Language model; Language processing; Large language model; Machine learning; Machine-learning; Multiple-choice questions; Natural language processing; Natural languages; Ontology's; Medical education},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access}
}

@ARTICLE{Tupayachi20242392,
	author = {Tupayachi, Jose and Xu, Haowen and Omitaomu, Olufemi A. and Camur, Mustafa Can and Sharmin, Aliza and Li, Xueping},
	title = {Towards Next-Generation Urban Decision Support Systems through AI-Powered Construction of Scientific Ontology Using Large Language Models—A Case in Optimizing Intermodal Freight Transportation},
	year = {2024},
	journal = {Smart Cities},
	volume = {7},
	number = {5},
	pages = {2392 - 2421},
	doi = {10.3390/smartcities7050094},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207278547&doi=10.3390%2Fsmartcities7050094&partnerID=40&md5=8aac4eff5a2cd83bba2ce13f8f44fd59},
	abstract = {Highlights: What are the main findings? We have developed an integrated and automated methodology that leverages a pre-trained Large Language Model (LLM) to generate scenario-based ontologies and knowledge graphs from research articles and technical manuals. Our methodology utilizes the ChatGPT API as the primary reasoning engine, supplemented by Natural Language Processing modules and carefully engineered prompts. This combination enables an automated tool capable of generating ontologies independently. The ontologies generated through our AI-powered method are interoperable and can significantly facilitate the design of data models and software architecture, particularly in the development of urban decision support systems. What is the implication of the main finding? We compared ontologies generated by our LLM with those created by human experts through CQ-based qualitative evaluation, assessing the reliability and feasibility of our approach. The methodology has been successfully applied to intermodal freight data and simulations. This has allowed us to generate a scenario-based ontology and knowledge graph that enhances data discovery, integration, and management, thereby supporting network optimization and multiple criteria decision analysis. Our methodology is both generalizable and adaptive, enabling the automation of ontology generation to support the development of urban and environmental decision support systems across various disciplines. The incorporation of Artificial Intelligence (AI) models into various optimization systems is on the rise. However, addressing complex urban and environmental management challenges often demands deep expertise in domain science and informatics. This expertise is essential for deriving data and simulation-driven insights that support informed decision-making. In this context, we investigate the potential of leveraging the pre-trained Large Language Models (LLMs) to create knowledge representations for supporting operations research. By adopting ChatGPT-4 API as the reasoning core, we outline an applied workflow that encompasses natural language processing, Methontology-based prompt tuning, and Generative Pre-trained Transformer (GPT), to automate the construction of scenario-based ontologies using existing research articles and technical manuals of urban datasets and simulations. From these ontologies, knowledge graphs can be derived using widely adopted formats and protocols, guiding various tasks towards data-informed decision support. The performance of our methodology is evaluated through a comparative analysis that contrasts our AI-generated ontology with the widely recognized pizza ontology, commonly used in tutorials for popular ontology software. We conclude with a real-world case study on optimizing the complex system of multi-modal freight transportation. Our approach advances urban decision support systems by enhancing data and metadata modeling, improving data integration and simulation coupling, and guiding the development of decision support strategies and essential software components. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Intermodal Freight Transportation; Large Language Models; Ontology; Urban Decision Support System},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Xiao2024,
	author = {Xiao, Yongkang and Zhang, Sinian and Zhou, Huixue and Li, Mingchen and Yang, Han and Zhang, Rui},
	title = {FuseLinker: Leveraging LLM's pre-trained text embeddings and domain knowledge to enhance GNN-based link prediction on biomedical knowledge graphs},
	year = {2024},
	journal = {Journal of Biomedical Informatics},
	volume = {158},
	pages = {},
	doi = {10.1016/j.jbi.2024.104730},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205147730&doi=10.1016%2Fj.jbi.2024.104730&partnerID=40&md5=93bcbae9d3a2abb5afa9fba0ee9fc0fd},
	abstract = {Objective: To develop the FuseLinker, a novel link prediction framework for biomedical knowledge graphs (BKGs), which fully exploits the graph's structural, textual and domain knowledge information. We evaluated the utility of FuseLinker in the graph-based drug repurposing task through detailed case studies. Methods: FuseLinker leverages fused pre-trained text embedding and domain knowledge embedding to enhance the graph neural network (GNN)-based link prediction model tailored for BKGs. This framework includes three parts: a) obtain text embeddings for BKGs using embedding-visible large language models (LLMs), b) learn the representations of medical ontology as domain knowledge information by employing the Poincaré graph embedding method, and c) fuse these embeddings and further learn the graph structure representations of BKGs by applying a GNN-based link prediction model. We evaluated FuseLinker against traditional knowledge graph embedding models and a conventional GNN-based link prediction model across four public BKG datasets. Additionally, we examined the impact of using different embedding-visible LLMs on FuseLinker's performance. Finally, we investigated FuseLinker's ability to generate medical hypotheses through two drug repurposing case studies for Sorafenib and Parkinson's disease. Results: By comparing FuseLinker with baseline models on four BKGs, our method demonstrates superior performance. The Mean Reciprocal Rank (MRR) and Area Under receiver operating characteristic Curve (AUROC) for KEGG50k, Hetionet, SuppKG and ADInt are 0.969 and 0.987, 0.548 and 0.903, 0.739 and 0.928, and 0.831 and 0.890, respectively. Conclusion: Our study demonstrates that FuseLinker is an effective novel link prediction framework that integrates multiple graph information and shows significant potential for practical applications in biomedical and clinical tasks. Source code and data are available at https://github.com/YKXia0/FuseLinker. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Drug Repurposing; Graph Neural Network; Knowledge Graph; Large Language Model; Link Prediction; Artemether; Cisapride; Deferoxamine; Delavirdine; Fosphenytoin Sodium; Meloxicam; Omeprazole; Sildenafil; Sorafenib; Tamsulosin; C (programming Language); Graph Embeddings; Graph Neural Networks; Network Embeddings; Neurodegenerative Diseases; Prediction Models; Domain Knowledge; Drug Repurposing; Embeddings; Knowledge Graphs; Language Model; Large Language Model; Link Prediction; Network-based; Repurposing; Knowledge Graph; Artemether; Cisapride; Deferoxamine; Delavirdine; Fosphenytoin Sodium; Meloxicam; Omeprazole; Sildenafil; Sorafenib; Tamsulosin; Area Under The Curve; Article; Artificial Neural Network; Biomedical Knowledge Graph; Biomedicine; Breast Cancer; Coronary Artery Disease; Drug Repositioning; Embedding; Graph Neural Network; Hematologic Malignancy; Information Processing; Kegg; Kidney Cancer; Large Language Model; Liver Cancer; Lung Cancer; Lymphatic System Malignancy; Malaria; Mathematical Analysis; Mathematical And Statistical Procedures; Medical Ontology; Melanoma; Multiple Sclerosis; Parkinson Disease; Prediction; Receiver Operating Characteristic; Algorithm; Human; Machine Learning; Medical Informatics; Natural Language Processing; Procedures; Algorithms; Drug Repositioning; Humans; Machine Learning; Medical Informatics; Natural Language Processing; Neural Networks, Computer},
	keywords = {C (programming language); Graph embeddings; Graph neural networks; Network embeddings; Neurodegenerative diseases; Prediction models; Domain knowledge; Drug repurposing; Embeddings; Knowledge graphs; Language model; Large language model; Link prediction; Network-based; Repurposing; Knowledge graph; artemether; cisapride; deferoxamine; delavirdine; fosphenytoin sodium; meloxicam; omeprazole; sildenafil; sorafenib; tamsulosin; area under the curve; Article; artificial neural network; biomedical knowledge graph; biomedicine; breast cancer; coronary artery disease; drug repositioning; embedding; graph neural network; hematologic malignancy; information processing; KEGG; kidney cancer; large language model; liver cancer; lung cancer; lymphatic system malignancy; malaria; mathematical analysis; mathematical and statistical procedures; medical ontology; melanoma; multiple sclerosis; Parkinson disease; prediction; receiver operating characteristic; algorithm; human; machine learning; medical informatics; natural language processing; procedures; Algorithms; Drug Repositioning; Humans; Machine Learning; Medical Informatics; Natural Language Processing; Neural Networks, Computer},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Youn2024,
	author = {Youn, Jason and Li, Fangzhou and Simmons, Gabriel and Kim, Shanghyeon and Tagkopoulos, Ilias},
	title = {FoodAtlas: Automated knowledge extraction of food and chemicals from literature},
	year = {2024},
	journal = {Computers in Biology and Medicine},
	volume = {181},
	pages = {},
	doi = {10.1016/j.compbiomed.2024.109072},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202569044&doi=10.1016%2Fj.compbiomed.2024.109072&partnerID=40&md5=c6551785ff5a6390b9797347f4a423a9},
	abstract = {Automated generation of knowledge graphs that accurately capture published information can help with knowledge organization and access, which have the potential to accelerate discovery and innovation. Here, we present an integrated pipeline to construct a large-scale knowledge graph using large language models in an active learning setting. We apply our pipeline to the association of raw food, ingredients, and chemicals, a domain that lacks such knowledge resources. By using an iterative active learning approach of 4120 manually curated premise-hypothesis pairs as training data for ten consecutive cycles, the entailment model extracted 230,848 food-chemical composition relationships from 155,260 scientific papers, with 106,082 (46.0 %) of them never been reported in any published database. To augment the knowledge incorporated in the knowledge graph, we further incorporated information from 5 external databases and ontology sources. We then applied a link prediction model to identify putative food-chemical relationships that were not part of the constructed knowledge graph. Validation of the 443 hypotheses generated by the link prediction model resulted in 355 new food-chemical relationships, while results show that the model score correlates well (R<sup>2</sup> = 0.70) with the probability of a novel finding. This work demonstrates how automated learning from literature at scale can accelerate discovery and support practical applications through reproducible, evidence-based capture of latent interactions of diverse entities, such as food and chemicals. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Mining; Deep Learning; Food Chemical; Knowledge Graph; Large Language Model; Link Prediction; Nutrition; Quality Control; Beta Carotene; Ergosterol; Lauric Acid; Lumisterol; Matairesinol; Prediction Models; Active Learning; Chemical Relationships; Deep Learning; Food Chemical; Knowledge Extraction; Knowledge Graphs; Language Model; Large Language Model; Link Prediction; Prediction Modelling; Knowledge Graph; Beta Carotene; Chemical Agent; Ergosterol; Lauric Acid; Lumisterol; Matairesinol; Article; Chemical Composition; Coconut; Cocos (genus); Data Mining; Deep Learning; Evidence Based Practice; Food; Garlic; Ginseng; Knowledge; Language Model; Large Language Model; Lupinus Albus; Maximum Entropy Model; Musa X Paradisiaca; Muskmelon; Nutrition; Ontology; Prediction; Probability; Tomato; Factual Database; Human; Machine Learning; Procedures; Data Mining; Databases, Factual; Food; Humans; Machine Learning},
	keywords = {Prediction models; Active Learning; Chemical relationships; Deep learning; Food chemical; Knowledge extraction; Knowledge graphs; Language model; Large language model; Link prediction; Prediction modelling; Knowledge graph; beta carotene; chemical agent; ergosterol; lauric acid; lumisterol; matairesinol; Article; chemical composition; coconut; Cocos (genus); data mining; deep learning; evidence based practice; food; garlic; ginseng; knowledge; language model; large language model; Lupinus albus; maximum entropy model; Musa x paradisiaca; muskmelon; nutrition; ontology; prediction; probability; tomato; factual database; human; machine learning; procedures; Data Mining; Databases, Factual; Food; Humans; Machine Learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Ciroku2024,
	author = {Ciroku, Fiorela and de Berardinis, Jacopo and Kim, Jongmo and Meroño-Peñuela, Albert and Presutti, Valentina and Simperl, Elena Paslaru Bontas},
	title = {RevOnt: Reverse engineering of competency questions from knowledge graphs via language models},
	year = {2024},
	journal = {Journal of Web Semantics},
	volume = {82},
	pages = {},
	doi = {10.1016/j.websem.2024.100822},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196541227&doi=10.1016%2Fj.websem.2024.100822&partnerID=40&md5=edfbb7cfa8d1b4d1acca7a50e84dfc09},
	abstract = {The process of developing ontologies – a formal, explicit specification of a shared conceptualisation – is addressed by well-known methodologies. As for any engineering development, its fundamental basis is the collection of requirements, which includes the elicitation of competency questions. Competency questions are defined through interacting with domain and application experts or by investigating existing datasets that may be used to populate the ontology i.e. its knowledge graph. The rise in popularity and accessibility of knowledge graphs provides an opportunity to support this phase with automatic tools. In this work, we explore the possibility of extracting competency questions from a knowledge graph. This reverses the traditional workflow in which knowledge graphs are built from ontologies, which in turn are engineered from competency questions. We describe in detail RevOnt, an approach that extracts and abstracts triples from a knowledge graph, generates questions based on triple verbalisations, and filters the resulting questions to yield a meaningful set of competency questions; the WDV dataset. This approach is implemented utilising the Wikidata knowledge graph as a use case, and contributes a set of core competency questions from 20 domains present in the WDV dataset. To evaluate RevOnt, we contribute a new dataset of manually-annotated high-quality competency questions, and compare the extracted competency questions by calculating their BLEU score against the human references. The results for the abstraction and question generation components of the approach show good to high quality. Meanwhile, the accuracy of the filtering component is above 86%, which is comparable to the state-of-the-art classifications. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Competency Question Extraction; Knowledge Engineering; Knowledge Graph; Ontology Development; Abstracting; Data Mining; Extraction; Filtration; Ontology; And Filters; Automatic Tools; Competency Question Extraction; Engineering Development; High Quality; Knowledge Graphs; Language Model; Ontology Development; Ontology's; Work-flows; Knowledge Graph},
	keywords = {Abstracting; Data mining; Extraction; Filtration; Ontology; And filters; Automatic tools; Competency question extraction; Engineering development; High quality; Knowledge graphs; Language model; Ontology development; Ontology's; Work-flows; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access; Gold Open Access}
}

@ARTICLE{Hu202410464,
	author = {Hu, Xiaoxuan and Zhang, Xuechun and Sun, Wen and Liu, Chunhong and Deng, Pujuan and Cao, Yuanwei and Zhang, Chenze and Xu, Ning and Zhang, Tongtong and Zhang, Yong Edward},
	title = {Systematic discovery of DNA-binding tandem repeat proteins},
	year = {2024},
	journal = {Nucleic Acids Research},
	volume = {52},
	number = {17},
	pages = {10464 - 10489},
	doi = {10.1093/nar/gkae710},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204819311&doi=10.1093%2Fnar%2Fgkae710&partnerID=40&md5=3e38574180177d7a02f9f211be0de285},
	abstract = {Tandem repeat proteins (TRPs) are widely distributed and bind to a wide variety of ligands. DNA-binding TRPs such as zinc finger (ZNF) and transcription activator-like effector (TALE) play important roles in biology and biotechnology. In this study, we first conducted an extensive analysis of TRPs in public databases, and found that the enormous diversity of TRPs is largely unexplored. We then focused our efforts on identifying novel TRPs possessing DNA-binding capabilities. We established a protein language model for DNA-binding protein prediction (PLM-DBPPred), and predicted a large number of DNA-binding TRPs. A subset was then selected for experimental screening, leading to the identification of 11 novel DNA-binding TRPs, with six showing sequence specificity. Notably, members of the STAR (Short TALE-like Repeat proteins) family can be programmed to target specific 9 bp DNA sequences with high affinity. Leveraging this property, we generated artificial transcription factors using reprogrammed STAR proteins and achieved targeted activation of endogenous gene sets. Furthermore, the members of novel families such as MOON (Marine Organism-Originated DNA binding protein) and pTERF (prokaryotic mTERF-like protein) exhibit unique features and distinct DNA-binding characteristics, revealing interesting biological clues. Our study expands the diversity of DNA-binding TRPs, and demonstrates that a systematic approach greatly enhances the discovery of new biological insights and tools. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Dna; Dna; Dna-binding Proteins; Transcription Activator-like Effectors; Transcription Factors; Fei Tecnai-f20 Electron Microscope; Graphpad Prism 8; Illumina Novaseq; R Package; R Studio; Ultrascan 4000; Dna Binding Protein; Protein; Short Tale Like Repeat Protein; Transcription Factor; Unclassified Drug; Zinc Finger Protein; Dna; Protein Binding; Transcription Activator Like Effector; Amino Acid Sequence; Article; Binding Affinity; Biotechnology; Correlation Analysis; Diagnostic Test Accuracy Study; Dna Binding; Dna Sequence; Entropy; Escherichia Coli; Flow Cytometry; Gene Expression; Gene Ontology; Genetic Transfection; Hek293t Cell Line; Human; Human Cell; Machine Learning; Male; Nonhuman; Phylogenetic Tree; Prevalence; Protein Expression; Protein Purification; Receiver Operating Characteristic; Tandem Repeat; Binding Site; Chemistry; Genetics; Metabolism; Protein Database; Amino Acid Sequence; Binding Sites; Databases, Protein; Dna-binding Proteins; Humans; Protein Binding; Tandem Repeat Sequences; Transcription Activator-like Effectors; Transcription Factors; Zinc Fingers},
	keywords = {DNA binding protein; protein; short TALE like repeat protein; transcription factor; unclassified drug; zinc finger protein; DNA; protein binding; transcription activator like effector; amino acid sequence; Article; binding affinity; biotechnology; correlation analysis; diagnostic test accuracy study; DNA binding; DNA sequence; entropy; Escherichia coli; flow cytometry; gene expression; gene ontology; genetic transfection; HEK293T cell line; human; human cell; machine learning; male; nonhuman; phylogenetic tree; prevalence; protein expression; protein purification; receiver operating characteristic; tandem repeat; binding site; chemistry; genetics; metabolism; protein database; Amino Acid Sequence; Binding Sites; Databases, Protein; DNA-Binding Proteins; Humans; Protein Binding; Tandem Repeat Sequences; Transcription Activator-Like Effectors; Transcription Factors; Zinc Fingers},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{Kloosterman20245336,
	author = {Kloosterman, Daan Juri and Erbani, Johanna and Boon, Menno and Farber, Martina and Handgraaf, Shanna M. and Ando-Kuri, Masami and Sánchez-López, Elena and Fontein, Bauke and Mertz, Marjolijn and Nieuwland, Marja},
	title = {Macrophage-mediated myelin recycling fuels brain cancer malignancy},
	year = {2024},
	journal = {Cell},
	volume = {187},
	number = {19},
	pages = {5336 - 5356.e30},
	doi = {10.1016/j.cell.2024.07.030},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201934715&doi=10.1016%2Fj.cell.2024.07.030&partnerID=40&md5=bbb8adf18c60d303529824ad1214c0cc},
	abstract = {Tumors growing in metabolically challenged environments, such as glioblastoma in the brain, are particularly reliant on crosstalk with their tumor microenvironment (TME) to satisfy their high energetic needs. To study the intricacies of this metabolic interplay, we interrogated the heterogeneity of the glioblastoma TME using single-cell and multi-omics analyses and identified metabolically rewired tumor-associated macrophage (TAM) subpopulations with pro-tumorigenic properties. These TAM subsets, termed lipid-laden macrophages (LLMs) to reflect their cholesterol accumulation, are epigenetically rewired, display immunosuppressive features, and are enriched in the aggressive mesenchymal glioblastoma subtype. Engulfment of cholesterol-rich myelin debris endows subsets of TAMs to acquire an LLM phenotype. Subsequently, LLMs directly transfer myelin-derived lipids to cancer cells in an LXR/Abca1-dependent manner, thereby fueling the heightened metabolic demands of mesenchymal glioblastoma. Our work provides an in-depth understanding of the immune-metabolic interplay during glioblastoma progression, thereby laying a framework to unveil targetable metabolic vulnerabilities in glioblastoma. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cancer Immunity; Cholesterol; Glioblastoma; Lipid Metabolism; Macrophages; Myelin Recycling; Tumor Microenvironment; Cholesterol; Abca1 Protein, Human; Atp Binding Cassette Transporter 1; Cholesterol; Liver X Receptors; Agilent Gc-ms 5977b Gas Chromatograph; Bd Fortessa Instrument; Fiji Software; Flowjo V10 Software; Gas Chromatograph Agilent 8890 Gc System; Ggplot2; Illumina Nextera Xt Dna Sample Preparation Kit; Imaris Software; Kapa Library Quantification Kit; Kapa Library Quantification Kit; Leica Em Uc-7 Ultra-cryomicrotome; Mass Spectrometer Agilent 5977b Gc/msd; Nextseq 500/550 High Output Kit V2.5; Nextseq550; Novaseq 6000 System; Novaseq Sp Reagent Kit V1.5; Oxford Instruments V9.8; Phenomenex Securityguard Ultra C8; Prism V9; R Version 4.1.1; Rneasy Micro Kit; Sciex Tripletof 6600; Seurat V4.4; Seuratobject V4.0; Shimadzu Nexera X2; Tecnai12g2 Electron Microscope; Transcriptor First Strand Cdna Synthesis Kit; Tumor Dissociation Kit; Zeiss Axioscan Z1 Slide Scanner; Zeiss Confocal Lsm 980 Airyscan 2; Zeiss Observer Z1; Zen 2.6 Software; Zen 3.3 Zeiss Software; Abc Transporter A1; Cell Surface Marker; Liver X Receptor; Myelin; Abca1 Protein, Human; Cholesterol; Animal Cell; Animal Experiment; Article; Autofluorescence; Bicinchoninic Acid Assay; Bone Marrow Derived Macrophage; Brain Cancer; Brain Interstitial Fluid; Bulk Atac Seq Library Preparation; Cancer Recurrence; Cdna Synthesis; Cell Proliferation; Cell Proliferation Assay; Cell Viability; Cholesterol Blood Level; Cholesterol Staining; Chromatin; Chromatin Landscape; Coculture; Controlled Study; Dna Synthesis; Electron Microscopy; Electrospray Mass Spectrometry; Epigenetics; Ex Vivo Study; Extracellular Acidification Rate; Fatty Acid Analysis; Female; Flow Cytometry; Fluorescence Activated Cell Sorting; Fluorescence Intensity; Frozen Section; Gene Expression Profiling; Gene Library; Gene Ontology; Gene Set Enrichment Analysis; Glioblastoma; Illumina Sequencing; Image Analysis; Immunofluorescence Assay; Immunosuppressive Treatment; In Vitro Study; In Vivo Study; Interstitial Fluid; Kegg; Lipid Analysis; Lipid Extraction; Lipid Laden Macrophage; Lipid Storage; Lipidomics; Liquid Chromatography-mass Spectrometry; Live Cell Imaging; Macrophage; Male; Mass Fragmentography; Metabolic Activity Assay; Metabolite; Metabolomics; Microglia; Mitochondrial Respiration; Monoculture; Motif Enrichment Analysis; Mouse; Mrna Expression Assay; Multiomics; Nonhuman; Nuclear Magnetic Resonance Spectroscopy; Oxygen Consumption; Phenotype; Predictive Value; Real Time Reverse Transcription Polymerase Chain Reaction; Recycling; Remyelinization; Rna Extraction; Rna Sequence; Seahorse Assay; Single Cell Rna Seq; Staining; Sterol Analysis; Survival Analysis; Tissue And Cell Imaging; Tumor Cell Monoculture; Tumor Growth; Tumor Microenvironment; Tumor-associated Macrophage; Ultracentrifugation; Animal; Brain Tumor; Human; Immunology; Metabolism; Myelin Sheath; Pathology; Tumor Cell Line; Animals; Atp Binding Cassette Transporter 1; Brain Neoplasms; Cell Line, Tumor; Cholesterol; Female; Glioblastoma; Humans; Liver X Receptors; Macrophages; Male; Mice; Myelin Sheath; Tumor Microenvironment; Tumor-associated Macrophages},
	keywords = {ABC transporter A1; cell surface marker; liver X receptor; myelin; ABCA1 protein, human; cholesterol; animal cell; animal experiment; Article; autofluorescence; bicinchoninic acid assay; bone marrow derived macrophage; brain cancer; brain interstitial fluid; bulk ATAC seq library preparation; cancer recurrence; cDNA synthesis; cell proliferation; cell proliferation assay; cell viability; cholesterol blood level; cholesterol staining; chromatin; chromatin landscape; coculture; controlled study; DNA synthesis; electron microscopy; electrospray mass spectrometry; epigenetics; ex vivo study; extracellular acidification rate; fatty acid analysis; female; flow cytometry; fluorescence activated cell sorting; fluorescence intensity; frozen section; gene expression profiling; gene library; gene ontology; gene set enrichment analysis; glioblastoma; illumina sequencing; image analysis; immunofluorescence assay; immunosuppressive treatment; in vitro study; in vivo study; interstitial fluid; KEGG; lipid analysis; lipid extraction; lipid laden macrophage; lipid storage; lipidomics; liquid chromatography-mass spectrometry; live cell imaging; macrophage; male; mass fragmentography; metabolic activity assay; metabolite; metabolomics; microglia; mitochondrial respiration; monoculture; motif enrichment analysis; mouse; mRNA expression assay; multiomics; nonhuman; nuclear magnetic resonance spectroscopy; oxygen consumption; phenotype; predictive value; real time reverse transcription polymerase chain reaction; recycling; remyelinization; RNA extraction; RNA sequence; seahorse assay; single cell RNA seq; staining; sterol analysis; survival analysis; tissue and cell imaging; tumor cell monoculture; tumor growth; tumor microenvironment; tumor-associated macrophage; ultracentrifugation; animal; brain tumor; human; immunology; metabolism; myelin sheath; pathology; tumor cell line; Animals; ATP Binding Cassette Transporter 1; Brain Neoplasms; Cell Line, Tumor; Cholesterol; Female; Glioblastoma; Humans; Liver X Receptors; Macrophages; Male; Mice; Myelin Sheath; Tumor Microenvironment; Tumor-Associated Macrophages},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 59; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{2024,
	title = {Proceedings of the 23rd International Conference on New Trends in Intelligent Software Methodologies, Tools and Techniques, SoMeT 2024},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {389},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217071336&partnerID=40&md5=83e6b380ba016061fcde7e0e8c02fb09},
	abstract = {The proceedings contain 40 papers. The topics discussed include: design a knowledge chatbot system in education based on ontology approach; frame-level deepfake detection on explicit content with ID-unaware binary classification; detecting and mitigating the weakest cybersecurity link in an information system; encryption and compression scheme using compressing sensing and chaotic mixing; extended engineering software support for multiphysical research in applied informatics; transformation approach for safe source code through the application of a large language model and adaptation of a generative adversarial network; and improve breast cancer classification based on deep feature fusion and hyperparameter customization using transfer learning. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kondinski20242070,
	author = {Kondinski, Aleksandar and Rutkevych, Pavlo P. and Pascazio, Laura and Tran, Dan N. and Farazi, Feroz and Ganguly, Srishti and Kraft, Markus},
	title = {Knowledge graph representation of zeolitic crystalline materials},
	year = {2024},
	journal = {Digital Discovery},
	volume = {3},
	number = {10},
	pages = {2070 - 2084},
	doi = {10.1039/d4dd00166d},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205728208&doi=10.1039%2Fd4dd00166d&partnerID=40&md5=9aa22f05a71e9ba9bdca274e3e0310f1},
	abstract = {Zeolites are complex and porous crystalline inorganic materials that serve as hosts for a variety of molecular, ionic and cluster species. Formal, machine-actionable representation of this chemistry presents a challenge as a variety of concepts need to be semantically interlinked. This work demonstrates the potential of knowledge engineering in overcoming this challenge. We develop ontologies OntoCrystal and OntoZeolite, enabling the representation and instantiation of crystalline zeolite information into a dynamic, interoperable knowledge graph called The World Avatar (TWA). In TWA, crystalline zeolite instances are semantically interconnected with chemical species that act as guests in these materials. Information can be obtained via custom or templated SPARQL queries administered through a user-friendly web interface. Unstructured exploration is facilitated through natural language processing using the Marie System, showcasing promise for the blended large language model - knowledge graph approach in providing accurate responses on zeolite chemistry in natural language. © 2024 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Gold Open Access}
}

@ARTICLE{Chertykova202415,
	author = {Chertykova, Maria Dmitrievna},
	title = {Good wishes as a linguistic reflection of the traditional values of the Khakases: research methodology},
	year = {2024},
	journal = {Eposovedenie},
	volume = {35},
	number = {3},
	pages = {15 - 29},
	doi = {10.25587/2782-4861-2024-3-15-29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213064157&doi=10.25587%2F2782-4861-2024-3-15-29&partnerID=40&md5=7ad3eebe1485ffb636b0d55c360117f1},
	abstract = {In this article, we presented our vision of the tasks, methods and prospects for studying texts of good wishes in the Khakas language, which are the most widespread, vibrant and ancient form of linguistic representation of the spiritual culture of the Khakas. While in folkloristic and ethnographic studies this folklore genre was considered in the system of ideological values and etiquette behavior of the ethnic group, its linguistic corpus remains an untouched area. However, the basic basis of traditional good wishes is its language – a special verbal form of manifestation of the ethnic mentality, therefore, it should receive scientific coverage at all linguistic levels: semantic, grammatical, syntactic, pragmatic, etc. For a more in-depth consideration in order to identify the system of traditional values and worldview of the ethnos complex linguo-folkloristic, ethno-, linguocultural methods are used. The purpose of the article is to describe the methods, tasks and prospects for studying texts of good wishes. To achieve the goal, tasks related to the designation of texts of good wishes as an object of research are set and methods are proposed for a more in-depth comprehensive study of their plan of expression and plan of content. Thus, the study of complex problems of value categories of good wishes, pragmatic situations of their implementation and other forms of good wishes, reflected in the relationships of linguistic individuals, form an independent scientific direction. The relevance of the study is due to the lack of research into the language of the textual structure of well-wishes and the need to develop their linguistic-folkloristic paradigm, as well as the formation of a new conceptual apparatus. The novelty of the study lies in the fact that for the first time texts of good wishes are becoming the subject of special research on linguistic parameters correlated with pragmatic, communicative, folkloristic and other characteristics. The article identifies questions for potential research into texts of good wishes and describes methods for achieving these goals. This topic has good prospects for further research in interdisciplinary areas. In addition, the popularization of texts of traditional good wishes in the public environment, their spiritual and mental attitudes will make a certain contribution to the preservation and development of the Khakas language. First of all, the use of traditional expressions of good wishes in the modern discursive space allows us to expand the ontological function of the Khakas language. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Khakas Language; Language Models; Methods; National And Ideological Values; Priority Areas; Research Prospects; Study; Text Structure; Traditional Good Wishes},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Wu202424,
	author = {Wu, Jingzhu and Li, Lin and Wu, Zong Ning and Yu, Chongchong and Cheng, Junhu and Zeng, Xinan and Zhao, Xia and Yang, Yi and Ma, Ji},
	title = {Research Progress on Digitalization and Intelligence in Food Domain Based on Knowledge Graphs; 基于知识图谱的食品领域数智化研究进展},
	year = {2024},
	journal = {Journal of Food Science and Technology (China)},
	volume = {42},
	number = {5},
	pages = {24 - 32},
	doi = {10.12301/spxb202400610},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208097327&doi=10.12301%2Fspxb202400610&partnerID=40&md5=e7e1e71104f2075e9558016d7fb1fa0e},
	abstract = {With the development of technologies such as big data and cloud computing, the scale of data in the food domain is growing at an astonishing rate. These data not only come from diverse sources and have complex structures, but also lack standardized terminology, which poses challenges to the effective integration and utilization of food-related data. Knowledge graphs, as fundamental cornerstone of achieving general artificial intelligence, provides support for the organization and management of food data and its higher-level applications in terms of integration and semantic understanding. By summarizing recent research achievements of knowledge graphs in the food domain, the construction methods of knowledge graphs in food domain was reviewed, covering key steps such as ontology construction, knowledge extraction, knowledge fusion, and processing. The current applications of knowledge graphs in the food domain, particularly in three areas, food nutrition and health, food innovation and research, and food safety and traceability. Based on current state of development of knowledge graphs in the food domain, incorporating multimodal data fusion technology, large language model construction, and the intelligentization of industrial equipment in the food field, the future development directions of knowledge graphs in food domain were anticipated. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Food Research And Development; Food Safety; Food Traceability; Knowledge Graph; Nutrition Analysis; Ontology Construction; Cloud-computing; Complexes Structure; Food Research; Food Research And Development; Food Traceabilitys; Food-safety; Knowledge Graphs; Nutrition Analyze; Ontology Construction; Research And Development; Knowledge Graph},
	keywords = {Cloud-computing; Complexes structure; Food research; Food research and development; Food traceabilitys; Food-safety; Knowledge graphs; Nutrition analyze; Ontology construction; Research and development; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Ghajari2024271,
	author = {Ghajari, Adrián and Ros, Salvador Cador and Pérez, Alvaro},
	title = {Querying the Depths: Unveiling the Strengths and Struggles of Large Language Models in SPARQL Generation; Explorando las Profundidades: Revelando las Fortalezas y Desafíos de los Modelos de Lenguaje de Gran Escala en la Generación de SPARQL},
	year = {2024},
	journal = {Procesamiento del Lenguaje Natural},
	number = {73},
	pages = {271 - 281},
	doi = {10.26342/2024-73-20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206510658&doi=10.26342%2F2024-73-20&partnerID=40&md5=8cb9a9b1380cf379a5c5a43896866c23},
	abstract = {The emergence of the Semantic Web has precipitated a proliferation of structured data manifested in the form of knowledge graphs, underscoring the imperative of natural language interfaces to enhance accessibility to these repositories of information. The capacity to articulate queries in natural language and subsequently retrieve data through SPARQL queries assumes paramount importance. In the present investigation, we have scrutinized the efficacy of in-context learning based on an agent-based architecture in facilitating the construction of SPARQL queries. Contrary to initial expectations, the augmentation of in-context learning prompts through agent-based mechanisms has been found to diminish the efficacy of Language Model-based Systems (LLMS), as it is perceived as extraneous”noise,” thereby delineating the constraints inherent in this approach. The results highlight the need to delve deeper into the intricacies of model training and fine-tuning, focusing on the relational aspects of ontology schemas. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Agents; Knowledge Retrieval; Prompt Engineering; Sparql Queries},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Buehler2024,
	author = {Buehler, Markus J.},
	title = {Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning},
	year = {2024},
	journal = {Machine Learning: Science and Technology},
	volume = {5},
	number = {3},
	pages = {},
	doi = {10.1088/2632-2153/ad7228},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206499137&doi=10.1088%2F2632-2153%2Fad7228&partnerID=40&md5=21a0937dc10d125ed87c7888c5c0d635},
	abstract = {Leveraging generative Artificial Intelligence (AI), we have transformed a dataset comprising 1000 scientific papers focused on biological materials into a comprehensive ontological knowledge graph. Through an in-depth structural analysis of this graph, we have calculated node degrees, identified communities along with their connectivities, and evaluated clustering coefficients and betweenness centrality of pivotal nodes, uncovering fascinating knowledge architectures. We find that the graph has an inherently scale-free nature, shows a high level of connectedness, and can be used as a rich source for downstream graph reasoning by taking advantage of transitive and isomorphic properties to reveal insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, propose never-before-seen material designs, and predict material behaviors. Using a large language embedding model we compute deep node representations and use combinatorial node similarity ranking to develop a path sampling strategy that allows us to link dissimilar concepts that have previously not been related. One comparison revealed detailed structural parallels between biological materials and Beethoven’s 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. In another example, the algorithm proposed an innovative hierarchical mycelium-based composite based on integrating path sampling with principles extracted from Kandinsky’s ‘Composition VII’ painting. The resulting material integrates an innovative set of concepts that include a balance of chaos and order, adjustable porosity, mechanical strength, and complex patterned chemical functionalization. We uncover other isomorphisms across science, technology and art, revealing a nuanced ontology of immanence that reveal a context-dependent heterarchical interplay of constituents. Because our method transcends established disciplinary boundaries through diverse data modalities (graphs, images, text, numerical data, etc), graph-based generative AI achieves a far higher degree of novelty, explorative capacity, and technical detail, than conventional approaches and establishes a widely useful framework for innovation by revealing hidden connections. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomaterials; Generative Ai; Graph Theory; Language Modeling; Materials Informatics; Materials Science; Natural Language Processing; Biological Materials; Biomaterials; Chaos Theory; Crystalline Materials; Generative Adversarial Networks; Ontology; Generative Artificial Intelligence; Knowledge Extraction; Language Model; Language Processing; Material Informatics; Material Science; Natural Language Processing; Natural Languages; Path Sampling; Scientific Discovery; Knowledge Graph},
	keywords = {Biological materials; Biomaterials; Chaos theory; Crystalline materials; Generative adversarial networks; Ontology; Generative artificial intelligence; Knowledge extraction; Language model; Language processing; Material Informatics; Material science; Natural language processing; Natural languages; Path sampling; Scientific discovery; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Haris2024,
	author = {Haris, Erum and Cohn, Anthony G. and Stell, John G.},
	title = {Semantic Perspectives on the Lake District Writing: Spatial Ontology Modeling and Relation Extraction for Deeper Insights},
	year = {2024},
	journal = {Leibniz International Proceedings in Informatics, LIPIcs},
	volume = {315},
	pages = {},
	doi = {10.4230/LIPIcs.COSIT.2024.11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205767309&doi=10.4230%2FLIPIcs.COSIT.2024.11&partnerID=40&md5=72fff5300bf9859aec90a94affa75bee},
	abstract = {Extracting spatial details from historical texts can be difficult, hindering our understanding of past landscapes. The study addresses this challenge by analyzing the Corpus of the Lake District Writing, focusing on the English Lake District region. We systematically link the theoretical notions from the core concepts of spatial information to provide basis for the problem domain. The conceptual foundation is further complemented with a spatial ontology and a custom gazetteer, allowing a formal and insightful semantic exploration of the massive unstructured corpus. The other contrasting side of the framework is the usage of LLMs for spatial relation extraction. We formulate prompts leveraging understanding of the LLMs of the intended task, curate a list of spatial relations representing the most recurring proximity or vicinity relations terms and extract semantic triples for the top five place names appearing in the corpus. We compare the extraction capabilities of three benchmark LLMs for a scholarly significant historical archive, representing their potential in a challenging and interdisciplinary research problem. Finally, the network comprising the semantic triples is enhanced by incorporating a gazetteer-based classification of the objects involved thus improving their spatial profiling. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Ontology; Spatial Humanities; Spatial Narratives; Benchmarking; Classification (of Information); Economic And Social Effects; Modeling Languages; Semantics; Text Mining; Language Model; Large Language Model; Ontology Model; Ontology Relations; Ontology's; Relation Extraction; Spatial Humanity; Spatial Narratives; Spatial Ontologies; Spatial Relations; Ontology},
	keywords = {Benchmarking; Classification (of information); Economic and social effects; Modeling languages; Semantics; Text mining; Language model; Large language model; Ontology model; Ontology relations; Ontology's; Relation extraction; Spatial humanity; Spatial narratives; Spatial ontologies; Spatial relations; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Bernasconi2024,
	author = {Bernasconi, Eleonora and Ceriani, Miguel and Ferilli, Stefano},
	title = {LPG Semantic Ontologies: A Tool for Interoperable Schema Creation and Management},
	year = {2024},
	journal = {Information (Switzerland)},
	volume = {15},
	number = {9},
	pages = {},
	doi = {10.3390/info15090565},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205234457&doi=10.3390%2Finfo15090565&partnerID=40&md5=a21f5e70e09e16acf7684bd642e50912},
	abstract = {Ontologies are essential for the management and integration of heterogeneous datasets. This paper presents OntoBuilder, an advanced tool that leverages the structural capabilities of semantic labeled property graphs (SLPGs) in strict alignment with semantic web standards to create a sophisticated framework for data management. We detail OntoBuilder’s architecture, core functionalities, and application scenarios, demonstrating its proficiency and adaptability in addressing complex ontological challenges. Our empirical assessment highlights OntoBuilder’s strengths in enabling seamless visualization, automated ontology generation, and robust semantic integration, thereby significantly enhancing user workflows and data management capabilities. The performance of the linked data tools across multiple metrics further underscores the effectiveness of OntoBuilder. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Management; Large Language Models; Ontologies; Semantic Labeled Property Graphs; Semantic Web; Information Management; Knowledge Graph; Labeled Data; Ontology; Heterogeneous Datasets; Language Model; Large Language Model; Ontology's; Property; Semantic Labeled Property Graph; Semantic Ontology; Semantic Web Standards; Semantic-web; Structural Capabilities; Semantics},
	keywords = {Information management; Knowledge graph; Labeled data; Ontology; Heterogeneous datasets; Language model; Large language model; Ontology's; Property; Semantic labeled property graph; Semantic ontology; Semantic web standards; Semantic-Web; Structural capabilities; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{Mo2024,
	author = {Mo, Zhenchong and Gong, Lin and Gao, Jun and Cui, Haoran and Lan, Junde},
	title = {A Knowledge Graph-Based Implicit Requirement Mining Method in Personalized Product Development},
	year = {2024},
	journal = {Applied Sciences (Switzerland)},
	volume = {14},
	number = {17},
	pages = {},
	doi = {10.3390/app14177550},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203650272&doi=10.3390%2Fapp14177550&partnerID=40&md5=c0accd43af21e027a07400dc084ae125},
	abstract = {In the context of crowd innovation and the generative design driven by big language models, the exploration of personalized requirements has become a key in significantly improving product innovation, concept feasibility, and design interaction efficiency. To mine a large number of vague and unexpressed implicit requirements of personalized products, a domain knowledge graph-based method is proposed in this research. First, based on the classical theory of design science, the characteristics and categories of personalized implicit requirements are analyzed, and the theoretical basis of implicit requirement mining is formed. Next, in order to improve the practicability and construction efficiency of the domain knowledge graph, a more informative ontology is constructed, and better-performing natural language processing (NLP) models are proposed. Then, a multi-category personalized implicit requirement mining method based on a knowledge graph is proposed. Finally, a platform was developed based on the technical solution proposed in this study, and an example verification was conducted in the field of electromechanical engineering. The efficiency improvement of the training model proposed in the research was analyzed, and the practicality of implicit requirement mining methods are discussed. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Crowd Innovation; Engineering Design; Generative Design; Implicit Requirement; Knowledge Graph; Patent Analysis; Requirement Mining; Generative Adversarial Networks; Graphitization; Mine Safety; Solution Mining; Crowd Innovation; Domain Knowledge; Engineering Design; Generative Design; Implicit Requirement; Knowledge Graphs; Mining Methods; Patent Analysis; Personalized Products; Requirement Mining; Knowledge Graph},
	keywords = {Generative adversarial networks; Graphitization; Mine safety; Solution mining; Crowd innovation; Domain knowledge; Engineering design; Generative design; Implicit requirement; Knowledge graphs; Mining methods; Patent analysis; Personalized products; Requirement mining; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Ying20241912,
	author = {Ying, Huaiyuan and Zhao, Zhengyun and Zhao, Yang and Zeng, Sihang and Yu, Sheng},
	title = {CoRTEx: Contrastive learning for representing terms via explanations with applications on constructing biomedical knowledge graphs},
	year = {2024},
	journal = {Journal of the American Medical Informatics Association},
	volume = {31},
	number = {9},
	pages = {1912 - 1920},
	doi = {10.1093/jamia/ocae115},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201754632&doi=10.1093%2Fjamia%2Focae115&partnerID=40&md5=2133dce86e8c920ce827923ca7d5e207},
	abstract = {Objectives: Biomedical Knowledge Graphs play a pivotal role in various biomedical research domains. Concurrently, term clustering emerges as a crucial step in constructing these knowledge graphs, aiming to identify synonymous terms. Due to a lack of knowledge, previous contrastive learning models trained with Unified Medical Language System (UMLS) synonyms struggle at clustering difficult terms and do not generalize well beyond UMLS terms. In this work, we leverage the world knowledge from large language models (LLMs) and propose Contrastive Learning for Representing Terms via Explanations (CoRTEx) to enhance term representation and significantly improves term clustering. Materials and Methods: The model training involves generating explanations for a cleaned subset of UMLS terms using ChatGPT. We employ contrastive learning, considering term and explanation embeddings simultaneously, and progressively introduce hard negative samples. Additionally, a ChatGPT-assisted BIRCH algorithm is designed for efficient clustering of a new ontology. Results: We established a clustering test set and a hard negative test set, where our model consistently achieves the highest F1 score. With CoRTEx embeddings and the modified BIRCH algorithm, we grouped 35 580 932 terms from the Biomedical Informatics Ontology System (BIOS) into 22 104 559 clusters with O(N) queries to ChatGPT. Case studies highlight the model's efficacy in handling challenging samples, aided by information from explanations. Conclusion: By aligning terms to their explanations, CoRTEx demonstrates superior accuracy over benchmark models and robustness beyond its training set, and it is suitable for clustering terms for large-scale biomedical ontologies. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Contrastive Learning; Knowledge Injection; Large Language Models; Term Clustering; Algorithm; Article; Biomedical Knowledge Graph; Case Study; Chatgpt; Contrastive Learning For Representing Terms Via Explanation; Language Model; Large Language Model; Medical Informatics; Sample; Scoring System; Unified Medical Language System},
	keywords = {algorithm; Article; biomedical knowledge graph; case study; ChatGPT; contrastive learning for representing terms via explanation; language model; large language model; medical informatics; sample; scoring system; Unified Medical Language System},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Zhao2024,
	author = {Zhao, Chenguang and Liu, Tong and Wang, Zheng},
	title = {PANDA-3D: Protein function prediction based on AlphaFold models},
	year = {2024},
	journal = {NAR Genomics and Bioinformatics},
	volume = {6},
	number = {3},
	pages = {},
	doi = {10.1093/nargab/lqae094},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200742481&doi=10.1093%2Fnargab%2Flqae094&partnerID=40&md5=9df6163a5c0a053626b10af62baa3667},
	abstract = {Previous protein function predictors primarily make predictions from amino acid sequences instead of tertiary structures because of the limited number of experimentally determined structures and the unsatisfying qualities of predicted structures. AlphaFold recently achieved promising performances when predicting protein tertiary structures, and the AlphaFold protein structure database (AlphaFold DB) is fast-expanding. Therefore, we aimed to develop a deep-learning tool that is specifically trained with AlphaFold models and predict GO terms from AlphaFold models. We developed an advanced learning architecture by combining geometric vector perceptron graph neural networks and variant transformer decoder layers for multi-label classification. PANDA-3D predicts gene ontology (GO) terms from the predicted structures of AlphaFold and the embeddings of amino acid sequences based on a large language model. Our method significantly outperformed a state-of-the-art deep-learning method that was trained with experimentally determined tertiary structures, and either outperformed or was comparable with several other language-model-based state-of-the-art methods with amino acid sequences as input. PANDA-3D is tailored to AlphaFold models, and the AlphaFold DB currently contains over 200 million predicted protein structures (as of May 1st, 2023), making PANDA-3D a useful tool that can accurately annotate the functions of a large number of proteins. PANDA-3D can be freely accessed as a web server from http://dna.cs.miami.edu/PANDA-3D/ and as a repository from https://github.com/zwang-bioinformatics/PANDA-3D. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Amino Acid Sequence; Article; Bioinformatics; Controlled Study; Deep Learning; Gene Ontology; Language Model; Large Language Model; Multilabel Classification; Nerve Cell Network; Nonhuman; Prediction; Protein Function; Protein Structure; Protein Tertiary Structure},
	keywords = {amino acid sequence; article; bioinformatics; controlled study; deep learning; gene ontology; language model; large language model; multilabel classification; nerve cell network; nonhuman; prediction; protein function; protein structure; protein tertiary structure},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Wang20242076,
	author = {Wang, Andy and Liu, Cong and Yang, Jingye and Weng, Chunhua},
	title = {Fine-tuning large language models for rare disease concept normalization},
	year = {2024},
	journal = {Journal of the American Medical Informatics Association},
	volume = {31},
	number = {9},
	pages = {2076 - 2083},
	doi = {10.1093/jamia/ocae133},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200384913&doi=10.1093%2Fjamia%2Focae133&partnerID=40&md5=34bdf2a0977f10a77449d997fdf341f7},
	abstract = {Objective: We aim to develop a novel method for rare disease concept normalization by fine-tuning Llama 2, an open-source large language model (LLM), using a domain-specific corpus sourced from the Human Phenotype Ontology (HPO). Methods: We developed an in-house template-based script to generate two corpora for fine-tuning. The first (NAME) contains standardized HPO names, sourced from the HPO vocabularies, along with their corresponding identifiers. The second (NAMEþSYN) includes HPO names and half of the concept’s synonyms as well as identifiers. Subsequently, we fine-tuned Llama 2 (Llama2-7B) for each sentence set and conducted an evaluation using a range of sentence prompts and various phenotype terms. Results: When the phenotype terms for normalization were included in the fine-tuning corpora, both models demonstrated nearly perfect performance, averaging over 99% accuracy. In comparison, ChatGPT-3.5 has only ∼20% accuracy in identifying HPO IDs for phenotype terms. When single-character typos were introduced in the phenotype terms, the accuracy of NAME and NAMEþSYN is 10.2% and 36.1%, respectively, but increases to 61.8% (NAMEþSYN) with additional typo-specific fine-tuning. For terms sourced from HPO vocabularies as unseen synonyms, the NAME model achieved 11.2% accuracy, while the NAMEþSYN model achieved 92.7% accuracy. Conclusion: Our fine-tuned models demonstrate ability to normalize phenotype terms unseen in the fine-tuning corpus, including misspellings, synonyms, terms from other ontologies, and laymen’s terms. Our approach provides a solution for the use of LLMs to identify named medical entities from clinical narratives, while successfully normalizing them to standard concepts in a controlled vocabulary. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Normalization; Fine-tuning; Hpo; Large Language Model; Llama 2; Article; Chatgpt; Large Language Model; Rare Disease; Biological Ontology; Controlled Vocabulary; Human; Natural Language Processing; Phenotype; Biological Ontologies; Humans; Natural Language Processing; Phenotype; Rare Diseases; Vocabulary, Controlled},
	keywords = {Article; ChatGPT; large language model; rare disease; biological ontology; controlled vocabulary; human; natural language processing; phenotype; Biological Ontologies; Humans; Natural Language Processing; Phenotype; Rare Diseases; Vocabulary, Controlled},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 30; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Remy20241844,
	author = {Remy, François and Demuynck, Kris and Demeester, Thomas},
	title = {BioLORD-2023: Semantic textual representations fusing large language models and clinical knowledge graph insights},
	year = {2024},
	journal = {Journal of the American Medical Informatics Association},
	volume = {31},
	number = {9},
	pages = {1844 - 1855},
	doi = {10.1093/jamia/ocae029},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198060632&doi=10.1093%2Fjamia%2Focae029&partnerID=40&md5=d85d0f2c2171d4835e71438df835ddf1},
	abstract = {Objective: In this study, we investigate the potential of large language models (LLMs) to complement biomedical knowledge graphs in the training of semantic models for the biomedical and clinical domains. Materials and Methods: Drawing on the wealth of the Unified Medical Language System knowledge graph and harnessing cutting-edge LLMs, we propose a new state-of-the-art approach for obtaining high-fidelity representations of biomedical concepts and sentences, consisting of 3 steps: an improved contrastive learning phase, a novel self-distillation phase, and a weight averaging phase. Results: Through rigorous evaluations of diverse downstream tasks, we demonstrate consistent and substantial improvements over the previous state of the art for semantic textual similarity (STS), biomedical concept representation (BCR), and clinically named entity linking, across 15+ datasets. Besides our new state-of-the-art biomedical model for English, we also distill and release a multilingual model compatible with 50+ languages and finetuned on 7 European languages. Discussion: Many clinical pipelines can benefit from our latest models. Our new multilingual model enables a range of languages to benefit from our advancements in biomedical semantic representation learning, opening a new avenue for bioinformatics researchers around the world. As a result, we hope to see BioLORD-2023 becoming a precious tool for future biomedical applications. Conclusion: In this article, we introduced BioLORD-2023, a state-of-the-art model for STS and BCR designed for the clinical domain. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Biological Ontologies; Knowledge Bases; Machine Learning; Natural Language Processing; Semantics; Article; Bioinformatics; Biological Ontology; Distillation; Feature Learning (machine Learning); Human; Knowledge; Knowledge Base; Large Language Model; Learning; Machine Learning; Natural Language Processing; Semantics; Unified Medical Language System; Humans; Natural Language Processing; Semantics},
	keywords = {article; bioinformatics; biological ontology; distillation; feature learning (machine learning); human; knowledge; knowledge base; large language model; learning; machine learning; natural language processing; semantics; Unified Medical Language System; Humans; Natural Language Processing; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Barrios-Núñez2024,
	author = {Barrios-Núñez, Israel and Martínez-Redondo, Gemma I. and Medina-Burgos, Patricia and Cases, Ildefonso and Fernández, Rosa and Rojas, Ana María},
	title = {Decoding functional proteome information in model organisms using protein language models},
	year = {2024},
	journal = {NAR Genomics and Bioinformatics},
	volume = {6},
	number = {3},
	pages = {},
	doi = {10.1093/nargab/lqae078},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197633024&doi=10.1093%2Fnargab%2Flqae078&partnerID=40&md5=7de8106991c9c6dffeb326123fcfdcbe},
	abstract = {Protein language models have been tested and proved to be reliable when used on curated datasets but have not yet been applied to full proteomes. Accordingly, we tested how two different machine learning-based methods performed when decoding functional information from the proteomes of selected model organisms. We found that protein language models are more precise and informative than deep learning methods for all the species tested and across the three gene ontologies studied, and that they better recover functional information from transcriptomic experiments. The results obtained indicate that these language models are likely to be suitable for large-scale annotation and downstream analyses, and we recommend a guide for their use. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Calcium Ion; Collagen; Interferon Induced Helicase C Domain Containing Protein 1; Rna Helicase; Rna Polymerase; Bcl2 Adenovirus E1b 19 Kda Interacting Protein 2; Binding Protein; Calcium Ion; Collagen; Netrin; Proteome; Rna Helicase; Rna Polymerase; Tbk1 Binding Protein 1; Unclassified Drug; Upf0184 Protein; Animal Cell; Article; Calcium Binding; Cell Adhesion; Comparative Study; Deep Learning; Differential Gene Expression; Embedding; Gene Ontology; Global Change; Human; Human Cell; Information Retrieval; Innate Immunity; Molecular Genetics; Mouse; Mus Musculus; Nonhuman; Prediction; Protein Depletion; Protein Function; Protein Language Model; Protein Protein Interaction; Protein Transport; Sequence Analysis; Synaptic Transmission; Transcriptomics; Tumor Growth},
	keywords = {bcl2 adenovirus e1b 19 kda interacting protein 2; binding protein; calcium ion; collagen; netrin; proteome; RNA helicase; RNA polymerase; tbk1 binding protein 1; unclassified drug; upf0184 protein; animal cell; Article; calcium binding; cell adhesion; comparative study; deep learning; differential gene expression; embedding; gene ontology; global change; human; human cell; information retrieval; innate immunity; molecular genetics; mouse; Mus musculus; nonhuman; prediction; protein depletion; protein function; protein language model; protein protein interaction; protein transport; sequence analysis; synaptic transmission; transcriptomics; tumor growth},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Kushwah20241097,
	author = {Kushwah, Ankita Singh and Dixit, Himisha and Upadhyay, Vipin and Verma, Shailender Kumar and Prasad, Ramasare A.},
	title = {The study of iron- and copper-binding proteome of Fusarium oxysporum and its effector candidates},
	year = {2024},
	journal = {Proteins: Structure, Function and Bioinformatics},
	volume = {92},
	number = {9},
	pages = {1097 - 1112},
	doi = {10.1002/prot.26696},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191296052&doi=10.1002%2Fprot.26696&partnerID=40&md5=11c9f10b3010f52caf59707128903ef5},
	abstract = {Fusarium oxysporum f.sp. lycopersici is a phytopathogen which causes vascular wilt disease in tomato plants. The survival tactics of both pathogens and hosts depend on intricate interactions between host plants and pathogenic microbes. Iron-binding proteins (IBPs) and copper-binding proteins (CBPs) play a crucial role in these interactions by participating in enzyme reactions, virulence, metabolism, and transport processes. We employed high-throughput computational tools at the sequence and structural levels to investigate the IBPs and CBPs of F. oxysporum. A total of 124 IBPs and 37 CBPs were identified in the proteome of Fusarium. The ranking of amino acids based on their affinity for binding with iron is Glu > His> Asp > Asn > Cys, and for copper is His > Asp > Cys respectively. The functional annotation, determination of subcellular localization, and Gene Ontology analysis of these putative IBPs and CBPs have unveiled their potential involvement in a diverse array of cellular and biological processes. Three iron-binding glycosyl hydrolase family proteins, along with four CBPs with carbohydrate-binding domains, have been identified as potential effector candidates. These proteins are distinct from the host Solanum lycopersicum proteome. Moreover, they are known to be located extracellularly and function as enzymes that degrade the host cell wall during pathogen–host interactions. The insights gained from this report on the role of metal ions in plant–pathogen interactions can help develop a better understanding of their fundamental biology and control vascular wilt disease in tomato plants. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bioinformatics; Fusarium Oxysporum; Metalloproteome; Plant–pathogen Interaction; Vascular Wilt; Catalase; Copper; Cytochrome P450; Glutathione; Glycine; Glycosidase; Hemicellulose; Iron; Lactate Dehydrogenase; Lactate Dehydrogenase A; Lignin; Nitrate Reductase; Nitric Oxide; Pectin; Reduced Nicotinamide Adenine Dinucleotide Dehydrogenase; Succinate Dehydrogenase; Xanthine Dehydrogenase; Carrier Protein; Carrier Proteins; Copper; Copper-binding Protein; Fungal Proteins; Iron; Iron-binding Proteins; Proteome; Cytoscape V3.9.0 Software; Catalase; Copper; Cytochrome P450; Glutathione; Glycine; Glycosidase; Hemicellulose; Iron; Lactate Dehydrogenase; Lignin; Metalloprotein; Mycotoxin; Nitrate Reductase; Nitric Oxide; Pectin; Proteome; Reactive Oxygen Metabolite; Reduced Nicotinamide Adenine Dinucleotide Dehydrogenase; Succinate Dehydrogenase; Xanthine Dehydrogenase; Carrier Protein; Copper-binding Protein; Fungal Protein; Iron Binding Protein; Protein Binding; Amino Acid Sequence; Amino Acid Synthesis; Antioxidant Activity; Article; Aspergillus Fumigatus; Aspergillus Niger; Binding Affinity; Bioinformatics; Cellular Distribution; Fungal Viability; Fungal Virulence; Fusarium Oxysporum; Gene Ontology; Glycolysis; Host Pathogen Interaction; Iron Homeostasis; Machine Learning; Morphogenesis; Nonhuman; Oxidative Stress; Phytopathogen; Protein Interaction; Protein Language Model; Protein Synthesis; Proteomics; Rna Isolation; Saccharomyces Cerevisiae; Sequence Alignment; Support Vector Machine; Tomato; Chemistry; Fusarium; Genetics; Metabolism; Microbiology; Plant Disease; Carrier Proteins; Copper; Fungal Proteins; Iron; Iron-binding Proteins; Plant Diseases; Protein Binding; Proteome; Solanum Lycopersicum},
	keywords = {catalase; copper; cytochrome P450; glutathione; glycine; glycosidase; hemicellulose; iron; lactate dehydrogenase; lignin; metalloprotein; mycotoxin; nitrate reductase; nitric oxide; pectin; proteome; reactive oxygen metabolite; reduced nicotinamide adenine dinucleotide dehydrogenase; succinate dehydrogenase; xanthine dehydrogenase; carrier protein; copper-binding protein; fungal protein; iron binding protein; protein binding; amino acid sequence; amino acid synthesis; antioxidant activity; Article; Aspergillus fumigatus; Aspergillus niger; binding affinity; bioinformatics; cellular distribution; fungal viability; fungal virulence; Fusarium oxysporum; gene ontology; glycolysis; host pathogen interaction; iron homeostasis; machine learning; morphogenesis; nonhuman; oxidative stress; phytopathogen; protein interaction; protein language model; protein synthesis; proteomics; RNA isolation; Saccharomyces cerevisiae; sequence alignment; support vector machine; tomato; chemistry; Fusarium; genetics; metabolism; microbiology; plant disease; Carrier Proteins; Copper; Fungal Proteins; Iron; Iron-Binding Proteins; Plant Diseases; Protein Binding; Proteome; Solanum lycopersicum},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Zhang20246644,
	author = {Zhang, Yunyi and Zhong, Ming and Ouyang, Siru and Jiao, Yizhu and Zhou, Sizhe and Ding, Linyi and Han, Jiawei},
	title = {Automated Mining of Structured Knowledge from Text in the Era of Large Language Models},
	year = {2024},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {6644 - 6654},
	doi = {10.1145/3637528.3671469},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203709209&doi=10.1145%2F3637528.3671469&partnerID=40&md5=3f93c8ef5b9f84c2112f6d558038140b},
	abstract = {Massive amount of unstructured text data are generated daily, ranging from news articles to scientific papers. How to mine structured knowledge from the text data remains a crucial research question. Recently, large language models (LLMs) have shed light on the text mining field with their superior text understanding and instruction-following ability. There are typically two ways of utilizing LLMs: fine-tune the LLMs with human-annotated training data, which is labor intensive and hard to scale; prompt the LLMs in a zero-shot or few-shot way, which cannot take advantage of the useful information in the massive text data. Therefore, it remains a challenge on automated mining of structured knowledge from massive text data in the era of large language models. In this tutorial, we cover the recent advancements in mining structured knowledge using language models with very weak supervision. We will introduce the following topics in this tutorial: (1) introduction to large language models, which serves as the foundation for recent text mining tasks, (2) ontology construction, which automatically enriches an ontology from a massive corpus, (3) weakly-supervised text classification in flat and hierarchical label space, (4) weakly-supervised information extraction, which extracts entity and relation structures. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Text Mining; Weak Supervision; Self-supervised Learning; Zero-shot Learning; Automated Mining; Language Model; Large Language Model; News Articles; Scientific Papers; Structured Knowledge; Text Data; Text-mining; Unstructured Texts; Weak Supervision; Ontology},
	keywords = {Self-supervised learning; Zero-shot learning; Automated mining; Language model; Large language model; News articles; Scientific papers; Structured knowledge; Text data; Text-mining; Unstructured texts; Weak supervision; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Komarlu20241407,
	author = {Komarlu, Tanay and Jiang, Minhao and Wang, Xuan and Han, Jiawei},
	title = {OntoType: Ontology-Guided and Pre-Trained Language Model Assisted Fine-Grained Entity Typing},
	year = {2024},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {1407 - 1417},
	doi = {10.1145/3637528.3671745},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203677734&doi=10.1145%2F3637528.3671745&partnerID=40&md5=97c82da512a0a544fda0beb9f63b47b8},
	abstract = {Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, is a basic but important task for knowledge extraction from unstructured text. FET has been studied extensively in natural language processing and typically relies on human-annotated corpora for training, which is costly and difficult to scale. Recent studies explore the utilization of pre-trained language models (PLMs) as a knowledge base to generate rich and context-aware weak supervision for FET. However, a PLM still requires direction and guidance to serve as a knowledge base as they often generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel annotation-free, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods as well as a typical LLM method, ChatGPT. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Fine-grained Entity Typing; Masked Language Model Prompting; Natural Language Understanding; Zero-shot Entity Typing; Context Sensitive Languages; Economic And Social Effects; Inference Engines; Modeling Languages; Natural Language Processing Systems; Ontology; Context-sensitive; Fine Grained; Fine-grained Entity Typing; Language Model; Masked Language Model Prompting; Natural Language Understanding; Natural Languages; Ontological Structures; Ontology's; Zero-shot Entity Typing; Semantics},
	keywords = {Context sensitive languages; Economic and social effects; Inference engines; Modeling languages; Natural language processing systems; Ontology; Context-sensitive; Fine grained; Fine-grained entity typing; Language model; Masked language model prompting; Natural language understanding; Natural languages; Ontological structures; Ontology's; Zero-shot entity typing; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Ouyang20242318,
	author = {Ouyang, Siru and Huang, Jiaxin and Pillai, Pranav and Zhang, Yunyi and Zhang, Yu and Han, Jiawei},
	title = {Ontology Enrichment for Effective Fine-grained Entity Typing},
	year = {2024},
	journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {2318 - 2327},
	doi = {10.1145/3637528.3671857},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203669221&doi=10.1145%2F3637528.3671857&partnerID=40&md5=af6cf658b176516fbabd50b079820512},
	abstract = {Fine-grained entity typing (FET) is the task of identifying specific entity types at a fine-grained level for entity mentions based on their contextual information. Conventional methods for FET require extensive human annotation, which is time-consuming and costly given the massive scale of data. Recent studies have been developing weakly supervised or zero-shot approaches. We study the setting of zero-shot FET where only an ontology is provided. However, most existing ontology structures lack rich supporting information and even contain ambiguous relations, making them ineffective in guiding FET. Recently developed language models, though promising in various few-shot and zero-shot NLP tasks, may face challenges in zero-shot FET due to their lack of interaction with task-specific ontology. In this study, we propose øurs, where we (1) enrich each node in the ontology structure with two categories of extra information:instance information for training sample augmentation andtopic information to relate types with contexts, and (2) develop a coarse-to-fine typing algorithm that exploits the enriched information by training an entailment model with contrasting topics and instance-based augmented training samples. Our experiments show that øurs achieves high-quality fine-grained entity typing without human annotation, outperforming existing zero-shot methods by a large margin and rivaling supervised methods. øurs also enjoys strong transferability to unseen and finer-grained types. We will open source this work upon acceptance. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Fine-grained Entity Typing; Language Models; Natural Language Inference; Zero-shot Learning; Inference Engines; Modeling Languages; Natural Language Processing Systems; Ontology; Supervised Learning; Fine Grained; Fine-grained Entity Typing; Human Annotations; Language Inference; Language Model; Natural Language Inference; Natural Languages; Ontology Enrichment; Ontology's; Training Sample; Zero-shot Learning},
	keywords = {Inference engines; Modeling languages; Natural language processing systems; Ontology; Supervised learning; Fine grained; Fine-grained entity typing; Human annotations; Language inference; Language model; Natural language inference; Natural languages; Ontology enrichment; Ontology's; Training sample; Zero-shot learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Kakalou20241018,
	author = {Kakalou, Christina Asimina and Karamanidou, Christina and Dalamagas, Theodore M. and Koubarakis, Manolis},
	title = {Enhancing Patient Empowerment and Health Literacy: Integrating Knowledge Graphs with Language Models for Personalized Health Content Delivery},
	year = {2024},
	journal = {Studies in Health Technology and Informatics},
	volume = {316},
	pages = {1018 - 1022},
	doi = {10.3233/SHTI240582},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202005489&doi=10.3233%2FSHTI240582&partnerID=40&md5=494c9246fdc5ef47fd4961b34820e80a},
	abstract = {Health literacy empowers people to access, understand and apply health information to effectively manage their own health and to be an active participant in healthcare decisions. In this paper we propose a conceptual model for cognitive factors affecting health literacy and related socioeconomic aspects. Then we develop the HEALIE Knowledge Graph to represent the model, drawing from various medical ontologies, resources, and insights from domain experts. Finally, we combine the Knowledge Graph with a Large Language Model to generate personalised medical content and showcase the results through an example. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Health Literacy; Knowledge Graphs; Natural Language Generation; Patient Empowerment; Retrieval Augmented Generation; Electronic Health Record; Contents Deliveries; Health Informations; Health Literacy; Knowledge Graphs; Language Model; Natural Language Generation; Patient Empowerments; Patient Health; Personalized Healths; Retrieval Augmented Generation; Knowledge Graph; Cognition; Conceptual Model; Conference Paper; Female; Health Literacy; Human; Knowledge Graph; Language Model; Large Language Model; Medical Information; Patient Empowerment; Retrieval Augmented Generation; Socioeconomics; Empowerment; Natural Language Processing; Patient Participation; Personalized Medicine; Empowerment; Health Literacy; Humans; Natural Language Processing; Patient Participation; Precision Medicine},
	keywords = {Electronic health record; Contents deliveries; Health informations; Health literacy; Knowledge graphs; Language model; Natural language generation; Patient empowerments; Patient health; Personalized healths; Retrieval augmented generation; Knowledge graph; cognition; conceptual model; conference paper; female; health literacy; human; knowledge graph; language model; large language model; medical information; patient empowerment; retrieval augmented generation; socioeconomics; empowerment; natural language processing; patient participation; personalized medicine; Empowerment; Health Literacy; Humans; Natural Language Processing; Patient Participation; Precision Medicine},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Santos2024230,
	author = {Santos, Ricardo L. and Cruz-Correia, Ricardo João},
	title = {Improving Healthcare Quality with a LHS: From Patient-Generated Health Data to Evidence-Based Recommendations},
	year = {2024},
	journal = {Studies in Health Technology and Informatics},
	volume = {316},
	pages = {230 - 234},
	doi = {10.3233/SHTI240387},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202002694&doi=10.3233%2FSHTI240387&partnerID=40&md5=6c6130bf45b89fd963629a613e26b847},
	abstract = {One approach to enriching the Learning Health System (LHS) is leveraging vital signs and data from wearable technologies. Blood oxygen, heart rate, respiration rates, and other data collected by wearables (like sleep and exercise patterns) can be used to monitor and predict health conditions. This data is already being collected and could be used to improve healthcare in several ways. Our approach will be health data interoperability with HL7 FHIR (for data exchange between different systems), openEHR (to store researchable data separated from software but connected to ontologies, external terminologies and code sets) and maintain the semantics of data. OpenEHR is a standard that has an important role in modelling processes and clinical decisions. The six pillars of Lifestyle Medicine can be a first attempt to change how patients see their daily decisions, affecting the mid to long-term evolution of their health. Our objective is to develop the first stage of the LHS based on a co-produced personal health recording (CoPHR) built on top of a local LLM that interoperates health data through HL7 FHIR, openEHR, OHDSI and terminologies that can ingest external evidence and produces clinical and personal decision support and, when combined with many other patients, can produce or confirm evidence. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Decision; Health Data Interoperability; Hl7 Fhir; Learning Health System; Openehr; Patient-generated Health Data; Personal Health Recording; Clinical Research; Clinical Decision; Data Interoperability; Health Data; Health Data Interoperability; Health Systems; Hl7 Fhir; Learning Health System; Openehr; Patient-generated Health Data; Personal Health; Personal Health Recording; Electronic Health Record; Data Interoperability; Electronic Health Record; Evidence Based Medicine; Human; Learning Health System; Medical Record; Total Quality Management; Wearable Computer; Electronic Health Records; Evidence-based Medicine; Health Information Interoperability; Humans; Learning Health System; Patient Generated Health Data; Quality Improvement; Wearable Electronic Devices},
	keywords = {Clinical research; Clinical decision; Data interoperability; Health data; Health data interoperability; Health systems; HL7 FHIR; Learning health system; Openehr; Patient-generated health data; Personal health; Personal health recording; Electronic health record; data interoperability; electronic health record; evidence based medicine; human; learning health system; medical record; total quality management; wearable computer; Electronic Health Records; Evidence-Based Medicine; Health Information Interoperability; Humans; Learning Health System; Patient Generated Health Data; Quality Improvement; Wearable Electronic Devices},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Zeng2024,
	author = {Zeng, Kaisheng and Jin, Hailong and Lv, Xin and Zhu, Fangwei and Hou, Lei and Zhang, Yi and Pang, Fan and Qi, Yu and Liu, Dingxiao and Li, Juanzi},
	title = {XLORE 3: A Large-Scale Multilingual Knowledge Graph from Heterogeneous Wiki Knowledge Resources},
	year = {2024},
	journal = {ACM Transactions on Information Systems},
	volume = {42},
	number = {6},
	pages = {},
	doi = {10.1145/3660521},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208282862&doi=10.1145%2F3660521&partnerID=40&md5=2f212e4daf0c28d298e92695aee893b4},
	abstract = {In recent years, knowledge graph (KG) has attracted significant attention from academia and industry, resulting in the development of numerous technologies for KG construction, completion, and application. XLORE is one of the largest multilingual KGs built from Baidu Baike and Wikipedia via a series of knowledge modeling and acquisition methods. In this article, we utilize systematic methods to improve XLORE's data quality and present its latest version, XLORE 3, which enables the effective integration and management of heterogeneous knowledge from diverse resources. Compared with previous versions, XLORE 3 has three major advantages: (1) We design a comprehensive and reasonable schema, namely XLORE ontology, which can effectively organize and manage entities from various resources. (2) We merge equivalent entities in different languages to facilitate knowledge sharing. We provide a large-scale entity linking system to establish the associations between unstructured text and structured KG. (3) We design a multi-strategy knowledge completion framework, which leverages pre-trained language models and vast amounts of unstructured text to discover missing and new facts. The resulting KG contains 446 concepts, 2,608 properties, 66 million entities, and more than 2 billion facts. It is available and downloadable online at https://www.xlore.cn/, providing a valuable resource for researchers and practitioners in various fields. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Entity Alignment; Entity Linking; Entity Typing; Knowledge Completion; Knowledge Fusion; Knowledge Graph; Knowledge Management; Schema Construction; Entity Alignment; Entity Linking; Entity Typing; Knowledge Completion; Knowledge Fusion; Knowledge Graphs; Knowledge Resource; Large-scales; Schema Construction; Unstructured Texts; Knowledge Graph},
	keywords = {Entity alignment; Entity linking; Entity typing; Knowledge completion; Knowledge fusion; Knowledge graphs; Knowledge resource; Large-scales; Schema construction; Unstructured texts; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Bronze Open Access}
}

@ARTICLE{Huang20246790,
	author = {Huang, Yiru and Zhang, Lei},
	title = {Descriptor Design for Perovskite Material with Compatible Molecules via Language Model and First-Principles},
	year = {2024},
	journal = {Journal of Chemical Theory and Computation},
	volume = {20},
	number = {15},
	pages = {6790 - 6800},
	doi = {10.1021/acs.jctc.4c00465},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199279977&doi=10.1021%2Facs.jctc.4c00465&partnerID=40&md5=4e915d82b30e9435b0b10c5385080495},
	abstract = {Directly applying big language models for material and molecular design is not straightforward, particularly for real-scenario cases, where experimental validation accuracy is required. In this study, we propose a multimode descriptor design method for materials prediction and analysis, leveraging the advantages of the natural language processing literature model and density functional theory (DFT) calculations with the assistance of the genetic algorithm (GA). A case study on prediction of aqueous photocurrents of multisolvent engineered halide perovskite CH<inf>3</inf>NH<inf>3</inf>PbI<inf>3</inf> is performed, and the following-up validation experiments are carried out to demonstrate the improved accuracy of the multimode descriptors (an unprecedented experimental validation accuracy of 87.5% via the GA is achieved) for predicting aqueous photocurrents of perovskite materials (c.f. only 50% experimental accuracy for other common machine learning models). The improved experimental accuracy of the descriptors is attributed to the successful deployment of a language model incorporating concise scientific information from >1 million articles into molecular descriptors in combination with DFT calculations. The subsequent machine learning analysis suggests the importance of cation···π and crystallization in molecule-modified halide perovskite materials representing ontological and conceptual understanding. Importantly, the genetic process affords an accurate “white-box” model to describe the perovskite stability (accuracy = 90.2% for the test data set and 92.3% for the train data set) with the mathematical equation Formula Presented, where F<inf>1</inf> ∼ F<inf>5</inf> atomic-level structural and chemical details such as cation···π interactions and highest occupied molecular orbital levels. This study offers a feasible descriptor design route to accurately predict complex material properties, leveraging both language models and density functional theories. © 2024 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{2024,
	title = {Asian Internet Engineering Conference, AINTEC 2024},
	year = {2024},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203294942&partnerID=40&md5=f89a468b6b5f9e71f70f6b1d2d04e8ad},
	abstract = {The proceedings contain 9 papers. The topics discussed include: measuring Genai usage patterns in a university campus via network traffic analysis; on the impact of heterogeneity on federated learning at the edge with DGA malware detection; systematic mapping and temporal reasoning of IoT cyber risks using structured data; TrafficGPT: an LLM approach for open-set encrypted traffic classification; examining technologies to reduce response time in hands-on exercise environment over widely distributed computer network utilizing RENs; dynamic fixed-point values in eBPF: a case for fully in-kernel anomaly detection; the quest for a resilient internet access in a constrained geopolitical environment; and detecting inconsistency between network design and current state based on network ontology bonsai. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Tzimas2024,
	author = {Tzimas, Giannis E. and Zotos, Nikos and Mourelatos, Evangelos and Giotopoulos, Konstantinos C. and Zervas, Panagiotis},
	title = {From Data to Insight: Transforming Online Job Postings into Labor-Market Intelligence},
	year = {2024},
	journal = {Information (Switzerland)},
	volume = {15},
	number = {8},
	pages = {},
	doi = {10.3390/info15080496},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202350400&doi=10.3390%2Finfo15080496&partnerID=40&md5=7c6af7355097eab19102a8b7e975fd95},
	abstract = {In the continuously changing labor market, understanding the dynamics of online job postings is crucial for economic and workforce development. With the increasing reliance on Online Job Portals, analyzing online job postings has become an essential tool for capturing real-time labor-market trends. This paper presents a comprehensive methodology for processing online job postings to generate labor-market intelligence. The proposed methodology encompasses data source selection, data extraction, cleansing, normalization, and deduplication procedures. The final step involves information extraction based on employer industry, occupation, workplace, skills, and required experience. We address the key challenges that emerge at each step and discuss how they can be resolved. Our methodology is applied to two use cases: the first focuses on the analysis of the Greek labor market in the tourism industry during the COVID-19 pandemic, revealing shifts in job demands, skill requirements, and employment types. In the second use case, a data-driven ontology is employed to extract skills from job postings using machine learning. The findings highlight that the proposed methodology, utilizing NLP and machine-learning techniques instead of LLMs, can be applied to different labor market-analysis use cases and offer valuable insights for businesses, job seekers, and policymakers. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Data-driven Ontology; Labor Information Extraction; Labor-market Analysis; Machine Learning; Nlp; Online Job Postings; Job Analysis; Leisure Industry; Ontology; Data Driven; Data-driven Ontology; Job Postings; Labor Information Extraction; Labor-market Analyze; Labour Market; Machine-learning; Market Analysis; Online Job Posting; Ontology's; Tourism Industry},
	keywords = {Job analysis; Leisure industry; Ontology; Data driven; Data-driven ontology; Job postings; Labor information extraction; Labor-market analyze; Labour market; Machine-learning; Market analysis; Online job posting; Ontology's; Tourism industry},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@ARTICLE{Delgadillo2024,
	author = {Delgadillo, Josiel and Kinyua, Johnson D. and Mutigwe, Charles},
	title = {FinSoSent: Advancing Financial Market Sentiment Analysis through Pretrained Large Language Models},
	year = {2024},
	journal = {Big Data and Cognitive Computing},
	volume = {8},
	number = {8},
	pages = {},
	doi = {10.3390/bdcc8080087},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202346804&doi=10.3390%2Fbdcc8080087&partnerID=40&md5=9483643ed340e92ae8e3e4ce1d581d45},
	abstract = {Predicting the directions of financial markets has been performed using a variety of approaches, and the large volume of unstructured data generated by traders and other stakeholders on social media microblog platforms provides unique opportunities for analyzing financial markets using additional perspectives. Pretrained large language models (LLMs) have demonstrated very good performance on a variety of sentiment analysis tasks in different domains. However, it is known that sentiment analysis is a very domain-dependent NLP task that requires knowledge of the domain ontology, and this is particularly the case with the financial domain, which uses its own unique vocabulary. Recent developments in NLP and deep learning including LLMs have made it possible to generate actionable financial sentiments using multiple sources including financial news, company fundamentals, technical indicators, as well social media microblogs posted on platforms such as StockTwits and X (formerly Twitter). We developed a financial social media sentiment analyzer (FinSoSent), which is a domain-specific large language model for the financial domain that was pretrained on financial news articles and fine-tuned and tested using several financial social media corpora. We conducted a large number of experiments using different learning rates, epochs, and batch sizes to yield the best performing model. Our model outperforms current state-of-the-art FSA models based on over 860 experiments, demonstrating the efficacy and effectiveness of FinSoSent. We also conducted experiments using ensemble models comprising FinSoSent and the other current state-of-the-art FSA models used in this research, and a slight performance improvement was obtained based on majority voting. Based on the results obtained across all models in these experiments, the significance of this study is that it highlights the fact that, despite the recent advances of LLMs, sentiment analysis even in domain-specific contexts remains a difficult research problem. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Financial Markets; Llm; Sentiment Analysis; Social Media; Stocktwits; Twitter/x; Decentralized Finance; Financial Markets; Market Research; Problem Oriented Languages; Sentiment Analysis; Bert; Financial Domains; Language Model; Large Language Model; Micro-blog; Performance; Social Media; Stocktwit; Twitter/x; Tweets},
	keywords = {Decentralized finance; Financial markets; Market Research; Problem oriented languages; Sentiment analysis; BERT; Financial domains; Language model; Large language model; Micro-blog; Performance; Social media; Stocktwit; Twitter/X; Tweets},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access; Gold Open Access}
}

@ARTICLE{Dickson2024,
	author = {Dickson, Andrew M. and Kaazempur-Mofrad, Mohammad Reza},
	title = {Fine-tuning protein embeddings for functional similarity evaluation},
	year = {2024},
	journal = {Bioinformatics},
	volume = {40},
	number = {8},
	pages = {},
	doi = {10.1093/bioinformatics/btae445},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200763463&doi=10.1093%2Fbioinformatics%2Fbtae445&partnerID=40&md5=7c4bbf622ec33c0a5721bdb08c0c5bb2},
	abstract = {Motivation: Proteins with unknown function are frequently compared to better characterized relatives, either using sequence similarity, or recently through similarity in a learned embedding space. Through comparison, protein sequence embeddings allow for interpretable and accurate annotation of proteins, as well as for downstream tasks such as clustering for unsupervised discovery of protein families. However, it is unclear whether embeddings can be deliberately designed to improve their use in these downstream tasks. Results: We find that for functional annotation of proteins, as represented by Gene Ontology (GO) terms, direct fine-tuning of language models on a simple classification loss has an immediate positive impact on protein embedding quality. Fine-tuned embeddings show stronger performance as representations for K-nearest neighbor classifiers, reaching stronger performance for GO annotation than even directly comparable fine-tuned classifiers, while maintaining interpretability through protein similarity comparisons. They also maintain their quality in related tasks, such as rediscovering protein families with clustering. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Amino Acid Sequence; Article; Classifier; Gene Ontology; Human; K Nearest Neighbor; Language Model; Algorithm; Bioinformatics; Chemistry; Cluster Analysis; Molecular Genetics; Procedures; Protein Database; Protein; Algorithms; Cluster Analysis; Computational Biology; Databases, Protein; Gene Ontology; Molecular Sequence Annotation; Proteins},
	keywords = {amino acid sequence; article; classifier; gene ontology; human; k nearest neighbor; language model; algorithm; bioinformatics; chemistry; cluster analysis; molecular genetics; procedures; protein database; protein; Algorithms; Cluster Analysis; Computational Biology; Databases, Protein; Gene Ontology; Molecular Sequence Annotation; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Li20246872,
	author = {Li, Xiaodong and Tian, Guohui and Cui, Yongcheng},
	title = {Fine-Grained Task Planning for Service Robots Based on Object Ontology Knowledge via Large Language Models},
	year = {2024},
	journal = {IEEE Robotics and Automation Letters},
	volume = {9},
	number = {8},
	pages = {6872 - 6879},
	doi = {10.1109/LRA.2024.3412593},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196082646&doi=10.1109%2FLRA.2024.3412593&partnerID=40&md5=75ddbef911fd12968cb418b84b3dc2d0},
	abstract = {In domestic environment, the successful execution of service tasks heavily relies on the robot's capability to identify and understand objects within its surrounding. This crucial process predominantly takes place during task planning, prior to the actual performance of service tasks. Therefore, it is vital that the robot is capable of formulating object-specific action sequences through task planning. In this letter, we propose the Fine-Grained Task Planning (FGTP) framework, an innovative method that combines object ontology knowledge with Large Language Models (LLMs) to create detailed action sequences. The FGTP framework is uniquely designed to process both text descriptions of service tasks and images of relevant objects, enabling a thorough comprehension of object attributes essential for task execution. Moreover, we have developed a set of rules based on these attributes to assist in the robot's decision-making process. In scenarios where service tasks fail because the object is in an unsuitable state, our framework deploys a logic-based reasoning method, concentrating on object attributes to identify suitable substitutes. This process leverages a pre-established semantic map to locate these alternatives, thus enabling a transition back to standard task planning. Our evaluations, conducted in both the VirtualHome simulation environment and with the TIAGo real robot, demonstrate the efficacy of our approach. This confirms our framework's capability to generate practical and implementable plans for various service tasks. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Service Robotics; Task Planning; Computational Linguistics; Decision Making; Job Analysis; Ontology; Robot Programming; Semantics; Fine Grained; Language Model; Object Ontology; Objects Recognition; Ontology's; Robot Kinematics; Service Robotics; Service Robots; Task Analysis; Task Planning; Object Recognition},
	keywords = {Computational linguistics; Decision making; Job analysis; Ontology; Robot programming; Semantics; Fine grained; Language model; Object ontology; Objects recognition; Ontology's; Robot kinematics; Service robotics; Service robots; Task analysis; Task planning; Object recognition},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Rosen20241492,
	author = {Rosen, Yanay and Brbić, Maria and Roohani, Yusuf H. and Swanson, Kyle and Li, Ziang and Leskovec, Jure},
	title = {Toward universal cell embeddings: integrating single-cell RNA-seq datasets across species with SATURN},
	year = {2024},
	journal = {Nature Methods},
	volume = {21},
	number = {8},
	pages = {1492 - 1500},
	doi = {10.1038/s41592-024-02191-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185130025&doi=10.1038%2Fs41592-024-02191-z&partnerID=40&md5=ba74f7a77c034a25094fdd016901e879},
	abstract = {Analysis of single-cell datasets generated from diverse organisms offers unprecedented opportunities to unravel fundamental evolutionary processes of conservation and diversification of cell types. However, interspecies genomic differences limit the joint analysis of cross-species datasets to homologous genes. Here we present SATURN, a deep learning method for learning universal cell embeddings that encodes genes’ biological properties using protein language models. By coupling protein embeddings from language models with RNA expression, SATURN integrates datasets profiled from different species regardless of their genomic similarity. SATURN can detect functionally related genes coexpressed across species, redefining differential expression for cross-species analysis. Applying SATURN to three species whole-organism atlases and frog and zebrafish embryogenesis datasets, we show that SATURN can effectively transfer annotations across species, even when they are evolutionarily remote. We also demonstrate that SATURN can be used to find potentially divergent gene functions between glaucoma-associated genes in humans and four other species. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Rna; Protein; Rna; Amino Acid Sequence; Anura; Article; Controlled Study; Deep Learning; Differential Expression Analysis; Embedding; Embryo Development; Gene; Gene Expression; Gene Function; Gene Ontology; Glaucoma; Human; Nonhuman; Single Cell Rna Seq; Zebra Fish; Animal; Genetics; Procedures; Rna Sequencing; Single Cell Analysis; Single-cell Gene Expression Analysis; Species Difference; Animals; Deep Learning; Humans; Rna-seq; Single-cell Analysis; Single-cell Gene Expression Analysis; Species Specificity; Zebrafish},
	keywords = {protein; RNA; amino acid sequence; Anura; Article; controlled study; deep learning; differential expression analysis; embedding; embryo development; gene; gene expression; gene function; gene ontology; glaucoma; human; nonhuman; single cell RNA seq; zebra fish; animal; genetics; procedures; RNA sequencing; single cell analysis; single-cell gene expression analysis; species difference; Animals; Deep Learning; Humans; RNA-Seq; Single-Cell Analysis; Single-Cell Gene Expression Analysis; Species Specificity; Zebrafish},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28; All Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Goel2024417,
	author = {Goel, Drishti and Husain, Fiza and Singh, Aditya and Ghosh, Supriyo and Parayil, Anjaly and Bansal, Chetan and Zhang, Xuchao and Rajmohan, Saravan},
	title = {X-Lifecycle Learning for Cloud Incident Management using LLMs},
	year = {2024},
	pages = {417 - 428},
	doi = {10.1145/3663529.3663861},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198654519&doi=10.1145%2F3663529.3663861&partnerID=40&md5=8448341c30437754978bf56d51c9e332},
	abstract = {Incident management for large cloud services is a complex and tedious process that requires a significant amount of manual effort from on-call engineers (OCEs). OCEs typically leverage data from different stages of the software development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service properties, service dependencies, trouble-shooting documents, etc.) to generate insights for detection, root cause analysis and mitigation of incidents. Recent advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini) have created opportunities to automatically generate contextual recommendations for the OCEs, assisting them in quickly identifying and mitigating critical issues. However, existing research typically takes a silo-ed view of solving a certain task in incident management by leveraging data from a single stage of the SDLC. In this paper, we demonstrate that augmenting additional contextual data from different stages of the SDLC improves the performance of two critically important and practically challenging tasks: (1) automatically generating root cause recommendations for dependency failure related incidents, and (2) identifying the ontology of service monitors used for automatically detecting incidents. By leveraging a dataset of 353 incidents and 260 monitors from Microsoft, we demonstrate that augmenting contextual information from different stages of the SDLC improves the performance over state-of-the-art methods. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cloud Services; Large Language Models; Monitor Management; Reliability; Root-cause Analysis; Computational Linguistics; Distributed Database Systems; Life Cycle; Reliability Analysis; Software Design; Cloud Services; Code Configuration; Different Stages; Incident Management; Language Model; Large Language Model; Monitor Management; Performance; Root Cause Analysis; Software Development Life-cycle; Web Services},
	keywords = {Computational linguistics; Distributed database systems; Life cycle; Reliability analysis; Software design; Cloud services; Code configuration; Different stages; Incident Management; Language model; Large language model; Monitor management; Performance; Root cause analysis; Software development life-cycle; Web services},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Moore2024122,
	author = {Moore, Steven and Schmucker, Robin and Mitchell, Tom M. and Stamper, John C.},
	title = {Automated Generation and Tagging of Knowledge Components from Multiple-Choice Questions},
	year = {2024},
	pages = {122 - 133},
	doi = {10.1145/3657604.3662030},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199923750&doi=10.1145%2F3657604.3662030&partnerID=40&md5=46e69a9deb94066d6a2508deaf9d0cc1},
	abstract = {Knowledge Components (KCs) linked to assessments enhance the measurement of student learning, enrich analytics, and facilitate adaptivity. However, generating and linking KCs to assessment items requires significant effort and domain-specific knowledge. To streamline this process for higher-education courses, we employed GPT-4 to generate KCs for multiple-choice questions (MCQs) in Chemistry and E-Learning. We analyzed discrepancies between the KCs generated by the Large Language Model (LLM) and those made by humans through evaluation from three domain experts in each subject area. This evaluation aimed to determine whether, in instances of non-matching KCs, evaluators showed a preference for the LLM-generated KCs over their human-created counterparts. We also developed an ontology induction algorithm to cluster questions that assess similar KCs based on their content. Our most effective LLM strategy accurately matched KCs for 56% of Chemistry and 35% of E-Learning MCQs, with even higher success when considering the top five KC suggestions. Human evaluators favored LLM-generated KCs, choosing them over human-assigned ones approximately two-thirds of the time, a preference that was statistically significant across both domains. Our clustering algorithm successfully grouped questions by their underlying KCs without needing explicit labels or contextual information. This research advances the automation of KC generation and classification for assessment items, alleviating the need for student data or predefined KC labels. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Labeling; Knowledge Component; Knowledge Labeling; Learning Engineering; Multiple-choice Question; Clustering Algorithms; Domain Knowledge; Education Computing; Students; Automated Generation; Concept Labeling; E - Learning; Knowledge Components; Knowledge Labeling; Labelings; Language Model; Learning Engineering; Multiple-choice Questions; E-learning},
	keywords = {Clustering algorithms; Domain Knowledge; Education computing; Students; Automated generation; Concept labeling; E - learning; Knowledge components; Knowledge labeling; Labelings; Language model; Learning engineering; Multiple-choice questions; E-learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Yuan2024W248,
	author = {Yuan, Qianmu and Tian, Chong and Song, Yidong and Ou, Peihua and Zhu, Mingming and Zhao, Huiying and Yang, Yuedong},
	title = {GPSFun: Geometry-aware protein sequence function predictions with language models},
	year = {2024},
	journal = {Nucleic Acids Research},
	volume = {52},
	number = {W1},
	pages = {W248 - W255},
	doi = {10.1093/nar/gkae381},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197753062&doi=10.1093%2Fnar%2Fgkae381&partnerID=40&md5=e5d9e4277ebf95b1f2fb422b0f741f7b},
	abstract = {Knowledge of protein function is essential for elucidating disease mechanisms and discovering new drug targets. However, there is a widening gap between the exponential growth of protein sequences and their limited function annotations. In our prior studies, we have developed a series of methods including GraphPPIS, GraphSite, LMetalSite and SPROF-GO for protein function annotations at residue or protein level. To further enhance their applicability and performance, we now present GPSFun, a versatile web server for Geometry-aware Protein Sequence Function annotations, which equips our previous tools with language models and geometric deep learning. Specifically, GPSFun employs large language models to efficiently predict 3D conformations of the input protein sequences and extract informative sequence embeddings. Subsequently, geometric graph neural networks are utilized to capture the sequence and structure patterns in the protein graphs, facilitating various downstream predictions including protein-ligand binding sites, gene ontologies, subcellular locations and protein solubility. Notably, GPSFun achieves superior performance to state-of-the-art methods across diverse tasks without requiring multiple sequence alignments or experimental protein structures. GPSFun is freely available to all users at https://bio-web1.nscc-gz.cn/app/GPSFun with user-friendly interfaces and rich visualizations. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Algorithm; Amino Acid Sequence; Article; Base Pairing; Binding Site; Computer Model; Deep Learning; Entropy; Gene Mapping; Gene Ontology; Geometry; Language Model; Large Language Model; Ligand Binding; Machine Learning; Metagenome; Nerve Cell Network; Nonhuman; Prediction; Protein Function; Protein Phosphorylation; Protein Secondary Structure; Protein Structure; Receiver Operating Characteristic; Sequence Alignment; Sequence Homology; Three-dimensional Imaging; Training; Transcriptomics; Artificial Neural Network; Chemistry; Human; Internet; Metabolism; Molecular Genetics; Protein Conformation; Sequence Analysis; Software; Protein; Amino Acid Sequence; Binding Sites; Deep Learning; Humans; Molecular Sequence Annotation; Neural Networks, Computer; Protein Conformation; Proteins; Sequence Analysis, Protein; Software},
	keywords = {algorithm; amino acid sequence; Article; base pairing; binding site; computer model; deep learning; entropy; gene mapping; gene ontology; geometry; language model; large language model; ligand binding; machine learning; metagenome; nerve cell network; nonhuman; prediction; protein function; protein phosphorylation; protein secondary structure; protein structure; receiver operating characteristic; sequence alignment; sequence homology; three-dimensional imaging; training; transcriptomics; artificial neural network; chemistry; human; Internet; metabolism; molecular genetics; protein conformation; sequence analysis; software; protein; Amino Acid Sequence; Binding Sites; Deep Learning; Humans; Molecular Sequence Annotation; Neural Networks, Computer; Protein Conformation; Proteins; Sequence Analysis, Protein; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access; Gold Open Access}
}

@ARTICLE{Chen2024,
	author = {Chen, Nanjiang and Lin, Xuhui and Jiang, Hai and An, Yi},
	title = {Automated Building Information Modeling Compliance Check through a Large Language Model Combined with Deep Learning and Ontology},
	year = {2024},
	journal = {Buildings},
	volume = {14},
	number = {7},
	pages = {},
	doi = {10.3390/buildings14071983},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199602988&doi=10.3390%2Fbuildings14071983&partnerID=40&md5=eafbf129c07a06daf310af3daa64c1cf},
	abstract = {Ensuring compliance with complex industry standards and regulations during the design and implementation phases of construction projects is a significant challenge in the building information modeling (BIM) domain. Traditional manual compliance checking methods are inefficient and error-prone, failing to meet modern engineering demands. Natural language processing (NLP) and deep learning methods have improved efficiency and accuracy in rule interpretation and compliance checking. However, these methods still require extensive manual feature engineering, large, annotated datasets, and significant computational resources. Large language models (LLMs) provide robust language understanding with minimal labeled data due to their pre-training and few-shot learning capabilities. However, their application in the AEC field is still limited by the need for fine-tuning for specific tasks, handling complex texts with nested clauses and conditional statements. This study introduces an innovative automated compliance checking framework that integrates LLM, deep learning models, and ontology knowledge models. The use of LLM is motivated by its few-shot learning capability, which significantly reduces the need for large, annotated datasets required by previous methods. Deep learning is employed to preliminarily classify regulatory texts, which further enhances the accuracy of structured information extraction by the LLM compared to directly feeding raw data into the LLM. This novel combination of deep learning and LLM significantly enhances the efficiency and accuracy of compliance checks by automating the processing of regulatory texts and reducing manual intervention. This approach is crucial for architects, engineers, project managers, and regulators, providing a scalable and adaptable solution for automated compliance in the construction industry with broad application prospects. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Automated Compliance Check; Bim; Deep Learning; Design Regulations; Large Language Models (llms); Ontology Knowledge Models; Architectural Design; Automation; Classification (of Information); Compliance Control; Computational Linguistics; Construction Industry; Deep Learning; Efficiency; Information Theory; Large Datasets; Learning Systems; Natural Language Processing Systems; Ontology; Personnel Training; Regulatory Compliance; Structural Design; Automated Compliance Check; Building Information Modelling; Compliance Checks; Design Regulation; Knowledge Model; Language Model; Large Language Model; Ontology Knowledge Model; Ontology's; Modeling Languages},
	keywords = {Architectural design; Automation; Classification (of information); Compliance control; Computational linguistics; Construction industry; Deep learning; Efficiency; Information theory; Large datasets; Learning systems; Natural language processing systems; Ontology; Personnel training; Regulatory compliance; Structural design; Automated compliance check; Building Information Modelling; Compliance checks; Design regulation; Knowledge model; Language model; Large language model; Ontology knowledge model; Ontology's; Modeling languages},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access; Gold Open Access}
}

@CONFERENCE{Haghighi202470,
	author = {Haghighi, Nava},
	title = {Ontological Breakdown: Toward a World of Many Worlds},
	year = {2024},
	pages = {70 - 73},
	doi = {10.1145/3656156.3665133},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198901569&doi=10.1145%2F3656156.3665133&partnerID=40&md5=c0c9ae22ced7c3d502f80ff07b59c625},
	abstract = {Examining the taken-for-granted assumptions and views of the world underlying the design of technological artifacts, this work posits that a lack of ontological self-reflection can constrain imagination, impeding movement toward a world of many worlds. I propose ontological breakdown as an analytic lens for interrogating the default assumptions underlying the design of technology, using LLMs as a case-study and drawing parallels to the discourse on values in design. Then, I share three ways in which I have used ontological breakdowns generatively to (1) surface ontological difference and create spaces for experiencing ontological alternatives to our defaults, (2) explore ontological alternatives in the design of artifacts and enable the end-users to notice their own ontological defaults, and (3) expand ontological diversity by empowering the end-users to move beyond the prescribed defaults. I demonstrate the generative potential of ontological breakdowns by providing examples of my work in personal informatics. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Generative Ai; Llm; Ontological Breakdown; Ontological Design; Ontologies; Personal Informatics; Case-studies; End-users; Generative Ai; Llm; Ontological Breakdown; Ontological Design; Ontology's; Personal Informatics; Self Reflection; Value In Designs; Ontology},
	keywords = {Case-studies; End-users; Generative AI; LLM; Ontological breakdown; Ontological design; Ontology's; Personal informatics; Self reflection; Value in designs; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mukanova2024,
	author = {Mukanova, Assel S. and Milosz, Marek and Dauletkaliyeva, Assem and Nazyrova, Aizhan and Yelibayeva, Gaziza and Kuzin, Dmitrii A. and Kussepova, Lazzat T.},
	title = {LLM-Powered Natural Language Text Processing for Ontology Enrichment},
	year = {2024},
	journal = {Applied Sciences (Switzerland)},
	volume = {14},
	number = {13},
	pages = {},
	doi = {10.3390/app14135860},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198457806&doi=10.3390%2Fapp14135860&partnerID=40&md5=4e0679495529c9ad85ba2cee224641c0},
	abstract = {This paper describes a method and technology for processing natural language texts and extracting data from the text that correspond to the semantics of an ontological model. The proposed method is distinguished by the use of a Large Language Model algorithm for text analysis. The extracted data are stored in an intermediate format, after which individuals and properties that reflect the specified semantics are programmatically created in the ontology. The proposed technology is implemented using the example of an ontological model that describes the geographical configuration and administrative–territorial division of Kazakhstan. The proposed method and technology can be applied in any subject areas for which ontological models have been developed. The results of the study can significantly improve the efficiency of using knowledge bases based on semantic networks by converting texts in natural languages into semantically linked data. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Geographic Question Answering System; Large Language Model; Natural Language Processing; Ontology; Semantic Web},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access; Gold Open Access}
}

@ARTICLE{Groza2024,
	author = {Groza, Tudor and Gration, Dylan and Baynam, Gareth S. and Robinson, Peter Nicholas},
	title = {FastHPOCR: pragmatic, fast, and accurate concept recognition using the human phenotype ontology},
	year = {2024},
	journal = {Bioinformatics},
	volume = {40},
	number = {7},
	pages = {},
	doi = {10.1093/bioinformatics/btae406},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197975714&doi=10.1093%2Fbioinformatics%2Fbtae406&partnerID=40&md5=9ddcf00c24fc3ace81d0daa956c6cedd},
	abstract = {Motivation: Human Phenotype Ontology (HPO)-based phenotype concept recognition (CR) underpins a faster and more effective mechanism to create patient phenotype profiles or to document novel phenotype-centred knowledge statements. While the increasing adoption of large language models (LLMs) for natural language understanding has led to several LLM-based solutions, we argue that their intrinsic resource-intensive nature is not suitable for realistic management of the phenotype CR lifecycle. Consequently, we propose to go back to the basics and adopt a dictionary-based approach that enables both an immediate refresh of the ontological concepts as well as efficient re-analysis of past data. Results: We developed a dictionary-based approach using a pre-built large collection of clusters of morphologically equivalent tokens-to address lexical variability and a more effective CR step by reducing the entity boundary detection strictly to candidates consisting of tokens belonging to ontology concepts. Our method achieves state-of-the-art results (0.76 F1 on the GSCþ corpus) and a processing efficiency of 10 000 publication abstracts in 5 s. Availability and implementation: FastHPOCR is available as a Python package installable via pip. The source code is available at https://github.com/tudorgroza/fast_hpo_cr. A Java implementation of FastHPOCR will be made available as part of the Fenominal Java library available at https://github.com/monarch-initiative/fenominal. The up-to-date GCS-2024 corpus is available at https://github.com/tudorgroza/code-forpapers/tree/main/gsc-2024. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Algorithm; Biological Ontology; Human; Natural Language Processing; Phenotype; Software; Algorithms; Biological Ontologies; Humans; Natural Language Processing; Phenotype; Software},
	keywords = {algorithm; biological ontology; human; natural language processing; phenotype; software; Algorithms; Biological Ontologies; Humans; Natural Language Processing; Phenotype; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Mao2024,
	author = {Mao, Dianhui and Liu, Yiming and Li, Ruixuan and Chen, Junhua and Hao, Yuanrong and Wu, Jianwei},
	title = {Research on the joint event extraction method orientates food live e-commerce},
	year = {2024},
	journal = {Electronic Commerce Research and Applications},
	volume = {66},
	pages = {},
	doi = {10.1016/j.elerap.2024.101413},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194958472&doi=10.1016%2Fj.elerap.2024.101413&partnerID=40&md5=baeec1bd6ad810b965ec47effec8aa2e},
	abstract = {In the evolving landscape of food e-commerce live streaming, the profusion of textual data, marked by an excess of promotional vernacular and unstructured formats, presents a formidable challenge for event extraction. Addressing these hurdles, we introduce a tailored ontology-based method alongside FMLEE (Food Marketing Live Event Extraction), a joint event extraction algorithm. This approach simplifies the event identification process through meticulous segmentation and the development of an ontology comprising 5 event categories and 19 argument roles. By integrating context-aware embeddings derived from pre-trained language models and applying an adversarial learning tactic, our methodology not only bolsters the robustness of our model but also significantly refines its accuracy in discerning relevant events within the scarce-resource milieu of food live streaming promotions. The effectiveness of the FMLEE model is validated by its achievement of an F1 score of 73.05%, with the inclusion of adversarial learning contributing to a 2.61% enhancement in performance. This evidences our novel contribution to the domain, offering robust technical support for the optimal exploitation of information within the sphere of food live streaming promotions. Simultaneously, this aids in the investigation of innovative applications for consumer engagement within marketing strategies and the smart regulation of marketing activities. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Event Extraction; Food E-commerce Live Streaming; Knowledge Graph; Ontology Construction; Data Mining; Electronic Commerce; Extraction; Learning Systems; Ontology; Adversarial Learning; E- Commerces; Events Extractions; Extraction Method; Food E-commerce Live Streaming; Knowledge Graphs; Live Streaming; Ontology Construction; Ontology-based Methods; Textual Data; Marketing},
	keywords = {Data mining; Electronic commerce; Extraction; Learning systems; Ontology; Adversarial learning; E- commerces; Events extractions; Extraction method; Food e-commerce live streaming; Knowledge graphs; Live streaming; Ontology construction; Ontology-based methods; Textual data; Marketing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2024,
	author = {Li, Yunqing and Starly, Binil},
	title = {Building a knowledge graph to enrich ChatGPT responses in manufacturing service discovery},
	year = {2024},
	journal = {Journal of Industrial Information Integration},
	volume = {40},
	pages = {},
	doi = {10.1016/j.jii.2024.100612},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190988149&doi=10.1016%2Fj.jii.2024.100612&partnerID=40&md5=15940b2de5b14cdd1e18b79bf45902e5},
	abstract = {Sourcing and identification of new manufacturing partners is crucial for manufacturing system integrators to enhance agility and reduce risk through supply chain diversification in the global economy. The advent of advanced large language models has captured significant interest, due to their ability to generate comprehensive and articulate responses across a wide range of knowledge domains. However, the system often falls short in accuracy and completeness when responding to domain-specific inquiries, particularly in areas like manufacturing service discovery. This research explores the potential of leveraging Knowledge Graphs in conjunction with ChatGPT to streamline the process for prospective clients in identifying small manufacturing enterprises. In this study, we propose a method that integrates bottom-up ontology with advanced machine learning models to develop a Manufacturing Service Knowledge Graph from an array of structured and unstructured data sources, including the digital footprints of small-scale manufacturers throughout North America. The Knowledge Graph and the learned graph embedding vectors are leveraged to tackle intricate queries within the digital supply chain network, responding with enhanced reliability and greater interpretability. The approach highlighted is scalable to millions of entities that can be distributed to form a global Manufacturing Service Knowledge Network Graph that can potentially interconnect multiple types of Knowledge Graphs that span industry sectors, geopolitical boundaries, and business domains. The dataset developed for this study, now publicly accessible, encompasses more than 13,000 manufacturers’ weblinks, manufacturing services, certifications, and location entity types. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Digital Supply Chain; Knowledge Graph; Manufacturing Service Discovery; Industrial Research; Knowledge Management; Supply Chains; Chatgpt; Digital Supply Chain; Global Economies; Knowledge Domains; Knowledge Graphs; Language Model; Manufacturing Service; Manufacturing Service Discovery; Service Discovery; System Integrators; Knowledge Graph},
	keywords = {Industrial research; Knowledge management; Supply chains; ChatGPT; Digital supply chain; Global economies; Knowledge domains; Knowledge graphs; Language model; Manufacturing service; Manufacturing service discovery; Service discovery; System integrators; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Malakhov2024,
	author = {Malakhov, K. S.},
	title = {Innovative Hybrid Cloud Solutions for Physical Medicine and Telerehabilitation Research},
	year = {2024},
	journal = {International Journal of Telerehabilitation},
	volume = {16},
	number = {1},
	pages = {},
	doi = {10.5195/ijt.2024.6635},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197791828&doi=10.5195%2Fijt.2024.6635&partnerID=40&md5=c862077970655656856e98c5397f7584},
	abstract = {Purpose: The primary objective of this study was to develop and implement a Hybrid Cloud Environment for Telerehabilitation (HCET) to enhance patient care and research in the Physical Medicine and Rehabilitation (PM&R) domain. This environment aims to integrate advanced information and communication technologies to support both traditional in-person therapy and digital health solutions. Background: Telerehabilitation is emerging as a core component of modern healthcare, especially within the PM&R field. By applying digital health technologies, telerehabilitation provides continuous, comprehensive support for patient rehabilitation, bridging the gap between traditional therapy, and remote healthcare delivery. This study focuses on the design, and implementation of a hybrid HCET system tailored for the PM&R domain. Methods: The study involved the development of a comprehensive architectural and structural organization for the HCET, including a three-layer model (infrastructure, platform, service layers). Core components of the HCET were designed and implemented, such as the Hospital Information System (HIS) for PM&R, the MedRehabBot system, and the MedLocalGPT project. These components were integrated using advanced technologies like large language models (LLMs), word embeddings, and ontology-related approaches, along with APIs for enhanced functionality and interaction. Findings: The HCET system was successfully implemented and is operational, providing a robust platform for telerehabilitation. Key features include the MVP of the HIS for PM&R, supporting patient profile management, and rehabilitation goal tracking; the MedRehabBot and WhiteBookBot systems; and the MedLocalGPT project, which offers sophisticated querying capabilities, and access to extensive domain-specific knowledge. The system supports both Ukrainian and English languages, ensuring broad accessibility and usability. Interpretation: The practical implementation, and operation of the HCET system demonstrate its potential to transform telerehabilitation within the PM&R domain. By integrating advanced technologies, and providing comprehensive digital health solutions, the HCET enhances patient care, supports ongoing rehabilitation, and facilitates advanced research. Future work will focus on optimizing services and expanding language support to further improve the system's functionality and impact. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cloud Computing; Digital Health; Hybrid Cloud Environment; Medlocalgpt; Medrehabbot; Telerehabilitation},
	type = {Note},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Carta2024467,
	author = {Carta, Salvatore Mario and Giuliani, Alessandro and Manca, Marco Manolo and Piano, Leonardo and Tiddia, Sandro Gabriele},
	title = {Towards Zero-shot Knowledge Graph building: Automated Schema Inference},
	year = {2024},
	pages = {467 - 473},
	doi = {10.1145/3631700.3665234},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198932122&doi=10.1145%2F3631700.3665234&partnerID=40&md5=b0ea6cd716a53c10035c34378d6515d1},
	abstract = {In the current Digital Transformation scenario, Knowledge Graphs are essential for comprehending, representing, and exploiting complex information in a structured form. The main paradigm in automatically generating proper Knowledge Graphs relies on predefined schemas or ontologies. Such schemas are typically manually constructed, requiring an intensive human effort, and are often sensitive to information loss due to negligence, incomplete analysis, or human subjectivity or inclination. Limiting human bias and the resulting information loss in creating proper Knowledge Graphs is paramount, particularly for user modeling in various sectors, such as education or healthcare. To this end, we propose a novel approach to automatically generating a proper entity schema. The devised methodology combines the language understanding capabilities of LLM with classical machine learning methods such as clustering to properly build an entity schema from a set of documents. This solution eliminates the need for human intervention and fosters a more efficient and comprehensive knowledge representation. The assessment of our proposal concerns adopting a state-of-the-art entity extraction model (UniNER) to estimate the relevance of the extracted entities based on the generated schema. Results confirm the potential of our approach, as we observed a negligible difference between the topic similarity score obtained with the ground truth and with the automatically generated schema (less than 1% on average on three different datasets). Such an outcome confirms that the proposed approach may be valuable in automatically creating an entity schema from a set of documents. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Named Entity Recognition; Ontology Learning; Learning Systems; Ontology; Semantics; User Profile; Zero-shot Learning; 'current; Complex Information; Digital Transformation; Information Loss; Knowledge Graphs; Language Model; Large Language Model; Named Entity Recognition; Ontology Learning; Schema Inference; Knowledge Graph},
	keywords = {Learning systems; Ontology; Semantics; User profile; Zero-shot learning; 'current; Complex information; Digital transformation; Information loss; Knowledge graphs; Language model; Large language model; Named entity recognition; Ontology learning; Schema inference; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Kwon2024488,
	author = {Kwon, Jason J. and Pan, Joshua and Gonzalez, Guadalupe and Hahn, William Chun and Žitnik, Marinka},
	title = {On knowing a gene: A distributional hypothesis of gene function},
	year = {2024},
	journal = {Cell Systems},
	volume = {15},
	number = {6},
	pages = {488 - 496},
	doi = {10.1016/j.cels.2024.04.008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195872321&doi=10.1016%2Fj.cels.2024.04.008&partnerID=40&md5=ef6d35fb01bf0c21f0518ac39a78e4f1},
	abstract = {As words can have multiple meanings that depend on sentence context, genes can have various functions that depend on the surrounding biological system. This pleiotropic nature of gene function is limited by ontologies, which annotate gene functions without considering biological contexts. We contend that the gene function problem in genetics may be informed by recent technological leaps in natural language processing, in which representations of word semantics can be automatically learned from diverse language contexts. In contrast to efforts to model semantics as “is-a” relationships in the 1990s, modern distributional semantics represents words as vectors in a learned semantic space and fuels current advances in transformer-based models such as large language models and generative pre-trained transformers. A similar shift in thinking of gene functions as distributions over cellular contexts may enable a similar breakthrough in data-driven learning from large biological datasets to inform gene function. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Distributed Representations; Gene Function; Large Language Models; Lexical Semantics; Machine Learning; Transformers; Word Embeddings; Automation; Computer Model; Data Availability; Gene Activity; Gene Expression; Gene Function; Gene Mapping; Generative Pretrained Transformer; Genetic Analysis; Human; Hypothesis; Language Processing; Linguistics; Machine Learning; Molecular Biology; Natural Language Processing; Pattern Recognition; Review; Semantics; Word Processing; Word Recognition; Writing; Animal; Bioinformatics; Gene; Gene Ontology; Genetics; Procedures; Animals; Computational Biology; Gene Ontology; Genes; Humans; Natural Language Processing; Semantics},
	keywords = {automation; computer model; data availability; gene activity; gene expression; gene function; gene mapping; generative pretrained transformer; genetic analysis; human; hypothesis; language processing; linguistics; machine learning; molecular biology; natural language processing; pattern recognition; Review; semantics; word processing; word recognition; writing; animal; bioinformatics; gene; gene ontology; genetics; procedures; Animals; Computational Biology; Gene Ontology; Genes; Humans; Natural Language Processing; Semantics},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Maratsi2024165,
	author = {Maratsi, Maria Ioanna and Ahmed, Umair and Alexopoulos, Charalampos and Charalabidis, Yannis K. and Polini, Andrea},
	title = {Towards Cross-Domain Linking of Data: A Semantic Mapping of Cultural Heritage Ontologies},
	year = {2024},
	pages = {165 - 176},
	doi = {10.1145/3657054.3657077},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195318383&doi=10.1145%2F3657054.3657077&partnerID=40&md5=de011e648428e0e55f711167805496dd},
	abstract = {The Linked Open Vocabularies (LOV) registry, designed with the Linked Data principles at core, provides an environment suitable for research which targets domain-specific, but also potentially reusable, information representation. The main purpose of this study is to follow the recommendations pertaining to the utilisation of LOV as a basis for experimentation in order to examine how information within the Cultural Heritage (CH) domain can be improved in terms of reusability and interoperability. The present lack of cross-domain knowledge transfer forms the motivation behind this study, with the aim of facilitating the transition from conventional, domain-specific knowledge representation to reusable and semantically interoperable information. The methodology of this study involves the manual semantic mapping of elements from 12 vocabularies in the LOV registry, reinforced by a small-scale experiment using contemporary large language models (LLMs), particularly GPT, for a preliminary assessment of the mapping process. The findings revealed several key aspects to consider regarding the alignment of semantically adjacent vocabulary elements in the CH domain and beyond, emphasising the potential unveiled by linking domain-focused schemata to standardised, established ones while preserving the conceptual hierarchies inherent to each individual knowledge domain. The contribution of this research pertains to the vision of linking data across different domains by initiating the alignment among representation schemata in CH, with the ultimate aim to expand beyond the boundaries of the in-word knowledge domain, while employing combinatory methodological approaches of technological means and human expertise to facilitate this process. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Interoperability; Knowledge Management; Knowledge Representation; Linked Data; Mapping; Natural Language Processing Systems; Reusability; Semantics; Cross-domain; Cultural Heritages; Domain Knowledge; Domain Specific; Information Representation; Knowledge Domains; Linked Data Principles; Ontology's; Semantics Mappings; Target Domain; Domain Knowledge},
	keywords = {Computational linguistics; Interoperability; Knowledge management; Knowledge representation; Linked data; Mapping; Natural language processing systems; Reusability; Semantics; Cross-domain; Cultural heritages; Domain knowledge; Domain specific; Information representation; Knowledge domains; Linked Data principles; Ontology's; Semantics mappings; Target domain; Domain Knowledge},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Sequeda2024,
	author = {Sequeda, Juan Federico and Allemang, Dean and Jacob, Bryon},
	title = {A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases},
	year = {2024},
	pages = {},
	doi = {10.1145/3661304.3661901},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198723406&doi=10.1145%2F3661304.3661901&partnerID=40&md5=fb1d226f08b66fda268e3edb29ddbc5a},
	abstract = {Enterprise applications of Large Language Models (LLMs) hold promise for question answering on enterprise SQL databases. However, the extent to which LLMs can accurately respond to enterprise questions in such databases remains unclear, given the absence of suitable Text-to-SQL benchmarks tailored to enterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to enhance LLM-based question answering by providing business context is not well understood. This study aims to evaluate the accuracy of LLM-powered question answering systems in the context of enterprise questions and SQL databases, while also exploring the role of knowledge graphs in improving accuracy. To achieve this, we introduce a benchmark comprising an enterprise SQL schema in the insurance domain, a range of enterprise queries encompassing reporting to metrics, and a contextual layer incorporating an ontology and mappings that define a knowledge graph. Our primary finding reveals that question answering using GPT-4, with zero-shot prompts directly on SQL databases, achieves an accuracy of 16%. Notably, this accuracy increases to 54% when questions are posed over a Knowledge Graph representation of the enterprise SQL database. Therefore, investing in Knowledge Graph provides higher accuracy for LLM powered question answering systems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Database Systems; Zero-shot Learning; Business Contexts; Enterprise Applications; Knowledge Graphs; Language Model; Model-based Opc; Modeling Accuracy; Question Answering; Question Answering Systems; Question Database; Sql Database; Knowledge Graph},
	keywords = {Computational linguistics; Database systems; Zero-shot learning; Business contexts; Enterprise applications; Knowledge graphs; Language model; Model-based OPC; Modeling accuracy; Question Answering; Question answering systems; Question database; SQL database; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Grigis2024,
	author = {Grigis, Paolo and De Angeli, Antonella},
	title = {Playwriting with Large Language Models: Perceived Features, Interaction Strategies and Outcomes},
	year = {2024},
	pages = {},
	doi = {10.1145/3656650.3656688},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195379233&doi=10.1145%2F3656650.3656688&partnerID=40&md5=ab5af99a132d8ef363a46a1e611686d4},
	abstract = {Large Language Models (LLMs) are sparking debates about creativity, intellectual property, and artistic integrity. This paper focuses on creativity, defined as consensual agreement among domain experts. It presents an inductive analysis of seven semi-structured interviews with professional playwrights who engaged in a longitudinal project with the aim of writing a theatre script using commercial systems. Overall, participants regarded LLMs as unsuitable for playwrighting. However, they enjoyed the experience and identified utility for editorial tasks and brainstorming. A significant obstacle was associated with the politics embedded in LLMs. Not only did these systems avoid a language that could offend sensibilities, but they also refused to engage in taboos and conflicts, which are the core of dramaturgy. Other system features (speed, exploitation, and unpredictability) were sometimes considered conducive and sometimes detrimental to creativity. Participants experienced difficulties and tried to build common ground by trial and error. Often, this strategy evolved into role play: the playwright instructed the LLM to enact characters. The interaction provided hints of inspiration and fostered suspension of disbelief and ontological reflection. However, it often led to technology rejection. Comparing and contrasting our insights with related work, we conclude by opening new directions for research at the boundaries of HCI and AI. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Creative Ai; Creativity; Roleplay; Suspension Of Disbelief; Theatre; Unpredictability; Creative Ai; Creatives; Creativity; Domain Experts; Feature Interactions; Interaction Strategy; Language Model; Role-plays; Suspension Of Disbelief; Unpredictability; Computational Linguistics},
	keywords = {Creative AI; Creatives; Creativity; Domain experts; Feature interactions; Interaction strategy; Language model; Role-plays; Suspension of disbelief; Unpredictability; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Wang202470,
	author = {Wang, Tao},
	title = {The Design and Application of a Chinese Audio-Visual Corpus Based on AI Technology},
	year = {2024},
	journal = {Journal of Technology and Chinese Language Teaching},
	volume = {15},
	number = {1},
	pages = {70 - 81},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198664872&partnerID=40&md5=62078d54b3cec1eaa04d18af48c034f7},
	abstract = {Cultivating an AI-powered language teaching ecosystem has introduced a new model of human-computer collaboration. Many learners are acquiring information and knowledge in a manner that is more contextualized and intelligent. Many educators are adaptive to explore a variety of media resources and AI technologies to guide learners in developing capabilities in problems resolving and knowledge integration. This article describes the design and application of a Chinese Audio-Visual Corpus (CVC) that collects visual language materials from native Chinese speakers in their daily lives to associate textbook content, ontological knowledge, and video materials data in order to provide learners with context-aware teaching resource applications. This AI-based audio-visual corpus offers resourceful and intelligent language teaching methods with the priority of video content retrieval. In addition to serving language teaching, the annotated results from this audio-visual corpus will aid in the training of language models in the interdisciplinary field of Natural Language Processing (NLP) and Computer Vision (CV). It meets the demand for multimodal big data in artificial intelligence for applications such as multimodal discourse analysis, speech act recognition, and sentiment analysis, thereby contributing to the future development of the field of artificial intelligence. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Audio-visual Corpus; International Chinese Language Education; Multimodal; Video Retrieval},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Obuchi202446,
	author = {Obuchi, Kiichi and Funaya, Koichi and Toyama, Kiyohiko and Tanaka, Shukichi and Oñoro-Rubio, Daniel and Renqiang Min, Martin Renqiang and Melvin, Iain},
	title = {LLMs and MI Bring Innovation to Material Development Platforms},
	year = {2024},
	journal = {NEC Technical Journal},
	volume = {17},
	number = {2},
	pages = {46 - 50},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198045683&partnerID=40&md5=d61ec7193698a4188e5cdfbff961a6e7},
	abstract = {In this paper, we introduce efforts to apply large language models (LLMs) to the field of material development. NEC is advancing the development of a material development platform. By applying core technologies corresponding to two material development steps, namely investigation activities (Read paper/patent) and experimental planning (Design Experiment Plan), the platform organizes documents such as papers and reports as well as data such as experimental results and then presents in an interactive way to users. In addition, with techniques that reflect physical and chemical principles into machine learning models, AI can learn even with limited data and accurately predict material properties. Through this platform, we aim to achieve the seamless integration of materials informatics (MI) with a vast body of industry literature and knowledge, thereby bringing innovation to the material development process. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Model For Generating Molecular Structures; Information Extraction Technology; Knowledge Graph; Material Area-specific Llm; Materials Informatics (mi); Ontology; Piml (physics-informed Machine Learning); Machine Learning; Ai Model For Generating Molecular Structure; Information Extraction Technology; Knowledge Graphs; Language Model; Machine-learning; Material Area-specific Large Language Model; Material Informatic; Material Informatics; Ontology's; Physic-informed Machine Learning; Knowledge Graph},
	keywords = {Machine learning; AI model for generating molecular structure; Information extraction technology; Knowledge graphs; Language model; Machine-learning; Material area-specific large language model; Material informatic; Material Informatics; Ontology's; Physic-informed machine learning; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Labala202462,
	author = {Labala, Rajendra K. and Khan, Zeeshan Ahmad and Mondal, Gopinath and Hazra, Subhajit and Banerjee, Bidisha and Chattoraj, Asamanja},
	title = {Transcriptome Analysis on the ALAN-Induced Zebrafish Ovary: Desynchronization of Life Processes and Initiation of Diseases},
	year = {2024},
	journal = {Chronobiology in Medicine},
	volume = {6},
	number = {2},
	pages = {62 - 76},
	doi = {10.33069/cim.2024.0009},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197454469&doi=10.33069%2Fcim.2024.0009&partnerID=40&md5=28fbcfd254dd2be2a5d9595412692a01},
	abstract = {Objective: The effect of artificial light at night (ALAN) on "transcriptome"is prominent owing to its capacity for "desynchronization"of organismal physiology. Light influences the circadian rhythm. This study aims to explore the ALAN-induced ovarian transcriptome of zebrafish for desynchronization of life processes. Methods: Four experimental conditions were set up for female zebrafish: one normal 12-hour light and 12-hour dark (LD) cycle, and three continuous exposures to ALAN for one week (LLW), one month (LLM), and one year (LLY). The whole transcriptome data analysis of the ALAN-exposed samples was then compared with the normal sample using RNA-Seq, followed by exploratory analyses. Results: The analysis revealed two different patterns of expression of genes where LLW and LLM differ with LLY samples in comparison to LD. Compared to LD, downregulation of the predicted hub genes was observed in all treatments; ribosome and oxidative phosphorylation pathways were enriched. LLY vs. LD contrast depicts the enrichment of three more pathways-RNA polymerase, adhesion junction, and signaling. The gene ontology (GO) enrichment portrays more prevalent biological processes in LLW vs. LD and LLM vs. LD than in LLY vs. LD. Contrast-wise disease annotation represents neoplasms as the most prevalent; disease enrichment denotes the major class of neoplasm, carcinoma, coupled with intellectual disability, global developmental delay, and seizures. Conclusion: Our study displayed desynchronization of various genes and pathways leading to the initiation of diseases, for the first time in zebrafish. Even though, this data shows that ALAN is a serious threat, further research is needed to determine the intensity and the duration of ALAN which might cause potential repercussions. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Light At Night; Clock; Desynchronization; Disease; Ovary; Rnaseq},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@ARTICLE{Osman2024,
	author = {Osman, Inès and Pileggi, Salvatore Flavio and Ben Yahia, Sadok},
	title = {Uncertainty in Automated Ontology Matching: Lessons from an Empirical Evaluation},
	year = {2024},
	journal = {Applied Sciences (Switzerland)},
	volume = {14},
	number = {11},
	pages = {},
	doi = {10.3390/app14114679},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195951776&doi=10.3390%2Fapp14114679&partnerID=40&md5=9914bb8d841fc6417e46fb783b045148},
	abstract = {Data integration is considered a classic research field and a pressing need within the information science community. Ontologies play a critical role in such processes by providing well-consolidated support to link and semantically integrate datasets via interoperability. This paper approaches data integration from an application perspective by looking at ontology matching techniques. As the manual matching of different sources of information becomes unrealistic once the system scales up, the automation of the matching process becomes a compelling need. Therefore, we have conducted experiments on actual non-semantically enriched relational data with the support of existing tools (pre-LLM technology) for automatic ontology matching from the scientific community. Even considering a relatively simple case study—i.e., the spatio–temporal alignment of macro indicators—outcomes clearly show significant uncertainty resulting from errors and inaccuracies along the automated matching process. More concretely, this paper aims to test on real-world data a bottom-up knowledge-building approach, discuss the lessons learned from the experimental results of the case study, and draw conclusions about uncertainty and uncertainty management in an automated ontology matching process. While the most common evaluation metrics clearly demonstrate the unreliability of fully automated matching solutions, properly designed semi-supervised approaches seem to be mature for more generalized application. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Integration; Ontology Matching; Uncertainty},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Nédellec2024,
	author = {Nédellec, Claire and Sauvion, Clara and Bossy, Robert and Borovikova, Mariya N. and Deléger, Louise},
	title = {TaeC: A manually annotated text dataset for trait and phenotype extraction and entity linking in wheat breeding literature},
	year = {2024},
	journal = {PLOS ONE},
	volume = {19},
	number = {6 June},
	pages = {},
	doi = {10.1371/journal.pone.0305475},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195849965&doi=10.1371%2Fjournal.pone.0305475&partnerID=40&md5=68b8ef8784c05bd96a37e9684ac08885},
	abstract = {Wheat varieties show a large diversity of traits and phenotypes. Linking them to genetic variability is essential for shorter and more efficient wheat breeding programs. A growing number of plant molecular information networks provide interlinked interoperable data to support the discovery of gene-phenotype interactions. A large body of scientific literature and observational data obtained in-field and under controlled conditions document wheat breeding experiments. The cross-referencing of this complementary information is essential. Text from databases and scientific publications has been identified early on as a relevant source of information. However, the wide variety of terms used to refer to traits and phenotype values makes it difficult to find and cross-reference the textual information, e.g. simple dictionary lookup methods miss relevant terms. Corpora with manually annotated examples are thus needed to evaluate and train textual information extraction methods. While several corpora contain annotations of human and animal phenotypes, no corpus is available for plant traits. This hinders the evaluation of text mining-based crop knowledge graphs (e.g. AgroLD, KnetMiner, WheatIS-FAIDARE) and limits the ability to train machine learning methods and improve the quality of information. The Triticum aestivum trait Corpus is a new gold standard for traits and phenotypes of wheat. It consists of 528 PubMed references that are fully annotated by trait, phenotype, and species. We address the interoperability challenge of crossing sparse assay data and publications by using the Wheat Trait and Phenotype Ontology to normalize trait mentions and the species taxonomy of the National Center for Biotechnology Information to normalize species. The paper describes the construction of the corpus. A study of the performance of state-of-the-art language models for both named entity recognition and linking tasks trained on the corpus shows that it is suitable for training and evaluation. This corpus is currently the most comprehensive manually annotated corpus for natural language processing studies on crop phenotype information from the literature. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Article; Book; Breeding; Data Base; Genetic Variability; Human; Language Model; Machine Learning; Natural Language Processing; Nonhuman; Phenotype; Taxonomy; Triticum Aestivum; Wheat; Data Mining; Genetics; Plant Breeding; Procedures; Data Mining; Phenotype; Plant Breeding; Triticum},
	keywords = {article; book; breeding; data base; genetic variability; human; language model; machine learning; natural language processing; nonhuman; phenotype; taxonomy; Triticum aestivum; wheat; data mining; genetics; plant breeding; procedures; Data Mining; Phenotype; Plant Breeding; Triticum},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access; Green Open Access}
}

@CONFERENCE{Baldazzi2024,
	author = {Baldazzi, Teodoro and Bellomarini, Luigi and Ceri, Stefano and Colombo, Andrea and Gentili, Andrea and Sallinger, Emanuel and Atzeni, Paolo},
	title = {Explaining Enterprise Knowledge Graphs with Large Language Models and Ontological Reasoning},
	year = {2024},
	journal = {OpenAccess Series in Informatics},
	volume = {119},
	pages = {},
	doi = {10.4230/OASIcs.Tannen.2024.1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195697101&doi=10.4230%2FOASIcs.Tannen.2024.1&partnerID=40&md5=259306883a4aeb1be54a6ebd156fa957},
	abstract = {In recent times, the demand for transparency and accountability in AI-driven decisions has intensified, particularly in high-stakes domains like finance and bio-medicine. This focus on the provenance of AI-generated conclusions underscores the need for decision-making processes that are not only transparent but also readily interpretable by humans, to built trust of both users and stakeholders. In this context, the integration of state-of-the-art Large Language Models (LLMs) with logic-oriented Enterprise Knowledge Graphs (EKGs) and the broader scope of Knowledge Representation and Reasoning (KRR) methodologies is currently at the cutting edge of industrial and academic research across numerous data-intensive areas. Indeed, such a synergy is paramount as LLMs bring a layer of adaptability and human-centric understanding that complements the structured insights of EKGs. Conversely, the central role of ontological reasoning is to capture the domain knowledge, accurately handling complex tasks over a given realm of interest, and to infuse the process with transparency and a clear provenance-based explanation of the conclusions drawn, addressing the fundamental challenge of LLMs' inherent opacity and fostering trust and accountability in AI applications. In this paper, we propose a novel neuro-symbolic framework that leverages the underpinnings of provenance in ontological reasoning to enhance state-of-the-art LLMs with domain awareness and explainability, enabling them to act as natural language interfaces to EKGs. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graphs; Language Models; Ontological Reasoning; Provenance; Computational Linguistics; Decision Making; Industrial Research; Interface States; Knowledge Graph; Natural Language Processing Systems; Academic Research; Cutting Edges; Decision-making Process; Knowledge Graphs; Knowledge Representation And Reasoning; Language Model; Model Reasonings; Ontological Reasoning; Provenance; State Of The Art; Transparency},
	keywords = {Computational linguistics; Decision making; Industrial research; Interface states; Knowledge graph; Natural language processing systems; Academic research; Cutting edges; Decision-making process; Knowledge graphs; Knowledge representation and reasoning; Language model; Model reasonings; Ontological reasoning; Provenance; State of the art; Transparency},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {The Provenance of Elegance in Computation - Essays Dedicated to Val Tannen},
	year = {2024},
	journal = {OpenAccess Series in Informatics},
	volume = {119},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195646205&partnerID=40&md5=dc7c92f67df02e0bf3a66f3f46e72dc6},
	abstract = {The proceedings contain 11 papers. The topics discussed include: explaining enterprise knowledge graphs with large language models and ontological reasoning; traversal-invariant characterizations of logarithmic space; semiring provenance in the infinite; annotation and more annotation: some problems posed by (and to) Val Tannen; chasing parallelism in aggregating graph queries; fishing fort: a system for graph analytics with ML prediction and logic deduction; a note on logical pers and reducibility. logical relations strike again!; AutoML for explainable anomaly detection (XAD); on the impact of provenance semiring theory on the design of a provenance-aware database system; and different differences in semirings. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ofer2024,
	author = {Ofer, Dan and Linial, Michal},
	title = {Automated annotation of disease subtypes},
	year = {2024},
	journal = {Journal of Biomedical Informatics},
	volume = {154},
	pages = {},
	doi = {10.1016/j.jbi.2024.104650},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192085433&doi=10.1016%2Fj.jbi.2024.104650&partnerID=40&md5=6dcf3e841213d410237f64e78260e4e0},
	abstract = {Background: Distinguishing diseases into distinct subtypes is crucial for study and effective treatment strategies. The Open Targets Platform (OT) integrates biomedical, genetic, and biochemical datasets to empower disease ontologies, classifications, and potential gene targets. Nevertheless, many disease annotations are incomplete, requiring laborious expert medical input. This challenge is especially pronounced for rare and orphan diseases, where resources are scarce. Methods: We present a machine learning approach to identifying diseases with potential subtypes, using the approximately 23,000 diseases documented in OT. We derive novel features for predicting diseases with subtypes using direct evidence. Machine learning models were applied to analyze feature importance and evaluate predictive performance for discovering both known and novel disease subtypes. Results: Our model achieves a high (89.4%) ROC AUC (Area Under the Receiver Operating Characteristic Curve) in identifying known disease subtypes. We integrated pre-trained deep-learning language models and showed their benefits. Moreover, we identify 515 disease candidates predicted to possess previously unannotated subtypes. Conclusions: Our models can partition diseases into distinct subtypes. This methodology enables a robust, scalable approach for improving knowledge-based annotations and a comprehensive assessment of disease ontology tiers. Our candidates are attractive targets for further study and personalized medicine, potentially aiding in the unveiling of new therapeutic indications for sought-after targets. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Disease Ontology; Disease Subtypes; Explainability; Machine Learning; Medical Language Models; Ontology Completion; Open Targets; Orphanet; Personalized Medicine; Classification (of Information); Computational Linguistics; Deep Learning; Knowledge Based Systems; Learning Systems; Disease Ontology; Disease Subtype; Explainability; Language Model; Machine-learning; Medical Language Model; Ontology Completion; Ontology's; Open Target; Orphanet; Personalized Medicines; Ontology; Adolescent; Adult; Area Under The Curve; Article; Controlled Study; Deep Learning; Disease Ontology; Human; Language Model; Machine Learning; Male; Ontology; Personalized Medicine; Receiver Operating Characteristic; Therapy; Algorithm; Bioinformatics; Classification; Diseases; Procedures; Algorithms; Computational Biology; Deep Learning; Disease; Humans; Machine Learning; Roc Curve},
	keywords = {Classification (of information); Computational linguistics; Deep learning; Knowledge based systems; Learning systems; Disease ontology; Disease subtype; Explainability; Language model; Machine-learning; Medical language model; Ontology completion; Ontology's; Open target; Orphanet; Personalized medicines; Ontology; adolescent; adult; area under the curve; article; controlled study; deep learning; disease ontology; human; language model; machine learning; male; ontology; personalized medicine; receiver operating characteristic; therapy; algorithm; bioinformatics; classification; diseases; procedures; Algorithms; Computational Biology; Deep Learning; Disease; Humans; Machine Learning; ROC Curve},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Strader20244886,
	author = {Strader, Jared and Hughes, Nathan and Chen, William and Speranzon, Alberto and Carlone, Luca},
	title = {Indoor and Outdoor 3D Scene Graph Generation Via Language-Enabled Spatial Ontologies},
	year = {2024},
	journal = {IEEE Robotics and Automation Letters},
	volume = {9},
	number = {6},
	pages = {4886 - 4893},
	doi = {10.1109/LRA.2024.3384084},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189615964&doi=10.1109%2FLRA.2024.3384084&partnerID=40&md5=86b59d43a6f2399478a764bbb05e2117},
	abstract = {This letter proposes an approach to build 3D scene graphs in arbitrary indoor and outdoor environments. Such extension is challenging; the hierarchy of concepts that describe an outdoor environment is more complex than for indoors, and manually defining such hierarchy is time-consuming and does not scale. Furthermore, the lack of training data prevents the straightforward application of learning-based tools used in indoor settings. To address these challenges, we propose two novel extensions. First, we develop methods to build a spatial ontology defining concepts and relations relevant for indoor and outdoor robot operation. In particular, we use a Large Language Model (LLM) to build such an ontology, thus largely reducing the amount of manual effort required. Second, we leverage the spatial ontology for 3D scene graph construction using Logic Tensor Networks (LTN) to add logical rules, or axioms (e.g., 'a beach contains sand'), which provide additional supervisory signals at training time thus reducing the need for labelled data, providing better predictions, and even allowing predicting concepts unseen at training time. We test our approach in a variety of datasets, including indoor, rural, and coastal environments, and show that it leads to a significant increase in the quality of the 3D scene graph generation with sparsely annotated data. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {3d Scene Graphs; Ai-based Methods; Semantic Scene Understanding; Spatial Ontologies; Modeling Languages; Ontology; Personnel Training; Three Dimensional Computer Graphics; Three Dimensional Displays; 3d Scene Graph; 3d Scenes; Ai-based Method; Ontology's; Scene Understanding; Scene-graphs; Semantic Scene Understanding; Solid Modelling; Spatial Ontologies; Three-dimensional Display; Training Data; Semantics},
	keywords = {Modeling languages; Ontology; Personnel training; Three dimensional computer graphics; Three dimensional displays; 3d scene graph; 3D scenes; AI-based method; Ontology's; Scene understanding; Scene-graphs; Semantic scene understanding; Solid modelling; Spatial ontologies; Three-dimensional display; Training data; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Bombieri202469,
	author = {Bombieri, Marco and Rospocher, Marco and Ponzetto, Simone Paolo and Fiorini, Paolo},
	title = {Surgicberta: a pre-trained language model for procedural surgical language},
	year = {2024},
	journal = {International Journal of Data Science and Analytics},
	volume = {18},
	number = {1},
	pages = {69 - 81},
	doi = {10.1007/s41060-023-00433-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168115302&doi=10.1007%2Fs41060-023-00433-5&partnerID=40&md5=457773ec567d107b220397c8f531cd07},
	abstract = {Pre-trained language models are now ubiquitous in natural language processing, being successfully applied for many different tasks and in several real-world applications. However, even though there is a wealth of high-quality written materials on surgery, and the scientific community has shown a growing interest in the application of natural language processing techniques in surgery, a pre-trained language model specific to the surgical domain is still missing. The creation and public release of such a model would serve numerous useful clinical applications. For example, it could enhance existing surgical knowledge bases employed for task automation, or assist medical students in summarizing complex surgical descriptions. For this reason, in this paper, we introduce SurgicBERTa, a pre-trained language model specific for the English surgical language, i.e., the language used in the surgical domain. SurgicBERTa has been obtained from RoBERTa through continued pre-training with the Masked language modeling objective on 300 k sentences taken from English surgical books and papers, for a total of 7 million words. By publicly releasing SurgicBERTa, we make available a resource built from the content collected in many high-quality surgical books, online textual resources, and academic papers. We performed several assessments in order to evaluate SurgicBERTa, comparing it with the general domain RoBERTa. First, we intrinsically assessed the model in terms of perplexity, accuracy, and evaluation loss resulting from the continual training according to the masked language modeling task. Then, we extrinsically evaluated SurgicBERTa on several downstream tasks, namely (i) procedural sentence detection, (ii) procedural knowledge extraction, (iii) ontological information discovery, and (iv) surgical terminology acquisition. Finally, we conducted some qualitative analysis on SurgicBERTa, showing that it contains a lot of surgical knowledge that could be useful to enrich existing state-of-the-art surgical knowledge bases or to extract surgical knowledge. All the assessments show that SurgicBERTa better deals with surgical language than a general-purpose pre-trained language model such as RoBERTa, and therefore can be effectively exploited in many computer-assisted applications in the surgical domain. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Language Models; Medicine; Natural Language Processing; Transformers; Computational Linguistics; Data Mining; Natural Language Processing Systems; Surgery; High Quality; Language Model; Language Processing; Language Processing Techniques; Natural Language Processing; Natural Languages; Real-world; Scientific Community; Still Missing; Transformer; Modeling Languages},
	keywords = {Computational linguistics; Data mining; Natural language processing systems; Surgery; High quality; Language model; Language processing; Language processing techniques; Natural language processing; Natural languages; Real-world; Scientific community; Still missing; Transformer; Modeling languages},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Portisch2024845,
	author = {Portisch, Jan Philipp and Paulheim, Heiko},
	title = {The RDF2vec family of knowledge graph embedding methods},
	year = {2024},
	journal = {Semantic Web},
	volume = {15},
	number = {3},
	pages = {845 - 876},
	doi = {10.3233/SW-233514},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193700037&doi=10.3233%2FSW-233514&partnerID=40&md5=619955408f0b22952f000a6b1386865d},
	abstract = {Knowledge graph embeddings represent a group of machine learning techniques which project entities and relations of a knowledge graph to continuous vector spaces. RDF2vec is a scalable embedding approach rooted in the combination of random walks with a language model. It has been successfully used in various applications. Recently, multiple variants to the RDF2vec approach have been proposed, introducing variations both on the walk generation and on the language modeling side. The combination of those different approaches has lead to an increasing family of RDF2vec variants. In this paper, we evaluate a total of twelve RDF2vec variants on a comprehensive set of benchmark models, and compare them to seven existing knowledge graph embedding methods from the family of link prediction approaches. Besides the established GEval benchmark introducing various downstream machine learning tasks on the DBpedia knowledge graph, we also use the new DLCC (Description Logic Class Constructors) benchmark consisting of two gold standards, one based on DBpedia, and one based on synthetically generated graphs. The latter allows for analyzing which ontological patterns in a knowledge graph can actually be learned by different embedding. With this evaluation, we observe that certain tailored RDF2vec variants can lead to improved performance on different downstream tasks, given the nature of the underlying problem, and that they, in particular, have a different behavior in modeling similarity and relatedness. The findings can be used to provide guidance in selecting a particular RDF2vec method for a given task. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Embedding Evaluation; Knowledge Graph Embedding; Rdf2vec; Representation Learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Hsu2024537,
	author = {Hsu, Chiyang and Cox, Kyle and Xu, Jiawei and Tan, Zhen and Zhai, Tianhua and Hu, Mengzhou and Pratt, Dexter and Chen, Tianlong and Hu, Ziniu and Ding, Ying},
	title = {Thought Graph: Generating Thought Process for Biological Reasoning},
	year = {2024},
	pages = {537 - 540},
	doi = {10.1145/3589335.3651572},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194487248&doi=10.1145%2F3589335.3651572&partnerID=40&md5=a2305146a3f509e2f340aea97b3985fc},
	abstract = {We present the Thought Graph as a novel framework to support complex reasoning and use gene set analysis as an example to uncover semantic relationships between biological processes. Our framework stands out for its ability to provide a deeper understanding of gene sets, significantly surpassing GSEA by 40.28% and LLM baselines by 5.38% based on cosine similarity to human annotations. Our analysis further provides insights into future directions of biological processes naming, and implications for bioinformatics and precision medicine. Here’s our Github Code. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bioinformatics; Gene Ontology; Large Language Model; Natural Language Processing; Semantic Web Biological Process; Bioinformatics; Gene Ontology; Genes; Natural Language Processing Systems; Biological Process; Gene Ontology; Gene Sets; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Semantic Web Biological Process; Semantic-web; Semantic Web},
	keywords = {Bioinformatics; Gene Ontology; Genes; Natural language processing systems; Biological process; Gene ontology; Gene sets; Language model; Language processing; Large language model; Natural language processing; Natural languages; Semantic web biological process; Semantic-Web; Semantic Web},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Shi20242159,
	author = {Shi, Jingchuan and Dong, Hang and Chen, Jiaoyan and Wu, Zhe and Horrocks, Ian},
	title = {Taxonomy Completion via Implicit Concept Insertion},
	year = {2024},
	pages = {2159 - 2169},
	doi = {10.1145/3589334.3645584},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194098825&doi=10.1145%2F3589334.3645584&partnerID=40&md5=da488ad8993cfe4959da2345dcb10219},
	abstract = {\beginabstract High quality taxonomies play a critical role in various domains such as e-commerce, web search and ontology engineering. While there has been extensive work on expanding taxonomies from externally mined data, there has been less attention paid to enriching taxonomies by exploiting existing concepts and structure within the taxonomy. In this work, we show the usefulness of this kind of enrichment, and explore its viability with a new taxonomy completion system ICON (I mplicit CON cept Insertion). ICON generates new concepts by identifying implicit concepts based on the existing concept structure, generating names for such concepts and inserting them in appropriate positions within the taxonomy. ICON integrates techniques from entity retrieval, text summary, and subsumption prediction; this modular architecture offers high flexibility while achieving state-of-the-art performance. We have evaluated ICON on two e-commerce taxonomies, and the results show that it offers significant advantages over strong baselines including recent taxonomy completion models and the large language model, ChatGPT. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology Engineering; Pre-trained Language Model; Taxonomy Completion; Taxonomy Enrichment; Text Summarisation; Computational Linguistics; Electronic Commerce; Ontology; E- Commerces; High Quality; Language Model; Ontology Engineering; Pre-trained Language Model; Taxonomy Completion; Taxonomy Enrichment; Text Summarisation; Web Ontology; Web Searches; Taxonomies},
	keywords = {Computational linguistics; Electronic commerce; Ontology; E- commerces; High quality; Language model; Ontology engineering; Pre-trained language model; Taxonomy completion; Taxonomy enrichment; Text Summarisation; Web ontology; Web searches; Taxonomies},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Chen2024,
	author = {Chen, Liuqing and Jiang, Zhaojun and Xia, Duowei and Cai, Zebin and Sun, Lingyun and Childs, Peter R.N. and Zuo, Haoyu},
	title = {BIDTrainer: An LLMs-driven Education Tool for Enhancing the Understanding and Reasoning in Bio-inspired Design},
	year = {2024},
	journal = {Conference on Human Factors in Computing Systems - Proceedings},
	pages = {},
	doi = {10.1145/3613904.3642887},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194885520&doi=10.1145%2F3613904.3642887&partnerID=40&md5=4e4d9029f454344be21e1d843bf140de},
	abstract = {Bio-inspired design (BID) fosters innovations in engineering. Learning BID is crucial for developing multidisciplinary innovation skills of designers and engineers. Current BID education aims to enhance learners' understanding and analogical reasoning skills. However, it often heavily relies on the teachers' expertise. When learners pursue independent learning using some educational tools, they face challenges in understanding and reasoning practice within this multidisciplinary field. Additionally, evaluating their learning outcomes comprehensively becomes problematic. Addressing these challenges, we introduce a LLMs-driven BID education method based on a structured ontology and three strategies: enhancing understanding through LLMs-enpowered "learning by asking", assisting reasoning by providing hints and feedback, and assessing learning outcomes through benchmarking against existing BID cases. Implementing the method, we developed BIDTrainer, a BID education tool. User studies indicate that learners using BIDTrainer understood BID knowledge better, reason faster with higher interactivity than the baseline, and BIDTrainer assessed the learning outcomes consistent with experts. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Analogy Training; Bio-inspired Design; Design Education; Design Evaluation; Biomimetics; 'current; Analogical Reasoning; Analogy Training; Bio-inspired Designs; Design Education; Design Evaluation; Education Tool; Independent Learning; Learning Outcome; Teachers'; Learning Systems},
	keywords = {Biomimetics; 'current; Analogical reasoning; Analogy training; Bio-inspired designs; Design Education; Design evaluation; Education tool; Independent learning; Learning outcome; Teachers'; Learning systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Bronze Open Access}
}

@ARTICLE{Fenggui2024353,
	author = {Fenggui, Niu and Bei, Zhang and Chen, Shi},
	title = {Review and perspective of Earth Science Knowledge Graph in Big Data Era; 大数据时代的地球科学知识图谱研究现状与展望},
	year = {2024},
	journal = {Acta Seismologica Sinica},
	volume = {46},
	number = {3},
	pages = {353 - 376},
	doi = {10.11939/jass.20230157},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196662469&doi=10.11939%2Fjass.20230157&partnerID=40&md5=82ee8b90bb5da14ae8be898df5f465f9},
	abstract = {Earth Science is a discipline that heavily relies on data,yet it is not fully harnessing the advantages of Earth data with existing technological means though covers many subject areas,Knowledge Graphs (KGs) is widely recognized as an effective approach to fully harness and utilize the extensive data in this field. Earth Science Knowledge Graphs can integrate geoscience knowledge,enhance research efficiency,and facilitate interdisciplinary collaboration. By analyzing network connections and semantic relationships,they uncover knowledge associations and patterns,andaid researchers in identifying new domains and posing novel research questions. Unlike conventional advancements in large-scale modeling technologies, Knowledge Graph offers precise knowledge that enhances both the intelligence and dependability of generated outcomes from such models. Firstly,this study provides a detailed exposition of Knowledge Graph concepts and construction methods. Knowledge Graphs,as a form of data graph,are designed to collect and convey knowledge from the real world. Their universal expression is in the form of triples,consisting of head entities,tail entities,and the relationships between them. Knowledge Graphs have emerged as a significant approach for organizing structured knowledge and integrating information from multiple data sources in the organizational world. Their architectural framework primarily encompasses four components:source data acquisition,knowledge fusion, knowledge computation,and knowledge application. Source data acquisition stands as the primary step in building Knowledge Graphs,focusing on extracting useful information from various types of data. Knowledge fusion is pivotal in addressing the heterogeneity of different Knowledge Graphs,with the aim of enhancing their quality through integration. Knowledge computation represents the primary output capability of Knowledge Graphs,currently applied in fields such as semantic search,question answering,and visualization analysis. Knowledge Graph construction technology enables the extraction of information from structured,unstructured,and semi-structured data sources,organizing this information into knowledge and presenting it in graphical form. Presently,the construction of Knowledge Graphs in the field of Earth Sciences primarily employs two methods:Top-down and bottom-up approaches,with the overarching principle being the synthesis of both methods while allowing flexibility in their specific sequencing. Secondly,this study offers a comprehensive introduction to the widely applied top-level ontology,the Basic Formal Ontology (BFO) model,in the scientific domain. The paper briefly summarizes existing Knowledge Graph in the geoscience field,emphasizing the GeoCore Ontology and Geoscience ontology (GSO) in the Earth Science domain,highlighting their similarities and differences. BFO,comprising 38 classes,is designed to facilitate information integration,retrieval,and analysis in scientific research. Presently,BFO has been successfully employed in over 350 ontology projects worldwide. The GeoCore Ontology,built upon BFO, serves as a specialized framework to describe the core concepts within the domain of Earth Science,rigorously defining a set of universal geological concepts during its development. Conversely,GSO provides a systematic framework for representing crucial geological science knowledge,encompassing three hierarchical layers:foundational,geological,and detailed modules. GeoCore can be viewed as an intermediary layer within GSO,which can be further expanded,while detailed modules have already been constructed within GSO. Additionally, researchers worldwide employ various methods such as literature mining,domain expert interviews,and data mining techniques to extract Earth Science knowledge from relevant literature, databases,and open data,subsequently to construct Knowledge Graphs. These Knowledge Graphs are found in applications across various domains including geological exploration,natural disaster prediction,and environmental conservation,and are utilized in practical projects such as oil and gas exploration,water resource management,and climate change research. In summary,the application scope of Earth Science Knowledge Graphs is extensive,providing a crucial foundation of data and knowledge for scientific research,decision support,and sustainable development. Finally,the study introduces international Earth Science data science initiatives such as the Deep-time Digital Earth (DDE) project related to constructing Earth Science Knowledge Graph,and the challenges and application prospects for the future development of Earth Science Knowledge Graph,with a focus on seismic science. The DDE aims to connect and coordinate global deep-earth data,promoting the sharing of geoscientific knowledge worldwide and facilitating research on Earth's evolution in a data-driven manner. Apart from the DDE,numerous domestic and international organizations and initiatives are driving the development of Knowledge Graph in Earth Science,such as OneGeology,EarthCube,and LinkedGeoData projects. Despite facing various challenges,Knowledge Graph is gradually overcoming these hurdles with advancements in technology and tools. These challenges are not exclusive to the field of Earth Science but are prevalent across all Knowledge Graph construction endeavors. However,due to the complexity and diversity of Earth Science,Knowledge Graph construction in this field encounters unique difficulties. Nevertheless,there is ample room for the creation and application of Knowledge Graph in Earth Science,with the introduction of Large Language Models (LLMs) bringing forth new opportunities. Earthquake Science,as a crucial branch of Earth Science,encompasses intersections of multiple primary disciplines such as geology,geophysics,and engineering seismology. However,the application of Knowledge Graphs in the field of Earthquake Science still faces significant gaps and urgently requires further research building upon existing models. In conclusion,the future development of Earth Science Knowledge Graphs will be an ongoing process of evolution and refinement,bringing more opportunities and benefits for fields such as Earth Science research,decision-making,and public education through sustained technological innovation and interdisciplinary collaboration. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Earth Science; Geological Ontology; Knowledge Graph; Ontology Construction; Data Acquisition; Data Mining; Data Set; Earth Science; Knowledge; Seismic Data},
	keywords = {data acquisition; data mining; data set; Earth science; knowledge; seismic data},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Zhou2024236,
	author = {Zhou, Wei and Nie, Jun and Zhang, Dafa},
	title = {Comprehensive Analysis of Key Endoplasmic Reticulum Stress-Related Genes and Immune Infiltrates in Stanford Type A Aortic Dissection},
	year = {2024},
	journal = {Anatolian Journal of Cardiology},
	volume = {28},
	number = {5},
	pages = {236 - 244},
	doi = {10.14744/AnatolJCardiol.2024.4251},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191976337&doi=10.14744%2FAnatolJCardiol.2024.4251&partnerID=40&md5=bc48d6683ed9c31f11dc5fc106313e16},
	abstract = {Background: Type A aortic dissection is a fatal disease. However, the role of endoplasmic reticulum stress-related genes (ERSRGs) in type A aortic dissection has not yet been fully clarified. Methods: Differentially expressed genes in the aorta of type A aortic dissection patients were analyzed based on the GSE52093 database. The ERSRGs were downloaded from the GeneCards website. Functional enrichment analysis and protein–protein interaction analysis were performed on the acquired differentially expressed ERSRGs. The mRNA expression of the 5 top key differentially expressed ERSRGs was further explored in GSE153434 and clinical samples. Immune infiltration correlation analysis was performed on the validated key genes. Finally, we constructed regulatory networks of transcription factors, miRNAs, and chemicals. Results: Twelve differentially expressed ERSRGs were identified, of which 8 genes were downregulated and 4 genes were upregulated. GeneMANIA was adopted to analyze these genes and their interacting proteins, and the results showed that the main function was calcium ion transport. Four key genes, ACTC1, CASQ2, SPP1, and REEP1, were validated in GSE153434 and clinical samples. The area under the ROC curve values for ACTC1, CASQ2, SPP1, and REEP1 were 0.92, 0.96, 0.89, and 1.00, respectively. ACTC1 and REEP1 correlated with multiple immune cells. Many transcription factors, microRNAs, and chemicals were identified with the potential to regulate these 4 key genes. Conclusion: In this study, we identified 12 differentially expressed ERSRGs by analyzing the Gene Expression Omnibus database. Four key genes may influence the development of type A aortic dissection by regulating endoplasmic reticulum stress. These results expand our understanding of type A aortic dissection, and the 4 key genes are expected to be diagnostic markers and potential therapeutic targets. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Aortic Dissection; Gene Expression Profiling; Gene Ontology; Calcium Ion; Hiscript Ii Q Rtsuperpmix; R Software V1.03; Calcium Ion; Microrna; Transcription Factor; Actc1; Article; Artificial Intelligence; Casq2; Cd8+ T Lymphocyte; Cell Infiltration; Coronary Artery Bypass Graft; Correlation Analysis; Dendritic Cell; Diagnostic Test Accuracy Study; Differential Gene Expression; Down Regulation; Endoplasmic Reticulum Stress; Functional Enrichment Analysis; Gene; Gene Expression; Gene Ontology; Gse153434; Human; Immunocompetent Cell; Ion Transport; Ischemic Heart Disease; Language Model; Macrophage; Mrna Expression Level; Natural Killer Cell; Protein Protein Interaction; Receiver Operating Characteristic; Reep1; Regulatory T Lymphocyte; Reverse Transcription Polymerase Chain Reaction; Rna Extraction; Spp1; Type A Aortic Dissection; Upregulation},
	keywords = {calcium ion; microRNA; transcription factor; ACTC1; Article; artificial intelligence; CASQ2; CD8+ T lymphocyte; cell infiltration; coronary artery bypass graft; correlation analysis; dendritic cell; diagnostic test accuracy study; differential gene expression; down regulation; endoplasmic reticulum stress; functional enrichment analysis; gene; gene expression; gene ontology; GSE153434; human; immunocompetent cell; ion transport; ischemic heart disease; language model; macrophage; mRNA expression level; natural killer cell; protein protein interaction; receiver operating characteristic; REEP1; regulatory T lymphocyte; reverse transcription polymerase chain reaction; RNA extraction; SPP1; type A aortic dissection; upregulation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Hu20243158,
	author = {Hu, Fan and Zhang, Weihong and Huang, Huazhen and Li, Wang and Li, Yang and Yin, Peng},
	title = {A Transferability-Based Method for Evaluating the Protein Representation Learning},
	year = {2024},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	volume = {28},
	number = {5},
	pages = {3158 - 3166},
	doi = {10.1109/JBHI.2024.3370680},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186999797&doi=10.1109%2FJBHI.2024.3370680&partnerID=40&md5=67be716debc6229a1cf0c9bc6cd01e13},
	abstract = {Self-supervised pre-trained language models have recently risen as a powerful approach in learning protein representations, showing exceptional effectiveness in various biological tasks, such as drug discovery. Amidst the evolving trend in protein language model development, there is an observable shift towards employing large-scale multimodal and multitask models. However, the predominant reliance on empirical assessments using specific benchmark datasets for evaluating these models raises concerns about the comprehensiveness and efficiency of current evaluation methods. Addressing this gap, our study introduces a novel quantitative approach for estimating the performance of transferring multi-task pre-trained protein representations to downstream tasks. This transferability-based method is designed to quantify the similarities in latent space distributions between pre-trained features and those fine-tuned for downstream tasks. It encompasses a broad spectrum, covering multiple domains and a variety of heterogeneous tasks. To validate this method, we constructed a diverse set of protein-specific pre-training tasks. The resulting protein representations were then evaluated across several downstream biological tasks. Our experimental results demonstrate a robust correlation between the transferability scores obtained using our method and the actual transfer performance observed. This significant correlation highlights the potential of our method as a more comprehensive and efficient tool for evaluating protein representation learning. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Optimal Transport; Protein Representation Learning; Transferability; Protein; Proteins; Biological Systems; Computational Linguistics; Information Theory; Job Analysis; Learning Systems; Biological Information Theories; Biological System Modeling; Computational Modelling; Down-stream; Optimal Transport; Predictive Models; Protein Engineering; Protein Representation Learning; Task Analysis; Transferability; Proteins; Accuracy; Algorithm; Amino Acid Sequence; Article; Computer Model; Drug Development; Gene Ontology; Human; Language Model; Learning; Mathematical Model; Natural Language Processing; Probability; Protein Function; Protein Language Model; Protein Representation Learning; Protein Structure; Sequence Homology; Bioinformatics; Machine Learning; Procedures; Protein Database; Protein; Algorithms; Computational Biology; Databases, Protein; Humans; Machine Learning},
	keywords = {Biological systems; Computational linguistics; Information theory; Job analysis; Learning systems; Biological information theories; Biological system modeling; Computational modelling; Down-stream; Optimal transport; Predictive models; Protein engineering; Protein representation learning; Task analysis; Transferability; Proteins; accuracy; algorithm; amino acid sequence; Article; computer model; drug development; gene ontology; human; language model; learning; mathematical model; natural language processing; probability; protein function; protein language model; protein representation learning; protein structure; sequence homology; bioinformatics; machine learning; procedures; protein database; protein; Algorithms; Computational Biology; Databases, Protein; Humans; Machine Learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Fan2024,
	author = {Fan, Zhanling and Chen, Chongcheng},
	title = {CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models},
	year = {2024},
	journal = {Information Processing and Management},
	volume = {61},
	number = {3},
	pages = {},
	doi = {10.1016/j.ipm.2024.103646},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183365319&doi=10.1016%2Fj.ipm.2024.103646&partnerID=40&md5=3199493b7031da63f49a979dabcb71bb},
	abstract = {Tourism knowledge graphs lack cultural content, limiting their usefulness for cultural tourists.This paper presents the development of a cultural perspective-based knowledge graph (CuPe-KG). We evaluated fine-tuning ERNIE 3.0 (FT-ERNIE) and ChatGPT for cultural type recognition to strengthen the relationship between tourism resources and cultures. Our investigation used an annotated cultural tourism resource dataset containing 2,745 items across 16 cultural types. The results showed accuracy scores for FT-ERNIE and ChatGPT of 0.81 and 0.12, respectively, with FT-ERNIE achieving a micro-F1 score of 0.93, a 26 percentage point lead over ChatGPT's score of 0.67. These underscore FT-ERNIE's superior performance (the shortcoming is the need to annotate data) while highlighting ChatGPT's limitations because of insufficient Chinese training data and lower identification accuracy in professional knowledge. A novel ontology was designed to facilitate the construction of CuPe-KG, including elements such as cultural types, historical figures, events, and intangible cultural heritage. CuPe-KG effectively addresses cultural tourism visitors’ information retrieval needs. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Cultural Tourism; Cultural Type; Knowledge Graph; Pretrained Language Models; Travel Intelligence; Computational Linguistics; Tourism; Chatgpt; Cultural Content; Cultural Tourism; Cultural Type; Fine Tuning; Graph Construction; Knowledge Graphs; Language Model; Pretrained Language Model; Travel Intelligence; Knowledge Graph},
	keywords = {Computational linguistics; Tourism; ChatGPT; Cultural content; Cultural tourism; Cultural type; Fine tuning; Graph construction; Knowledge graphs; Language model; Pretrained language model; Travel intelligence; Knowledge graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21}
}

@CONFERENCE{Nir2024,
	author = {Nir, Oron and Vidra, Idan Dov and Neeman, Avi and Kinarti, Barak and Shamir, Ariel},
	title = {VCR: Video representation for Contextual Retrieval},
	year = {2024},
	pages = {},
	doi = {10.1145/3661725.3661766},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197313901&doi=10.1145%2F3661725.3661766&partnerID=40&md5=659c279a9c9e56081f56ee9d2af46aa0},
	abstract = {Streamlining content discovery in media archives requires advanced data representations and effective visualization techniques for clear communication of video topics to users. The proposed system addresses the challenge of efficiently navigating large video collections by exploiting a fusion of visual, audio, and textual features to accurately index and categorize video content through a text-based method. Additionally, semantic embeddings are employed to provide contextually relevant information and recommendations to users, resulting in an intuitive and engaging exploratory experience over our topics ontology map using LLMs (GitHub). © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Archive Exploration; Media Search; Video Representation; Data Visualization; Video Recording; Archive Exploration; Audio Features; Contents Discovery; Data Representations; Media Archives; Media Searches; Video Collections; Video Representations; Visual Feature; Visualization Technique; Semantics},
	keywords = {Data visualization; Video recording; Archive exploration; Audio features; Contents discovery; Data representations; Media archives; Media searches; Video collections; Video representations; Visual feature; Visualization technique; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{2024,
	title = {Sustainable Production through Advanced Manufacturing, Intelligent Automation and Work Integrated Learning - Proceedings of the 11th Swedish Production Symposium, SPS2024},
	year = {2024},
	journal = {Advances in Transdisciplinary Engineering},
	volume = {52},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191318908&partnerID=40&md5=4bce1e86d2198af59b5a3ee703261d09},
	abstract = {The proceedings contain 59 papers. The topics discussed include: towards in-line measurements of sawn wood surfaces; combining ontology and large language models to identify recurring machine failures in free-text fields; study of pulsed laser beam welding of nickel-based superalloy G27; strategies for effective chip management in machining of ductile cast iron; drop detachment under intense laser irradiation; relationship between tool temperature distribution and stagnation point behavior for different process factors in machining operations; process parameter impact on axial plasma sprayed ytterbium disilicate coatings for environment barrier coating applications; effect of laser power on the deposition of alloy 718 powder on alumina substrate using laser directed energy deposition: a single-track study; and ultrasonic signal response from internal manufactured defects in PBF-LB manufactured superalloys. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Bengtsson202427,
	author = {Bengtsson, Marcus and D’Cruze, Ricky Stanley and Ahmed, Mobyen Uddin and Sakao, Tomohiko and Funk, Peter J. and Sohlberg, Rickard},
	title = {Combining Ontology and Large Language Models to Identify Recurring Machine Failures in Free-Text Fields},
	year = {2024},
	journal = {Advances in Transdisciplinary Engineering},
	volume = {52},
	pages = {27 - 38},
	doi = {10.3233/ATDE240151},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191305248&doi=10.3233%2FATDE240151&partnerID=40&md5=da0ede59df41ae4bd71be8b685a65793},
	abstract = {Companies must enhance total maintenance effectiveness to stay competitive, focusing on both digitalization and basic maintenance procedures. Digitalization offers technologies for data-driven decision-making, but many maintenance decisions still lack a factual basis. Prioritizing efficiency and effectiveness require analyzing equipment history, facilitated by using Computerized Maintenance Management Systems (CMMS). However, CMMS data often contains unstructured free-text, leading to manual analysis, which is resource-intensive and reactive, focusing on short time periods and specific equipment. Two approaches are available to solve the issue: minimizing free-text entries or using advanced methods for processing them. Free-text allows detailed descriptions but may lack completeness, while structured reporting aids automated analysis but may limit fault description richness. As knowledge and experience are vital assets for companies this research uses a hybrid approach by combining Natural Language Processing with domain specific ontology and Large Language Models to extract information from free-text entries, enabling the possibility of real-time analysis e.g., identifying recurring failure and knowledge sharing across global sites. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Experience Reuse; Industrial Maintenance; Large Language Models; Natural Language Processing; Computational Linguistics; Decision Making; Failure (mechanical); Natural Language Processing Systems; Ontology; Computerized Maintenance Management System; Experience Reuse; Free Texts; Industrial Maintenance; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Text Entry; Maintenance},
	keywords = {Computational linguistics; Decision making; Failure (mechanical); Natural language processing systems; Ontology; Computerized maintenance management system; Experience reuse; Free texts; Industrial maintenance; Language model; Language processing; Large language model; Natural language processing; Natural languages; Text entry; Maintenance},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Alharbi20241650,
	author = {Alharbi, Reham and Tamma, Valentina A.M. and Grasso, Floriana and Payne, Terry R.},
	title = {An Experiment in Retrofitting Competency Questions for Existing Ontologies},
	year = {2024},
	pages = {1650 - 1658},
	doi = {10.1145/3605098.3636053},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195594103&doi=10.1145%2F3605098.3636053&partnerID=40&md5=6657a470ae0b71a26ceae73b3a800de1},
	abstract = {Competency Questions (CQs) are a form of ontology functional requirements expressed as natural language questions. Inspecting CQs together with the axioms in an ontology provides critical insights into the intended scope and applicability of the ontology. CQs also underpin a number of tasks in the development of ontologies e.g. ontology reuse, ontology testing, requirement specification, and the definition of patterns that implement such requirements. Although CQs are integral to the majority of ontology engineering methodologies, the practice of publishing CQs alongside the ontological artefacts is not widely observed by the community.In this context, we present an experiment in retrofitting CQs from existing ontologies. We propose RETROFIT-CQs, a method to extract candidate CQs directly from ontologies using Generative AI. In the paper we present the pipeline that facilitates the extraction of CQs by leveraging Large Language Models (LLMs) and we discuss its application to a number of existing ontologies. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Competency Questions; Large Language Models; Ontology Engineering; Computational Linguistics; Natural Language Processing Systems; Retrofitting; Competency Question; Functional Requirement; Language Model; Large Language Model; Natural Language Questions; Ontology Engineering; Ontology Reuse; Ontology's; Requirements Specifications; Testing Requirements; Ontology},
	keywords = {Computational linguistics; Natural language processing systems; Retrofitting; Competency question; Functional requirement; Language model; Large language model; Natural language questions; Ontology engineering; Ontology reuse; Ontology's; Requirements specifications; Testing requirements; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Gu2024,
	author = {Gu, Zhanzhong and He, Xiangjian and Yu, Ping and Jia, Wenjing and Yang, Xiguang and Peng, Gang and Hu, Penghui and Chen, Shiyan and Chen, Hongjie and Lin, Yiguang},
	title = {Automatic quantitative stroke severity assessment based on Chinese clinical named entity recognition with domain-adaptive pre-trained large language model},
	year = {2024},
	journal = {Artificial Intelligence in Medicine},
	volume = {150},
	pages = {},
	doi = {10.1016/j.artmed.2024.102822},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186860362&doi=10.1016%2Fj.artmed.2024.102822&partnerID=40&md5=8042f2a2315276c21d9958a04143ea99},
	abstract = {Background: Stroke is a prevalent disease with a significant global impact. Effective assessment of stroke severity is vital for an accurate diagnosis, appropriate treatment, and optimal clinical outcomes. The National Institutes of Health Stroke Scale (NIHSS) is a widely used scale for quantitatively assessing stroke severity. However, the current manual scoring of NIHSS is labor-intensive, time-consuming, and sometimes unreliable. Applying artificial intelligence (AI) techniques to automate the quantitative assessment of stroke on vast amounts of electronic health records (EHRs) has attracted much interest. Objective: This study aims to develop an automatic, quantitative stroke severity assessment framework through automating the entire NIHSS scoring process on Chinese clinical EHRs. Methods: Our approach consists of two major parts: Chinese clinical named entity recognition (CNER) with a domain-adaptive pre-trained large language model (LLM) and automated NIHSS scoring. To build a high-performing CNER model, we first construct a stroke-specific, densely annotated dataset “Chinese Stroke Clinical Records” (CSCR) from EHRs provided by our partner hospital, based on a stroke ontology that defines semantically related entities for stroke assessment. We then pre-train a Chinese clinical LLM coined “CliRoberta” through domain-adaptive transfer learning and construct a deep learning-based CNER model that can accurately extract entities directly from Chinese EHRs. Finally, an automated, end-to-end NIHSS scoring pipeline is proposed by mapping the extracted entities to relevant NIHSS items and values, to quantitatively assess the stroke severity. Results: Results obtained on a benchmark dataset CCKS2019 and our newly created CSCR dataset demonstrate the superior performance of our domain-adaptive pre-trained LLM and the CNER model, compared with the existing benchmark LLMs and CNER models. The high F1 score of 0.990 ensures the reliability of our model in accurately extracting the entities for the subsequent automatic NIHSS scoring. Subsequently, our automated, end-to-end NIHSS scoring approach achieved excellent inter-rater agreement (0.823) and intraclass consistency (0.986) with the ground truth and significantly reduced the processing time from minutes to a few seconds. Conclusion: Our proposed automatic and quantitative framework for assessing stroke severity demonstrates exceptional performance and reliability through directly scoring the NIHSS from diagnostic notes in Chinese clinical EHRs. Moreover, this study also contributes a new clinical dataset, a pre-trained clinical LLM, and an effective deep learning-based CNER model. The deployment of these advanced algorithms can improve the accuracy and efficiency of clinical assessment, and help improve the quality, affordability and productivity of healthcare services. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Automatic Stroke Severity Assessment; Chinese Electronic Health Records; Clinical Named Entity Recognition; Domain-adaptive Pre-training; Large Language Model; Benchmarking; Computational Linguistics; Deep Learning; Diagnosis; E-learning; Natural Language Processing Systems; Records Management; Automatic Stroke Severity Assessment; Chinese Electronic Health Record; Clinical Named Entity Recognition; Domain-adaptive Pre-training; Electronic Health; Health Records; Language Model; Large Language Model; Named Entity Recognition; Pre-training; Automation; Article; Artificial Intelligence; Automation; Cerebrovascular Accident; Chinese; Chinese Clinical Named Entity Recognition; Clinical Assessment; Clinical Evaluation; Clinical Outcome; Comparative Study; Conceptual Framework; Deep Learning; Disease Severity; Electronic Health Record; Health Service; Hospital Care; Human; Interrater Reliability; Laboratory Test; Large Language Model; National Institutes Of Health Stroke Scale; Ontology; Outcome Assessment; Quantitative Analysis; Recognition; Reliability; Transfer Of Learning; China; Language; Natural Language Processing; Reproducibility; Artificial Intelligence; Electronic Health Records; Humans; Language; Natural Language Processing; Reproducibility Of Results; Stroke},
	keywords = {Benchmarking; Computational linguistics; Deep learning; Diagnosis; E-learning; Natural language processing systems; Records management; Automatic stroke severity assessment; Chinese electronic health record; Clinical named entity recognition; Domain-adaptive pre-training; Electronic health; Health records; Language model; Large language model; Named entity recognition; Pre-training; Automation; Article; artificial intelligence; automation; cerebrovascular accident; Chinese; Chinese clinical named entity recognition; clinical assessment; clinical evaluation; clinical outcome; comparative study; conceptual framework; deep learning; disease severity; electronic health record; health service; hospital care; human; interrater reliability; laboratory test; large language model; National Institutes of Health Stroke Scale; ontology; outcome assessment; quantitative analysis; recognition; reliability; transfer of learning; China; language; natural language processing; reproducibility; Artificial Intelligence; Electronic Health Records; Humans; Language; Natural Language Processing; Reproducibility of Results; Stroke},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Rajjoub2024998,
	author = {Rajjoub, Rami and Arroyave, Juan Sebastian and Zaidat, Bashar and Ahmed, Wasil and Restrepo Mejia, Mateo and Tang, Justin E. and Kim, Jun S. and Cho, Samuel Kang Wook},
	title = {ChatGPT and its Role in the Decision-Making for the Diagnosis and Treatment of Lumbar Spinal Stenosis: A Comparative Analysis and Narrative Review},
	year = {2024},
	journal = {Global Spine Journal},
	volume = {14},
	number = {3},
	pages = {998 - 1017},
	doi = {10.1177/21925682231195783},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167623905&doi=10.1177%2F21925682231195783&partnerID=40&md5=49f441feedd6a87251d14d9261db539b},
	abstract = {Study Design: Comparative Analysis and Narrative Review. Objective: To assess and compare ChatGPT’s responses to the clinical questions and recommendations proposed by The 2011 North American Spine Society (NASS) Clinical Guideline for the Diagnosis and Treatment of Degenerative Lumbar Spinal Stenosis (LSS). We explore the advantages and disadvantages of ChatGPT’s responses through an updated literature review on spinal stenosis. Methods: We prompted ChatGPT with questions from the NASS Evidence-based Clinical Guidelines for LSS and compared its generated responses with the recommendations provided by the guidelines. A review of the literature was performed via PubMed, OVID, and Cochrane on the diagnosis and treatment of lumbar spinal stenosis between January 2012 and April 2023. Results: 14 questions proposed by the NASS guidelines for LSS were uploaded into ChatGPT and directly compared to the responses offered by NASS. Three questions were on the definition and history of LSS, one on diagnostic tests, seven on non-surgical interventions and three on surgical interventions. The review process found 40 articles that were selected for inclusion that helped corroborate or contradict the responses that were generated by ChatGPT. Conclusions: ChatGPT’s responses were similar to findings in the current literature on LSS. These results demonstrate the potential for implementing ChatGPT into the spine surgeon’s workplace as a means of supporting the decision-making process for LSS diagnosis and treatment. However, our narrative summary only provides a limited literature review and additional research is needed to standardize our findings as means of validating ChatGPT’s use in the clinical space. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Clinical Guidelines; Decision-making; Large Language Models; Lumbar Stenosis; Spine; Acetylsalicylic Acid; Duloxetine; Endorphin; Gabapentin; Ibuprofen; Naproxen; Paracetamol; Acetylsalicylic Acid; Anticonvulsive Agent; Antidepressant Agent; Corticosteroid; Duloxetine; Endorphin; Gabapentin; Ibuprofen; Naproxen; Nonsteroid Antiinflammatory Agent; Paracetamol; Adult; Aerobic Exercise; Analgesia; Artificial Intelligence; Atrophy; Claudication; Cochrane Library; Decision Making; Decompression; Diagnostic Imaging; Electromyography; Electrostimulation; Functional Electrical Stimulation; Human; Large Language Model; Lumbar Puncture; Lumbar Spinal Stenosis; Manipulative Medicine; Medical Ontology; Medline; Myelography; Nerve Root; Neurogenic Claudication; Neuromuscular Electrical Stimulation; Neuropathic Pain; Nuclear Magnetic Resonance Imaging; Paresthesia; Practice Guideline; Prognosis; Questionnaire; Radicular Pain; Range Of Motion; Review; Spinal Cord Disease; Spine; Spine Fusion; Spine Manipulation; Spine Surgery; Spondylolisthesis; Surgeon; Systematic Review; Transcutaneous Electrical Nerve Stimulation; Vertebral Canal Stenosis; Weakness; X Ray; X-ray Computed Tomography},
	keywords = {acetylsalicylic acid; anticonvulsive agent; antidepressant agent; corticosteroid; duloxetine; endorphin; gabapentin; ibuprofen; naproxen; nonsteroid antiinflammatory agent; paracetamol; adult; aerobic exercise; analgesia; artificial intelligence; atrophy; claudication; Cochrane Library; decision making; decompression; diagnostic imaging; electromyography; electrostimulation; functional electrical stimulation; human; large language model; lumbar puncture; lumbar spinal stenosis; manipulative medicine; medical ontology; Medline; myelography; nerve root; neurogenic claudication; neuromuscular electrical stimulation; neuropathic pain; nuclear magnetic resonance imaging; paresthesia; practice guideline; prognosis; questionnaire; radicular pain; range of motion; Review; spinal cord disease; spine; spine fusion; spine manipulation; spine surgery; spondylolisthesis; surgeon; systematic review; transcutaneous electrical nerve stimulation; vertebral canal stenosis; weakness; X ray; x-ray computed tomography},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 49; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Lui202422797,
	author = {Lui, Shengjie and Cheng, Xiang and Krishnaswamy, Shonali Priyadarsini},
	title = {KAMEL: Knowledge Aware Medical Entity Linkage to Automate Health Insurance Claims Processing},
	year = {2024},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume = {38},
	number = {21},
	pages = {22797 - 22805},
	doi = {10.1609/aaai.v38i21.30314},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189611909&doi=10.1609%2Faaai.v38i21.30314&partnerID=40&md5=e29b0f967cb2d26c2423d04d82a8a5fd},
	abstract = {Automating the processing of health insurance claims to achieve “Straight-Through Processing” is one of the holy grails that all insurance companies aim to achieve. One of the major impediments to this automation is the difficulty in establishing the relationship between the underwriting exclusions that a policy has and the incoming claim's diagnosis information. Typically, policy underwriting exclusions are captured in free-text such as “Respiratory illnesses are excluded due to a pre-existing asthma condition”. A medical claim coming from a hospital would have the diagnosis represented using the International Classification of Disease (ICD) codes from the World Health Organization. The complex and labour-intensive task of establishing the relationship between free-text underwriting exclusions in health insurance policies and medical diagnosis codes from health insurance claims is critical towards determining if a claim should be rejected due to underwriting exclusions. In this work, we present a novel framework that leverages both explicit and implicit domain knowledge present in medical ontologies and pre-trained language models respectively, to effectively establish the relationship between free-text describing medical conditions present in underwriting exclusions and the ICD-10CM diagnosis codes in health insurance claims. Termed KAMEL (Knowledge Aware Medical Entity Linkage), our proposed framework addresses the limitations faced by prior approaches when evaluated on real-world health insurance claims data. Our proposed framework have been deployed in several multi-national health insurance providers to automate their health insurance claims. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Codes (symbols); Diagnosis; Diseases; Domain Knowledge; Condition; Diagnosis Information; Free Texts; Insurance Claims; Insurance Companies; International Classification Of Disease; Medical Claims; Respiratory Illness; Straight Through Processing; World Health Organization; Health Insurance},
	keywords = {Codes (symbols); Diagnosis; Diseases; Domain Knowledge; Condition; Diagnosis information; Free texts; Insurance claims; Insurance companies; International classification of disease; Medical claims; Respiratory illness; Straight Through Processing; World Health Organization; Health insurance},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Shang202418934,
	author = {Shang, Ziyu and Ke, Wenjun and Xiu, Nana and Wang, Peng and Liu, Jiajun and Li, Yanhui and Luo, Zhizhao and Ji, Ke},
	title = {OntoFact: Unveiling Fantastic Fact-Skeleton of LLMs via Ontology-Driven Reinforcement Learning},
	year = {2024},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume = {38},
	number = {17},
	pages = {18934 - 18943},
	doi = {10.1609/aaai.v38i17.29859},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186379985&doi=10.1609%2Faaai.v38i17.29859&partnerID=40&md5=efd4c3b06d154bf1a194a0d5237195e3},
	abstract = {Large language models (LLMs) have demonstrated impressive proficiency in information retrieval, while they are prone to generating incorrect responses that conflict with reality, a phenomenon known as intrinsic hallucination. The critical challenge lies in the unclear and unreliable fact distribution within LLMs trained on vast amounts of data. The prevalent approach frames the factual detection task as a question-answering paradigm, where the LLMs are asked about factual knowledge and examined for correctness. However, existing studies primarily focused on deriving test cases only from several specific domains, such as movies and sports, limiting the comprehensive observation of missing knowledge and the analysis of unexpected hallucinations. To address this issue, we propose OntoFact, an adaptive framework for detecting unknown facts of LLMs, devoted to mining the ontology-level skeleton of the missing knowledge. Specifically, we argue that LLMs could expose the ontology-based similarity among missing facts and introduce five representative knowledge graphs (KGs) as benchmarks. We further devise a sophisticated ontology-driven reinforcement learning (ORL) mechanism to produce error-prone test cases with specific entities and relations automatically. The ORL mechanism rewards the KGs for navigating toward a feasible direction for unveiling factual errors. Moreover, empirical efforts demonstrate that dominant LLMs are biased towards answering Yes rather than No, regardless of whether this knowledge is included. To mitigate the overconfidence of LLMs, we leverage a hallucination-free detection (HFD) strategy to tackle unfair comparisons between baselines, thereby boosting the result robustness. Experimental results on 5 datasets, using 32 representative LLMs, reveal a general lack of fact in current LLMs. Notably, ChatGPT exhibits fact error rates of 51.6% on DBpedia and 64.7% on YAGO, respectively. Additionally, the ORL mechanism demonstrates promising error prediction scores, with F1 scores ranging from 70% to 90% across most LLMs. Compared to the exhaustive testing, ORL achieves an average recall of 80% while reducing evaluation time by 35.29% to 63.12%. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Errors; Knowledge Graph; Musculoskeletal System; Ontology; Critical Challenges; Detection Tasks; Factual Knowledge; Knowledge Graphs; Language Model; Learning Mechanism; Ontology's; Question Answering; Reinforcement Learnings; Test Case; Reinforcement Learning},
	keywords = {Errors; Knowledge graph; Musculoskeletal system; Ontology; Critical challenges; Detection tasks; Factual knowledge; Knowledge graphs; Language model; Learning mechanism; Ontology's; Question Answering; Reinforcement learnings; Test case; Reinforcement learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Gold Open Access}
}

@CONFERENCE{Wu202419234,
	author = {Wu, Chenxiao and Ke, Wenjun and Wang, Peng and Luo, Zhizhao and Li, Guozheng and Chen, Wanyi},
	title = {ConsistNER: Towards Instructive NER Demonstrations for LLMs with the Consistency of Ontology and Context},
	year = {2024},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume = {38},
	number = {17},
	pages = {19234 - 19242},
	doi = {10.1609/aaai.v38i17.29892},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186365966&doi=10.1609%2Faaai.v38i17.29892&partnerID=40&md5=af1fab4da2b7811b86acd906ab4cf6a8},
	abstract = {Named entity recognition (NER) aims to identify and classify specific entities mentioned in textual sentences. Most existing superior NER models employ the standard fully supervised paradigm, which requires a large amount of annotated data during training. In order to maintain performance with insufficient annotation resources (i.e., low resources), in-context learning (ICL) has drawn a lot of attention, due to its plug-and-play nature compared to other methods (e.g., meta-learning and prompt learning). In this manner, how to retrieve high-correlated demonstrations for target sentences serves as the key to emerging ICL ability. For the NER task, the correlation implies the consistency of both ontology (i.e., generalized entity type) and context (i.e., sentence semantic), which is ignored by previous NER demonstration retrieval techniques. To address this issue, we propose ConsistNER, a novel three-stage framework that incorporates ontological and contextual information for low-resource NER. Firstly, ConsistNER employs large language models (LLMs) to pre-recognize potential entities in a zero-shot manner. Secondly, ConsistNER retrieves the sentence-specific demonstrations for each target sentence based on the two following considerations: (1) Regarding ontological consistency, demonstrations are filtered into a candidate set based on ontology distribution. (2) Regarding contextual consistency, an entity-aware self-attention mechanism is introduced to focus more on the potential entities and semantic-correlated tokens. Finally, ConsistNER feeds the retrieved demonstrations for all target sentences into LLMs for prediction. We conduct experiments on four widely-adopted NER datasets, including both general and specific domains. Experimental results show that ConsistNER achieves a 6.01%-26.37% and 3.07%-21.18% improvement over the state-of-the-art baselines on Micro-F1 scores under 1- and 5-shot settings, respectively. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology; Semantics; Zero-shot Learning; Context Learning; In Contexts; Language Model; Large Amounts; Metalearning; Named Entity Recognition; Ontology's; Performance; Plug-and-play; Recognition Models; Demonstrations},
	keywords = {Ontology; Semantics; Zero-shot learning; Context learning; In contexts; Language model; Large amounts; Metalearning; Named entity recognition; Ontology's; Performance; Plug-and-play; Recognition models; Demonstrations},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Gold Open Access}
}

@CONFERENCE{Baldazzi2024834,
	author = {Baldazzi, Teodoro and Bellomarini, Luigi and Ceri, Stefano and Colombo, Andrea and Gentili, Andrea and Sallinger, Emanuel},
	title = {“Please, Vadalog, tell me why”: Interactive Explanation of Datalog-based Reasoning},
	year = {2024},
	journal = {Advances in Database Technology - EDBT},
	volume = {27},
	number = {3},
	pages = {834 - 837},
	doi = {10.48786/edbt.2024.82},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191027240&doi=10.48786%2Fedbt.2024.82&partnerID=40&md5=fed02f99deec886f8cf8628c691ed3ec},
	abstract = {Integrating Large Language Models (LLMs) with logic-based Enterprise Knowledge Graphs (EKGs) and more generally with Knowledge Representation and Reasoning (KRR) approaches is currently at the forefront of research in many data-intensive areas, as language models may complement EKGs and ontological reasoning with flexibility and human orientation. Conversely, EKGs provide transparency and explainability on the conclusions drawn, a typical weak point of LLMs, which operate opaquely. In this demo, we integrate Llama 2 with our reasoning system Vadalog and use it to turn a chase graph, i.e., the trace of an ontological reasoning process, into a human-readable business report. In other words, we show the amazing capabilities of state-of-the-art LLMs in combination with a principled exploitation of the theoretical underpinnings of logic-based reasoning. We walk the audience through a visual environment, unfolding real-world reasoning settings from the Central Bank of Italy. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computer Circuits; Knowledge Graph; Ontology; Based Reasonings; Data Intensive; Datalog; Human Orientation; Knowledge Graphs; Knowledge Representation And Reasoning; Language Model; Ontological Reasoning; Reasoning Approach; Weak Points; Computational Linguistics},
	keywords = {Computer circuits; Knowledge graph; Ontology; Based reasonings; Data intensive; Datalog; Human orientation; Knowledge graphs; Knowledge representation and reasoning; Language model; Ontological reasoning; Reasoning approach; Weak points; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Yáñez-Romero2024137,
	author = {Yáñez-Romero, Fabio Antonio and Montoyo, Andrés and Muñoz-Guillena, Rafael R. and Gutiérrez, Yoan and Suárez, Armando},
	title = {OntoLM: Integrating Knowledge Bases and Language Models for classification in the medical domain; OntoLM: Integrando bases de conocimiento y modelos de lenguaje para clasificación en dominio médico},
	year = {2024},
	journal = {Procesamiento del Lenguaje Natural},
	volume = {72},
	pages = {137 - 148},
	doi = {10.26342/2024-72-10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200508113&doi=10.26342%2F2024-72-10&partnerID=40&md5=9a744233e874ad7d56e0a116aaa25dd0},
	abstract = {Large language models have shown impressive performance in Natural Language Processing tasks, but their black box characteristics render the explainability of the model's decision difficult to achieve and the integration of semantic knowledge. There has been a growing interest in combining external knowledge sources with language models to address these drawbacks. This paper, OntoLM, proposes a novel architecture combining an ontology with a pre-trained language model to classify biomedical entities in text. This approach involves constructing and processing graphs from ontologies and then using a graph neural network to contextualize each entity. Next, the language model and the graph neural network output are combined into a final classifier. Results show that OntoLM improves the classification of entities in medical texts using a set of categories obtained from the Unified Medical Language System. We can create more traceable natural language processing architectures using ontology graphs and graph neural networks. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {External Knowledge; Graph Neural Networks; Large Language Models; Ontologies},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Muzanenhamo2024,
	author = {Muzanenhamo, Penelope and Power, Sean Bradley},
	title = {ChatGPT and accounting in African contexts: Amplifying epistemic injustice},
	year = {2024},
	journal = {Critical Perspectives on Accounting},
	volume = {99},
	pages = {},
	doi = {10.1016/j.cpa.2024.102735},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192186390&doi=10.1016%2Fj.cpa.2024.102735&partnerID=40&md5=11a0cc173ae9140e7184282c421be488},
	abstract = {Large Language Models (LLMs) such as ChatGPT are likely to amplify epistemic injustice through the lack of transparency and traceability of data sources. The unethical alienation of original knowledge producers from their intellectual products, which are repackaged by LLMs as artificial intelligence, conceals power asymmetries in the global knowledge production and dissemination system. As elaborated by Miranda Fricker (2010), Western White male actors traditionally dominate knowledge production; therefore, ChatGPT and other LLMs are inclined to reproduce patriarchal perspectives as universal understandings of the World. Our commentary applies this logic to accounting practice and research in Africa, and asserts that epistemic injustice, resulting from colonization and racism, means that ontological and epistemological approaches situated in the accounting needs and experiences of African communities are missing from or poorly articulated by ChatGPT and other LLMs. If LLMs are to attain legitimacy as (ethical) sources of knowledge, regulation must be enforced to ensure transparency—as a foundation for promoting pluriversality and eliminating epistemic injustice. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Accounting; Africa; Chatgpt; Epistemic Injustice; Large Language Models; Pluriversality},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Caufield2024,
	author = {Caufield, John Harry and Hegde, Harshad B. and Emonet, Vincent and Harris, Nomi L. and Joachimiak, Marcin Pawel and Matentzoglu, Nicolas A. and Kim, Hyeongsik and Moxon, Sierra A.Taylor and Reese, Justin T. and Haendel, Melissa Anne},
	title = {Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES): a method for populating knowledge bases using zero-shot learning},
	year = {2024},
	journal = {Bioinformatics},
	volume = {40},
	number = {3},
	pages = {},
	doi = {10.1093/bioinformatics/btae104},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187509853&doi=10.1093%2Fbioinformatics%2Fbtae104&partnerID=40&md5=c8ab6d67018f4989386c6cc3a12eb9dc},
	abstract = {Motivation: Creating knowledge bases and ontologies is a time consuming task that relies on manual curation. AI/NLP approaches can assist expert curators in populating these knowledge bases, but current approaches rely on extensive training data, and are not able to populate arbitrarily complex nested knowledge schemas. Results: Here we present Structured Prompt Interrogation and Recursive Extraction of Semantics (SPIRES), a Knowledge Extraction approach that relies on the ability of Large Language Models (LLMs) to perform zero-shot learning and general-purpose query answering from flexible prompts and return information conforming to a specified schema. Given a detailed, user-defined knowledge schema and an input text, SPIRES recursively performs prompt interrogation against an LLM to obtain a set of responses matching the provided schema. SPIRES uses existing ontologies and vocabularies to provide identifiers for matched elements. We present examples of applying SPIRES in different domains, including extraction of food recipes, multispecies cellular signaling pathways, disease treatments, multi-step drug mechanisms, and chemical to disease relationships. Current SPIRES accuracy is comparable to the mid-range of existing Relation Extraction methods, but greatly surpasses an LLM’s native capability of grounding entities with unique identifiers. SPIRES has the advantage of easy customization, flexibility, and, crucially, the ability to perform new tasks in the absence of any new training data. This method supports a general strategy of leveraging the language interpreting capabilities of LLMs to assemble knowledge bases, assisting manual knowledge curation and acquisition while supporting validation with publicly-available databases and ontologies external to the LLM. Availability and implementation: SPIRES is available as part of the open source OntoGPT package: https://github.com/monarch-initiative/ontogpt. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Factual Database; Knowledge Base; Semantics; Databases, Factual; Knowledge Bases; Semantics},
	keywords = {factual database; knowledge base; semantics; Databases, Factual; Knowledge Bases; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 43; All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Chen20244383,
	author = {Chen, Long and Li, Yuchen and Silamu, Wushour and Li, Qingquan and Ge, Shirong and Wang, Feiyue},
	title = {Smart Mining With Autonomous Driving in Industry 5.0: Architectures, Platforms, Operating Systems, Foundation Models, and Applications},
	year = {2024},
	journal = {IEEE Transactions on Intelligent Vehicles},
	volume = {9},
	number = {3},
	pages = {4383 - 4393},
	doi = {10.1109/TIV.2024.3365997},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186065598&doi=10.1109%2FTIV.2024.3365997&partnerID=40&md5=eab7b1a313e27297809660f660a410d7},
	abstract = {The increasing importance of mineral resources in contemporary society is becoming more prominent, playing an indispensable and crucial role in the global economy. These resources not only provide essential raw materials for the global economic system but also play an irreplaceable role in supporting the development of modern industry, technology, and infrastructure. With the rapid development of intelligent technologies such as Industry 5.0 and advanced Large Language Models (LLMs), the mining industry is facing unprecedented opportunities and challenges. The development of smart mines has become a crucial direction for industry progress. This article aims to explore the strategic requirements for the development of smart mines by combining advanced products or technologies such as Chat-GPT (one of the successful applications of LLMs), digital twins, and scenario engineering. We propose a comprehensive architecture consisting of three different levels, the mining industrial Internet of Things (IoT) platform, mining operating systems, and foundation models. The systems and models empower the mining equipment for transportation. The architecture delivers a comprehensive solution that aligns perfectly with the demands of Industry 5.0. The application and validation outcomes of this intelligent solution showcase a noteworthy enhancement in mining efficiency and a reduction in safety risks, thereby laying a sturdy groundwork for the advent of Mining 5.0. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Architectures; Autonomous Driving; Industry 5.0; Mining 5.0; Mining Transportation Trucks; Smart Mining; Architecture; Autonomous Vehicles; Computer Architecture; Internet Of Things; Job Analysis; Mineral Resources; Mining Industry; Autonomous Driving; Biological System Modeling; Fifth Industrial Revolution; Industrial Revolutions; Industry 5.0; Mining 5.0; Mining Transportation Truck; Mining Transportations; Ontology's; Smart Mining; Task Analysis; Mining},
	keywords = {Architecture; Autonomous vehicles; Computer architecture; Internet of things; Job analysis; Mineral resources; Mining industry; Autonomous driving; Biological system modeling; Fifth industrial revolution; Industrial revolutions; Industry 5.0; Mining 5.0; Mining transportation truck; Mining transportations; Ontology's; Smart mining; Task analysis; Mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27}
}

@ARTICLE{Buehler2024,
	author = {Buehler, Markus J.},
	title = {MechGPT, a Language-Based Strategy for Mechanics and Materials Modeling That Connects Knowledge Across Scales, Disciplines, and Modalities},
	year = {2024},
	journal = {Applied Mechanics Reviews},
	volume = {76},
	number = {2},
	pages = {},
	doi = {10.1115/1.4063843},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184374431&doi=10.1115%2F1.4063843&partnerID=40&md5=6ef681acf0141b5e9df2cc6f5ea0baf3},
	abstract = {For centuries, researchers have sought out ways to connect disparate areas of knowledge. While early scholars (Galileo, da Vinci, etc.) were experts across fields, specialization took hold later. With the advent of Artificial Intelligence, we can now explore relationships across areas (e.g., mechanics-biology) or disparate domains (e.g., failure mechanics-art). To achieve this, we use a fine-tuned large language model (LLM), here for a subset of knowledge in multiscale materials failure. The approach includes the use of a general-purpose LLM to distill question-answer pairs from raw sources followed by LLM fine-tuning. The resulting MechGPTLLMfoundation model is used in a series of computational experiments to explore its capacity for knowledge retrieval, various language tasks, hypothesis generation, and connecting knowledge across disparate areas. While the model has some ability to recall knowledge from training, we find that LLMs are particularly useful for extracting structural insights through Ontological Knowledge Graphs. These interpretable graph structures provide explanatory insights, frameworks for new research questions, and visual representations of knowledge that also can be used in retrieval-augmented generation. Three versions of MechGPT are discussed, featuring different sizes from 13×109 to 70×109 parameters, and reaching context lengths of more than 10,000 tokens. This provides ample capacity for sophisticated retrieval augmented strategies, as well as agentbased modeling where multiple LLMs interact collaboratively and/or adversarially, the incorporation of new data from the literature or web searches, as well as multimodality. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai; Attention; Failure; Gpt; Human-machine; Language Model; Materials; Mechanics; Scientific Ml; Transformer; Computational Linguistics; Graphic Methods; Knowledge Graph; Modeling Languages; Attention; Galileo; Gpt; Human-machine; Language Model; Material Modeling; Mechanic Model; Scientific Ml; Specialisation; Transformer; Failure (mechanical)},
	keywords = {Computational linguistics; Graphic methods; Knowledge graph; Modeling languages; Attention; GALILEO; GPT; Human-machine; Language model; Material modeling; Mechanic model; Scientific ML; Specialisation; Transformer; Failure (mechanical)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 58; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Kulmanov2024220,
	author = {Kulmanov, Maxat and Guzmán-Vega, Francisco J. and Duek-Roggli, Paula and Lane, Lydie and Arold, Stefan T. and Hoehndorf, Robert},
	title = {Protein function prediction as approximate semantic entailment},
	year = {2024},
	journal = {Nature Machine Intelligence},
	volume = {6},
	number = {2},
	pages = {220 - 228},
	doi = {10.1038/s42256-024-00795-w},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185149701&doi=10.1038%2Fs42256-024-00795-w&partnerID=40&md5=375ceaf08252941c8c3de0bd6152d18e},
	abstract = {The Gene Ontology (GO) is a formal, axiomatic theory with over 100,000 axioms that describe the molecular functions, biological processes and cellular locations of proteins in three subontologies. Predicting the functions of proteins using the GO requires both learning and reasoning capabilities in order to maintain consistency and exploit the background knowledge in the GO. Many methods have been developed to automatically predict protein functions, but effectively exploiting all the axioms in the GO for knowledge-enhanced learning has remained a challenge. We have developed DeepGO-SE, a method that predicts GO functions from protein sequences using a pretrained large language model. DeepGO-SE generates multiple approximate models of GO, and a neural network predicts the truth values of statements about protein functions in these approximate models. We aggregate the truth values over multiple models so that DeepGO-SE approximates semantic entailment when predicting protein functions. We show, using several benchmarks, that the approach effectively exploits background knowledge in the GO and improves protein function prediction compared to state-of-the-art methods. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Forecasting; Proteins; Semantics; Approximate Modeling; Axiomatic Theory; Background Knowledge; Biological Process; Gene Ontology; Molecular Function; Protein Function Prediction; Protein Functions; Semantic Entailment; Truth Values; Gene Ontology},
	keywords = {Forecasting; Proteins; Semantics; Approximate modeling; Axiomatic theory; Background knowledge; Biological process; Gene ontology; Molecular function; Protein function prediction; Protein functions; Semantic entailment; Truth values; Gene Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 41; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Yang2024,
	author = {Yang, Jian and Shu, Liqi and Han, Mingyu and Pan, Jiarong and Chen, Lihua and Yuan, Tianming and Tan, Linhua and Shu, Qiang and Duan, Huilong and Li, Haomin},
	title = {RDmaster: A novel phenotype-oriented dialogue system supporting differential diagnosis of rare disease},
	year = {2024},
	journal = {Computers in Biology and Medicine},
	volume = {169},
	pages = {},
	doi = {10.1016/j.compbiomed.2024.107924},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181539226&doi=10.1016%2Fj.compbiomed.2024.107924&partnerID=40&md5=96e8f5deb2acc9fcc319180e0576d42f},
	abstract = {Background: Clinicians often lack the necessary expertise to differentially diagnose multiple underlying rare diseases (RDs) due to their complex and overlapping clinical features, leading to misdiagnoses and delayed treatments. The aim of this study is to develop a novel electronic differential diagnostic support system for RDs. Method: Through integrating two Bayesian diagnostic methods, a candidate list was generated with enhance clinical interpretability for the further Q&A based differential diagnosis (DDX). To achieve an efficient Q&A dialogue strategy, we introduce a novel metric named the adaptive information gain and Gini index (AIGGI) to evaluate the expected gain of interrogated phenotypes within real-time diagnostic states. Results: This DDX tool called RDmaster has been implemented as a web-based platform (http://rdmaster.nbscn.org/). A diagnostic trial involving 238 published RD patients revealed that RDmaster outperformed existing RD diagnostic tools, as well as ChatGPT, and was shown to enhance the diagnostic accuracy through its Q&A system. Conclusions: The RDmaster offers an effective multi-omics differential diagnostic technique and outperforms existing tools and popular large language models, particularly enhancing differential diagnosis in collecting diagnostically beneficial phenotypes. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Differential Diagnosis; Electronic Differential Diagnostic Support System; Human Phenotype Ontology; Phenomic And Genomic Diagnostics; Rare Disease; 1,1 Dichloro 2,2 Bis(4 Chlorophenyl)ethylene; Dichlorodiphenyl Dichloroethylene; Diagnosis; Diseases; Genes; Speech Processing; Diagnostic Support Systems; Dialogue Systems; Differential Diagnosis; Electronic Differential; Electronic Differential Diagnostic Support System; Genomics; Human Phenotype Ontology; Ontology's; Phenomic And Genomic Diagnostic; Rare Disease; Genome; Article; Chatgpt; Diagnostic Accuracy; Diagnostic Procedure; Diagnostic Test Accuracy Study; Differential Diagnosis; Female; Gini Coefficient; Human; Human Computer Interaction; Knowledge Base; Major Clinical Study; Male; Multiomics; Phenotype; Rare Disease; Retrospective Study; Whole Exome Sequencing; Workflow; Bayes Theorem; Genetics; 1,1 Dichloro 2,2 Bis(4 Chlorophenyl)ethylene; Bayes Theorem; Diagnosis, Differential; Dichlorodiphenyl Dichloroethylene; Humans; Phenotype; Rare Diseases},
	keywords = {Diagnosis; Diseases; Genes; Speech processing; Diagnostic support systems; Dialogue systems; Differential diagnosis; Electronic differential; Electronic differential diagnostic support system; Genomics; Human phenotype ontology; Ontology's; Phenomic and genomic diagnostic; Rare disease; Genome; Article; ChatGPT; diagnostic accuracy; diagnostic procedure; diagnostic test accuracy study; differential diagnosis; female; Gini coefficient; human; human computer interaction; knowledge base; major clinical study; male; multiomics; phenotype; rare disease; retrospective study; whole exome sequencing; workflow; Bayes theorem; genetics; 1,1 dichloro 2,2 bis(4 chlorophenyl)ethylene; Bayes Theorem; Diagnosis, Differential; Dichlorodiphenyl Dichloroethylene; Humans; Phenotype; Rare Diseases},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Yang2024,
	author = {Yang, Jingye and Liu, Cong and Deng, Wendy and Wu, Da and Weng, Chunhua and Zhou, Yunyun and Wang, Kai},
	title = {Enhancing phenotype recognition in clinical notes using large language models: PhenoBCBERT and PhenoGPT},
	year = {2024},
	journal = {Patterns},
	volume = {5},
	number = {1},
	pages = {},
	doi = {10.1016/j.patter.2023.100887},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182025246&doi=10.1016%2Fj.patter.2023.100887&partnerID=40&md5=0640561e3fac9d736333317606e443b9},
	abstract = {To enhance phenotype recognition in clinical notes of genetic diseases, we developed two models—PhenoBCBERT and PhenoGPT—for expanding the vocabularies of Human Phenotype Ontology (HPO) terms. While HPO offers a standardized vocabulary for phenotypes, existing tools often fail to capture the full scope of phenotypes due to limitations from traditional heuristic or rule-based approaches. Our models leverage large language models to automate the detection of phenotype terms, including those not in the current HPO. We compare these models with PhenoTagger, another HPO recognition tool, and found that our models identify a wider range of phenotype concepts, including previously uncharacterized ones. Our models also show strong performance in case studies on biomedical literature. We evaluate the strengths and weaknesses of BERT- and GPT-based models in aspects such as architecture and accuracy. Overall, our models enhance automated phenotype detection from clinical texts, improving downstream analyses on human diseases. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Clinical Notes; Dsml 3: Development/pre-production: Data Science Output Has Been Rolled Out/validated Across Multiple Domains/problems; Electronic Health Records; Gpt; Human Phenotype Ontology; Named Entity Recognition; Transformer; Computational Linguistics; Bert; Clinical Notes; Domain Problems; Dsml 3: Development/pre-production: Data Science Output Have Been Rolled Out/validated Across Multiple Domain/problem; Electronic Health; Electronic Health Record; Gpt; Health Records; Human Phenotype Ontology; Multiple Domains; Named Entity Recognition; Ontology's; Pre-production; Production Data; Transformer; Ontology},
	keywords = {Computational linguistics; BERT; Clinical notes; Domain problems; DSML 3: development/pre-production: data science output have been rolled out/validated across multiple domain/problem; Electronic health; Electronic health record; GPT; Health records; Human phenotype ontology; Multiple domains; Named entity recognition; Ontology's; Pre-production; Production data; Transformer; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Putman2024D938,
	author = {Putman, Tim E. and Schaper, Kevin and Matentzoglu, Nicolas A. and Rubinetti, Vincent P. and Alquaddoomi, Faisal S. and Cox, Corey and Caufield, John Harry and Elsarboukh, Glass and Gehrke, Sarah and Hegde, Harshad B.},
	title = {The Monarch Initiative in 2024:ãnãnalytic platform integrating phenotypes, g enesãnd diseasesãcross species},
	year = {2024},
	journal = {Nucleic Acids Research},
	volume = {52},
	number = {D1},
	pages = {D938 - D949},
	doi = {10.1093/nar/gkad1082},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181766041&doi=10.1093%2Fnar%2Fgkad1082&partnerID=40&md5=9cd167a167fb25baee13f5894b89092d},
	abstract = {Bridging the gap between genetic vãriations, en vironmentãl determinants,ãnd phenot ypic outcomes is critical for supporting clinical diagnosisãnd understanding mechanisms of diseases. It requires integrating open dataãtã global scale. The Monarch Initiativeãdvances these goals b y de v eloping open ontologies, semantic data models,ãnd kno wledge graphs f or translational research. T he Monarch App isãn integrated platform combining dataãbout genes, phenotypes,ãnd diseasesãcross species. Monarch's APIs enableãccess to carefully curated datasetsãndãdvãncedãnaly sis tools that support the understandingãnd diagnosis of disease for diverseãpplications suchãs variant prioritization, deep phenot yping ,ãnd patient profile-matching . We ha v e migrated our sy stem intoã scalable, cloud-based infrastr uct ure; simplified Monarch's data ingestionãnd knowledge graph integration systems; enhanced data mappingãnd integration standards;ãnd de v elopedã ne w user interfãce with no v el searchãnd graph navigation features. Furthermore, weãdvanced Monarch'sãnalytic tools by developingã customized plugin for OpenAI's ChatGPT to increase the reliability of its responsesãbout phenot ypic datã,ãllowing us to interrogate the knowledge in the Monarch graph using state-of-the-art Large Language Models. The resources of the Monarch Initiative can be foundãt monarchinitiative.orgãnd its corresponding code repositoryãt github.com / monarc h-initiative / monarc h-app. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Article; Chatgpt; Diseases; Ehlers Danlos Syndrome; Gene; Gene Ontology; Generative Pretrained Transformer; Genetic Background; Genetic Variation; Large Language Model; Machine Learning; Natural Language Processing; Nonhuman; Phenotype; Quality Control; Retinitis Pigmentosa; Translational Research; Usher Syndrome; Written Communication; Human; Reproducibility; Humans; Phenotype; Reproducibility Of Results},
	keywords = {Article; ChatGPT; diseases; Ehlers Danlos syndrome; gene; gene ontology; generative pretrained transformer; genetic background; genetic variation; large language model; machine learning; natural language processing; nonhuman; phenotype; quality control; retinitis pigmentosa; translational research; Usher syndrome; written communication; human; reproducibility; Humans; Phenotype; Reproducibility of Results},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Marchenko202411,
	author = {Marchenko, Oleksandr O. and Dvoichenkov, Danylo},
	title = {TaxoRankConstruct: A Novel Rank-based Iterative Approach to Taxonomy Construction with Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3933},
	pages = {11 - 27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000713232&partnerID=40&md5=e01af4385899f16ca8a870408073ad18},
	abstract = {Paper presents a novel method for the construction of taxonomical classifications (concept hierarchies) for concepts using large language models. Traditional methods of taxonomy construction often focus heavily on hypernym-hyponym relationships, emphasizing hierarchical connections between concepts. However, these approaches tend to overlook the qualitative attributes of objects that form the foundation of classification. In contrast, the approach proposed in this paper is based on the premise that "the properties of objects are primary, while the types of objects are secondary."This foundational idea drives the development of TaxoRankConstruct, a novel rank-based iterative approach that leverages Large Language Models (LLMs) to construct more nuanced taxonomies. This method aims to enhance the clarity and precision of taxonomical hierarchies by systematically organizing concepts based on specific, identifiable characteristics. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Hierarchies; Hierarchical Classification; Human-ai Collaboration; Iterative Methods; Large Language Models; Natural Language Processing; Ontology Learning; Taxonomy Construction; Taxonomies; Concept Hierarchies; Hierarchical Classification; Human-ai Collaboration; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Ontology Learning; Taxonomy Construction; Contrastive Learning},
	keywords = {Taxonomies; Concept hierarchies; Hierarchical classification; Human-AI collaboration; Language model; Language processing; Large language model; Natural language processing; Natural languages; Ontology learning; Taxonomy construction; Contrastive Learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Zhou202421,
	author = {Zhou, Xiaofa and Shi, Jianyong and Dong, Lei and Zhang, You and Pan, Jin and Huang, Hao},
	title = {Construction of a Multimodal Knowledge Graph for Power Grid Construction Safety Based on Large Language Models},
	year = {2024},
	pages = {21 - 28},
	doi = {10.1109/NPSPE62515.2024.00013},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000264602&doi=10.1109%2FNPSPE62515.2024.00013&partnerID=40&md5=40fdb4c39be07596baf7f192abb6fa10},
	abstract = {In response to the difficulties of safety management and the complexity of information at power grid construction sites, a multimodal knowledge graph construction method based on large language models is proposed. Data from the construction site is collected and filtered, and an ontology for safety management at the construction site is constructed. The ontology is then used as retrieval augmented generation(RAG) for assistance, enabling multimodal large model image extraction, resulting in structured data in the power grid safety field. Finally, the extracted results are displayed using a graph database, completing the construction of the multimodal knowledge graph. The constructed knowledge graph includes multimodal data from the construction site, allowing for quick querying of on-site safety incidents, providing safety managers with a valuable tool for site management. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Extraction; Knowledge Graph; Large Language Model; Ontology; Safety Management; Graph Databases; Information Management; Ontology; Project Management; Structured Query Language; Construction Sites; Grid Constructions; Knowledge Extraction; Knowledge Graphs; Language Model; Large Language Model; Multi-modal; Ontology's; Power Grids; Safety Management; Knowledge Graph},
	keywords = {Graph Databases; Information management; Ontology; Project management; Structured Query Language; Construction sites; Grid constructions; Knowledge extraction; Knowledge graphs; Language model; Large language model; Multi-modal; Ontology's; Power grids; Safety management; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Li2024,
	author = {Li, Yingna and Ding, Zhiguo and Yan, Zheng and Li, Zhuahua and Shao, Hang},
	title = {Insider Threat Detection based on Knowledge Graph and Large Language Model},
	year = {2024},
	pages = {},
	doi = {10.1109/DSIT61374.2024.10881842},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000251547&doi=10.1109%2FDSIT61374.2024.10881842&partnerID=40&md5=71973844c9a5df7acdc955c09a1119af},
	abstract = {This article proposes an insider threat detection method based on a combination of knowledge graph and large language model (LLM); first, the internal systems, users, IPs, access behaviors, etc. are modeled through the knowledge graph ontology; then, a few-shot learning information extraction method based on LLMs is used to extract knowledge from the behavior logs to complete the threat detection knowledge graph. Finally, the representation learning method based on the knowledge graph and the embedding based on the LLM are used to extract feature vectors, which are used as input of the insider threat detection model training based on Deep SVDD. The experimental results show that this method can automatically detect abnormal threat behaviors from massive logs at high accuracy, and has the ability to detect deeply hidden abnormal threat behaviors. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep-svdd; Insider Threat Detection; Knowledge Graph; Large Language Model; Contrastive Learning; Graph Embeddings; Zero-shot Learning; Deep-svdd; Detection Methods; Information Extraction Methods; Insider Threat Detections; Internal Systems; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Knowledge Graph},
	keywords = {Contrastive Learning; Graph embeddings; Zero-shot learning; Deep-SVDD; Detection methods; Information extraction methods; Insider threat detections; Internal systems; Knowledge graphs; Language model; Large language model; Ontology's; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lee2024177,
	author = {Lee, Sejin and Kim, Dongha and Song, Min},
	title = {Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot},
	year = {2024},
	pages = {177 - 185},
	doi = {10.1109/ICKG63256.2024.00030},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000199329&doi=10.1109%2FICKG63256.2024.00030&partnerID=40&md5=e836e730709478b16616e45c21443428},
	abstract = {Goal-oriented chatbots are essential for automating user tasks, such as booking flights or making restaurant reservations. A key component of these systems is Dialogue State Tracking (DST), which interprets user intent and maintains the dialogue state. However, existing DST methods often rely on fixed ontologies and manually compiled slot values, limiting their adaptability to open-domain dialogues. We propose a novel approach that leverages instruction tuning and advanced prompt strategies to enhance DST performance, without relying on any predefined ontologies. Our method enables Large Language Model (LLM) to infer dialogue states through carefully designed prompts and includes an anti-hallucination mechanism to ensure accurate tracking in diverse conversation contexts. Additionally, we employ a Variational Graph Auto-Encoder (VGAE) to model and predict subsequent user intent. Our approach achieved state-of-the-art with a JGA of 42.57% outperforming existing ontologyless DST models, and performed well in open-domain real-world conversations. This work presents a significant advancement in creating more adaptive and accurate goal-oriented chatbots. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatbot; Dialogue State Tracking; Goal-oriented Dialogue; Graph Neural Network; Prompt Engineering; Bot (internet); Graph Neural Networks; Ontology; Chatbots; Dialog State Tracking; Goal-oriented; Goal-oriented Dialog; Ontology's; Prompt Engineering; State Tracking; Tracking Method; Tracking Performance},
	keywords = {Bot (Internet); Graph neural networks; Ontology; Chatbots; Dialog state tracking; Goal-oriented; Goal-oriented dialog; Ontology's; Prompt engineering; State tracking; Tracking method; Tracking performance},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{2024,
	title = {IAM 2024 - Proceedings of the 7th International Conference on Informatics and Applied Mathematics},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3922},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85219193931&partnerID=40&md5=b8a375c2e8b656dfafbaecb2a1430040},
	abstract = {The proceedings contain 13 papers. The topics discussed include: a survey on dataset development techniques for CQA systems; a NLP and rule-based approach to extract spatial entities and relationships in Arabic text; sentiment analysis of digital currency discussions: a machine learning and ontology approaches; a hybrid consensus mechanism for enhancing security and efficiency in IoV networks; emergence detection using an fuzzy expert system in complex system; detection and classification of emotion recognition system for TESS and Crema-d audio datasets using hybrid deep learning architecture; advancing cybersecurity with LLMs: a comprehensive review of intrusion detection systems and emerging application; and overview of current trends in machine learning approaches for EEG-based brain computer interface applications. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {DCMI 2024 - Metadata Innovation: Trust, Transformation, and Humanity},
	year = {2024},
	journal = {Proceedings of the International Conference on Dublin Core and Metadata Applications},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218680578&partnerID=40&md5=336abca0b820117e802299f614b6535a},
	abstract = {The proceedings contain 33 papers. The topics discussed include: leveraging generative AI for multilingual thesaurus development: insights from the Confucius ceremony cultural vocabulary; the ontology enhanced multimodal large language models for the knowledge organization and representation of multi-modal cultural memory resources; using LLMs for enriching metadata with links to KOS and knowledge graphs: case Finnish named entity linking; the African knowledge hub: role of the SDG taxonomy (KOS) in harnessing, exploring, and navigating knowledge of the United Nations System – Africa; patent citation link prediction based on graph neural network; exploring patient perspectives on anticipating and mitigating potential harms of LLMs in depression self-management; and metadata modeling based on the one-to-one principle to digitally curate and archive intangible entities in the cultural and historical domains. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Chen2024183,
	author = {Chen, Quanlin and Jia, Jun},
	title = {Research on Event Extraction and Event Relation Extraction for Strategic Operations Analysis},
	year = {2024},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {1266},
	pages = {183 - 198},
	doi = {10.1007/978-981-97-7770-9_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218501855&doi=10.1007%2F978-981-97-7770-9_17&partnerID=40&md5=83e4c31c13dc37b4e113980a60b8333f},
	abstract = {News contains all kinds of political, military, economic, social and other event information of interest to the strategic level, which can be extracted through intelligent methods, and can lay the foundation for the subsequent enhancement of strategic situational awareness capabilities. This paper proposes an information extraction framework and event ontology model for strategic operations research analysis based on the needs and characteristics of strategic operations research analysis. Using the method of “a small amount of manual annotation + fine-tuned large language model annotation”, we construct the information extraction dataset EfSOA for strategic operations research analysis, and propose the event extraction method based on the ABBSAC model and the event relation extraction method combining Roberta and Bi-FLASH-SRU. The experimental results show that the dataset constructed in this paper has complete elements, and the event extraction and event relation extraction methods are better than the comparative model, which enhances the ability of deep mining text information, expands the means of strategic operations research analysis, and effectively improves the comprehensiveness and scientificity of strategic decision-making. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Event Extraction; Event Relation Extraction; Large Language Modeling; Strategic Operations Analysis; Bismuth Alloys; Decision Making; Event Relation Extraction; Events Extractions; Extraction Method; Language Model; Large Language Modeling; Operation Research; Operations Analysis; Relation Extraction; Research Analysis; Strategic Operation Analyze; Modeling Languages},
	keywords = {Bismuth alloys; Decision making; Event relation extraction; Events extractions; Extraction method; Language model; Large language modeling; Operation research; Operations analysis; Relation extraction; Research analysis; Strategic operation analyze; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Fu2024204,
	author = {Fu, Yibin and Ding, Zhaoyun and Xu, Xiaojie},
	title = {LLM & Bagging for 1-shot Joint IE},
	year = {2024},
	pages = {204 - 208},
	doi = {10.1109/DSC63484.2024.00034},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218462496&doi=10.1109%2FDSC63484.2024.00034&partnerID=40&md5=6b62b663ed4b5931142ef282d5f12949},
	abstract = {Domain-specific few-shot information extraction (IE) has always been the difficulty of domain knowledge graph construction, and there is a new solution in this direction after the emergence of large language models (LLM). In this paper, based on previous research, we propose LLM-based 1-shot relation-entity joint IE scheme, and the bagging enhance LLM IE method is proposed to take advantage of the randomness of the LLM output. Against the background of the concept of Internet of Things (IoT) which has received wide attention globally, we selects the IoT interconnective communication as a domain-specific example, crawls the text of the device pages of L3Harris, RockwellCollins as our corpus, selects large language models that differ in the number of parameters and invocation methods to test the proposed joint IE method in relations given by the IoT interconnective communication ontology. The bagging method is tested based on the IE results of GPT-4 Turbo, and there is an improvement of 1-3%, which shows the effectiveness of traditional machine learning methods in LLM. Finally, the results and shortcomings of this study are analyzed. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {1-shot; Bagging; Communication; Information Extraction (ie); Large Language Model (llm); Ontology; Zero-shot Learning; 1-shot; Bagging; Domain Knowledge; Domain Specific; Information Extraction; Information Extraction Methods; Joint Information; Knowledge Graphs; Language Model; Large Language Model; Knowledge Graph},
	keywords = {Ontology; Zero-shot learning; 1-shot; Bagging; Domain knowledge; Domain specific; Information extraction; Information extraction methods; Joint information; Knowledge graphs; Language model; Large language model; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mohsenzadegan202468,
	author = {Mohsenzadegan, Kabeh and Tavakkoli, Vahid and Kambale, Witesyavwirwa Vianney and Kyandoghere, Kyamakya},
	title = {Towards Seamless Data Translation Based on Data Models: A Hybrid AI Framework for Smart Transportation and Manufacturing},
	year = {2024},
	pages = {68 - 73},
	doi = {10.1109/SMAP63474.2024.00022},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218419329&doi=10.1109%2FSMAP63474.2024.00022&partnerID=40&md5=08e348bf0369bfc0b856a8e750ae7c2b},
	abstract = {Interoperability between different data standards is essential for advancing digital technologies in smart manufacturing and transportation. This paper presents a hybrid AI framework that integrates Ontology Learning (OL), Knowledge Graphs (KGs), and Large Language Models (LLMs) to improve data translation across these standards. Focusing on IEEE 1451, ISO 15926, and IEC 61499, we address the challenges posed by these data models' unique structures and semantics. Our comparative analysis evaluates the strengths and limitations of OL, KGs, and LLMs across key metrics like accuracy, scalability, efficiency, robustness, and flexibility. The proposed framework leverages OL for systematic structuring, KGs for relational modeling, and LLMs for linguistic processing, enhancing translation accuracy and adaptability. However, integrating these approaches introduces scalability and processing efficiency trade-offs, particularly in resource-constrained environments. This study contributes to developing more sophisticated and scalable data translation models tailored for heterogeneous data environments, with practical implications for smart manufacturing and transportation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai In Transportation; Cross-standard Data Integration; Data Interoperability; Data Model Translation; Hybrid Ai Framework; Knowledge Graphs (kgs); Large Language Models (llms); Ontology Learning (ol); Semantic Mapping; Smart Manufacturing; Data Accuracy; Data Assimilation; Semantics; Spatio-temporal Data; Ai In Transportation; Cross-standard Data Integration; Data Interoperability; Data Model Translation; Hybrid Ai Framework; Knowledge Graph; Knowledge Graphs; Language Model; Large Language Model; Model Translation; Ontology Learning; Semantics Mappings; Smart Manufacturing},
	keywords = {Data accuracy; Data assimilation; Semantics; Spatio-temporal data; AI in transportation; Cross-standard data integration; Data interoperability; Data model translation; Hybrid AI framework; Knowledge graph; Knowledge graphs; Language model; Large language model; Model translation; Ontology learning; Semantics mappings; Smart manufacturing},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Mou2024443,
	author = {Mou, Yongli and Chen, Hanbin and Lode, Gwendolyn Isabella and Truhn, Daniel and Sowe, Sulayman K. and Decker, Stefan},
	title = {RadLink: Linking Clinical Entities from Radiology Reports},
	year = {2024},
	pages = {443 - 449},
	doi = {10.1109/FLLM63129.2024.10852450},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218350606&doi=10.1109%2FFLLM63129.2024.10852450&partnerID=40&md5=0e228a5ae02d0021073a8c1aa9f6050b},
	abstract = {Radiology reports are a critical source of information for patient diagnosis and treatment in the medical domain. However, the vast amount of data contained in these reports is often unstructured, making it challenging to extract and normalize relevant clinical entities. Named Entity Normalization (NEN) is essential for mapping these entities to a standard ontology, facilitating better data integration, retrieval, and analysis. In this paper, we introduce RadLink, a benchmark for NEN in radiology. RadLink builds upon 425 expert-annotated radiology reports from the RadGraph dataset, extending it for NEN by mapping entities to the Unified Medical Language System (UMLS) ontology. We employ a combination of morphological and semantic matching approaches to generate normalization annotations, followed by human review for validation. We aim to set a standard with our benchmark for evaluating NEN methods in the radiology domain, that facilitate interoperability across healthcare systems and accelerate medical research by providing structured, standardized data. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Named Entity Normalization; Radiology Reports; Benchmarking; Clinical Research; Diagnosis; Medical Information Systems; Patient Treatment; Radiology; Language Model; Large Language Model; Medical Domains; Named Entity Normalizations; Ontology's; Patient Diagnosis; Radiology Reports; Sources Of Informations; System Ontology; Unified Medical Language Systems; Ontology},
	keywords = {Benchmarking; Clinical research; Diagnosis; Medical information systems; Patient treatment; Radiology; Language model; Large language model; Medical domains; Named entity normalizations; Ontology's; Patient diagnosis; Radiology reports; Sources of informations; System ontology; Unified medical language systems; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Oranekwu20246336,
	author = {Oranekwu, Ikechukwu and Elluri, Lavanya and Batra, Gunjan},
	title = {Automated Knowledge Framework for IoT Cybersecurity Compliance},
	year = {2024},
	pages = {6336 - 6345},
	doi = {10.1109/BigData62323.2024.10825755},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218064800&doi=10.1109%2FBigData62323.2024.10825755&partnerID=40&md5=a3faa0751cd49dee16c06424344db196},
	abstract = {Rapid expansion in the manufacture and use of Internet of Things (IoT) devices has introduced significant challenges in ensuring compliance with cybersecurity standards. To protect user data and privacy, all organizations providing IoT devices must adhere to complex guidelines such as the National Institute of Standards and Technology Inter agency Report (NIST IR) 8259, which defines essential cybersecurity guidelines for IoT manufacturers. However, interpreting and applying these rules from these guidelines and the privacy policies remains a significant challenge for companies. Thus, this project presents a novel approach to extract knowledge from NIST 8259 for creating semantically rich ontology mappings. Our ontology captures key compliance rules, which are stored in a knowledge graph (KG) that allows organizations to crosscheck and update privacy policy documents with ease. The KG also enables real-time querying using SPARQL and offers a transparent view of regulatory adherence for IoT manufacturers and users. By automating the process of verifying cybersecurity compliance, the framework ensures that companies remain aligned with NIST standards, eliminating manual checks and reducing the risk of non-compliance. We also demonstrate that compared to the baseline Large Language Models (LLMs), our proposed framework has more compliance accuracy, and is more efficient and scalable. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Automated Compliance; Cybersecurity; Iot; Kgs; Llms; Nist 8259 Standards; Privacy Policies; Regulatory Compliance; Sparql; Knowledge Graph; Regulatory Compliance; Automated Compliance; Cyber Security; Kg; Knowledge Frameworks; Knowledge Graphs; Language Model; Large Language Model; Nist 8259 Standard; Privacy Policies; Sparql; Ontology},
	keywords = {Knowledge graph; Regulatory compliance; Automated compliance; Cyber security; KG; Knowledge frameworks; Knowledge graphs; Language model; Large language model; NIST 8259 standard; Privacy policies; SPARQL; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ho20243401,
	author = {Ho, Duy H. and Das, Udiptaman and Ho, Regina and Lee, Yugyung},
	title = {Leveraging Multi-Agent Systems and Large Language Models for Diabetes Knowledge Graphs},
	year = {2024},
	pages = {3401 - 3410},
	doi = {10.1109/BigData62323.2024.10825608},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218046486&doi=10.1109%2FBigData62323.2024.10825608&partnerID=40&md5=c77248afa42583f64d40767d76a92524},
	abstract = {This paper presents a novel framework for constructing a diabetes-specific knowledge graph (KG) using a streamlined multi-agent system powered by Gemini-based Large Language Models (LLMs). Leveraging insights from the 2016 National Diabetes Survey (NNDS) conducted by the National Diabetes Education Program (NDEP), the framework extracts critical variables related to diagnosis, risk perception, medical advice, and self-management practices across diverse U.S. populations. By processing data from the NNDS's extensive 94-question survey, the methodology performs adaptive ontology mapping using APIs for six major medical standards (e.g., SNOMED CT, ICD-11), ensuring semantic interoperability. Relationships between variables are identified and structured using RDF, RDFS, and OWL standards. The integration of LLMs with ontology tools like Protege enhances automation and scalability. Results demonstrate the framework's effectiveness in generating contextually rich and clinically relevant knowledge graphs, providing a robust foundation for advancing healthcare informatics and personalized diabetes management. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai; Diabetes; Healthcare Informatics; Knowledge Graph; Large Language Models; Multi-agent System; Ontology Mapping; Owl; Rdf; Diagnosis; Diseases; Medical Education; Ontology; Education Programmes; Health Care Informatics; Knowledge Graphs; Language Model; Large Language Model; Multiagent Systems (mass); Ontology Mapping; Owl; Rdf; Specific Knowledge; Knowledge Graph},
	keywords = {Diagnosis; Diseases; Medical education; Ontology; Education programmes; Health care informatics; Knowledge graphs; Language model; Large language model; Multiagent systems (MASs); Ontology mapping; OWL; RDF; Specific knowledge; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Kim20243421,
	author = {Kim, Edward and Shrestha, Manil and Foty, Richard and Delay, Tom and Seyfert-Margolis, Vicki L.},
	title = {Structured Extraction of Real World Medical Knowledge using LLMs for Summarization and Search},
	year = {2024},
	pages = {3421 - 3430},
	doi = {10.1109/BigData62323.2024.10825160},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218046365&doi=10.1109%2FBigData62323.2024.10825160&partnerID=40&md5=1e4a8a263e1891acf94e028d2cdd5621},
	abstract = {Creation and curation of knowledge graphs at scale can be used to exponentially accelerate the discovery, matching, and analysis of diseases in real-world data. While disease ontologies are useful for annotation, integration, and analysis of biological data, codified disease and procedure categories e.g. SNOMED-CT, ICD10, CPT, etc. rarely capture all of the nuances in a patient condition or, in the case of rare disease, may not even exist. Furthermore, there are multiple disease definitions used in data sources and publications, each having its own structure and hierarchy. Mapping between ontologies, finding disease clusters, and building a representation of the chosen disease area are resource-intensive, often requiring significant human capital. We propose the creation and curation of a patient knowledge graph utilizing large language model extraction techniques. In order to expand in volume and scale, knowledge graphs with generalized language capability allow for data to be extracted using natural language rather than being constrained by the exact terminology or hierarchy of existing ontologies. We develop a method of mapping back to existing ontologies such as MeSH, SNOMED-CT, RxNORM, HPO, etc. to ground the extracted entities to known entities in the medical community.We have access to one of the largest ambulatory care EHR databases in the country. To demonstrate the effectiveness of our method, we benchmark our extraction in a test set with over 33.6M unique patients, in the area of patient search. In this case study, we perform a patient search for a rare disease: Dravet syndrome. Dravet syndrome was codified as an ICD10 recognizable disease in October 2020. In the following research, we describe our method of the construction of patient-specific knowledge graphs and subsequent searches for patients who exhibit symptoms of a particular disease. Using patients with confirmed ICD10 codes for Dravet syndrome as our ground truth, we utilize our LLM-based entity extraction techniques and formalize an algorithmic way of characterizing patients in a grounded ontology to assist in mapping patients to specific diseases. Finally, we present the results of a real-world discovery method on Beta-propeller protein-associated neurodegeneration (BPAN), identifying patients with a rare disease, where no ground truth currently exists. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Beta-propeller Protein-associated Neurodegeneration (bpan); Dravet Syndrome; Knowledge Graphs; Large Language Models; Ontology Mapping; Structured Extraction; Arthroplasty; Benchmarking; Data Curation; Decision Trees; Disease Control; Flow Visualization; Fracture Fixation; Neurodegenerative Diseases; Ontology; Photomapping; Propellers; Steganography; Beta-propeller Protein-associated Neurodegeneration; Dravet Syndrome; Knowledge Graphs; Language Model; Large Language Model; Neurodegeneration; Ontology Mapping; Ontology's; Real-world; Structured Extraction; Knowledge Graph},
	keywords = {Arthroplasty; Benchmarking; Data curation; Decision trees; Disease control; Flow visualization; Fracture fixation; Neurodegenerative diseases; Ontology; Photomapping; Propellers; Steganography; Beta-propeller protein-associated neurodegeneration; Dravet syndrome; Knowledge graphs; Language model; Large language model; Neurodegeneration; Ontology mapping; Ontology's; Real-world; Structured extraction; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Linxen20242449,
	author = {Linxen, Andrea and Schmidt, Vera Maria and Klinke, Harald and Beecks, Christian},
	title = {Ontology-driven knowledge base for digital humanities: Restructuring knowledge organization at the library of the Folkwang University of the Arts},
	year = {2024},
	pages = {2449 - 2455},
	doi = {10.1109/BigData62323.2024.10825984},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217992027&doi=10.1109%2FBigData62323.2024.10825984&partnerID=40&md5=e53d737ab95fdc6a61b8464689e3814b},
	abstract = {Academic libraries are increasingly challenged by the need to efficiently manage and analyse vast collections of data and knowledge. The divers formats and organisation methods of these collections, ranging from traditional print media to digital archives and multimedia assets, can hinder researchers' ability to easily access and retrieve relevant information. This paper introduces an ontology-driven knowledge base to address this issue by enabling the efficient access to knowledge in the application domain and enhancing the semantic search capabilities in the field of Digital Humanities. Our approach focuses on the development of an ontology-drive knowledge base for semantic search in academic libraries by the example of the library of the Folkwang University of Arts that captures the knowledge concepts present in the library's archival collections. The resulting ontology framework provides a structured representation of domain knowledge, facilitating the integration of diverse data sources, including structured, semi-structured, and unstructured data from the application domain into a triple store knowledge base. By leveraging SPARQL queries generated from Large Language Model (LLM) prompts, we aim to facilitate more intuitive and effective knowledge retrieval. This approach allows users to express their information needs in a more natural and flexible way, leading to more accurate and relevant search results. We evaluate the proposed ontology-driven knowledge base in terms of its integrity, consistency, flexibility, relevance, and scalability. Our evaluation methodology includes a combination of verification and validation techniques, including automated reasoners and query results based on competence questions. Our findings demonstrate the potential of ontology engineering to enhance complex information retrieval in academic libraries. However, we also identify limitations related to processing speed for complex queries and the quality of search results. This research contributes to the field of computational archival science by providing a novel approach to semantic search in academic libraries. By enabling more precise and efficient access to knowledge, our ontology-driven knowledge base has the potential to enrich the academic and Digital Humanities landscape, empowering researchers to delve deeper into the vast resources available within these institutions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Digital Humanities; Knowledge Base; Knowledge Engineering; Ontology Framework; Semantic Search; Domain Knowledge; Humanities Computing; Information Retrieval; Knowledge Engineering; Ontology; Query Languages; Semantics; Academic Libraries; Applications Domains; Digital Humanities; Digital Multimedia; Knowledge Base; Knowledge Organization; Ontology Framework; Ontology's; Print Media; Semantic Search; Structured Query Language},
	keywords = {Domain Knowledge; Humanities computing; Information retrieval; Knowledge engineering; Ontology; Query languages; Semantics; Academic libraries; Applications domains; Digital humanities; Digital multimedia; Knowledge base; Knowledge organization; Ontology framework; Ontology's; Print media; Semantic search; Structured Query Language},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Mankari2024,
	author = {Mankari, Sagar and Sanghavi, Abhishek},
	title = {Enhancing Vector based Retrieval Augmented Generation with Contextual Knowledge Graph Construction},
	year = {2024},
	pages = {},
	doi = {10.1109/IDICAIEI61867.2024.10842699},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217854156&doi=10.1109%2FIDICAIEI61867.2024.10842699&partnerID=40&md5=640d200493915690a5a654ada13c4c03},
	abstract = {The proliferation of unstructured text data necessitates efficient information retrieval systems. Traditional vector-based Retrieval Augmented Generation (RAG) models often fail to capture complex relationships and contextual nuances, limiting effectiveness in knowledge-intensive tasks. We introduce Contextual Knowledge Graph Construction (CKGC), a novel approach enhancing vector-based RAG by dynamically building a knowledge graph that reflects inherent data structures and connections.CKGC leverages text chunking, large language models (LLMs), and ontology mapping. By segmenting text and using LLMs to identify key entities and relationships, CKGC constructs a contextualized knowledge graph enriching information representation. This bridges the gap between semantic similarity and deeper contextual understanding, enabling more accurate and nuanced retrieval.Experiments on 2,000 lease agreements demonstrate that CKGC significantly improves vector-based RAG in information retrieval and question answering tasks, with substantial gains in Mean Reciprocal Rank (MRR) and Top-k Accuracy. CKGC's adaptability across domains positions it as a valuable tool for enhancing performance and understanding of complex textual data. Our findings underscore CKGC's transformative potential in unlocking insights from vast text corpora, paving the way for more intelligent and context-aware information retrieval systems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Information Retrieval; Knowledge Graph Construction; Ontology Mapping; Retrieval Augmented Generation; Content Based Retrieval; Mapping; Ontology; Semantics; Contextual Knowledge; Graph Construction; Information-retrieval Systems; Knowledge Graph Construction; Knowledge Graphs; Language Model; Ontology Mapping; Retrieval Augmented Generation; Text Data; Unstructured Texts; Knowledge Graph},
	keywords = {Content based retrieval; Mapping; Ontology; Semantics; Contextual knowledge; Graph construction; Information-retrieval systems; Knowledge graph construction; Knowledge graphs; Language model; Ontology mapping; Retrieval augmented generation; Text data; Unstructured texts; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Jinghong20244936,
	author = {Jinghong, Li and Phan, Huy and Gu, Wen and Ota, Kouichi and Shinobu Hasegawa, Shinobu},
	title = {Fish-Bone Diagram of Research Issue: Gain a Bird's-Eye View on a Specific Research Topic},
	year = {2024},
	journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
	pages = {4936 - 4941},
	doi = {10.1109/SMC54092.2024.10830995},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217833147&doi=10.1109%2FSMC54092.2024.10830995&partnerID=40&md5=7b09adfbb70ce24c77bc4685f35e780a},
	abstract = {Novice researchers often face difficulties in understanding a multitude of academic papers and grasping the fundamentals of a new research field. To solve such problems, the knowledge graph supporting research survey is gradually being developed. Existing keyword-based knowledge graphs make it difficult for researchers to deeply understand abstract concepts. Meanwhile, novice researchers may find it difficult to use ChatGPT effectively for research surveys due to their limited understanding of the research field. Without the ability to ask proficient questions that align with key concepts, obtaining desired and accurate answers from this large language model (LLM) could be inefficient. This study aims to help novice researchers by providing a fish-bone diagram that includes causal relationships, offering an overview of the research topic. The diagram is constructed using the issue ontology from academic papers, and it offers a broad, highly generalized perspective of the research field, based on relevance and logical factors. Furthermore, we evaluate the strengths and improvable points of the fish-bone diagram derived from this study's development pattern, emphasizing its potential as a viable tool for supporting research survey. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Abstract Concept; Academic Paper; Bird's Eye View; Fishbone Diagrams; Keyword-based; Knowledge Graphs; Research Fields; Research Issues; Research Survey; Research Topics},
	keywords = {Knowledge graph; Abstract concept; Academic paper; Bird's eye view; Fishbone diagrams; Keyword-based; Knowledge graphs; Research fields; Research issues; Research survey; Research topics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Sun202414272,
	author = {Sun, Jimin and Min, So-yeon and Chang, Yingshan and Bisk, Yonatan},
	title = {Tools Fail: Detecting Silent Errors in Faulty Tools},
	year = {2024},
	pages = {14272 - 14289},
	doi = {10.18653/v1/2024.emnlp-main.790},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217789356&doi=10.18653%2Fv1%2F2024.emnlp-main.790&partnerID=40&md5=742a527fbd5f205e055b99d86e9c7411},
	abstract = {Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not in their weights, to perform tasks on the web, and even to control robots. However, most ontologies and surveys of tool-use have assumed the core challenge for LLMs is choosing the tool. Instead, we introduce a framework for tools more broadly which guides us to explore a model's ability to detect “silent” tool errors, and reflect on how to plan. This more directly aligns with the increasingly popular use of models as tools. We provide an initial approach to failure recovery with promising results both on a controlled calculator setting and embodied agent planning. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Control Robots; Embodied Agent; Failure Recovery; Modeling Abilities; Ontology's; Setting Agents; Tool Error; Tool Use; Computational Linguistics},
	keywords = {Control robots; Embodied agent; Failure recovery; Modeling abilities; Ontology's; Setting agents; Tool error; Tool use; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Wang20242503,
	author = {Wang, Keyu and Qi, Guilin Lin and Li, Jiaqi and Zhai, Songlin},
	title = {Can Large Language Models Understand DL-Lite Ontologies? An Empirical Study},
	year = {2024},
	pages = {2503 - 2519},
	doi = {10.18653/v1/2024.findings-emnlp.141},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217620022&doi=10.18653%2Fv1%2F2024.findings-emnlp.141&partnerID=40&md5=8d8bc155649a7cf2fc35f64030909200},
	abstract = {Large language models (LLMs) have shown significant achievements in solving a wide range of tasks. Recently, LLMs' capability to store, retrieve and infer with symbolic knowledge has drawn a great deal of attention, showing their potential to understand structured information. However, it is not yet known whether LLMs can understand Description Logic (DL) ontologies. In this work, we empirically analyze the LLMs' capability of understanding DL-Lite ontologies covering 6 representative tasks from syntactic and semantic aspects. With extensive experiments, we demonstrate both the effectiveness and limitations of LLMs in understanding DL-Lite ontologies. We find that LLMs can understand formal syntax and model-theoretic semantics of concepts and roles quite well. However, LLMs struggle with understanding TBox NI (Negative Inclusion) transitivity and handling ontologies with large ABoxes. We hope that our experiments and analyses provide more insights into LLMs and inspire to build more faithful knowledge engineering solutions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Engineering; Latent Semantic Analysis; Ontology; Semantics; Syntactics; Description Logic; Empirical Studies; Experiment And Analysis; Formal Modeling; Formal Syntaxes; Language Model; Model-theoretic Semantics; Ontology's; Structured Information; Symbolic Knowledge; Computational Linguistics},
	keywords = {Knowledge engineering; Latent semantic analysis; Ontology; Semantics; Syntactics; Description logic; Empirical studies; Experiment and analysis; Formal modeling; Formal syntaxes; Language model; Model-theoretic semantics; Ontology's; Structured information; Symbolic knowledge; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Li20248316,
	author = {Li, Na and Bailleux, Thomas and Bouraoui, Zied and Schockaert, Steven},
	title = {CONTOR: Benchmarking Strategies for Completing Ontologies with Plausible Missing Rules},
	year = {2024},
	pages = {8316 - 8334},
	doi = {10.18653/v1/2024.findings-emnlp.488},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217619692&doi=10.18653%2Fv1%2F2024.findings-emnlp.488&partnerID=40&md5=7a78c2a7b4f92cab2fdc05cef8ffecdc},
	abstract = {We consider the problem of finding plausible rules that are missing from a given ontology.A number of strategies for this problem have already been considered in the literature.Little is known about the relative performance of these strategies, however, as they have thus far been evaluated on different ontologies.Moreover, existing evaluations have focused on distinguishing held-out ontology rules from randomly corrupted ones, which often makes the task unrealistically easy and leads to the presence of incorrectly labelled negative examples.To address these concerns, we introduce a benchmark with manually annotated hard negatives and use this benchmark to evaluate ontology completion models.In addition to previously proposed models, we test the effectiveness of several approaches that have not yet been considered for this task, including LLMs and simple but effective hybrid strategies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology; Hybrid Strategies; Negative Examples; Ontology's; Relative Performance; Simple++; Computational Linguistics},
	keywords = {Ontology; Hybrid strategies; Negative examples; Ontology's; Relative performance; Simple++; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Dimitrakopoulos20241955,
	author = {Dimitrakopoulos, George J. and Ehm, Hans and Tsaousi, Eleni},
	title = {ENHANCED ONTOLOGY EXTRACTION: INTEGRATING GPT AI WITH HUMAN KNOWLEDGE ON THE EXAMPLE OF EU STANDARDS RELATED TO SEMICONDUCTOR SUPPLY CHAINS},
	year = {2024},
	journal = {Proceedings - Winter Simulation Conference},
	pages = {1955 - 1965},
	doi = {10.1109/WSC63780.2024.10838760},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217617379&doi=10.1109%2FWSC63780.2024.10838760&partnerID=40&md5=8e67c6383c556395af5749406deb7bf3},
	abstract = {This paper addresses challenges in creating ontologies for the semiconductor supply chain. Ontologies are crucial for seamless data exchange within the semantic web, enabling initiatives like GAIA-X and CatenA-X. Traditionally, ontology creation is complex. Here, we propose a novel AI-assisted method using large language models (LLMs) like ChatGPT 4 Turbo to support human experts. This collaboration aims to expedite ontology generation while maintaining quality. While initial tests show promise, refining the human-AI interface for clear content generation remains a focus. By improving this collaboration, we expect to create more accurate and complete ontologies, fostering efficient information sharing and strengthening the meaningfulness of standards within the semiconductor supply chain. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Modeling Languages; Ontology; Semiconducting Indium Phosphide; Human Expert; Human Knowledge; Language Model; Ontology Creations; Ontology Extraction; Ontology Generation; Ontology's; Semantic-web; Semiconductor Supply Chain; Web-enabling; Semantics},
	keywords = {Modeling languages; Ontology; Semiconducting indium phosphide; Human expert; Human knowledge; Language model; Ontology creations; Ontology Extraction; Ontology generation; Ontology's; Semantic-Web; Semiconductor supply chain; Web-enabling; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Heddaya202467,
	author = {Heddaya, Mourad and Zeng, Qingcheng and Tan, Chenhao and Voigt, Rob and Zentefis, Alexander K.},
	title = {Causal Micro-Narratives},
	year = {2024},
	pages = {67 - 84},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217564641&partnerID=40&md5=567adfcdff6444eda39a2112ed76e704},
	abstract = {We present a novel approach to classify causal micro-narratives from text. These narratives are sentence-level explanations of the cause(s) and/or effect(s) of a target subject. The approach requires only a subject-specific ontology of causes and effects, and we demonstrate it with an application to inflation narratives. Using a human-annotated dataset spanning historical and contemporary US news articles for training, we evaluate several large language models (LLMs) on this multi-label classification task. The best-performing model—a fine-tuned Llama 3.1 8B—achieves F1 scores of 0.87 on narrative detection and 0.71 on narrative classification. Comprehensive error analysis reveals challenges arising from linguistic ambiguity and highlights how model errors often mirror human annotator disagreements. This research establishes a framework for extracting causal micro-narratives from real-world data, with wide-ranging applications to social science research. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Annotated Datasets; Cause And Effects; Classification Tasks; Language Model; Multi-label Classifications; News Articles; Ontology's; S Effect; Sentence Level; Subject-specific; Computational Linguistics},
	keywords = {Annotated datasets; Cause and effects; Classification tasks; Language model; Multi-label classifications; News articles; Ontology's; S effect; Sentence level; Subject-specific; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Armary2024837,
	author = {Armary, Pauline and El-Vaigh, Cheikh Brahim and Spicher, Antoine and Labbani, Ouassila and Nicolle, Christophe},
	title = {Identifying Logical Patterns in Text for Reasoning},
	year = {2024},
	pages = {837 - 844},
	doi = {10.1109/ICTAI62512.2024.00122},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217432359&doi=10.1109%2FICTAI62512.2024.00122&partnerID=40&md5=57088e55fc285fbc3412ea1f7c228efb},
	abstract = {Translating unstructured text into logical format is a key challenge for building ontologies automatically and addressing deductive inference. Most of the approaches have tackled the identification of concepts and relations in text, but few of them have addressed the most complex axioms like class expression subsumption. This work proposes DeLIR, a neuro-symbolic approach to identify complex logical patterns in text by combining a grammatical translation of dependency parsing trees and a fine-tuned Large language Model (LLM). DeLIR combines the strength of the parsing accuracy provided by a grammatical approach and pattern flexibility provided by a finetuned LLM. We evaluated our approach on FOLIO dataset for both translation capacity and inference capability. Our grammatical approach has a perfect parsing accuracy and combining the grammatical approach with LLMs improves the LLMS translation capacity: tinyLlama, T5-small-text2logic, Llama-7B and Mistral-7B. We also evaluate the inference capacity of the different LLMs. Mistral-7B, while being smaller than the state-of-the-art approach using GPT-4, presents similar results to predict the correct inference labels. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Natural Language Inference; Ontology Learning; Translation To Logic; Computer Aided Language Translation; Computer Circuits; Contrastive Learning; Dependency Parsing; Language Inference; Language Model; Natural Language Inference; Natural Languages; Ontology Learning; Ontology's; State-of-the-art Approach; Translation To Logic; Unstructured Texts; Ontology},
	keywords = {Computer aided language translation; Computer circuits; Contrastive Learning; Dependency parsing; Language inference; Language model; Natural language inference; Natural languages; Ontology learning; Ontology's; State-of-the-art approach; Translation to logic; Unstructured texts; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Adjei-Frempah2024485,
	author = {Adjei-Frempah, Douglas and Chen, Lisa and LePendu, Paea},
	title = {KB2Bench: Toward a Benchmark Framework for Large Language Models on Medical Knowledge},
	year = {2024},
	pages = {485 - 493},
	doi = {10.1109/ICTAI62512.2024.00075},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217405922&doi=10.1109%2FICTAI62512.2024.00075&partnerID=40&md5=377fc09faaa1bd5fb481d0e449248ca8},
	abstract = {While Large Language Models (LLMs) have trans-formed question answering tasks, their propensity for hallucinations continues to drive an area of active research. Efforts toward creating benchmarks to test LLMs' performance on queries, in particular for the field of medicine, have led to a few reputable benchmarks, but these are limited in scope because of the amount of human annotation required. Our framework addresses this issue by leveraging existing, large knowledge bases for medicine to generate vast query and answer sets dynamically, which are less likely to be memorized by LLMs. The framework rests on designing a few key knowledge patterns, which can then generate millions (potentially billions) of queries. This offers a more efficient, cost-effective, and scalable alternative to human-curated annotations used in medical question-and-answer benchmarks. Applying our framework to a small sample of five drug related ontologies, we are already capable of more than 100,000 unique drug related queries, which is 10 to 1000 times larger than existing various human annotation efforts. This paper introduces the KB2Bench framework. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmarking; Biomedical Informatics; Knowledge Bases; Knowledge Representation And Reasoning; Large Language Models; Ontologies; Vocabularies; Benchmarking; Medical Informatics; Ontology; Query Languages; Structured Query Language; Biomedical Informatics; Human Annotations; Knowledge Base; Knowledge Representation And Reasoning; Language Model; Large Language Model; Medical Knowledge; Ontology's; Question Answering Task; Vocabulary; Knowledge Representation},
	keywords = {Benchmarking; Medical informatics; Ontology; Query languages; Structured Query Language; Biomedical informatics; Human annotations; Knowledge base; Knowledge representation and reasoning; Language model; Large language model; Medical knowledge; Ontology's; Question Answering Task; Vocabulary; Knowledge representation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Saini2024181,
	author = {Saini, Anmol and Ethier, Jeffrey G. and Shimizu, Cogan Matthew},
	title = {An Ontology for Conversations with Virtual Research Assistants},
	year = {2024},
	pages = {181 - 186},
	doi = {10.1109/ICTAI62512.2024.00034},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217375666&doi=10.1109%2FICTAI62512.2024.00034&partnerID=40&md5=14f6195a6fb341b084b05f06ba9ba51c},
	abstract = {Conversational artificial intelligence has expanded rapidly in recent years, especially with the growth of large language models (LLMs). Its incorporation in scientific research in the form of research assistants has also become more common-place but remains limited in some capacities, such as in the realm of polymer science. The limitations of LLMs, especially in terms of domain knowledge, warrant the need for other tools, such as knowledge graphs (KGs), to better guide conversations. While such conversational models have been developed in the past, they are generally restricted to particular domains and lack the ability to integrate semantics from various kinds of conversations. Thus, we make progress toward the construction of a universal conversational model that has a focus on the materials domain by combining aspects of existing models. We aim to implement it in such a way that renders it amenable to modifications and usable in a variety of situations. We posit that this model will be adopted and extended by others seeking to accomplish a similar goal in the future. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Conversational Model; Knowledge Graph; Large Language Model; Ontology; Ontology Design Pattern; Polymer Science; Ontology; Semantics; Conversational Model; Design Patterns; Knowledge Graphs; Language Model; Large Language Model; Ontology Design; Ontology Design Pattern; Ontology's; Polymer Science; Virtual Research; Knowledge Graph},
	keywords = {Ontology; Semantics; Conversational model; Design Patterns; Knowledge graphs; Language model; Large language model; Ontology design; Ontology design pattern; Ontology's; Polymer science; Virtual research; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Yang20245240,
	author = {Yang, Hang and Xiao, Liang and Zhu, Rujun and Liu, Ziji and Chen, Jianxia},
	title = {An LLM supported approach to ontology and knowledge graph construction},
	year = {2024},
	pages = {5240 - 5246},
	doi = {10.1109/BIBM62325.2024.10822222},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217280910&doi=10.1109%2FBIBM62325.2024.10822222&partnerID=40&md5=5fb25a53e38880819cbef3caab99f1a6},
	abstract = {The continuous development in the medical field faces multiple challenges in managing a large amount of literature and research results using traditional ontology and knowledge graph construction methods. These challenges include high labor costs, limited coverage, and poor dynamism of traditional ontology and knowledge graph construction methods. Large language models (LLMs) can solve various natural language processing tasks and can understand and generate human-like natural language, which makes automated construction of ontology expansion and knowledge graphs (KGs) possible. This paper proposes an ontology expansion method based on LLMs, using LLMs to formulate competency questions (CQs) to extend the initial ontology, and then constructing the knowledge graph based on the extended ontology. We demonstrated the feasibility of the method by creating a knowledge graph for breast cancer treatment. The combination of LLMs-based medical ontology and knowledge graph can achieve more efficient medical knowledge management and application, promoting the informatization and intelligent development of the medical field. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Breast Cancer Treatment; Knowledge Graph; Llm; Ontology; Graphitization; Natural Language Processing Systems; Ontology; Wages; Breast Cancer Treatment; Graph-construction Method; Knowledge Graphs; Language Model; Large Language Model; Medical Fields; Medical Knowledge; Natural Languages; Ontology Graphs; Ontology's; Knowledge Graph},
	keywords = {Graphitization; Natural language processing systems; Ontology; Wages; Breast cancer treatment; Graph-construction method; Knowledge graphs; Language model; Large language model; Medical fields; Medical knowledge; Natural languages; Ontology graphs; Ontology's; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Qi20241135,
	author = {Qi, Jiewei and Luo, Ling and Yang, Zhihao and Wang, Jian and Zhou, Huiwei and Lin, Hongfei},
	title = {An Improved Method for Phenotype Concept Recognition Using Rich HPO Information},
	year = {2024},
	pages = {1135 - 1140},
	doi = {10.1109/BIBM62325.2024.10822556},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217280863&doi=10.1109%2FBIBM62325.2024.10822556&partnerID=40&md5=1a5a2eaea86f6b840c3b2833d8c1f091},
	abstract = {Automatically identifying human phenotype ontology (HPO) concepts from text is important for disease analysis. Existing ontology-driven methods for phenotype concept recognition mainly rely on concept names and synonym information from the ontology, without fully exploiting the rich ontology information. In this paper, we present an improved phenotype concept recognition method by incorporating rich HPO information. We first design prompts with HPO information and use a cutting-edge large language model GPT-4 to generate synonym augmentation for expanding distant supervised training data. We then propose an ontology vector-enhanced phenotype concept classification model to efficiently integrate the taxonomic hierarchical structure of HPO. Additionally, we employ noisy data augmentation to improve the model's recognition ability in noisy texts and implement a negation detection function. Experimental results on three standard corpora and two typo corpora show our method compares favorably to previous methods and achieves a significant improvement in noisy texts. The source code and data are freely available at https://github.com/DUTIR-BioNLP/PhenoTagger-Updates. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Human Phenotype Ontology; Ontology Information Enhancement; Phenotype Concept Recognition; Concept Recognition; Disease Analysis; First Designs; Human Phenotype Ontology; Ontology Concepts; Ontology Information Enhancement; Ontology's; Phenotype Concept Recognition; Recognition Methods},
	keywords = {Concept recognition; Disease analysis; First designs; Human phenotype ontology; Ontology concepts; Ontology information enhancement; Ontology's; Phenotype concept recognition; Recognition methods},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Chen20245616,
	author = {Chen, Dehua and Shen, Zijian and Wang, Mei and Dong, Na and Qiao, Pan and Su, Jianwen},
	title = {Extracting Structure Information from Narrative Medical Reports based on LLMs},
	year = {2024},
	pages = {5616 - 5623},
	doi = {10.1109/BIBM62325.2024.10822688},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217280093&doi=10.1109%2FBIBM62325.2024.10822688&partnerID=40&md5=2ef76c9f7b1b72efc12493a0c6955074},
	abstract = {Extracting structured information and key details from medical report narratives is crucial to support healthcare data management, analysis and decision-making. However, the specialized nature of the reports, the complexity of the contents, and the high accuracy requirements of the results pose significant challenges to the structuring task. In this paper, we develop an LLM-based method to extract structure information from medical report narratives. Defining the structuring problem as mapping the narrative reports to the domain ontology, we design a framework to develop specialized LLMs that automatically learn and establish the mappings. At the core of this framework are report partitioning and interactive training data generation modules are. By separating complete reports into logically independent segments and training the LLMs on these segments independently, the trained LLMs can accurately capture the semantic relationships within each segment. Additionally, we explore different LLMs and formulate a simplistic scoring method to compare their accuracy, enabling us to select the best-performing model. Experimental evaluation on a real-world breast ultrasound report dataset demonstrates that our method achieves high accuracy with a small training dataset (400 samples). Specifically, the accuracy of structural information extraction and the attribute-value matching accuracy both exceed 96%. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Medical Examination Reports; Report Structuring; Ontology; Semantics; Data Decision; High-accuracy; Language Model; Large Language Model; Management Analysis; Management Decisions; Medical Examination Report; Report Structuring; Structure Information; Structured Information; Data Accuracy},
	keywords = {Ontology; Semantics; Data decision; High-accuracy; Language model; Large language model; Management analysis; Management decisions; Medical examination report; Report structuring; Structure information; Structured information; Data accuracy},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Postiglione2024314,
	author = {Postiglione, Alberto},
	title = {Linguistic Text Mining},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {398},
	pages = {314 - 331},
	doi = {10.3233/FAIA241433},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217166575&doi=10.3233%2FFAIA241433&partnerID=40&md5=0b52c14b857df57f02126191e18a6da0},
	abstract = {This paper explores the challenges of processing the increasing volume of natural language text, which often surpasses traditional methods’ real-time processing abilities. These texts are typically authored by individuals from diverse educational, cultural, and experiential backgrounds. The paper highlights the main linguistic and semantic issues that arise in the analysis of natural language text. Linguistic Text Mining is a computational approach that combines linguistic principles with computational techniques to extract high-quality information from natural language texts. Despite the frequent mentions of “Linguistic” and “Text-Mining” in scientific literature, no formal definition exists; this paper proposes one. It further explores LTMs potential in enhancing knowledge extraction by emphasizing linguistic features, such as multi-word units (MWUs). Traditional text analysis relies heavily on statistical methods, focusing on simple words, which are often polysemous, or on aggregating words without semantic context and this limits systems’ ability to interpret domain-specific semantics. By contrast, MWUs, like “credit card”, convey specific, unambiguous meanings, critical for identifying specialized domains. MWUs are typically organized within ontologies that represent distinct knowledge domains. Building on previous work, the study compares AUTOMETA, an ontology-based approach using finite automata for MWU identification, with large language model (LLM)-based and other ontology-driven Linguistic Text Mining methods. Findings suggest that integrating linguistic frameworks significantly improves information extraction, offering a deeper understanding of complex language structures. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Finite Automata; Linguistic; Linguistic Text Mining; Multi-word Unit; Natural Language Processing; Text Mining; Computational Linguistics; Finite Automata; Latent Semantic Analysis; Natural Language Processing Systems; Ontology; Language Processing; Linguistic Text Mining; Multiword Units; Natural Language Processing; Natural Languages; Natural Languages Texts; Ontology's; Realtime Processing; Text-mining; Semantics},
	keywords = {Computational linguistics; Finite automata; Latent semantic analysis; Natural language processing systems; Ontology; Language processing; Linguistic text mining; Multiword units; Natural language processing; Natural languages; Natural languages texts; Ontology's; Realtime processing; Text-mining; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lopes2024,
	author = {Lopes, Alcides Gonçalves and Carbonera, Joel Lúis and Rodrigues, Fabrício Henrique Henrique and Garcia, Luan Fonseca and Abel, Mara},
	title = {Unveiling influential factors in classifying domain entities into top-level ontology concepts: an analysis using GO and ChEBI ontologies},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3905},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217157524&partnerID=40&md5=7bdaf4a1ee1e1cc8882040e74b392dac},
	abstract = {In the realm of ontology engineering, accurately classifying domain entities into top-level ontology concepts is a critical task, with significant implications in the time and effort required to build ontologies from scratch. This paper delves into the influential factors affecting the performance of using informal definitions to represent domain entities textually, Language Models to represent these definitions as embedding vectors, and the K-Nearest Neighbors (KNN) algorithm to classify these embeddings into top-level ontology concepts. Also, we particularly focused on the Gene Ontology (GO) and Chemical Entities of Biological Interest (ChEBI) ontologies. We hypothesize that the embedding representation of informal definitions of highly specialized domains may present different behaviors regarding their proximity with other informal definitions of other domains, influencing the predicted top-level ontology concept. To test our hypothesis, we conducted a series of experiments using variations on the number of GO and ChEBI domain entities in the training sample of our classifier. Our results indicate that the relation between the proximity of domain entities in the embedding space and the top-level ontology concept of these domain entities varies according to the domain specificity. Also, this result is strongly influenced by how ontology developers write the informal definitions in each domain. The findings underscore the potential of informal definitions in reflecting top-level ontology concepts and point toward using consolidated domain entities in a domain ontology during the training stage of the classifier. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Informal Definition; Language Model; Ontology Learning; Top-level Ontology Classification; Classification (of Information); Embeddings; Modeling Languages; Nearest Neighbor Search; Chemical Entity; Domain Entities; Gene Ontology; Informal Definition; Language Model; Ontology Concepts; Ontology Learning; Ontology's; Top-level Ontology Classification; Gene Ontology},
	keywords = {Classification (of information); Embeddings; Modeling languages; Nearest neighbor search; Chemical entity; Domain entities; Gene ontology; Informal definition; Language model; Ontology concepts; Ontology learning; Ontology's; Top-level ontology classification; Gene Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Soares2024,
	author = {Soares, Vivian Magri A. and Wassermann, Renata},
	title = {Ontology extraction and evaluation for the Blue Amazon},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3905},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217154037&partnerID=40&md5=b2a209ef181b53111c5b31849a3da165},
	abstract = {The Brazilian maritime territory, often referred to as Blue Amazon, has invaluable significance for its resources, biodiversity, commercial importance etc. Yet, information about it is disperse. This project is geared towards the organization of knowledge on the form of an ontology. Searching for efficient methods with satisfying results for this task, a recent approach involving Large Language Models (LLMs) in the role of experts for building a conceptual hierarchy has shown promising results. This work presents the proposal for the experimentation with the construction of an ontology about the Blue Amazon related concepts using LLMs, followed by human and application-based evaluations. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Blue Amazon; Large Language Models; Ontology Evaluation; Ontology Extraction; Blue Amazon; Conceptual Hierarchy; Language Model; Large Language Model; Ontology Evaluations; Ontology Extraction; Ontology's; Organization Of Knowledge; Ontology},
	keywords = {Blue amazon; Conceptual hierarchy; Language model; Large language model; Ontology evaluations; Ontology Extraction; Ontology's; Organization of knowledge; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Garbacz202489,
	author = {Garbacz, Pawel},
	title = {Large Language Models and Foundational Ontologies},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {394},
	pages = {89 - 103},
	doi = {10.3233/FAIA241296},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217048189&doi=10.3233%2FFAIA241296&partnerID=40&md5=3f935f7c2ce5d3f67d5dd43983ea066d},
	abstract = {Large language models are capable of translating natural language texts into context-free grammar languages. The paper presents an initial assessment of whether such models can be used to produce ontological theories that formalise natural language descriptions of certain situations. More specifically speaking, I will focus on translating a small set of natural language descriptions of some situations of ontological interests into a fixed formal ontological framework. The model I use will not be trained or fine-tuned for this purpose but prompted. In order to build the appropriate prompts I will take advantage of the formalisations from the 17th volume of the Applied Ontology journal, where six examples of such situations were formalised within the context of seven upper-level formal ontologies. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clif; Formal Ontology; Formalisation; Large Language Model; Computer Aided Language Translation; Context Free Grammars; Context Sensitive Grammars; Modeling Languages; Ontology; Clif; Context-free Grammars; Formal Ontology; Formalisation; Foundational Ontologies; Language Description; Language Model; Large Language Model; Natural Languages; Natural Languages Texts; Context Free Languages},
	keywords = {Computer aided language translation; Context free grammars; Context sensitive grammars; Modeling languages; Ontology; CLIF; Context-free grammars; Formal ontology; Formalisation; Foundational ontologies; Language description; Language model; Large language model; Natural languages; Natural languages texts; Context free languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {Formal Ontology in Information Systems - Proceedings of the 14th International Conference, FOIS 2024},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {394},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217038884&partnerID=40&md5=b3736921c1d36792a77f78329af1f2a9},
	abstract = {The proceedings contain 19 papers. The topics discussed include: hanging around: cognitive inspired reasoning for reactive robotics; Core-O: a competence reference ontology for professional and learning ecosystems; OnNER: an ontology for semantic representation of named entities in scholarly publications; large language models and foundational ontologies; ontology as structure, domain and definition; interpreting texts and their characters; ontological analysis of money; ontological analysis of malfunctions: some formal considerations; unpacking the semantics of risk in climate change discourses; and towards semantic interoperability among heterogeneous cancer data models using a layered modular hyper-ontology. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wu2024141,
	author = {Wu, Shenghan and Hsu, Wynne and Lee, Mongli},
	title = {EHDChat: A Knowledge-Grounded, Empathy-Enhanced Language Model for Healthcare Interactions},
	year = {2024},
	pages = {141 - 151},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216934377&partnerID=40&md5=a6d3aa9e8470ff0a5b7a6da2514f633c},
	abstract = {Large Language Models (LLMs) excel at a range of tasks but often struggle with issues like hallucination and inadequate empathy support. To address hallucinations, we ground our dialogues in medical knowledge sourced from external repositories such as Disease Ontology and DrugBank. To improve empathy support, we develop the Empathetic Healthcare Dialogues dataset, which utilizes multiple dialogue strategies in each response. This dataset is then used to fine-tune an LLM, and we introduce a lightweight, adaptable method called Strategy Combination Guidance to enhance the emotional support capabilities of the fine-tuned model, named EHDChat. Our evaluations show that EHDChat significantly outperforms existing models in providing emotional support and medical accuracy, demonstrating the effectiveness of our approach in enhancing empathetic and informed AI interactions in healthcare. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology; Dialogue Strategy; Drugbank; Emotional Supports; Excel; Language Model; Medical Knowledge; Ontology's; Support Capability; Computational Linguistics},
	keywords = {Ontology; Dialogue strategy; Drugbank; Emotional supports; Excel; Language model; Medical knowledge; Ontology's; Support capability; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Chowdhury2024194,
	author = {Chowdhury, Saurav and Joshi, Suyog and Dey, Lipika},
	title = {Cross Examine: An Ensemble-based approach to leverage Large Language Models for Legal Text Analytics},
	year = {2024},
	pages = {194 - 204},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216927826&partnerID=40&md5=b3aabe5a7b1fbf37b80d701a5e10fe40},
	abstract = {Legal documents are complex in nature, describing a course of argumentative reasoning that is followed to settle a case. Churning through large volumes of legal documents is a daily requirement for a large number of professionals who need access to the information embedded in them. Natural Language Processing(NLP) methods that help in document summarization with key information components, insight extraction and question answering play a crucial role in legal text processing. Most of the existing document analysis systems use supervised machine learning, which require large volumes of annotated training data for every different application and are expensive to build. In this paper we propose a legal text analytics pipeline using Large Language Models (LLMs), which can work with little or no training data. For document summarization, we propose an iterative pipeline using retrieval augmented generation to ensure that the generated text remains contextually relevant. For question answering, we propose a novel ontology-driven ensemble approach similar to cross-examination that exploits questioning and verification principles. A knowledge graph, created with the extracted information, stores the key entities and relationships reflecting the repository content structure. A new dataset is created with Indian court documents related to bail applications for cases filed under POCSO1 Act. Analysis of insights extracted from the answers reveal patterns of crime and social conditions leading to those crimes, which are important inputs for social scientists as well as legal system. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Crime; Data Analytics; Knowledge Graph; Modeling Languages; Natural Language Processing Systems; Personnel Training; Semi-supervised Learning; Document Summarization; Language Model; Language Processing; Large Volumes; Legal Documents; Legal Texts; Natural Languages; Processing Method; Question Answering; Text Analytics; Question Answering},
	keywords = {Crime; Data Analytics; Knowledge graph; Modeling languages; Natural language processing systems; Personnel training; Semi-supervised learning; Document summarization; Language model; Language processing; Large volumes; Legal documents; Legal texts; Natural languages; Processing method; Question Answering; Text analytics; Question answering},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Anaguchi202411,
	author = {Anaguchi, Fumikatsu and Chakraborty, Sudesna and Morita, Takeshi and Egami, Shusaku and Ugai, Takanori and Fukuda, Ken},
	title = {Reasoning and Justification System for Domestic Hazardous Behaviors Based on Knowledge Graph of Daily Activities and Retrieval-Augmented Generation},
	year = {2024},
	pages = {11 - 20},
	doi = {10.1109/CANDAR64496.2024.00010},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216925907&doi=10.1109%2FCANDAR64496.2024.00010&partnerID=40&md5=7dd189ba6fad7093487c3d8e24e382b9},
	abstract = {Accidents among people over 65 years of age predominantly occur within residential settings, making the maintenance of a safe home environment a crucial social issue. To address this issue, previous research has developed systems that construct Knowledge Graphs (KG) based on simulations of daily household activities, and studies have been conducted on detecting hazardous behaviors using such KG analysis. In this current study, we propose a system capable of presenting the reason and justification for the detected domestic hazardous behaviors. Our system will first generates the reason for the detected behavior using a Large Language Model (LLM). To ensure the accuracy, reliability and reproducibility of the LLM output, the system will provides reliable sources to support the output. We employed Retrieval-Augmented Generation (RAG) to search for sentences similar to the reason generated by the LLM within reliable, authoritative documents describing domestic accident cases and their causes and these will be presented as the evidence alongside the search engine results to the users. Consequently, a knowledge graph (KG) of domestic hazardous behavior is developed based on evidence ontology. Finally, to evaluate the ability of our proposed system in appropriately generating reasons for domestic hazardous behaviors and the adequacy of the justifications provided, the output was rated using LLMs and human volunteers. The rating results showed a significant correlation between LLMs and human evaluation, indicating that the proposed system can provide sufficient reasons and justifications for domestic hazardous behaviors at residential setting. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Explainable Ai; Knowledge Graph; Large Language Model; Retrieval-augmented Generation; Behavior-based; Daily Activity; Explainable Ai; Graph-based; Home Environment; Knowledge Graphs; Language Model; Large Language Model; Retrieval-augmented Generation; Social Issues; Knowledge Graph},
	keywords = {Behavior-based; Daily activity; Explainable AI; Graph-based; Home environment; Knowledge graphs; Language model; Large language model; Retrieval-augmented generation; Social issues; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Sophaken2024606,
	author = {Sophaken, Chotanansub and Vongpanich, Kantapong and Intaphan, Wachirawit and Utasri, Tharathon and Deepho, Chutamas and Takhom, Akkharawoot},
	title = {Leveraging Graph-RAG for Enhanced Diagnostic and Treatment Strategies in Dentistry},
	year = {2024},
	pages = {606 - 611},
	doi = {10.1109/InCIT63192.2024.10810521},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216814042&doi=10.1109%2FInCIT63192.2024.10810521&partnerID=40&md5=01a0bd3872977dfe4e639c5983f17ebc},
	abstract = {This paper presents a method for extracting and interpreting information from diverse, unstructured dental literature using advanced AI techniques. By integrating information extraction, ontologies, and knowledge graphs, the approach enhances the efficiency and accuracy of dental data analysis. Named Entity Recognition (NER) and a Large Language Model (LLM) are employed to extract relevant entities and relationships, which are then structured into triples and integrated with a dental ontology to ensure contextual relevance. This enriched ontology supports Retrieval-Augmented Generation (RAG) applications, enabling advanced querying and analysis. The methodology improves the identification and categorization of dental conditions, treatments, and anatomical terms, providing a structured representation of dental knowledge. Knowledge graphs facilitate the representation and analysis of relationships between entities, fostering insightful interpretations and supporting hypothesis generation, thereby enhancing the accessibility and usability of dental knowledge. Experimental results demonstrate the effectiveness of this approach in managing complex dental information, showcasing the benefits of combining Knowledge Representation (KR) with Machine Learning (ML). This research contributes to dental studies by offering a robust framework for extracting and utilizing knowledge from diverse and extensive datasets. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Dental Literature; Dentistry; Information Extraction; Knowledge Graphs; Large Language Models; Ontologies; Oral Health; Dentistry; Diagnosis; Ontology; Structured Query Language; Ai Techniques; Dental Literature; Extraction Ontologies; Information Extraction; Integrating Information; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Oral Healths; Knowledge Graph},
	keywords = {Dentistry; Diagnosis; Ontology; Structured Query Language; AI techniques; Dental literature; Extraction ontologies; Information extraction; Integrating information; Knowledge graphs; Language model; Large language model; Ontology's; Oral healths; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Li2024588,
	author = {Li, Haotian and Xia, Congming Min and Hou, Youjuan and Hu, Sile and Quan, Jiang and Liu, Yanjun},
	title = {TCMRD-KG: Design and Development of a Rheumatism Knowledge Graph Based on Ancient Chinese Literature},
	year = {2024},
	pages = {588 - 593},
	doi = {10.1109/MedAI62885.2024.00083},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216679297&doi=10.1109%2FMedAI62885.2024.00083&partnerID=40&md5=2403f6bb0f32ac49a26147694d9bc8bb},
	abstract = {The use of Traditional Chinese medicineTCM in rheumatic diseases dates back to thousands of years ago. Compared with standardized treatment, TCM has the advantages of low cost, low side effects, and flexible medication. Ancient books of traditional Chinese medicine play an important role in clinical and scientific research. This study takes the content related to rheumatism in ancient books of traditional Chinese medicine as the research object, integrates the ontology theory and technology in the knowledge graph, realizes the reconstruction of traditional Chinese medicine information knowledge, and provides basic data structure for data mining and knowledge discovery. This study is the first rheumatism-specific knowledge graph constructed based on ancient books of traditional Chinese medicine; it has tried the construction method of knowledge graph of ancient books of traditional Chinese medicine by combining automatic labeling of mainstream large language models with manual review; and according to the knowledge characteristics of ancient books of traditional Chinese medicine, the existing word segmentation technology is difficult to accurately reproduce the accurate meaning of the original text of ancient books, a new type of entity extraction method is given. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Rheumatic Diseases; Traditional Chinese Medicine; Knowledge Graph; Chinese Literature; Clinical Research; Design And Development; Graph-based; Knowledge Graphs; Low-costs; Rheumatic Disease; Scientific Researches; Side Effect; Traditional Chinese Medicine},
	keywords = {Knowledge graph; Chinese literature; Clinical research; Design and Development; Graph-based; Knowledge graphs; Low-costs; Rheumatic disease; Scientific researches; Side effect; Traditional Chinese Medicine},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Cornelio202412435,
	author = {Cornelio, Cristina and Diab, Mohammed},
	title = {Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery},
	year = {2024},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	pages = {12435 - 12442},
	doi = {10.1109/IROS58592.2024.10801853},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216491450&doi=10.1109%2FIROS58592.2024.10801853&partnerID=40&md5=49e070f22026e9cc1ce47e917b811109},
	abstract = {Recognizing failures during task execution and implementing recovery procedures is challenging in robotics. Traditional approaches rely on the availability of extensive data or a tight set of constraints, while more recent approaches leverage large language models (LLMs) to verify task steps and replan accordingly. However, these methods often operate offline, necessitating scene resets and incurring in high costs. This paper introduces Recover, a neuro-symbolic framework for online failure identification and recovery. By integrating ontologies, logical rules, and LLM-based planners, Recover exploits symbolic information to enhance the ability of LLMs to generate recovery plans and also to decrease the associated costs. In order to demonstrate the capabilities of our method in a simulated kitchen environment, we introduce OntoThor, an ontology describing the AI2Thor simulator setting. Empirical evaluation shows that OntoThor's logical rules accurately detect all failures in the analyzed tasks, and that Recover considerably outperforms, for both failure detection and recovery, a baseline method reliant solely on LLMs. Supplementary material, including the OntoThor ontology, is available at: https://recover-ontothor.github.io. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Robotics; Failure Detection And Recoveries; High Costs; Language Model; Logical Rules; Offline; Ontology's; Recovery Procedure; Task Executions; Traditional Approaches; Traditional Approachs; Ontology},
	keywords = {Robotics; Failure detection and recoveries; High costs; Language model; Logical rules; Offline; Ontology's; Recovery procedure; Task executions; Traditional approaches; Traditional approachs; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Nakajima20244755,
	author = {Nakajima, Haru and Miura, Jun},
	title = {Combining Ontological Knowledge and Large Language Model for User-Friendly Service Robots},
	year = {2024},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	pages = {4755 - 4762},
	doi = {10.1109/IROS58592.2024.10802273},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216487892&doi=10.1109%2FIROS58592.2024.10802273&partnerID=40&md5=ab838891e345c0c78ded5553dc4f308d},
	abstract = {Lifestyle support through robotics is an increasingly promising field, with expectations for robots to take over or assist with chores like floor cleaning, table setting and clearing, and fetching items. The growth of AI, particularly foundation models, such as large language models (LLMs) and visual language models (VLMs), is significantly shaping this sector. LLMs, by facilitating natural interactions and providing vast general knowledge, are proving invaluable for robotic tasks. This paper focuses on the benefits of LLMs for "bring-me"tasks, where robots fetch specific items for users, often based on ambiguous instructions. Our previous efforts utilized an ontology extended to handle environmental data to resolve such ambiguities, but faced limitations when unresolvable ambiguities required user intervention for clarity. Here, we enhance our approach by integrating LLMs for providing additional commonsense knowledge, pairing it with ontological data to mitigate the issue of hallucinations and reduce the need for user queries, thus improving system usability. We present a system that merges these knowledge bases and assess its efficacy on "bring-me"tasks, aiming to provide a more seamless and efficient robotic assistance experience. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Microrobots; Modeling Languages; Ontology; Floor Cleaning; Foundation Models; General Knowledge; Language Model; Natural Interactions; Ontology's; Robotic Tasks; Service Robots; User Friendly; Visual Language Model; Visual Languages},
	keywords = {Microrobots; Modeling languages; Ontology; Floor cleaning; Foundation models; General knowledge; Language model; Natural interactions; Ontology's; Robotic tasks; Service robots; User friendly; Visual language model; Visual languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{2024,
	title = {KiL 2024 - Proceedings of the 4th International Workshop on Knowledge-Infused Learning: Towards Consistent, Reliable, Explainable, and Safe LLMs, co-located with 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3894},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216428382&partnerID=40&md5=557b0d16e7b3753501280f2c81cd4391},
	abstract = {The proceedings contain 13 papers. The topics discussed include: towards optimizing with large language model; enhancing the reliability of LLMs-based systems for survey generation through distributional drift detection; GraphEval: a knowledge-graph based LLM hallucination evaluation framework; enhancing semantic understanding in vision language models using meaning representation negative generation; can LLMs solve reading comprehension tests as second language learners?; Infodeslib: Python library for dynamic ensemble learning using late fusion of multimodal data; design and optimization of heat exchangers using large language models; and encoding medical ontologies with holographic reduced representations for transformers. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Sousa202443,
	author = {Sousa, Guilherme and Lima, Rinaldo Jose De and Trojahn, Cassia},
	title = {Towards Generating Complex Alignments with Large Language Models via Prompt Engineering},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3897},
	pages = {43 - 56},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216423330&partnerID=40&md5=0a2afeab7c303ab8425938365e9b4ee6},
	abstract = {Still few ontology matching approaches focus on generating alignments by a Large Language Model (LLM), especially in the complex matching task. This paper proposes an approach that leverages the capabilities of LLMs to perform complex ontology matching. The method integrates subsets of both source and target ontologies into the prompt and, as a response, the LLM generates alignments in the structured EDOAL format, rather than natural language descriptions. This reduction technique, based on the automatic generation of SPARQL queries, tackles the challenge of large prompt sizes, reduces the search space, and enables efficient processing on consumer-grade hardware. This approach is evaluated on the Conference and Geolink datasets from the OAEI complex track, demonstrating improved scalability and the ability to produce well-formed EDOAL. Key contributions include the development of a SPARQL-based prompt engineering strategy and the application of few-shot learning techniques to complex alignment generation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Complex Matching; Llm; Sparql; Modeling Languages; Natural Language Processing Systems; Complex Matching; Language Description; Language Model; Large Language Model; Matchings; Natural Languages; Ontology Matching; Ontology's; Reduction Techniques; Sparql; Ontology},
	keywords = {Modeling languages; Natural language processing systems; Complex matching; Language description; Language model; Large language model; Matchings; Natural languages; Ontology matching; Ontology's; Reduction techniques; SPARQL; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Menad2024132,
	author = {Menad, Safaa and Abdeddaïm, Saïd and Soualmia, Lina F.},
	title = {BioSTransMatch Results @ OAEI 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3897},
	pages = {132 - 137},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216422049&partnerID=40&md5=09e3f4db3c9ae39a3db014f93e3bc435},
	abstract = {This paper aims at presenting the results obtained by BioSTransMatch at the OAEI 2024 competition, marking its first participation in this event. In this context, we applied the model BioSTransformers we developed to the equivalence matching task in the Bio-ML track of the OAEI challenge. The model is founded on sentence transformers, that have recently achieved remarkable results in ontology matching tasks. Here, we leverage a transformer-based language model to identify similarities between ontology concepts. BioSTransformers is a siamese neural model that we developed and trained using biomedical scientific articles from PubMed. It embeds texts into a vector space to compare and identify similarities. The model optimizes a self-supervised contrastive learning objective using articles from the MEDLINE bibliographic database and their associated MeSH (Medical Subject Headings) kywds. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Ontologies; Oaei 2024; Ontology Matching; Siamese Transformers; Distribution Transformers; Ontology; Biomedical Ontologies; Language Model; Learning Objectives; Matchings; Neural Modelling; Oaei 2024; Ontology Concepts; Ontology Matching; Scientific Articles; Siamese Transformer},
	keywords = {Distribution transformers; Ontology; Biomedical ontologies; Language model; Learning objectives; Matchings; Neural modelling; OAEI 2024; Ontology concepts; Ontology matching; Scientific articles; Siamese transformer},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2024,
	title = {OM 2024 - Proceedings of the 19th International Workshop on Ontology Matching, co-located with the 23rd International Semantic Web Conference, ISWC 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3897},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216413567&partnerID=40&md5=4793bb35261c18fa0ff4878bd40dbd4d},
	abstract = {The proceedings contain 17 papers. The topics discussed include: a gold standard benchmark dataset for digital humanities; a scalable method for large-scale entity alignment via multi-channel retrieval and fusion; MDMapper: a framework for aligning master data models using ontology matching techniques; towards generating complex alignments with large language models via prompt engineering; towards tailoring ontology embeddings for ontology matching tasks; enhancing entity matching through systematic association of matchers to linking problem types; results of the ontology alignment evaluation initiative 2024; and TOMATO: results of the 2024 OAEI evaluation campaign. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Sousa2024156,
	author = {Sousa, Guilherme and Lima, Rinaldo Jose De and Trojahn, Cassia},
	title = {Results of CANARD in OAEI 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3897},
	pages = {156 - 160},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216401149&partnerID=40&md5=03702571a288fcfb0c99cf667c644ee8},
	abstract = {This paper presents the 2024 results of an enhanced version of the CANARD system, which integrates Large Language Models (LLMs) to address the challenges of complex alignments. By leveraging LLM-based embeddings, the system better captures semantic and contextual relationships, improving both precision and coverage. Four architectural settings - Label Embedding Similarity (LES), Embeddings of SPARQL Query (ESQ), Subgraph Embeddings (SE) and Instance Embeddings (IE) - were explored to improve the alignment quality. Experiments on the Populated Conference dataset from the OAEI Complex Track demonstrate improvements over baseline approaches, with an increase in F-measure up to 45% in some cases. However, challenges such as runtime overhead in IE and noise in SE components were identified, where future work can explore better aggregation techniques or fine-tuned LLMs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Complex Ontology Matching; Embeddings; Llm; Modeling Languages; Ontology; Semantics; Structured Query Language; Complex Ontology Matching; Contextual Relationships; Embeddings; Language Model; Large Language Model; Model-based Opc; Ontology Matching; Semantic Relationships; Similarity Embedding; Subgraphs},
	keywords = {Modeling languages; Ontology; Semantics; Structured Query Language; Complex ontology matching; Contextual relationships; Embeddings; Language model; Large language model; Model-based OPC; Ontology matching; Semantic relationships; Similarity embedding; Subgraphs},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hu202481,
	author = {Hu, Bing and Yu, Trevor and Tuinstra, Tia and Rezai, Ryan and Bokadia, Harshit and DiMaio, Rachel and Fortin, Thomas and Vartian, Brian and Tripp, Bryan P.},
	title = {Encoding Medical Ontologies With Holographic Reduced Representations for Transformers},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3894},
	pages = {81 - 92},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216395492&partnerID=40&md5=caa5c8f83c013d7f5bd9429e3a43b0c4},
	abstract = {Transformer models trained on NLP tasks with medical codes often have randomly initialized embeddings that are then adjusted based on training data. For terms appearing infrequently in the dataset, there is little opportunity to improve these representations and learn semantic similarity with other concepts. Medical ontologies represent many biomedical concepts and define a relationship structure between these concepts, making ontologies a valuable source of domain-specific information. Holographic Reduced Representations (HRR) are capable of encoding ontological structure by composing atomic vectors to create structured higher-level concept vectors. We developed an embedding layer that generates concept vectors for clinical diagnostic codes by applying HRR operations that compose atomic vectors based on the SNOMED CT ontology. This approach allows for learning the atomic vectors while maintaining structure in the concept vectors. We trained a Bidirectional Encoder Representations from the Transformers (BERT) model to process sequences of clinical diagnostic codes and used the resulting HRR concept vectors as the embedding matrix for the model. The HRR-based approach introduced interpretable structure into code embeddings while maintaining or modestly improving performance on the masked language modeling (MLM) pre-training task (particularly for rare codes) as well as the fine-tuning tasks of mortality and disease prediction. This approach also better maintains semantic similarity between medically related concept vectors, due to both shared atomic vectors and disentangling of code-frequency information. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Knowledge-integration; Ontology; Ontology; Semantics; Vectors; Clinical Diagnostics; Deep Learning; Embeddings; Encodings; Holographic Reduced Representations; Knowledge Integration; Medical Ontology; Ontology's; Semantic Similarity; Transformer Modeling; Encoding (symbols)},
	keywords = {Ontology; Semantics; Vectors; Clinical diagnostics; Deep learning; Embeddings; Encodings; Holographic reduced representations; Knowledge integration; Medical ontology; Ontology's; Semantic similarity; Transformer modeling; Encoding (symbols)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Faria2024110,
	author = {Faria, Daniel and Silva, Marta Contreiras and Cotovio, Pedro Giesteira and Ferraz, Lucas and Balbi, Laura and Pesquita, Cátia},
	title = {Results in OAEI 2024 for Matcha},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3897},
	pages = {110 - 117},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216394360&partnerID=40&md5=1f466bbbb35a9bd67a4dbd0138bb74a2},
	abstract = {Matcha is an ontology matching system designed to tackle long-standing challenges such as complex and holistic ontology matching. It incorporates all of the key algorithms from AgreementmakerLight over a novel broader core architecture that includes several new algorithms. In this year's edition, some strategies were modified to rectify some gaps found in last year, and a few new strategies were debuted, with particular note for the inclusion of Language Models in two of our algorithms. Matcha performed well overall, achieving the highest F-measure in 15 out of 43 distinct OAEI tasks and ranking in the top three in ten others. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Modeling Languages; F Measure; Language Model; Matching System; Ontology Matching; Ontology},
	keywords = {Modeling languages; F measure; Language model; Matching system; Ontology matching; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Çöplü2024144,
	author = {Çöplü, Tolga and Bendiken, Arto and Skomorokhov, Andrii and Bateiko, Eduard and Cobb, Stephen},
	title = {Prompt-Time Ontology-Driven Symbolic Knowledge Capture with Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3894},
	pages = {144 - 149},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216389497&partnerID=40&md5=deecd7ac298892018293f0ff1524b524},
	abstract = {In applications such as personal assistants, large language models (LLMs) must consider the user’s personal information and preferences. However, LLMs lack the inherent ability to learn from user interactions. This paper explores capturing personal information from user prompts using ontology and knowledge-graph approaches. We use a subset of the KNOW ontology, which models personal information, to train the language model on these concepts. We then evaluate the success of knowledge capture using a specially constructed dataset. Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTODSKC. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Fine-tuning; Know Ontology; Knowledge Graphs; Large Language Models; Ontology-driven Symbolic Knowledge Capture; Symbolic Representation; Ontology; Fine Tuning; Know Ontology; Knowledge Capture; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Ontology-driven Symbolic Knowledge Capture; Symbolic Knowledge; Symbolic Representation; Knowledge Graph},
	keywords = {Ontology; Fine tuning; KNOW ontology; Knowledge capture; Knowledge graphs; Language model; Large language model; Ontology's; Ontology-driven symbolic knowledge capture; Symbolic knowledge; Symbolic representation; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Totoian2024138,
	author = {Totoian, Marius Horatiu and Marginean, Anca Nicoleta and Blohm, Philipp and Hussain, Mir Nawab},
	title = {HybridOM: Ontology Matching using Hybrid Search},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3897},
	pages = {138 - 145},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216388200&partnerID=40&md5=6c536aafb8b92a07ed02734fedb6f96c},
	abstract = {Ontology matching targets identical concepts from different ontologies with the final purpose of interoperability and ontologies merging. The matching task is not restricted to ontologies, it is also relevant for knowledge graphs. Ontology matching solutions based on transformer-based embeddings, textual similarity, logical mapping, or Large Language Models (LLMs) are still facing problems, mainly due to the lack of uniform information about the concepts and lack of homogeneous semantic granularity along different ontologies. In this work, we present a framework that combines vector-based similarity and string-based similarity through hybrid searches. LLMs are used to generate descriptions for ontology concepts, hence the concepts' representation is enriched and the alignment process can benefit from both the knowledge captured by the initial ontologies and the extended LLM-generated textual descriptions. The proposed system, HybridOM, is an unsupervised approach independent of the ontologies' domain. HybridOM is evaluated within Bio-ML 2024 track for the task of concept matching. It achieves the highest values for F1-score and Recall for most of the ontology pairs while maintaining a balance between precision and recall. The proposed method has been adapted for industrial usage in a human capital management product called msg.ProfileMap. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Hybrid Search; Large Language Model; Ontology Matching; Vector Database; Verbalization; Knowledge Graph; Ontology; Semantics; Hybrid Search; Knowledge Graphs; Language Model; Large Language Model; Ontology Matching; Ontology Merging; Ontology's; Vector Database; Verbalization; Human Resource Management},
	keywords = {Knowledge graph; Ontology; Semantics; Hybrid search; Knowledge graphs; Language model; Large language model; Ontology matching; Ontology merging; Ontology's; Vector database; Verbalization; Human resource management},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Sampels2024124,
	author = {Sampels, Julian},
	title = {OntoMatch Results for OAEI 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3897},
	pages = {124 - 131},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216387360&partnerID=40&md5=f3574fe259d893ec448d7aeae809d538},
	abstract = {This paper presents the results of OntoMatch in the OAEI 2024 competition. OntoMatch is an ontology matching system that combines graph search algorithms with zero-shot prompting of Large Language Models (LLMs) to produce class correspondences. The system follows an iterative approach involving neighbourhood candidate selection, context extraction using graph search techniques, verbalising of context and zero-shot LLM prompting with templates. Each iteration concludes with a cardinality filter to refine the alignments. OntoMatch was evaluated on the OAEI conference benchmark dataset. The results demonstrate the impact of incorporating graph-based contextual information alongside carefully crafted prompt templates, achieving competitive scores and highlighting the effectiveness of LLM-driven approaches for ontology alignment. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Search; Knowledge Graphs; Large Language Model; Ontology Matching; Prompt Generation; Modeling Languages; Ontology; Zero-shot Learning; Graph Search; Graph-search Algorithms; Iterative Approach; Knowledge Graphs; Language Model; Large Language Model; Matching System; Neighbourhood; Ontology Matching; Prompt Generation; Knowledge Graph},
	keywords = {Modeling languages; Ontology; Zero-shot learning; Graph search; Graph-search algorithms; Iterative approach; Knowledge graphs; Language model; Large language model; Matching system; Neighbourhood; Ontology matching; Prompt generation; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kosten2024,
	author = {Kosten, Catherine and Nooralahzadeh, Farhad and Stockinger, Kurt},
	title = {Evaluating the effectiveness of prompt engineering for knowledge graph question answering},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence},
	volume = {7},
	pages = {},
	doi = {10.3389/frai.2024.1454258},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216184385&doi=10.3389%2Ffrai.2024.1454258&partnerID=40&md5=6daf2b7c501d6e2fa5a622db2d279159},
	abstract = {Many different methods for prompting large language models have been developed since the emergence of OpenAI's ChatGPT in November 2022. In this work, we evaluate six different few-shot prompting methods. The first set of experiments evaluates three frameworks that focus on the quantity or type of shots in a prompt: a baseline method with a simple prompt and a small number of shots, random few-shot prompting with 10, 20, and 30 shots, and similarity-based few-shot prompting. The second set of experiments target optimizing the prompt or enhancing shots through Large Language Model (LLM)-generated explanations, using three prompting frameworks: Explain then Translate, Question Decomposition Meaning Representation, and Optimization by Prompting. We evaluate these six prompting methods on the newly created Spider4SPARQL benchmark, as it is the most complex SPARQL-based Knowledge Graph Question Answering (KGQA) benchmark to date. Across the various prompting frameworks used, the commercial model is unable to achieve a score over 51%, indicating that KGQA, especially for complex queries, with multiple hops, set operations and filters remains a challenging task for LLMs. Our experiments find that the most successful prompting framework for KGQA is a simple prompt combined with an ontology and five random shots. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Question Answering; Llms; Prompt Engineering; Rdf; Sparql},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@ARTICLE{Lee2024,
	author = {Lee, Changshing and Wang, Meihui and Tseng, Guanying and Yue, Chao Cyuan and Hsieh, Haochun and Reformat, Marek Z.},
	title = {Cao Robot for Taiwanese/English Knowledge Graph Application},
	year = {2024},
	pages = {},
	doi = {10.1109/O-COCOSDA64382.2024.10800729},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215670709&doi=10.1109%2FO-COCOSDA64382.2024.10800729&partnerID=40&md5=7e6c49ab713609fc99f4d57214ddbf38},
	abstract = {This paper proposes a Content Attention Ontology (CAO) robot for constructing Taiwanese/English Knowledge Graphs (KGs) by prompting audio or texts to Large Language Models (LLMs), including TAIDE, Zephyr, and Llama 3.1. The collected data includes lecture videos from the IEEE WCCI 2024 in Japan and the 2024 National Language Development Forum in Taiwan, along with students' learning data from the 2024 Summer School on Taiwanese/English Human and Robot Co-Learning at Rende Elementary School (RDES). In addition, the fundamental concepts of Computational Intelligence (CI) and Quantum CI (QCI) learning were incorporated into the study. The generative KGs highlight important concepts, relations, and communities within the collected teaching and learning data. Additionally, we utilized data from subjects wearing braincomputer interface (BCI) devices while speaking Taiwanese/English to generate KGs. We also compared the differences in these KGs and analyzed the similarities between the transcribed texts of lectures and learners. In the future, we plan to expand the CAO robot to more validation fields across Taiwan, aiming to engage young students in speaking Taiwanese while concurrently enhancing their English language skills through interaction with the robot. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Cao Robot; Knowledge Graph; Large Language Model; Llama 3.1; Taide; Taiwanese/english Language Co-learning; Adversarial Machine Learning; Contrastive Learning; Federated Learning; Robot Learning; Robots; Students; Co-learning; Content Attention Ontology Robot; English Languages; Knowledge Graphs; Language Model; Large Language Model; Llama 3.1; Ontology's; Taide; Taiwanese/english Language Co-learning; Knowledge Graph},
	keywords = {Adversarial machine learning; Contrastive Learning; Federated learning; Robot learning; Robots; Students; Co-learning; Content attention ontology robot; English languages; Knowledge graphs; Language model; Large language model; Llama 3.1; Ontology's; TAIDE; Taiwanese/english language co-learning; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Wang2024,
	author = {Wang, Changlong and Sang, Xiujuan and Wang, Xijie and Gao, Yuan and Liu, Yi},
	title = {Research on Knowledge Graph Extraction Methods for Chinese STEM Curriculum},
	year = {2024},
	pages = {},
	doi = {10.1109/MLNLP63328.2024.10800180},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215598070&doi=10.1109%2FMLNLP63328.2024.10800180&partnerID=40&md5=6e0a74fed87abe5a73acc1ed74985a8b},
	abstract = {STEM education, as an innovative teaching model, has gained widespread attention in recent years. However, the lack of relevant textbooks and learning resources has made its implementation challenging. Developing interdisciplinary knowledge graphs tailored for STEM education has become an urgent issue. To address this, a knowledge extraction framework named Llms4edu is proposed, which utilizes a series of effective prompts to guide large language models in knowledge extraction. Specifically, the knowledge extraction task is transformed into multiple rounds of question-and-answer interactions with the LLM, gradually identifying entity-relation triplets from subject data. Through experiments, an F1-score of 89.4% was achieved on the named entity recognition task in the chemistry subject, and an F1-score of 66.7% on the relation extraction task. Finally, a subject ontology model was built for subject text, and a subject data set was constructed using Llms4edu, which includes three subjects of junior high school mathematics, physics, and chemistry, a total of 2,511 entities, 2,010 relationship triples, and cross-disciplinary knowledge is linked to construct a cross-disciplinary knowledge graph. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Interdisciplinary Knowledge Graph; Large Language Model; Prompt Engineering; Adversarial Machine Learning; Contrastive Learning; Federated Learning; Cross-disciplinary; F1 Scores; Graph Extractions; Interdisciplinary Knowledge Graph; Knowledge Extraction; Knowledge Graphs; Language Model; Large Language Model; Prompt Engineering; Stem Education; Knowledge Graph},
	keywords = {Adversarial machine learning; Contrastive Learning; Federated learning; Cross-disciplinary; F1 scores; Graph extractions; Interdisciplinary knowledge graph; Knowledge extraction; Knowledge graphs; Language model; Large language model; Prompt engineering; STEM education; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Schmidt2024,
	author = {Schmidt, David Maria and Cimiano, Philipp},
	title = {Grammar-constrained decoding for structured information extraction with fine-tuned generative models applied to clinical trial abstracts},
	year = {2024},
	journal = {Frontiers in Artificial Intelligence},
	volume = {7},
	pages = {},
	doi = {10.3389/frai.2024.1406857},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215524351&doi=10.3389%2Ffrai.2024.1406857&partnerID=40&md5=5fb62684f748034492e184dcdc23848d},
	abstract = {Background: In the field of structured information extraction, there are typically semantic and syntactic constraints on the output of information extraction (IE) systems. These constraints, however, can typically not be guaranteed using standard (fine-tuned) encoder-decoder architectures. This has led to the development of constrained decoding approaches which allow, e.g., to specify constraints in form of context-free grammars. An open question is in how far an IE system can be effectively guided by a domain-specific grammar to ensure that the output structures follow the requirements of a certain domain data model. Methods: In this work we experimentally investigate the influence of grammar-constrained decoding as well as pointer generators on the performance of a domain-specific information extraction system. For this, we consider fine-tuned encoder-decoder models, Longformer and Flan-T5 in particular, and experimentally investigate whether the addition of grammar-constrained decoding and pointer generators improve information extraction results. Toward this goal, we consider the task of inducing structured representations from abstracts describing clinical trials, relying on the C-TrO ontology to semantically describe the clinical trials and their results. We frame the task as a slot filling problem where certain slots of templates need to be filled with token sequences occurring in the input text. We use a dataset comprising 211 annotated clinical trial abstracts about type 2 diabetes and glaucoma for training and evaluation. Our focus is on settings in which the available training data is in the order of a few hundred training examples, which we consider as a low-resource setting. Results: In all our experiments we could demonstrate the positive impact of grammar-constrained decoding, with an increase in F<inf>1</inf> score of pp 0.351 (absolute score 0.413) and pp 0.425 (absolute score 0.47) for the best-performing models on type 2 diabetes and glaucoma datasets, respectively. The addition of the pointer generators had a detrimental impact on the results, decreasing F<inf>1</inf> scores by pp 0.15 (absolute score 0.263) and pp 0.198 (absolute score 0.272) for the best-performing pointer generator models on type 2 diabetes and glaucoma datasets, respectively. Conclusion: The experimental results indicate that encoder-decoder models used for structure prediction for information extraction tasks in low-resource settings clearly benefit from grammar-constrained decoding guiding the output generation. In contrast, the evaluated pointer generator models decreased the performance drastically in some cases. Moreover, the performance of the pointer models appears to depend both on the used base model as well as the function used for aggregating the attention values. How the size of large language models affects the performance benefit of grammar-constrained decoding remains to be more structurally investigated in future work. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Trials; Deep Learning; Evidence-based Medicine; Generative Large Language Models; Grammar-constrained Decoding; Pico; Structured Information Extraction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Li2024410,
	author = {Li, Yi and Tian, Liwei and Yi, Chengyi and Li, Jingjing and Qin, Xiaodong and He, Yuxuan and Su, Huai},
	title = {A Large Language Model Based Knowledge Mining Method for Improving the Reliability of Fire Water Systems},
	year = {2024},
	pages = {410 - 413},
	doi = {10.1109/SRSE63568.2024.10772514},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215273182&doi=10.1109%2FSRSE63568.2024.10772514&partnerID=40&md5=0b65baf1820c442bba07118662a1fbd9},
	abstract = {The fire water system plays a critical role in protecting both infrastructure and human lives. An essential aspect of enhancing the reliability of this system is fault diagnosis. However, the current fault diagnosis methods primarily rely on data-driven approaches, which often result in a high threshold for application due to their lack of interpretability. To tackle this challenge, this paper introduces a novel approach based on large language models for knowledge mining from textual data to extract fault information related to the fire water system, thereby enhancing the interpretability of data-driven fault diagnosis methods. The methodology followed in this paper consists of two main steps: firstly, analyzing the characteristics and principles of fire water system faults to develop a fault ontology, and secondly, creating a knowledge mining model using a large language model guided by the established fault ontology. Experimental findings indicate that the proposed model achieves an F1 score of 0.944, meeting the necessary criteria for effective knowledge mining in fire water system fault analysis. Furthermore, a comparative experiment was conducted to evaluate the performance of various encoder models, including GRU, BiGRU, LSTM, BiLSTM, and pre-trained large language model BERT. The results revealed a significant improvement in performance with the BERT encoder, showing increases in F1 scores of 22.12 %, 2.27 %, 17.41 %, and 3.16 % compared to the other models, respectively. This study provides valuable interpretative insights that can enhance the engineering applicability and reliability of data-driven fault diagnosis methods in fire water system. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Fire Water System; Knowledge Mining; Large Language Model; Safety Engineering; System Reliability; Fire Protection; Intelligent Systems; Knowledge Based Systems; Systems Analysis; Data-driven Fault Diagnosis; Fault Diagnosis Method; Fire Water System; Interpretability; Knowledge Mining; Language Model; Large Language Model; System Faults; System Reliability; Water System; Mine Fires},
	keywords = {Fire protection; Intelligent systems; Knowledge based systems; Systems analysis; Data-driven fault diagnosis; Fault diagnosis method; Fire water system; Interpretability; Knowledge mining; Language model; Large language model; System faults; System reliability; Water system; Mine fires},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {2024 Sensor Data Fusion: Trends, Solutions, Applications, SDF 2024},
	year = {2024},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215135182&partnerID=40&md5=c53ab4998d9086b78b694fcdcea0c77f},
	abstract = {The proceedings contain 18 papers. The topics discussed include: computational complexity reduction for the track-to-track association problem; modeling contour measurements of elliptical extended objects via gaussian spatial distributions; advanced sensor fusion for railway security - a hierarchical graph-based approach; performance evaluation of deep learning-based state estimation: a comparative study of KalmanNet; a hybrid AI framework integrating ontology learning, knowledge graphs, and large language models for improved data model translation in smart manufacturing and transportation; multi-sensor simulation from target tracking to a recognized air picture; and Voronoi trust regions for local calibration testing in supervised machine learning models. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mohsenzadegan2024,
	author = {Mohsenzadegan, Kabeh and Tavakkoli, Vahid and Kambale, Witesyavwirwa Vianney and Kyandoghere, Kyamakya},
	title = {A Hybrid AI Framework Integrating Ontology Learning, Knowledge Graphs, and Large Language Models for Improved Data Model Translation in Smart Manufacturing and Transportation},
	year = {2024},
	pages = {},
	doi = {10.1109/SDF63218.2024.10773919},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215099473&doi=10.1109%2FSDF63218.2024.10773919&partnerID=40&md5=614ed6d15113c76acc0dabf7bc5a2a86},
	abstract = {Interoperability among diverse data standards is crucial for advancing digital technologies in smart manufacturing and transportation. This paper studies and presents a hybrid AI framework that integrates Ontology Learning (OL), Knowledge Graphs (KGs), and Large Language Models (LLMs) to enhance data translation across different standards. Focusing on IEEE 1451, ISO 15926, and IEC 61499, which exemplify the challenges of translating between distinct data models, we evaluate the performance of OL, KGs, and LLMs in terms of accuracy, scalability, efficiency, robustness, and flexibility. The findings indicate that the hybrid framework effectively leverages OL for semantic structuring, KGs for relational modeling, and LLMs for linguistic and contextual processing. This integration significantly improves the accuracy and adaptability of data translations, offering a comprehensive solution tailored to the complex environments of smart manufacturing and transportation, thereby advancing cross-standard data interoperability. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai In Transportation; Cross-standard Data Integration; Data Interoperability; Data Model Translation; Hybrid Ai Framework; Knowledge Graphs (kgs); Large Language Models (llms); Ontology Learning (ol); Semantic Mapping; Smart Manufacturing; Data Accuracy; Data Assimilation; Knowledge Graph; Ai In Transportation; Cross-standard Data Integration; Data Interoperability; Data Model Translation; Hybrid Ai Framework; Knowledge Graphs; Language Model; Large Language Model; Model Translation; Ontology Learning; Semantics Mappings; Smart Manufacturing},
	keywords = {Data accuracy; Data assimilation; Knowledge graph; AI in transportation; Cross-standard data integration; Data interoperability; Data model translation; Hybrid AI framework; Knowledge graphs; Language model; Large language model; Model translation; Ontology learning; Semantics mappings; Smart manufacturing},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Munzir2024,
	author = {Munzir, Syed Ilyas and Hier, Daniel B. and Carrithers, Michael D.},
	title = {High Throughput Phenotyping of Physician Notes with Large Language and Hybrid NLP Models},
	year = {2024},
	journal = {Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings},
	pages = {},
	doi = {10.1109/EMBC53108.2024.10782119},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214971917&doi=10.1109%2FEMBC53108.2024.10782119&partnerID=40&md5=75f9d263408bc17def9755cb2816a48a},
	abstract = {Deep phenotyping is the detailed description of patient signs and symptoms using concepts from an ontology. The deep phenotyping of the numerous physician notes in electronic health records requires high throughput methods. Over the past 30 years, progress toward making high-throughput phenotyping feasible. In this study, we demonstrate that a large language model and a hybrid NLP model (combining word vectors with a machine learning classifier) can perform high throughput phenotyping on physician notes with high accuracy. Large language models will likely emerge as the preferred method for high throughput deep phenotyping physician notes.Clinical relevance: Large language models will likely emerge as the dominant method for the high throughput phenotyping of signs and symptoms in physician notes © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Electronic Health; Health Records; High-throughput Method; High-throughput Phenotyping; Language Model; Learning Classifiers; Machine-learning; Ontology's; Phenotyping; Word Vectors; Electronic Health Record; Electronic Health Record; Human; Machine Learning; Natural Language Processing; Phenotype; Physician; Electronic Health Records; Humans; Machine Learning; Natural Language Processing; Phenotype; Physicians},
	keywords = {Electronic health; Health records; High-throughput method; High-throughput phenotyping; Language model; Learning classifiers; Machine-learning; Ontology's; Phenotyping; Word vectors; Electronic health record; electronic health record; human; machine learning; natural language processing; phenotype; physician; Electronic Health Records; Humans; Machine Learning; Natural Language Processing; Phenotype; Physicians},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Alharbi20241,
	author = {Alharbi, Reham and Ahmed, Umair and Dobriy, Daniil and Lajewska, Weronika and Menotti, Laura and Saeedizade, Mohammad Javad and Dumontier, Michel J.},
	title = {Exploring the Role of Generative AI in Constructing Knowledge Graphs for Drug Indications with Medical Context},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3890},
	pages = {1 - 10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214896866&partnerID=40&md5=92194969cad015f54812f2354abcd8f4},
	abstract = {The medical context for a drug indication provides crucial information on how the drug can be used in practice. However, the extraction of medical context from drug indications remains poorly explored, as most research concentrates on the recognition of medications and associated diseases. Indeed, most databases cataloging drug indications do not contain their medical context in a machine-readable format. This paper proposes the use of a large language model for constructing DIAMOND-KG, a knowledge graph of drug indications and their medical context. The study 1) examines the change in accuracy and precision in providing additional instruction to the language model, 2) estimates the prevalence of medical context in drug indications, and 3) assesses the quality of DIAMOND-KG against NeuroDKG, a small manually curated knowledge graph. The results reveal that more elaborated prompts improve the quality of extraction of medical context; 71% of indications had at least one medical context; 63.52% of extracted medical contexts correspond to those identified in NeuroDKG. This paper demonstrates the utility of using large language models for specialized knowledge extraction, with a particular focus on extracting drug indications and their medical context. We provide DIAMOND-KG as a FAIR RDF graph supported with an ontology. Openly accessible, DIAMOND-KG may be useful for downstream tasks such as semantic query answering, recommendation engines, and drug repositioning research. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Construction; Llms In Kgc; Medical Knowledge Graph; Query Languages; Semantics; Accuracy And Precision; Graph Construction; Knowledge Graph Construction; Knowledge Graphs; Language Model; Llm In Kgc; Machine-readable Format; Medical Knowledge; Medical Knowledge Graph; Knowledge Graph},
	keywords = {Query languages; Semantics; Accuracy and precision; Graph construction; Knowledge graph construction; Knowledge graphs; Language model; LLM in KGC; Machine-readable format; Medical knowledge; Medical knowledge graph; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Vandermoortele202454,
	author = {Vandermoortele, Nathan and Steenwinckel, Bram and van Hoecke, Sofie and Ongenae, Femke},
	title = {Scalable Table-to-Knowledge Graph Matching from Metadata using LLMs},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3889},
	pages = {54 - 68},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214864616&partnerID=40&md5=73067eb5dcc1a79f668fbec7ee463d8a},
	abstract = {Addressing the challenge of interoperability when integrating and interpreting large datasets from diverse sources is essential for businesses aiming to make informed, data-driven decisions. Therefore, the 2024 Semantic Web Challenge on Tabular Data to Knowledge Graph Matching (SemTab) focuses on using metadata, such as column names, to map tables to semantic concepts within standardized vocabularies or Knowledge Graphs (KGs). The challenge involves mapping two datasets, one to the DBpedia ontology and the other to a custom vocabulary. Our approach begins with applying Retrieval-Augmented Generation (RAG) for a broad search of relevant matches, ensuring scalability. We then refine these matches using a Large Language Model (LLM) with Chain-of-Thought (CoT) prompting and Self-Consistency (SC). Finally, we combine the results using Reciprocal Rank Fusion (RRF) to obtain a final ranking of the matches. This method achieves hit rates of 62% (top 1) and 82% (top 5) for the first dataset, and 84% (top 1) and 98% (top 5) for the second. The LLM’s strong semantic understanding and extensive knowledge base provide significant advantages over traditional human labeling, which is often laborious and time-consuming. Furthermore, the LLM’s zero-shot capability removes the need for additional task-specific training data, making this solution applicable across various domains. Despite limitations like computational costs and the need for well-defined concepts in the ontology or vocabulary, our approach remains cost-effective compared to extensive human labeling. Moreover, we leave room to trade performance for scalability if needed, pending further research. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Linkage; Knowledge Graphs; Large Language Models; Metadata; Retrieval Augmented Generation; Zero-shot Learning; Data Consistency; Large Datasets; Metadata; Ontology; Scalability; Semantics; Spatio-temporal Data; Zero-shot Learning; Data Driven Decision; Data Linkage; Graph Matchings; Human Labelling; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Retrieval Augmented Generation; Knowledge Graph},
	keywords = {Data consistency; Large datasets; Metadata; Ontology; Scalability; Semantics; Spatio-temporal data; Zero-shot learning; Data driven decision; Data linkage; Graph matchings; Human labelling; Knowledge graphs; Language model; Large language model; Ontology's; Retrieval augmented generation; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {ISD8 2024 - Proceedings of the 8th Image Schema Day, co-located with the 23rd International Conference of the Italian Association for Artificial Intelligence, AI*IA 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3888},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214850330&partnerID=40&md5=ca3a1e605d00fd9ef01891984827f96f},
	abstract = {The proceedings contain 12 papers. The topics discussed include: multimodal meets intuitive? comparing visual and tangible image schema representations; generalized between icon, symbol and index: the physical dimension in isotype and Unicode; beyond space and time: an initial sketch of formal accounts to non-spatiotemporal conceptual sensory primitives; how to blend concepts in diffusion models; the boat, the house and the in-between: conceptual blending as betweenness relation; image schema and ontology-based rules to support planning activities: a study of the urban square; may the FORCE be with semantics: exploiting LLMs to image schematic knowledge enrichment; and left-right spatial orientation in dream reports: image schemas and interactional dynamics. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Barua2024,
	author = {Barua, Adrita},
	title = {Concept Induction Using LLMs},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3884},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214839760&partnerID=40&md5=a4f4be096d70304877c43b60b93d8ecf},
	abstract = {In this study, the capability of Large Language Models (LLMs) is explored to automate Concept Induction, a process traditionally reliant on formal logical reasoning using description logic ontologies, within the context of explainable AI (XAI). Initially, a pre-trained LLM like GPT-4 is employed to assess its ability to generate high-level concepts describing data differentials for a scene classification task via prompting. A human assessment study was conducted which revealed that concepts produced by GPT-4 are preferred over those from logical concept induction systems in terms of human understandability, despite some limitations in neuron activation analysis. Building on these insights, further research aims to automate the concept induction system using LLMs, potentially addressing the shortcomings of traditional logical reasoners. This approach has the potential to scale and provide a significant avenue for concept discovery in complex AI models. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Induction; Explainable Ai; Gpt-4; Llm; Formal Languages; Modeling Languages; Problem Oriented Languages; Concept Induction; Description Logic; Explainable Ai; Gpt-4; Induction System; Language Model; Large Language Model; Logical Reasoning; Ontology's; Scene Classification; Ontology},
	keywords = {Formal languages; Modeling languages; Problem oriented languages; Concept induction; Description logic; Explainable AI; GPT-4; Induction system; Language model; Large language model; Logical reasoning; Ontology's; Scene classification; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{de Giorgis2024,
	author = {de Giorgis, Stefano},
	title = {May the FORCE be with Semantics: exploiting LLMs to Image Schematic Knowledge Enrichment},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3888},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214838665&partnerID=40&md5=847a65a611064741707a6db0c189f376},
	abstract = {This paper addresses the underspecification of the FORCE image schema. We present a novel hybrid pipeline that combines large language model interactions, linguistic analysis, and knowledge extraction techniques to expand upon Johnson’s initial categorization of FORCE types. Our methodology employs Claude 3.5 Sonnet for domain exploration, generates a dataset of 100 force-expressing verbs with contextual sentences, and integrates findings into ImageSchemaNet through AMR2FRED processing and SPARQL querying. Key contributions include: (1) a more nuanced understanding of the FORCE image schema, (2) a validated dataset of force-related linguistic expressions, and (3) an enhanced ontology with empirically derived FORCE concepts. This work bridges the gap between abstract image schema theory and specific linguistic realizations of FORCE, offering practical tools for natural language processing, knowledge representation, and cognitive computing. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Representation; Large Datasets; Modeling Languages; Natural Language Processing Systems; Ontology; Domain Exploration; Extraction Techniques; Image Schemata; Knowledge Extraction; Language Model; Linguistic Analysis; Linguistic Expressions; Linguistic Knowledge; Model Interaction; Underspecification; Semantics},
	keywords = {Knowledge representation; Large datasets; Modeling languages; Natural language processing systems; Ontology; Domain exploration; Extraction techniques; Image schemata; Knowledge extraction; Language model; Linguistic analysis; Linguistic expressions; Linguistic knowledge; Model interaction; Underspecification; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Rajan202410185,
	author = {Rajan, Sai Sathiesh and Soremekun, Ezekiel Olamide and Chattopadhyay, Sudipta},
	title = {Knowledge-based Consistency Testing of Large Language Models},
	year = {2024},
	pages = {10185 - 10196},
	doi = {10.18653/v1/2024.findings-emnlp.596},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214724605&doi=10.18653%2Fv1%2F2024.findings-emnlp.596&partnerID=40&md5=c82588817fefb4322b111f75660b9313},
	abstract = {In this work, we systematically expose and measure the inconsistency and knowledge gaps of Large Language Models (LLMs).Specifically, we propose an automated testing framework (called KONTEST) which leverages a knowledge graph to construct test cases.KONTEST probes and measures the inconsistencies in the LLM's knowledge of the world via a combination of semantically-equivalent queries and test oracles (metamorphic or ontological oracle).KONTEST further mitigates knowledge gaps via a weighted LLM model ensemble.Using four state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that KONTEST generates 19.2% error inducing inputs (1917 errors from 9979 test inputs).It also reveals a 16.5% knowledge gap across all tested LLMs.A mitigation method informed by KONTEST's test suite reduces LLM knowledge gap by 32.48%.Our ablation study further shows that GPT3.5 is not suitable for knowledge-based consistency testing because it is only 60%-68% effective in knowledge construction. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Semantics; Structured Query Language; Automated Testing; Knowledge Based; Knowledge Gaps; Knowledge Graphs; Language Model; Model Ensembles; State Of The Art; Test Case; Test Oracles; Testing Framework; Knowledge Graph},
	keywords = {Computational linguistics; Semantics; Structured Query Language; Automated testing; Knowledge based; Knowledge gaps; Knowledge graphs; Language model; Model ensembles; State of the art; Test case; Test oracles; Testing framework; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Cao2024,
	author = {Cao, Lang and Sun, Jimeng and Cross, Adam R.},
	title = {An Automatic and End-to-End System for Rare Disease Knowledge Graph Construction Based on Ontology-Enhanced Large Language Models: Development Study},
	year = {2024},
	journal = {JMIR Medical Informatics},
	volume = {12},
	pages = {},
	doi = {10.2196/60665},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214677383&doi=10.2196%2F60665&partnerID=40&md5=481a2fdc59064483dde3b0ad07f7b144},
	abstract = {Background: Rare diseases affect millions worldwide but sometimes face limited research focus individually due to low prevalence. Many rare diseases do not have specific International Classification of Diseases, Ninth Edition (ICD-9) and Tenth Edition (ICD-10), codes and therefore cannot be reliably extracted from granular fields like “Diagnosis” and “Problem List” entries, which complicates tasks that require identification of patients with these conditions, including clinical trial recruitment and research efforts. Recent advancements in large language models (LLMs) have shown promise in automating the extraction of medical information, offering the potential to improve medical research, diagnosis, and management. However, most LLMs lack professional medical knowledge, especially concerning specific rare diseases, and cannot effectively manage rare disease data in its various ontological forms, making it unsuitable for these tasks. Objective: Our aim is to create an end-to-end system called automated rare disease mining (AutoRD), which automates the extraction of rare disease–related information from medical text, focusing on entities and their relations to other medical concepts, such as signs and symptoms. AutoRD integrates up-to-date ontologies with other structured knowledge and demonstrates superior performance in rare disease extraction tasks. We conducted various experiments to evaluate AutoRD’s performance, aiming to surpass common LLMs and traditional methods. Methods: AutoRD is a pipeline system that involves data preprocessing, entity extraction, relation extraction, entity calibration, and knowledge graph construction. We implemented this system using GPT-4 and medical knowledge graphs developed from the open-source Human Phenotype and Orphanet ontologies, using techniques such as chain-of-thought reasoning and prompt engineering. We quantitatively evaluated our system’s performance in entity extraction, relation extraction, and knowledge graph construction. The experiment used the well-curated dataset RareDis2023, which contains medical literature focused on rare disease entities and their relations, making it an ideal dataset for training and testing our methodology. Results: On the RareDis2023 dataset, AutoRD achieved an overall entity extraction F<inf>1</inf>-score of 56.1% and a relation extraction F<inf>1</inf>-score of 38.6%, marking a 14.4% improvement over the baseline LLM. Notably, the F<inf>1</inf>-score for rare disease entity extraction reached 83.5%, indicating high precision and recall in identifying rare disease mentions. These results demonstrate the effectiveness of integrating LLMs with medical ontologies in extracting complex rare disease information. Conclusions: AutoRD is an automated end-to-end system for extracting rare disease information from text to build knowledge graphs, addressing critical limitations of existing LLMs by improving identification of these diseases and connecting them to related clinical features. This work underscores the significant potential of LLMs in transforming health care, particularly in the rare disease domain. By leveraging ontology-enhanced LLMs, AutoRD constructs a robust medical knowledge base that incorporates up-to-date rare disease information, facilitating improved identification of patients and resulting in more inclusive research and trial candidacy efforts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Clinical Informatics; Data Extraction; Knowledge Graphs; Large Language Models; Llm; Machine Learning; Natural Language Processing; Ontologies; Rare Disease; Text Mining},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@CONFERENCE{2024,
	title = {JOWO 2024 - Proceedings of the Joint Ontology Workshops - Episode X: The Tukker Zomer of Ontology, and Satellite Events, co-located with the 14th International Conference on Formal Ontology in Information Systems, FOIS 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3882},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214581548&partnerID=40&md5=bcd59a2efd31d875f72a80342c127c68},
	abstract = {The proceedings contain 88 papers. The topics discussed include: understanding ASD: design and development of a domain ontology to assist professionals in understanding autistic children based on DSM-5; TAO: a document- and person-centric ontology for storing rich metadata of manuscripts; the geography of temperature space; from human cognitive expertise to ontological formalization: bridging the knowledge gap for nanophotonic calculator design and simulation; ontologies, arguments, and large-language models; what is abstraction in biomimetics?; rewriting SPARQL queries using an authorization ontology; and ontology pattern substitution: toward their use for domain ontologies. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Gupta2024,
	author = {Gupta, Saransh Kumar and Dey, Lipika and Das, Partha Pratim and Jain, Ramesh C.},
	title = {Building FKG.in: A knowledge graph for Indian food},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3882},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214574186&partnerID=40&md5=dc26c9218389b466f21493d4043457f6},
	abstract = {This paper presents an ontology design along with knowledge engineering and multilingual semantic reasoning techniques to build an automated system for assimilating culinary information for Indian food in the form of a knowledge graph. The main focus is on designing intelligent methods to derive ontology designs and capture all-encompassing knowledge about food, recipes, ingredients, cooking characteristics, and most importantly, nutrition, at scale. We present our ongoing work in this workshop paper, describe in some detail the relevant challenges in curating knowledge of Indian food, and propose our high-level ontology design. We also present a novel workflow that uses AI, LLM, and language technology to curate information from recipe blog sites in the public domain to build knowledge graphs for Indian food. The methods for knowledge curation proposed in this paper are generic and can be replicated for any domain. The design is application-agnostic and can be used for AI-driven smart analysis, building recommendation systems for Personalized Digital Health, and complementing the knowledge graph for Indian food with contextual information such as user information, food biochemistry, geographic information, agricultural information, etc. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Food Computing; Knowledge Engineering; Large Language Models; Nutrition Informatics; Ontology Design; Semantic Reasoning; Food Ingredients; Ontology; Automated Systems; Food Computing; Informatics; Knowledge Graphs; Language Model; Large Language Model; Nutrition Informatic; Ontology Design; Reasoning Techniques; Semantic Reasoning; Knowledge Graph},
	keywords = {Food ingredients; Ontology; Automated systems; Food computing; Informatics; Knowledge graphs; Language model; Large language model; Nutrition informatic; Ontology design; Reasoning techniques; Semantic reasoning; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Beverley2024,
	author = {Beverley, John C. and Franda, Francesco and Karray, Mohamed Hedi and Maxwell, Daniel T. and Benson, Carter and Smith, Barry C.},
	title = {Ontologies, Arguments, and Large-Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3882},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214573075&partnerID=40&md5=77206ea24fc3ee16e1e50095b26c08b4},
	abstract = {The explosion of interest in large language models (LLMs) has been accompanied by concerns over the extent to which generated outputs can be trusted, owing to the prevalence of bias, hallucinations, and so forth. Accordingly, there is a growing interest in the use of ontologies and knowledge graphs to make LLMs more trustworthy. This rests on the long history of ontologies and knowledge graphs in constructing human-comprehensible justification for model outputs as well as traceability concerning the impact of evidence on other evidence. Understanding the nature of arguments and argumentation is critical to each, especially when LLM output conflicts with what is expected by users. The central contribution of this article is to extend the Arguments Ontology (ARGO) - an ontology specific to the domain of argumentation and evidence broadly construed - into the space of LLM fact-checking in the interest of promoting justification and traceability research through the use of ARGO-based ‘blueprints’. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Arguments; Large Language Models; Ontology; Semantic Reasoning; Modeling Languages; Ontology; Semantics; Argument; Knowledge Graphs; Language Model; Large Language Model; Model Outputs; Ontology Graphs; Ontology's; Ontology-based; Semantic Reasoning; Knowledge Graph},
	keywords = {Modeling languages; Ontology; Semantics; Argument; Knowledge graphs; Language model; Large language model; Model outputs; Ontology graphs; Ontology's; Ontology-based; Semantic reasoning; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lopes2024,
	author = {Lopes, Alcides Gonçalves and Carbonera, Joel Lúis and Rodrigues, Fabrício Henrique Henrique and Garcia, Luan Fonseca and Abel, Mara},
	title = {How to classify domain entities into top-level ontology concepts using large language models: A study across multiple labels, resources, and languages},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3882},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214572340&partnerID=40&md5=2a7cc600567dd009cf3ccec08907c7c4},
	abstract = {Classifying domain entities into their respective top-level ontology concepts is a complex problem that typically demands manual analysis and deep expertise in the domain of interest and ontology engineering. Using an efficient approach to classify domain entities enhances data integration, interoperability, and the semantic clarity of ontologies, which are crucial for structured knowledge representation and modeling. Based on this, our main motivation is to help an ontology engineer with an automated approach to classify domain entities into top-level ontology concepts using informal definitions of these domain entities during the ontology development process. In this context, we hypothesize that the informal definitions encapsulate semantic information crucial for associating domain entities with specific top-level ontology concepts. Our approach leverages state-of-the-art language models to explore our hypothesis across multiple languages and informal definitions from different knowledge resources. In order to evaluate our proposal, we extracted multi-label datasets from the alignment of the OntoWordNet ontology and the BabelNet semantic network, covering the entire structure of the Dolce-Lite-Plus top-level ontology from most generic to most specific concepts. These datasets contain several different textual representation approaches of domain entities, including terms, example sentences, and informal definitions. Our experiments conducted 3 study cases, investigating the effectiveness of our proposal across different textual representation approaches, languages, and knowledge resources. We demonstrate that the best results are achieved using a classification pipeline with a K-Nearest Neighbor (KNN) method to classify the embedding representation of informal definitions from the Mistral large language model. The findings underscore the potential of informal definitions in reflecting top-level ontology concepts and point towards developing automated tools that could significantly aid ontology engineers during the ontology development process. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Informal Definition; Language Model; Ontology Learning; Top-level Ontology Classification; Classification (of Information); Data Encapsulation; Domain Knowledge; Knowledge Representation; Modeling Languages; Ontology; Development Process; Domain Entities; Informal Definition; Language Model; Multiple Languages; Ontology Concepts; Ontology Development; Ontology Learning; Ontology's; Top-level Ontology Classification; Semantics},
	keywords = {Classification (of information); Data encapsulation; Domain Knowledge; Knowledge representation; Modeling languages; Ontology; Development process; Domain entities; Informal definition; Language model; Multiple languages; Ontology concepts; Ontology development; Ontology learning; Ontology's; Top-level ontology classification; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Longo2024,
	author = {Longo, Carmelo Fabio and Santamaria, Daniele Francesco and Mongiovì, Misael and Bulla, Luana and Sanfilippo, Emilio M.},
	title = {Leveraging Knowledge Graphs inference for Semi-Explainable Systems based on Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3882},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214569139&partnerID=40&md5=825ad4c881d7d9dcade59aed9d11a5b4},
	abstract = {Recent advances in applying Large Language Models (LLMs) to natural language processing raise the challenge of integrating them with ontological models, to harness the features of Knowledge Graphs (KG) alongside the expressive abilities of LLMs. This paper introduces QuLIO-XR, a framework designed to integrate LLMs and ontologies, proposing an approach combining reasoning capabilities of OWL 2 with the expressive power of an LLM. Natural language text is structurally and semantically represented through the foundational ontology called LODO, which combines straightforward notation with human-like reasoning capabilities to address the issues derived from the expressive arbitrariness of natural languages. Experiments demonstrate also promising translation performances from RDF triples to the natural language, establishing QuLIO-XR as a valuable tool in the realm of LLMs explainability once fine-tuned with the same knowledge employed to build LODO KGs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Large Language Models; Natural Language Processing; Semantic Web; Latent Semantic Analysis; Natural Language Processing Systems; Ontology; Semantics; Translation (languages); Knowledge Graphs; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Ontological Modeling; Ontology's; Reasoning Capabilities; Semantic-web; Knowledge Graph},
	keywords = {Latent semantic analysis; Natural language processing systems; Ontology; Semantics; Translation (languages); Knowledge graphs; Language model; Language processing; Large language model; Natural language processing; Natural languages; Ontological modeling; Ontology's; Reasoning capabilities; Semantic-Web; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Piazza2024,
	author = {Piazza, Nancirose and Upadhayay, Bibek and Scarpa, Ronald and Behzadan, Vahid},
	title = {Large Language Models for Automatic Standardization of Cyber Deception Plans based on the Adversary Engagement Ontology},
	year = {2024},
	journal = {Proceedings - IEEE Military Communications Conference MILCOM},
	pages = {},
	doi = {10.1109/MILCOM61039.2024.10773797},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214561637&doi=10.1109%2FMILCOM61039.2024.10773797&partnerID=40&md5=6c47ba88a0ce6b2a790968dfdddcf22a},
	abstract = {Adversary Engagement Ontology (AEO) is a candidate ontology for the Unified Cyber Ontology (UCO), a community effort aimed at ontological standardization of cyber domain concepts and objects under a unifying framework. It forms a part of the Cyber Domain Ontology (CDO). In the past, community efforts and development have always been labor-intensive with regards to changes in ontology, example generation for adopters, and documentation generation. Large Language Models (LLMs), such as Claude-3.5-Sonnet and GPT4, have been proven capable of automating many tasks and aiding in human expert decision-making. Additionally, LLMs have been used in code interpretation, generation, and evaluation with efficiency and accuracy comparable to that of humans. This emergent capability of LLMs has led to the advantage of using LLMs to streamline the process of ontology development. Motivated by the aforementioned-approaches, we aim to demonstrate how these foundational LLMs can assist in ontology example generation and development, as well as be utilized to automate structured, albeit tedious tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adversary Engagement; Ontology; Decision Making; Adversary Engagement; Decisions Makings; Domain Concepts; Domain Ontologies; Human Expert; Labour-intensive; Language Model; Ontology Development; Ontology's; Plan-based; Ontology},
	keywords = {Decision making; Adversary engagement; Decisions makings; Domain concepts; Domain ontologies; Human expert; Labour-intensive; Language model; Ontology development; Ontology's; Plan-based; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Coutinho2024,
	author = {Coutinho, Matheus L.},
	title = {Leveraging LLMs in text-based ontology-driven conceptual modeling},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3882},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214559880&partnerID=40&md5=7787bbb654a022118ddc4e18f807e705},
	abstract = {This research introduces the use of a Large Language Models (LLM) assistant into the modeling tool proposed for Tonto, a text-based language for ontologies based on the Unified Foundational Ontology (UFO). By integrating Tonto with LLMs, we aim to improve conceptual modeling by employing LLMs in assisting tasks such as summarizing models, inferring meta properties of classes and the ontological nature of their instances, and creating elements based on context. This project investigates how this integration can improve modeling efficiency, accuracy, and the overall user experience with an LLM-powered assistant. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai; Large Language Models (llm); Machine Learning (ml); Ontology Development; Ontouml; Textual Language; Ufo; Adversarial Machine Learning; Computational Linguistics; Digital Elevation Model; Machine Learning; Unified Modeling Language; Foundational Ontologies; Language Model; Large Language Model; Machine-learning; Ontology Development; Ontology-driven Conceptual Modeling; Ontouml; Textual Language; Unified Foundational Ontology; Ontology},
	keywords = {Adversarial machine learning; Computational linguistics; Digital elevation model; Machine learning; Unified Modeling Language; Foundational ontologies; Language model; Large language model; Machine-learning; Ontology development; Ontology-Driven Conceptual Modeling; OntoUML; Textual language; Unified foundational ontology; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hull2024,
	author = {Hull, Richard and Bishop, Matt S. and Gendreau, Joseph D. and Levitt, Karl N. and Sadoghi, Mohammad and Lange, Matthew C.},
	title = {Conceptual modeling to advance agrifood cybersecurity ontologies},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3882},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214557424&partnerID=40&md5=288e11c6576c3b60f67e19de433f2f85},
	abstract = {Agriculture is central to the survival and comfort of the human race. In recent decades tremendous advances in the application of digital technologies increasingly enable significant efficiency, productivity, environmental sustainability and climate change resilience gains across the continuum of agrifood systems, including agricultural processes and the inputs they use, processing, product distribution, purveyance, knowledge and practice. Digital technologies now underpin new methods, practices, and equipment, altering the way we define and manage issues and indicators, meaningful metrics ranging across topics stretching from soil quality and agricultural practices, to food processing, to wholesaling/retailing, and transportation and warehousing logistics. The increasing ubiquity of digital agrifood technologies has brought a substantial expansion in the cybersecurity attack surface, in the range and kinds of cybersecurity vulnerabilities, and the magnitude of their potential consequences, which will continue to grow in the foreseeable future. As a step towards reducing the cyber risks to modern agrifood systems, this paper describes work to develop a conceptual model that will underpin a comprehensive agrifood cybersecurity ontology. This ontology would enable much smoother, structured information sharing between the agrifood and cybersecurity communities, and specifically support efficient data and knowledge sharing about cyber threats and defenses for the agrifood industries. This ontology will include systems actors/agents, the types of technologies they use and their prevalence across food systems, the cyber and social vulnerabilities associated with these actors and technologies, known and previously unseen attacks on the technologies, and best practices for preventing, detecting, mitigating and deterring cyber attacks. The approach for building this ontology includes bringing together cybersecurity and agriculture experts, applying Large Language Models, and integrating relevant existing ontologies and other structured vocabularies in the cybersecurity and agricultural spaces. At this point the team has constructed a preliminary conceptual model that can act as an initial guide for developing a formal ontology across digital local-to-global food systems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Agrifood Systems; Cybersecurity; Ontologies; Energy Security; Fertilizers; Knowledge Acquisition; Ontology; Agri-food System; Agricultural Process; Agrifood; Conceptual Model; Cyber Security; Digital Technologies; Environmental Sustainability; Food System; Human Races; Ontology's; Cyber Attacks},
	keywords = {Energy security; Fertilizers; Knowledge acquisition; Ontology; Agri-food system; Agricultural process; Agrifood; Conceptual model; Cyber security; Digital technologies; Environmental sustainability; Food system; Human races; Ontology's; Cyber attacks},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Tharaniya Sairaj2024,
	author = {Tharaniya Sairaj, R. and Balasundaram, Sadhu Ramakrishnan},
	title = {Exploring SPARQL query types to improve Ontology Mapping and Retrieval Augmented Modelling for auto-generated questions},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3882},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214557306&partnerID=40&md5=d2de42da80fa9d707bb8fc4602b53c74},
	abstract = {The use of SPARQL is evergreen for querying, extracting and expanding named entities from RDF datasets, significantly enhancing text processing applications such as Automatic Question Generation (AQG). Recent research employs pretrained Large Language Models (LLMs) for AQG to reduce manual costs, but these models are limited by their dependence on training data. This in turn makes LLMs to conterfeit the answer-revealingness challenge in automatic Multi-hop Question Generation. This challenge occurs in the process of Multi-hop Question Generation as it requires integration of named entities from multiple sources and deep comprehension of these interconnected concepts, which is quite challenging. In this context, Retrieval Augmented Models (RAM) have gained attention in NLP for improving text processing through enhanced information extraction, yet their application in AQG is limited. This research addresses this gap by RAM's workflow with attention to its first phase - Input Text Enrichment via Named Entity Expansion—is crucial for generating diverse, comprehensive questions. But, Effective named entity expansion is facilitated by ontology mapping to align entities to various ontologies, which is more demanding. To address this requirement, SPARQL querying techniques such as multi-querying, step-back querying, and sub-querying are examined to enhance named entity expansion accuracy, thereby improving RAM's efficacy, leading to well-formed auto-generated questions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Automatic Question Generation; Named Entity Set Expansion; Ontology Mapping; Retrieval Augmented Model; Semantic Similarity; Ontology; Query Languages; Query Processing; Question Answering; Semantics; Automatic Question Generation; Language Model; Multi-hops; Named Entities; Named Entity Set Expansion; Ontology Mapping; Retrieval Augmented Model; Semantic Similarity; Set Expansions; Text-processing; Structured Query Language},
	keywords = {Ontology; Query languages; Query processing; Question answering; Semantics; Automatic question generation; Language model; Multi-hops; Named entities; Named entity set expansion; Ontology mapping; Retrieval augmented model; Semantic similarity; Set expansions; Text-processing; Structured Query Language},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Langer2024,
	author = {Langer, Stefan and Neuhaus, Fabian and Nürnberger, Andreas},
	title = {CEAR: Automatic construction of a knowledge graph of chemical entities and roles from scientific literature},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3882},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214555647&partnerID=40&md5=04edc0d3b18355b78d45472177627d17},
	abstract = {Ontologies are formal representations of knowledge in specific domains that provide a structured framework for organizing and understanding complex information. Creating ontologies, however, is a complex and time-consuming endeavor. ChEBI is a well-known ontology in the field of chemistry, which provides a comprehensive resource for defining chemical entities and their properties. However, it covers only a small fraction of the rapidly growing knowledge in chemistry and does not provide references to the scientific literature. To address this, we propose a methodology that involves augmenting existing annotated text corpora with knowledge from Chebi and fine-tuning a large language model (LLM) to recognize chemical entities and their roles in scientific text. Our experiments demonstrate the effectiveness of our approach. By combining ontological knowledge and the language understanding capabilities of LLMs, we achieve high precision and recall rates in identifying both the chemical entities and roles in scientific literature. Furthermore, we extract them from a set of 8,000 ChemRxiv articles, and apply a second LLM to create a knowledge graph (KG) of chemical entities and roles (CEAR), which provides complementary information to ChEBI, and can help to extend it. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Chebi; Knowledge Graphs; Large Language Models; Named Entity Recognition; Ontologies; Ontology; Automatic Construction; Chebi; Chemical Entity; Formal Representations; Knowledge Graphs; Language Model; Large Language Model; Named Entity Recognition; Ontology's; Scientific Literature; Knowledge Graph},
	keywords = {Ontology; Automatic construction; ChEBI; Chemical entity; Formal representations; Knowledge graphs; Language model; Large language model; Named entity recognition; Ontology's; Scientific literature; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Schaeffer20241,
	author = {Schaeffer, Marion and Sesboüé, Matthias and Charbonnier, Léa and Delestre, Nicolas and Kotowicz, Jean Philippe and Zanni-Merk, Cécilia},
	title = {On the Pertinence of LLMs for Ontology Learning},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3874},
	pages = {1 - 18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214247692&partnerID=40&md5=a817900503c26a5320cf55725e593f4d},
	abstract = {Ontology learning from text is traditionally approached as sub-tasks tackled with linguistic, statistical or logic-based methods. Large language models and their generation capabilities have recently caught much interest. We investigate the pertinence of such generative models for ontology learning. We evaluate the created ontologies on two different use cases by aligning with a reference ontology and compare components for each sub-task using the OLAF ontology learning framework. In addition to demonstrating the relevance of large language models for ontology learning, we discuss component combinations, LLM size, and environmental impact in creating efficient pipelines while limiting resource consumption. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Construction; Llm; Ontology Learning; Adversarial Machine Learning; Federated Learning; Knowledge Graph; Ontology; Generative Model; Graph Construction; Knowledge Graph Construction; Knowledge Graphs; Language Model; Learning From Texts; Llm; Ontology Learning; Ontology's; Subtask; Contrastive Learning},
	keywords = {Adversarial machine learning; Federated learning; Knowledge graph; Ontology; Generative model; Graph construction; Knowledge graph construction; Knowledge graphs; Language model; Learning from texts; LLM; Ontology learning; Ontology's; Subtask; Contrastive Learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bakker202470,
	author = {Bakker, Roos M. and Di Scala, Daan L. and de Boer, Maaike H.T.},
	title = {Ontology Learning from Text: an Analysis on LLM Performance},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3874},
	pages = {70 - 87},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214243423&partnerID=40&md5=8356243bab75422799ef66c7e3d26eff},
	abstract = {Ontologies provide a structured framework to represent and integrate domain knowledge. Developing them is a complex and time-consuming task, requiring domain expertise to ensure accuracy and consistency. Ontology learning aims to automate this process by learning full ontologies, or parts of them, from sources such as textual data. In this paper, we research the potential of Large Language Models (LLMs), specifically GPT-4o, in ontology learning, using a real-world use case. We introduce a manually constructed ontology based on knowledge in a news article, and compare it to ontologies extracted using three different prompting approaches over multiple runs. The resulting ontologies are evaluated both quantitatively and qualitatively, to ensure that differences in performance due to modelling choices are also considered. The results show that, while the LLM effectively identifies important classes and individuals, it often does not include properties between classes, and adds inconsistent and incorrect properties between individuals. Prompting on a sentence level leads to more correct individuals and properties, however, quantitative evaluation shows more hallucinations and incorrect triples. Despite these issues, LLMs advance previous ontology learning methods by considering classes, individuals, and properties as a whole, creating a more complete ontology rather than isolated elements. This provides a new perspective on ontology learning and highlights the potential of LLMs to offer a first version of an ontology or an extension to an existing one based on new information. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Information Extraction; Knowledge Graph Extraction; Large Language Models; Ontology Learning; Adversarial Machine Learning; Federated Learning; Knowledge Graph; Graph Extractions; Information Extraction; Knowledge Graph Extraction; Knowledge Graphs; Language Model; Large Language Model; Learning From Texts; Ontology Learning; Ontology's; Property; Contrastive Learning},
	keywords = {Adversarial machine learning; Federated learning; Knowledge graph; Graph extractions; Information extraction; Knowledge graph extraction; Knowledge graphs; Language model; Large language model; Learning from texts; Ontology learning; Ontology's; Property; Contrastive Learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {AIxEDU 2024 - Proceedings of the 2nd International Workshop on Artificial INtelligent Systems in Education, co-located with 23rd International Conference of the Italian Association for Artificial Intelligence, AIxIA 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3879},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214242226&partnerID=40&md5=9e46d795ec69e5b966dee3bc3682d29f},
	abstract = {The proceedings contain 16 papers. The topics discussed include: enhancing instructional design: the impact of CONALI ontology and ChatGPT in primary education training; generative AI for teaching Latin and Greek in high school; fostering metacognitive skills in programming: leveraging AI to reflect on code; speeding up design and making to reduce time-to-project and time-to-market: an AI-enhanced approach in engineering education; enhancing educational outcomes and well-being through technology-supported object-based learning: the RESTART Project at the University of Tor Vergata; EULER: fine-tuning a large language model for Socratic interactions; large language models for the assessment of students’ authentic tasks: a replication study in higher education; and uninvited generative ai has joined our students: tackling disinformation and creating content with the help of AI apps. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Molinari2024,
	author = {Molinari, Andrea and Sandri, Simone},
	title = {Evolution of LMS Design and Implementation in the Age of AI and Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3879},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214206144&partnerID=40&md5=d001822ff7fa4fe9baea5f9da4905800},
	abstract = {The integration of generative AI and large language models (LLMs) into Learning Management Systems (LMSs) offers significant potential but remains underutilized. Current implementations typically involve the LMS requesting services from an AI engine, such as responses, suggestions, and content summaries, reflecting a superficial interaction between two distinct systems. This limited integration arises from the architecture of most major LMS platforms (e.g., Moodle, Docebo, Sakai), which were designed in the early 2000s. These systems are structured around relational databases and multi-layered logic, which separates business logic from the user interface. While they offer extensive multimedia resources for education, their content is generally organized into folders based on modules or lessons, limiting their adaptability to modern AI functionalities. Additionally, while some platforms support text-based interactions (forums, FAQs, blogs), their overall design remains outdated. This paper proposes a new software architecture for LMSs, designed to natively support AI functionalities. It advocates for a shift from relational to graph-based models for content storage, allowing the creation of a knowledge graph that integrates ontological frameworks. This transformation would enable more advanced AI-driven functionalities, such as content recommendations, semantic searches, and personalized learning experiences. We also address the challenges inherent in such a transformation, including a) the need for a complete redesign of the platform's persistence and business logic layers; b) the complexity of integrating multi-domain ontologies within educational institutions; and c) the technological hurdles in training large, adaptive language models that can evolve in real-time, especially during periods of high content generation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graphs; Learning Management Systems; Named-entity Recognition; C (programming Language); Ontology; Relational Database Systems; Semantics; 'current; Creative Commons; Design And Implementations; Knowledge Graphs; Language Model; Learning Management System; Multi-layered; Named Entity Recognition; Relational Database; System Platforms; Knowledge Graph},
	keywords = {C (programming language); Ontology; Relational database systems; Semantics; 'current; Creative Commons; Design and implementations; Knowledge graphs; Language model; Learning management system; Multi-layered; Named entity recognition; Relational Database; System platforms; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2024,
	title = {NLP4KGC 2024 - Proceedings of the 3rd International Workshop on Natural Language Processing for Knowledge Graph Creation, co-located with 20th International Conference on Semantic Systems, SEMANTiCS 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3874},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214203615&partnerID=40&md5=dd68a799fc0ffdd80b02ba49c9de8999},
	abstract = {The proceedings contain 11 papers. The topics discussed include: on the pertinence of LLMs for ontology learning; towards the automation of knowledge graph construction using large language models; assessing SPARQL capabilities of large language models; breaking down financial news impact: a novel ai approach with geometric hypergraphs; ontology learning from text: an analysis on LLM performance; accessing the capabilities of KGs and LLMs in mapping indicators within sustainability reporting standards; converting fire safety regulations to SHACL shapes using natural language processing; ontology-based dataset discovery in the BUILDSPACE data management platform; pruning cycles in UMLS Metathesaurus: a neuro symbolic ai approach; and automated generation of competency questions using large language models and knowledge graphs. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kommineni202419,
	author = {Kommineni, Vamsi Krishna and König-Ries, Birgitta and Samuel, Sheeba},
	title = {Towards the Automation of Knowledge Graph Construction Using Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3874},
	pages = {19 - 34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85214197878&partnerID=40&md5=c4ce0ae9c8b2f8c98f2364b50cb3c716},
	abstract = {The conventional process of building Ontologies and Knowledge Graphs (KGs) heavily relies on human domain experts to define entities and relationship types, establish hierarchies, maintain relevance to the domain, fill the ABox (i.e., populate with instances), and ensure data quality (including amongst others accuracy and completeness). On the other hand, Large Language Models (LLMs) have recently gained popularity for their ability to understand and generate human-like natural language, offering promising ways to automate aspects of this process. This work explores the (semi-)automatic construction of KGs facilitated by different state-of-the-art LLMs: Mixtral 8x22B Instruct v0.1, GPT-4o, GPT-3.5, and Gemini. Our pipeline involves formulating competency questions (CQs), developing an ontology (TBox) based on these CQs, constructing KGs using the developed ontology, and evaluating the resultant KG with minimal to no involvement of human experts. We showcase the feasibility of our semi-automated pipeline by creating a KG on deep learning methodologies by exploiting scholarly publications. The answers generated via Retrieval-Augmented-Generation (RAG) were evaluated by a domain expert manually, and the KG was evaluated by matching the KG individuals to RAG-generated answers. Our findings suggest that employing LLMs could potentially reduce the human effort involved in the construction of KGs, although a human-in-the-loop approach is recommended to evaluate automatically generated KGs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Competency Questions; Knowledge Graphs; Large Language Models; Ontology; Retrieval-augmented Generation; Deep Learning; Domain Knowledge; Modeling Languages; Natural Language Processing Systems; Ontology; Search Engines; Competency Question; Domain Experts; Graph Construction; Human Domain; Knowledge Graphs; Language Model; Large Language Model; Ontology Graphs; Ontology's; Retrieval-augmented Generation; Knowledge Graph},
	keywords = {Deep learning; Domain Knowledge; Modeling languages; Natural language processing systems; Ontology; Search engines; Competency question; Domain experts; Graph construction; Human domain; Knowledge graphs; Language model; Large language model; Ontology graphs; Ontology's; Retrieval-augmented generation; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Tang2024,
	author = {Tang, Rongjiang and You, Zeyu and Wei, Yanhong and Li, Jian and Liang, Xiu},
	title = {Automatic generation of 2D surface topography specifications based on wed ontology language},
	year = {2024},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {13444},
	pages = {},
	doi = {10.1117/12.3056134},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213811990&doi=10.1117%2F12.3056134&partnerID=40&md5=3e94656bbd7fbbe7747d720fccc4b69c},
	abstract = {An ontology-based automatic two-dimensional surface topography specification (TSTS) generation method was proposed to solve the problem of different designers' different TSTS generation results and semantic interoperability during the process. According to the existing category model representing two-dimensional (2D) surface topography knowledge and the EXPRESS-language model representing Computer Aided Design (CAD) system extraction information, a knowledge representation model is constructed consisting of part surface layer, extraction information layer, TSTS information layer, and part information mapping layer. The part surface layer, extraction information layer, and 2D surface morphology feature information layer are constructed as meta-ontology using Web Ontology Language to reuse or inherit 2D surface morphology knowledge in specific fields. The part information mapping layer is represented by Semantic Web Rule Language (SWRL). Then, the meta-ontology and knowledge based on the SWRL representation are mapped to Java expert systems shell (JESS) facts and JESS rules, and the JESS inference machine is used to generate the TSTS. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Representation Mode; Semantic Web Rule Language; Two-dimensional Surface Topography Specification; Wed Ontology Language; Inference Engines; Mapping; Modeling Languages; Ontology; Problem Oriented Languages; Semantics; Thesauri; Information Layers; Java Expert System Shell; Ontology Language; Representation Mode; Rules Languages; Semantic Web Rule Language; Semantic Web Rules; Two-dimensional Surface; Two-dimensional Surface Topography Specification; Wed Ontology Language; Knowledge Representation},
	keywords = {Inference engines; Mapping; Modeling languages; Ontology; Problem oriented languages; Semantics; Thesauri; Information layers; Java expert system shell; Ontology language; Representation mode; Rules languages; Semantic web rule language; Semantic Web rules; Two-dimensional surface; Two-dimensional surface topography specification; Wed ontology language; Knowledge representation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Loffredo202437,
	author = {Loffredo, Rocco and de Santo, Massimo},
	title = {Using Ontologies for LLM Applications in Cultural Heritage},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3865},
	pages = {37 - 43},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213724566&partnerID=40&md5=f016cf214de3b6a96a88252565f5c838},
	abstract = {Applying Large Language models (LLMs) offers the potential for transformative change in cultural heritage. This short paper is based on ongoing doctoral research. It examines innovative methodologies for enhancing the accessibility, comprehension, and preservation of cultural heritage by utilizing AI technologies such as LLM. This research aims to improve AI-generated responses' contextual precision and dependability by employing sophisticated knowledge representations, such as ontologies. The approach promises to overcome the challenges associated with data complexity and information retrieval, thereby creating new avenues for heritage documentation, education, and public engagement. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cultural Heritage; Large Language Model; Ontology; Retrieval Augmented Generation; Knowledge Representation; Ai Technologies; Cultural Heritages; Doctoral Research; Innovative Methodologies; Knowledge-representation; Language Model; Large Language Model; Model Application; Ontology's; Retrieval Augmented Generation; Ontology},
	keywords = {Knowledge representation; AI Technologies; Cultural heritages; Doctoral research; Innovative methodologies; Knowledge-representation; Language model; Large language model; Model application; Ontology's; Retrieval augmented generation; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{2024,
	title = {IAI4CH 2024 - Proceedings of the 3rd Workshop on Artificial Intelligence for Cultural Heritage, co-located with the 23rd International Conference of the Italian Association for Artificial Intelligence, AIxIA 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3865},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213708974&partnerID=40&md5=77724c8f72714ba8af197480368c1b16},
	abstract = {The proceedings contain 15 papers. The topics discussed include: new frontiers in digital libraries: the trajectory of digital humanities through a computational lens; how BERT speaks Shakespearean English? evaluating historical bias in masked language models; constructing a knowledge graph for Italian cinema divas’ autobiographies; transfer learning for renaissance illuminated manuscripts: starting a journey from classification to interpretation; saliency-driven 3D reconstruction and printing for accessible museums; using ontologies for LLM applications in cultural heritage; intelligent human-computer interaction in innovative ai solutions in travel and its impact on the e-society; digital augmented reality app for historic architecture; ontological representation of narrative places for cinema archives; and developing a comprehensive dataset for enhancing social inclusion and cohesion through citizen curation in cultural heritage. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bernasconi202477,
	author = {Bernasconi, Eleonora and Ferilli, Stefano},
	title = {Semantic Label Property Graph Ontologies: A Methodology for Enhanced Data Management in Digital Libraries},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3865},
	pages = {77 - 84},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213708014&partnerID=40&md5=e08f9a45847ec1264c35ec62d1820708},
	abstract = {Ontologies are crucial for managing and integrating diverse datasets in digital libraries, where data heterogeneity poses ongoing challenges. This paper presents a novel framework specifically designed to address the unique needs of digital libraries using Semantic Label Property Graphs. Our methodology aligns with semantic web standards, offering a sophisticated approach to data management that enhances integration, querying, and visualization of complex datasets. The proposed framework supports automated ontology generation, advanced semantic integration, and seamless visualization, leveraging the structural efficiency of Property Graphs with semantic annotations to optimize resource discovery, management, and retrieval. We detail the architecture and core functionalities of the framework, demonstrating its adaptability in managing complex ontologies and improving workflows for researchers and practitioners. Empirical evaluations reveal significant performance improvements in data management and linked data integration, underscoring the framework's potential to streamline workflows and enhance semantic interoperability. This innovative approach addresses the evolving challenges of large-scale data management, positioning the framework as a valuable tool for the future of digital libraries. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Digital Libraries; Label Property Graph; Large Language Models; Schema Management; Semantic Ontologies; Semantic Web; Label Property Graph; Language Model; Large Language Model; Ontology's; Property; Schema Management; Semantic Labels; Semantic Ontology; Semantic-web; Work-flows; Semantics},
	keywords = {Label property graph; Language model; Large language model; Ontology's; Property; Schema management; Semantic labels; Semantic ontology; Semantic-Web; Work-flows; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ling20242148,
	author = {Ling, Hongtao and Harzallah, Mounira and Bernelin, Margo and Marinica, Claudia and Serrano-Alvarado, Patricia},
	title = {A new incremental pipeline for concept formation driven by prior knowledge: Application on the AI Act domain},
	year = {2024},
	journal = {Procedia Computer Science},
	volume = {246},
	number = {C},
	pages = {2148 - 2157},
	doi = {10.1016/j.procs.2024.09.618},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213329685&doi=10.1016%2Fj.procs.2024.09.618&partnerID=40&md5=de7233c04a3f407dd8694add9c7c1636},
	abstract = {In the Ontology Learning research domain, despite recent advancements, the performance of current non or semi-supervised approaches for concept formation remains sub-optimal, particularly from a single, small-sized corpus for a specialized domain. In order to answer the performance drawback, this paper introduces a novel pipeline, called CO-ISSC (Core Ontology-based Incremental Semi-Supervised Clustering), for concept formation towards ontology learning. This pipeline uses a PLM (Pre-trained Language Model) and combines in an incremental manner a semi-supervised dimension reduction technique and a clustering technique, guided by core concepts as prior knowledge to align results with the ontology domain. Its incremental nature enhances prior knowledge and boosts its performance. The CO-ISSC pipeline's performance is evaluated on the recent and significant AI Act text established by the European Union, which aims to ensure the safety, transparency, and non-discrimination of AI systems. To this end, we manually built a benchmark terminology for the AI Act domain given that no reference model exists yet. The results demonstrate promising performance of the CO-ISSC pipeline, outperforming baseline non-supervised or semi-supervised approaches such as DBSCAN, similarity measure based approaches, support vector machines and ANN. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Act Domain; Clustering; Concept Formation; Dimension Reduction; Incremental Pipeline; Ontology Learning; Prior Knowledge; Adversarial Machine Learning; Contrastive Learning; Domain Knowledge; Ontology; Self-supervised Learning; Semi-supervised Learning; Ai Act Domain; Clusterings; Concept Formation; Dimension Reduction; Incremental Pipeline; Ontology Learning; Ontology-based; Performance; Prior-knowledge; Semi-supervised; Support Vector Machines},
	keywords = {Adversarial machine learning; Contrastive Learning; Domain Knowledge; Ontology; Self-supervised learning; Semi-supervised learning; AI act domain; Clusterings; Concept formation; Dimension reduction; Incremental pipeline; Ontology learning; Ontology-based; Performance; Prior-knowledge; Semi-supervised; Support vector machines},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Macilenti20241289,
	author = {Macilenti, Giulio and Stellato, Armando and Fiorelli, Manuel},
	title = {Prompting is not all you need Evaluating GPT-4 performance on a real-world ontology alignment use case},
	year = {2024},
	journal = {Procedia Computer Science},
	volume = {246},
	number = {C},
	pages = {1289 - 1298},
	doi = {10.1016/j.procs.2024.09.557},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213321670&doi=10.1016%2Fj.procs.2024.09.557&partnerID=40&md5=4395b728eb94e98a4e4f110b2a408f1f},
	abstract = {Ontology Alignment (OA) is a complex, demanding and error-prone task, requiring the intervention of domain and Semantic Web experts. Automating the alignment process thus becomes a must-do, especially when involving large datasets, to at least produce a first input for human experts. Automated ontology alignment could benefit from the outstanding language ability of Large Language Models (LLMs), which could implicitly provide the background knowledge that has been the Achilles' heel of traditional alignment systems. However, this requires a correct evaluation of the performance of LLMs and understanding the best way to incorporate them into more specific tools. In this paper, we show that a naive prompting approach on the popular GPT-4 model could face several problems when transferred to real-world use cases. To this end, we replicated the methods of Norouzi et al. (2023), applied to the OAEI 2022 conference track, on a reference alignment between a pair of datasets (reduced versions of two popular thesauri: European Commission's EuroVoc and TESEO, from the Italian Senate of the Republic), which has never been tested in OAEI evaluation campaigns. This reference alignment has several features common to real-world use cases: it is has a larger size than those considered in the study we replicated, it is not published online and is therefore not subject to data contamination and it involves relations between concepts that are more complex than simple equivalence. The replicated methods achieved a significantly lower performance on our reference alignment than on the OAEI 2022 conference track, suggesting that size, data contamination, and semantic complexity need to be considered when using LLMs for the alignment task. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Ontology Alignment; Semantic Technologies; Latent Semantic Analysis; Ontology; Error Prone Tasks; Human Expert; Language Model; Large Datasets; Large Language Model; Ontology Alignment; Performance; Real-world; Semantic Technologies; Semantic-web; Semantics},
	keywords = {Latent semantic analysis; Ontology; Error prone tasks; Human expert; Language model; Large datasets; Large language model; Ontology alignment; Performance; Real-world; Semantic technologies; Semantic-Web; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{2024,
	title = {8th International Conference on Inventive Communication and Computational Technologies, ICICCT 2024},
	year = {2024},
	journal = {Lecture Notes in Networks and Systems},
	volume = {23 LNNS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213302444&partnerID=40&md5=0002488e211b2448101cd033826a1f8f},
	abstract = {The proceedings contain 75 papers. The special focus in this conference is on Inventive Communication and Computational Technologies. The topics include: Performance Insights of Attention-Free Language Models in Sentiment Analysis: A Case Study for E-Commerce Platforms in Vietnam; An AutoML Approach Integrated with Live Weather Data in Rain Forecasting System (RFS); decentralized Identity Management Using Self-Sovereign Identity Approach Through Blockchain; Stock Price Prediction Model Using Enhanced LSTM and ARIMA; prefetching Mechanism for Distributed Cache Architecture: Trends and Challenges; a Concise Review of Crop Disease Identification: Integrating Conventional and Deep Learning Feature Extraction for Effective Diagnosis and Mitigation Strategies; fortifying Health-Care Data Security in Cloud Environments; Review on the Enhancement of 5G Communications Using LEO Satellites; Design and Implementation of a Python-Based GUI Tool for Eye-Hand Coordination Analysis; detection of Strabismus Using Convolutional Neural Network-Based Classification Models; video Anomaly Detection Using Liquid Neural Networks; a Blockchain Solution to Counterfeiting in the Semiconductor Supply Chain; performance Analysis of Various Encryption Algorithms for Securing Modules of Educational Chatbot; implementation of a Temperature Monitoring System Utilizing Cortex-M3 with I2C-Based Sensor Integration; Unifying Perspectives: CNN-LSTM Integration for Anxiety and Depression Prediction Through Textual Analysis; An Architectural Methodology for Developing Domain Ontology for Efficient Knowledge Management for AI Systems; A Novel Hybrid Integration of BERT and Conventional Machine Learning Techniques for Robust Airline Twitter Sentiment Analysis; an Automated System with Deep Learning Technique for Posting Water-Related Issues; Reliable Smart Wrist Pulse Oximeter for Hypoxemia and COVID-19 Patients; Optimizing Lettuce Crop Growth Modeling with XGBoost-SVM and Gaussian Process Regression Fusion; Detection of Printed Circuit Board (PCB) Defects Using Deep Learning Approach. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ji2024,
	author = {Ji, Yongxin and Shang, Jiayu and Guan, Jiaojiao and Zou, Wei and Liao, Herui and Tang, Xubo and Sun, Yanni},
	title = {PlasGO: enhancing GO-based function prediction for plasmid-encoded proteins based on genetic structure},
	year = {2024},
	journal = {GigaScience},
	volume = {13},
	pages = {},
	doi = {10.1093/gigascience/giae104},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212964338&doi=10.1093%2Fgigascience%2Fgiae104&partnerID=40&md5=70aa0e97e117fecfa6a0247adf83153d},
	abstract = {Background: Plasmid, as a mobile genetic element, plays a pivotal role in facilitating the transfer of traits, such as antimicrobial resistance, among the bacterial community. Annotating plasmid-encoded proteins with the widely used Gene Ontology (GO) vocabulary is a fundamental step in various tasks, including plasmid mobility classification. However, GO prediction for plasmid-encoded proteins faces 2 major challenges: the high diversity of functions and the limited availability of high-quality GO annotations. Results: In this study, we introduce PlasGO, a tool that leverages a hierarchical architecture to predict GO terms for plasmid proteins. PlasGO utilizes a powerful protein language model to learn the local context within protein sentences and a BERT model to capture the global context within plasmid sentences. Additionally, PlasGO allows users to control the precision by incorporating a self-attention confidence weighting mechanism. We rigorously evaluated PlasGO and benchmarked it against 7 state-of-the-art tools in a series of experiments. The experimental results collectively demonstrate that PlasGO has achieved commendable performance. PlasGO significantly expanded the annotations of the plasmid-encoded protein database by assigning high-confidence GO terms to over 95% of previously unannotated proteins, showcasing impressive precision of 0.8229, 0.7941, and 0.8870 for the 3 GO categories, respectively, as measured on the novel protein test set. Conclusions: PlasGO, a hierarchical tool incorporating protein language models and BERT, significantly expanded plasmid protein annotations by predicting high-confidence GO terms. These annotations have been compiled into a database, which will serve as a valuable contribution to downstream plasmid analysis and research. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Go Term Prediction; Plasmid Protein Function; Protein Language Model; Bacterial Proteins; Antibiotic Resistance; Article; Gene Ontology; Gene Structure; Human; Microbial Community; Mobile Genetic Element; Plasmid; Prediction; Protein Database; Protein Function; Protein Language Model; Bioinformatics; Genetics; Metabolism; Molecular Genetics; Procedures; Software; Bacterial Protein; Bacterial Proteins; Computational Biology; Databases, Protein; Gene Ontology; Molecular Sequence Annotation; Plasmids; Software},
	keywords = {antibiotic resistance; article; gene ontology; gene structure; human; microbial community; mobile genetic element; plasmid; prediction; protein database; protein function; protein language model; bioinformatics; genetics; metabolism; molecular genetics; procedures; software; bacterial protein; Bacterial Proteins; Computational Biology; Databases, Protein; Gene Ontology; Molecular Sequence Annotation; Plasmids; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Crum2024,
	author = {Crum, Elias and De Santis, Antonio and Ovide, Manon and Pan, Jiaxin and Pisu, Alessia and Lazzari, Nicolas and Rudolph, Sebastian},
	title = {Enriching Ontologies with Disjointness Axioms using Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3853},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212711225&partnerID=40&md5=8e4efddbdf745c0480df90e78e4b6d2b},
	abstract = {Ontologies often lack explicit disjointness declarations between classes, despite their usefulness for sophisticated reasoning and consistency checking in Knowledge Graphs. In this study, we explore the potential of Large Language Models (LLMs) to enrich ontologies by identifying and asserting class disjointness axioms. Our approach aims at leveraging the implicit knowledge embedded in LLMs, using prompt engineering to elicit this knowledge for classifying ontological disjointness. We validate our methodology on the DBpedia ontology, focusing on open-source LLMs. Our findings suggest that LLMs, when guided by effective prompt strategies, can reliably identify disjoint class relationships, thus streamlining the process of ontology completion without extensive manual input. For comprehensive disjointness enrichment, we propose a process that takes logical relationships between disjointness and subclass statements into account in order to maintain satisfiability and reduce the number of calls to the LLM. This work provides a foundation for future applications of LLMs in automated ontology enhancement and offers insights into optimizing LLM performance through strategic prompt design. Our code is publicly available on GitHub at https://github.com/n28div/llm-disjointness. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Disjointness Learning; Large Language Models; Ontology Enrichment; Contrastive Learning; Knowledge Graph; Consistency Checking; Dbpedia; Disjointness; Disjointness Learning; Implicit Knowledge; Knowledge Graphs; Language Model; Large Language Model; Ontology Enrichment; Ontology's; Ontology},
	keywords = {Contrastive Learning; Knowledge graph; Consistency checking; Dbpedia; Disjointness; Disjointness learning; Implicit knowledge; Knowledge graphs; Language model; Large language model; Ontology enrichment; Ontology's; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Vrolijk2024,
	author = {Vrolijk, Jarno and Poslavsky, Victor and Bijl, Thijmen and Popov, Maksim and Mahdavi, Rana and Shokri, Mohammad},
	title = {Ontology Learning for ESCO: Leveraging LLMs to Navigate Labor Dynamics},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3853},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212710838&partnerID=40&md5=c99afbb428f4560ed7610a55af6ef3b2},
	abstract = {The labor market is a dynamic environment that supports numerous knowledge-driven applications through ontologies, such as ESCO and O*NET. Maintaining the relevance and accuracy of information within these ontologies and taxonomies is both resource-intensive and time-consuming. In this paper, we propose an ontology learning system that utilizes self-supervised learning, retrieval-augmented generation, and autoregressive language models to identify, classify, and link labor market mentions and entities from raw job postings. Additionally, we demonstrate the language model's ability to discover "alternative labels" and "preferred labels", and perform relation classification. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Natural Language Processing; Ontology Learning; Adversarial Machine Learning; Commerce; Knowledge Graph; Ontology; Semi-supervised Learning; Wages; Dynamic Environments; Knowledge Graphs; Labor Dynamics; Labour Market; Language Model; Language Processing; Natural Language Processing; Natural Languages; Ontology Learning; Ontology's; Self-supervised Learning},
	keywords = {Adversarial machine learning; Commerce; Knowledge graph; Ontology; Semi-supervised learning; Wages; Dynamic environments; Knowledge graphs; Labor dynamics; Labour market; Language model; Language processing; Natural language processing; Natural languages; Ontology learning; Ontology's; Self-supervised learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Díaz2024,
	author = {Díaz, Camila and Dunstan, Jocelyn and Etcheverry, Lorena and Fonck, Antonia and Grez, Alejandro and Mery, Domingo and Reutter, Juan L. and Rojas, Hugo},
	title = {Automatic knowledge-graph creation from historical documents: The Chilean dictatorship as a case study},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3853},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212705684&partnerID=40&md5=d0fcc6d861c72eaca0134114f37b278e},
	abstract = {We present our results regarding the construction of a knowledge graph from historical documents related to the Chilean dictatorship period (1973-1990). Our approach uses LLMs to automatically recognize entities and relations between them and resolve conflicts between these values. To prevent hallucination, the interaction with the LLM is grounded in a simple ontology with four types of entities and seven types of relations. To evaluate our architecture, we use a gold standard graph constructed using a small subset of the documents, and compare this to the graph obtained from our approach when processing the same set of documents. Results show that the automatic construction manages to recognize a good portion of all the entities in the gold standard and that those not recognized are explained mainly by the level of granularity in which the information is structured in the graph and not because the automatic approach misses an important entity in the graph. Looking forward, we expect this report to encourage work on other similar projects focused on enhancing research in humanities and social science. However, we remark that better evaluation metrics are needed to accurately fine-tune these types of architectures. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {History; Ontology; Automatic Approaches; Automatic Construction; Case-studies; Gold Standards; Historical Documents; Humanities And Social Science; Knowledge Graphs; Ontology's; Simple++; Types Of Relations; Knowledge Graph},
	keywords = {History; Ontology; Automatic approaches; Automatic construction; Case-studies; Gold standards; Historical documents; Humanities and social science; Knowledge graphs; Ontology's; Simple++; Types of relations; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hertling2024,
	author = {Hertling, Sven and Sack, Harald},
	title = {Towards Large Language Models Interacting with Knowledge Graphs Via Function Calling},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3853},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212704419&partnerID=40&md5=0f837f0ab24f9d2027035b1f583f7ebe},
	abstract = {This position paper introduces a new way for large language models (LLMs) to interact with Knowledge Graphs (KGs), especially for information extraction. Most relation extraction approaches focus on finding the corresponding textual spans for subject, relation, and object in a text but ignore the actual state of the KG and the schema defined therein. In this work, the function-calling possibility of LLMs is explored to search for already defined entities in the KG and add information in the form of triples to the KG. A huge improvement over previous techniques is that it can also deal with various large ontologies/KGs and that the added information fits to a provided KG on a schema and instance level. Further validation checks can be added to increase the quality of the extracted statements. A small playground is provided to analyze the effectiveness of the approach. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Mining; Information Retrieval; Modeling Languages; Knowledge Graphs; Language Model; Ontology's; Position Papers; Relation Extraction; Validation Checks; Knowledge Graph},
	keywords = {Data mining; Information retrieval; Modeling languages; Knowledge graphs; Language model; Ontology's; Position papers; Relation extraction; Validation checks; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hao2024,
	author = {Hao, Xubing and Cui, Licong and Tao, Cui and Roberts, Kirk E. and Amith, Muhammad Tuan},
	title = {Analyzing Llama 3-based Approach for Axiom Translation from Ontologies},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3853},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212688634&partnerID=40&md5=2bb61c3b90845531b2a9b19696705ec7},
	abstract = {Ontology development involves a top-down approach where ontology engineers and domain experts collaboratively define and evaluate ontological elements and axioms. Translating ontology axioms into natural language can significantly aid in ontology evaluation by making the content more understandable to subject matter experts who may lack a background in knowledge engineering. In this preliminary study, we investigate the potential of large language models (LLMs) in axiom translation from ontologies to facilitate ontology evaluation. We utilize Llama 3 to translate 1,192 ontology axioms across 19 distinct axiom types from five published ontologies. Results show that 163 (13.67%) of the Llama 3 translation of the axiom are accurately represented, 268 (22.48%) are not accurately represented, and 761 (63.84%) are partially accurate. Our manual evaluation of the Llama 3 translation indicates some competency in producing hierarchical natural language equivalents while revealing some limitations when translating complex axioms. Nonetheless, there are opportunities to improve the results with few-shot training or using LLMs to provide support in knowledge engineering for ontologies. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology; Translation (languages); Domain Experts; Language Model; Natural Languages; Ontological Elements; Ontology Axioms; Ontology Development; Ontology Evaluations; Ontology's; Subject Matter Experts; Top Down Approaches; Knowledge Engineering},
	keywords = {Ontology; Translation (languages); Domain experts; Language model; Natural languages; Ontological elements; Ontology axioms; Ontology development; Ontology evaluations; Ontology's; Subject matter experts; Top down approaches; Knowledge engineering},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {KBC-LM-LM-KBC 2024 - Joint Proceedings of the 2nd Workshop on Knowledge Base Construction from Pre-Trained Language Models and the 3rd Challenge on Language Models for Knowledge Base Construction, co-located with the 23nd International Semantic Web Conference, ISWC 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3853},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212674154&partnerID=40&md5=f6e6e461a5f87fb9df6a460a10e2ac8a},
	abstract = {The proceedings contain 14 papers. The topics discussed include: enriching ontologies with disjointness axioms using large language models; ontology learning for ESCO: leveraging LLMs to navigate labor dynamics; ontology-guided on-device conversational knowledge capture with large language models; the effects of hallucinations in synthetic training data for relation extraction; analyzing Llama 3-based approach for axiom translation from ontologies; LLM store: leveraging large language models as sources of Wikidata-structured knowledge; towards large language models interacting with knowledge graphs via function calling; automatic knowledge-graph creation from historical documents: the Chilean dictatorship as a case study; and HybridContextQA: a hybrid approach for complex question answering using knowledge graph construction and context retrieval with LLMs. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Çöplü2024,
	author = {Çöplü, Tolga and Bendiken, Arto and Skomorokhov, Andrii and Bateiko, Eduard and Cobb, Stephen},
	title = {Ontology-Guided On-Device Conversational Knowledge Capture with Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3853},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212670794&partnerID=40&md5=ed0f3bd8cee345316a2d76349d063d84},
	abstract = {Generative AI applications must integrate users' personal information into the response generation process to offer an advanced user experience. One of the most effective methods for obtaining accurate and current user information is by capturing this data from AI interactions. This paper examines conversational knowledge capture using ontology and knowledge-graph approaches. We propose enhancing the large language model's (LLM) ability to capture precise and relevant information by training it with a subset of the KNOW ontology, which models personal knowledge. Our paper details the ontology-guided training process and evaluates the success of knowledge capture using a specially constructed dataset. Additionally, we emphasize the importance of privacy in handling personal information and investigate the implementation of knowledge capture with on-device language models. Our findings highlight the potential of on-device solutions to effectively capture personal knowledge while preserving user privacy. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; 'current; Ai Applications; Generation Process; Knowledge Capture; Language Model; Ontology's; Personal Information; Response Generation; User Information; Users' Experiences; Differential Privacy},
	keywords = {Knowledge graph; 'current; AI applications; Generation process; Knowledge capture; Language model; Ontology's; Personal information; Response generation; User information; Users' experiences; Differential privacy},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Haque2024,
	author = {Haque, Mohd Ariful and Kamal, Marufa and George, Roy and Gupta, Kishor Datta},
	title = {Utilizing structural metrics from knowledge graphs to enhance the robustness quantification of large language models},
	year = {2024},
	journal = {International Journal of Data Science and Analytics},
	pages = {},
	doi = {10.1007/s41060-024-00643-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211953282&doi=10.1007%2Fs41060-024-00643-5&partnerID=40&md5=e9af36440f1cf8bc13db28e79bd28060},
	abstract = {Knowledge graphs (KGs) play a critical role in organizing large stores of unstructured information into structured formats. This structured information is then accessible through SPARQL queries or graph libraries based on their structure. KGs enhance search, power AI systems, and facilitate knowledge discovery across domains. In this research, we explore the capabilities of different large language models (LLMs) like CodeLlama, Mistral, and Vicuna, which are recognized for text generation, in handling textual information tasks for constructing knowledge graphs with structured data. Utilizing these LLMs, we generate class descriptions for all the classes of well-known KGs like DBpedia, YAGO, and Google Knowledge Graph. Using these class descriptions, we have extracted RDF triples and used different preprocessing techniques for better refinement and extraction of the graph triples from the generated result. These extracted triples are used for the graph ontology creation. Highlighting the contribution of LLMs to structured graph formation, our study includes a comparison of the constructed KGs using the three LLMs with the existing Knowledge Graphs. Later, these KGs are evaluated using six structural quality metrics encompassing both class and property-related information crucial for KG formation. Our insights prove valuable for researchers exploring these domains, offering guidance on overcoming challenges and maximizing the potential of large language models in knowledge graph construction, text generation, and text extraction. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Codellama; Knowledge Graph; Llm; Mistral; Structural Metrics; Vicuna; Domain Knowledge; Structured Query Language; Codellama; Knowledge Graphs; Language Model; Large Language Model; Mistral; Robustness Quantifications; Structural Metrics; Structured Information; Text Generations; Vicuna; Knowledge Graph},
	keywords = {Domain Knowledge; Structured Query Language; Codellama; Knowledge graphs; Language model; Large language model; Mistral; Robustness quantifications; Structural metrics; Structured information; Text generations; Vicuna; Knowledge graph},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {SEPLN-P 2024 - Poster Proceedings of the 40th Annual Conference of the Spanish Association for Natural Language Processing 2024, co-located with the 40th International Conference of the Spanish Society for Natural Language Processing, SEPLN 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3846},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211943049&partnerID=40&md5=c32c2e859b8cdf5afd99fbeca5aef2f1},
	abstract = {The proceedings contain 17 papers. The topics discussed include: on the relationship of social gender equality and grammatical gender in pre-trained large language models; the difficulty of misinformation labelling: a case study for radon gas-related searches; findings of a machine translation shared task focused on Covid-19 related documents; COCOTEROS: a Spanish corpus with contextual knowledge for natural language generation; emotions and news structure: an analysis of the language of fake news in Spanish; towards multi-class smishing detection: a novel feature vector approach and the Smishing-4C Dataset; synthetic annotated data for named entity recognition in computed tomography scan reports; Spanish FatPhoCorpus 2023: combating fatphobia in social media in Spanish using transformers; Spanish-language platform for drug-disease evidence search based on scientific articles; and automatic pathology detection in Spanish clinical notes combining language models and medical ontologies. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Schaub-Torre202499,
	author = {Schaub-Torre, León Paul and Quiros, Pelayo and García-Mieres, Helena},
	title = {Automatic Pathology Detection in Spanish Clinical Notes Combining Language Models and Medical Ontologies; Detección Automática de Patologías en Notas Clínicas en Español Combinando Modelos de Lenguaje y Ontologías Médicos},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3846},
	pages = {99 - 120},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211928611&partnerID=40&md5=613c3536921986de8d515c03015d719e},
	abstract = {In this paper we present a hybrid method for the automatic detection of dermatological pathologies in medical reports. We use a large language model combined with medical ontologies to predict, given a first appointment or follow-up medical report, the pathology a person may suffer from. The results show that teaching the model to learn the type, severity and location on the body of a dermatological pathology as well as in which order it has to learn these three features significantly increases its accuracy. The article presents the demonstration of state-of-the-art results for classification of medical texts with a precision of 0.84, micro and macro F1-score of 0.82 and 0.75, and makes both the method and the dataset used available to the community. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical; Hybrid Method; Language Model; Ontology; Contrastive Learning; Oncology; Ontology; Automatic Detection; Biomedical; Clinical Notes; Follow Up; Hybrid Method; Language Model; Learn+; Medical Ontology; Ontology's; Pathologies Detections; Dermatology},
	keywords = {Contrastive Learning; Oncology; Ontology; Automatic Detection; Biomedical; Clinical notes; Follow up; Hybrid method; Language model; Learn+; Medical ontology; Ontology's; Pathologies detections; Dermatology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Barthelmess2024,
	author = {Barthelmess, Paulo and Blais, Curtis L.},
	title = {Exploring Large Language Models for Scenario Generation in Support of C2SIM Autonomous Systems Ontology Extension Development},
	year = {2024},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211637924&partnerID=40&md5=0d9c47332bd152f084c3b161bf4e9d74},
	abstract = {The C2SIM Autonomous Systems sub-group has embarked on an explorative study employing GPT-4, a Large Language Model (LLM), to facilitate scenario development foundational to extending ontologies. This paper outlines the initial findings of GPT-4's application in generating context-specific scenarios, highlighting both its utility and limitations. We detail the methods adopted for directing GPT-4's output, including 0-shot learning and prompt engineering, which serve as techniques for curating scenario content in line with C2SIM requirements. These methods offer a novel approach to not only summarizing existing knowledge in the literature but also in extracting embedded domain knowledge from the model, contributing to a dynamic, user-guided refinement process for scenarios. The insights from this investigation reveal the practical implications of deploying LLMs in scenario generation, thereby informing subsequent research trajectories focused on synthetic data contributions to ontology development. The paper concludes by mapping out potential avenues for future inquiry, tempered by lessons learned from the current application of LLMs in this domain. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Asx; Autonomous Systems; C2sim; Gpt-4; Large Language Models; Llm; Ontology; Scenario Generation; Computer Simulation Languages; Digital Elevation Model; Modeling Languages; Ontology; Asx; Autonomous System; C2sim; Gpt-4; Language Model; Large Language Model; Ontology's; Scenarios Generation; Zero-shot Learning},
	keywords = {Computer simulation languages; Digital elevation model; Modeling languages; Ontology; ASX; Autonomous system; C2SIM; GPT-4; Language model; Large language model; Ontology's; Scenarios generation; Zero-shot learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {Simulation Innovation Workshop 2024, SIW 2024},
	year = {2024},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211635519&partnerID=40&md5=7ac3142c63aaca7cde2e0d72feeac0a0},
	abstract = {The proceedings contain 11 papers. The topics discussed include: aptitude for command evaluations: implementation opportunities for cognitive science; incorporating ethics in defense M & S: new frontiers in standards; weapon simulation as a service for the entire tactical cycle; standards for ChatGPT-enhanced technical writing: objectives, obstacles, options, and opportunities; advancing modelling and simulation in NATO federated mission networking; a helper framework for simplifying HLA interfaces; initial comparison of network loading under HLA and dis standards; C2SIM as a mission planning tool standard; exploring large language models for scenario generation in support of C2SIM autonomous systems ontology extension development; and live-virtual interoperability in large flag exercises. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Congès2024,
	author = {Congès, Aurélie and Fertier, Audrey and Salatgé, Nicolas and Rebiere, Sebastien and Benaben, Frederick A.},
	title = {R-IO SUITE: integration of LLM-based AI into a knowledge management and model-driven based platform dedicated to crisis management},
	year = {2024},
	journal = {Software and Systems Modeling},
	pages = {},
	doi = {10.1007/s10270-024-01237-2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211435891&doi=10.1007%2Fs10270-024-01237-2&partnerID=40&md5=2ac2cfc44fcf45dedda96043273ad393},
	abstract = {This article presents how the R-IO SUITE software platform, a decision support system for crisis management entirely based on model-driven engineering principles, considerably benefits from large language model (LLM)-based artificial intelligence (AI). The different components of the R-IO SUITE platform are used to climb the four abstraction layers: data, information, decision and action through interpretation (from data to information), exploitation (from information to decision) and implementation (from decision to action). These transitions between layers are supported by a knowledge base embedding knowledge instances structured according to a crisis management metamodel. From a functional perspective, this platform is fully operational, however, to be able to cover any type of crisis situation, the knowledge base should be enriched, first, from a “resource perspective” (to embed the various available means to deal with any faced situation), and second, from an “issue perspective” (to understand all risks and damage that can appear on a crisis situation). It is not reasonable to consider creating and maintaining such an exhaustive knowledge base. However, the connection of the R-IO SUITE platform with LLM software such as ChatGPT© makes it possible, by generating appropriate prompts, to update on-the-fly the knowledge base according to the faced context. This article shows how the LLM AI can provide complementary knowledge to formally fulfil the knowledge base to make it relevant to the faced crisis situation. This article presents the R-IO SUITE as a LLM-empowered model-driven platform to become an extended crisis management supporting system. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Business Process Management; Complex Event Processing; Decision Support System; Knowledge Base; Large Language Model; Metamodel; Model-driven Engineering; Ontologies; Enterprise Resource Management; Enterprise Resource Planning; Information Management; Metadata; Modeling Languages; Ontology; Public Administration; Risk Management; Business Process; Business Process Management; Complex Event Processing; Complex Events; Decision Supports; Event Processing; Knowledge Base; Language Model; Large Language Model; Meta Model; Model-driven Engineering; Ontology's; Process Management; Support Systems; Decision Support Systems},
	keywords = {Enterprise resource management; Enterprise resource planning; Information management; Metadata; Modeling languages; Ontology; Public administration; Risk management; Business Process; Business process management; Complex event processing; Complex events; Decision supports; Event Processing; Knowledge base; Language model; Large language model; Meta model; Model-driven Engineering; Ontology's; Process management; Support systems; Decision support systems},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lecu2024443,
	author = {Lecu, Alexandru and Groza, Adrian Petru and Hawizy, Lezan},
	title = {Using LLMs and ontologies to extract causal relationships from medical abstracts},
	year = {2024},
	journal = {Procedia Computer Science},
	volume = {244},
	pages = {443 - 452},
	doi = {10.1016/j.procs.2024.10.219},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211237678&doi=10.1016%2Fj.procs.2024.10.219&partnerID=40&md5=e5e39bde5d194696d986c1acd42d7cb2},
	abstract = {The substantiation of the causal relationships behind its development is very important in identifying possible interventions and early treatment. Knowledge Graphs (KG) play a crucial role in the medical research domain by organizing data into interconnected structures that represent relationships between entities such as disease, treatments, and progressions. This paper shows a complete workflow that demonstrates the extraction of causal relationships from medical abstracts using a fine-tuned GPT-based model and the integration of these relationships into a KG. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Age-related Macular Degeneration; Causal Relation Extraction; Knowledge Graphs; Large Language Models; Medical Informatics; Age-related Macular Degeneration; Causal Relation Extractions; Causal Relationships; Interconnected Structures; Knowledge Graphs; Language Model; Large Language Model; Medical Research; Ontology's; Research Domains; Knowledge Graph},
	keywords = {Medical informatics; Age-related macular degeneration; Causal relation extractions; Causal relationships; Interconnected structures; Knowledge graphs; Language model; Large language model; Medical research; Ontology's; Research domains; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Sahbi20243083,
	author = {Sahbi, Aya and Alec, Céline and Beust, Pierre},
	title = {Automatic Ontology Population from Textual Advertisements: LLM vs. Semantic Approach},
	year = {2024},
	journal = {Procedia Computer Science},
	volume = {246},
	number = {C},
	pages = {3083 - 3092},
	doi = {10.1016/j.procs.2024.09.364},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211219376&doi=10.1016%2Fj.procs.2024.09.364&partnerID=40&md5=a07ef1696c731a6c2952f13728042482},
	abstract = {Automatic ontology population involves identifying, extracting and integrating information from various sources to instantiate the classes and properties of an ontology, thereby building a domain Knowledge Graph (KG). In this paper, we compare two text-based ontology population techniques: KOnPoTe, a semantic approach based on textual and domain knowledge analysis, and a generative AI approach utilizing Claude, a Large Language Model (LLM). We present experiments conducted on two French sales advertisement domains: real estate and boats, and discuss the strengths and limitations of both approaches. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Llm; Ontology Population; Textual Advertisement; Domain Knowledge; Generative Adversarial Networks; Knowledge Graph; Modeling Languages; Ontology; Semantics; Automatic Ontology; Domain Knowledge; Extracting Information; Integrating Information; Language Model; Large Language Model; Ontology Population; Property; Semantic Approach; Textual Advertisement; Marketing},
	keywords = {Domain Knowledge; Generative adversarial networks; Knowledge graph; Modeling languages; Ontology; Semantics; Automatic ontology; Domain knowledge; Extracting information; Integrating information; Language model; Large language model; Ontology Population; Property; Semantic approach; Textual advertisement; Marketing},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{Mao2024,
	author = {Mao, Hanyue and Fan, Yang and Tong, Mingwen},
	title = {Research on aspect-based sentiment analysis of movie reviews based on deep learning},
	year = {2024},
	journal = {Journal of Information Science},
	pages = {},
	doi = {10.1177/01655515241292353},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211180425&doi=10.1177%2F01655515241292353&partnerID=40&md5=50e792765330bf5bf1f0c67a5cb8d212},
	abstract = {Aspect-based sentiment analysis aims to extract the sentiment polarity of different aspects within a text. In recent years, most methods have relied on pre-trained language models such as BERT and Roberta to learn semantic representations from the context. However, in texts with ambiguous sentiment expression, the absence of domain knowledge guidance may lead pre-trained language models to miss critical information, and the attention mechanism might incorrectly focus on text that is irrelevant to the aspect categories. To address these issues, this study integrates the ontology of movie reviews to construct an aspect-based sentiment analysis model based on the ERNIE(OMR-EBA). We annotated a new Chinese data set focused on movie reviews to evaluate the model’s performance. Experimental results show that our model achieves 86% accuracy in aspectual sentiment analysis, which is better than other baseline models. The movie review domain ontology and aspect-based sentiment analysis model proposed in this study can provide valuable reference and guidance for research in the field of online movie reviews. It can also help movie production teams understand genuine user sentiments, aiding in subsequent marketing and production efforts. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Aspect Recognition; Aspect-based Sentiment Analysis; Attention Mechanism; Domain Ontology; Ernie; Contrastive Learning; Deep Learning; Latent Semantic Analysis; Ontology; Analysis Models; Aspect Recognition; Aspect-based Sentiment Analyze; Attention Mechanisms; Domain Ontologies; Ernie; Language Model; Learn+; Movie Reviews; Sentiment Analysis; Semantics},
	keywords = {Contrastive Learning; Deep learning; Latent semantic analysis; Ontology; Analysis models; Aspect recognition; Aspect-based sentiment analyze; Attention mechanisms; Domain ontologies; ERNIE; Language model; Learn+; Movie reviews; Sentiment analysis; Semantics},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Sun2024,
	author = {Sun, Tianda and Carr, Jamie A. and Kazakov, Dimitar},
	title = {A Hybrid Question Answering Model with Ontological Integration for Environmental Information},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3833},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211099694&partnerID=40&md5=6e9cddb1f471d558dd4b193383d41387},
	abstract = {This paper presents a novel approach to constructing a Question Answering model for analysing Nationally Determined Contributions (NDC) reports within the environmental sector. The approach is based on Large Language Models (LLMs) equipped with Retrieval Augmented Generation (RAG) and enhanced by ontology integration. Acknowledging the challenges inherent in directly applying RAG, our approach begins with the development of a specialised ontology framework for NDC reports. This framework supports the construction of a knowledge graph that provides essential, verifiable information for a Question Answering (QA) model. In the next step, the model combines RAG embeddings with ontology-based queries, aiming to enhance the reliability of answers across various NDC reports. We evaluate the performance of our hybrid model through testing with a set of questions and human/AI evaluation across different LLMs. While the results indicate improvements in the efficiency of climate change-related QA models, they also underscore the complexity of achieving significant enhancements in this domain. Our findings contribute to ongoing discussions about the potential and limitations of integrating ontological methods with LLM for environmental information retrieval. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Construction; Relation Extraction; Retrieval Augmentation Generation; Knowledge Graph; Ontology; Environmental Information; Environmental Sector; Graph Construction; Knowledge Graph Construction; Knowledge Graphs; Language Model; Ontology: Integration; Question Answering; Relation Extraction; Retrieval Augmentation Generation; Question Answering},
	keywords = {Knowledge graph; Ontology; Environmental information; Environmental sector; Graph construction; Knowledge graph construction; Knowledge graphs; Language model; Ontology: Integration; Question Answering; Relation extraction; Retrieval augmentation generation; Question answering},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Feng2024117,
	author = {Feng, Xiaohan and Wu, Xixin and Meng, Helen Mei Ling},
	title = {Ontology-grounded Automatic Knowledge Graph Construction by LLM under Wikidata schema},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3841},
	pages = {117 - 135},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210894055&partnerID=40&md5=42721ceb91e4dca1bd023f200ae069d5},
	abstract = {We propose an ontology-grounded approach to Knowledge Graph (KG) construction using Large Language Models (LLMs) on a knowledge base. An ontology is authored by generating Competency Questions (CQ) on knowledge base to discover knowledge scope, extracting relations from CQs, and attempt to replace equivalent relations by their counterpart in Wikidata. To ensure consistency and interpretability in the resulting KG, we ground generation of KG with the authored ontology based on extracted relations. Evaluation on benchmark datasets demonstrates competitive performance in knowledge graph construction task. Our work presents a promising direction for scalable KG construction pipeline with minimal human intervention, that yields high quality and human-interpretable KGs, which are interoperable with Wikidata semantics for potential knowledge base expansion. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Interpretable Ai; Knowledge Graph; Large Language Model; Relation Extraction; Wikidata; Ontology; Semantics; Equivalent Relation; Graph Construction; Interpretability; Interpretable Ai; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Relation Extraction; Wikidata; Knowledge Graph},
	keywords = {Ontology; Semantics; Equivalent relation; Graph construction; Interpretability; Interpretable AI; Knowledge graphs; Language model; Large language model; Ontology's; Relation extraction; Wikidata; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2024,
	title = {HI-AI 2024 - Proceedings of the KDD Workshop on Human-Interpretable AI 2024, co-located with 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3841},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210869022&partnerID=40&md5=6bde3d13a771845bffd6dcb76dfe19a3},
	abstract = {The proceedings contain 12 papers. The topics discussed include: towards fast visual explanations of local path planning with LIME and GAN; using longitudinal data for plausible counterfactual explanations; generative models for counterfactual explanations; logically explainable malware detection; MASALA: model-agnostic surrogate explanations by locality adaptation; GLEAMS: bridging the gap between local and global explanations; from must to may: enabling test-time feature imputation and interventions; evaluating the reliability of Shapley value estimates: an interval-based approach; enhancing interpretability in multivariate time series classification through dimension and feature selection; sparse oblique decision trees: a tool to understand and manipulate neural net features; and ontology-grounded automatic knowledge graph construction by LLM under Wikidata schema. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {SOFLIM2KG-SEMIIM 2024 - Joint Proceedings of the 1st Software Lifecycle Management for Knowledge Graphs Workshop and the 3rd International Workshop on Semantic Industrial Information Modelling, co-located with 23th International Semantic Web Conference, ISWC 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3830},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210832346&partnerID=40&md5=9d6432eea269e88e41d16c348919b682},
	abstract = {The proceedings contain 7 papers. The topics discussed include: RDF-Connect: A declarative framework for streaming and cross-environment data processing pipelines; generating transparent and query-based RDF layers; vision of knowledge graph lifecycle management within hybrid artificial intelligence solutions; knowledge representation and engineering for smart diagnosis of cyber physical systems; SPARQL-based relaxed rules for learning over knowledge graphs; Integrating I4.0 knowledge graphs with large language models beyond SPARQL endpoints; and towards an ontology for procedural knowledge in Industry 5.0. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Monti2024,
	author = {Monti, Marco and Kutz, Oliver and Righetti, Guendalina and Troquard, Nicolas},
	title = {Improving the Accuracy of Black-Box Language Models with Ontologies: A Preliminary Roadmap},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3882},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210825174&partnerID=40&md5=60d777f52ba7a7467274422944bdb59d},
	abstract = {Large Language Models (LLMs) have revolutionised natural language generation. But their statistical and auto-regressive nature makes them unreliable. It has become clear to the research community that in order to produce reliably correct answers, LLMs need to be enriched in some way with ‘world models’ reflecting the semantics of the domains being queried. We here propose a simple workflow to address this problem through a neuro-symbolic interaction protocol with the LLM treated as a blackbox. Answers given by an LLM are checked against accepted knowledge provided by a domain ontology. The approach aims to combine conflict detection with explanation extraction and formal repairs presented to the LLM in the form of specific artificial speech acts. The goal is to build constraining, incremental prompts that improve repeatability and veracity in the LLM’s output. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Llm; Neuro-symbolic Reasoning; Ontologies; Latent Semantic Analysis; Natural Language Processing Systems; Ontology; Speech Enhancement; Auto-regressive; Black Boxes; Language Model; Large Language Model; Natural Language Generation; Neuro-symbolic Reasoning; Ontology's; Research Communities; Roadmap; Symbolic Reasoning; Semantics},
	keywords = {Latent semantic analysis; Natural language processing systems; Ontology; Speech enhancement; Auto-regressive; Black boxes; Language model; Large language model; Natural language generation; Neuro-symbolic reasoning; Ontology's; Research communities; Roadmap; Symbolic reasoning; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Maldonado-Sifuentes20241401,
	author = {Maldonado-Sifuentes, Christian Efraín and Vargas-Santiago, Mariano and Solis-Gamboa, Samuel and Sidorov, G. and Lechuga-Gutierrez, Luis and González-Andrade, Francisco and del Carmen Heras Sánchez, María},
	title = {Towards a Proto Artificial General Intelligence: The Role of Large Language Model Ontologies in its Development},
	year = {2024},
	journal = {Computacion y Sistemas},
	volume = {28},
	number = {3},
	pages = {1401 - 1415},
	doi = {10.13053/CyS-28-3-5200},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210772833&doi=10.13053%2FCyS-28-3-5200&partnerID=40&md5=11b4e51d55ef1d1b31e9000fc6d96fbe},
	abstract = {Proto Artificial General Intelligence (ProtoAGI) aims to create a versatile artificial intelligence system capable of autonomously performing diverse tasks. A foundational element of ProtoAGI is the Large Language Model (LLM) ontology, which plays a crucial role in organizing and retrieving information about different LLMs, enabling the selection of the most appropriate model for specific tasks. This ontology, the first of several designed to support ProtoAGI, addresses key challenges in managing and accessing information regarding LLM capabilities, performance, and task suitability. We present the methodology for constructing this ontology, covering data extraction, enrichment, and model recommendation using a generalized LLM API. The initial version of this ontology involved processing over a million tokens, underscoring the system’s complexity and the scale of information integrated. This ontology is designed for continuous updates, ensuring that ProtoAGI remains current with the latest advancements in LLMs. The ongoing development of this ontology marks a significant step in ProtoAGI’s evolution, following an initial proof-of-concept demonstrated during the 2024 eclipse, where the feasibility of integrating such a comprehensive LLM ontology into a general-purpose AI system was shown. By making this ontology accessible to the broader AI community, we aim to accelerate further advancements in AGI research and applications. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial General Intelligence; Hybrid Intelligent Systems; Large Language Models; Multi-agent Systems; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wu202439,
	author = {Wu, Shanglin and Yao, Xiaodong and Liu, Shu and Liang, Haitao and Liu, Zhenyuan},
	title = {Domain Knowledge Graph Construction Methods of Construction Schedule in Steel Structure Projects},
	year = {2024},
	pages = {39 - 44},
	doi = {10.1109/IHMSC62065.2024.00017},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210517148&doi=10.1109%2FIHMSC62065.2024.00017&partnerID=40&md5=d7167f0cb424e3293e241c61f24cf467},
	abstract = {In the domain of engineering construction, steel structure engineering construction has accumulated a large amount of data, and the development of knowledge graph construction technology to structure this data can provide effective support for high-quality construction organization. This paper analyzes the knowledge sources of construction schedule for steel structure projects, constructs a domain ontology of construction schedule for assembled steel structure projects by using a seven-step method and points out the current problems of knowledge extraction for construction organization design for assembled steel structure projects. The semi-structured construction schedule data is subject to event extraction, and the unstructured knowledge of construction schedule is discussed from the perspective of few-shot methods and large language modeling for the knowledge extraction of construction schedule for steel structure projects. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Construction Schedule; Domain Knowledge Graphs; Event Extraction; Ontology Construction; Ontology; Project Management; Steel Structures; Structural Design; Construction Organizations; Construction Schedules; Domain Knowledge; Domain Knowledge Graph; Engineering Constructions; Events Extractions; Graph-construction Method; Knowledge Extraction; Knowledge Graphs; Ontology Construction; Knowledge Graph},
	keywords = {Ontology; Project management; Steel structures; Structural design; Construction organizations; Construction schedules; Domain knowledge; Domain knowledge graph; Engineering constructions; Events extractions; Graph-construction method; Knowledge extraction; Knowledge graphs; Ontology construction; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kondo2024,
	author = {Kondo, Ryoma and Watanabe, Tomohiro and Yoshida, Takahiro and Yamasawa, Kazuyuki and Hisano, Ryohei},
	title = {Collaborative System Synergizing Human Expertise and Large-scale Language Models for Legal Knowledge Graph Construction},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3828},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210257570&partnerID=40&md5=d29fbf341f952385411b05d9839047f0},
	abstract = {Court judgments are rich with information that details the combination of facts and legal norms to arrive at judicial decisions. This intricate process can be conceptualized as a hierarchical tree structure, wherein facts are aggregated in a bottom-up manner to highlight pivotal facts. These critical facts are subsequently linked to legal norms, facilitating the derivation of specific decisions. Despite the intrinsic value of this information, there is no legal knowledge graph(KG) that can represent this, and even if there were, the task of text-mining or text-annotation of such legal structures from court judgments presents considerable challenges. We show a legal ontology that links facts and norms and a new collaborative framework that harnesses the capabilities of both LLMs and legal experts for extracting these intricate relationships. Our approach is underpinned by a carefully designed LLM, tailored through prompt engineering, that can capture such structures. The results from this are then refined by legal experts through a graphic user interface, also providing us with an overall score of the annotation. We believe that a synergistic integration of machine efficiency and human expertise will lead to the improvement of future legal KGs and legal search engines. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Annotation Tools; Artificial Intelligence And Law; Court Judgments; Large-scale Language Models; Legal Knowledge Graph; Semantic Web; Semantics; Trees (mathematics); Annotation Tool; Artificial Intelligence And Laws; Court Judgement; Knowledge Graphs; Language Model; Large-scale Language Model; Large-scales; Legal Knowledge; Legal Knowledge Graph; Semantic-web; Knowledge Graph},
	keywords = {Semantics; Trees (mathematics); Annotation tool; Artificial intelligence and laws; Court judgement; Knowledge graphs; Language model; Large-scale language model; Large-scales; Legal knowledge; Legal knowledge graph; Semantic-Web; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {SICSA-REALLM 2024 - Proceedings of the SICSA Workshop on Reasoning, Evaluation and Application of Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3822},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210021824&partnerID=40&md5=a37311d16502cece39ea494853bb34f1},
	abstract = {The proceedings contain 8 papers. The topics discussed include: towards improving open-box hallucination detection in large language models (LLMs); extended results for enhancing abstract screening classification in evidence-based medicine: incorporating domain knowledge into pre-trained models; towards LLM-driven automated verification of best practices in digital public infrastructures; formal dialogue and large language models; evaluating large language models on qualitative reasoning tasks: a case study using OpenAI GPT models; dual-task dialogue understanding; SCaLe-QA: Sri Lankan case law embeddings for legal QA; and integrating KGs and ontologies with RAG for personalized summarization in regulatory compliance. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Reyes2024313,
	author = {Reyes, G. Mitchell},
	title = {AI CHATBOTS, TRANSLATIVE RHETORIC, AND THE FUTURE OF PUBLIC DISCOURSE},
	year = {2024},
	pages = {313 - 325},
	doi = {10.4324/9781003430858-29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209869472&doi=10.4324%2F9781003430858-29&partnerID=40&md5=b860bb0a9ca18b2141674d7d0ce629f2},
	abstract = {The algorithmic culture of the Anthropocene has arrived and with it a whole new set of rhetorical situations and power dynamics. How should we think about concepts like rhetorical agency amidst a proliferation of “talking machines” and “algorithmic influencers”? How should we account for symbolic action in the context of large language models (LLMs) producing complex discourses that, on occasion, surprise us humans right out of our seats? And what of “the human,” that oft taken-for-granted concept within which lies a whole nest of ontological justifications that have, in many ways, ushered us into the Anthropocene? In this chapter I address these questions through close critical engagement with the discourses of AI chatbots, of which ChatGPT is only the most famous. This analysis demonstrates the need for a new rhetorical paradigm that can account for the rise of agential machines, emergent technocracies, and the translative rhetorical force of technoscience, which instead of foreclosing spaces for rhetoric to thrive are in fact opening up whole new realms of rhetorical action, the likes of which we’ve never seen. I use “translative rhetoric” as a conceptual frame under which to organize the critical commitments of this new rhetorical approach. © 2024 Elsevier B.V., All rights reserved.},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Dutta2024,
	author = {Dutta, Subhabrata and Singh, Joykirat and Chakrabarti, Soumen and Chakraborty, Tanmoy},
	title = {How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning},
	year = {2024},
	journal = {Transactions on Machine Learning Research},
	volume = {2024},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209773373&partnerID=40&md5=ab371b9b0df421335af3124c3e66310a},
	abstract = {Despite superior reasoning prowess demonstrated by Large Language Models (LLMs) with Chain-of-Thought (CoT) prompting, a lack of understanding prevails around the internal mechanisms of the models that facilitate CoT generation. This work investigates the neural sub-structures within LLMs that manifest CoT reasoning from a mechanistic point of view. From an analysis of Llama-2 7B applied to multistep reasoning over fictional ontologies, we demonstrate that LLMs deploy multiple parallel pathways of answer generation for step-by-step reasoning. These parallel pathways provide sequential answers from the input question context as well as the generated CoT. We observe a functional rift in the middle layers of the LLM. Token representations in the initial half remain strongly biased towards the pretraining prior, with the in-context prior taking over in the later half. This internal phase shift mani-fests in different functional components: attention heads that write the answer token appear in the later half, attention heads that move information along ontological relationships appear in the initial half, and so on. To the best of our knowledge, this is the first attempt towards mechanistic investigation of CoT reasoning in LLMs. The source code and data are made available at https://github.com/joykirat18/How-To-Think-Step-by-Step. © 2025 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Santini2024,
	author = {Santini, Cristian},
	title = {Combining language models for knowledge extraction from Italian TEI editions},
	year = {2024},
	journal = {Frontiers in Computer Science},
	volume = {6},
	pages = {},
	doi = {10.3389/fcomp.2024.1472512},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209410230&doi=10.3389%2Ffcomp.2024.1472512&partnerID=40&md5=ce9cf19205203dea63559d8a761a4b9d},
	abstract = {This study investigates the integration of language models for knowledge extraction (KE) from Italian TEI/XML encoded texts, focusing on Giacomo Leopardi's works. The objective is to create structured, machine-readable knowledge graphs (KGs) from unstructured texts for better exploration and linkage to external resources. The research introduces a methodology that combines large language models (LLMs) with traditional relation extraction (RE) algorithms to overcome the limitations of current models with Italian literary documents. The process adopts a multilingual LLM, that is, ChatGPT, to extract natural language triples from the text. These are then converted into RDF/XML format using the REBEL model, which maps natural language relations to Wikidata properties. A similarity-based filtering mechanism using SBERT is applied to keep semantic consistency. The final RDF graph integrates these filtered triples with document metadata, utilizing established ontologies and controlled vocabularies. The research uses a dataset of 41 TEI/XML files from a semi-diplomatic edition of Leopardi's letters as case study. The proposed KE pipeline significantly outperformed the baseline model, that is, mREBEL, with remarkable improvements in semantic accuracy and consistency. An ablation study demonstrated that combining LLMs with traditional RE models enhances the quality of KGs extracted from complex texts. The resulting KG had fewer, but semantically richer, relations, predominantly related to Leopardi's literary activities and health, highlighting the extracted knowledge's relevance to understanding his life and work. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Entity Linking; Giacomo Leopardi; Knowledge Extraction; Large Language Models (llms); Relation Extraction; Semantic Web; Tei/xml; Wikidata},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@CONFERENCE{2024,
	title = {RuleML+RR-Companion 2024 - Companion Proceedings of the 8th International Joint Conference on Rules and Reasoning, co-located with 20th Reasoning Web Summer School, RW 2024 and 16th DecisionCAMP 2024 as part of Declarative AI 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3816},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209395083&partnerID=40&md5=c7b259ed974b03d907c4e0fa7f8153e1},
	abstract = {The proceedings contain 30 papers. The topics discussed include: RuleMiner: an interactive web tool for rule-based data analysis; rule-aware datalog fact explanation using Group-SAT Solver; PolyCoP: a connection prover for (possibly) any logical language; query optimization of backward-chaining reasoning with learned heuristics; softening ontological reasoning with large language models; distributed component interoperation and execution for norm-based real-time compliance; explanatory dialogues with active learning for rule-based expertise; integrating symbolic knowledge and machine learning in healthcare; fair shifts by the rule: a rule-based compliance methodology for medical rosters; and towards optimizing ontology-based data federation: performance insights from experimental studies. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Baldazzi2024,
	author = {Baldazzi, Teodoro and Benedetto, Davide and Bellomarini, Luigi and Sallinger, Emanuel and Vlad, Adriano},
	title = {Softening Ontological Reasoning with Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3816},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209387015&partnerID=40&md5=60a46323ab00b3fd9d2fcdd038e7e6ae},
	abstract = {Logic-based Knowledge Graphs (KGs) and Knowledge Representation and Reasoning (KRR) have emerged as fundamental methodologies in many data-intensive areas, fostering trust and accountability for effective decision-making. However, the knowledge captured by such approaches is often restricted by the rigidity of their structured rule-based formalisms. More recently, the rising adoption of Large Language Models (LLMs) has introduced a new layer of semantic understanding and flexibility in human-data interaction. Yet, these models are inherently limited in reasoning capabilities and lack systematic and explainable outcomes due to their opaque nature. To address today’s challenge of combining the strengths of both technologies, we propose a novel neurosymbolic solution that leverages the power of LLMs to “soften” rule activations, enhancing adaptability in ontological reasoning while preserving robustness and transparency of KRR systems. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graphs; Language Models; Ontological Reasoning; Semantics; Data Intensive; Decisions Makings; Graph Representation; Human Data; Knowledge Graphs; Knowledge Representation And Reasoning; Language Model; Ontological Reasoning; Rule Based; Semantics Understanding; Knowledge Graph},
	keywords = {Semantics; Data intensive; Decisions makings; Graph representation; Human data; Knowledge graphs; Knowledge representation and reasoning; Language model; Ontological reasoning; Rule based; Semantics understanding; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kaverinsky2024253,
	author = {Kaverinsky, Vladislav and Palagin, Oleksandr V. and Litvin, Anna A.},
	title = {Development and Testing of a Large Language Models Prompt for Natural Language Phrases Synthesis from Ontological Semantic Structures},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3806},
	pages = {253 - 264},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209356688&partnerID=40&md5=9b710c00d303747e430b3cd53683b2e6},
	abstract = {The article introduces an innovative approach that leverages a specially designed structured prompt for Chat GPT, a large language model. This approach was tested through a series of experiments aimed at generating natural language phrases from their underlying ontological representations. These representations were automatically derived from sentences in scientific and technical texts using advanced software tools. They encapsulate the entities identified in the text and the semantic relationships between them, which can be expressed in the sentences of the analyzed text. In more detail, the system identifies relationships between concepts and links them to entities within a sentence. These entities can be either simple sentences or parts of complex ones. The structured prompt provided to the language model includes detailed explanations of these semantic relationships and a set of concept pairs connected by these relationships, serving as the building blocks for sentence creation. The generated sentences were then compared to the original ones using the cosine similarity measure across various vectorization methods. The similarity scores, calculated using the xx_ent_wiki_sm model, ranged from 0.8193 to 0.9722. Despite these high similarity scores, some stylistic differences were noted in the generated sentences. This research holds significant practical value for the development of dialogue systems that integrate ontological methods with advanced language models, paving the way for more accurate and contextually aware information systems. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cosine Similarity; Large Language Model; Natural Language Text Analysis; Natural Language Text Synthesis; Ontology; Text Vectorization; Natural Language Processing Systems; Ontology; Semantics; Software Testing; Cosine Similarity; Language Model; Large Language Model; Natural Language Text Analyze; Natural Language Text Synthesis; Natural Languages Texts; Ontology's; Text Analysis; Text Vectorization; Vectorization; Latent Semantic Analysis},
	keywords = {Natural language processing systems; Ontology; Semantics; Software testing; Cosine similarity; Language model; Large language model; Natural language text analyze; Natural language text synthesis; Natural languages texts; Ontology's; Text analysis; Text vectorization; Vectorization; Latent semantic analysis},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {UkrPROG 2024 - Proceedings of the 14th International Scientific and Practical Programming Conference},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3806},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209354177&partnerID=40&md5=e930525f365399723c08846284caf15c},
	abstract = {The proceedings contain 39 papers. The topics discussed include: development and testing of a large language models prompt for natural language phrases synthesis from ontological semantic structures; method of developing an ontological system with automatic formation of a knowledge base and user queries; predicting 24-hour nationwide electrical energy consumption based on regression techniques; expressive capabilities of semantic MediaWiki: advantages and limitations; integrating hybrid cloud solutions in telerehabilitation; methods and tools for building open systems of scientific research support; neural networks as an intellectualization tool of OLAP technology; an approach to model network dynamics of a decentralized supply chain network using optimal predictive control; a video-based approach to learning debugging techniques; and semantic support of personal learning trajectory development. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Xu2024162638,
	author = {Xu, Jun and Zhang, Hao and Zhang, Haijing and Lu, Jiawei and Xiao, Gang},
	title = {ChatTf: A Knowledge Graph-Enhanced Intelligent Q&A System for Mitigating Factuality Hallucinations in Traditional Folklore},
	year = {2024},
	journal = {IEEE Access},
	volume = {12},
	pages = {162638 - 162650},
	doi = {10.1109/ACCESS.2024.3485877},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209217304&doi=10.1109%2FACCESS.2024.3485877&partnerID=40&md5=1750490cfa14cde7c31225b029ca7fe3},
	abstract = {Large language models are rapidly advancing the field of artificial intelligence, with current research focusing primarily on traditional natural language understanding tasks, such as question answering and information extraction. However, in knowledge-intensive domains, such as intangible cultural heritage, hallucination problems due to insufficient domain knowledge persist. To address this, we present ChatTf, a knowledge graph-enhanced intelligent Q&A system, exemplified by Chinese traditional folklore, aimed at reducing factuality hallucinations in this domain. Specifically, we constructed the Traditional Folklore Ontology (TFOnto) and proposed the Zero-shot Traditional Folklore Triplet Extraction (ZFTE) framework. Driven by TFOnto, ZFTE builds a Traditional Folklore Knowledge Graph (TFKG). We then proposed a dual-stage Retrieval-Augmented Generation framework (TFKG-RAG) based on TFKG to provide traditional folklore knowledge to large language models, mitigating factuality hallucinations in folklore Q&A tasks. In the experimental phase, ChatTf achieved an accuracy of 96.7% on a self-built TFCQD test set, outperforming several state-of-the-art baseline methods. This demonstrates the accuracy and reliability of folklore domain question answering. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Large Language Model; Question Answering; Retrieval-augmented Generation; Traditional Folklore; Domain Knowledge; Knowledge Graph; 'current; Intangible Cultural Heritages; Knowledge Graphs; Language Model; Large Language Model; Natural Language Understanding; Ontology's; Question Answering; Retrieval-augmented Generation; Traditional Folklore; Question Answering},
	keywords = {Domain Knowledge; Knowledge graph; 'current; Intangible cultural heritages; Knowledge graphs; Language model; Large language model; Natural language understanding; Ontology's; Question Answering; Retrieval-augmented generation; Traditional folklore; Question answering},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access}
}

@ARTICLE{Cabas-Mora2024,
	author = {Cabas-Mora, Gabriel and Daza, Anamaría and Soto-García, Nicole and Garrido, Valentina and Alvarez, Diego and Navarrete, Marcelo A. and Sarmiento-Varón, Lindybeth and SepúlvedaYañez, Julieta H. and Davari, Mehdi D. and Cadet, Frédéric R.},
	title = {Peptipedia v2.0: a peptide sequence database and user-friendly web platform. A major update},
	year = {2024},
	journal = {Database : the journal of biological databases and curation},
	volume = {2024},
	pages = {},
	doi = {10.1093/database/baae113},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209144325&doi=10.1093%2Fdatabase%2Fbaae113&partnerID=40&md5=8ec9e1306e049b747af0e189ec078e92},
	abstract = {In recent years, peptides have gained significant relevance due to their therapeutic properties. The surge in peptide production and synthesis has generated vast amounts of data, enabling the creation of comprehensive databases and information repositories. Advances in sequencing techniques and artificial intelligence have further accelerated the design of tailor-made peptides. However, leveraging these techniques requires versatile and continuously updated storage systems, along with tools that facilitate peptide research and the implementation of machine learning for predictive systems. This work introduces Peptipedia v2.0, one of the most comprehensive public repositories of peptides, supporting biotechnological research by simplifying peptide study and annotation. Peptipedia v2.0 has expanded its collection by over 45% with peptide sequences that have reported biological activities. The functional biological activity tree has been revised and enhanced, incorporating new categories such as cosmetic and dermatological activities, molecular binding, and antiageing properties. Utilizing protein language models and machine learning, more than 90 binary classification models have been trained, validated, and incorporated into Peptipedia v2.0. These models exhibit average sensitivities and specificities of 0.877±0.0530 and 0.873±0.054, respectively, facilitating the annotation of more than 3.6 million peptide sequences with unknown biological activities, also registered in Peptipedia v2.0. Additionally, Peptipedia v2.0 introduces description tools based on structural and ontological properties and user-friendly machine learning tools to facilitate the application of machine learning strategies to study peptide sequences. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Peptides; Peptide; Amino Acid Sequence; Chemistry; Computer Interface; Internet; Machine Learning; Protein Database; Amino Acid Sequence; Databases, Protein; Machine Learning; Peptides; User-computer Interface},
	keywords = {peptide; amino acid sequence; chemistry; computer interface; Internet; machine learning; protein database; Amino Acid Sequence; Databases, Protein; Machine Learning; Peptides; User-Computer Interface},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Tavakkoli202449,
	author = {Tavakkoli, Vahid and Mohsenzadegan, Kabeh and Kyandoghere, Kyamakya},
	title = {Leveraging Context-Aware Emotion and Fatigue Recognition Through Large Language Models for Enhanced Advanced Driver Assistance Systems (ADAS)},
	year = {2024},
	journal = {Studies in Computational Intelligence},
	volume = {1175},
	pages = {49 - 85},
	doi = {10.1007/978-3-031-71821-2_2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208981614&doi=10.1007%2F978-3-031-71821-2_2&partnerID=40&md5=d1afb61c5d2e2df2321b87e6769c5282},
	abstract = {The automotive sector is swiftly advancing, focusing on driving experiences that prioritize safety and integrate with human emotions and well-being. This chapter explores the transformative potential of combining large language models (LLMs) with context-aware emotion and fatigue recognition techniques in Advanced Driver Assistance Systems (ADAS). The primary objective is to enhance driving experiences and overall well-being through real-time emotional and fatigue assessments. Grounded in Active and Assisted Living (AAL) principles, this chapter emphasizes the practical implementation of bio-signal-based emotion and fatigue estimation techniques within the ADAS framework. Utilizing formal knowledge representation techniques, such as ontologies, demonstrates how contextual modeling can facilitate optimal support services in dynamic driving contexts. Central to this approach is the integration of LLMs with emotion and fatigue recognition methodologies. The chapter details using non-intrusive bio-signal sensors to analyze facial expressions through video, EEG measurements, and voice analysis. This synergy between language comprehension and bio-signal insights enables real-time emotional assessments and fatigue estimations, empowering safer and more responsive driving experiences. LLMs act as the cognitive bridge, enhancing human driver assistance with context-aware emotion and fatigue recognition. The chapter reveals the potential of language models to decode emotional cues and detect fatigue levels, which are crucial for shaping proactive and adaptive driver assistance strategies. Integrating LLMs within ADAS showcases their ability to anticipate driver needs, provide timely alerts, and improve decision-making processes. Importantly, this chapter offers a roadmap for integrating LLMs with context-aware emotion and fatigue recognition into ADAS. Leveraging the capabilities of LLMs presents a scalable method for embedding bio-signal-based emotion and fatigue estimation techniques into ADAS, highlighting the AAL principles while demonstrating the transformative potential of LLMs. This work envisions an advanced iteration of ADAS, fostering a safer, more intuitive, and supportive driving experience. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Advanced Driver Assistance Systems (adas); Bio-signal Processing; Context-aware Systems; Emotion And Fatigue Detection; Human-centered Design; Large Language Models (llms); Real-time Assessments},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Fan2024,
	author = {Fan, Yi and Sun, Yu and Mi, Baigang},
	title = {FLIGHT CONTROL SYSTEM KNOWLEDGE GRAPH CONSTRUCTION BASED ON AERONAUTICAL DOMAIN KNOWLEDGE AUGMENTED LARGE LANGUAGE MODEL},
	year = {2024},
	journal = {ICAS Proceedings},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208787293&partnerID=40&md5=b8d0e22d4fc368b6768afd0a6d270dbb},
	abstract = {In the complex competitive aerospace environment, the continuous intelligent assurance of the Flight Control System (FCS) is essential for the successful execution of aerospace missions. Monitoring and maintaining the FCS under dynamic and complex conditions rely heavily on empirical expert knowledge. Domain Knowledge Graphs (KG) serve as an efficient mechanism for FCS status management within expert systems, enabling effective value extraction. This paper outlines the construction of an FCS KG through large language model (LLM) fine-tuning, enriched with aeronautical domain knowledge derived from multi-source heterogeneous FCS texts. The process begins with the creation of an FCS ontology, integrating aeronautical domain knowledge and defining entity-relationship types. Subsequently, the Llama3-8B LLM was fine-tuned using methods such as LoRA, QLoRA, and AdaLoRA for parameter-efficient tuning, and prior aeronautical knowledge was incorporated via a chain of thought (COT) prompt template to facilitate intelligent discovery from a common FCS dataset. Experimental results indicate that the aeronautical domain knowledgeaugmented LLM method achieved an F1 score of 97.86%, representing a 6.48% improvement over traditional methods. Finally, the FCS KG was visualized using the graph database Neo4j, demonstrating the effectiveness and superiority of this approach in constructing FCS KG. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Flight Control System (fcs), Knowledge Graph (kg); Knowledge Discovery (kd); Large Language Model (llm); Parameter-efficient Fine-tuning (peft); Domain Knowledge; Fine Tuning; Flight Control System , Knowledge Graph; Flight-control Systems; Knowledge Discovery; Knowledge Graphs; Language Model; Large Language Model; Parameter-efficient Fine-tuning; System Knowledge; Knowledge Graph},
	keywords = {Domain knowledge; Fine tuning; Flight control system , knowledge graph; Flight-control systems; Knowledge discovery; Knowledge graphs; Language model; Large language model; Parameter-efficient fine-tuning; System knowledge; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Dai2024292,
	author = {Dai, Yun and Lin, Ziyan and Liu, Ang},
	title = {Facilitating Students' Adaptive Help-seeking and Peer Interactions through an Analytics-enhanced Forum in Engineering Design Education},
	year = {2024},
	journal = {Procedia CIRP},
	volume = {128},
	pages = {292 - 297},
	doi = {10.1016/j.procir.2024.06.024},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208785034&doi=10.1016%2Fj.procir.2024.06.024&partnerID=40&md5=a742f8bc5195809d954203d402be71a8},
	abstract = {Design often takes place in collective and collaborative settings, and interactions and mutual support among peers have been a critical component of design education. However, in most of the existing design courses, students often work in small groups and peer interactions are limited to group members, which limits the range and depth of knowledge exchange. To complement the group-based activities, this study designs and assesses an analytics-enhanced discussion forum for whole-class interactions. The forum adopts ontology-based recommender systems and anomaly detection techniques to tailor the threads and contents for individual students in a personalized way. This analytics-enhanced forum was implemented in a large-size undergraduate design course (n = 313), and data about student responses to this forum was compared with data from the previous year's course that adopted a conventional forum (n = 280). From the statistical analysis, students learning with the analytics-enhanced forum demonstrated significantly higher degrees of design practices (specifically, empathize, define, ideate, and test), collaborative learning, and course satisfaction. Qualitative analysis of students' focus-group interviews shows their perceived benefits and concerns of the analytics-enhanced forum. The study also suggests integrating generative artificial intelligence and large language models to support students' design thinking and collaborative design. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Design Thinking; Engineering Education; Help-seeking; Learning Analytics; Peer Support; Adversarial Machine Learning; Contrastive Learning; Collaborative Interaction; Collaborative Settings; Critical Component; Design Course; Design Thinking; Engineering Design Education; Help Seeking; Learning Analytic; Peer Interactions; Peer Support; Collaborative Learning},
	keywords = {Adversarial machine learning; Contrastive Learning; Collaborative interaction; Collaborative settings; Critical component; Design course; Design thinking; Engineering design education; Help seeking; Learning analytic; Peer interactions; Peer support; Collaborative learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{Lee20241191,
	author = {Lee, Woo-sung and Kim, Tae-yeon and Choi, Woensug},
	title = {Autonomous Navigation Command Generation System for Underwater Robots Using LLM},
	year = {2024},
	journal = {Journal of Institute of Control, Robotics and Systems},
	volume = {30},
	number = {10},
	pages = {1191 - 1196},
	doi = {10.5302/J.ICROS.2024.24.0142},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208623885&doi=10.5302%2FJ.ICROS.2024.24.0142&partnerID=40&md5=cf8aa075c3a8c4e5a7bda6125892c80a},
	abstract = {Recent advances in artificial intelligence, particularly in natural language processing with large language models (LLMs), enable robots to perform complex tasks. Integration with ROS to simulate control room conversation enhances robot autonomy, allowing them to automatically navigate and determine optimal paths. This study demonstrates the feasibility of implementing a system where robots can generate their own navigation commands by integrating ROS and LLMs. To achieve this, a simulation environment was constructed, data processing and command generation processes were designed, and three methodologies were experimented with by applying ontology and prompt engineering. The experimental results showed that using a single GPT model with information filtering provided high efficiency and success rates in command generation, and higher versions of the GPT model delivered improved performance. These findings present a preliminary methodology that improves robot autonomy and efficiency in autonomous controls and establishes a foundation for further enhancing the performance of robot control systems. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Autonomous Surface Vehicle; Autonomous Underwater Vehicle; Large-scale Language Model; Marine Robots; Prompt Engineering; Ros-gazebo Framework; Modeling Languages; Robots; Unmanned Surface Vehicles; Autonomous Surface Vehicles; Autonomous Underwater Vehicles]; Command Generation; Language Model; Large-scale Language Model; Large-scales; Marine Robots; Navigation Commands; Prompt Engineering; Ros-gazebo Framework; Natural Language Processing Systems},
	keywords = {Modeling languages; Robots; Unmanned surface vehicles; Autonomous surface vehicles; Autonomous underwater vehicles]; Command generation; Language model; Large-scale language model; Large-scales; Marine robots; Navigation commands; Prompt engineering; ROS-gazebo framework; Natural language processing systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Baddour2024,
	author = {Baddour, Moussa and Paquelet, Stéphane and Rollier, Paul and de Tayrac, Marie and Dameron, Olivier and Labbe, Thomas},
	title = {Phenotypes Extraction from Text: Analysis and Perspective in the LLM Era},
	year = {2024},
	journal = {International IEEE Conference proceedings, IS},
	number = {2024},
	pages = {},
	doi = {10.1109/IS61756.2024.10705235},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208419027&doi=10.1109%2FIS61756.2024.10705235&partnerID=40&md5=c15b07740ba389e2596b86f8908a8285},
	abstract = {Collecting the relevant list of patient phenotypes, known as deep phenotyping, can significantly improve the final diagnosis. As textual clinical reports are the richest source of phenotypes information, their automatic extraction is a critical task. The main challenges of this Information Extraction (IE) task are to identify precisely the text spans related to a phenotype and to link them unequivocally to referenced entities from a source such as the Human Phenotype Ontology (HPO). Recently, Language Models (LMs) have been the most suc-cessful approach for extracting phenotypes from clinical reports. Solutions such as PhenoBERT, relying on BERT or GPT, have shown promising results when applied to datasets built on the hypothesis that most phenotypes are explicitly mentioned in the text. However, this assumption is not always true in medical genetics. Hence, although the LMs carry powerful semantic abilities, their contributions are not clear compared to syntactic string-matching steps that are used within the current pipelines. The goal of this study is to improve phenotype extraction from clinical notes related to genetic diseases. Our contributions are threefold: First, we provide a clear definition of the phenotype extraction task from free text, along with a high-level overview of the involved functions. Second, we conduct an in-depth analysis of PhenoBERT, one of the best existing solutions, to evaluate the proportion of phenotypes predicted with simple string-matching. Third, we demonstrate how utilizing and incorporating large language models (LLMs) for span detection step can improve performance especially with implicit phenotypes. In addition, this experiment revealed that the annotations of existing dataset are not exhaustive, and that LLM can identify relevant spans missed by human labelers. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Embed Dings; Entity Linking; Genetic; Llm; Phenobert; Phenotype; Embed Ding; Entity Linking; Genetic; Language Model; Large Language Model; Phenobert; Phenotype; Phenotyping; String Matching; Text Analysis; Semantics},
	keywords = {Embed ding; Entity linking; Genetic; Language model; Large language model; Phenobert; Phenotype; Phenotyping; String matching; Text analysis; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Momčilović2024121,
	author = {Momčilović, Tomas Bueno and Buesser, Beat A. and Zizzo, Giulio and Purcell, Mark E. and Balta, Dian},
	title = {Towards Assurance of LLM Adversarial Robustness using Ontology-Driven Argumentation},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3793},
	pages = {121 - 128},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208283226&partnerID=40&md5=2be6d1e85223a03b9147402cf6dfd07f},
	abstract = {Despite the impressive adaptability of large language models (LLMs), challenges remain in ensuring their security, transparency, and interpretability. Given their susceptibility to adversarial attacks, LLMs need to be defended with an evolving combination of adversarial training and guardrails. However, managing the implicit and heterogeneous knowledge for continuously assuring robustness is difficult. We introduce a novel approach for assurance of the adversarial robustness of LLMs based on formal argumentation. Using ontologies for formalization, we structure state-of-the-art attacks and defenses, facilitating the creation of a human-readable assurance case, and a machine-readable representation. We demonstrate its application with examples in English language and code translation tasks, and provide implications for theory and practice, by targeting engineers, data scientists, users, and auditors. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Adversarial Robustness; Argumentation; Assurance; Llm; Ontologies; Generative Adversarial Networks; Guard Rails; Ontology; Translation (languages); Adversarial Robustness; Argumentation; Assurance; Heterogeneous Knowledge; Implicit Knowledge; Interpretability; Language Model; Large Language Model; Model-based Opc; Ontology's; Adversarial Machine Learning},
	keywords = {Generative adversarial networks; Guard rails; Ontology; Translation (languages); Adversarial robustness; Argumentation; Assurance; Heterogeneous Knowledge; Implicit knowledge; Interpretability; Language model; Large language model; Model-based OPC; Ontology's; Adversarial machine learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Pardo2024178,
	author = {Pardo, Joel},
	title = {Automatic Medical Knowledge Graph Construction},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3797},
	pages = {178 - 186},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207927035&partnerID=40&md5=31607c7152cc7f36715e1f533e37df82},
	abstract = {Knowledge Graphs are crucial for structuring and integrating large amounts of data, improving decision-making and data interoperability, especially in the healthcare domain. This PhD thesis aims to implement a unified end-to-end framework for building a cross-lingual KG for English and Spanish in the healthcare sector using NLP techniques. Addressing the reliance on traditional methods in KG construction and the limited non-English language resources, this work seeks to refine the information extraction process within unstructured medical texts and facilitate the (re)use of existent ontologies (schema to represent the real-world). © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Electronic Health Records; Information Extraction; Knowledge Graph Construction; Large Language Models; Knowledge Graph; Medical Informatics; Electronic Health; Graph Construction; Health Records; Information Extraction; Knowledge Graph Construction; Knowledge Graphs; Language Model; Large Amounts Of Data; Large Language Model; Medical Knowledge; Electronic Health Record},
	keywords = {Knowledge graph; Medical informatics; Electronic health; Graph construction; Health records; Information extraction; Knowledge graph construction; Knowledge graphs; Language model; Large amounts of data; Large language model; Medical knowledge; Electronic health record},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Reif2024,
	author = {Reif, Jonathan T. and Jeleniewski, Tom and Gill, Milapji Singh and Gehlhoff, Felix and Fay, Alexander},
	title = {Chatbot-Based Ontology Interaction Using Large Language Models and Domain-Specific Standards},
	year = {2024},
	journal = {IEEE International Conference on Emerging Technologies and Factory Automation, ETFA},
	pages = {},
	doi = {10.1109/ETFA61755.2024.10711065},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207845394&doi=10.1109%2FETFA61755.2024.10711065&partnerID=40&md5=6e55dc21dbf2c13a30448996fcb6cb63},
	abstract = {The following contribution introduces a concept that employs Large Language Models (LLMs) and a chatbot interface to enhance SPARQL query generation for ontologies, thereby facilitating intuitive access to formalized knowledge. Utilizing natural language inputs, the system converts user inquiries into accurate SPARQL queries that strictly query the factual content of the ontology, effectively preventing misinformation or fabrication by the LLM. To enhance the quality and precision of outcomes, additional textual information from established domain-specific standards is integrated into the ontology for precise descriptions of its concepts and relationships. An experimental study assesses the accuracy of generated SPARQL queries, revealing significant benefits of using LLMs for querying ontologies and highlighting areas for future research. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cyber-physical Systems; Industry 4.0; Large Language Models; Ontologies; Semantic Web; Ontology; Query Languages; Semantics; Chatbots; Cybe-physical Systems; Cyber-physical Systems; Domain Specific; Language Model; Large Language Model; Natural Languages; Ontology's; Query Generation; Semantic-web; Structured Query Language},
	keywords = {Ontology; Query languages; Semantics; Chatbots; Cybe-physical systems; Cyber-physical systems; Domain specific; Language model; Large language model; Natural languages; Ontology's; Query generation; Semantic-Web; Structured Query Language},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Schoch2024,
	author = {Schoch, Nicolai and Hoernicke, Mario and Strem, Nika and Stark, Katharina},
	title = {Engineering Data Funnel (WIP) - An Ontology-Enhanced LLM-Based Agent and MoE System for Engineering Data Processing},
	year = {2024},
	journal = {IEEE International Conference on Emerging Technologies and Factory Automation, ETFA},
	pages = {},
	doi = {10.1109/ETFA61755.2024.10710789},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207839628&doi=10.1109%2FETFA61755.2024.10710789&partnerID=40&md5=77445eddb39fc5dcb5176c5c97d69efc},
	abstract = {Automation Engineering of a process automation system is still a very manual effort due to limited support for the interpretation and processing of process design specification documents. Even though standards for digital data exchange between process and automation engineering do exist, those formats are rarely used and consequently the immense automation potential in automation engineering cannot be lifted. This contribution presents an AI -based approach and prototype - using an ontology-enhanced LLM -based agent and a mixture-of-experts system - to structure and formalize multimodal unstructured process design information as in PDF, Excel, and Word formats and make it available for state-of-the-art engineering tools for the long-known 'Automation of Automation'. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Automation Of Automation; Engineering Data Processing; Engineering Design Specification; Llm-based Agent; Mixture Of Experts; Ontology-driven Information Processing; Network Security; Ontology; Automation Of Automation; Design Specification; Engineering Data; Engineering Data Processing; Engineering Design; Engineering Design Specification; Llm-based Agent; Mixture Of Experts; Ontology's; Ontology-driven Information Processing; Specifications},
	keywords = {Network security; Ontology; Automation of automation; Design specification; Engineering data; Engineering data processing; Engineering design; Engineering design specification; LLM-based agent; Mixture of experts; Ontology's; Ontology-driven information processing; Specifications},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Vieira da Silva2024,
	author = {Vieira da Silva, Luis Miguel and Köcher, Aljosha and Gehlhoff, Felix and Fay, Alexander},
	title = {Toward a Method to Generate Capability Ontologies from Natural Language Descriptions},
	year = {2024},
	journal = {IEEE International Conference on Emerging Technologies and Factory Automation, ETFA},
	pages = {},
	doi = {10.1109/ETFA61755.2024.10710783},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207822437&doi=10.1109%2FETFA61755.2024.10710783&partnerID=40&md5=102ee20eccf408c633442c86eed6632e},
	abstract = {To achieve a flexible and adaptable system, capabil-ity ontologies are increasingly leveraged to describe functions in a machine-interpretable way. However, modeling such complex ontological descriptions is still a manual and error-prone task that requires a significant amount of effort and ontology expertise. This contribution presents an innovative method to automate capability ontology modeling using Large Language Models (LLMs), which have proven to be well suited for such tasks. Our approach requires only a natural language description of a capability, which is then automatically inserted into a predefined prompt using a few-shot prompting technique. After prompting an LLM, the resulting capability ontology is automatically verified through various steps in a loop with the LLM to check the overall correctness of the capability ontology. First, a syntax check is performed, then a check for contradictions, and finally a check for hallucinations and missing ontology elements. Our method greatly reduces manual effort, as only the initial natural language description and a final human review and possible correction are necessary, thereby streamlining the capability ontology generation process. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Capabilities; Large Language Models; Llms; Model-generation; Ontologies; Semantic Web; Skills; Natural Language Processing Systems; Ontology; Semantics; Capability; Language Model; Large Language Model; Model Generation; Natural Languages; Ontology's; Semantic-web; Skill; Modeling Languages},
	keywords = {Natural language processing systems; Ontology; Semantics; Capability; Language model; Large language model; Model generation; Natural languages; Ontology's; Semantic-Web; Skill; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Soysal2024,
	author = {Soysal, Ergin and Roberts, Kirk E.},
	title = {PheNormGPT: a framework for extraction and normalization of key medical findings},
	year = {2024},
	journal = {Database : the journal of biological databases and curation},
	volume = {2024},
	pages = {},
	doi = {10.1093/database/baae103},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207727058&doi=10.1093%2Fdatabase%2Fbaae103&partnerID=40&md5=5681379afe8ba70a87acc7cd54f21d91},
	abstract = {This manuscript presents PheNormGPT, a framework for extraction and normalization of key findings in clinical text. PheNormGPT relies on an innovative approach, leveraging large language models to extract key findings and phenotypic data in unstructured clinical text and map them to Human Phenotype Ontology concepts. It utilizes OpenAI's GPT-3.5 Turbo and GPT-4 models with fine-tuning and few-shot learning strategies, including a novel few-shot learning strategy for custom-tailored few-shot example selection per request. PheNormGPT was evaluated in the BioCreative VIII Track 3: Genetic Phenotype Extraction from Dysmorphology Physical Examination Entries shared task. PheNormGPT achieved an F1 score of 0.82 for standard matching and 0.72 for exact matching, securing first place for this shared task. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Mining; Human; Natural Language Processing; Phenotype; Procedures; Data Mining; Humans; Natural Language Processing; Phenotype},
	keywords = {data mining; human; natural language processing; phenotype; procedures; Data Mining; Humans; Natural Language Processing; Phenotype},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Incitti2024,
	author = {Incitti, Francesca and Salfinger, Andrea and Snidaro, Lauro and Challapalli, Sri},
	title = {Leveraging LLMs for Knowledge Engineering from Technical Manuals: A Case Study in the Medical Prosthesis Manufacturing Domain},
	year = {2024},
	pages = {},
	doi = {10.23919/FUSION59988.2024.10706469},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207690564&doi=10.23919%2FFUSION59988.2024.10706469&partnerID=40&md5=a81498f29b7325e537a3070f58f46227},
	abstract = {Ontologies are nowadays widely used to organize information across specific domains, being effective due to their hierarchical structure and the ability to explicitly represent relationships between concepts. Knowledge engineering, like compiling companies' vast bodies of knowledge into these structures, however, still represents a time-consuming, largely manually performed process, esp. with significant amounts of knowledge often only recorded within unstructured text documents. Since the recently introduced Large Language Models (LLMs) excel on text summarization, this raises the question whether these could be exploited within dedicated knowledge fusion architectures to assist human knowledge engineers by automatically suggesting relevant classes, instances and relations extracted from textual corpora. We therefore propose a novel approach that leverages the taxonomic structure of a partially defined ontology to prompt LLMs for hierarchical knowledge organization. Unlike conventional methods that rely solely on static ontologies, our methodology dynamically generates prompts based on the ontology's existing class taxonomy, prompting the LLM to generate responses that extract supplementary information from unstructured documents. It thus introduces the concept of using ontologies as scaffolds for guiding LLMs, in order to realize a mutual interplay between structured ontological knowledge and the soft fusion capabilities of LLMs. We evaluate our proposed algorithm on a real-world case study, performing a knowledge fusion task on heterogeneous technical documentation from a medical prosthesis manufacturer. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Engineering; Large Language Models; Natural Language Processing; Ontology Population; Soft Fusion; Natural Language Processing Systems; Ontology; Prosthetics; Scaffolds; Scaffolds (biology); Case-studies; Language Model; Language Processing; Large Language Model; Medical Prosthesis; Natural Language Processing; Natural Languages; Ontology Population; Ontology's; Soft Fusions; Taxonomies},
	keywords = {Natural language processing systems; Ontology; Prosthetics; Scaffolds; Scaffolds (biology); Case-studies; Language model; Language processing; Large language model; Medical prosthesis; Natural language processing; Natural languages; Ontology Population; Ontology's; Soft fusions; Taxonomies},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Chang2024,
	author = {Chang, Eunsuk and Sung, Sumi},
	title = {Use of SNOMED CT in Large Language Models: Scoping Review},
	year = {2024},
	journal = {JMIR Medical Informatics},
	volume = {12},
	pages = {},
	doi = {10.2196/62924},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207323832&doi=10.2196%2F62924&partnerID=40&md5=bbb3a56a42eea3ff3990dd0bcea8dca6},
	abstract = {Background: Large language models (LLMs) have substantially advanced natural language processing (NLP) capabilities but often struggle with knowledge-driven tasks in specialized domains such as biomedicine. Integrating biomedical knowledge sources such as SNOMED CT into LLMs may enhance their performance on biomedical tasks. However, the methodologies and effectiveness of incorporating SNOMED CT into LLMs have not been systematically reviewed. Objective: This scoping review aims to examine how SNOMED CT is integrated into LLMs, focusing on (1) the types and components of LLMs being integrated with SNOMED CT, (2) which contents of SNOMED CT are being integrated, and (3) whether this integration improves LLM performance on NLP tasks. Methods: Following the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines, we searched ACM Digital Library, ACL Anthology, IEEE Xplore, PubMed, and Embase for relevant studies published from 2018 to 2023. Studies were included if they incorporated SNOMED CT into LLM pipelines for natural language understanding or generation tasks. Data on LLM types, SNOMED CT integration methods, end tasks, and performance metrics were extracted and synthesized. Results: The review included 37 studies. Bidirectional Encoder Representations from Transformers and its biomedical variants were the most commonly used LLMs. Three main approaches for integrating SNOMED CT were identified: (1) incorporating SNOMED CT into LLM inputs (28/37, 76%), primarily using concept descriptions to expand training corpora; (2) integrating SNOMED CT into additional fusion modules (5/37, 14%); and (3) using SNOMED CT as an external knowledge retriever during inference (5/37, 14%). The most frequent end task was medical concept normalization (15/37, 41%), followed by entity extraction or typing and classification. While most studies (17/19, 89%) reported performance improvements after SNOMED CT integration, only a small fraction (19/37, 51%) provided direct comparisons. The reported gains varied widely across different metrics and tasks, ranging from 0.87% to 131.66%. However, some studies showed either no improvement or a decline in certain performance metrics. Conclusions: This review demonstrates diverse approaches for integrating SNOMED CT into LLMs, with a focus on using concept descriptions to enhance biomedical language understanding and generation. While the results suggest potential benefits of SNOMED CT integration, the lack of standardized evaluation methods and comprehensive performance reporting hinders definitive conclusions about its effectiveness. Future research should prioritize consistent reporting of performance comparisons and explore more sophisticated methods for incorporating SNOMED CT’s relational structure into LLMs. In addition, the biomedical NLP community should develop standardized evaluation frameworks to better assess the impact of ontology integration on LLM performance. © 2024 Elsevier B.V., All rights reserved.},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@CONFERENCE{2024,
	title = {EvalLAC 2024 - Proceedings of the 1st Workshop on Automated Evaluation of Learning and Assessment Content, co-located with the 25th International Conference on Artificial Intelligence in Education, AIED 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3772},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207219495&partnerID=40&md5=528768a3557797326bfc5c4f4170683d},
	abstract = {The proceedings contain 10 papers. The topics discussed include: question difficulty prediction based on virtual test-takers and item response theory; can LLMs evaluate items measuring collaborative problem-solving?; enhancing cross-prompt automated essay scoring by selecting training data based on reinforcement learning; exploring large language models for evaluating automatically generated questions; conceptual map assessment through structure classification; evaluation with language models in non-formal education: understanding student’s persuasion style in competitive debating; towards automatic evaluation of questions generated from ontologies; difficulty of items – predictions on linguistic features; and evaluating LLMs’ performance at automatic short-answer grading. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {Proceedings of the 31st International Workshop on Intelligent Computing in Engineering, EG-ICE 2024},
	year = {2024},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207207232&partnerID=40&md5=f491ca64175e0e36f6072e1116ccf016},
	abstract = {The proceedings contain 68 papers. The topics discussed include: LLM-informed drone control for visual inspection of infrastructure; accurate rail track detection from high-resolution photogrammetric flights: photogrammetry, computer vision, and AI; accurate detection of road markings from high-resolution photogrammetric flights: photogrammetry, computer vision, and AI; digital rules in infrastructure planning: presentation of an ontology- based approach; extension of accessibility ontologies by requirements for inclusive indoor navigation; evaluating topological and geometric requirements for enhanced spatial data management: review and proposals within the context of level of information need; and towards a comprehensive digital twin of a road infrastructure system – requirements analysis and system architecture. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Würsch2024153,
	author = {Würsch, Maxime and Percia David, Dimitri and Mermoud, Alain},
	title = {Monitoring Emerging Trends in LLM Research},
	year = {2024},
	pages = {153 - 161},
	doi = {10.1007/978-3-031-54827-7_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207186959&doi=10.1007%2F978-3-031-54827-7_17&partnerID=40&md5=d9c0c9c7cad04e395f340132498414d8},
	abstract = {Established methodologies for monitoring and forecasting trends in technological development fall short of capturing advancements in Large Language Models (LLMs). This chapter suggests a complementary and alternative approach to mitigate this concern. Traditional indicators, such as search volumes and citation frequencies, are demonstrated to inadequately reflect the rapid evolution of LLMrelated technologies due to biases, semantic drifts, and inherent lags in data documentation. Our presented methodology analyzes the proximity of technological terms related to LLMs, leveraging the OpenAlex and arXiv databases, and focuses on extracting nouns from scientific papers to provide a nuanced portrayal of advancements in LLM technologies. The approach aims to counteract the inherent lags in data, accommodate semantic drift, and distinctly differentiate between various topics, offering both retrospective and prospective insights in their analytical purview. The insights derived underline the need for refined, robust, adaptable, and precise forecasting models as LLMs intersect with domains like cyber defense. At the same time, they are considering the limitations of singular ontologies and integrating advanced anticipatory measures for a nuanced understanding of evolving LLM technologies. © 2024 Elsevier B.V., All rights reserved.},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Efeoğlu2024,
	author = {Efeoğlu, Şefika and Chen, Zongxiong and Schimmler, Sonja},
	title = {Ensuring FAIRness in Machine Learning Projects},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3780},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207042709&partnerID=40&md5=6ba090d3e012e3bf12816486f5fc0b09},
	abstract = {Subsymbolic approaches like machine learning (ML), deep learning, and Large Language Models (LLMs) have significantly advanced Artificial Intelligence, excelling in tasks such as question answering and ontology matching. Despite their success, the lack of openness in LLMs’ training datasets and source codes poses challenges. For instance, some ML-based models do not share training data, limiting transparency. Current standards like schema.org provide a framework for dataset and software metadata but lack ML-specific guidelines. This position paper addresses this gap by proposing a comprehensive schema for ML model metadata aligned with the FAIR (Findability, Accessibility, Interoperability, Reusability) principles. We aim to provide insights into the necessity of an essential metadata format for ML models, demonstrate its integration into ML repository platforms, and show how this schema, combined with dataset metadata, can evaluate an ML model’s adherence to the FAIR principles, fostering FAIRness in ML development. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Fair Ml; Machine Learning; Ml Metadata; Contrastive Learning; Federated Learning; Reusability; Findability, Accessibility, Interoperability, Reusability Machine Learning; Language Model; Learning Projects; Machine Learning Metadata; Machine Learning Models; Machine-learning; Question Answering; Sub-symbolic Approach; Adversarial Machine Learning},
	keywords = {Contrastive Learning; Federated learning; Reusability; Findability, accessibility, interoperability, reusability machine learning; Language model; Learning projects; Machine learning metadata; Machine learning models; Machine-learning; Question Answering; Sub-symbolic approach; Adversarial machine learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Aggarwal2024,
	author = {Aggarwal, Tanay and Salatino, Angelo Antonio and Osborne, Francesco and Motta, Enrico},
	title = {Identifying Semantic Relationships Between Research Topics Using Large Language Models in a Zero-Shot Learning Setting},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3780},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207042155&partnerID=40&md5=595559c5582ddfa5a9fb95c64b924450},
	abstract = {Knowledge Organization Systems (KOS), such as ontologies, taxonomies, and thesauri, play a crucial role in organising scientific knowledge. They help scientists navigate the vast landscape of research literature and are essential for building intelligent systems such as smart search engines, recommendation systems, conversational agents, and advanced analytics tools. However, the manual creation of these KOSs is costly, time-consuming, and often leads to outdated and overly broad representations. As a result, researchers have been exploring automated or semi-automated methods for generating ontologies of research topics. This paper analyses the use of large language models (LLMs) to identify semantic relationships between research topics. We specifically focus on six open and lightweight LLMs (up to 10.7 billion parameters) and use two zero-shot reasoning strategies to identify four types of relationships: broader, narrower, same-as, and other. Our preliminary analysis indicates that Dolphin2.1-OpenOrca-7B performs strongly in this task, achieving a 0.853 F1-score against a gold standard of 1,000 relationships derived from the IEEE Thesaurus. These promising results bring us one step closer to the next generation of tools for automatically curating KOSs, ultimately making the scientific literature easier to explore. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Ontology Generation; Research Topics; Scholarly Knowledge; Scientific Knowledge Graphs; Zero-shot Learning; Adversarial Machine Learning; Contrastive Learning; Federated Learning; Knowledge Graph; Knowledge Organization System (kos); Latent Semantic Analysis; Ontology; Recommender Systems; Semantics; Taxonomies; Knowledge Graphs; Language Model; Large Language Model; Ontology Generation; Ontology's; Research Topics; Scholarly Knowledge; Scientific Knowledge; Scientific Knowledge Graph; Semantic Relationships; Zero-shot Learning},
	keywords = {Adversarial machine learning; Contrastive Learning; Federated learning; Knowledge graph; Knowledge organization system (KOS); Latent semantic analysis; Ontology; Recommender systems; Semantics; Taxonomies; Knowledge graphs; Language model; Large language model; Ontology generation; Ontology's; Research topics; Scholarly knowledge; Scientific knowledge; Scientific knowledge graph; Semantic relationships; Zero-shot learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Wu2024,
	author = {Wu, Guanchen and Ling, Chen and Graetz, Ilana and Zhao, Liang},
	title = {Ontology extension by online clustering with large language model agents},
	year = {2024},
	journal = {Frontiers in Big Data},
	volume = {7},
	pages = {},
	doi = {10.3389/fdata.2024.1463543},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206991474&doi=10.3389%2Ffdata.2024.1463543&partnerID=40&md5=4a013d00d9628be6b3bacdc7b5c1de48},
	abstract = {An ontology is a structured framework that categorizes entities, concepts, and relationships within a domain to facilitate shared understanding, and it is important in computational linguistics and knowledge representation. In this paper, we propose a novel framework to automatically extend an existing ontology from streaming data in a zero-shot manner. Specifically, the zero-shot ontology extension framework uses online and hierarchical clustering to integrate new knowledge into existing ontologies without substantial annotated data or domain-specific expertise. Focusing on the medical field, this approach leverages Large Language Models (LLMs) for two key tasks: Symptom Typing and Symptom Taxonomy among breast and bladder cancer survivors. Symptom Typing involves identifying and classifying medical symptoms from unstructured online patient forum data, while Symptom Taxonomy organizes and integrates these symptoms into an existing ontology. The combined use of online and hierarchical clustering enables real-time and structured categorization and integration of symptoms. The dual-phase model employs multiple LLMs to ensure accurate classification and seamless integration of new symptoms with minimal human oversight. The paper details the framework's development, experiments, quantitative analyses, and data visualizations, demonstrating its effectiveness in enhancing medical ontologies and advancing knowledge-based systems in healthcare. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Model; Medical Ontology; Online Hierarchical Clustering; Ontology Extension; Zero-shot Classification},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@ARTICLE{2024,
	title = {28th International Conference on Theory and Practice of Digital Libraries, TPDL 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {15178 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206271537&partnerID=40&md5=e4bc400ed0beb9c91fdc300a3068b3f2},
	abstract = {The proceedings contain 43 papers. The special focus in this conference is on Theory and Practice of Digital Libraries. The topics include: A Reputation System for Scientific Contributions Based on a Token Economy; bibliotheca Eugeniana Digital—Unveiling and Visualizing the Treasures of Prince Eugene of Savoy’s Library; comparative Analysis of Evaluation Measures for Scientific Text Simplification; promoting Interoperability on the Datasets of the Arrowheads Findings of the Chalcolithic and the Early/Middle Bronze Age; tracing the Retraction Cascade: Identifying Non-retracted but Potentially Retractable Articles; mapping Techniques for an Automated Library Classification: The Case Study of Library Loans at Bibliotheca Hertziana; LIT: Label-Informed Transformers on Token-Based Classification; Improving Retrieval and Expression of Iconographical and Iconological Semantic Statements: An Extension of the ICON Ontology; scholarly Quality Measurements: A Systematic Literature Review; assessing the Accessibility and Usability of Web Archives for Blind Users; leveraging Transfer Learning for Article Segmentation in Historical Newspapers; Multi-dimensional Edge-Embedded GCNs for Arabic Text Classification; LIAS: Layout Information-Based Article Separation in Historical Newspapers; CALM: Context Augmentation with Large Language Model for Named Entity Recognition; database Approaches to the Modelling and Querying of Musical Scores: A Survey; Content-Based Dataset Retrieval Methods: Reproducibility of the ACORDAR Test Collection; enhancing Identification of Scholarly Reference on YouTube: Method Development and Analysis of External Link Characteristics; mining Literary Trends: A Tool for Digital Library Analysis; PRET19: Automatic Recognition and Indexing of Handwritten Loan Registers from 19<sup>th</sup> Century Parisian Universities; leveraging Open Large Language Models for Historical Named Entity Recognition; Enriching Archival Linked Data Descriptions with Information from Wikidata and DBpedia; OpenPSS: An Open Page Stream Segmentation Benchmark. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Penkov2024272,
	author = {Penkov, Stanislav},
	title = {Mitigating Hallucinations in Large Language Models via Semantic Enrichment of Prompts: Insights from BioBERT and Ontological Integration},
	year = {2024},
	journal = {Proceedings of the International Conference Computational Linguistics in Bulgaria},
	pages = {272 - 276},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206264551&partnerID=40&md5=0831974af08e95cba6ebb47261f40da5},
	abstract = {The advent of Large Language Models (LLMs) has been transformative for natural language processing, yet their tendency to produce “hallucinations”-outputs that are factually incorrect or entirely fabricated-remains a significant hurdle. This paper introduces a proactive methodology for reducing hallucinations by strategically enriching LLM prompts. This involves identifying key entities and contextual cues from varied domains and integrating this information into the LLM prompts to guide the model towards more accurate and relevant responses. Leveraging examples from BioBERT for biomedical entity recognition and ChEBI for chemical ontology, we illustrate a broader approach that encompasses semantic prompt enrichment as a versatile tool for enhancing LLM output accuracy. By examining the potential of semantic and ontological enrichment in diverse contexts, we aim to present a scalable strategy for improving the reliability of AI-generated content, thereby contributing to the ongoing efforts to refine LLMs for a wide range of applications. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Biobert Entity Recognition; Domain-specific Ontologies; Hallucination Mitigation; Large Language Models (llms); Semantic Prompt Enrichment},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kougioumtzidou2024247,
	author = {Kougioumtzidou, Anna and Papoutsis, Angelos and Kavallieros, Dimitris and Mavropoulos, Thanassis and Tsikrika, Theodora and Vrochidis, Stefanos and Kompatsiaris, Ioannis (Yiannis)},
	title = {An End-to-End Framework for Cybersecurity Taxonomy and Ontology Generation and Updating},
	year = {2024},
	pages = {247 - 254},
	doi = {10.1109/CSR61664.2024.10679346},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206170969&doi=10.1109%2FCSR61664.2024.10679346&partnerID=40&md5=6b9b5f05f1f9120cb0652ff6fa09d2ad},
	abstract = {Effective cyber-defense practices often require the use of structured knowledge representations, such as taxonomies and ontologies, to organise vast amounts of data and facili-tate knowledge representation and reasoning. To this end, we present an Artificial Intelligence (AI)-assisted framework for the construction and update of cybersecurity taxonomies and ontologies. The proposed framework can be divided into three main phases: Taxonomy Construction, Ontology Construction, and Taxonomy/Ontology Update, each phase consisting of both information extraction and semantic knowledge representation components. For information extraction, we employ a variety of techniques originating from Natural Language Processing (NLP), particularly Transformer Neural Networks. For constructing ontologies, we propose a conceptual ontology schema based on the STIX 2.1 standard for modeling information related to attacks, threats, and vulnerabilities, and use the Owlready2 Python library. Overall, our framework effectively builds cybersecurity taxonomies and ontologies and updates existing knowledge of both the generated and open-source taxonomies and ontologies. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Attacks; Cybersecurity; Dynamic Update; Large Language Models; Natural Language Processing; Ontologies; Taxonomies; Vulnerabilities; Knowledge Representation; Phishing; Semantics; Attack; Cyber Security; Dynamic Update; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Ontology's; Vulnerability; Cyber Attacks},
	keywords = {Knowledge representation; Phishing; Semantics; Attack; Cyber security; Dynamic update; Language model; Language processing; Large language model; Natural language processing; Natural languages; Ontology's; Vulnerability; Cyber attacks},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{2024,
	title = {28th International Conference on Theory and Practice of Digital Libraries, TPDL 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {15177 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206100277&partnerID=40&md5=ef6d5dce8949de2e3e90ad5b8e4391cc},
	abstract = {The proceedings contain 43 papers. The special focus in this conference is on Theory and Practice of Digital Libraries. The topics include: A Reputation System for Scientific Contributions Based on a Token Economy; bibliotheca Eugeniana Digital—Unveiling and Visualizing the Treasures of Prince Eugene of Savoy’s Library; comparative Analysis of Evaluation Measures for Scientific Text Simplification; promoting Interoperability on the Datasets of the Arrowheads Findings of the Chalcolithic and the Early/Middle Bronze Age; tracing the Retraction Cascade: Identifying Non-retracted but Potentially Retractable Articles; mapping Techniques for an Automated Library Classification: The Case Study of Library Loans at Bibliotheca Hertziana; LIT: Label-Informed Transformers on Token-Based Classification; Improving Retrieval and Expression of Iconographical and Iconological Semantic Statements: An Extension of the ICON Ontology; scholarly Quality Measurements: A Systematic Literature Review; assessing the Accessibility and Usability of Web Archives for Blind Users; leveraging Transfer Learning for Article Segmentation in Historical Newspapers; Multi-dimensional Edge-Embedded GCNs for Arabic Text Classification; LIAS: Layout Information-Based Article Separation in Historical Newspapers; CALM: Context Augmentation with Large Language Model for Named Entity Recognition; database Approaches to the Modelling and Querying of Musical Scores: A Survey; Content-Based Dataset Retrieval Methods: Reproducibility of the ACORDAR Test Collection; enhancing Identification of Scholarly Reference on YouTube: Method Development and Analysis of External Link Characteristics; mining Literary Trends: A Tool for Digital Library Analysis; PRET19: Automatic Recognition and Indexing of Handwritten Loan Registers from 19<sup>th</sup> Century Parisian Universities; leveraging Open Large Language Models for Historical Named Entity Recognition; Enriching Archival Linked Data Descriptions with Information from Wikidata and DBpedia; OpenPSS: An Open Page Stream Segmentation Benchmark. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Saini2024,
	author = {Saini, Shashwat and Vrindavanam, Jayavrinda and Mondal, Subhash},
	title = {Methodological Insights into Protein Clustering Using BERT & RoBERTa},
	year = {2024},
	pages = {},
	doi = {10.1109/CONECCT62155.2024.10677287},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205832131&doi=10.1109%2FCONECCT62155.2024.10677287&partnerID=40&md5=6656d8b16e169ba15a9b81849b3d8247},
	abstract = {Proteins are present in all living organisms, and understanding their processes is vital. Protein databases such as SWISS-PROT include curated information on only 570,000 protein sequences, representing a fraction of the 250 million known evidential and predicted sequences; it becomes crucial to cluster proteins into similar groups. This research explores the application of two transformer architectures, BERT and RoBERTa in clustering proteins in the supervised prediction of Gene Ontology (GO) annotations. The detailed methodology for both the pre-training and fine-tuning processes, as well as results that showcase RoBERTa outperforming BERT in the context of protein clustering, on performance metrics of accuracy and loss. Operating under constrained computational resources, the deployed model exhibits strong performance and highlight the robustness of methodology in protein clustering within resource constraints. This study not only contributes to the understanding of protein clustering but also signifies the potential of transformer models to handle biological data. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Masked Language Modelling; Natural Language Processing; Protein Clustering; Roberta; Transformers; Gene Ontology; Invertebrates; Modeling Languages; Natural Language Processing Systems; Bert; Clusterings; Language Model; Language Processing; Masked Language Modeling; Natural Language Processing; Natural Languages; Protein Clustering; Roberta; Transformer; Biotic},
	keywords = {Gene Ontology; Invertebrates; Modeling languages; Natural language processing systems; BERT; Clusterings; Language model; Language processing; Masked language modeling; Natural language processing; Natural languages; Protein clustering; RoBERTa; Transformer; Biotic},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Gupta2024,
	author = {Gupta, Pranav and Sharma, Raunak and Kumari, Rashmi and Aditya, Sri Krishna and Choudhary, Shwetank and Kumar, Sumit and Kanchana, M. and Thilagavathy, R.},
	title = {ECHO: Environmental Sound Classification with Hierarchical Ontology-guided Semi-Supervised Learning},
	year = {2024},
	pages = {},
	doi = {10.1109/CONECCT62155.2024.10677303},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205825848&doi=10.1109%2FCONECCT62155.2024.10677303&partnerID=40&md5=dd9414b66e627f0744370228215bd482},
	abstract = {Environment Sound Classification has been a well-studied research problem in the field of signal processing and till now more focus has been laid on fully supervised approaches. Recently, the focus has moved towards semi-supervised methods which concentrate on utilizing unlabeled data, and self-supervised methods which learn the intermediate representation through pretext tasks or contrastive learning. However, both approaches require a vast amount of unlabelled data to improve performance. In this work, we propose a novel framework called Environmental Sound Classification with Hierarchical Ontology-guided semi-supervised Learning (ECHO) that utilizes label ontology-based hierarchy to learn semantic representation by defining a novel pretext task. The model tries to predict coarse labels represented by the Large Language Model (LLM) based on ground truth label ontology, then further fine-tuned in a supervised way to predict the actual task. ECHO achieves a 1% to 8% accuracy improvement over baseline systems across UrbanSound8K, ESC-10, and ESC-50 datasets. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Environment Sound Classification; Label Ontology; Semi-supervised Learning; Adversarial Machine Learning; Contrastive Learning; Federated Learning; Ontology; Semantics; Semi-supervised Learning; Environment Sound Classification; Environmental Sound Classifications; Label Ontology; Learn+; Ontology's; Research Problems; Signal-processing; Sound Classification; Unlabeled Data; Self-supervised Learning},
	keywords = {Adversarial machine learning; Contrastive Learning; Federated learning; Ontology; Semantics; Semi-supervised learning; Environment sound classification; Environmental sound classifications; Label ontology; Learn+; Ontology's; Research problems; Signal-processing; Sound classification; Unlabeled data; Self-supervised learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Perera2024299,
	author = {Perera, Olga and Liu, Jun},
	title = {Exploring large language models for ontology learning},
	year = {2024},
	journal = {Issues in Information Systems},
	volume = {25},
	number = {4},
	pages = {299 - 310},
	doi = {10.48009/4_iis_2024_124},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205759289&doi=10.48009%2F4_iis_2024_124&partnerID=40&md5=1ac03c1e27be91453f94fc7de6e48f8d},
	abstract = {Ontology Learning aims to facilitate automatic or semi-automatic ontology development based on machine learning techniques in context of big data. Recent evolution of technology has introduced Generative Artificial Intelligence (AI) capable of creating new data, extracting insights from the existing data, and generating coherent texts from various inputs. This ability supports analysis of text data, providing insights and annotations that reduce human effort. This study explores the emerging field of Generative AI, specifically, Large Language Models for ontology learning. We conducted a survey of the current state of Generative AI research with focus on applicability and efficacy for ontology development tasks, and assessment of evaluation techniques. We discussed challenges related to explainability and interpretability of Generative AI and outlined directions for future research. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Generative Ai; Large Language Models; Llm; Ontology Learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@CONFERENCE{Carta2024396,
	author = {Carta, Salvatore Mario and Giuliani, Alessandro and Manca, Marco Manolo and Piano, Leonardo and Pisu, Alessia and Tiddia, Sandro Gabriele},
	title = {Instruct Large Language Models for Public Administration Document Information Extraction},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3762},
	pages = {396 - 401},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205580023&partnerID=40&md5=dc5035afabc75dd414fc210deae4e3b4},
	abstract = {With the rapid digitization of institutions, there is an ever-increasing problem of effectively organizing and accessing information. Public Administrations (PAs) manage large volumes of disparate data from a variety of sources. Thus, these organizations would greatly benefit from AI, particularly Natural Language Processing solutions that help organize, structure, and search for information effectively. In the context of Italian PA, which we address in this paper, there are two main challenges: the lack of ontologies and the limited tools available for Italian information extraction. In this paper, we attempt to advance Information Extraction for Italian PAs by instructing a Large Language Model on a set of automatically labeled triplets of public tenders. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Italian Open Information Extraction; Large Language Models; Public Administration; Tenders; Natural Language Processing Systems; Open Data; Digitisation; Italian Open Information Extraction; Language Model; Language Processing; Large Language Model; Large Volumes; Natural Languages; Ontology's; Processing Solutions; Tender},
	keywords = {Natural language processing systems; Open Data; Digitisation; Italian open information extraction; Language model; Language processing; Large language model; Large volumes; Natural languages; Ontology's; Processing solutions; Tender},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {5th International Conference on Higher Education Learning Methodologies and Technologies Online, HELMeTO 2023},
	year = {2024},
	journal = {Communications in Computer and Information Science},
	volume = {2076 CCIS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205388146&partnerID=40&md5=56b41c5891711234ed5a42db9e4f6bc0},
	abstract = {The proceedings contain 52 papers. The special focus in this conference is on Higher Education Learning Methodologies and Technologies Online. The topics include: Get in (Multi)touch with the Tangent; internationalisation Experience on a Digital Platform and Its Impact on Self-efficacy: The Results on a Sample of Initial Teacher Education Students; digital Multisensory Storytelling as Educational-Didactic Methodology for Emotional Literacy; automated Online Assessment and Cloud-Based Programming: Advancing Computer Engineering Education; Leveraging Explainable AI Methods and Tools for Educational Data; botBid - From Botany to Big Data: Combining Citizen Science and Innovative Teaching Methodologies; detecting the Usage of Large Language Models Exploiting Generative Adversarial Networks; ontology for Constructively Aligned, Collaborative, and Evolving Engineer Knowledge-Management Platforms; augmented Didactic: The Potential of Gesture in Mobile Learning to Enhance Learning; digital Twins and E-Learning: Navigating Challenges and Opportunities; emotions in Practice: Studying Lectures and Seminars in On-Line and Offline Education; performing Art-Based Methodology for Empathetic Transposition into Online Learning Experiences; improving Student Online Interactions and Teacher’s Ability to Manage Them with the Quick Chat Moodle Plugin; promoting Meaningful Learning in Topology Supported by Undergraduate Students’ Video Creations; the Feedback in a Formative Assessment Path: Development of Communicative Skills in a Workshop Online; online Resources for Training Pre-service Primary School Teachers in Mathematics; mathematics Interpretative Tasks and Formative Assessment: A Digital Device for Teachers Training; Cybersecurity for Teens (CS4T) – A Project by Ludoteca of Registro .it; Learning CyberSecurity with Story-Driven CTF Challenges: CyberTrials 2023; superCyberKids: Enhancing Cybersecurity Education in K-12 Through Digital Game-Based Learning; Empowering Higher Education with ChatGPT: Innovating University Instructional Design. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Banerjee2024137,
	author = {Banerjee, Shubhanker and Chakravarthi, Bharathi Raja and Mccrae, John Philip},
	title = {Large Language Models for Few-Shot Automatic Term Extraction},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14762 LNCS},
	pages = {137 - 150},
	doi = {10.1007/978-3-031-70239-6_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205377483&doi=10.1007%2F978-3-031-70239-6_10&partnerID=40&md5=1d8b90d62ef32b8cf5241528fc134779},
	abstract = {Automatic term extraction is the process of identifying domain-specific terms in a text using automated algorithms and is a key first step in ontology learning and knowledge graph creation. Large language models have shown good few-shot capabilities, thus, in this paper, we present a study to evaluate the few-shot in-context learning performance of GPT-3.5-Turbo on automatic term extraction. To benchmark the performance we compare the results with fine-tuning of a BERT-sized model. We also carry out experiments with count-based term extractors to assess their applicability to few-shot scenarios. We quantify prompt sensitivity with experiments to analyze the variation in performance of large language models across different prompt templates. Our results show that in-context learning with GPT-3.5-Turbo outperforms the BERT-based model and unsupervised count-based methods in few-shot scenarios. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Automatic Term Extraction; Few-shot; Large Language Models; Contrastive Learning; Knowledge Graph; Ontology; Automated Algorithms; Automatic Term Extraction; Context Learning; Domain Specific; Few-shot; In Contexts; Language Model; Large Language Model; Ontology Learning; Performance; Zero-shot Learning},
	keywords = {Contrastive Learning; Knowledge graph; Ontology; Automated algorithms; Automatic term extraction; Context learning; Domain specific; Few-shot; In contexts; Language model; Large language model; Ontology learning; Performance; Zero-shot learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Jiao2024185,
	author = {Jiao, Yizhu and Li, Sha and Zhou, Sizhe and Ji, Heng and Han, Jiawei},
	title = {TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents},
	year = {2024},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	pages = {185 - 205},
	doi = {10.18653/v1/2024.findings-acl.12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205299812&doi=10.18653%2Fv1%2F2024.findings-acl.12&partnerID=40&md5=f5ffe4a4cd92a0399e4bd5902c63d2b9},
	abstract = {The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new formulation of IE TEXT2DB that emphasizes the integration of IE output and the target database (or knowledge base). Given a user instruction, a document set, and a database, our task requires the model to update the database with values from the document set to satisfy the user instruction. This task requires understanding user instructions for what to extract and adapting to the given DB/KB schema for how to extract on the fly. To evaluate this new task, we introduce a new benchmark featuring common demands such as data infilling, row population, and column addition. In addition, we propose an LLM agent framework OPAL (Observe-Plan-Analyze LLM) which includes an Observer component that interacts with the database, the Planner component that generates a code-based plan with calls to IE models, and the Analyzer component that provides feedback regarding code quality before execution. Experiments show that OPAL can successfully adapt to diverse database schemas by generating different code plans and calling the required IE models. We also highlight difficult cases such as dealing with large databases with complex dependencies and extraction hallucination, which we believe deserve further investigation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Database Systems; Document Sets; Downstream Applications; Extraction Modeling; Extraction Ontologies; Infilling; Language Model; Model Agents; On-the-fly; Structured Knowledge; Target Database; Computational Linguistics},
	keywords = {Database systems; Document sets; Downstream applications; Extraction modeling; Extraction ontologies; Infilling; Language model; Model agents; On-the-fly; Structured knowledge; Target database; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{2024,
	title = {8th International Joint Conference on Rules and Reasoning, RuleML+RR 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {15183 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205109055&partnerID=40&md5=0bbfa206ee7ce0a3bb505e2114fc607b},
	abstract = {The proceedings contain 17 papers. The special focus in this conference is on Rules and Reasoning. The topics include: Collaborative Benchmarking Rule-Reasoners with B-Runner; rule Confidence Aggregation for Knowledge Graph Completion; RIFF: Inducing Rules for Fraud Detection from Decision Trees; ontology-Based Update in Virtual Knowledge Graphs via Schema Mapping Recovery; reevaluation of Inductive Link Prediction; judicial Explanations; OntoRaster: Extending VKGs with Raster Data; complete Approximations of Incomplete Queries; reasoning in Rough Description Logics with Multiple Indiscernibility Relations; a Benchmark for Rule Induction in Automated Business Decisions; revising Defeasible Theories via Instructions; FaithEL: Strongly TBox Faithful Knowledge Base Embeddings for EL; RDF Surfaces as a First-Order Language for the Semantic Web; ambiguities in Defeasible Logic: A Computational Efficient Framework and Algorithm; legally-Guided Automated Decision-Making System Using Language Model Agents for Autonomous Driving. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wang2024234,
	author = {Wang, Ya and Barta, Dainel and Hesse, Julian and Buchwald, Philip and Paschke, Adrian},
	title = {Legally-Guided Automated Decision-Making System Using Language Model Agents for Autonomous Driving},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {15183 LNCS},
	pages = {234 - 248},
	doi = {10.1007/978-3-031-72407-7_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205093338&doi=10.1007%2F978-3-031-72407-7_17&partnerID=40&md5=811d60b0fe6e4f948ed65404df8376e5},
	abstract = {Recent advances in language models have facilitated the development of agent-based systems. Despite their encouraging results in various reasoning tasks, these systems often operate as “black boxes”, raising concerns about potential illegal behavior due to opaque decision-making processes. This concern is particularly critical in autonomous driving, where precise decision-making requires a thorough understanding of traffic scenes and strict adherence to established norms. In this paper, we propose a legally-guided automated decision making system (LAD) that employs language models to dynamically retrieve facts for related rules through context-based query generation while delegating decision-making to a symbolic solver. In our experiments, we demonstrate that this neuro-symbolic system, with a limited number of formalized traffic rules, provides a more accurate, interpretable, and traceable solution for rule-compliant decision-making compared to pure language models. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Autonomous Driving; Large Language Model Agent; Neurosymbolic System; Ontological Reasoning; Rule Compliance; Crime; Modeling Languages; Ontology; Query Languages; Structured Query Language; Agent-based Systems; Automated Decision Making Systems; Autonomous Driving; Decisions Makings; Language Model; Large Language Model Agent; Model Agents; Neuro-symbolic System; Ontological Reasoning; Rule Compliance},
	keywords = {Crime; Modeling languages; Ontology; Query languages; Structured Query Language; Agent-based systems; Automated decision making systems; Autonomous driving; Decisions makings; Language model; Large language model agent; Model agents; Neuro-symbolic system; Ontological reasoning; Rule compliance},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Jian2024,
	author = {Jian, Zhaorui and Liu, Shengquan and Gao, Wei and Cheng, Jianming},
	title = {Distantly Supervised Relation Extraction based on Non-taxonomic Relation and Self-Optimization},
	year = {2024},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	pages = {},
	doi = {10.1109/IJCNN60899.2024.10650745},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205022179&doi=10.1109%2FIJCNN60899.2024.10650745&partnerID=40&md5=d2d77fa81dbf65e1b6c6a51250f1793d},
	abstract = {Distantly supervised relation extraction (DS-RE) leverages existing knowledge bases to generate annotated data for relation extraction (RE), addressing the issue of scarce labeled data. However, distant supervision (DS) is often limited by coarse annotations and insufficient contextual awareness, leading to relational ambiguity and introducing noise in the labeled results. Moreover, although one can optimize the classifiers in DS-RE models through weight updates, the static nature of the guiding rules for such adjustments often falls short when addressing the challenges posed by diverse non-taxonomic relations and complex noise patterns in datasets. In this paper, we propose a DS-RE framework that capitalizes on non-taxonomic relations and a self-optimizing mechanism. We define a set of consistent DS relation candidates and combine DS with a LLM to enhance the perception of entities' contextual states during the DS process. Then, we design a Self-Optimizing Ontology-Enhanced Non-taxonomic Relation Extraction Model (SO-NRE). The model incorporates additional entity-relation knowledge to enhance the semantic depth of Non-taxonomic relation ontologies and uses an adaptive dynamic scheduling mechanism to refine the classification strategy through iterations informed by self-perception outcomes. The experimental results show that the improved DS annotation workflow has enhanced accuracy, and SO-NRE outperforms mainstream baselines in RE performance. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Distantly Supervised Relation Extraction; Llm; Non-taxonomic Relation; Self-optimization; Ontology; Self-supervised Learning; Distantly Supervised Relation Extraction; Extraction Modeling; Labeled Data; Llm; Non-taxonomic Relation; Ontology's; Relation Extraction; Self-optimization; Self-optimizing; Weight Update; Semantics},
	keywords = {Ontology; Self-supervised learning; Distantly supervised relation extraction; Extraction modeling; Labeled data; LLM; Non-taxonomic relation; Ontology's; Relation extraction; Self-optimization; Self-optimizing; Weight update; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{2024,
	title = {SeMatS 2024 - Proceedings of the 1st International Workshop on Semantic Materials Science: Harnessing the Power of Semantic Web Technologies in Materials Science, co-located with the 20th International Conference on Semantic Systems, SEMANTiCS 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3760},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205014978&partnerID=40&md5=5ec6a2830248aac190d3634934914798},
	abstract = {The proceedings contain 10 papers. The topics discussed include: PBF-AMP-onto: an ontology for powder bed fusion additive manufacturing processes; an ontology for units of measures across history, standards, and scientific and technology domains; top level ontologies: desirable characteristics in the context of materials science; PolyMat - bringing semantics to polymer membrane research; implementing semantic technologies in materials science and engineering; enhancing semantic interoperability across materials science with HIVE4MAT; the landscape of ontologies in materials science and engineering: a survey and evaluation; leveraging large language models for automated knowledge graphs generation in non-destructive testing; and battery manufacturing knowledge infrastructure requirements for multicriteria optimization based decision support in design of simulation. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Liu2024,
	author = {Liu, Xu and Chen, Xinming and Zhu, Yangfu and Wu, Bin},
	title = {Prompt-Enhanced Prototype Framework for Few-shot Event Detection},
	year = {2024},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	pages = {},
	doi = {10.1109/IJCNN60899.2024.10651359},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204998092&doi=10.1109%2FIJCNN60899.2024.10651359&partnerID=40&md5=0bed9e0efc27aa6edceae2346f8eaa30},
	abstract = {Few-shot event detection (ED) aims at identifying and typing event mentions from text with limited annotations. Most existing methods for few-shot ED use event ontology and related knowledge to construct prototypes and fail to fully leverage the rich knowledge of pre-trained language models (PLMs) which could help improve the representation of prototypes. Motivated by this, we propose an prompt-enhanced prototype framework which combines prototype and prompt for few-shot ED. Considering the scarcity of labeled data, we also introduce contrastive learning to enrich prototypes. Specifically, we use heuristic rules to align FrameNet with annotated data to get corresponding prompts for each event and convert them into prompt prototype. We then leverage contrastive learning to aggregate event mentions into prototypes and maintain these prototypes for few-shot ED. Furthermore, We explore diverse prompt formats for representing prompt prototypes and introduce a more comprehensive lexical prompt which improves the performance of few-shot ED. We conduct extensive experiments on the MAVEN corpus to reveal the effectiveness of the proposed framework compared to state-of-the-art methods. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Representation; Labeled Data; Zero-shot Learning; Event Ontology; Events Detection; Framenet; Heuristic Rules; Language Model; Performance; State-of-the-art Methods; Contrastive Learning},
	keywords = {Knowledge representation; Labeled data; Zero-shot learning; Event ontology; Events detection; FrameNet; Heuristic rules; Language model; Performance; State-of-the-art methods; Contrastive Learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Dembska202456,
	author = {Dembska, Marta and Helle, Oliver and Yadav, Itisha and Peters, Diana},
	title = {Implementing semantic technologies in materials science and engineering},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3760},
	pages = {56 - 66},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204950662&partnerID=40&md5=5374b3393dbd66ed5168538c1b61ed01},
	abstract = {The Materials Science and Engineering (MSE) field is an interdisciplinary branch of engineering characterized by high volumes of heterogeneous data, which also serve as inputs for other fields reliant on materials. While semantic technologies are already utilized in various research areas to address data management challenges, their adoption in MSE is still in its early stages. This paper provides an overview of data management issues in MSE, existing semantic technologies, and the potential application of these technologies to address those issues. This leads to a roadmap for the implementation of semantic technologies in MSE. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Management; Interoperability; Large Language Models; Materials Science And Engineering; Ontologies; Semantic Technologies; Data Management Challenges; Engineering Fields; Heterogeneous Data; High Volumes; Language Model; Large Language Model; Materials Science And Engineering; Ontology's; Research Areas; Semantic Technologies},
	keywords = {Data management challenges; Engineering fields; Heterogeneous data; High volumes; Language model; Large language model; Materials science and engineering; Ontology's; Research areas; Semantic technologies},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Eells202451,
	author = {Eells, Andrew and Dave, Brandon and Hitzler, Pascal Al and Shimizu, Cogan Matthew},
	title = {Commonsense Ontology Micropatterns},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14980 LNAI},
	pages = {51 - 59},
	doi = {10.1007/978-3-031-71170-1_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204924291&doi=10.1007%2F978-3-031-71170-1_6&partnerID=40&md5=0b24aa4261f72c54883ac97bc1f05f77},
	abstract = {The previously introduced Modular Ontology Modeling methodology (MOMo) attempts to mimic the human analogical process by using modular patterns to assemble more complex concepts. To support this, MOMo organizes ontology design patterns (ODPs) into design libraries, which are programmatically queryable. However, a major bottleneck to large-scale deployment of MOMo is the (to-date) limited availability of ready-to-use ODPs. At the same time, Large Language Models (LLMs) have quickly become a source of common knowledge and, in some cases, replacing search engines for questions. In this paper, we thus present a collection of 104 ODPs representing often occurring nouns, curated from the common-sense knowledge available in LLMs, organized into a fully-annotated modular ontology design library ready for use with MOMo. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Human Form Models; Structured Query Language; Design Library; Design Patterns; Language Model; Micro Pattern; Modeling Methodology; Modular Ontologies; Modulars; Ontology Design; Ontology Model; Ontology's; Ontology},
	keywords = {Human form models; Structured Query Language; Design library; Design Patterns; Language model; Micro pattern; Modeling methodology; Modular ontologies; Modulars; Ontology design; Ontology model; Ontology's; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Saetia202453,
	author = {Saetia, Chanatip and Phruetthiset, Jiratha and Chalothorn, Tawunrat and Lertsutthiwong, Monchai and Taerungruang, Supawat and Buabthong, Pakpoom},
	title = {Financial Product Ontology Population with Large Language Models},
	year = {2024},
	pages = {53 - 60},
	doi = {10.18653/v1/2024.textgraphs-1.4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204898551&doi=10.18653%2Fv1%2F2024.textgraphs-1.4&partnerID=40&md5=bfa3b333c19f905f0559a241ed50be00},
	abstract = {Ontology population, which aims to extract structured data to enrich domain-specific ontologies from unstructured text, typically faces challenges in terms of data scarcity and linguistic complexity, particularly in specialized fields such as retail banking. In this study, we investigate the application of large language models (LLMs) to populate domain-specific ontologies of retail banking products from Thai corporate documents. We compare traditional span-based approaches to LLMs-based generative methods, with different prompting techniques. Our findings reveal that while span-based methods struggle with data scarcity and the complex linguistic structure, LLMs-based generative approaches substantially outperform, achieving a 61.05% F1 score, with the most improvement coming from providing examples in the prompts. This improvement highlights the potential of LLMs for ontology population tasks, offering a scalable and efficient solution for structured information extraction, especially in low-resource language settings. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Data Scarcity; Domain-specific Ontologies; Financial Products; Language Model; Model-based Opc; Ontology Population; Product Ontologies; Retail Banking; Structured Data; Unstructured Texts; Ontology},
	keywords = {Computational linguistics; Data scarcity; Domain-specific ontologies; Financial products; Language model; Model-based OPC; Ontology Population; Product Ontologies; Retail banking; Structured data; Unstructured texts; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{2024,
	title = {TextGraphs at ACL 2024 - Proceedings of TextGraphs-17: Graph-Based Methods for Natural Language Processing, 62nd Annual Meeting of the Association of Computational Linguistics},
	year = {2024},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204880560&partnerID=40&md5=68ee36dcb78c4c5c0712c8bf46814866},
	abstract = {The proceedings contain 15 papers. The topics discussed include: learning human action representations from temporal context in lifestyle vlogs; a pipeline approach for parsing documents into uniform meaning representation graphs; financial product ontology population with large language models; prompt me one more time: a two-step knowledge extraction pipeline with ontology-based verification; towards understanding attention-based reasoning through graph structures in medical codes classification; leveraging graph structures to detect hallucinations in large language models; semantic graphs for syntactic simplification: a revisit from the age of LLM; Skoltech at TextGraphs-17 Shared Task: finding GPT-4 prompting strategies for multiple choice que stions; and JellyBell at TextGraphs-17 Shared Task: fusing large language models with external knowledge for enhanced question answering. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {18th International Conference on Neural-Symbolic Learning and Reasoning, NeSy 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14980 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204878154&partnerID=40&md5=ed1a72ce355c9be92ea92bd4af9de564},
	abstract = {The proceedings contain 48 papers. The special focus in this conference is on Neural-Symbolic Learning and Reasoning. The topics include: Variable Assignment Invariant Neural Networks for Learning Logic Programs; viPro: Enabling and Controlling Video Prediction for Complex Dynamical Scenarios Using Procedural Knowledge; the Role of Foundation Models in Neuro-Symbolic Learning and Reasoning; a Fuzzy Loss for Ontology Classification; On the Use of Neurosymbolic AI for Defending Against Cyber Attacks; bayesian Inverse Graphics for Few-Shot Concept Learning; simple and Effective Transfer Learning for Neuro-Symbolic Integration; ethical Reward Machine; embed2Rule Scalable Neuro-Symbolic Learning via Latent Space Weak-Labelling; ULLER: A Unified Language for Learning and Reasoning; disentangling Visual Priors: Unsupervised Learning of Scene Interpretations with Compositional Autoencoder; Probing LLMs for Logical Reasoning; enhancing Machine Learning Predictions Through Knowledge Graph Embeddings; terminating Differentiable Tree Experts; Valid Text-to-SQL Generation with Unification-Based DeepStochLog; Enhancing Geometric Ontology Embeddings for EL<sup>++</sup> with Negative Sampling and Deductive Closure Filtering; Lattice-Preserving ALC Ontology Embeddings; Towards Learning Abductive Reasoning Using VSA Distributed Representations; learning to Solve Abstract Reasoning Problems with Neurosymbolic Program Synthesis and Task Generation; Leveraging Neurosymbolic AI for Slice Discovery. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ŝváb-Zamazal2024,
	author = {Ŝváb-Zamazal, Ondr̂ej Ř.Ej},
	title = {Towards Pattern-based Complex Ontology Matching using SPARQL and LLM},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3759},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204733796&partnerID=40&md5=5f38b3109696be1d51c0bc9e2ef0d2c3},
	abstract = {Complex ontology matching is a process to match complex structures in ontologies.While many matching tools tackle simple ontology matching, complex ontology matching is still rare.However, one entity in one ontology can be similar to a complex structure (1-to-n) or even complex structures can be on both sides (m-to-n).Therefore, the application, e.g., data integration, must consider complex correspondences within ontology alignment.Our poster paper presents a pattern-based approach where particular SPARQL queries correspond to a specific pattern, e.g., Class by Attribute Type (CAT), for its detection.SPARQL queries are anchored to entities from simple correspondences on input.Detected complex correspondence candidates are verbalized to be validated by the Large Language Model (LLM).Further, we provide a zero-shot prompting preliminary experiment and evaluation.The poster paper is equipped with the Jupyter notebook for automation of the pipeline and the full report of the experiment at: https://github.com/OndrejZamazal/ComplexOntologyMatching-SEMANTiCS2024. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Complex Ontology Matching; Knowledge Graph; Large Language Model; Ontology; Ontology Matching; Knowledge Graph; Modeling Languages; Ontology; Structured Query Language; Complex Correspondences; Complex Ontology Matching; Complexes Structure; Knowledge Graphs; Language Model; Large Language Model; Ontology Matching; Ontology's; Simple++; Semantics},
	keywords = {Knowledge graph; Modeling languages; Ontology; Structured Query Language; Complex correspondences; Complex ontology matching; Complexes structure; Knowledge graphs; Language model; Large language model; Ontology matching; Ontology's; Simple++; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Pisu2024,
	author = {Pisu, Alessia and Pompianu, Livio and Salatino, Angelo Antonio and Osborne, Francesco and Riboni, Daniele and Motta, Enrico and Reforgiato Recupero, Diego},
	title = {Classifying Scientific Topic Relationships with SciBERT},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3759},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204687914&partnerID=40&md5=a41e6340c629d84f102448f83d2f523d},
	abstract = {Current AI systems, including smart search engines and recommendation systems tools for streamlining literature reviews, and interactive question-answering platforms, are becoming indispensable for researchers to navigate and understand the vast landscape of scientific knowledge.Taxonomies and ontologies of research topics are key to this process, but manually creating them is costly and often leads to outdated results.This poster paper shows the use of SciBERT model to automatically generate research topic ontologies.Our model excels at identifying semantic relationships between research topics, outperforming traditional methods.This approach promises to streamline the creation of accurate and up-to-date ontologies, enhancing the effectiveness of AI tools for researchers. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Generation; Language Models; Ontology Generation; Research Topics; Scibert; Question Answering; Recommender Systems; Search Engines; Semantics; 'current; Ai Systems; Graph Generation; Knowledge Graph Generation; Knowledge Graphs; Language Model; Ontology Generation; Ontology's; Research Topics; Scibert; Knowledge Graph},
	keywords = {Question answering; Recommender systems; Search engines; Semantics; 'current; AI systems; Graph generation; Knowledge graph generation; Knowledge graphs; Language model; Ontology generation; Ontology's; Research topics; SciBERT; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2024,
	title = {SEMANTiCS-PDWT 2024 - Joint Proceedings of Posters, Demos, Workshops, and Tutorials of the 20th International Conference on Semantic Systems, co-located with 20th International Conference on Semantic Systems, SEMANTiCS 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3759},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204683099&partnerID=40&md5=966a950f145c2afc591f922a8de34ec7},
	abstract = {The proceedings contain 31 papers. The topics discussed include: facilitating learning analytics in histology courses with knowledge graphs; 8-star linked open data model: extending the 5-star model for better reuse, quality, and trust of data; continuous knowledge graph quality assessment through comparison using ABECTO; ORKG ASK: a neuro-symbolic scholarly search and exploration system; towards pattern-based complex ontology matching using SPARQL and LLM; data-sovereign enterprise collaboration using the solid protocol; the Helmholtz digitization ontology: representing digital assets in the Helmholtz digital ecosystem; and facilitating search of the virtual record treasury of Ireland knowledge graph using ChatGPT. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Tang2024,
	author = {Tang, Tianyi and Xiong, Yiming and Zhang, Ruige and Zhang, Jian and Li, Wenfei and Wang, Jun and Wang, Wei},
	title = {Progress in protein pre-training models integrating structural knowledge; 融合结构知识的蛋白质预训练模型进展},
	year = {2024},
	journal = {Wuli Xuebao/Acta Physica Sinica},
	volume = {73},
	number = {18},
	pages = {},
	doi = {10.7498/aps.73.20240811},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204566544&doi=10.7498%2Faps.73.20240811&partnerID=40&md5=28fcc874214c2edd109762b22644812b},
	abstract = {The AI revolution, sparked by natural language and image processing, has brought new ideas and research paradigms to the field of protein computing. One significant advancement is the development of pre-training protein language models through self-supervised learning from massive protein sequences. These pre-trained models encode various information about protein sequences, evolution, structures, and even functions, which can be easily transferred to various downstream tasks and demonstrate robust generalization capabilities. Recently, researchers have further developed multimodal pre-trained models that integrate more diverse types of data. The recent studies in this direction are summarized and reviewed from the following aspects in this paper. Firstly, the protein pre-training models that integrate protein structures into language models are reviewed: this is particularly important, for protein structure is the primary determinant of its function. Secondly, the pretrained models that integrate protein dynamic information are introduced. These models may benefit downstream tasks such as protein-protein interactions, soft docking of ligands, and interactions involving allosteric proteins and intrinsic disordered proteins. Thirdly, the pre-trained models that integrate knowledge such as gene ontology are described. Fourthly, we briefly introduce pre-trained models in RNA fields. Finally, we introduce the most recent developments in protein designs and discuss the relationship of these models with the aforementioned pre-trained models that integrate protein structure information. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Machine Learning; Protein Foundation Model; Protein Multi-modal Model; Protein Structure; Adversarial Machine Learning; Gene Ontology; Semi-supervised Learning; Foundation Models; Language Model; Machine-learning; Modal Models; Multi-modal; Pre-training; Protein Foundation Model; Protein Multi-modal Model; Proteins Structures; Training Model; Self-supervised Learning},
	keywords = {Adversarial machine learning; Gene Ontology; Semi-supervised learning; Foundation models; Language model; Machine-learning; Modal models; Multi-modal; Pre-training; Protein foundation model; Protein multi-modal model; Proteins structures; Training model; Self-supervised learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{2024,
	title = {18th International Conference on Neural-Symbolic Learning and Reasoning, NeSy 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14979 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204540661&partnerID=40&md5=01812243a4e8a5625224e116fc358475},
	abstract = {The proceedings contain 48 papers. The special focus in this conference is on Neural-Symbolic Learning and Reasoning. The topics include: Variable Assignment Invariant Neural Networks for Learning Logic Programs; viPro: Enabling and Controlling Video Prediction for Complex Dynamical Scenarios Using Procedural Knowledge; the Role of Foundation Models in Neuro-Symbolic Learning and Reasoning; a Fuzzy Loss for Ontology Classification; On the Use of Neurosymbolic AI for Defending Against Cyber Attacks; bayesian Inverse Graphics for Few-Shot Concept Learning; simple and Effective Transfer Learning for Neuro-Symbolic Integration; ethical Reward Machine; embed2Rule Scalable Neuro-Symbolic Learning via Latent Space Weak-Labelling; ULLER: A Unified Language for Learning and Reasoning; disentangling Visual Priors: Unsupervised Learning of Scene Interpretations with Compositional Autoencoder; Probing LLMs for Logical Reasoning; enhancing Machine Learning Predictions Through Knowledge Graph Embeddings; terminating Differentiable Tree Experts; Valid Text-to-SQL Generation with Unification-Based DeepStochLog; Enhancing Geometric Ontology Embeddings for EL<sup>++</sup> with Negative Sampling and Deductive Closure Filtering; Lattice-Preserving ALC Ontology Embeddings; Towards Learning Abductive Reasoning Using VSA Distributed Representations; learning to Solve Abstract Reasoning Problems with Neurosymbolic Program Synthesis and Task Generation; Leveraging Neurosymbolic AI for Slice Discovery. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Khettari2024457,
	author = {Khettari, Oumaima El and Nishida, Noriki and Liu, Shanshan and Munne, Rumana Ferdous and Yamagata, Yuki and Quiniou, Solen and Chaffron, Samuel and Matsumoto, Yuji},
	title = {Mention-Agnostic Information Extraction for Ontological Annotation of Biomedical Articles},
	year = {2024},
	pages = {457 - 473},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204491490&partnerID=40&md5=ac874f0f7c93237fe46b2d479e87994e},
	abstract = {Biomedical information extraction is crucial for advancing research, enhancing healthcare, and discovering treatments by efficiently analyzing extensive data. Given the extensive amount of biomedical data available, automated information extraction methods are necessary due to manual extraction’s labor-intensive, expertise-dependent, and costly nature. In this paper, we propose a novel two-stage system for information extraction where we annotate biomedical articles based on a specific ontology (HOIP). The major challenge is annotating relation between biomedical processes often not explicitly mentioned in text articles. Here, we first predict the candidate processes and then determine the relationships between these processes without relying on mentions. The experimental results show promising outcomes in mention-agnostic process identification using Large Language Models (LLMs). In relation classification, our proposed BERT-based models outperform LLMs significantly. The end-to-end evaluation results suggest the difficulty of this task and room for improvement in both process identification and relation classification.. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Classification (of Information); Computational Linguistics; Ontology; Automated Information; Biomedical Data; Biomedical Information Extractions; Information Extraction Methods; Labour-intensive; Language Model; Ontology's; Process Identification; Relation Classifications; Two Stage System; Information Retrieval},
	keywords = {Classification (of information); Computational linguistics; Ontology; Automated information; Biomedical data; Biomedical information extractions; Information extraction methods; Labour-intensive; Language model; Ontology's; Process identification; Relation classifications; Two stage system; Information retrieval},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Wen202410370,
	author = {Wen, Yilin and Wang, Zifeng and Sun, Jimeng},
	title = {MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models},
	year = {2024},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	volume = {1},
	pages = {10370 - 10388},
	doi = {10.18653/v1/2024.acl-long.558},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204489189&doi=10.18653%2Fv1%2F2024.acl-long.558&partnerID=40&md5=21d0cb061d5c93fcef52c63dddd92bd2},
	abstract = {Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named MindMap, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question & answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wylwilling/MindMap. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Natural Language Processing Systems; Implicit Knowledge; Knowledge Graphs; Language Model; Mind Maps; Model Inference; Model Transparency; Natural Language Generation; Natural Language Understanding; Performance; Reasoning Process; Knowledge Graph},
	keywords = {Computational linguistics; Natural language processing systems; Implicit knowledge; Knowledge graphs; Language model; Mind maps; Model inference; Model transparency; Natural language generation; Natural language understanding; Performance; Reasoning process; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Cai20242842,
	author = {Cai, Zefan and Kung, Po Nien and Suvarna, Ashima and Ma, Mingyu Derek and Bansal, Hritik and Chang, Baobao and Brantingham, Paul Jeffrey and Wang, Wei and Peng, Nanyun},
	title = {Improving Event Definition Following For Zero-Shot Event Detection},
	year = {2024},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	volume = {1},
	pages = {2842 - 2863},
	doi = {10.18653/v1/2024.acl-long.157},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204456047&doi=10.18653%2Fv1%2F2024.acl-long.157&partnerID=40&md5=e73063875e2b655e271a052050258b7e},
	abstract = {Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations. In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten examples per event type. Beyond scaling, we incorporate event ontology information and hard-negative samples during training, further boosting the performance. Based on these findings, we fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that surpasses SOTA large language models like GPT-3.5 across three open benchmarks on zero-shot event detection. Our code and data can be found at https://github.com/PlusLabNLP/ZeroED. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Large Datasets; Modeling Languages; Event Types; Events Detection; Events Extractions; High Quality; Learn+; Performance; Set Of Events; Sporadics; Train Model; Training Model; Zero-shot Learning},
	keywords = {Computational linguistics; Large datasets; Modeling languages; Event Types; Events detection; Events extractions; High quality; Learn+; Performance; Set of events; Sporadics; Train model; Training model; Zero-shot learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Achara2024493,
	author = {Achara, Akshit and Sasidharan, Sanand and Gagan, N.},
	title = {Efficient Biomedical Entity Linking: Clinical Text Standardization with Low-Resource Techniques},
	year = {2024},
	pages = {493 - 505},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204439187&partnerID=40&md5=1f0e7b5f08efd0660af2b1d5a9fa7d1c},
	abstract = {Clinical text is rich in information, with mentions of treatment, medication and anatomy among many other clinical terms. Multiple terms can refer to the same core concepts which can be referred as a clinical entity. Ontologies like the Unified Medical Language System (UMLS) are developed and maintained to store millions of clinical entities including the definitions, relations and other corresponding information. These ontologies are used for standardization of clinical text by normalizing varying surface forms of a clinical term through Biomedical entity linking. With the introduction of transformer-based language models, there has been significant progress in Biomedical entity linking. In this work, we focus on learning through synonym pairs associated with the entities. As compared to the existing approaches, our approach significantly reduces the training data and resource consumption. Moreover, we propose a suite of context-based and context-less reranking techniques for performing the entity disambiguation. Overall, we achieve similar performance to the state-of-the-art zero-shot and distant supervised entity linking techniques on the Medmentions dataset, the largest annotated dataset on UMLS, without any domain-based training. Finally, we show that retrieval performance alone might not be sufficient as an evaluation metric and introduce an article level quantitative and qualitative analysis to reveal further insights on the performance of entity linking methods.. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology; Unified Modeling Language; Clinical Terms; Context-based; Language Model; Ontology's; Performance; Re-ranking Techniques; Resources Consumption; Surface Forms; Training Data; Unified Medical Language Systems; Computational Linguistics},
	keywords = {Ontology; Unified Modeling Language; Clinical terms; Context-based; Language model; Ontology's; Performance; Re-ranking techniques; Resources consumption; Surface forms; Training data; Unified medical language systems; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Shlyk2024380,
	author = {Shlyk, Darya and Groza, Tudor and Montanelli, Stefano and Cavalleri, Emanuele and Mesiti, Marco},
	title = {REAL: A Retrieval-Augmented Entity Linking Approach for Biomedical Concept Recognition},
	year = {2024},
	pages = {380 - 389},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204426361&partnerID=40&md5=add561ad3e2397b0ec15ef41855d1983},
	abstract = {Large Language Models (LLMs) offer an appealing alternative to training dedicated models for many Natural Language Processing (NLP) tasks. However, outdated knowledge and hallucination issues can be major obstacles in their application in knowledge-intensive biomedical scenarios. In this study, we consider the task of biomedical concept recognition (CR) from unstructured scientific literature and explore the use of Retrieval Augmented Generation (RAG) to improve accuracy and reliability of the LLM-based biomedical CR. Our approach, named REAL (Retrieval Augmented Entity Linking), combines the generative capabilities of LLMs with curated knowledge bases to automatically annotate natural language texts with concepts from bio-ontologies. By applying REAL to benchmark corpora on phenotype concept recognition, we show its effectiveness in improving LLM-based CR performance. This research highlights the potential of combining LLMs with external knowledge sources to advance biomedical text processing. Source code is available at: https://github.com/dash-ka/REAL-BioCR.. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmarking; Natural Language Processing Systems; Bio-ontologies; Concept Recognition; External Knowledge; Language Model; Language Processing; Model-based Opc; Natural Languages; Natural Languages Texts; Performance; Scientific Literature; Computational Linguistics},
	keywords = {Benchmarking; Natural language processing systems; Bio-ontologies; Concept recognition; External knowledge; Language model; Language processing; Model-based OPC; Natural languages; Natural languages texts; Performance; Scientific literature; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{van Cauter202475,
	author = {van Cauter, Zeno and Yakovets, Nikolay},
	title = {Ontology-guided Knowledge Graph Construction from Maintenance Short Texts},
	year = {2024},
	pages = {75 - 84},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204424427&partnerID=40&md5=603dc56e6e13fbfd87205e48724e6583},
	abstract = {Large-scale knowledge graph construction remains infeasible since it requires significant human-expert involvement. Further complications arise when building graphs from domain-specific data due to their unique vocabularies and associated contexts. In this work, we demonstrate the ability of open-source large language models (LLMs), such as Llama-2 and Llama-3, to extract facts from domain-specific Maintenance Short Texts (MSTs). We employ an approach which combines ontology-guided triplet extraction and in-context learning. By using only 20 semantically similar examples with the Llama-3-70B-Instruct model, we achieve performance comparable to previous methods that relied on fine-tuning techniques like SpERT and REBEL. This indicates that domain-specific fact extraction can be accomplished through inference alone, requiring minimal labeled data. This opens up possibilities for effective and efficient semi-automated knowledge graph construction for domain-specific data. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Adversarial Machine Learning; Computational Linguistics; Contrastive Learning; Domain Knowledge; Ontology; Semantics; Domain Specific; Graph Construction; Human Expert; In Contexts; Knowledge Graphs; Language Model; Large-scales; Ontology's; Open-source; Short Texts; Knowledge Graph},
	keywords = {Adversarial machine learning; Computational linguistics; Contrastive Learning; Domain Knowledge; Ontology; Semantics; Domain specific; Graph construction; Human expert; In contexts; Knowledge graphs; Language model; Large-scales; Ontology's; Open-source; Short texts; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{2024,
	title = {KaLLM 2024 - 1st Workshop on Knowledge Graphs and Large Language Models, Proceedings of the Workshop},
	year = {2024},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204424171&partnerID=40&md5=d776a5cc59adeb65a48bc011547363b2},
	abstract = {The proceedings contain 13 papers. The topics discussed include: multi-hop database reasoning with virtual knowledge graph; zero- and few-shots knowledge graph triplet extraction with large language models; analysis of LLM’s ‘spurious’ correct answers using evidence information of multi-hop QA datasets; KGAST: from knowledge graphs to annotated synthetic texts; HRGraph: leveraging LLMs for HR data knowledge graphs with information propagation-based job recommendation; adapting multilingual LLMs to low-resource languages with knowledge graphs via adapters; ontology-guided knowledge graph construction from maintenance short texts; educational material to knowledge graph conversion: a methodology to enhance digital education; and zero-shot fact-checking with semantic triples and knowledge graphs. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Usmanova2024168,
	author = {Usmanova, Aida and Usbeck, Ricardo},
	title = {Structuring Sustainability Reports for Environmental Standards with LLMs guided by Ontology},
	year = {2024},
	pages = {168 - 177},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204423193&partnerID=40&md5=37d7cf082d5590cc536fab064c9a9cd2},
	abstract = {Following the introduction of the European Sustainability Reporting Standard (ESRS), companies will have to adapt to a new policy and provide mandatory sustainability reports. However, implementing such reports entails a challenge, such as the comprehension of a large number of textual information from various sources. This task can be accelerated by employing Large Language Models (LLMs) and ontologies to effectively model the domain knowledge. In this study, we extended an existing ontology to model ESRS Topical Standard for disclosure. The developed ontology would enable automated reasoning over the data and assist in constructing Knowledge Graphs (KGs). Moreover, the proposed ontology extension would also help to identify gaps in companies’ sustainability reports with regard to the ESRS requirements. Additionally, we extracted knowledge from corporate sustainability reports via LLMs guided with a proposed ontology and developed their KG representation. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Automated Reasoning; Domain Knowledge; Environmental Standards; Knowledge Graphs; Language Model; Ontology's; Reporting Standards; Sustainability Report; Sustainability Reporting; Textual Information; Sustainable Development Goals},
	keywords = {Automated reasoning; Domain knowledge; Environmental standards; Knowledge graphs; Language model; Ontology's; Reporting standards; Sustainability report; Sustainability reporting; Textual information; Sustainable development goals},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Gurgurov202463,
	author = {Gurgurov, Daniil and Hartmann, Mareike and Ostermann, Simon},
	title = {Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters},
	year = {2024},
	pages = {63 - 74},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204422426&partnerID=40&md5=c5a4754d9c368a6b48bc5247ac4e71a4},
	abstract = {This paper explores the integration of graph knowledge from linguistic ontologies into multilingual Large Language Models (LLMs) using adapters to improve performance for low-resource languages (LRLs) in sentiment analysis (SA) and named entity recognition (NER). Building upon successful parameter-efficient fine-tuning techniques, such as K-ADAPTER (Wang et al., 2021) and MAD-X (Pfeiffer et al., 2020), we propose a similar approach for incorporating knowledge from multilingual graphs, connecting concepts in various languages with each other through linguistic relationships, into multilingual LLMs for LRLs. Specifically, we focus on eight LRLs —Maltese, Bulgarian, Indonesian, Nepali, Javanese, Uyghur, Tibetan, and Sinhala — and employ language-specific adapters fine-tuned on data extracted from the language-specific section of ConceptNet, aiming to enable knowledge transfer across the languages covered by the knowledge graph. We compare various fine-tuning objectives, including standard Masked Language Modeling (MLM), MLM with full-word masking, and MLM with targeted masking, to analyze their effectiveness in learning and integrating the extracted graph data. Through empirical evaluation on language-specific tasks, we assess how structured graph knowledge affects the performance of multilingual LLMs for LRLs in SA and NER, providing insights into the potential benefits of adapting language models for low-resource scenarios. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Conceptnet; Fine Tuning; Improve Performance; Knowledge Graphs; Language Model; Linguistic Ontology; Low Resource Languages; Named Entity Recognition; Sentiment Analysis; Tibetans; Knowledge Graph},
	keywords = {Computational linguistics; ConceptNet; Fine tuning; Improve performance; Knowledge graphs; Language model; Linguistic ontology; Low resource languages; Named entity recognition; Sentiment analysis; Tibetans; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Tanaka20248815,
	author = {Tanaka, Shinnosuke and Barry, James and Kuruvanthodi, Vishnudev and Moses, Movina and Giammona, Maxwell J. and Herr, Nathan and Elkaref, Mohab and de Mel, Geeth Ranmal},
	title = {KnowledgeHub: An End-to-End Tool for Assisted Scientific Discovery},
	year = {2024},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	pages = {8815 - 8819},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204305030&partnerID=40&md5=05345ad9d182604337998f8d0f327d26},
	abstract = {This paper describes the KnowledgeHub tool, a scientific literature Information Extraction (IE) and Question Answering (QA) pipeline. This is achieved by supporting the ingestion of PDF documents that are converted to text and structured representations. An ontology can then be constructed where a user defines the types of entities and relationships they want to capture. A browser-based annotation tool enables annotating the contents of the PDF documents according to the ontology. Named Entity Recognition (NER) and Relation Classification (RC) models can be trained on the resulting annotations and can be used to annotate the unannotated portion of the documents. A knowledge graph is constructed from these entity and relation triples which can be queried to obtain insights from the data. Furthermore, we integrate a suite of Large Language Models (LLMs) that can be used for QA and summarisation that is grounded in the included documents via a retrieval component. KnowledgeHub is a unique tool that supports annotation, IE and QA, which gives the user full insight into the knowledge discovery pipeline. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Modeling Languages; Question Answering; Annotation Tool; Classification Models; End To End; Named Entity Recognition; Ontology's; Pdf Document; Question Answering; Relation Classifications; Scientific Discovery; Scientific Literature; Ontology},
	keywords = {Knowledge graph; Modeling languages; Question answering; Annotation tool; Classification models; End to end; Named entity recognition; Ontology's; PDF document; Question Answering; Relation classifications; Scientific discovery; Scientific literature; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {KG4S 2024 - Proceedings of the 2nd International Workshop on Knowledge Graphs for Sustainability, colocated with the 21st Extended Semantic Web Conference, ESWC 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3753},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204298667&partnerID=40&md5=11dd58466e8a3a00e5b8f94d5b3c511d},
	abstract = {The proceedings contain 8 papers. The topics discussed include: semantic asset administration shell for circular economy; modelling digital product passports for the circular economy; supporting companion planting with the CoPla ontology; the open circularity platform: a decentralized data sharing platform for circular value networks; an ontology for the reuse and tracking of prefabricated building components; knowledge graph aided LLM based ESG question-answering from news; initial and experimental ontology alignment results in the circular economy domain; and communication in design-for-circularity: requirements to a knowledge graph. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Li20246368,
	author = {Li, Guozheng and Wang, Peng and Ke, Wenjun and Guo, Yikai and Ji, Ke and Shang, Ziyu and Liu, Jiajun and Xu, Zijie},
	title = {Recall, Retrieve and Reason: Towards Better In-Context Relation Extraction},
	year = {2024},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	pages = {6368 - 6376},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204281542&partnerID=40&md5=5e706d22ad9f81aa779535978a4d9727},
	abstract = {Relation extraction (RE) aims to identify relations between entities mentioned in texts. Although large language models (LLMs) have demonstrated impressive in-context learning (ICL) abilities in various tasks, they still suffer from poor performances compared to most supervised fine-tuned RE methods. Utilizing ICL for RE with LLMs encounters two challenges: (1) retrieving good demonstrations from training examples, and (2) enabling LLMs exhibit strong ICL abilities in RE. On the one hand, retrieving good demonstrations is a non-trivial process in RE, which easily results in low relevance regarding entities and relations. On the other hand, ICL with an LLM achieves poor performance in RE while RE is different from language modeling in nature or the LLM is not large enough. In this work, we propose a novel recall-retrieve-reason RE framework that synergizes LLMs with retrieval corpora (training examples) to enable relevant retrieving and reliable in-context reasoning. Specifically, we distill the consistently ontological knowledge from training datasets to let LLMs generate relevant entity pairs grounded by retrieval corpora as valid queries. These entity pairs are then used to retrieve relevant training examples from the retrieval corpora as demonstrations for LLMs to conduct better ICL via instruction tuning. Extensive experiments on different LLMs and RE datasets demonstrate that our method generates relevant and valid entity pairs and boosts ICL abilities of LLMs, achieving competitive or new state-of-the-art performance on sentence-level RE compared to previous supervised fine-tuning methods and ICL-based methods. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Modeling Languages; Problem Oriented Languages; Self-supervised Learning; Structured Query Language; Supervised Learning; Context Learning; Context Reasoning; Extraction Method; In Contexts; Language Model; Learning Abilities; Non-trivial; Poor Performance; Relation Extraction; Training Example; Demonstrations},
	keywords = {Modeling languages; Problem oriented languages; Self-supervised learning; Structured Query Language; Supervised learning; Context learning; Context reasoning; Extraction method; In contexts; Language model; Learning abilities; Non-trivial; Poor performance; Relation extraction; Training example; Demonstrations},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Liu2024248,
	author = {Liu, Hao and Zhou, Shuxin and Chen, Zhehuan and Perl, Yehoshua and Wang, Jiayin},
	title = {Using Generative Large Language Models for Hierarchical Relationship Prediction in Medical Ontologies},
	year = {2024},
	pages = {248 - 256},
	doi = {10.1109/ICHI61247.2024.00040},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203690166&doi=10.1109%2FICHI61247.2024.00040&partnerID=40&md5=44d6691ccccf632afe6aa2f13128c9c9},
	abstract = {This study extends the exploration of ontology enrichment by evaluating the performance of various open-sourced Large Language Models (LLMs) on the task of predicting hierarchical relationships (IS-A) in medical ontologies including SNOMED CT Clinical Finding and Procedure hierarchies and the human Disease Ontology. With the previous finetuned BERT models for hierarchical relationship prediction as the baseline, we assessed eight open-source generative LLMs for the same task. We observed only three models, without finetuning, demonstrated comparable or superior performance compared to the baseline BERT -based models. The best performance model OpenChat achieved a macro average F1 score of 0.96 (0.95) on SNOMED CT Clinical Finding (Procedure) hierarchy, an increase over 7% from the baseline 0.89 (0.85). On human Disease Ontology, OpenChat excels with an F1 score of 0.91, outperforming the second-best performance model Vicuna (0.84). Notably, some LLMs prove unsuitable for hierarchical relationship prediction tasks or appliable for concept placement of medical ontologies. We also explored various prompt templates and ensemble techniques to uncover potential confounding factors in applying LLMs for IS-A relation predictions for medical ontologies. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Hieratical Relation Prediction; Large Language Models; Medical Ontology; Prompts Design; Snomed Ct; Generative Adversarial Networks; Hieratical Relation Prediction; Human Disease; Language Model; Large Language Model; Medical Ontology; Performance; Performance Modeling; Prompt Design; Snomed-ct; Prediction Models},
	keywords = {Generative adversarial networks; Hieratical relation prediction; Human disease; Language model; Large language model; Medical ontology; Performance; Performance Modeling; Prompt design; SNOMED-CT; Prediction models},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Reforgiato Recupero2024,
	author = {Reforgiato Recupero, Diego and Boi, Lorenzo},
	title = {Towards Seamless Human-Robot Dialogue through a Robot Action Ontology},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3749},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203599306&partnerID=40&md5=0024f1635504dfd41e02758f4eb2d9a5},
	abstract = {This research paper introduces a novel methodology enabling the Zora humanoid robot to effectively engage in dynamic interactions by responding to user queries and complementing its responses with appropriate gestures. Notably, these inquiries may extend beyond mere questions to encompass action commands articulated by the user, which the robot proficiently recognizes and executes. The integration of a Large Language Model enhances the system's capabilities, particularly in the domain of questionanswering. To bolster the recognition and execution of action commands, we have employed a robot action ontology established in previous research endeavors. This ontology defines relevant classes and individuals, forming the basis for a nuanced understanding of user-inputted action commands. Further refinement involves the generation of succinct three-word strings for each action, ensuring semantic alignment with the user's verbal instructions. Importantly, our system operates in two distinctive modes: STATELESS and STATEFUL. In STATEFUL mode, the robot possesses awareness of its present posture, allowing it to execute action commands only when they align with its current state. This adaptive feature enhances the overall effectiveness of the system, catering to the dynamic nature of human-robot interactions and promoting a seamless and contextually aware dialogue between the NAO humanoid robot and its users. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Action Robot Ontology; Human-robot Interaction; Large Language Models; Natural Language Processing; Anthropomorphic Robots; Human Robot Interaction; Microrobots; Modeling Languages; Natural Language Processing Systems; Ontology; Action Robot Ontology; Humanoid Robot; Humans-robot Interactions; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Ontology's; Robot Actions; Semantics},
	keywords = {Anthropomorphic robots; Human robot interaction; Microrobots; Modeling languages; Natural language processing systems; Ontology; Action robot ontology; Humanoid robot; Humans-robot interactions; Language model; Language processing; Large language model; Natural language processing; Natural languages; Ontology's; Robot actions; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bouchouras2024,
	author = {Bouchouras, Georgios and Bitilis, Pavlos and Kotis, Konstantinos I. and Vouros, George A.},
	title = {LLMs for the Engineering of a Parkinson Disease Monitoring and Alerting Ontology},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3749},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203587310&partnerID=40&md5=6cf0a5bc51abfa68aafd9a5fe1f9d3ea},
	abstract = {This paper investigates the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology. The focus is on the ontology engineering methodology which combines the capabilities of LLMs and human expertise to develop more robust and comprehensive domain ontologies, faster than humans do alone. Evaluating models like ChatGPT-3.5, ChatGPT4, Gemini, and Llama2, this study explores various LLM based ontology engineering methods. The findings reveal that the proposed hybrid approach (both LLM and human involvement), namely X-HCOME, consistently excelled in class generation and F-1 score, indicating its efficiency in creating valid and comprehensive ontologies faster than humans do alone. The study underscores the potential of the combined LLMs and human intelligence to enrich PD domain knowledge and enhance expert-generated PD ontologies. In overall, the presented approach exemplifies a promising collaboration between machine capabilities and human expertise in developing ontologies for complex domains. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Human-llm Teaming; Llms; Ontology Engineering; Parkinson Disease; Disease Control; Human Engineering; Ontology; Disease Monitoring; Human Expertise; Human-large Language Model Teaming; Language Model; Large Language Model; Ontology Engineering; Ontology Engineering Methodologies; Ontology's; Parkinson's Disease; Neurodegenerative Diseases},
	keywords = {Disease control; Human engineering; Ontology; Disease monitoring; Human expertise; Human-large language model teaming; Language model; Large language model; Ontology engineering; Ontology Engineering Methodologies; Ontology's; Parkinson's disease; Neurodegenerative diseases},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2024,
	title = {ESWC-JP 2024 - Joint Proceedings of the ESWC 2024 Workshops and Tutorials, co-located with 21st European Semantic Web Conference, ESWC 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3749},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203581764&partnerID=40&md5=9712f25f3551d5bfc25d246cfad1c3a8},
	abstract = {The proceedings contain 18 papers. The topics discussed include: towards seamless human-robot dialogue through a robot action ontology; the SPA ontology: towards a web of things ready for robotic agents; KB4RL: towards a knowledge base for automatic creation of state and action spaces for reinforcement learning; towards a knowledge engineering methodology for flexible robot manipulation in everyday tasks; steps towards generalized manipulation action plans tackling mixing task; towards improving large language models’ planning capabilities on WoT thing descriptions by generating python objects as intermediary representations; transforming web knowledge into actionable knowledge graphs for robot manipulation tasks; and NeSy is alive and well: a LLM-driven symbolic approach for better code comment data generation and classification. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Boscariol2024404,
	author = {Boscariol, Marta and Meschini, Silvia and Tagliabue, Lavinia Chiara},
	title = {A METHODOLOGICAL APPROACH TO ASSET INFORMATION MANAGEMENT VIA KNOWLEDGE GRAPHS AND LARGE LANGUAGE MODELS},
	year = {2024},
	journal = {Proceedings of the European Conference on Computing in Construction},
	volume = {2024},
	pages = {404 - 411},
	doi = {10.35490/EC3.2024.286},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203450817&doi=10.35490%2FEC3.2024.286&partnerID=40&md5=50bf8ee205a09ed23cb6003cd2122658},
	abstract = {Tackling the need of large organizations for a proactive Asset Information Management (AIM) System, a methodological approach to knowledge management applied to built assets portfolios is proposed. It aims at synergically leveraging Knowledge Graphs (KGs) and Artificial Intelligence (AI) technologies to enable analytics on input data. In the theorized pipeline Large Language Models (LLMs) are meant to be used both in the graph creation phase, extracting data from unstructured sources and organizing them according to domain ontologies, as tested on a use-case sample, and in the knowledge extraction phase via queries. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {TEXT2KG 2024 and DQMLKG 2024 - Joint Proceedings of the 3rd International Workshop One Knowledge Graph Generation from Text and Data Quality Meets Machine Learning and Knowledge Graphs, co-located with the Extended Semantic Web Conference, ESWC 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3747},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203398354&partnerID=40&md5=7821ee35299e62f9deb0f39558ddc0bf},
	abstract = {The proceedings contain 17 papers. The topics discussed include: open knowledge base canonicalization: techniques and challenges; incorporating type information into zero-shot relation extraction; generating e-commerce related knowledge graph from text: open challenges and early results using LLMs; towards dataset for extracting relations in the climate-change domain; on constructing biomedical text-to-graph systems with large language models; fine-tuning vs. prompting: evaluating the knowledge graph construction with LLMs; Battalogy: empowering battery data management through ontology-driven knowledge graph; leveraging language models for generating ontologies of research topics; towards harnessing large language models as autonomous agents for semantic triple extraction from unstructured text; knowledge graphs for digital transformation monitoring in social media; and moving from tabular knowledge graph quality assessment to RDF triples leveraging ChatGPT. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Tsaneva202415,
	author = {Tsaneva, Stefani and Vasic, Stefan and Sabou, Marta},
	title = {LLM-driven Ontology Evaluation: Verifying Ontology Restrictions with ChatGPT},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3747},
	pages = {15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203385177&partnerID=40&md5=7ab507fcd01da145dd09b126148403a6},
	abstract = {Recent advancements in artificial intelligence, particularly in large language models (LLMs), have sparked interest in their application to knowledge engineering (KE) tasks. While existing research has primarily explored the utilisation of LLMs for constructing and completing semantic resources such as ontologies and knowledge graphs, the evaluation of these resources-addressing quality issues- has not yet been thoroughly investigated. To address this gap, we propose an LLM-driven approach for the verification of ontology restrictions. We replicate our previously conducted human-in-the-loop experiment using ChatGPT-4 instead of human contributors to assess whether comparable ontology verification results can be obtained. We find that (1) ChatGPT-4 achieves intermediate-to-expert scores on an ontology modelling qualification test; (2) the model performs ontology restriction verification with accuracy of 92.22%; (3) combining model answers on the same ontology axiom represented in different formalisms improves the accuracy to 96.67%; and (4) higher accuracy is observed in identifying defects related to the incompleteness of ontology axioms compared to errors due to restrictions misuse. Our results highlight the potential of LLMs in supporting knowledge engineering tasks and outline future research directions in the area. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Defect Detection; Large Language Models; Ontology Evaluation; Engineering Research; Errors; Knowledge Graph; Modeling Languages; Ontology; Defect Detection; Engineering Tasks; Language Model; Large Language Model; Model-driven; Ontology Axioms; Ontology Evaluations; Ontology Graphs; Ontology's; Semantic Resources; Semantics},
	keywords = {Engineering research; Errors; Knowledge graph; Modeling languages; Ontology; Defect detection; Engineering tasks; Language model; Large language model; Model-driven; Ontology axioms; Ontology evaluations; Ontology graphs; Ontology's; Semantic resources; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Pisu202411,
	author = {Pisu, Alessia and Pompianu, Livio and Salatino, Angelo Antonio and Osborne, Francesco and Riboni, Daniele and Motta, Enrico and Reforgiato Recupero, Diego},
	title = {Leveraging Language Models for Generating Ontologies of Research Topics},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3747},
	pages = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203384989&partnerID=40&md5=be1194ae4f01daf079586c6797976a77},
	abstract = {The current generation of artificial intelligence technologies, such as smart search engines, recommendation systems, tools for systematic reviews, and question-answering applications, plays a crucial role in helping researchers manage and interpret scientific literature. Taxonomies and ontologies of research topics are a fundamental part of this environment as they allow intelligent systems and scientists to navigate the ever-growing number of research papers. However, creating these classifications manually is an expensive and time-consuming process, often resulting in outdated and coarse-grained representations. Consequently, researchers have been focusing on developing automated or semi-automated methods to create taxonomies of research topics. This paper studies the application of transformer-based language models for generating research topic ontologies. Specifically, we have developed a model leveraging SciBERT to identify four semantic relationships between research topics (supertopic, subtopic, same-as, and other) and conducted a comparative analysis against alternative solutions. The preliminary findings indicate that the transformer-based model significantly surpasses the performance of models reliant on traditional features. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Generation; Language Models; Ontology Generation; Research Topics; Scibert; Knowledge Graph; Ontology; Recommender Systems; Semantics; Taxonomies; Artificial Intelligence Technologies; Current Generation; Graph Generation; Knowledge Graph Generation; Knowledge Graphs; Language Model; Ontology Generation; Ontology's; Research Topics; Scibert; Question Answering},
	keywords = {Knowledge graph; Ontology; Recommender systems; Semantics; Taxonomies; Artificial intelligence technologies; Current generation; Graph generation; Knowledge graph generation; Knowledge graphs; Language model; Ontology generation; Ontology's; Research topics; SciBERT; Question answering},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Bertolini202412,
	author = {Bertolini, Lorenzo and Hulsman, Roel and Consoli, Sergio and Puertas Gallardo, Antonio and Ceresa, Mario},
	title = {On Constructing Biomedical Text-to-Graph Systems with Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3747},
	pages = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203342156&partnerID=40&md5=0c711da8e40443cb507f91dbd2c34fb2},
	abstract = {Knowledge graphs and ontologies represent symbolic and factual information that can offer structured and interpretable knowledge. Extracting and manipulating this type of information is a crucial step in complex processes such as human reasoning. While Large Language Models (LLMs) are known to be useful for extracting and enriching knowledge graphs and ontologies, previous work has largely focused on comparing architecture-specific models (e.g. encoder-decoder only) across benchmarks from similar domains. In this work, we provide a large-scale comparison of the performance of certain LLM features (e.g. model architecture and size) and task learning methods (fine-tuning vs. in-context learning (iCL)) on text-to-graph benchmarks in the biomedical domain. Our experiment suggests that, while a simple truncation-based heuristic can notably boost the performance of decoder-only models used with iCL, small fine-tuned encoder-decoder models produce the most stable and strong performance. Moreover, we found that a massive out-of-domain text-graph pre-training has a positive impact on fine-tuned models, while we observed only a marginal impact of pre-training and size for decoder-only iCL models. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {In-context Learning; Information Extraction; Knowledge Graphs; Large Language Models; Word Embeddings; Benchmarking; Contrastive Learning; Decoding; Graph Embeddings; Ontology; Context Learning; Embeddings; In Contexts; In-context Learning; Information Extraction; Knowledge Graphs; Language Model; Large Language Model; Performance; Word Embedding; Knowledge Graph},
	keywords = {Benchmarking; Contrastive Learning; Decoding; Graph embeddings; Ontology; Context learning; Embeddings; In contexts; In-context learning; Information extraction; Knowledge graphs; Language model; Large language model; Performance; Word embedding; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Regino202418,
	author = {Regino, Andre Gomes and César Dos Reis, Júlio},
	title = {Generating E-commerce Related Knowledge Graph from Text: Open Challenges and Early Results using LLMs},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3747},
	pages = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203334838&partnerID=40&md5=ac53dff8d619e6e1d33d20a00efdd161},
	abstract = {E-commerce systems need to use and manage vast amounts of unstructured textual data. This poses significant challenges for knowledge representation, information retrieval, and recommendation tasks. This study investigates the generation of E-commerce-related Knowledge Graphs (KGs) from text. In particular, we explore using Large Language Models (LLMs). Our approach integrates ontology with text-based examples from existing KGs via prompts to create structured RDF triples. We outline a four-step method encompassing text classification, extracting relevant characteristics, generating RDF triples, and assessing the generated triples. Each step leverages LLM instructions to process unstructured text. We discuss the insights, challenges, and potential future directions, highlighting the significance of integrating ontology elements with unstructured text for generating semantically enriched KGs. Through case experimentations, we demonstrate the effectiveness and applicability of our solution in the E-commerce domain. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Kg Enhancement; Knowledge Graphs; Large Language Models; Text-to-triple; Marketplaces; Ontology; Structured Query Language; E-commerce Systems; Knowledge Graph Enhancement; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Rdf Triples; Text-to-triple; Unstructured Texts; Knowledge Graph},
	keywords = {Marketplaces; Ontology; Structured Query Language; E-commerce systems; Knowledge graph enhancement; Knowledge graphs; Language model; Large language model; Ontology's; RDF triples; Text-to-triple; Unstructured texts; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Dobriy202418,
	author = {Dobriy, Daniil},
	title = {Employing RAG to Create a Conference Knowledge Graph from Text},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3747},
	pages = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203317673&partnerID=40&md5=16e9ffb1be06509864b372b91808c489},
	abstract = {In this paper, we present Semantic Observer, a platform that 1) defines a FAIR Conference Ontology for describing academic conferences, 2) presents an RAG architecture that constructs a Conference Knowledge Graph based on this ontology, 3) evaluates the architecture on a corpus of latest available CORE conference websites. The Conference Ontology models key entities such as conferences, workshops and challenges, organizer and programme committees, calls for papers and proposals as well as major deadlines and relevant topics. In the evaluation, we compare the performance of three leading Large Language Models: GPT-4 Turbo and Claude 3 Opus - in supporting the Knowledge Graph construction from text. The best-performing RAG architecture is then implemented in Semantic Observer and available in a SPARQL endpoint to make up-to-date conference information FAIR: findable, accessible, interoperable and reusable. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Ontology Engineering; Research Ecosystem; Retrieval-augmented Generation; Semantic Web; Web Crawling; Engineering Research; Interoperability; Latent Semantic Analysis; Ontology; Reusability; Semantics; Web Crawler; Academic Conferences; Graph-based; Knowledge Graphs; Ontology Engineering; Ontology Model; Ontology's; Research Ecosystem; Retrieval-augmented Generation; Semantic-web; Web Crawling; Knowledge Graph},
	keywords = {Engineering research; Interoperability; Latent semantic analysis; Ontology; Reusability; Semantics; Web crawler; Academic conferences; Graph-based; Knowledge graphs; Ontology engineering; Ontology model; Ontology's; Research ecosystem; Retrieval-augmented generation; Semantic-Web; Web Crawling; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Boadu2024,
	author = {Boadu, Frimpong and Cheng, Jianlin},
	title = {Improving protein function prediction by learning and integrating representations of protein sequences and function labels},
	year = {2024},
	journal = {Bioinformatics Advances},
	volume = {4},
	number = {1},
	pages = {},
	doi = {10.1093/bioadv/vbae120},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203282775&doi=10.1093%2Fbioadv%2Fvbae120&partnerID=40&md5=c4fa2ec328f6ed44ba3b2232ce2dc6c7},
	abstract = {Motivation: As fewer than 1% of proteins have protein function information determined experimentally, computationally predicting the function of proteins is critical for obtaining functional information for most proteins and has been a major challenge in protein bioinformatics. Despite the significant progress made in protein function prediction by the community in the last decade, the general accuracy of protein function prediction is still not high, particularly for rare function terms associated with few proteins in the protein function annotation database such as the UniProt. Results: We introduce TransFew, a new transformer model, to learn the representations of both protein sequences and function labels [Gene Ontology (GO) terms] to predict the function of proteins. TransFew leverages a large pre-trained protein language model (ESM2-t48) to learn function-relevant representations of proteins from raw protein sequences and uses a biological natural language model (BioBert) and a graph convolutional neural network-based autoencoder to generate semantic representations of GO terms from their textual definition and hierarchical relationships, which are combined together to predict protein function via the cross-attention. Integrating the protein sequence and label representations not only enhances overall function prediction accuracy, but delivers a robust performance of predicting rare function terms with limited annotations by facilitating annotation transfer between GO terms. Availability and implementation: https://github.com/BioinfoMachineLearning/TransFew. © 2024 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{2024,
	title = {11th International Work-Conference on Bioinformatics and Biomedical Engineering, IWBBIO 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14849 LNBI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203158031&partnerID=40&md5=5a886dd08535efc80d1e727ca508d604},
	abstract = {The proceedings contain 54 papers. The special focus in this conference is on Bioinformatics and Biomedical Engineering. The topics include: GPACDA – circRNA-Disease Association Prediction with Generating Polynomials; Bioinformatics Analysis Provides Insight into the Identification of miRNAs as Transcriptional Regulators in Respiratory Syncytial Virus Infection; risk Factors of Recurrence and Metastasis of Breast Cancer Sub-types Based on Magnetic Resonance Imaging Techniques; Analysis of a Parallel and Distributed BPSO Algorithm for EEG Classification: Impact on Energy, Time and Accuracy; Naïve Bayes for Health-Status Predictive Monitoring in COVID-19: Leveraging Drugs and Diagnoses; speech Analysis for Autism Spectrum Disorder Detection for Children; SiMHOMer: Siamese Models for Health Ontologies Merging and Validation Through Large Language Models; lossless Compression of Nanopore Sequencing Raw Signals; medical Equipment Real-Time Locating System in Hospitals Based on Bluetooth Low Energy; Validation of WHO Charts Mobile Applications for Body Length and Weight Assessment in Healthy Newborns; Predicting Atherosclerotic Plaque Onset and Growth in Carotid Arteries: A CFD-Driven Approach; hemispherical Directional Reflectance as a Screening Tool to Distinguish Effervescent Tablets with Acetylsalicylic Acid and Vitamin C Stored Under Stress Conditions from Those Stored in Ambient Conditions; enhanced Ergonomics in Laryngoscopic Surgery. Exploring Innovative Solutions; maximal Deadlift Strength and Bone Mass in a Group of Healthy Elderly Men; multilayer Networks: A Survey on Models, Analysis of Algorithms and Database; bone Mineral Density in Middle-Aged Former Sprinters and Middle-Aged Active Men; spectrum Filtering to Extract Pulse Rate Variability from Signals Recorded by Wearable Devices; machine Learning Model for Anxiety Disorder Diagnosis Based on Sensory Time-Series Data; analysis of the Relationship Between Electrodermal Activity and Blood Glucose Level in Diabetics. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Anderson202414,
	author = {Anderson, Paul E. and Lin, Damon and Davidson, Jean M. and Migler, Theresa Anne and Ho, Iris and Koenig, Cooper and Bittner, Madeline and Kaplan, Samuel and Paraiso, Mayumi and Buhn, Nasreen},
	title = {Bridging Domains in Chronic Lower Back Pain: Large Language Models and Ontology-Driven Strategies for Knowledge Graph Construction},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14849 LNBI},
	pages = {14 - 30},
	doi = {10.1007/978-3-031-64636-2_2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203121027&doi=10.1007%2F978-3-031-64636-2_2&partnerID=40&md5=effd146e14b12dc8ec8203184ae51a7d},
	abstract = {Link prediction and entity resolution play pivotal roles in uncovering hidden relationships within networks and ensuring data quality in the era of heterogeneous data integration. This paper explores the utilization of large language models to enhance link prediction, particularly through knowledge graphs derived from transdisciplinary literature. Investigating zero-shot entity resolution techniques, we examine the impact of ontology-based and large language model approaches on the stability of link prediction results. Through a case study focusing on chronic lower back pain research, we analyze workflow decisions and their influence on prediction outcomes. Our research underscores the importance of robust methodologies in improving predictive accuracy and data integration across diverse domains. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chronic Lower Back Pain; Entity Resolution; Knowledge Graphs; Link Prediction; Data Accuracy; Knowledge Graph; Prediction Models; Bridging Domains; Chronic Low Back Pain; Data Quality; Entity Resolutions; Graph Construction; Heterogeneous Data Integrations; Knowledge Graphs; Language Model; Link Prediction; Ontology's},
	keywords = {Data accuracy; Knowledge graph; Prediction models; Bridging domains; Chronic low back pain; Data quality; Entity resolutions; Graph construction; Heterogeneous data integrations; Knowledge graphs; Language model; Link prediction; Ontology's},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Magnini2024,
	author = {Magnini, Matteo and Ozaki, Ana and Squarcialupi, Riccardo},
	title = {Actively Learning Ontologies from LLMs: First Results (Extended Abstract)},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3739},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202829104&partnerID=40&md5=c17031ab39e0c1d51fce6e8892f975f6},
	abstract = {In active learning a learner attempts to acquire some kind of knowledge by posing questions to a teacher. Here we consider that the teacher is a language model and study the case in which the knowledge is expressed as an ontology. To evaluate the approach, we present first results testing logical consistency and the performance of GPT and other language models when answering whether concept inclusions from existing ℰℒ ontologies are ‘true’ or ‘false’. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Active Learning; Language Models; Ontologies; Adversarial Machine Learning; Contrastive Learning; Federated Learning; Reinforcement Learning; Active Learning; Extended Abstracts; Language Model; Language Study; Logical Consistency; Ontology's; Performance; Teachers'; Active Learning},
	keywords = {Adversarial machine learning; Contrastive Learning; Federated learning; Reinforcement learning; Active Learning; Extended abstracts; Language model; Language study; Logical consistency; Ontology's; Performance; Teachers'; Active learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Menad2024117,
	author = {Menad, Safaa and Abdeddaïm, Saïd and Soualmia, Lina F.},
	title = {SiMHOMer: Siamese Models for Health Ontologies Merging and Validation Through Large Language Models},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14848 LNBI},
	pages = {117 - 129},
	doi = {10.1007/978-3-031-64629-4_9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202640943&doi=10.1007%2F978-3-031-64629-4_9&partnerID=40&md5=de2865e3afc8b4c91c14e156b4378612},
	abstract = {Ontologies play a key role in representing and structuring domain knowledge. In the biomedical domain, the need for this type of representation is crucial for structuring, coding, and retrieving data. However, available ontologies do not encompass all the relevant concepts and relationships. In this paper, we propose the framework SiMHOMer (Siamese Modela for Health Ontologies Merging), to semantically merge and integrate the most relevant ontologies in the healthcare domain, including diseases, symptoms, drugs, and adverse events. We propose to rely on the siamese neural models we developed and trained on biomedical data, BioSTransformers, to identify new relevant relations between different concepts and to create new semantic relations, the objective being to build a new consistent merging ontology that specialists could use as a new resource for various health-related use cases. To validate the new relations, we have leveraged existing relations in the UMLS Metathesaurus and the Semantic Network. To evaluate our findings, a large language model is also used. Our first results show promising improvements for future research. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Ontologies; Large Language Models; Ontology Merging; Siamese Neural Models; Umls; Electronic Health Record; Biomedical Ontologies; Domain Knowledge; Language Model; Large Language Model; Neural Modelling; Ontology Merging; Ontology Validations; Ontology's; Siamese Neural Model; Umls; Semantics},
	keywords = {Electronic health record; Biomedical ontologies; Domain knowledge; Language model; Large language model; Neural modelling; Ontology merging; Ontology validations; Ontology's; Siamese neural model; UMLS; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{2024,
	title = {11th International Work-Conference on Bioinformatics and Biomedical Engineering, IWBBIO 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14848 LNBI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202600199&partnerID=40&md5=951b4755d4f8a5f78d8c21ce116d34a2},
	abstract = {The proceedings contain 54 papers. The special focus in this conference is on Bioinformatics and Biomedical Engineering. The topics include: GPACDA – circRNA-Disease Association Prediction with Generating Polynomials; Bioinformatics Analysis Provides Insight into the Identification of miRNAs as Transcriptional Regulators in Respiratory Syncytial Virus Infection; risk Factors of Recurrence and Metastasis of Breast Cancer Sub-types Based on Magnetic Resonance Imaging Techniques; Analysis of a Parallel and Distributed BPSO Algorithm for EEG Classification: Impact on Energy, Time and Accuracy; Naïve Bayes for Health-Status Predictive Monitoring in COVID-19: Leveraging Drugs and Diagnoses; speech Analysis for Autism Spectrum Disorder Detection for Children; SiMHOMer: Siamese Models for Health Ontologies Merging and Validation Through Large Language Models; lossless Compression of Nanopore Sequencing Raw Signals; medical Equipment Real-Time Locating System in Hospitals Based on Bluetooth Low Energy; Validation of WHO Charts Mobile Applications for Body Length and Weight Assessment in Healthy Newborns; Predicting Atherosclerotic Plaque Onset and Growth in Carotid Arteries: A CFD-Driven Approach; hemispherical Directional Reflectance as a Screening Tool to Distinguish Effervescent Tablets with Acetylsalicylic Acid and Vitamin C Stored Under Stress Conditions from Those Stored in Ambient Conditions; enhanced Ergonomics in Laryngoscopic Surgery. Exploring Innovative Solutions; maximal Deadlift Strength and Bone Mass in a Group of Healthy Elderly Men; multilayer Networks: A Survey on Models, Analysis of Algorithms and Database; bone Mineral Density in Middle-Aged Former Sprinters and Middle-Aged Active Men; spectrum Filtering to Extract Pulse Rate Variability from Signals Recorded by Wearable Devices; machine Learning Model for Anxiety Disorder Diagnosis Based on Sensory Time-Series Data; analysis of the Relationship Between Electrodermal Activity and Blood Glucose Level in Diabetics. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {16th Asian Conference on Intelligent Information and Database Systems , ACIIDS 2024},
	year = {2024},
	journal = {Communications in Computer and Information Science},
	volume = {2145 CCIS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202151787&partnerID=40&md5=a57b7fd285d282a8b11dd25c8f9caf3f},
	abstract = {The proceedings contain 58 papers. The special focus in this conference is on Intelligent Information and Database Systems. The topics include: A Deep Neural Networks Approach for Speaker Verification on Embedded Devices; phishing Detection Using Ensemble of Classifiers; an Effective Ensemble Classification Algorithm for Intrusion Detection System; mean-Square Consensus of Second-Order Multi-agent Systems with Semi-Markov Switching for Non-periodic DoS Attacks; an Attention-Driven Hybrid Network for Survival Analysis of Tumorigenesis Patients Using Whole Slide Images; a Deep Learning Approach to Diabetes Diagnosis; a Novel Approach for Medical E-Consent: Leveraging Language Models for Informed Consent Management; Multi-scale and Multi-level Attention Based on External Knowledge in EHRs; Exploring XAI Attention Maps to Investigate the Effect of Distance Metric and Lesion-Shaped Border Expansion Size for Effective Content-Based Dermatological Lesion Retrieval; d-MiQ: Deep Multimodal Interactive Healthcare Query Expansion Approach for Web Search Engines Retrieval Effectiveness; complexity of Transforming Decision Rule Systems into Decision Trees and Acyclic Decision Graphs; depth of Deterministic and Nondeterministic Decision Trees for Decision Tables with Many-Valued Decisions from Closed Classes; gradient Overdrive: Avoiding Negative Randomness Effects in Stochastic Gradient Descent; blockchain-Based Educational Certification Systems Using a Modified Hash Algorithm; the Merged Longest Common Increasing Subsequence Problem; applying Transformation to Reduce Model Sizes for Constrained Optimization Problems; multiple Traveling Salesman Problem with a Drone Station: Using Multi-package Payload Compartments; unveiling the Power of Hybrid Balancing Techniques and Ensemble Stacked and Blended Classifiers for Enhanced Churn Prediction; improvement of a Forward Reasoning Engine FreeEnCal for Trust Reasoning; a Structure Based on B+ Trees to Represent a Large Number of k-Multisets Stored in Non-volatile Memory; optimizing Option Short Strangle Strategies Through Genetic Algorithm; optimizing Airline Pilots Training Plans: A Mixed Integer Linear Programming Approach; towards an Ontological Approach for Verifying the Well-Formedness of Training Programs; penetration Testing and Security Assessment Methodology for Biomedical Devices. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Rula2024368,
	author = {Rula, Anisa and D’Souza, Jennifer},
	title = {Exploring Large Language Models for Procedural Extraction from Documents},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3741},
	pages = {368 - 376},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202037828&partnerID=40&md5=fdeabcbd1e2576ec5ecb063814560000},
	abstract = {Recent advancements in Natural Language Processing (NLP), notably the emergence of extensive language models pre-trained on vast datasets, are opening new avenues in Knowledge Engineering. This study delves into the utilization of these large language models (LLMs) in two learning scenarios - zero-shot and in-context learning - to address the extraction of procedures from unstructured PDF texts through incremental question-answering techniques. Specifically, we employ the cutting-edge GPT-4 (Generative Pre-trained Transformer 4) model, alongside two variations of in-context learning methodologies. These methods incorporate an ontology with definitions of procedures and steps, as well as a limited set of samples for few-shot learning. Our investigation underscores the potential of this approach and underscores the significance of tailored in-context learning adaptations. These adjustments hold promise in mitigating the challenge of acquiring adequate training data, a common obstacle in deep learning-based NLP methods for procedure extraction. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Capture; Knowledge Graphs; Large Language Models; Procedural Knowledge; Adversarial Machine Learning; Contrastive Learning; Deep Learning; Knowledge Graph; Large Datasets; Natural Language Processing Systems; Question Answering; Context Learning; In Contexts; Knowledge Capture; Knowledge Graphs; Language Model; Language Processing; Large Language Model; Learning Scenarios; Natural Languages; Procedural Knowledge; Zero-shot Learning},
	keywords = {Adversarial machine learning; Contrastive Learning; Deep learning; Knowledge graph; Large datasets; Natural language processing systems; Question answering; Context learning; In contexts; Knowledge capture; Knowledge graphs; Language model; Language processing; Large language model; Learning scenarios; Natural languages; Procedural knowledge; Zero-shot learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Cavalleri202461,
	author = {Cavalleri, Emanuele and Soto Gomez, Mauricio A. and Pashaeibarough, Ali and Malchiodi, Dario and Caufield, John Harry and Reese, Justin T. and Mungall, Christopher John and Robinson, Peter Nicholas and Casiraghi, Elena and Valentini, Giorgio},
	title = {Initial achievements in relation extraction from RNA-focused scientific papers},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3741},
	pages = {61 - 69},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85202018471&partnerID=40&md5=903a15b130d3508fdc1210b503158de3},
	abstract = {Relation extraction from the scientific literature to comply with a domain ontology is a well-known problem in natural language processing and is particularly critical in precision medicine. The advent of large language models (LLMs) has paved the way for the development of new effective approaches to this problem, but the extracted relations can be affected by issues such as hallucination, which must be minimized. In this paper, we present the initial design and preliminary experimental validation of SPIREX, an extension of the SPIRES-based system for the extraction of RDF triples from scientific literature involving RNA molecules. Our system exploits schema constraints in the formulations of LLM prompts along with our RNA-based KG, RNA-KG, for evaluating the plausibility of the extracted triples. RNA-KG contains more than 9M edges representing different kinds of relationships in which RNA molecules can be involved. Initial experimental results on a controlled data set are quite encouraging. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Link Prediction; Llm; Prompt Engineering; Relation Discovery; Rna-based Knowledge Graphs; Natural Language Processing Systems; Ontology; Knowledge Graphs; Language Model; Large Language Model; Link Prediction; Prompt Engineering; Relation Discovery; Relation Extraction; Rna Molecules; Rna-based Knowledge Graph; Scientific Literature; Knowledge Graph},
	keywords = {Natural language processing systems; Ontology; Knowledge graphs; Language model; Large language model; Link prediction; Prompt engineering; Relation discovery; Relation extraction; RNA molecules; RNA-based knowledge graph; Scientific literature; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {1st International Conference on Artificial Intelligence on Healthcare, AIiH 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14975 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201982548&partnerID=40&md5=bfb8211403ff17f25f0f50aaa03effa0},
	abstract = {The proceedings contain 47 papers. The special focus in this conference is on Artificial Intelligence on Healthcare. The topics include: Augmenting Infrequent Relationships in Clinical Language Models with Graph-Encoded Hierarchical Ontologies; identifying Clusters on Multiple Long-Term Conditions for Adults with Learning Disabilities; interpreting Pretrained Speech Models for Automatic Speech Assessment of Voice Disorders; word or Phoneme? To Optimise Prosodic Features to Predict Lung Function with Helicopter Task; electrical Impedance Spectroscopy Based Preterm Birth Prediction with Machine Learning; transfer Learning in Hypoglycemia Classification; a Comparative Analysis of Eleven Augmentation Techniques for Enhanced Retinal Pathology Recognition; multi-stage Chronic Kidney Disease Classification on Longitudinal Data; contrastive Multitask Transformer for Hospital Mortality and Length-of-Stay Prediction; development of Life Support Devices by Using Inclusive Design; design and Operation Requirements for an Ankle Assisting Device; promoting Healthy Eating Habits via Intelligent Virtual Assistants, Improving Monitoring by Nutritional Specialists: State of the Art; Exploration of AI-Enhanced Wearable Devices for Advanced Cardiovascular Monitoring in the Elderly; development of a Control Algorithm for an Ankle Joint Rehabilitation Device; towards Quantification of Eye Contacts Between Trainee Doctors and Simulated Patients in Consultation Videos; laboratory Experiences with an Intelligent Robotic Crank for Arm Exercises; ADALINE Neurons Used for Targeting Performance on the Deep Brain Stimulation Platform; Evaluating the Feasibility and Acceptability of a GPT-Based Chatbot for Depression Screening: A Mixed-Methods Study; structural Brain Network Generation via Brain Denoising Diffusion Probabilistic Model; conversation Analysis of Remote Dialogue System for Mental Health Interventions; unveiling Disparities in Maternity Care: A Topic Modelling Approach to Analysing Maternity Incident Investigation Reports. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ramchand202431,
	author = {Ramchand, Suraj and Xie, Xianghua},
	title = {Augmenting Infrequent Relationships in Clinical Language Models with Graph-Encoded Hierarchical Ontologies},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14975 LNCS},
	pages = {31 - 44},
	doi = {10.1007/978-3-031-67278-1_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201980956&doi=10.1007%2F978-3-031-67278-1_3&partnerID=40&md5=3a7af12aa81a95b9a6413b7483c9178b},
	abstract = {Harnessing primary-care data can facilitate earlier clinical interventions via predictive modelling. Nonetheless, the intricacy of medical terminology and the breadth of ontological data often obscure the inner workings of such models. Despite the growing complexity of artificial intelligence methodologies and the pressing demand for medical tools that seamlessly integrate into clinical workflows, this opacity persists. We propose enhancing clinical Bidirectional Encoder Representations from Transformers (BERT) models with graph attention networks that encode diagnosis and medication concept hierarchies derived from primary care data. In 10-fold cross-validation on cardiovascular and respiratory detection tasks, our graph-enhanced model marginally improves F1 performance over baseline BERT. More importantly, our approach surfaces clinically deterministic patterns in patient groups, provides modular visualisations of influential terminal and ancestral medical concepts, and improves clustering of related conditions. Additionally, the hierarchical encoding allows quantitative analysis of edge relevance within and across diagnosis and medical ontologies. Our research shows that injecting structured knowledge graphs into language model architectures can improve performance through domain-specific regularisation. Additionally, the use of class activation maps throughout the approach allows for richer interpretations of predictions by following activation flows along concept relationships. The dual utility of precise ontology encoding and Large Language Models makes our graph-injected clinical language model more accurate and trustworthy, propelling preventive precision medicine forward. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Electronic Health Records; Graph Neural Networks; Large Language Models; Medical Ontology; Clinical Research; Diagnosis; Electronic Health Record; Ontology; Clinical Interventions; Electronic Health; Graph Neural Networks; Health Records; Language Model; Large Language Model; Medical Ontology; Ontology's; Predictive Models; Primary Care},
	keywords = {Clinical research; Diagnosis; Electronic health record; Ontology; Clinical interventions; Electronic health; Graph neural networks; Health records; Language model; Large language model; Medical ontology; Ontology's; Predictive models; Primary care},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {16th Asian Conference on Intelligent Information and Database Systems , ACIIDS 2024},
	year = {2024},
	journal = {Communications in Computer and Information Science},
	volume = {2144 CCIS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201968307&partnerID=40&md5=1f272029380e0da84a0e046159b4a562},
	abstract = {The proceedings contain 58 papers. The special focus in this conference is on Intelligent Information and Database Systems. The topics include: A Deep Neural Networks Approach for Speaker Verification on Embedded Devices; phishing Detection Using Ensemble of Classifiers; an Effective Ensemble Classification Algorithm for Intrusion Detection System; mean-Square Consensus of Second-Order Multi-agent Systems with Semi-Markov Switching for Non-periodic DoS Attacks; an Attention-Driven Hybrid Network for Survival Analysis of Tumorigenesis Patients Using Whole Slide Images; a Deep Learning Approach to Diabetes Diagnosis; a Novel Approach for Medical E-Consent: Leveraging Language Models for Informed Consent Management; Multi-scale and Multi-level Attention Based on External Knowledge in EHRs; Exploring XAI Attention Maps to Investigate the Effect of Distance Metric and Lesion-Shaped Border Expansion Size for Effective Content-Based Dermatological Lesion Retrieval; d-MiQ: Deep Multimodal Interactive Healthcare Query Expansion Approach for Web Search Engines Retrieval Effectiveness; complexity of Transforming Decision Rule Systems into Decision Trees and Acyclic Decision Graphs; depth of Deterministic and Nondeterministic Decision Trees for Decision Tables with Many-Valued Decisions from Closed Classes; gradient Overdrive: Avoiding Negative Randomness Effects in Stochastic Gradient Descent; blockchain-Based Educational Certification Systems Using a Modified Hash Algorithm; the Merged Longest Common Increasing Subsequence Problem; applying Transformation to Reduce Model Sizes for Constrained Optimization Problems; multiple Traveling Salesman Problem with a Drone Station: Using Multi-package Payload Compartments; unveiling the Power of Hybrid Balancing Techniques and Ensemble Stacked and Blended Classifiers for Enhanced Churn Prediction; improvement of a Forward Reasoning Engine FreeEnCal for Trust Reasoning; a Structure Based on B+ Trees to Represent a Large Number of k-Multisets Stored in Non-volatile Memory; optimizing Option Short Strangle Strategies Through Genetic Algorithm; optimizing Airline Pilots Training Plans: A Mixed Integer Linear Programming Approach; towards an Ontological Approach for Verifying the Well-Formedness of Training Programs; penetration Testing and Security Assessment Methodology for Biomedical Devices. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Naji2024100,
	author = {Naji, Mouncef and Masmoudi, Maroua and Baazaoui-Zghal, Hajer},
	title = {A Novel Approach for Medical E-Consent: Leveraging Language Models for Informed Consent Management},
	year = {2024},
	journal = {Communications in Computer and Information Science},
	volume = {2144 CCIS},
	pages = {100 - 112},
	doi = {10.1007/978-981-97-5937-8_9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201949912&doi=10.1007%2F978-981-97-5937-8_9&partnerID=40&md5=4eaf6b1041987875f78ef3212180f281},
	abstract = {In the context of healthcare, the issue of informed and voluntary consent stands a matter of paramount concern. Despite its stringent regulation within the medical field, the process of obtaining informed consent is frequently hindered by systemic, clinician-related, and patient-related factors, necessitating interventions at various levels. Notably, studies have shown that these factors often result in uninformed decisions, particularly in the context of hospitalization or intervention. This paper introduces a novel approach for enhancing the medical e-consent process by leveraging Large Language Models (LLMs) and knowledge graphs. The objective is to provide support during the consent process. Our proposal deal with 1) legal validation of consent documents for content clarity and comprehension verification; 2) personalization of content and interactions based on patient preferences and medical history; and 3) semantic reasoning integration into healthcare system information using knowledge graphs and ontologies. The overarching architectural objective of our proposal is to ensure a well-informed, adaptable, and legally valid consent process. Scenarios explaining the process based on data provided by our collaborators, is also detailed. The results show an improvement in the process and confirm the interest of the proposed LLM for informed consent management. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {E-consent; Healthcare Information Systems; Knowledge Graphs; Large Language Models; Electronic Health Record; Modeling Languages; Consent Managements; E-consent; Healthcare Information System; Knowledge Graphs; Language Model; Large Language Model; Medical Fields; Personalizations; Related Factors; Stringent Regulations; Knowledge Graph},
	keywords = {Electronic health record; Modeling languages; Consent managements; E-consent; Healthcare information system; Knowledge graphs; Language model; Large language model; Medical fields; Personalizations; Related factors; Stringent regulations; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liu2024444,
	author = {Liu, Bingqian and Wu, Xinxin and Pan, Deng and Chen, Yanyu and Huang, Jianye and Liao, Feilong and Lin, Shuang},
	title = {Enhancing Large Language Models with Graph-Based Node Sampling for Fault Attribution in Power Distribution Networks},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14874 LNCS},
	pages = {444 - 455},
	doi = {10.1007/978-981-97-5618-6_37},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201126619&doi=10.1007%2F978-981-97-5618-6_37&partnerID=40&md5=995d12035f4871097a714dbf5508222d},
	abstract = {The increasing complexity of power distribution networks necessitates advanced methods for fault attribution analysis to uphold system reliability and stability. This paper introduces a novel approach that integrates large language models (LLMs) with domain-specific knowledge graphs to tackle the challenges posed by high-dimensional and intricate fault data in power distribution networks. A multi-dimensional fault ontology [13] is proposed to efficiently structure the fault data, facilitating the creation of a comprehensive fault knowledge graph. To enhance the diagnostic predictions of the LLM, a reinforcement learning-based node selection algorithm is implemented, strategically choosing pertinent nodes from the fault graph to enhance the model’s reasoning abilities. Experimental findings illustrate that this approach surpasses traditional statistical methods and direct LLM reasoning, achieving superior accuracy and efficiency in fault diagnosis. By incorporating selective knowledge graph node sampling, irrelevant noise is filtered out from the fault data, sharpening the LLM’s focus and eradicating “AI hallucinations,” thereby improving analytical precision. Validation on a real-world dataset from a power company confirms the method’s efficacy, promising swift and accurate fault analysis while reducing the time required for power grid fault diagnosis. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Fault Attribution Analysis; Knowledge Graph; Large Language Models; Computational Linguistics; Domain Knowledge; Electric Fault Currents; Electric Network Analysis; Electric Power Distribution; Electric Utilities; Fault Detection; Graph Theory; Graphic Methods; Knowledge Graph; Reinforcement Learning; Reliability Analysis; Sentiment Analysis; Fault Attribution Analyse; Fault Data; Faults Diagnosis; Graph-based; Knowledge Graphs; Language Model; Large Language Model; Node Sampling; Power Distribution Network; System's Stabilities; Failure Analysis},
	keywords = {Computational linguistics; Domain Knowledge; Electric fault currents; Electric network analysis; Electric power distribution; Electric utilities; Fault detection; Graph theory; Graphic methods; Knowledge graph; Reinforcement learning; Reliability analysis; Sentiment analysis; Fault attribution analyse; Fault data; Faults diagnosis; Graph-based; Knowledge graphs; Language model; Large language model; Node sampling; Power distribution network; System's stabilities; Failure analysis},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Mazumder2024464,
	author = {Mazumder, Sourav and Banipal, Indervir Singh and Asthana, Shubhi and Zhang, Bing},
	title = {Label Engineering Methods for ML Systems},
	year = {2024},
	journal = {Lecture Notes in Networks and Systems},
	volume = {1068 LNNS},
	pages = {464 - 474},
	doi = {10.1007/978-3-031-66336-9_33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201101387&doi=10.1007%2F978-3-031-66336-9_33&partnerID=40&md5=1eaf31b2bce6564e49eaf695e4efef63},
	abstract = {The goal of label engineering is to generate labels that accurately reflect the intended meaning or purpose of the machine learning model, while also being useful and relevant for the downstream artificial intelligent systems. These range from generating labels for computer vision use cases, natural language tasks, language models, and other custom enterprise models. Label engineering is becoming a hot topic when it comes to training intelligent systems with initial ground truth data and iterating on existing models for improvement, for large-scale business use cases. The processes for generating ground truth data through labeling and natural language annotations can be very cumbersome, because very few standardization approaches have been researched on. In this paper, we review the existing literature and present the challenges in label engineering. We also propose methodologies that can help generate trustworthy and reliable labels through well-governed and regulated processes and pipelines. It is important to understand the benefits and drawbacks of different approaches and choose which best suits our use cases. After understanding these challenges and choosing what approach to take during the architecture phase, it becomes important to have explainability for the model. In case of unexpected outcomes, the model can be traced back to the set of labels that led to the model training in a direction leading to the problematic classification. We conclude the paper with initial experiments and recommendations. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Intelligent Systems; Label Engineering; Natural Language Processing; Ontology Engineering; Software Engineering; Learning Algorithms; Natural Language Processing Systems; Ontology; Software Engineering; Artificial Intelligent; Down-stream; Engineering Methods; Ground Truth Data; Label Engineering; Language Processing; Machine Learning Models; Natural Language Processing; Natural Languages; Ontology Engineering; Intelligent Systems},
	keywords = {Learning algorithms; Natural language processing systems; Ontology; Software engineering; Artificial intelligent; Down-stream; Engineering methods; Ground truth data; Label engineering; Language processing; Machine learning models; Natural language processing; Natural languages; Ontology engineering; Intelligent systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kalisz2024447,
	author = {Kalisz, Vít and Kalisz, Adam},
	title = {Prompt Engineering for Domain-Oriented AI Support Tools: Ontologies, Mind Maps, Namespaces, Source Code Fragments},
	year = {2024},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {1198 LNEE},
	pages = {447 - 482},
	doi = {10.1007/978-3-031-61221-3_22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201017640&doi=10.1007%2F978-3-031-61221-3_22&partnerID=40&md5=3dde669c6d90523827828f3386838acb},
	abstract = {Chat AI systems, recently popularised by ChatGPT, allow interactions by exchanging linear text messages. Graph-like structures introduced by Z.Hedrlín represent and transfer information better than the classical linear form. We examine the possibility of using these structures to improve communication with these AI systems. We show that it is possible to create a simple graph-like structure about a topic that better captures and transfers understanding of the AI system. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chat Ai; Chatgpt; Concept Maps; Diagrams; Free Structuring; Google Bard; Graph-like Structures; Graphs; Knowledge Trees; Llm; Mind Maps; Mind Maps; Non-linear Form; Non-linear Structuring; Orgpad; Tony Buzan; Web Application; Chat Ai; Chatgpt; Concept Maps; Diagram; Free Structuring; Google Bard; Google+; Graph; Graph-like Structures; Knowledge Tree; Llm; Mind Maps; Non Linear; Non-linear Form; Non-linear Structuring; Orgpad; Tony Buzan; Web Application; Web Applications; Trees (mathematics)},
	keywords = {Chat AI; ChatGPT; Concept maps; Diagram; Free structuring; Google bard; Google+; Graph; Graph-like structures; Knowledge tree; LLM; Mind maps; Non linear; Non-linear form; Non-linear structuring; Orgpad; Tony buzan; WEB application; Web applications; Trees (mathematics)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{An202418,
	author = {An, Yuan and Greenberg, Jane and Uribe-Romo, Fernando J. and Gómez-Gualdrón, Diego Armando and Langlois, Kyle R. and Furst, Jacob I. and Kalinowski, Alexander and Zhao, Xintong and Hu, Xiaohua},
	title = {Knowledge Graph Question Answering for Materials Science (KGQA4MAT)},
	year = {2024},
	journal = {Communications in Computer and Information Science},
	volume = {2048 CCIS},
	pages = {18 - 29},
	doi = {10.1007/978-3-031-65990-4_2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200974884&doi=10.1007%2F978-3-031-65990-4_2&partnerID=40&md5=6d9b2462c53e30da765cabfee7a67ebb},
	abstract = {We present a study on Knowledge Graph Question Answering in Materials Science (KGQA4MAT), with a focus on metal-organic frameworks (MOFs). A knowledge graph for metal-organic frameworks (MOF-KG) has been constructed by integrating structured data, metadata, and knowledge extracted from the literature. We aim to develop a natural language (NL) interface for domain expert to query the MOF-KG. A first step is our benchmark, which consists of 161 complex questions involving comparison, aggregation, and intricate graph structures. Each question has been rephrased into three additional variations, totaling 644 questions and 161 KG queries. We then developed a systematic approach for utilizing ChatGPT to translate natural language questions into formal KG queries. We experimented with different prompt strategies. The research indicated that using an ontology, providing a few-shot examples, and offering a chain-of-thought explanation resulted in the top F1-score of 0.89. We also applied this method to the well-known QALD-9 dataset, achieving performance on par with the state-of-the-art techniques. The results indicate applicability of this model for MOF research and potentially other scientific foci. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Question Answering; Metal-organic Frameworks; Natural Language Interface; Pre-trained Large Language Models; Graphic Methods; Natural Language Processing Systems; Organometallics; Query Processing; Knowledge Graph Question Answering; Knowledge Graphs; Language Model; Material Science; Metalorganic Frameworks (mofs); Natural Language Interfaces; Pre-trained Large Language Model; Question Answering; Structured Data; Structured Knowledge; Knowledge Graph},
	keywords = {Graphic methods; Natural language processing systems; Organometallics; Query processing; Knowledge graph question answering; Knowledge graphs; Language model; Material science; Metalorganic frameworks (MOFs); Natural language interfaces; Pre-trained large language model; Question Answering; Structured data; Structured knowledge; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Li202457,
	author = {Li, Gen and Tang, Cheng and Chen, Li and Deguchi, Daisuke and Yamashita, Takayoshi and Shimada, Atsushi},
	title = {LLM-Driven Ontology Learning to Augment Student Performance Analysis in Higher Education},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14886 LNAI},
	pages = {57 - 68},
	doi = {10.1007/978-981-97-5498-4_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200754108&doi=10.1007%2F978-981-97-5498-4_5&partnerID=40&md5=b38d1eef02307126f549df7cd2a0491c},
	abstract = {In educational settings, a challenge is the lack of linked and labeled data, hindering effective analysis. The integration of ontology facilitates the formulation of educational knowledge concepts, student behaviors, and their relations. Traditional ontology creation requires deep domain knowledge and significant manual effort. However, advancements in Large Language Models (LLMs) have offered a novel opportunity to automate and refine this process. In this paper, we propose an LLMs-driven educational ontology learning approach aimed to enhance student performance predictions. We leverage LLMs to process lecture slide texts to identify knowledge concepts and their interrelations, while question texts are used to associate them with the concepts they assess. This process facilitates the generation of the educational ontology that links knowledge concepts and maps to student interactions. Additionally, we deploy a dual-branch Graph Neural Network (GNN) with distance-weighted pooling to analyze both global and local graph information for student performance prediction. Our empirical results demonstrate the effectiveness of using LLMs for ontology-based enhancements in educational settings. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Educational Data Mining; Gnn; Knowledge Graph; Llm; Ontology; Student Performance Prediction; Domain Knowledge; Education Computing; Forecasting; Graph Neural Networks; Knowledge Graph; Knowledge Management; Ontology; Students; Educational Data Mining; Knowledge Graphs; Language Model; Large Language Model; Model-driven; Ontology's; Performance Prediction; Student Performance; Student Performance Prediction; Data Mining},
	keywords = {Domain Knowledge; Education computing; Forecasting; Graph neural networks; Knowledge graph; Knowledge management; Ontology; Students; Educational data mining; Knowledge graphs; Language model; Large language model; Model-driven; Ontology's; Performance prediction; Student performance; Student performance prediction; Data mining},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Koohborfardhaghighi2024156,
	author = {Koohborfardhaghighi, Somayeh and de Geyter, Gert and Kaliner, Evan},
	title = {Unlocking the Power of LLM-Based Question Answering Systems: Enhancing Reasoning, Insight, and Automation with Knowledge Graphs},
	year = {2024},
	journal = {Lecture Notes in Networks and Systems},
	volume = {1052 LNNS},
	pages = {156 - 171},
	doi = {10.1007/978-3-031-64776-5_16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200688445&doi=10.1007%2F978-3-031-64776-5_16&partnerID=40&md5=07ede2b34c7dcba6ebaeb5f3101f9593},
	abstract = {In today’s data-driven business landscape, Knowledge Graphs can be effectively layered on top of relational databases and ontologies, a powerful combination for transforming how businesses tackle complex queries and decision-making processes. In this paper, we present a series of experiments that demonstrate the opportunities and advantages of blending knowledge graphs with Large Language Models (LLMs) through a practical use case. Our experimental results provide insights into the reasoning capabilities of LLMs when utilizing Knowledge Graph-Prompting. Furthermore, we observed the significance of maintaining uniformity in the language employed during knowledge graph construction to ensure precise responses from LLMs when querying the knowledge graph. This consistency also resonates in the embedding space of the model, where elements like relationship types are reflected in the resulting embeddings. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Intelligent Systems; Knowledge Graphs; Large Language Models (llms); Ontologies; Blending; Computational Linguistics; Decision Making; Graph Embeddings; Graphic Methods; Knowledge Graph; Metadata; Ontology; Data Driven; Embeddings; Knowledge Graphs; Language Model; Large Language Model; Model-based Opc; Ontology's; Power; Question Answering Systems; Relational Database; Intelligent Systems},
	keywords = {Blending; Computational linguistics; Decision making; Graph embeddings; Graphic methods; Knowledge graph; Metadata; Ontology; Data driven; Embeddings; Knowledge graphs; Language model; Large language model; Model-based OPC; Ontology's; Power; Question answering systems; Relational Database; Intelligent systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Chepurova202461,
	author = {Chepurova, Alla and Kuratov, Yuri M. and Bulatov, Aydar and Burtsev, Mikhail S.},
	title = {Prompt Me One More Time: A Two-Step Knowledge Extraction Pipeline with Ontology-Based Verification},
	year = {2024},
	pages = {61 - 77},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200615475&partnerID=40&md5=0c13b89310c5035073d1803f66af5f52},
	abstract = {This study explores a method for extending real-world knowledge graphs (specifically, Wikidata) by extracting triplets from texts with the aid of Large Language Models (LLMs). We propose a two-step pipeline that includes the initial extraction of entity candidates, followed by their refinement and linkage to the canonical entities and relations of the knowledge graph. Finally, we utilize Wikidata relation constraints to select only verified triplets. We compare our approach to a model that was fine-tuned on a machine-generated dataset and demonstrate that it performs better on natural data. Our results suggest that LLM-based triplet extraction from texts, with subsequent verification, is a viable method for real-world applications. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Ontology; Knowledge Extraction; Knowledge Graphs; Language Model; Model-based Opc; Ontology-based; Real-world; World Knowledge; Knowledge Graph},
	keywords = {Computational linguistics; Ontology; Knowledge extraction; Knowledge graphs; Language model; Model-based OPC; Ontology-based; Real-world; World knowledge; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Cavalleri20245639,
	author = {Cavalleri, Emanuele and Mesiti, Marco},
	title = {Construction and Enhancement of an RNA-Based Knowledge Graph for Discovering New RNA Drugs},
	year = {2024},
	journal = {Proceedings - International Conference on Data Engineering},
	pages = {5639 - 5643},
	doi = {10.1109/ICDE60146.2024.00453},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200506979&doi=10.1109%2FICDE60146.2024.00453&partnerID=40&md5=6f0b57b06f9d152f71c1d5910f4067e6},
	abstract = {Cutting-edge technologies in RNA biology are pushing the study of fundamental biological processes and human diseases and accelerate the development of new drugs tailored to the patient's biomolecular characteristics. Even if many structured and unstructured data sources report the interaction among different RNA molecules and some other biomedical entities (e.g., drugs, diseases, genes), we still lack a comprehensive and well-described RNA-centered Knowledge Graph (KG) that contains such information and sophisticated services that support the user in its creation, maintenance, and enhancement. This PhD project aims to create a biomedical KG (named RNA-KG) to represent, and eventually infer, biological, experimentally validated interactions between different RNA molecules. We also wish to enhance the KG content and develop sophisticated services designed ad-hoc to support the user in predicting uncovered relationships and identifying new RNA-based drugs. Services will rely on deep learning methods that consider the heterogeneity of the graph and the presence of an ontology that describes the possible relationships existing among the involved entities. Moreover, we will consider Large Language Models (LLMs) in combination with RNA-KG for interacting with the user with the ground truth information contained in our KG for extracting relationships from unstructured data sources. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Knowledge Graphs; Graph Representation Learning; Llms; Rna Therapeutics; Bioinformatics; Deep Learning; Drug Interactions; Knowledge Graph; Learning Systems; Biomedical Knowledge Graph; Data-source; Graph Representation; Graph Representation Learning; Knowledge Graphs; Language Model; Large Language Model; Rna Molecules; Rna Therapeutics; Unstructured Data; Rna},
	keywords = {Bioinformatics; Deep learning; Drug interactions; Knowledge graph; Learning systems; Biomedical knowledge graph; Data-source; Graph representation; Graph representation learning; Knowledge graphs; Language model; Large language model; RNA molecules; RNA therapeutics; Unstructured data; RNA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Kahlawi20249,
	author = {Kahlawi, Adham and Martelli, Cristina},
	title = {Enhancing Administrative Source Registers for the Development of a Robust Large Language Model: A Novel Methodological Approach},
	year = {2024},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {15},
	number = {7},
	pages = {9 - 17},
	doi = {10.14569/IJACSA.2024.0150702},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200333718&doi=10.14569%2FIJACSA.2024.0150702&partnerID=40&md5=c05edaa24618ace65f3b11d69ccd4fd9},
	abstract = {Accurate statistical information is critical for understanding, describing, and managing socio-economic systems. While data availability has increased, often it does not meet the quality requirements for effective governance. Administrative registers are crucial for statistical information production, but their potential is hampered by quality issues stemming from administrative inconsistencies. This paper explores the integration of semantic technologies, including ontologies and knowledge graphs, with administrative databases to improve data quality. We discuss the development of large language models (LLMs) that enable a robust, queryable framework, facilitating the integration of disparate data sources. This approach ensures high-quality administrative data, essential for statistical reuse and the development of comprehensive, dynamic knowledge graphs and LLMs tailored for administrative applications. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Administrative Data Reuse; Database; Knowledge Graph; Llm; Ontology; Semantic Web; Statistical Information Systems; Computational Linguistics; Data Integration; Knowledge Graph; Ontology; Statistics; Administrative Data Reuse; Data Reuse; Knowledge Graphs; Language Model; Large Language Model; Methodological Approach; Ontology's; Semantic-web; Statistical Information; Statistical Information System; Semantic Web},
	keywords = {Computational linguistics; Data integration; Knowledge graph; Ontology; Statistics; Administrative data reuse; Data reuse; Knowledge graphs; Language model; Large language model; Methodological approach; Ontology's; Semantic-Web; Statistical information; Statistical information system; Semantic Web},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Bertini2024,
	author = {Bertini, Flavio and Dal Palú, Alessandro and Fabiano, Francesco and Formisano, Andrea and Zaglio, Federica},
	title = {Concept2Text: an explainable multilingual rewriting of concepts into natural language},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3733},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200126703&partnerID=40&md5=e09884c42ba89e9a77cf5bd1e91aab98},
	abstract = {Automated and explainable data interpretation hinges on two critical steps: (i) identifying emerging properties from data and representing them into abstract concepts, and (ii) translating such concepts into natural language. While Large Language Models have recently demonstrated impressive capabilities in generating natural language, their trustworthiness remains difficult to ascertain. The deployment of an explainable pipeline enables its application in high-risk activities, such as decision making. Addressing this demanding requirement is facilitated by the fertile ground of knowledge representation and automated reasoning research. Building upon previous work that explored the first step, we focus on the second step, named Concept2Text. The design of an explainable translation naturally lends itself to a logic-based model, once again highlighting the contribution of declarative programming to achieving explainability in AI. This paper explores a Prolog/CLP-based rewriting system designed to interpret concepts expressed in terms of classes and relations derived from a generic ontology, generating text in natural language. Its key features encompass hierarchical tree rewritings, modular multilingual generation, support for equivalent variants across semantic, grammar, and lexical levels, and a transparent rule-based system. We present the architecture and illustrate a simple working example that allows the generation of hundreds of different and equivalent rewritings relative to the input concept. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept-to-text; Explainable Ai; Natural Language; Prolog; Abstracting; Decision Making; Logic Programming; Program Translators; Prolog (programming Language); Semantics; Translation (languages); Abstract Concept; Concept-to-text; Critical Steps; Data Interpretation; Explainable Ai; Its Applications; Language Model; Natural Languages; Prolog; Property; Knowledge Representation},
	keywords = {Abstracting; Decision making; Logic programming; Program translators; PROLOG (programming language); Semantics; Translation (languages); Abstract concept; Concept-to-text; Critical steps; Data interpretation; Explainable AI; ITS applications; Language model; Natural languages; Prolog; Property; Knowledge representation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Arrotta202455,
	author = {Arrotta, Luca and Bettini, Claudio and Civitarese, Gabriele and Fiori, Michele},
	title = {ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models},
	year = {2024},
	pages = {55 - 62},
	doi = {10.1109/SMARTCOMP61445.2024.00029},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199931500&doi=10.1109%2FSMARTCOMP61445.2024.00029&partnerID=40&md5=309392cfa6fca5f257e6a14d9246c2da},
	abstract = {Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human activities. In this work, we propose ContextGPT: a novel prompt engineering approach to retrieve from LLMs common-sense knowledge about the relationship between human activities and the context in which they are performed. Unlike ontologies, ContextGPT requires limited human effort and expertise, while sharing similar privacy concerns if the reasoning is performed in the cloud. An extensive evaluation using two public datasets shows how a NeSy model obtained by infusing common-sense knowledge from ContextGPT is effective in data scarcity scenarios, leading to similar (and sometimes better) recognition rates than logic-based approaches with a fraction of the effort. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Context-awareness; Human Activity Recognition; Large Language Models; Computational Linguistics; Learning Systems; Ontology; Pattern Recognition; Professional Aspects; Activity Recognition; Commonsense Knowledge; Context- Awareness; Context-aware; Human Activities; Human Activity Recognition; Language Model; Large Language Model; Model Knowledge; Ontology's; Deep Learning},
	keywords = {Computational linguistics; Learning systems; Ontology; Pattern recognition; Professional aspects; Activity recognition; Commonsense knowledge; Context- awareness; Context-Aware; Human activities; Human activity recognition; Language model; Large language model; Model knowledge; Ontology's; Deep learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Karim20245576,
	author = {Karim, Md Rezaul and Comet, Lina Molinas and Shajalal, Md and Beyan, Oya Deniz and Rebholz-Schuhmann, Dietrich and Decker, Stefan},
	title = {From Large Language Models to Knowledge Graphs for Biomarker Discovery},
	year = {2024},
	journal = {Proceedings of the Annual Hawaii International Conference on System Sciences},
	pages = {5576 - 5586},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199807340&partnerID=40&md5=ca150bc53d799919a51139e308fb8bb6},
	abstract = {Domain experts often rely on most recent knowledge for apprehending and disseminating specific biological processes that help them design strategies for developing prevention and therapeutic decision-making in various disease scenarios. A challenging scenarios for artificial intelligence (AI) is using biomedical data (e.g., texts, imaging, omics, and clinical) to provide diagnosis and treatment recommendations for cancerous conditions. Data and knowledge about biomedical entities like cancer, drugs, genes, proteins, and their mechanism is spread across structured (knowledge bases (KBs)) and unstructured (e.g., scientific articles) sources. A knowledge graph (KG) can be constructed by integrating and extracting facts about semantically interrelated entities and relations. Such a KG not only allows exploration and question answering (QA) but also enables domain experts to deduce new knowledge. However, exploring and querying large-scale KGs is tedious for non-domain users due to their lack of understanding of the data assets and semantic technologies. In this paper<sup>1</sup>, we develop a domain KG to leverage cancer-specific biomarker discovery and interactive QA. We constructed a domain ontology called OncoNet Ontology (ONO), which enables semantic reasoning for validating gene-disease (different types of cancer) relations. The KG is further enriched by harmonizing the ONO, controlled vocabularies, and biomedical concepts from scientific articles by employing BioBERT- and SciBERT-based information extractors. Further, since the medical domain is evolving, where new findings often replace old ones, without having access to up-to-date scientific findings, there is a high chance an AI system exhibits concept drift while providing diagnosis and treatment. Therefore, we finetune the KG using large language models (LLMs) based on more recent articles and KBs. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bioinformatics; Knowledge Graphs; Large Language Models; Machine Learning; Ontology; Bioinformatics; Biomarkers; Computational Linguistics; Data Mining; Decision Making; Diagnosis; Diseases; Gene Ontology; Genes; Machine Learning; Semantics; Bio-marker Discovery; Biological Process; Domain Experts; Knowledge Graphs; Language Model; Large Language Model; Machine-learning; Ontology's; Question Answering; Scientific Articles; Knowledge Graph},
	keywords = {Bioinformatics; Biomarkers; Computational linguistics; Data mining; Decision making; Diagnosis; Diseases; Gene Ontology; Genes; Machine learning; Semantics; Bio-marker discovery; Biological process; Domain experts; Knowledge graphs; Language model; Large language model; Machine-learning; Ontology's; Question Answering; Scientific articles; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Karmakar2024863,
	author = {Karmakar, Ankan and Patel, Chintan and Delhi, Venkata Santosh Kumar},
	title = {From Unstructured Data to Knowledge Graphs: An Application for Compliance Checking Problem},
	year = {2024},
	journal = {Proceedings of the International Symposium on Automation and Robotics in Construction},
	pages = {863 - 871},
	doi = {10.22260/ISARC2024/0112},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199652264&doi=10.22260%2FISARC2024%2F0112&partnerID=40&md5=6ede5ec9b90cd0b85d18aebed8162b28},
	abstract = {The rule requirements of a building code are frequently violated to create financially viable designs. These deviations are subjected to condonation by the municipal commissioner if recognizable hardships are faced. The historical concession applications for similar cases are stored in an unstructured manner, creating a barrier to knowledge transfer. The subjective statements given by applicants are composed of logical structure, language, and embedded knowledge that requires years of experience from the domain expert to decipher. A knowledge graph (KG) representation of the problem can capture concepts and represent them visually, which is easy for novice stakeholders to understand. A Large Language Model (LLM)-based method is used in this study for ontology extraction in the form of concepts and relationships. Also, unstructured input preprocessing and entity disambiguation were performed to evaluate the applicability of KG in this domain. The performance of the proposed method was checked qualitatively in a case study from real-life project examples. The limitations and scopes for improvements were also highlighted. The outcome of this study indicates KG as a potential candidate for knowledge generation from the unstructured archival data of compliance checking. The target audience for this application can be the new architects, reviewers, and programmers working on developing the end-to-end automated compliance checking systems. Finally, applying these Artificial Intelligence (AI)-based knowledge transfer mechanisms can ignite future research on automated concession applications and approvals, laying a path to the digital transformation of the industry. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Code Compliance Checking; Knowledge Graphs; Compliance Control; Knowledge Management; Code Compliance; Code Compliance Checking; Compliance Checking; Domain Experts; Graph Representation; Knowledge Graphs; Knowledge Transfer; Logical Structure; Similar Case; Unstructured Data; Knowledge Graph},
	keywords = {Compliance control; Knowledge management; Code compliance; Code compliance checking; Compliance checking; Domain experts; Graph representation; Knowledge graphs; Knowledge transfer; Logical structure; Similar case; Unstructured data; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Vanitha2024,
	author = {Vanitha, V. and Raj, Stephan Antony and Vinodhini, D. and Gnanaprasanambikai, L. and Senthil Kumar, L. and Renukadevi, S.},
	title = {Synergy of Human Language Processing and Artificial Intelligence},
	year = {2024},
	pages = {},
	doi = {10.1109/AMATHE61652.2024.10582050},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199428543&doi=10.1109%2FAMATHE61652.2024.10582050&partnerID=40&md5=30f7be15a8a305a3ce83a24268596b45},
	abstract = {This study explores the intriguing harmony between human language processing and artificial intelligence (AI). We delve into the intricate process of translating mental concepts into linguistic expressions, akin to the capabilities of AI language models. Through a survey of existing research, we uncover the parallelism between the ontological taxonomy guiding human cognition and AI's language understanding mechanisms. Moreover, we investigate how AI's multilingual proficiency mirrors the cognitive multilingualism found in individuals. This convergence holds implications for cross-lingual communication and AI-human collaboration. Our analysis anticipates a symbiotic future where the interplay of cognitive insights and AI advancements amplifies the potential of both realms. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Cross-lingual Communication; Language Cognition; Multilingualism; On Tological Taxonomy; Artificial Intelligence; Linguistics; Natural Language Processing Systems; Cross-lingual Communication; Human Cognition; Human Language Processing; Language Cognition; Language Model; Language Understanding; Linguistic Expressions; Mental Concepts; Multilingualism; On Tological Taxonomy; Taxonomies},
	keywords = {Artificial intelligence; Linguistics; Natural language processing systems; Cross-lingual communication; Human cognition; Human language processing; Language cognition; Language model; Language understanding; Linguistic expressions; Mental concepts; Multilingualism; On tological taxonomy; Taxonomies},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {SemDH 2024 - Proceedings of the 1st International Workshop of Semantic Digital Humanities, co-located with the European Semantic Web Conference 2024, ESWC 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3724},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199168653&partnerID=40&md5=f65a681ae3d940d0ae0fec4e7c6d943a},
	abstract = {The proceedings contain 13 papers. The topics discussed include: exploring prosopographical information in the virtual record treasury of Ireland’s knowledge graph for Irish history; publishing numismatic public finds on the semantic web for digital humanities research – CoinSampo linked open data service and semantic portal; towards a semantic representation of Egyptian demonology: requirements and benchmark study; eXtreme design for ontological engineering in the digital humanities with Viewsari, a knowledge graph of Giorgio Vasari’s The Lives; a corpus of biblical names in the Greek new testament to study the additions, omissions, and variations across different manuscripts; PaleOrdia: semantically describing (Cuneiform) paleography using paleographic linked open data; towards LLM-based semantic analysis of historical legal documents; and sustainable semantics for sustainable research data. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bruns2024,
	author = {Bruns, Oleksandra and Poltronieri, Andrea and Stork, Lise and Tietz, Tabea},
	title = {Proceedings of the First International Workshop of Semantic Digital Humanities co-located with the Extended Semantic Web Conference 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3724},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199140076&partnerID=40&md5=1c99cf6179ec26474aa0ce4f45292901},
	abstract = {Exploration, analysis, and preservation of the rich cultural and historical tapestry of the world are essential for our understanding of humanity’s past and shaping our future. In recent years, there has been increased interest in the creation and application of Ontologies, Knowledge Graphs, and other Semantic Web Technologies within cultural heritage (CH) and digital humanities (DH). However, to date, the distinct areas of expertise, methodologies and traditions across the fields have led to a noticeable gap between tech solutions and humanities’ needs. The aim of the International Workshop of Semantic Digital Humanities (SemDH) was to bridge this division and encourage closer collaboration and networking across diverse fields. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cultural Heritage; Digital Humanities; Fair; Knowledge Graphs; Large Language Models; Ontologies; Semantic Representation; Historic Preservation; Knowledge Graph; Semantic Web; Co-located; Cultural Heritages; Digital Humanities; Fair; International Workshops; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Semantic Representation; Ontology},
	keywords = {Historic preservation; Knowledge graph; Semantic Web; Co-located; Cultural heritages; Digital humanities; FAIR; International workshops; Knowledge graphs; Language model; Large language model; Ontology's; Semantic representation; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lehmann2024,
	author = {Lehmann, Alexander A. and Landes, Dieter},
	title = {Extracting Metadata from Learning Videos for Ontology-Based Recommender Systems Using Whisper & GPT},
	year = {2024},
	journal = {IEEE Global Engineering Education Conference, EDUCON},
	pages = {},
	doi = {10.1109/EDUCON60312.2024.10578858},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199069048&doi=10.1109%2FEDUCON60312.2024.10578858&partnerID=40&md5=eecad3788d4b65826c2465c2715e139f},
	abstract = {In modern education, individualized learning environments play a vital role by allowing learners to tailor their learning paths based on personal needs, interests, and abilities. Achieving effective individualization relies on dynamic adaptation of the learning path, typically facilitated by recommender systems. These systems offer personalized suggestions, commonly employing content-based or collaborative filtering approaches. However, traditional recommender systems often lack consideration of the semantics of learning elements. To address this limitation, ontology-based recommender systems integrate semantic modeling, establishing additional connections within a domain to enhance precision and context in recommendations. Notably, these systems mitigate the cold start problem and are particularly advantageous in learning environments with limited data. While videos are prevalent in learning platforms, their unstructured nature poses challenges for processing. This paper introduces an innovative approach, leveraging Large Language Models, specifically GPT, to extract metadata from learning videos. The proposed method intelligently augments videos and links them to a domain ontology, enabling the integration of videos into ontology-based recommender systems. The application of this approach is demonstrated through a case study in software engineering education, showcasing its potential to enhance individualized learning experiences in specific domains. The presented method offers an automated alternative to manual video processing, aligning with the evolving landscape of education technology. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Adaptive Learning Environments; Generative Ai; Large Language Models; Learning Analytics; Learning Videos; Ontology-based Recommender Systems; Collaborative Filtering; Computational Linguistics; Computer Aided Instruction; Engineering Education; Learning Systems; Metadata; Ontology; Semantics; Video Signal Processing; Adaptive Learning Environment; Generative Ai; Individualized Learning; Language Model; Large Language Model; Learning Analytic; Learning Environments; Learning Video; Ontology-based; Ontology-based Recommende System; Recommender Systems},
	keywords = {Collaborative filtering; Computational linguistics; Computer aided instruction; Engineering education; Learning systems; Metadata; Ontology; Semantics; Video signal processing; Adaptive learning environment; Generative AI; Individualized learning; Language model; Large language model; Learning analytic; Learning environments; Learning video; Ontology-based; Ontology-based recommende system; Recommender systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Randles2024,
	author = {Randles, Alex and O'Sullivan, Declan},
	title = {R2[RML]-ChatGPT Framework},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3718},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198658702&partnerID=40&md5=23af8057dbc804d39edd64b7c161265b},
	abstract = {The purpose of this paper is to explore the potential of applying Large Language Models (LLMs) in the processes involved in linked data publication, which require a high level of domain knowledge. In particular, we are interested in the semantic and syntactic correctness of data provided by LLMs, which could be used during the development of declarative uplift mappings. The R2[RML]-ChatGPT Framework is proposed, which integrates ChatGPT to gather useful quality insights on uplift mappings required in the publication of linked data. Two system experiments were conducted, which involved inputting mappings to test the correctness of returned knowledge. The semantic correctness of key ontology terms related to 50 distinct concepts were measured. Furthermore, 150 files of relevant code were automatically generated using the framework and measured for syntactic correctness. Moreover, the framework attempted to resolve invalid syntactics, which were then reassessed. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Linked Data Generation; Mapping Quality; Semantic Web; Data Handling; Domain Knowledge; High Level Languages; Linked Data; Mapping; Ontology; Syntactics; Chatgpt; Data Generation; Data Publications; Domain Knowledge; Language Model; Linked Data Generation; Linked Datum; Mapping Quality; Semantic-web; Useful Qualities; Semantic Web},
	keywords = {Data handling; Domain Knowledge; High level languages; Linked data; Mapping; Ontology; Syntactics; ChatGPT; Data generation; Data publications; Domain knowledge; Language model; Linked data generation; Linked datum; Mapping quality; Semantic-Web; Useful qualities; Semantic Web},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {KGCW 2024 - Proceedings of the 5th International Workshop on Knowledge Graph Construction, co-located with 21th Extended Semantic Web Conference, ESWC 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3718},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198658674&partnerID=40&md5=47c936f75cf1b1815e9544446412c575},
	abstract = {The proceedings contain 13 papers. The topics discussed include: propagating ontology changes to declarative mappings in construction of knowledge graphs; RML-view-to-CSV: a proof-of-concept implementation for RML logical views; not everybody speaks RDF: knowledge conversion between different data representations; BURPing through RML test cases; R2[RML]-ChatGPT framework; towards self-configuring knowledge graph construction pipelines using LLMs - a case study with RML; RMLStreamer supported by RML-view-to-CSV in the performance track of the KGCW Challenge 2024; RMLWeaver-JS: an algebraic mapping engine in the KGCW Challenge 2024; performance results of FlexRML in the KGCW Challenge 2024; and backwards or forwards? [R2]RML backwards compatibility in RMLMapper. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hofer2024,
	author = {Hofer, Marvin and Frey, Johannes and Rahm, Erhard},
	title = {Towards self-configuring Knowledge Graph Construction Pipelines using LLMs - A Case Study with RML},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3718},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198642897&partnerID=40&md5=371a1c9b734a0892d1b084ed8ffefc48},
	abstract = {This paper explores using large language models (LLMs) to generate RDF mapping language (RML) files in the RDF turtle format as a key step towards self-configuring RDF knowledge graph construction pipelines. Our case study involves mapping a subset of the Internet Movie Database (IMDB) in JSON format given a target Movie ontology (selection of DBpedia Ontology OWL statements). We define and compute several scores to assess both the generated mapping files and the resulting graph using a manually created reference. Our findings demonstrate the promising potential of the state-of-the-art commercial LLMs in a zero-shot scenario. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Automated Rml Mapping Generation; Knowledge Graph Construction; Llm-kg-engineering; Knowledge Graph; Ontology; Pipelines; Resource Description Framework (rdf); Semantics; Zero-shot Learning; Automated Rdf Mapping Language Mapping Generation; Graph Construction; Knowledge Graph Construction; Knowledge Graphs; Language Mapping; Language Model; Large Language Model-kg-engineering; Mapping Generations; Mapping Language; Rdf Mapping; Mapping},
	keywords = {Knowledge graph; Ontology; Pipelines; Resource Description Framework (RDF); Semantics; Zero-shot learning; Automated RDF mapping language mapping generation; Graph construction; Knowledge graph construction; Knowledge graphs; Language mapping; Language model; Large language model-KG-engineering; Mapping generations; Mapping Language; RDF mapping; Mapping},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@CONFERENCE{Azim2024,
	author = {Azim, Anee and Clark, Leon and Lau, Caleb and Cobb, Miles and Jenner, Kendall},
	title = {Grounding Ontologies with Pre-Trained Large Language Models for Activity Based Intelligence},
	year = {2024},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {13057},
	pages = {},
	doi = {10.1117/12.3013332},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197783254&doi=10.1117%2F12.3013332&partnerID=40&md5=6fcb5c9c3b7c989c45371adf82ddd67a},
	abstract = {The development of Activity Based Intelligence (ABI) requires an understanding of individual actors’ intents, their interactions with other entities in the environment, and how these interactions facilitate accomplishment of their goals. Statistical modelling alone is insufficient for such analyses, mandating higher-level representations such as ontology to capture important relationships. However, constructing ontologies for ABI, ensuring they remain grounded to real-world entities, and maintaining their applicability to downstream tasks requires substantial hand-tooling by domain experts. In this paper, we propose the use of a Large Language Model (LLM) to bootstrap a grounding for such an ontology. Subsequently, we demonstrate that the experience encoded within the weights of a pre-trained LLM can be used in a zero-shot manner to provide a model of normalcy, enabling ABI analysis at the semantics level, agnostic to the precise coordinate data. This is accomplished through a sequence of two transformations, made upon a kinematic track, toward natural language narratives suitable for LLM input. The first transformation generates an abstraction of the low-level kinematic track, embedding it within a knowledge graph using a domain-specific ABI ontology. Secondly, we employ a template-driven narrative generation process to form natural language descriptions of behavior. Computation of the LLM perplexity score upon these narratives achieves grounding of the ontology. This use does not rely on any prompt engineering. In characterizing the perplexity score for any given track, we observe significant variability given chosen parameters such as sentence verbosity, attribute count, clause ordering, and so on. Consequently, we propose an approach that considers multiple generated narratives for an individual track and the distribution of perplexity scores for downstream applications. We demonstrate the successful application of this methodology against a semantic track association task. Our subsequent analysis establishes how such an approach can be used to augment existing kinematics-based association algorithms. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Activity Based Intelligence; Large Language Model; Ontology; Track Association; Computational Linguistics; Kinematics; Knowledge Graph; Semantics; Zero-shot Learning; Activity Based Intelligences; Down-stream; Kinematics Tracks; Language Model; Large Language Model; Natural Languages; Ontology's; Real-world Entities; Statistic Modeling; Track Association; Ontology},
	keywords = {Computational linguistics; Kinematics; Knowledge graph; Semantics; Zero-shot learning; Activity based intelligences; Down-stream; Kinematics tracks; Language model; Large language model; Natural languages; Ontology's; Real-world entities; Statistic modeling; Track association; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Seredkina20241,
	author = {Seredkina, Elena V. and Liu, Yongmou},
	title = {Chat GPT and the Voices of Reason, Responsibility, and Regulation},
	year = {2024},
	journal = {Technology and Language},
	volume = {5},
	number = {2},
	pages = {1 - 10},
	doi = {10.48417/technolang.2024.02.01},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197669522&doi=10.48417%2Ftechnolang.2024.02.01&partnerID=40&md5=1c2c8713751a1fe322ee251fc1b80671},
	abstract = {ChatGPT, a large language model (LLM) by OpenAI, is expected to have a transformative impact on many aspects of society. There is much discussion in the media and a rapidly growing academic debate about its benefits and ethical risks. This article explores the profound influence of Socratic dialogue on Western and non-Western thought, emphasizing its role in the pursuit of truth through active thinking and dialectics. Unlike Socratic dialogue, ChatGPT generates plausible-sounding answers based on pre-trained data, lacking the pursuit of objective truth, personal experience, intuition, and empathy. The LLM’s responses are limited by its training dataset and algorithms, which can perpetuate biases or misinformation. While a true dialogue is a creative, philosophical exchange filled with ontological, ethical, and existential meanings, interactions with ChatGPT are characterized as interactive data processing. But is this really true? Perhaps we are underestimating the evolutionary growth potential of large language models? These questions have important implications for theoretical debates in cognitive science, changing our understanding of what cognition means in artificial and natural intelligence. This special issue examines ChatGPT as a subject of philosophical analysis from a position of cautious optimism and rather harsh criticism. It includes six articles covering a wide range of topics. The first group of researchers emphasizes that machine understanding and communication matches human practice. Others argue that AI cannot reach human levels of intelligence because it lacks conceptual thinking and the ability to create. Such contradictory interpretations only confirm the complexity and ambiguity of the phenomenon. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai Ethics Code; Articificial Intelligence; Chatgpt; Dialogue; Large Language Model; Responsibility},
	type = {Editorial},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Doumanas2024360,
	author = {Doumanas, Dimitrios and Soularidis, Andreas and Kotis, Konstantinos I. and Vouros, George A.},
	title = {Integrating LLMs in the Engineering of a SAR Ontology},
	year = {2024},
	journal = {IFIP Advances in Information and Communication Technology},
	volume = {714},
	pages = {360 - 374},
	doi = {10.1007/978-3-031-63223-5_27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197341155&doi=10.1007%2F978-3-031-63223-5_27&partnerID=40&md5=1c65f9035a3b5087fefbfddea2be3ff9},
	abstract = {In Search and Rescue (SAR) missions, the integration of multiple sources of information may enhance operational efficiency and increase responsiveness significantly, improving situation awareness and aiding decision-making to save lives and mitigate incident impact. Ontologies are crucial for integrating and reasoning with data from diverse sources. Engineering a domain ontology for SAR can be better supported from an agile, collaborative, and iterative ontology engineering methodology (OEM), incorporating the interests of several stakeholders. Large Language Models (LLMs) can play a significant role in completing OEM processes. The goal of this work is to identify how ontology engineering (OE) tasks can be completed with the collaboration of LLMs and humans. The objectives of this paper are, a) to present preliminary exploration of LLMs to generate domain ontologies for the modeling of SAR missions in wildfire incidents b) to propose and evaluate an LLM-enhanced OE approach. In overall, the main contribution of the work presented in this paper is the analysis of LLMs capabilities to ontology engineering, and the evaluation of the synergy between humans and machines to efficiently represent knowledge, with specific focus in the SAR domain. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Ontology Engineering; Search And Rescue; Computational Linguistics; Decision Making; Iterative Methods; Domain Ontologies; Language Model; Large Language Model; Multiple Source; Ontology Engineering; Ontology Engineering Methodologies; Ontology's; Rescue Missions; Search And Rescue; Search Missions; Ontology},
	keywords = {Computational linguistics; Decision making; Iterative methods; Domain ontologies; Language model; Large language model; Multiple source; Ontology engineering; Ontology Engineering Methodologies; Ontology's; Rescue missions; Search and rescue; Search missions; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Vieira da Silva2024,
	author = {Vieira da Silva, Luis Miguel and Köcher, Aljosha and Gehlhoff, Felix and Fay, Alexander},
	title = {On the Use of Large Language Models to Generate Capability Ontologies},
	year = {2024},
	journal = {IEEE International Conference on Emerging Technologies and Factory Automation, ETFA},
	pages = {},
	doi = {10.1109/ETFA61755.2024.10710775},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197107836&doi=10.1109%2FETFA61755.2024.10710775&partnerID=40&md5=544ef7ed4c7c396646d5da759dd60db8},
	abstract = {Capability ontologies are increasingly used to model functionalities of systems or machines. The creation of such onto-logical models with all properties and constraints of capabilities is very complex and can only be done by ontology experts. However, Large Language Models (LLMs) have shown that they can generate machine-interpretable models from natural language text input and thus support engineers / ontology experts. Therefore, this paper investigates how LLMs can be used to create capability ontologies. We present a study with a series of experiments in which capabilities with varying complexities are generated using different prompting techniques and with different LLMs. Errors in the generated ontologies are recorded and compared. To analyze the quality of the generated ontologies, a semi-automated approach based on RDF syntax checking, OWL reasoning, and SHACL constraints is used. The results of this study are very promising because even for complex capabilities, the generated ontologies are almost free of errors. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Capabilities; Large Language Models; Llms; Model-generation; Ontologies; Semantic Web; Skills; Computational Linguistics; Natural Language Processing Systems; Ontology; Capability; Language Model; Large Language Model; Logical Models; Model Generation; Ontology's; Semantic-web; Skill; Semantics},
	keywords = {Computational linguistics; Natural language processing systems; Ontology; Capability; Language model; Large language model; Logical models; Model generation; Ontology's; Semantic-Web; Skill; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{2024,
	title = {15th JSAI International Symposia on Artificial Intelligence, JSAI-isAI 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14644 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196755268&partnerID=40&md5=b70421e953ffa43e8ebb052b9b75abae},
	abstract = {The proceedings contain 15 papers. The special focus in this conference is on Artificial Intelligence. The topics include: Modeling Medical Data Access with Prova; compliance Checking in the Energy Domain via W3C Standards; preface; an Example of Argumentation Scheme from Liability: The Case of Vicarious Liability; programming Contract Amending; Reference Classification Using BERT Models to Support Scientific-Document Writing; development of a Macroeconomic Simulator with an Elaborated Firm Sector; do Young People in Regional Cities of Japan Really Shift Away from Owning Automobiles? – From an Empirical Study and a Semi-structured Interview; Nowcasting Corporate Product Development Activities Through News Article by BERTopic: The Case of the Japanese Chemical Company; Enhancing Legal Text Entailment with Prompt-Based ChatGPT: An Empirical Study; directional Generative Networks; Comparison to Evolutionary Algorithms, Using Measurements for Molecules; Using Ontological Knowledge and Large Language Model Vector Similarities to Extract Relevant Concepts in VAT-Related Legal Judgments; constructing and Explaining Case Models: A Case-Based Argumentation Perspective. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Liga2024115,
	author = {Liga, Davide and Fidelangeli, Alessia and Markovich, Réka},
	title = {Using Ontological Knowledge and Large Language Model Vector Similarities to Extract Relevant Concepts in VAT-Related Legal Judgments},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14644 LNAI},
	pages = {115 - 131},
	doi = {10.1007/978-3-031-60511-6_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196742580&doi=10.1007%2F978-3-031-60511-6_8&partnerID=40&md5=b1de8c34210cc607287d72ab5b93e1ed},
	abstract = {In this paper, we present OntoVAT, a multilingual ontology designed for extracting knowledge in legal judgments related to VAT (Value-Added Tax). This is, to our knowledge, the first extensive ontology in the VAT domain. OntoVAT aims to encapsulate critical concepts in the European VAT area and offers a scalable and reusable knowledge structure to support the automatic identification of VAT-specific concepts in legal texts. Additionally, OntoVAT supports various Artificial Intelligence and Law (AI &Law) tasks, such as extracting legal knowledge, identifying keywords, modeling topics, and extracting semantic relations. Developed using OWL with SKOS lexicalization, OntoVAT’s initial version includes ontological patterns and relations. It is available in three languages, marking a collaborative effort between computer scientists and subject matter experts. In this work, we also present an application scenario where the knowledge encoded within OntoVAT is leveraged in combination with several recent Large Language Models (LLMs). For this application, for which we used the most powerful open source LLMs available today (both generative and non-generative, including legal LLMs), we show the system’s design and some preliminary results. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai &law; Large Language Models; Legal Ontology; Vat; Automation; Computational Linguistics; Computer Software Reusability; Data Mining; Distributed Computer Systems; Knowledge Management; Open Systems; Semantics; Artificial Intelligence And Laws; Language Model; Large Language Model; Legal Judgements; Legal Ontology; Model Vectors; Multilingual Ontologies; Value-added Tax; Vector Similarity; Ontology},
	keywords = {Automation; Computational linguistics; Computer software reusability; Data mining; Distributed computer systems; Knowledge management; Open systems; Semantics; Artificial intelligence and laws; Language model; Large language model; Legal judgements; Legal ontology; Model vectors; Multilingual ontologies; Value-added tax; Vector similarity; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Asadi2024166,
	author = {Asadi, Amir Reza and Appiah, Joel Kwesi and Muntaka, Siddique Abubakr and Kropczynski, Jess N.},
	title = {Actions, Not Apps: Toward Using LLMs to Reshape Context Aware Interactions in Mixed Reality Systems},
	year = {2024},
	journal = {Communications in Computer and Information Science},
	volume = {2120 CCIS},
	pages = {166 - 176},
	doi = {10.1007/978-3-031-62110-9_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196733497&doi=10.1007%2F978-3-031-62110-9_17&partnerID=40&md5=c1b8393b350ca7b96431a0535c2310a7},
	abstract = {Mixed reality computing merges user perception of the environment with digital information. As we move from flatscreen computing toward head-mounted computing, the necessity for developing alternative interactions and user flows becomes more evident. Activity theory provides a holistic overview of user interactions and motives. In this work in progress, we propose Action Sandbox Workspace as an interaction framework for the future of MR systems by focusing on action-centric interactions rather than application-centric interactions, aiming to bridge the gap between user goals and system functionalities in everyday tasks. By integrating the ontology of actions, user intentions, and context and connecting it to spatial data mapping, this forward-looking framework aims to create a contextually adaptive user interaction environment. The recent development in large language models (LLMs) has made the implementation of this interaction flow feasible by enabling inference and decision-making based on text-based descriptions of a user’s state and intentions with data and actions users have access to. We propose this approach as a future direction for developing mixed reality platforms and integrating AI in interacting with computers. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Context Aware System; Interaction Design; Mixed Reality; Computation Theory; Decision Making; User Interfaces; Context-aware Interaction; Context-aware Systems; Digital Information; Flat-screens; Interaction Design; Language Model; Mixed Reality; Mixed Reality Systems; User Interaction; User Perceptions},
	keywords = {Computation theory; Decision making; User interfaces; Context-aware interaction; Context-aware systems; Digital information; Flat-screens; Interaction design; Language model; Mixed reality; Mixed reality systems; User interaction; User perceptions},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{2024,
	title = {36th International Conference on Advanced Information Systems Engineering, CAiSE 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14663 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196732784&partnerID=40&md5=c128c45a09008e145a19f2e9a0bd558e},
	abstract = {The proceedings contain 18 papers. The special focus in this conference is on Advanced Information Systems Engineering. The topics include: Incorporating Behavioral Recommendations Mined from Event Logs into AI Planning; trustworthy Collaborative Business Intelligence Using Zero-Knowledge Proofs and Blockchains; Towards Intelligent Systems to Improve IEC 62559 Use Cases and Smart Grid Architecture Models Quality; pricing4SaaS: Towards a Pricing Model to Drive the Operation of SaaS; Validity at the Forefront: Investigating Threats in Green AI Research; requirement-Based Methodological Steps to Identify Ontologies for Reuse; Toward Ontology-Guided IFRS Standard-Setting; towards an Explorable Conceptual Map of Large Language Models; proReco: A Process Discovery Recommender System; recPro: A User-Centric Recommendation Tool for Business Process Execution; predictive Maintenance in a Fleet Management System: The Navarchos Case; CDMiA: Revealing Impacts of Data Migrations on Schemas in Multi-model Systems; MApp-KG: Mobile App Knowledge Graph for Document-Based Feature Knowledge Generation; CAKE: Sharing Slices of Confidential Data on Blockchain; PADI-web for Plant Health Surveillance; PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ali2024107,
	author = {Ali, Syed Juned and Bork, Dominik},
	title = {A Graph Language Modeling Framework for the Ontological Enrichment of Conceptual Models},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14663 LNCS},
	pages = {107 - 123},
	doi = {10.1007/978-3-031-61057-8_7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196728599&doi=10.1007%2F978-3-031-61057-8_7&partnerID=40&md5=9d90503a65725b0cd711d59a877d41ae},
	abstract = {Conceptual models (CMs) offer a structured way to organize and communicate information in information systems. However, current models lack adequate semantics of the terminology of the underlying domain model, leading to inconsistent interpretations and uses of information. Ontology-driven conceptual modeling languages provide primitives for articulating these domain notions based on the ontological categories, i.e., stereotypes put forth by upper-level (or foundational) ontologies. Existing CMs have been created using ontologically-neutral languages (e.g., UML, ER). Enriching these models with ontological categories can better support model evaluation, meaning negotiation, semantic interoperability, and complexity management. However, manual stereotyping is prohibitive, given the sheer size of the legacy base of ontologically-neutral models. In this paper, we present a graph language modeling framework for conceptual models that combines finetuning pre-trained language models to learn the vector representation of OntoUML models’ data and then perform a graph neural networks-based node classification that exploits the model’s graph structure to predict the stereotype of model classes and relations. We show with an extensive comparative evaluation that our approach significantly outperforms existing stereotype prediction approaches. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Neural Networks; Ontology-driven Conceptual Models; Pre-trained Language Model; Representation Learning; Computational Linguistics; Graph Neural Networks; Graphic Methods; Ontology; Semantics; Conceptual Model; Current Modeling; Domain Model; Graph Languages; Language Model; Modelling Framework; Ontology-driven Conceptual Modeling; Pre-trained Language Model; Representation Learning; Modeling Languages},
	keywords = {Computational linguistics; Graph neural networks; Graphic methods; Ontology; Semantics; Conceptual model; Current modeling; Domain model; Graph languages; Language model; Modelling framework; Ontology-Driven Conceptual Modeling; Pre-trained language model; Representation learning; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Gao20241088,
	author = {Gao, Dequan and Zhu, Pengyu and Wang, Sheng and Zhao, Ziyan},
	title = {Deep Learning-Based Fault Knowledge Graph Construction for Power Communication Networks},
	year = {2024},
	pages = {1088 - 1093},
	doi = {10.1109/AEEES61147.2024.10544941},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196653424&doi=10.1109%2FAEEES61147.2024.10544941&partnerID=40&md5=c83a4d58497ffee841d54e82d70995b7},
	abstract = {Power communication network is a crucial infrastructure in the model power system, and its maintenance capability are crucial to ensuring the stable operation of power grid business. As an organized semantic knowledge base, the knowledge graph effectively organizes power communication network fault documentation and expert experience to enhance intelligent maintenance. This paper outlines a top-down approach to systematically construct a fault knowledge graph in the domain of power communication networks. The approach utilizes a seven-step method to establish a domain ontology model and integrates deep learning algorithms, including pre-trained language models, bidirectional long short time memory networks, convolutional neural networks and attention mechanisms. These algorithms process unstructured text to extract key entities and relationships. The effectiveness of the approach is verified through experiments using a product device document as a test case. Extracted knowledge is then visualized and stored using Neo4j database. Finally, this paper proposes a knowledge service model centered on fault knowledge graph and explores its application in fault diagnosis. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Fault Diagnosis; Fault Knowledge Graph; Power Communication Networks; Convolutional Neural Networks; Deep Learning; Fault Detection; Knowledge Graph; Learning Algorithms; Semantics; Fault Knowledge Graph; Faults Diagnosis; Graph Construction; Grid Business; Knowledge Graphs; Model Power Systems; Power Communication Networks; Power Grids; Stable Operation; Failure Analysis},
	keywords = {Convolutional neural networks; Deep learning; Fault detection; Knowledge graph; Learning algorithms; Semantics; Fault knowledge graph; Faults diagnosis; Graph construction; Grid business; Knowledge graphs; Model power systems; Power communication networks; Power grids; Stable operation; Failure analysis},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Nananukul2024,
	author = {Nananukul, Navapat and Kejriwal, Mayank},
	title = {HALO: An Ontology for Representing and Categorizing Hallucinations in Large Language Models},
	year = {2024},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {13058},
	pages = {},
	doi = {10.1117/12.3014048},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196524410&doi=10.1117%2F12.3014048&partnerID=40&md5=3f4e9bfa2c85f55cc69b967b613879d2},
	abstract = {Recent progress in generative AI, including large language models (LLMs) like ChatGPT, has opened up significant opportunities in fields ranging from natural language processing to knowledge discovery and data mining. However, there is also a growing awareness that the models can be prone to problems such as making information up or 'hallucinations', and faulty reasoning on seemingly simple problems. Because of the popularity of models like ChatGPT, both academic scholars and citizen scientists have documented hallucinations of several different types and severity. Despite this body of work, a formal model for describing and representing these hallucinations (with relevant meta-data) at a fine-grained level, is still lacking. In this paper, we address this gap by presenting the Hallucination Ontology or HALO, a formal, extensible ontology written in OWL that currently offers support for six different types of hallucinations known to arise in LLMs, along with support for provenance and experimental metadata. We also collect and publish a dataset containing hallucinations that we inductively gathered across multiple independent Web sources, and show that HALO can be successfully used to model this dataset and answer competency questions. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Hallucination; Large Language Model; Ontology; Computational Linguistics; Data Mining; Natural Language Processing Systems; Ontology; Chatgpt; Hallucination; In-field; Knowledge Discovery And Data Minings; Language Model; Language Processing; Large Language Model; Natural Languages; Ontology's; Recent Progress; Metadata},
	keywords = {Computational linguistics; Data mining; Natural language processing systems; Ontology; ChatGPT; Hallucination; In-field; Knowledge discovery and data minings; Language model; Language processing; Large language model; Natural languages; Ontology's; Recent progress; Metadata},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{2024,
	title = {Disruptive Technologies in Information Sciences VIII},
	year = {2024},
	journal = {Proceedings of SPIE - The International Society for Optical Engineering},
	volume = {13058},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196518388&partnerID=40&md5=7e526888240e22dd8a41e4c8298122c1},
	abstract = {The proceedings contain 38 papers. The topics discussed include: HALO: an ontology for representing and categorizing hallucinations in large language models; adaptive object detection algorithms for resource constrained autonomous robotic systems; methodology of soft partition for image classification; circumventing broken neural networks, both real and imaginary, through SPSF-based neural decoding and interconnected associative memory matrices; latency-aware service placement for Genai at the edge; risk considerations for the department of defense’s fielding of large language models; quantifying decision complexity in IADS operations; combining AI control systems and human decision support via robustness and criticality; and situational awareness on a graph: towards graph neural networks for spectrum analysis and battlefield management. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Biju20241903,
	author = {Biju, Vinai George and Babu, Bibin and Asghar, Ali and Pratap, Boppuru Rudra and Reddy, Vandana Jaipal},
	title = {From Text to Action: NLP Techniques for Washing Machine Manual Processing},
	year = {2024},
	journal = {Procedia Computer Science},
	volume = {235},
	pages = {1903 - 1919},
	doi = {10.1016/j.procs.2024.04.181},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196366834&doi=10.1016%2Fj.procs.2024.04.181&partnerID=40&md5=991367cb7bb55c3523e0790244eddaa8},
	abstract = {This scientific research study focuses on the advancements in Natural Language Processing (NLP) driven by large-scale parallel corpora and presents a comprehensive methodology for creating a parallel, multilingual corpus using NLP techniques and semantic technologies, with a particular focus on washing machine manuals. The study highlights the significant progress made in NLP through the utilization of large-scale parallel corpora and advanced NLP techniques. The successful creation of a parallel, multilingual corpus for washing machine manuals, coupled with the integration of semantic technologies and ontology modeling, demonstrates the broad applicability and potential of NLP in diverse domains.The research covers various aspects, including text extraction, segmentation, and the development of specialized pipelines for question-answering, translation, and text summarization tailored for washing machine manuals. Translation experiments using fine-tuned models demonstrated the feasibility of providing washing machine manuals in local languages, expanding accessibility and understanding for users worldwide. Additionally, the study explored text summarization using a powerful transformer-based model, which exhibited remarkable proficiency in generating concise and coherent summaries from complex input texts. The implementation of a question-answering pipeline showcased the effectiveness of various language models in handling question-answering tasks with high accuracy and effectiveness.Additionally, the article discusses the processes of data collection, information preparation, ontology creation, alignment strategies, and text analytics. Furthermore, the study addresses the challenges and potential future developments in this field, offering insights into the promising applications of NLP in the context of washing machine manuals. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Extraction; Hugging Face; Ontology; Q/a Pipeline; Segmentation Of Text; Text Summarization; Translation; Natural Language Processing Systems; Ontology; Pipelines; Semantics; Text Processing; Washing; Bert; Hugging Face; Language Processing; Language Processing Techniques; Natural Languages; Ontology's; Q/a Pipeline; Segmentation Of Text; Text Summarisation; Translation; Extraction},
	keywords = {Natural language processing systems; Ontology; Pipelines; Semantics; Text processing; Washing; BERT; Hugging face; Language processing; Language processing techniques; Natural languages; Ontology's; Q/A pipeline; Segmentation of text; Text Summarisation; Translation; Extraction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Hoseini2024,
	author = {Hoseini, Sayed and Burgdorf, Andreas and Paulus, Alexander and Meisen, Tobias and Quix, Christoph and Pomp, Andre},
	title = {Towards LLM-augmented Creation of Semantic Models for Dataspaces},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3705},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196284048&partnerID=40&md5=53768af53f4fe37e71ec1f1095e450b3},
	abstract = {Dataspaces aim to enable smooth and reliable data exchange between different organizations. They have gained increasing attention in Europe following the enactment of the European Data Governance Act. This legislation emphasizes trust, accessibility, and shared dataspaces, which require semantic interoperability grounded in the FAIR principles. Although semantic descriptions in the form of semantic models and ontologies are integral to dataspaces, their full potential remains underutilized. Meaningful metadata, including contextual information, enhances data usability, but manually creating semantic models can be challenging. Large Language Models (LLMs) offer a new way to utilize data in dataspaces. Their advanced natural language processing capabilities enable context-aware data processing and semantic understanding. This paper presents initial experiments on customizing and optimizing LLMs for semantic labeling and modeling tasks. The contributions of this work include research questions for future investigations, early experiments demonstrating the applicability of LLM for semantic labeling, and proposed directions to address discovered challenges. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Dataspace; Llms; Semantic Modeling; Laws And Legislation; Natural Language Processing Systems; Semantics; Contextual Information; Data Governances; Data Space; Language Model; Large Language Model; Semantic Descriptions; Semantic Interoperability; Semantic Labeling; Semantic Modelling; Semantic Ontology; Electronic Data Interchange},
	keywords = {Laws and legislation; Natural language processing systems; Semantics; Contextual information; Data governances; Data space; Language model; Large language model; Semantic descriptions; Semantic interoperability; Semantic labeling; Semantic modelling; Semantic ontology; Electronic data interchange},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{2024,
	title = {SDS 2024 - Proceedings of the 2nd International Workshop on Semantics in Dataspaces, co-located with the 21st Extended Semantic Web Conference, ESWC 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3705},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196272181&partnerID=40&md5=eee30806c543d90bfd980b0d68b5be23},
	abstract = {The proceedings contain 13 papers. The topics discussed include: FAIRness in dataspaces: the role of semantics for data management; the rights delegation proxy: an approach for delegations in the solid dataspace; towards LLM-augmented creation of semantic models for dataspaces; inter-pod credential exchange protocol via linked data notifications; selective disclosure of digital calibration certificate in a quality infrastructure data space; adapting ontology-based data access for data spaces; linking of open and private data in dataspace: a case study of air quality monitoring and forecasting; towards enabling FAIR dataspaces using large language models; interoperable and continuous usage control enforcement in dataspaces; and challenges and opportunities for enabling the next generation of cross-domain dataspaces. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Wegner20241409,
	author = {Wegner, Philipp and Balabin, Helena and Ay, Mehmet Can and Bauermeister, Sarah D. and Killin, Lewis O.J. and Gallacher, John E.J. and Hofmann-Apitius, Martin and Salimi, Yasamin},
	title = {Semantic Harmonization of Alzheimer’s Disease Datasets Using AD-Mapper},
	year = {2024},
	journal = {Journal of Alzheimer's Disease},
	volume = {99},
	number = {4},
	pages = {1409 - 1423},
	doi = {10.3233/JAD-240116},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196231451&doi=10.3233%2FJAD-240116&partnerID=40&md5=380e0f3ab119815ea40de2cb59f9eee9},
	abstract = {Background: Despite numerous past endeavors for the semantic harmonization of Alzheimer’s disease (AD) cohort studies, an automatic tool has yet to be developed. Objective: As cohort studies form the basis of data-driven analysis, harmonizing them is crucial for cross-cohort analysis. We aimed to accelerate this task by constructing an automatic harmonization tool. Methods: We created a common data model (CDM) through cross-mapping data from 20 cohorts, three CDMs, and ontology terms, which was then used to fine-tune a BioBERT model. Finally, we evaluated the model using three previously unseen cohorts and compared its performance to a string-matching baseline model. Results: Here, we present our AD-Mapper interface for automatic harmonization of AD cohort studies, which outperformed a string-matching baseline on previously unseen cohort studies. We showcase our CDM comprising 1218 unique variables. Conclusion: AD-Mapper leverages semantic similarities in naming conventions across cohorts to improve mapping performance. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Alzheimer’s Disease; Automatic Data Harmonization; Cohort Study; Common Data Model; Data Interoperability; Semantic Mapping; Alzheimer Disease; Article; Automation; Cohort Analysis; Data Analysis; Data Interoperability; Human; Language Model; Natural Language Processing; Semantics; Diagnosis; Alzheimer Disease; Cohort Studies; Humans; Semantics},
	keywords = {Alzheimer disease; Article; automation; cohort analysis; data analysis; data interoperability; human; language model; natural language processing; semantics; diagnosis; Alzheimer Disease; Cohort Studies; Humans; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{2024,
	title = {International workshops associated with the 36th International Conference on Advanced Information Systems Engineering, CAiSE 2024},
	year = {2024},
	journal = {Lecture Notes in Business Information Processing},
	volume = {521},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196159305&partnerID=40&md5=85f14af837281d59e3399cbc9387e98e},
	abstract = {The proceedings contain 30 papers. The special focus in this conference is on Advanced Information Systems Engineering. The topics include: Blockchain in E-Learning Platform to Enhance Trustworthy and Sharing of Micro-credentials; overstock Problems in a Purchase-to-Pay Process: An Object-Centric Process Mining Case Study; online Next Activity Prediction Under Concept Drifts; a Meta-Design Method for Modeling Customer Value; comparing Process Models Beyond Structural Equivalence; process-Specific Extensions for Enhanced Recommender Systems in Business Process Management; BPMN for Displaying the Progression of Musical Harmony and Chords - Case Study; conceptual Data Normalisation from the Practical View of Using Graph Databases; analyzing Customer Sentiments: A Comparative Evaluation of Large Language Models for Enhanced Business Intelligence; customizing a Generic Digital Transformation Objectives Model onto a Telecommunication Company; an Ontology-Based Meta-modelling Approach for Semantic-Driven Building Management Systems; LLMs for Knowledge-Graphs Enhanced Task-Oriented Dialogue Systems: Challenges and Opportunities; A Survey to Evaluate the Completeness and Correctness of a Morphological Box for AI Solutions; student Performance Prediction Model Based on Course Description and Student Similarity; Enhancing Research Clarity: Ontology-Based Modeling of Argumentation in RPML; a Conceptual Model for Blockchain-Based Trust in Digital Ecosystems (Short Paper); preface; deriving Object Oriented Normalisation from Conceptual Normalisation; empirical Insights into Context-Aware Process Predictions: Model Selection and Context Integration; improving the Service Quality in Fitness Industry by Using a Knowledge Graph Based Modeling Toolkit; An Explanation User Interface for a Knowledge Graph-Based XAI Approach to Process Analysis; Integrating Generative Artificial Intelligence into Supply Chain Management Education Using the SCOR Model; Towards Explainable Public Sector AI: An Exploration of Neuro-Symbolic AI and Enterprise Modeling (Short Paper). © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {20th International Conference on Generative Intelligence and Intelligent Tutoring Systems, ITS 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14799 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196082301&partnerID=40&md5=c9c240a61df68b258cebf9812cc2c7f8},
	abstract = {The proceedings contain 62 papers. The special focus in this conference is on Generative Intelligence and Intelligent Tutoring Systems. The topics include: Combined Maps as a Tool of Concentration and Visualization of Knowledge in the Logic of Operation of the Intelligent Tutoring Systems; fast Weakness Identification for Adaptive Feedback; quizMaster: An Adaptive Formative Assessment System; preliminary Systematic Review of Open-Source Large Language Models in Education; Jill Watson: Scaling and Deploying an AI Conversational Agent in Online Classrooms; Improving LLM Classification of Logical Errors by Integrating Error Relationship into Prompts; enhancement of Knowledge Concept Maps Using Deductive Reasoning with Educational Data; individualised Mathematical Task Recommendations Through Intended Learning Outcomes and Reinforcement Learning; developing Conversational Intelligent Tutoring for Speaking Skills in Second Language Learning; SAMI: An AI Actor for Fostering Social Interactions in Online Classrooms; exploring the Methodological Contexts and Constraints of Research in Artificial Intelligence in Education; a Constructivist Framing of Wheel Spinning: Identifying Unproductive Behaviors with Sequence Analysis; evaluating the Ability of Large Language Models to Generate Motivational Feedback; Towards Cognitive Coaching in Aircraft Piloting Tasks: Building an ACT-R Synthetic Pilot Integrating an Ontological Reference Model to Assist the Pilot and Manage Deviations; impact of Conversational Agent Language and Text Structure on Student Language; Analyzing the Role of Generative AI in Fostering Self-directed Learning Through Structured Prompt Engineering; detecting Function Inputs and Outputs for Learning-Problem Generation in Intelligent Tutoring Systems; automated Analysis of Algorithm Descriptions Quality, Through Large Language Models; An AI-Learner Shared Control Model Design for Adaptive Practicing; early Math Skill as a Predictor for Foundational Literacy; explaining Problem Recommendations in an Intelligent Tutoring System; implementing Distributed Feedback in a Tool that Supports Peer-to-Peer Simulation in Healthcare; Keeping Humans in the Loop: LLM Supported Oral Examinations. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Guo20241,
	author = {Guo, Qian and Guo, Yi and Zhao, Jin},
	title = {KBPT: knowledge-based prompt tuning for zero-shot relation triplet extraction},
	year = {2024},
	journal = {PeerJ Computer Science},
	volume = {10},
	pages = {1 - 45},
	doi = {10.7717/PEERJ-CS.2014},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196068441&doi=10.7717%2FPEERJ-CS.2014&partnerID=40&md5=41d894615152968dc653c12c2e938d4c},
	abstract = {Knowledge representation is increasingly recognized as an effective method for information extraction. Nevertheless, numerous studies have disregarded its potential applications in the zero-shot setting. In this article, a novel framework, called knowledge-based prompt tuning for zero-shot relation triplet extraction (KBPT), was developed, founded on external ontology knowledge. This framework serves as a catalyst for exploring relation triplet extraction (RTE) methods within low-resource scenarios, warranting further scrutiny. Zero-shot setting RTE aims to extract multiple triplets that consist of head entities, tail entities, and relation labels from an input sentence, where the extracted relation labels are those that do not exist in the training set. To address the data scarcity problem in zero-shot RTE, a technique was introduced to synthesize training samples by prompting language models to generate structured texts. Specifically, this involves integrating language model prompts with structured text methodologies to create a structured prompt template. This template draws upon relation labels and ontology knowledge to generate synthetic training examples. The incorporation of external ontological knowledge enriches the semantic representation within the prompt template, enhancing its effectiveness. Further, a multiple triplets decoding (MTD) algorithm was developed to overcome the challenge of extracting multiple relation triplets from a sentence. To bridge the gap between knowledge and text, a collective training method was established to jointly optimize embedding representations. The proposed model is model-agnostic and can be applied to various PLMs. Exhaustive experiments on four public datasets with zero-shot settings were conducted to demonstrate the effectiveness of the proposed method. Compared to the baseline models, KBPT demonstrated enhancements of up to 14.65% and 24.19% in F1 score on the Wiki-ZSL and TACRED-Revisit datasets, respectively. Moreover, the proposed model achieved better performance compared with the current state-ofthe- art (SOTA) model in terms of F1 score, precision-recall (P–R) curves and AUC. The code is available at https://Github.com/Phevos75/KBPT. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge-based; Prompt Tuning; Zero-shot Relation Triplet Extraction; Computational Linguistics; Data Mining; Knowledge Representation; Semantics; Zero-shot Learning; Extraction Method; F1 Scores; Knowledge Based; Knowledge-representation; Language Model; Ontology's; Prompt Tuning; Structured Text; Zero-shot Relation Triplet Extraction; ]+ Catalyst; Ontology},
	keywords = {Computational linguistics; Data mining; Knowledge representation; Semantics; Zero-shot learning; Extraction method; F1 scores; Knowledge based; Knowledge-representation; Language model; Ontology's; Prompt tuning; Structured text; Zero-shot relation triplet extraction; ]+ catalyst; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Nie20249777,
	author = {Nie, Binling and Shao, Yiming and Wang, Yigang},
	title = {Know-Adapter: Towards Knowledge-Aware Parameter-Efficient Fine-Tuning for Few-shot Named Entity Recognition},
	year = {2024},
	pages = {9777 - 9786},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195978715&partnerID=40&md5=9181c25237412eab8459003172e399c4},
	abstract = {Parameter-Efficient Fine-Tuning (PEFT) is a promising approach to mitigate the challenges about the model adaptation of pretrained language models (PLMs) for the named entity recognition (NER) task. Recent studies have highlighted the improvements that can be made to the quality of information retrieved from PLMs by adding explicit knowledge from external source like KGs to otherwise naive PEFTs. In this paper, we propose a novel knowledgeable adapter, Know-adapter, to incorporate structure and semantic knowledge of knowledge graphs into PLMs for few-shot NER. First, we construct a related KG entity type sequence for each sentence using a knowledge retriever. However, the type system of a domain-specific NER task is typically independent of that of current KGs and thus exhibits heterogeneity issue inevitably, which makes matching between the original NER and KG types (e.g. Person in NER potentially matches President in KBs) less likely, or introduces unintended noises. Thus, then we design a unified taxonomy based on KG ontology for KG entity types and NER labels. This taxonomy is used to build a learnable shared representation module, which provides shared representations for both KG entity type sequences and NER labels. Based on these shared representations, our Know-adapter introduces high semantic relevance knowledge and structure knowledge from KGs as inductive bias to guide the updating process of the adapter. Additionally, the shared representations guide the learnable representation module to reduce noise in the unsupervised expansion of label words. Extensive experiments on multiple NER datasets show the superiority of Know-Adapter over other state-of-the-art methods in both full-resource and low-resource settings. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Few-shot Ner; Knowledge Graph; Peft; Knowledge Graph; Natural Language Processing Systems; Semantics; Entity-types; Explicit Knowledge; Few-shot Named Entity Recognition; Fine Tuning; Knowledge Graphs; Language Model; Model Adaptation; Named Entity Recognition; Parameter-efficient Fine-tuning; Shared Representations; Taxonomies},
	keywords = {Knowledge graph; Natural language processing systems; Semantics; Entity-types; Explicit knowledge; Few-shot named entity recognition; Fine tuning; Knowledge graphs; Language model; Model Adaptation; Named entity recognition; Parameter-efficient fine-tuning; Shared representations; Taxonomies},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Park202411723,
	author = {Park, Dojun and Padó, Sebastian},
	title = {Multi-Dimensional Machine Translation Evaluation: Model Evaluation and Resource for Korean},
	year = {2024},
	pages = {11723 - 11744},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195978071&partnerID=40&md5=ce903b20f4a1bedadf039dd6be8ca381},
	abstract = {Almost all frameworks for the manual or automatic evaluation of machine translation characterize the quality of an MT output with a single number. An exception is the Multidimensional Quality Metrics (MQM) framework which offers a fine-grained ontology of quality dimensions for scoring (such as style, fluency, accuracy, and terminology). Previous studies have demonstrated the feasibility of MQM annotation but there are, to our knowledge, no computational models that predict MQM scores for novel texts, due to a lack of resources. In this paper, we address these shortcomings by (a) providing a 1200-sentence MQM evaluation benchmark for the language pair English-Korean and (b) reframing MT evaluation as the multi-task problem of simultaneously predicting several MQM scores using SOTA language models, both in a reference-based MT evaluation setup and a reference-free quality estimation (QE) setup. We find that reference-free setup outperforms its counterpart in the style dimension while reference-based models retain an edge regarding accuracy. Overall, RemBERT emerges as the most promising model. Through our evaluation, we offer an insight into the translation quality in a more fine-grained, interpretable manner. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Corpus; Evaluation Methodologies; Explainability; Machine Translation; Multilinguality; Benchmarking; Computational Linguistics; Computer Aided Language Translation; Quality Control; Corpus; Evaluation Methodologies; Explainability; Fine Grained; Machine Translations; Metric Scores; Mt Evaluations; Multilinguality; Quality Metrices; Reference-free; Machine Translation},
	keywords = {Benchmarking; Computational linguistics; Computer aided language translation; Quality control; Corpus; Evaluation methodologies; Explainability; Fine grained; Machine translations; Metric scores; MT evaluations; Multilinguality; Quality metrices; Reference-free; Machine translation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Gajbhiye2024801,
	author = {Gajbhiye, Amit and Bouraoui, Zied and Espinosa-Anke, Luis and Schockaert, Steven},
	title = {AMenDeD: Modelling Concepts by Aligning Mentions, Definitions and Decontextualised Embeddings},
	year = {2024},
	pages = {801 - 811},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195976382&partnerID=40&md5=3bc1a9578bc5511c0a3b56007404b5c5},
	abstract = {Contextualised Language Models (LM) improve on traditional word embeddings by encoding the meaning of words in context. However, such models have also made it possible to learn high-quality decontextualised concept embeddings. Three main strategies for learning such embeddings have thus far been considered: (i) fine-tuning the LM to directly predict concept embeddings from the name of the concept itself, (ii) averaging contextualised representations of mentions of the concept in a corpus, and (iii) encoding definitions of the concept. As these strategies have complementary strengths and weaknesses, we propose to learn a unified embedding space in which all three types of representations can be integrated. We show that this allows us to outperform existing approaches in tasks such as ontology completion, which heavily depends on access to high-quality concept embeddings. We furthermore find that mentions and definitions are well-aligned in the resulting space, enabling tasks such as target sense verification, even without the need for any fine-tuning. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Embeddings; Definitions; Language Models; Lexical Semantics; Computational Linguistics; Encoding (symbols); Modeling Languages; Semantics; Signal Encoding; Concept Embedding; Definition; Embeddings; Encodings; Fine Tuning; High Quality; Language Model; Learn+; Lexical Semantics; Modeling Concepts},
	keywords = {Computational linguistics; Encoding (symbols); Modeling languages; Semantics; Signal encoding; Concept embedding; Definition; Embeddings; Encodings; Fine tuning; High quality; Language model; Learn+; Lexical semantics; Modeling concepts},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Moskvoretskii20241498,
	author = {Moskvoretskii, Viktor and Panchenko, Alexander I. and Nikishina, Irina},
	title = {Are Large Language Models Good at Lexical Semantics? A Case of Taxonomy Learning},
	year = {2024},
	pages = {1498 - 1510},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195968824&partnerID=40&md5=62d830955a58a2e4366d6f4cd82571e1},
	abstract = {Recent studies on LLMs do not pay enousdfgh attention to linguistic and lexical semantic tasks, such as taxonomy learning. In this paper, we explore the capacities of Large Language Models featuring LLaMA-2 and Mistral for several Taxonomy-related tasks. We introduce a new methodology and algorithm for data collection via stochastic graph traversal leading to controllable data collection. Collected cases provide the ability to form nearly any type of graph operation. We test the collected dataset for learning taxonomy structure based on English WordNet and compare different input templates for fine-tuning LLMs. Moreover, we apply the fine-tuned models on such datasets on the downstream tasks achieving state-of-the-art results on the TexEval-2 dataset. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Hypernym Prediction; Llms; Taxonomy Construction; Wordnet; Computational Linguistics; Data Acquisition; Learning Systems; Ontology; Semantics; Statistical Tests; Stochastic Systems; Data Collection; Hypernym Prediction; Language Model; Lexical Semantics; Linguistic Semantics; Llm; Semantic Tasks; Taxonomy Construction; Taxonomy Learning; Wordnet; Taxonomies},
	keywords = {Computational linguistics; Data acquisition; Learning systems; Ontology; Semantics; Statistical tests; Stochastic systems; Data collection; Hypernym prediction; Language model; Lexical semantics; Linguistic semantics; LLM; Semantic tasks; Taxonomy construction; Taxonomy learning; Wordnet; Taxonomies},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Almeman202417683,
	author = {Almeman, Fatemah and Schockaert, Steven and Espinosa-Anke, Luis},
	title = {WordNet under Scrutiny: Dictionary Examples in the Era of Large Language Models},
	year = {2024},
	pages = {17683 - 17695},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195959037&partnerID=40&md5=e15ecd1594f4582b2aa659f387ad15ad},
	abstract = {Dictionary definitions play a prominent role in a wide range of NLP tasks, for instance by providing additional context about the meaning of rare and emerging terms. Many dictionaries also provide examples to illustrate the prototypical usage of words, which brings further opportunities for training or enriching NLP models. The intrinsic qualities of dictionaries, and related lexical resources such as glossaries and encyclopedias, are however still not well-understood. While there has been significant work on developing best practices, such guidance has been aimed at traditional usages of dictionaries (e.g. supporting language learners), and it is currently unclear how different quality aspects affect the NLP systems that rely on them. To address this issue, we compare WordNet, the most commonly used lexical resource in NLP, with a variety of dictionaries, as well as with examples that were generated by ChatGPT. Our analysis involves human judgments as well as automatic metrics. We furthermore study the quality of word embeddings derived from dictionary examples, as a proxy for downstream performance. We find that WordNet's examples lead to lower-quality embeddings than those from the Oxford dictionary. Surprisingly, however, the ChatGPT generated examples were found to be most effective overall. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Dictionary Examples; Lexical Resources; Semantics; Embeddings; Natural Language Processing Systems; Ontology; Best Practices; Dictionary Definitions; Dictionary Example; Human Judgments; Language Model; Lexical Resources; Nlp Systems; Quality Aspects; Wordnet; Semantics},
	keywords = {Embeddings; Natural language processing systems; Ontology; Best practices; Dictionary definitions; Dictionary example; Human judgments; Language model; Lexical resources; NLP systems; Quality aspects; Wordnet; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Mousavi20243782,
	author = {Mousavi, Ali and Zhan, Xin and Bai, He and Shi, Peng and Rekatsinas, Theodoros I. and Han, Benjamin and Li, Yunyao and Pound, Jeffrey and Susskind, Joshua M. and Schluter, Natalie},
	title = {Construction of Paired Knowledge Graph - Text Datasets Informed by Cyclic Evaluation},
	year = {2024},
	pages = {3782 - 3803},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195942803&partnerID=40&md5=1a4c03536a862d8e96e7ad7568b73e0c},
	abstract = {Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Informed by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We also construct two synthetic datasets using large language models (LLMs), and observe that these are conducive to models that perform significantly well on cyclic generation of text, but less so on cyclic generation of KGs, probably because of a lack of a consistent underlying ontology. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Forward Modeling; Knowledge Graphs; Language Model; Neural Modelling; Noisy Datasets; Ontology's; Reverse Modeling; Synthetic Datasets; Large Datasets},
	keywords = {Knowledge graph; Forward modeling; Knowledge graphs; Language model; Neural modelling; Noisy datasets; Ontology's; Reverse Modeling; Synthetic datasets; Large datasets},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Liu202412988,
	author = {Liu, Jianyu and Bi, Sheng and Qi, Guilin Lin},
	title = {PRIMO: Progressive Induction for Multi-hop Open Rule Generation},
	year = {2024},
	pages = {12988 - 12998},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195903887&partnerID=40&md5=1690e4280c54255a829857661381b2f1},
	abstract = {Open rule refer to the implication from premise atoms to hypothesis atoms, which captures various relations between instances in the real world. Injecting open rule knowledge into the machine helps to improve the performance of downstream tasks such as dialogue and relation extraction. Existing approaches focus on single-hop open rule generation, ignoring multi-hop scenarios, leading to logical inconsistencies between premise and hypothesis atoms, as well as semantic duplication of generated rule atoms. To address these issues, we propose a progressive multi-stage open rule generation method called PRIMO. We introduce ontology information during the rule generation stage to reduce ambiguity and improve rule accuracy. PRIMO constructs a multi-stage structure consisting of generation, extraction, and ranking modules to fully leverage the latent knowledge within the language model across multiple dimensions. Furthermore, we employ reinforcement learning from human feedback to further optimize model, enhancing the model's understanding of commonsense knowledge. Experiments show that compared to baseline models, PRIMO significantly improves rule quality and diversity while reducing the repetition rate of rule atoms. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Open Rule; Pre-trained Language Model; Reinforcement Learning From Human Feedback; Atoms; Computational Linguistics; Extraction; Knowledge Management; Learning Systems; Semantics; Language Model; Multi-hops; Multi-stages; Open Rule; Pre-trained Language Model; Real-world; Reinforcement Learning From Human Feedback; Reinforcement Learnings; Rule Generation; Rule Knowledge; Reinforcement Learning},
	keywords = {Atoms; Computational linguistics; Extraction; Knowledge management; Learning systems; Semantics; Language model; Multi-hops; Multi-stages; Open rule; Pre-trained language model; Real-world; Reinforcement learning from human feedback; Reinforcement learnings; Rule generation; Rule knowledge; Reinforcement learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Proietti2024924,
	author = {Proietti, Lorenzo and Perrella, Stefano and Tedeschi, Simone and Vulpis, Giulia and Lavalle, Leonardo and Sanchietti, Andrea and Ferrari, Andrea and Navigli, Roberto},
	title = {Analyzing Homonymy Disambiguation Capabilities of Pretrained Language Models},
	year = {2024},
	pages = {924 - 938},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195902853&partnerID=40&md5=336ca66d63740d2dfa410e9e820f3b93},
	abstract = {Word Sense Disambiguation (WSD) is a key task in Natural Language Processing (NLP), aiming to assign the correct meaning (sense) to a word in context. However, traditional WSD systems rely on WordNet as the underlying sense inventory, often differentiating meticulously between subtle nuances of word meanings, which may lead to excessive complexity and reduced practicality of WSD systems in today's NLP. Indeed, current Pretrained Language Models (PLMs) do seem to be able to perform disambiguation, but it is not clear to what extent, or to what level of granularity, they actually operate. In this paper, we address these points and, firstly, introduce a new large-scale resource that leverages homonymy relations to systematically cluster WordNet senses, effectively reducing the granularity of word senses to a very coarse-grained level; secondly, we use this resource to train Homonymy Disambiguation systems and investigate whether PLMs are inherently able to differentiate coarse-grained word senses. Our findings demonstrate that, while state-of-the-art models still struggle to choose the correct fine-grained meaning of a word in context, Homonymy Disambiguation systems are able to differentiate homonyms with up to 95% accuracy scores even without fine-tuning the underlying PLM. We release our data and code at https://github.com/SapienzaNLP/homonymy-wsd. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Corpus (creation, Annotation, Etc.); Semantics; Word Sense Disambiguation; Computational Linguistics; Lead Compounds; Natural Language Processing Systems; Ontology; Coarse-grained; Corpus (creation, Annotation, Etc.); In Contexts; Language Model; Language Processing; Natural Languages; Word Sense; Word Sense Disambiguation; Word Sense Disambiguation Systems; Wordnet; Semantics},
	keywords = {Computational linguistics; Lead compounds; Natural language processing systems; Ontology; Coarse-grained; Corpus (creation, annotation, etc.); In contexts; Language model; Language processing; Natural languages; Word sense; Word Sense Disambiguation; Word sense disambiguation systems; Wordnet; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{2024,
	title = {20th International Conference on Generative Intelligence and Intelligent Tutoring Systems, ITS 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14798 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195865074&partnerID=40&md5=1f791857aac1b408a39fb2f36361f056},
	abstract = {The proceedings contain 62 papers. The special focus in this conference is on Generative Intelligence and Intelligent Tutoring Systems. The topics include: Combined Maps as a Tool of Concentration and Visualization of Knowledge in the Logic of Operation of the Intelligent Tutoring Systems; fast Weakness Identification for Adaptive Feedback; quizMaster: An Adaptive Formative Assessment System; preliminary Systematic Review of Open-Source Large Language Models in Education; Jill Watson: Scaling and Deploying an AI Conversational Agent in Online Classrooms; Improving LLM Classification of Logical Errors by Integrating Error Relationship into Prompts; enhancement of Knowledge Concept Maps Using Deductive Reasoning with Educational Data; individualised Mathematical Task Recommendations Through Intended Learning Outcomes and Reinforcement Learning; developing Conversational Intelligent Tutoring for Speaking Skills in Second Language Learning; SAMI: An AI Actor for Fostering Social Interactions in Online Classrooms; exploring the Methodological Contexts and Constraints of Research in Artificial Intelligence in Education; a Constructivist Framing of Wheel Spinning: Identifying Unproductive Behaviors with Sequence Analysis; evaluating the Ability of Large Language Models to Generate Motivational Feedback; Towards Cognitive Coaching in Aircraft Piloting Tasks: Building an ACT-R Synthetic Pilot Integrating an Ontological Reference Model to Assist the Pilot and Manage Deviations; impact of Conversational Agent Language and Text Structure on Student Language; Analyzing the Role of Generative AI in Fostering Self-directed Learning Through Structured Prompt Engineering; detecting Function Inputs and Outputs for Learning-Problem Generation in Intelligent Tutoring Systems; automated Analysis of Algorithm Descriptions Quality, Through Large Language Models; An AI-Learner Shared Control Model Design for Adaptive Practicing; early Math Skill as a Predictor for Foundational Literacy; explaining Problem Recommendations in an Intelligent Tutoring System; implementing Distributed Feedback in a Tool that Supports Peer-to-Peer Simulation in Healthcare; Keeping Humans in the Loop: LLM Supported Oral Examinations. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Feng20241637,
	author = {Feng, Jun and Chang, Yanghong and Lu, Jiamin and Tang, Hailin and Lyu, Zhipeng and Qiu, Yuchun},
	title = {Construction and Application of Knowledge Graph for Water Engineering Scheduling Based on Large Language Model; 基于大语言模型的水工程调度知识图谱的构建与应用},
	year = {2024},
	journal = {Journal of Frontiers of Computer Science and Technology},
	volume = {18},
	number = {6},
	pages = {1637 - 1647},
	doi = {10.3778/j.issn.1673-9418.2311098},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195585807&doi=10.3778%2Fj.issn.1673-9418.2311098&partnerID=40&md5=09fbfb1b464d422201314ff64ff1a36b},
	abstract = {With the growth of water conservancy and the increasing demand for information, handling and representing large volumes of water-related data has become complex. Particularly, scheduling textual data often exists in natural language form, lacking clear structure and standardization. Processing and utilizing such diverse data necessitates extensive domain knowledge and professional expertise. To tackle this challenge, a method based on large language model has been proposed to construct a knowledge graph for water engineering scheduling. This approach involves collecting and preprocessing scheduling rule data at the data layer, leveraging large language models to extract embedded knowledge, constructing the ontology at the conceptual layer, and extracting the“three- step”method prompt strategy at the instance layer. Under the interaction of the data, conceptual, and instance layers, high-performance extraction of rule texts is achieved, and the construction of the dataset and knowledge graph is completed. Experimental results show that the F1 value of the extraction method in this paper reaches 85.5%, and the effectiveness and rationality of the modules of the large language model are validated through ablation experiments. This graph integrates dispersed water conservancy rule information, effectively handles unstructured textual data, and offers visualization querying and functionality tracing. It aids professionals in assessing water conditions and selecting appropriate scheduling schemes, providing valuable support for conservancy decision-making and intelligent reasoning. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Extraction; Knowledge Graph; Large Language Model (llm); Ontology Construction; Water Engineering Scheduling},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@ARTICLE{2024,
	title = {36th International Conference on Advanced Information Systems Engineering, CAiSE 2024},
	year = {2024},
	journal = {Lecture Notes in Business Information Processing},
	volume = {520 LNBIP},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195536573&partnerID=40&md5=60130c44ff2f52a3ec44b235618e92fd},
	abstract = {The proceedings contain 18 papers. The special focus in this conference is on Advanced Information Systems Engineering. The topics include: Incorporating Behavioral Recommendations Mined from Event Logs into AI Planning; trustworthy Collaborative Business Intelligence Using Zero-Knowledge Proofs and Blockchains; Towards Intelligent Systems to Improve IEC 62559 Use Cases and Smart Grid Architecture Models Quality; pricing4SaaS: Towards a Pricing Model to Drive the Operation of SaaS; Validity at the Forefront: Investigating Threats in Green AI Research; requirement-Based Methodological Steps to Identify Ontologies for Reuse; Toward Ontology-Guided IFRS Standard-Setting; towards an Explorable Conceptual Map of Large Language Models; proReco: A Process Discovery Recommender System; recPro: A User-Centric Recommendation Tool for Business Process Execution; predictive Maintenance in a Fleet Management System: The Navarchos Case; CDMiA: Revealing Impacts of Data Migrations on Schemas in Multi-model Systems; MApp-KG: Mobile App Knowledge Graph for Document-Based Feature Knowledge Generation; CAKE: Sharing Slices of Confidential Data on Blockchain; PADI-web for Plant Health Surveillance; PROMISE: A Framework for Model-Driven Stateful Prompt Orchestration. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Luo202412361,
	author = {Luo, Zhizhao and Wang, Youchen and Ke, Wenjun and Qi, Rui and Guo, Yikai and Wang, Peng},
	title = {BOOSTING LLMS WITH ONTOLOGY-AWARE PROMPT FOR NER DATA AUGMENTATION},
	year = {2024},
	journal = {Proceedings - ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing},
	pages = {12361 - 12365},
	doi = {10.1109/ICASSP48485.2024.10446860},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195380625&doi=10.1109%2FICASSP48485.2024.10446860&partnerID=40&md5=fe90a9ba5179ce13cc89bd5fa6b4ea7c},
	abstract = {Named Entity Recognition (NER) data augmentation (DA) aims to improve the performance and generalization capabilities of NER models by generating scalable training data. The key challenge lies in ensuring the generated samples maintain contextual diversity while preserving label consistency. However, existing dominant methods fail to simultaneously satisfy both criteria. Inspired by the extensive generative capabilities of large language models (LLMs), we propose ANGEL, a frAmework integrating the oNtoloGy structure and instructivE prompting within LLMs. Specifically, the hierarchical ontology structure guides prompt ranking, while instructive prompting enhances LLMs' mastery of domain knowledge, empowering synthetic sample generation and annotation. Experiments show ANGEL surpasses state-of-the-art (SOTA) baselines, conferring absolute F1 increases of 2.86% and 0.93% on two benchmark datasets, respectively. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Augmentation; Knowledge Graph; Large Language Model; Named Entity Recognition},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Yuan202475,
	author = {Yuan, Jicheng and Le-Tuan, Anh and Nguyen-Duc, Manh and Tran, Trung Kien and Hauswirth, Manfred and Le-Phuoc, Danh},
	title = {VisionKG: Unleashing the Power of Visual Datasets via Knowledge Graph},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14665 LNCS},
	pages = {75 - 93},
	doi = {10.1007/978-3-031-60635-9_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195295743&doi=10.1007%2F978-3-031-60635-9_5&partnerID=40&md5=eab58926055b88a002fdb11a494dccd6},
	abstract = {The availability of vast amounts of visual data with diverse and fruitful features is a key factor for developing, verifying, and benchmarking advanced computer vision (CV) algorithms and architectures. Most visual datasets are created and curated for specific tasks or with limited data distribution for very specific fields of interest, and there is no unified approach to manage and access them across diverse sources, tasks, and taxonomies. This not only creates unnecessary overheads when building robust visual recognition systems, but also introduces biases into learning systems and limits the capabilities of data-centric AI. To address these problems, we propose the VisionKnowledge Graph (VisionKG), a novel resource that interlinks, organizes and manages visual datasets via knowledge graphs and Semantic Web technologies. It can serve as a unified framework facilitating simple access and querying of state-of-the-art visual datasets, regardless of their heterogeneous formats and taxonomies. One of the key differences between our approach and existing methods is that VisionKG is not only based on metadata but also utilizes a unified data schema and external knowledge bases to integrate, interlink, and align visual datasets. It enhances the enrichment of the semantic descriptions and interpretation at both image and instance levels and offers data retrieval and exploratory services via SPARQL and natural language empowered by Large Language Models (LLMs). VisionKG currently contains 617 million RDF triples that describe approximately 61 million entities, which can be accessed at https://vision.semkg.org and through APIs. With the integration of 37 datasets and four popular computer vision tasks, we demonstrate its usefulness across various scenarios when working with computer vision pipelines. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Computer Vision; Knowledge Graph; Linked Data; Ontology; Rdf; Computer Vision; Http; Image Enhancement; Knowledge Graph; Learning Systems; Resource Description Framework (rdf); Taxonomies; Computer Vision Algorithms; Computer Vision Architectures; Key Factors; Knowledge Graphs; Linked Datum; Ontology's; Power; Rdf; Specific Tasks; Visual Data; Linked Data},
	keywords = {Computer vision; HTTP; Image enhancement; Knowledge graph; Learning systems; Resource Description Framework (RDF); Taxonomies; Computer vision algorithms; Computer vision architectures; Key factors; Knowledge graphs; Linked datum; Ontology's; Power; RDF; Specific tasks; Visual data; Linked data},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Stork2024199,
	author = {Stork, Lise and Zijdeman, Richard Lindert and Tiddi, Ilaria and Ten Teije, Annette},
	title = {Enabling Social Demography Research Using Semantic Technologies},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14665 LNCS},
	pages = {199 - 216},
	doi = {10.1007/978-3-031-60635-9_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195271433&doi=10.1007%2F978-3-031-60635-9_12&partnerID=40&md5=b632337dffe85b9fc216b98f19dc9198},
	abstract = {A shift in scientific publishing from paper-based to knowledge-based practices promotes reproducibility, machine actionability and knowledge discovery. This is important for disciplines like social demography, where study indicators are often social constructs such as race or education, hypothesis tests are challenging to compare due to their limited temporal and spatial coverage, and research output is presented in natural language, which can be ambiguous and imprecise. In this work, we present the MIRA resource, to aid researchers in their research workflow, and publish FAIR findings. MIRA consists of: (1) an ontology for social demography research, (2) a method for automated ontology population by prompting Large Language Models, and (3) a knowledge graph populated in terms of the ontology by annotating a set of research papers on health inequality. The resource allows researchers to formally represent their social demography research hypotheses, discovering research biases and novel research questions. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Health Inequality; Hypothesis Representation; Information Extraction; Scientific Knowledge Graphs; Social Demography; Knowledge Graph; Ontology; Population Statistics; Semantic Web; Semantics; Health Inequality; Hypothesis Representation; Information Extraction; Knowledge Based; Knowledge Graphs; Ontology's; Scientific Knowledge; Scientific Knowledge Graph; Semantic Technologies; Social Demography; Demography},
	keywords = {Knowledge graph; Ontology; Population statistics; Semantic Web; Semantics; Health inequality; Hypothesis representation; Information extraction; Knowledge based; Knowledge graphs; Ontology's; Scientific knowledge; Scientific knowledge graph; Semantic technologies; Social demography; Demography},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Wang2024361,
	author = {Wang, Peng Edward and Karigiannis, John N. and Gao, Robert X.},
	title = {Ontology-integrated tuning of large language model for intelligent maintenance},
	year = {2024},
	journal = {CIRP Annals},
	volume = {73},
	number = {1},
	pages = {361 - 364},
	doi = {10.1016/j.cirp.2024.04.012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195267530&doi=10.1016%2Fj.cirp.2024.04.012&partnerID=40&md5=b69a6b37f674b0981d0e05eab23c863f},
	abstract = {As new AI technologies such as Large Language Models (LLM) quickly evolve, the need for enhancing general-purpose LLMs with physical knowledge to better serve the manufacturing community has been increasingly recognized. This paper presents a method that tailors GPT-3.5 with domain-specific knowledge for intelligent aircraft maintenance. Specifically, aircraft ontology is investigated to curate maintenance logs with encoded component hierarchical structure to fine-tune GPT-3.5. Experimental results demonstrate the effectiveness of the developed method in accurately identifying defective components and providing consistent maintenance action recommendations, outperforming general-purpose GPT-3.5 and GPT-4.0. The method can be adapted to other domains in manufacturing and beyond. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Machine Learning; Maintenance; Aircraft; Computational Linguistics; Domain Knowledge; Machine Learning; Ontology; Ai Technologies; Aircraft Maintenance; Consistent Maintenance; Domain-specific Knowledge; Hierarchical Structures; Intelligent Maintenance; Language Model; Large Language Model; Machine-learning; Ontology's; Maintenance},
	keywords = {Aircraft; Computational linguistics; Domain Knowledge; Machine learning; Ontology; AI Technologies; Aircraft maintenance; Consistent maintenance; Domain-specific knowledge; Hierarchical structures; Intelligent maintenance; Language model; Large language model; Machine-learning; Ontology's; Maintenance},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{2024,
	title = {21st European Semantic Web Conference, ESWC 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14665 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195259090&partnerID=40&md5=4a01a73d73a3f30ac3dae7ef63ca112e},
	abstract = {The proceedings contain 32 papers. The special focus in this conference is on Semantic Web. The topics include: Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction; QAGCN: Answering Multi-relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs; leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs; a Language Model Based Framework for New Concept Placement in Ontologies; low-Dimensional Hyperbolic Knowledge Graph Embedding for Better Extrapolation to Under-Represented Data; SC-Block: Supervised Contrastive Blocking Within Entity Resolution Pipelines; navigating Ontology Development with Large Language Models; ESLM: Improving Entity Summarization by Leveraging Language Models; explanation of Link Predictions on Knowledge Graphs via Levelwise Filtering and Graph Summarization; Large Language Models for Scientific Question Answering: An Extensive Analysis of the SciQA Benchmark; efficient Evaluation of Conjunctive Regular Path Queries Using Multi-way Joins; can Contrastive Learning Refine Embeddings; automation of Electronic Invoice Validation Using Knowledge Graph Technologies; towards Cyber Mapping the German Financial System with Knowledge Graphs; integrating Domain Knowledge for Enhanced Concept Model Explainability in Plant Disease Classification; generative Expression Constrained Knowledge-Based Decoding for Open Data. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ludwig2024143,
	author = {Ludwig, Heiner and Betker, Vincent and Schmidt, Thorsten Lars and Kuhn, Mathias},
	title = {SPEECH-TO-JOBSHOP: AN ONTOLOGY-DRIVEN DIGITAL ASSISTANT FOR SIMULATION MODELING},
	year = {2024},
	journal = {Proceedings - European Council for Modelling and Simulation, ECMS},
	volume = {38},
	number = {1},
	pages = {143 - 149},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195222610&partnerID=40&md5=4df1984998f2d5b0192a4f7b121cb30c},
	abstract = {This paper introduces a novel method utilizing speech-based digital assistants and large language models (LLMs) to streamline the creation of simulation models for Job Shop Scheduling Problems (JSSP). The system simplifies the process by allowing natural language interactions for ontology-based model generation. The study evaluates the performance of various LLMs in ontology-based simulation modeling by benchmarking their ability to extract and assign semantical entities and relations. We found that ChatGPT-4Turbo is able to correctly identify all model elements given in descriptions of the production scenarios we tested, while less resource-intensive and open source models like Mixtral-8x7b and Zephyr-beta perform well in a less complex scenario. The findings demonstrate the potential of integrating LLMs and natural language processing in simulation modeling, significantly enhancing efficiency and reducing the need for manual modeling. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Digital Assistant; Job Shop Scheduling; Large Language Models; Ontology; Simulation Modeling; Benchmarking; Computational Linguistics; Job Shop Scheduling; Modeling Languages; Natural Language Processing Systems; Digital Assistants; Job Shop Scheduling Problems; Job-shop; Job-shop Scheduling; Language Model; Large Language Model; Novel Methods; Ontology's; Simulation Model; Simulation-modelling; Ontology},
	keywords = {Benchmarking; Computational linguistics; Job shop scheduling; Modeling languages; Natural language processing systems; Digital assistants; Job shop scheduling problems; Job-shop; Job-Shop scheduling; Language model; Large language model; Novel methods; Ontology's; Simulation model; Simulation-modelling; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {Workshop on Cognitive Aspects of the Lexicon, CogALex 2024 at LREC-COLING 2024 - Workshop Proceedings},
	year = {2024},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195208755&partnerID=40&md5=2e51a712f619cec793267c9ace82e0a2},
	abstract = {The proceedings contain 19 papers. The topics discussed include: CLAVELL - cognitive linguistic annotation and visualization environment for language learning; individual text corpora predict openness, interests, knowledge and level of education; an empirical study on vague deictic temporal adverbials; symbolic learning of rules for semantic relation types identification in French genitive post-nominal prepositional phrases; idiom complexity in apple-pie order: the disentanglement of decomposability and transparency; can GPT-4 recover latent semantic relational information from word associations? a detailed analysis of agreement with human-annotated semantic ontologies; cross-linguistic processing of non-compositional expressions in Slavic languages; and representing abstract concepts with images: an investigation with large language models. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Salman202471,
	author = {Salman, Muhammad and Haller, Armin and Rodríguez Méndez, Sergio José and Naseem, Usman},
	title = {Tiny But Mighty: A Crowdsourced Benchmark Dataset for Triple Extraction from Unstructured Text},
	year = {2024},
	pages = {71 - 81},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195207916&partnerID=40&md5=194bc92f2923bd730c7105bffa281838},
	abstract = {In the context of Natural Language Processing (NLP) and Semantic Web applications, constructing Knowledge Graphs (KGs) from unstructured text plays a vital role. Several techniques have been developed for KG construction from text, but the lack of standardized datasets hinders the evaluation of triple extraction methods. The evaluation of existing KG construction approaches is based on structured data or manual investigations. To overcome this limitation, this work introduces a novel dataset specifically designed to evaluate KG construction techniques from unstructured text. Our dataset consists of a diverse collection of compound and complex sentences meticulously annotated by human annotators with potential triples (subject, predicate, object). The annotations underwent further scrutiny by expert ontologists to ensure accuracy and consistency. For evaluation purposes, the proposed F-measure criterion offers a robust approach to quantify the relatedness and assess the alignment between extracted triples and the ground-truth triples, providing a valuable tool for evaluating the performance of triple extraction systems. By providing a diverse collection of high-quality triples, our proposed benchmark dataset offers a comprehensive training and evaluation set for refining the performance of state-of-the-art language models on a triple extraction task. Furthermore, this dataset encompasses various KG-related tasks, such as named entity recognition, relation extraction, and entity linking. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph (kg); Large Language Models (llms); Natural Language Processing (nlp); Text Annotation; Triple; Computational Linguistics; Crowdsourcing; Extraction; Knowledge Graph; Large Datasets; Natural Language Processing Systems; Quality Control; Knowledge Graphs; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Text Annotations; Triple; Unstructured Texts; Benchmarking},
	keywords = {Computational linguistics; Crowdsourcing; Extraction; Knowledge graph; Large datasets; Natural language processing systems; Quality control; Knowledge graphs; Language model; Language processing; Large language model; Natural language processing; Natural languages; Text annotations; Triple; Unstructured texts; Benchmarking},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Romanyshyn202451,
	author = {Romanyshyn, Nataliia and Chaplynskyi, Dmytro and Romanyshyn, Mariana},
	title = {Automated Extraction of Hypo-Hypernym Relations for the Ukrainian WordNet},
	year = {2024},
	pages = {51 - 60},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195201848&partnerID=40&md5=16c677bffb6b68022a75edcf3d9625c0},
	abstract = {WordNet is a crucial resource in linguistics and natural language processing, providing a detailed and expansive set of lexico-semantic relationships among words in a language. The trend toward automated construction and expansion of WordNets has become increasingly popular due to the high costs of manual development. This study aims to automate the development of the Ukrainian WordNet, explicitly concentrating on hypo-hypernym relations that are crucial building blocks of the hierarchical structure of WordNet. Utilizing the linking between Princeton WordNet, Wikidata, and multilingual resources from Wikipedia, the proposed approach successfully mapped 17% of Princeton WordNet (PWN) content to Ukrainian Wikipedia. Furthermore, the study introduces three innovative strategies for generating new entries to fill in the gaps of the Ukrainian WordNet: machine translation, the Hypernym Discovery model, and the Hypernym Instruction-Following LLaMA model. The latter model shows a high level of effectiveness, evidenced by a 41.61% performance on the Mean Overlap Coefficient (MOC) metric. With the proposed approach that combines automated techniques with expert human input, we provide a reliable basis for creating the Ukrainian WordNet. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Hypernym Discovery; Large Language Models; Lexicography; Ukrainian; Wordnet; Automation; Computational Linguistics; Natural Language Processing Systems; Ontology; Automated Extraction; Hypernym Discovery; Language Model; Language Processing; Large Language Model; Lexicography; Natural Languages; Ukrainian; Wikipedia; Wordnet; Semantics},
	keywords = {Automation; Computational linguistics; Natural language processing systems; Ontology; Automated extraction; Hypernym discovery; Language model; Language processing; Large language model; Lexicography; Natural languages; Ukrainian; Wikipedia; Wordnet; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{De Deyne202468,
	author = {De Deyne, Simon and Liu, Chunhua and Frermann, Lea},
	title = {Can GPT-4 Recover Latent Semantic Relational Information from Word Associations? A Detailed Analysis of Agreement with Human-annotated Semantic Ontologies},
	year = {2024},
	pages = {68 - 78},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195200635&partnerID=40&md5=52eb858d22ac985b39a0b3e6f7929335},
	abstract = {Word associations, i.e., spontaneous responses to a cue word, provide not only a window into the human mental lexicon but have also been shown to be a repository of common-sense knowledge and can underpin efforts in lexicography and the construction of dictionaries. Especially the latter tasks require knowledge about the relations underlying the associations (e.g., Taxonomic vs. Situational); however, to date, there is neither an established ontology of relations nor an effective labelling paradigm. Here, we test GPT-4’s ability to infer semantic relations for human-produced word associations. We use four human-labelled data sets of word associations and semantic features, with differing relation inventories and various levels of annotator agreement. We directly prompt GPT-4 with detailed relation definitions without further fine-tuning or training. Our results show that while GPT-4 provided a good account of higher-level classifications (e.g., Taxonomic vs Situational), prompting instructions alone cannot obtain similar performance for detailed classifications (e.g., superordinate, subordinate or coordinate relations) despite high agreement among human annotators. This suggests that latent relations can at least be partially recovered from word associations and highlights ways in which LLMs could be improved and human annotation protocols could adapted to reduce coding ambiguity. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Semantic Relations; Word Associations; Ontology; Commonsense Knowledge; Labeled Data; Labelings; Language Model; Large Language Model; Latent Semantics; Ontology's; Semantic Ontology; Semantic Relations; Word-association; Semantics},
	keywords = {Ontology; Commonsense knowledge; Labeled data; Labelings; Language model; Large language model; Latent semantics; Ontology's; Semantic ontology; Semantic relations; Word-association; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Tseng20248,
	author = {Tseng, Yuhsiang and Chen, Piner and Lian, Dachen and Hsieh, Shukai},
	title = {The semantic relations in LLMs: an information-theoretic compression approach},
	year = {2024},
	pages = {8 - 21},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195184478&partnerID=40&md5=4dee6a48db55f833b4bf8b62456ab68c},
	abstract = {Compressibility is closely related to the predictability of the texts from the information theory viewpoint. As large language models (LLMs) are trained to maximize the conditional probabilities of upcoming words, they may capture the subtlety and nuances of the semantic constraints underlying the texts, and texts aligning with the encoded semantic constraints are more compressible than those that do not. This paper systematically tests whether and how LLMs can act as compressors of semantic pairs. Using semantic relations from English and Chinese Wordnet, we empirically demonstrate that texts with correct semantic pairings are more compressible than incorrect ones, measured by the proposed compression advantages index. We also show that, with the Pythia model suite and a fine-tuned model on Chinese Wordnet, compression capacities are modulated by the model’s seen data. These findings are consistent with the view that LLMs encode the semantic knowledge as underlying constraints learned from texts and can act as compressors of semantic information or potentially other structured knowledge. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Arithmetic Encoding; Chinese Wordnet; Compression; Large Language Model; Lexical Resource; Compressors; Computational Linguistics; Encoding (symbols); Ontology; Signal Encoding; Arithmetic Encoding; Chinese Wordnet; Compression; Encodings; Language Model; Large Language Model; Lexical Resources; Semantic Constraints; Semantic Relations; Wordnet; Semantics},
	keywords = {Compressors; Computational linguistics; Encoding (symbols); Ontology; Signal encoding; Arithmetic encoding; Chinese wordnet; Compression; Encodings; Language model; Large language model; Lexical resources; Semantic constraints; Semantic relations; Wordnet; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Shichman20241,
	author = {Shichman, Mollie and Bonial, Claire and Hudson, Taylor and Blodgett, Austin and Ferraro, Francis and Rudinger, Rachel},
	title = {PropBank-Powered Data Creation: Utilizing Sense-Role Labelling to Generate Disaster Scenario Data},
	year = {2024},
	pages = {1 - 10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195120613&partnerID=40&md5=66650431c3d8a76710b527a23e8673ae},
	abstract = {For human-robot dialogue in a search-and-rescue scenario, a strong knowledge of the conditions and objects a robot will face is essential for effective interpretation of natural language instructions. In order to utilize the power of large language models without overwhelming the limited storage capacity of a robot, we propose PropBank-Powered Data Creation. PropBank-Powered Data Creation is an expert-in-the-loop data generation pipeline which creates training data for disaster-specific language models. We leverage semantic role labeling and Rich Event Ontology resources to efficiently develop seed sentences for fine-tuning a smaller, targeted model that could operate onboard a robot for disaster relief. We developed 32 sentence templates, which we used to make 2 seed datasets of 175 instructions for earthquake search and rescue and train derailment response. We further leverage our seed datasets as evaluation data to test our baseline fine-tuned models. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Fine-tuning; Object Affordances; Propbank; Synthetic Data Creation; Computational Linguistics; Disaster Prevention; Disasters; Robots; Semantics; Affordances; Data Creation; Fine Tuning; Labelings; Language Model; Object Affordance; Propbank; Search And Rescue; Synthetic Data; Synthetic Data Creation; Digital Storage},
	keywords = {Computational linguistics; Disaster prevention; Disasters; Robots; Semantics; Affordances; Data creation; Fine tuning; Labelings; Language model; Object affordance; Propbank; Search and rescue; Synthetic data; Synthetic data creation; Digital storage},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Saeedizade2024143,
	author = {Saeedizade, Mohammad Javad and Blomqvist, Eva},
	title = {Navigating Ontology Development with Large Language Models},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14664 LNCS},
	pages = {143 - 161},
	doi = {10.1007/978-3-031-60626-7_8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194261410&doi=10.1007%2F978-3-031-60626-7_8&partnerID=40&md5=baaad02d5b7b04faf346edad72360685},
	abstract = {Ontology engineering is a complex and time-consuming task, even with the help of current modelling environments. Often the result is error-prone unless developed by experienced ontology engineers. However, with the emergence of new tools, such as generative AI, inexperienced modellers might receive assistance. This study investigates the capability of Large Language Models (LLMs) to generate OWL ontologies directly from ontological requirements. Specifically, our research question centres on the potential of LLMs in assisting human modellers, by generating OWL modelling suggestions and alternatives. We experiment with several state-of-the-art models. Our methodology incorporates diverse prompting techniques like Chain of Thoughts (CoT), Graph of Thoughts (GoT), and Decomposed Prompting, along with the Zero-shot method. Results show that currently, GPT-4 is the only model capable of providing suggestions of sufficient quality, and we also note the benefits and drawbacks of the prompting techniques. Overall, we conclude that it seems feasible to use advanced LLMs to generate OWL suggestions, which are at least comparable to the quality of human novice modellers. Our research is a pioneering contribution in this area, being the first to systematically study the ability of LLMs to assist ontology engineers. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Llm; Ontology; Ontology Engineering; Birds; Computational Linguistics; Zero-shot Learning; Current Modeling; Error Prones; Language Model; Large Language Model; Modeling Environments; Ontology Development; Ontology Engineering; Ontology's; Owl Ontologies; Time-consuming Tasks; Ontology},
	keywords = {Birds; Computational linguistics; Zero-shot learning; Current modeling; Error prones; Language model; Large language model; Modeling environments; Ontology development; Ontology engineering; Ontology's; OWL ontologies; Time-consuming tasks; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14}
}

@ARTICLE{2024,
	title = {21st European Semantic Web Conference, ESWC 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14664 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194235501&partnerID=40&md5=65e20693726d830550adc9ad39116169},
	abstract = {The proceedings contain 32 papers. The special focus in this conference is on Semantic Web. The topics include: Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction; QAGCN: Answering Multi-relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs; leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs; a Language Model Based Framework for New Concept Placement in Ontologies; low-Dimensional Hyperbolic Knowledge Graph Embedding for Better Extrapolation to Under-Represented Data; SC-Block: Supervised Contrastive Blocking Within Entity Resolution Pipelines; navigating Ontology Development with Large Language Models; ESLM: Improving Entity Summarization by Leveraging Language Models; explanation of Link Predictions on Knowledge Graphs via Levelwise Filtering and Graph Summarization; Large Language Models for Scientific Question Answering: An Extensive Analysis of the SciQA Benchmark; efficient Evaluation of Conjunctive Regular Path Queries Using Multi-way Joins; can Contrastive Learning Refine Embeddings; automation of Electronic Invoice Validation Using Knowledge Graph Technologies; towards Cyber Mapping the German Financial System with Knowledge Graphs; integrating Domain Knowledge for Enhanced Concept Model Explainability in Plant Disease Classification; generative Expression Constrained Knowledge-Based Decoding for Open Data. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Dong202479,
	author = {Dong, Hang and Chen, Jiaoyan and He, Yuan and Gao, Yongsheng and Horrocks, Ian},
	title = {A Language Model Based Framework for New Concept Placement in Ontologies},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14664 LNCS},
	pages = {79 - 99},
	doi = {10.1007/978-3-031-60626-7_5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194222248&doi=10.1007%2F978-3-031-60626-7_5&partnerID=40&md5=7646da39abb6a08ff3ab8c32975bb31e},
	abstract = {We investigate the task of inserting new concepts extracted from texts into an ontology using language models. We explore an approach with three steps: edge search which is to find a set of candidate locations to insert (i.e., subsumptions between concepts), edge formation and enrichment which leverages the ontological structure to produce and enhance the edge candidates, and edge selection which eventually locates the edge to be placed into. In all steps, we propose to leverage neural methods, where we apply embedding-based methods and contrastive learning with Pre-trained Language Models (PLMs) such as BERT for edge search, and adapt a BERT fine-tuning-based multi-label Edge-Cross-encoder, and Large Language Models (LLMs) such as GPT series, FLAN-T5, and Llama 2, for edge selection. We evaluate the methods on recent datasets created using the SNOMED CT ontology and the MedMentions entity linking benchmark. The best settings in our framework use fine-tuned PLM for search and a multi-label Cross-encoder for selection. Zero-shot prompting of LLMs is still not adequate for the task, and we propose explainable instruction tuning of LLMs for improved performance. Our study shows the advantages of PLMs and highlights the encouraging performance of LLMs that motivates future studies. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Placement; Large Language Models; Ontology Enrichment; Pre-trained Language Models; Snomed Ct; Computational Linguistics; Signal Encoding; Zero-shot Learning; Concept Placement; Edge Searches; Edge Selection; Language Model; Large Language Model; Ontology Enrichment; Ontology's; Pre-trained Language Model; Snomed-ct; Ontology},
	keywords = {Computational linguistics; Signal encoding; Zero-shot learning; Concept placement; Edge searches; Edge selection; Language model; Large language model; Ontology enrichment; Ontology's; Pre-trained language model; SNOMED-CT; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{Zhang2024197,
	author = {Zhang, Hao and Mohammed, Abrar and Dimitrova, Vania G.},
	title = {Weakly Supervised Short Text Classification for Characterising Video Segments},
	year = {2024},
	journal = {International Conference on Computer Supported Education, CSEDU - Proceedings},
	volume = {2},
	pages = {197 - 204},
	doi = {10.5220/0012618600003693},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193977562&doi=10.5220%2F0012618600003693&partnerID=40&md5=d5565c649b1812f0a8d822132017b8ab},
	abstract = {In this age of life-wide learning, video-based learning has increasingly become a crucial method of education. However, the challenge lies in watching numerous videos and connecting key points from these videos with relevant study domains. This requires video characterization. Existing research on video characterization focuses on manual or automatic methods. These methods either require substantial human resources (experts to identify domain related videos and domain related areas in the videos) or rely on learner input (by relating video parts to their learning), often overlooking the assessment of their effectiveness in aiding learning. Manual methods are subjective, prone to errors and time consuming. Automatic supervised methods require training data which in many cases is unavailable. In this paper we propose a weakly supervised method that utilizes concepts from an ontology to guide models in thematically classifying and characterising video segments. Our research is concentrated in the health domain, conducting experiments with several models, including the large language model GPT-4. The results indicate that CorEx significantly outperforms other models, while GLDA and Guided BERTopic show limitations in this task. Although GPT-4 demonstrates consistent performance, it still falls behind CorEx. This study offers an innovative perspective in video-based learning, especially in automating the detection of learning themes in video content. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Model; Video-based Learning; Weakly Supervised Text Classification; Classification (of Information); Computational Linguistics; Computer Vision; Learning Systems; Supervised Learning; Text Processing; Language Model; Large Language Model; Manual Methods; Short Text Classifications; Supervised Methods; Text Classification; Video Characterization; Video Segments; Video-based Learning; Weakly Supervised Text Classification; Image Segmentation},
	keywords = {Classification (of information); Computational linguistics; Computer vision; Learning systems; Supervised learning; Text processing; Language model; Large language model; Manual methods; Short text classifications; Supervised methods; Text classification; Video characterization; Video segments; Video-based learning; Weakly supervised text classification; Image segmentation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Lopes2024141,
	author = {Lopes, Alcides Gonçalves and Carbonera, Joel Lúis and Santos, Nicolau O. and Rodrigues, Fabrício Henrique Henrique and Garcia, Luan Fonseca and Abel, Mara},
	title = {Cross-Domain Classification of Domain Entities into Top-Level Ontology Concepts Using BERT: A Study Case on the BFO Domain Ontologies},
	year = {2024},
	journal = {International Conference on Enterprise Information Systems, ICEIS - Proceedings},
	volume = {2},
	pages = {141 - 148},
	doi = {10.5220/0012557600003690},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193935708&doi=10.5220%2F0012557600003690&partnerID=40&md5=e2743defc57c09f037e5c2bb310f611c},
	abstract = {Classifying domain entities into top-level ontology concepts using informal definitions remains an active research area with several open questions. One of these questions pertains to the quality of proposed pipelines employing language models for classifying informal definitions when training and testing samples are from different knowledge domains. This can introduce challenges due to varying vocabularies across domains or the potential for an entity to belong to different top-level concepts based on its domain. In this study, we present a study case where terms and informal definitions are extracted from 81 domain ontologies organized into 12 knowledge domains. We investigate the performance of a pipeline that utilizes the BERT language model for classifying domain entities into top-level concepts within a cross-domain classification scenario. Additionally, we explore various pipeline setups for input, preprocessing, and training steps. Our optimal classifier setup employs an unbalanced training methodology, no text preprocessing, and the concatenation of terms and informal definitions as input. Furthermore, we demonstrate that BERT yields promising results in classifying domain entities into top-level concepts within a cross-domain classification scenario. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cross-domain Classification; Informal Definition; Language Model; Top-level Ontology Classification; Computational Linguistics; Knowledge Management; Pipelines; Cross-domain; Cross-domain Classification; Domain Entities; Domain Ontologies; Informal Definition; Language Model; Ontology Concepts; Ontology's; Study Case; Top-level Ontology Classification; Ontology},
	keywords = {Computational linguistics; Knowledge management; Pipelines; Cross-domain; Cross-domain classification; Domain entities; Domain ontologies; Informal definition; Language model; Ontology concepts; Ontology's; Study case; Top-level ontology classification; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Zhang20242594,
	author = {Zhang, Shiyao and Dong, Yuji and Zhang, Yichuan and Payne, Terry R. and Zhang, Jie},
	title = {Large Language Model Assisted Multi-Agent Dialogue for Ontology Alignment},
	year = {2024},
	journal = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
	volume = {2024-May},
	pages = {2594 - 2596},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192997204&partnerID=40&md5=5a51d0a74903a3408ce05f2f077d62e6},
	abstract = {Ontology alignment is critical in cross-domain integration; however, it typically necessitates the involvement of a human domain-expert, which can make the task costly. Although a variety of machine-learning approaches have been proposed that can simplify this task by learning the patterns from experts, such techniques are still susceptible to domain knowledge updates that could potentially change the patterns and lead to extra expert involvement. The use of Large Language Models (LLMs) has demonstrated a general cognitive ability, which has the potential to assist ontology alignment from the cognition level, thus obviating the need for costly expert involvement. However, the process by which the output of LLMs is generated can be opaque and thus the reliability and interpretability of such models is not always predictable. This paper proposes a dialogue model, in which multiple agents negotiate the correspondence between two knowledge sets with the support from an LLM. We demonstrate that this approach not only reduces the need for the involvement of a domain expert for ontology alignment, but that the results are interpretable despite the use of LLMs. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Dialogue; Large Language Model; Multi-agent System; Negotiation; Ontology Alignment; Autonomous Agents; Computational Linguistics; Domain Knowledge; Learning Systems; Ontology; Cross-domain; Dialog; Domain Experts; Human Domain; Language Model; Large Language Model; Machine Learning Approaches; Multi Agent; Negotiation; Ontology Alignment; Multi Agent Systems},
	keywords = {Autonomous agents; Computational linguistics; Domain Knowledge; Learning systems; Ontology; Cross-domain; Dialog; Domain experts; Human domain; Language model; Large language model; Machine learning approaches; Multi agent; Negotiation; Ontology alignment; Multi agent systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Zha2024,
	author = {Zha, Yue and Ke, Yuanzhi and Hu, Xiao and Xiong, Caiquan},
	title = {Ontology Attention Layer for Medical Named Entity Recognition},
	year = {2024},
	journal = {Applied Sciences (Switzerland)},
	volume = {14},
	number = {1},
	pages = {},
	doi = {10.3390/app14010421},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192459633&doi=10.3390%2Fapp14010421&partnerID=40&md5=72fe34f834a6c898ae5c6bc2ae6147e7},
	abstract = {Named entity recognition (NER) is particularly challenging for medical texts due to the high domain specificity, abundance of technical terms, and sparsity of data in this field. In this work, we propose a novel attention layer, called the “ontology attention layer”, that enhances the NER performance of a language model for clinical text by utilizing an ontology consisting of conceptual classes related to the target entity set. The proposed layer computes the relevance between each input token and the classes in the ontology and then fuses the encoded token vectors and the class vectors to enhance the token vectors by explicit superior knowledge. In our experiments, we apply the proposed layer to various language models for an NER task based on a Chinese clinical dataset to evaluate the performance of the layer. We also investigate the influence of the granularity of the classes utilized in the ontology attention layer. The experimental results show that the proposed ontology attention layer improved F1 scores by 0.4% to 0.5%. The results suggest that the proposed method is an effective approach to improving the NER performance of existing language models for clinical datasets. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Attention Mechanism; Clinical Text Mining; Named Entity Recognition; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access}
}

@CONFERENCE{Chatterjee2024570,
	author = {Chatterjee, Ram and Pandey, Mrinal and Thakur, Hardeo Kumar and Gupta, Anand},
	title = {Checking Counterfeit Critiques on Commodities using Ensemble Classifiers Enhancing Information Credibility},
	year = {2024},
	journal = {Procedia Computer Science},
	volume = {233},
	pages = {570 - 579},
	doi = {10.1016/j.procs.2024.03.246},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192447328&doi=10.1016%2Fj.procs.2024.03.246&partnerID=40&md5=473d7399dc52cf91230a54899c953191},
	abstract = {The conundrum of the ubiquitous deceptive reviews has overruled the online ontology with the obsession of obscure but obligatory posting of product reviews for the customers to believe, behold and beget the online product marketing. This mandates contemporary research in the direction to delve deeper on the application and analysis of deceiving online reviews with matured and advanced AI models functional on large scale datasets to effectively and efficiently demarcate between the genuine and the sham. The research counteracts the counterfeiting product reviews via the applications, assessment and analysis of the befitting AI models - Elastic-net Classifier model based on block coordinate descent with Wordcloud and its further performance enhancement through LightGBM Trees Classifier with Grid Search and Early Stopping support, with Log-Loss as performance metric for experimentation to gain insight into the intricacies of detection, diagnosis and diminution of fake product reviews. The paper also delineates discriminative and affirmative aspects of the dataset quality, statistics, stability and standards inherent and coherent to the creation of the dataset using Large Language Models (LLMs) intrinsic to the zeitgeist juncture of recent times promoting machines to produce large scale, cost effective bogus reviews in lieu of the Amazon Mechanical Turks. The results obtained with the Log-Loss holdout score of 0.1462 conforming the LightGBM classifier proves its performance better than the Elastic-Net classifier, conforming it as better than the ROC-AUC in terms of its proximity to the prediction probability for the matching actual/true value. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Amazon Mechanical Turks; Elastic-net Classifier; Information Credibility; Large Language Models; Lightgbm Trees Classifier; Classification (of Information); Computational Linguistics; Fake Detection; Large Datasets; Amazon's Mechanical Turks; Elastic Net; Elastic-net Classifier; Ensemble-classifier; Information Credibilities; Language Model; Large Language Model; Lightgbm Tree Classifier; Product Reviews; Tree Classifiers; Cost Effectiveness},
	keywords = {Classification (of information); Computational linguistics; Fake detection; Large datasets; Amazon's mechanical turks; Elastic net; Elastic-net classifier; Ensemble-classifier; Information credibilities; Language model; Large language model; Lightgbm tree classifier; Product reviews; Tree classifiers; Cost effectiveness},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Vizcarra2024231,
	author = {Vizcarra, Julio and Haruta, Shuichiro and Kurokawa, Mori},
	title = {Representing the Interaction between Users and Products via LLM-assisted Knowledge Graph Construction},
	year = {2024},
	journal = {Proceedings - IEEE International Conference on Semantic Computing, ICSC},
	pages = {231 - 232},
	doi = {10.1109/ICSC59802.2024.00043},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192226431&doi=10.1109%2FICSC59802.2024.00043&partnerID=40&md5=1e6b06e16cb0ebab11075d09212e1302},
	abstract = {To understand user behavior, representing the semantic knowledge of user-product interaction is essential. In this paper, we represent the interaction between user and product via large language model (LLM)-assisted knowledge graph construction. We capture users' behavioral actions and static properties of the products from raw text data of 'user review' and 'product catalog'. Moreover, the information needed for updating the knowledge graph is captured by raw texts of 'news related to the products'. The proposed methodology integrates them as a single knowledge graph to provide causal reasoning on user-product interaction. To alleviate the situation where a small quantity of annotated text exists in these data, we use LLM as a data annotator and augmentor. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Causality; Knowledge Graph; Llm; Ontology; Text Mining; User-product Interaction; Behavioral Research; Semantics; Causality; Graph Construction; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Product Interaction; Text-mining; User Behaviors; User-product Interaction; Knowledge Graph},
	keywords = {Behavioral research; Semantics; Causality; Graph construction; Knowledge graphs; Language model; Large language model; Ontology's; Product interaction; Text-mining; User behaviors; User-product interaction; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Bayaga202420,
	author = {Bayaga, Anass},
	title = {Advancing STEM cognition with current AI landscape and systems},
	year = {2024},
	pages = {20 - 25},
	doi = {10.1109/ICTAS59620.2024.10507138},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192222653&doi=10.1109%2FICTAS59620.2024.10507138&partnerID=40&md5=2d791d1c9b369805a112a58311f8f839},
	abstract = {Application of AI explores the potential of algorithms to ensure fairness, accuracy, and efficiency in grading students' performance, offering valuable insights into their strengths and areas for improvement. While the current AI landscape showcases remarkable progress, there are several areas ripe for exploration. One such avenue is AI steps to consider in STEM, wherein researchers aim to develop specialised steps/models to understand and generate domain-specific STEM content. The systematic literature review highlighted the importance of domain adaptation techniques for enhancing STEM comprehension by fine-tuning transformer-based language models like BERT. Integrating domain knowledge through ontology-based and context of STEM disciplines. Future research should focus on building domain-specific annotated datasets to improve the performance models in STEM comprehension. Additionally, exploring unsupervised domain adaptation techniques and leveraging domain-specific knowledge graphs can further enhance the NLP models' adaptability to diverse STEM domains. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai-augmented Laboratories; Artificial Intelligence; Explainable Ai; Natural Language Processing; Stem Education; Grading; Knowledge Graph; Knowledge Management; Natural Language Processing Systems; Ontology; 'current; Adaptation Techniques; Ai-augmented Laboratory; Domain Adaptation; Domain Specific; Explainable Ai; Language Processing; Natural Language Processing; Natural Languages; Stem Education; Domain Knowledge},
	keywords = {Grading; Knowledge graph; Knowledge management; Natural language processing systems; Ontology; 'current; Adaptation techniques; AI-augmented laboratory; Domain adaptation; Domain specific; Explainable AI; Language processing; Natural language processing; Natural languages; STEM education; Domain Knowledge},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2024,
	title = {International workshops which were held in conjunction with 5th International Conference on Process Mining, ICPM 2023},
	year = {2024},
	journal = {Lecture Notes in Business Information Processing},
	volume = {503 LNBIP},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192162695&partnerID=40&md5=091f4fa03f88eba8ca553dc631f3b60a},
	abstract = {The proceedings contain 38 papers. The special focus in this conference is on Process Mining. The topics include: NICE: The Native IoT-Centric Event Log Model for Process Mining; CaseID Detection for Process Mining: A Heuristic-Based Methodology; parallelism-Based Session Creation to Identify High-Level Activities in Event Log Abstraction; From OCEL to DOCEL – Datasets and Automated Transformation; event Knowledge Graphs for Auditing: A Case Study; turning Logs into Lumber: Preprocessing Tasks in Process Mining; improving Precision in Process Trees Using Subprocess Tree Logs; generating Process Anomalies with Markov Chains: A Pattern-Driven Approach; sparse Mixtures of Shallow Linear Experts for Interpretable and Fast Outcome Prediction; understanding the Impact of Design Choices on the Performance of Predictive Process Monitoring; discovering Process-Based Drivers for Case-Level Outcome Explanation; detecting Anomalous Events in Object-Centric Business Processes via Graph Neural Networks; uncovering the Hidden Significance of Activities Location in Predictive Process Monitoring; using Process Mining to Explore the Impact of Socio-economic Status on the Treatment of Musculoskeletal Disorders – A Case Study; identifying Variation in Personal Daily Routine Through Process Mining: A Case Study; investigating an Ontology-Informed Approach to Event Log Generation in Healthcare; I-PALIA: Discovering BPMN Processes with Duplicated Activities for Healthcare Domains; enhancing Clinical Insights: Knowledge-Intensive and Context-Sensitive Process Instance Visualization in Health Care; hypergraphs for Frailty Analysis Research Paper; ontology-Based Multi-perspective Process Mining in Laboratories: A Case Study; clinical Event Knowledge Graphs: Enriching Healthcare Event Data with Entities and Clinical Concepts - Research Paper; error-Correcting Methodology for Evaluating Compliance to Clinical Guidelines: A Case Study on Rectal Cancer; exploring Gamification in Process Mining Education: Towards a Playful and Engaging Approach; process Mining Techniques for Collusion Detection in Online Exams; evidence-Based Student Career and Performance Analysis with Process Mining: A Case Study; extracting Rules from Event Data for Study Planning; LLMs and Process Mining: Challenges in RPA; checking Constraints for Object-Centric Process Executions. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Sivarajkumar2024,
	author = {Sivarajkumar, Sonish and Gao, Fengyi and Denny, Parker and Aldhahwani, Bayan and Visweswaran, Shyam and Bove, Allyn M. and Wang, Yanshan},
	title = {Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study},
	year = {2024},
	journal = {JMIR Medical Informatics},
	volume = {12},
	pages = {},
	doi = {10.2196/52289},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191372441&doi=10.2196%2F52289&partnerID=40&md5=b8818bb721f14375018df658da7f245d},
	abstract = {Background: The rehabilitation of a patient who had a stroke requires precise, personalized treatment plans. Natural language processing (NLP) offers the potential to extract valuable exercise information from clinical notes, aiding in the development of more effective rehabilitation strategies. Objective: This study aims to develop and evaluate a variety of NLP algorithms to extract and categorize physical rehabilitation exercise information from the clinical notes of patients who had a stroke treated at the University of Pittsburgh Medical Center. Methods: A cohort of 13,605 patients diagnosed with stroke was identified, and their clinical notes containing rehabilitation therapy notes were retrieved. A comprehensive clinical ontology was created to represent various aspects of physical rehabilitation exercises. State-of-the-art NLP algorithms were then developed and compared, including rule-based, machine learning–based algorithms (support vector machine, logistic regression, gradient boosting, and AdaBoost) and large language model (LLM)–based algorithms (ChatGPT [OpenAI]). The study focused on key performance metrics, particularly F<inf>1</inf>-scores, to evaluate algorithm effectiveness. Results: The analysis was conducted on a data set comprising 23,724 notes with detailed demographic and clinical characteristics. The rule-based NLP algorithm demonstrated superior performance in most areas, particularly in detecting the “Right Side” location with an F<inf>1</inf>-score of 0.975, outperforming gradient boosting by 0.063. Gradient boosting excelled in “Lower Extremity” location detection (F<inf>1</inf>-score: 0.978), surpassing rule-based NLP by 0.023. It also showed notable performance in the “Passive Range of Motion” detection with an F<inf>1</inf>-score of 0.970, a 0.032 improvement over rule-based NLP. The rule-based algorithm efficiently handled “Duration,” “Sets,” and “Reps” with F<inf>1</inf>-scores up to 0.65. LLM-based NLP, particularly ChatGPT with few-shot prompts, achieved high recall but generally lower precision and F<inf>1</inf>-scores. However, it notably excelled in “Backward Plane” motion detection, achieving an F<inf>1</inf>-score of 0.846, surpassing the rule-based algorithm’s 0.720. Conclusions: The study successfully developed and evaluated multiple NLP algorithms, revealing the strengths and weaknesses of each in extracting physical rehabilitation exercise information from clinical notes. The detailed ontology and the robust performance of the rule-based and gradient boosting algorithms demonstrate significant potential for enhancing precision rehabilitation. These findings contribute to the ongoing efforts to integrate advanced NLP techniques into health care, moving toward predictive models that can recommend personalized rehabilitation treatments for optimal patient outcomes. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Chatgpt; Electronic Health Records; Exercise; Machine Learning; Natural Language Processing; Physical Exercise; Physical Rehabilitation; Rehabilitation; Rehabilitation Therapy; Stroke},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Lee2024,
	author = {Lee, Changshing and Wang, Meihui and Chiang, Junkui and Kubota, Naoyuki and Sato-Shimokawara, Eri Shimokawara and Nojima, Yusuke and Acampora, Giovanni and Wu, Peiyu and Chiu, Szuchi and Yang, Shengchi},
	title = {Quantum Computational Intelligence with Generative AI Image for Human-Machine Interaction},
	year = {2024},
	journal = {IEEE International Conference on Fuzzy Systems},
	pages = {},
	doi = {10.1109/FUZZ-IEEE60900.2024.10611970},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191275657&doi=10.1109%2FFUZZ-IEEE60900.2024.10611970&partnerID=40&md5=a71e06505cbc4d920a6032f50764c4f5},
	abstract = {This paper introduces a Quantum Computational Intelligence (QCI) agent equipped with a content attention ontology model, specifically designed to enhance human-machine interaction based on a Generative Artificial Intelligence (GAI) image generation agent for Taiwanese/English learning and experience. Its diverse primary applications include social media analysis on Facebook groups and YouTube learning videos related to the 2023 IEEE CIS Education Portal (EP) Subcommittee, as well as in the areas of Taiwanese/English language learning and dialogue experience with GAI image generation. To establish the knowledge and inference models for the QCI agent, we initially developed a Taiwanese/English learning and experience ontology, including a content attention ontology, and an image attention ontology. The QCI agent utilizes metrics such as the number of views, posts, and comments to predict the fuzzy number of reactions. In addition, the GAI image agent generates Taiwanese speech-based/English text-based images and evaluates the fuzzy similarity score between Taiwanese/English and the attention ontology together with the Sentence BERT (SBERT) agent. This Taiwanese/English fuzzy similarity score is further validated through human assessments, with these evaluations subsequently serving as an additional metric for comparative analysis of Human-Machine Interaction (HMI). Furthermore, the GAI image agent is designed to create images and Chinese/English texts from text/speech translated by the Meta AI Universal Speech Translator (UST) Taiwanese/English agent. A Particle Swarm Optimization (PSO)-based machine learning mechanism is employed to train the QCI model for assessing learners' performance and predicting the performance of others. The National University of Tainan (NUTN) Taiwan-Large Language Model (NUTN.TW-LLM) agent has been further enhanced to support interactive learning experiences for HMI. An SBERT-based assessment agent is used to calculate fuzzy similarities between questions and answers in Taiwanese/English experiences and dialogues. Experimental results demonstrate the feasibility and efficacy of the proposed QCI model, equipped with QCI&AI-FML (Artificial Intelligence-Fuzzy Markup Language) and machine learning capabilities, for social media and language learning applications on HMI. In the future, we will extend the QCI model to various HMI applications for student learning around the world. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Content Attention Ontology; Fuzzy Markup Language; Generative Ai Image Agent; Ieee Cis Education Portal; Nutn.tw-llm; Quantum Ci Agent; Sentence Bert; Computational Linguistics; Computer Aided Language Translation; Curricula; Fuzzy Inference; Fuzzy Rules; Gene Transfer; Intelligent Systems; Machine Learning; Motion Estimation; Neural Networks; Personnel Training; Problem Oriented Languages; Semantics; Sgml; Students; Swarm Intelligence; Syntactics; Chatgpt; Ci-agent; Content Attention Ontology; Education Portals; Fuzzy Markup Languages; Generative Ai Image Agent; Ieee Cis Education Portal; National University Of Tainan.; Ontology's; Quantum Ci Agent; Sentence Bert; Tw-llm; Speech Enhancement},
	keywords = {Computational linguistics; Computer aided language translation; Curricula; Fuzzy inference; Fuzzy rules; Gene transfer; Intelligent systems; Machine learning; Motion estimation; Neural networks; Personnel training; Problem oriented languages; Semantics; SGML; Students; Swarm intelligence; Syntactics; ChatGPT; CI-Agent; Content attention ontology; Education portals; Fuzzy markup languages; Generative AI image agent; IEEE CIS education portal; National university of tainan.; Ontology's; Quantum CI agent; Sentence BERT; TW-LLM; Speech enhancement},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{2024,
	title = {30th International Working Conference on Requirements Engineering: Foundation for Software Quality, REFSQ 2024},
	year = {2024},
	journal = {Lecture Notes in Computer Science},
	volume = {14588 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190711457&partnerID=40&md5=8a1ac6e4ce36f9ac3846b7874da487cc},
	abstract = {The proceedings contain 22 papers. The special focus in this conference is on Requirements Engineering: Foundation for Software Quality. The topics include: Assessing the Understandability and Acceptance of Attack-Defense Trees for Modelling Security Requirements; learning to Rank Privacy Design Patterns: A Semantic Approach to Meeting Privacy Requirements; a New Usability Inspection Method: Experience-Based Analysis; governance-Focused Classification of Security and Privacy Requirements from Obligations in Software Engineering Contracts; what Impact Do My Preferences Have?; Candidate Solutions for Defining Explainability Requirements of AI Systems; Opportunities and Limitations of AI in Human-Centered Design a Research Preview; A Tertiary Study on AI for Requirements Engineering; Exploring LLMs’ Ability to Detect Variability in Requirements; Designing NLP-Based Solutions for Requirements Variability Management: Experiences from a Design Science Study at Visma; Natural2CTL: A Dataset for Natural Language Requirements and Their CTL Formal Equivalents; Towards a Comprehensive Ontology for Requirements Engineering for AI-Powered Systems; Operationalizing Machine Learning Using Requirements-Grounded MLOps; unveiling Competition Dynamics in Mobile App Markets Through User Reviews; exploring the Automatic Classification of Usage Information in Feedback; channeling the Voice of the Crowd: Applying Structured Queries in User Feedback Collection; requirements Information in Backlog Items: Content Analysis; Requirements Engineering for No-Code Development (RE4NCD); behavior-Driven Specification in Practice: An Experience Report; the Return of Formal Requirements Engineering in the Era of Large Language Models. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang2024300,
	author = {Zhang, Ruipeng and Xie, Mengjun},
	title = {ForensiQ: A Knowledge Graph Question Answering System for IoT Forensics},
	year = {2024},
	journal = {Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
	volume = {571 LNICST},
	pages = {300 - 314},
	doi = {10.1007/978-3-031-56583-0_20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190706066&doi=10.1007%2F978-3-031-56583-0_20&partnerID=40&md5=bd323f66332f4180b300f6ececf326b7},
	abstract = {The increasing number of attacks against the Internet of Things (IoT) has made IoT forensics critically important for reporting and mitigating cyber incidents and crimes. However, the heterogeneity of IoT environments and the complexity and volume of IoT data present significant challenges to forensic practitioners. The advent of question answering (QA) systems and large language models (LLM) offers a potential solution to accessing sophisticated IoT forensic knowledge and data. In light of this, we propose ForensiQ, a framework based on knowledge graph question answering (KGQA), to help investigators navigate complex IoT forensic artifacts and cybersecurity knowledge. Our framework integrates knowledge graphs (KG) into the IoT forensic workflow to better organize and analyze forensic artifacts. We also have developed a novel KGQA model that serves as a natural-language user interface to the IoT forensic KG. Our evaluation results show that, compared to existing KGQA models, ForensiQ demonstrates higher accuracy in answering natural language questions when applied to our experimental IoT forensic KG. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Digital Forensics; Internet Of Things; Knowledge Graph; Ontology Design; Question Answering; Computer Forensics; Cybersecurity; Electronic Crime Countermeasures; Knowledge Graph; Natural Language Processing Systems; User Interfaces; Cyber Security; Evaluation Results; Forensic Practitioner; Knowledge Graphs; Language Model; Natural Languages; Ontology Design; Question Answering; Question Answering Systems; Work-flows; Internet Of Things},
	keywords = {Computer forensics; Cybersecurity; Electronic crime countermeasures; Knowledge graph; Natural language processing systems; User interfaces; Cyber security; Evaluation results; Forensic practitioner; Knowledge graphs; Language model; Natural languages; Ontology design; Question Answering; Question answering systems; Work-flows; Internet of things},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Vladika2024376,
	author = {Vladika, Juraj and Fichtl, Alexander and Matthes, Florian},
	title = {Diversifying Knowledge Enhancement of Biomedical Language Models Using Adapter Modules and Knowledge Graphs},
	year = {2024},
	journal = {International Conference on Agents and Artificial Intelligence},
	volume = {2},
	pages = {376 - 387},
	doi = {10.5220/0012395200003636},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190687933&doi=10.5220%2F0012395200003636&partnerID=40&md5=089b69049d638af1b2cb5f61460e2699},
	abstract = {Recent advances in natural language processing (NLP) owe their success to pre-training language models on large amounts of unstructured data. Still, there is an increasing effort to combine the unstructured nature of LMs with structured knowledge and reasoning. Particularly in the rapidly evolving field of biomedical NLP, knowledge-enhanced language models (KELMs) have emerged as promising tools to bridge the gap between large language models and domain-specific knowledge, considering the available biomedical knowledge graphs (KGs) curated by experts over the decades. In this paper, we develop an approach that uses lightweight adapter modules to inject structured biomedical knowledge into pre-trained language models (PLMs). We use two large KGs, the biomedical knowledge system UMLS and the novel biochemical ontology OntoChem, with two prominent biomedical PLMs, BERT and BioLinkBERT. The approach includes partitioning knowledge graphs into smaller subgraphs, fine-tuning adapter modules for each subgraph, and combining the knowledge in a fusion layer. We test the performance on three downstream tasks: document classification, question answering, and natural language inference. We show that our methodology leads to performance improvements in several instances while keeping requirements in computing power low. Finally, we provide a detailed interpretation of the results and report valuable insights for future work. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Adapters; Biomedical Nlp; Biomedicine; Domain Knowledge; Knowledge Enhancement; Knowledge Graphs; Natural Language Processing (nlp); Pre-trained Language Models},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Alarcia2024334,
	author = {Alarcia, Ramon Maria García and Russo, Pietro and Renga, Alfredo and Golkar, Alessandro},
	title = {Bringing Systems Engineering Models to Large Language Models: An Integration of OPM with an LLM for Design Assistants},
	year = {2024},
	journal = {International Conference on Model-Driven Engineering and Software Development},
	volume = {1},
	pages = {334 - 345},
	doi = {10.5220/0012621900003645},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190371259&doi=10.5220%2F0012621900003645&partnerID=40&md5=9df3b55673b54310d9670b41c0b23b8c},
	abstract = {Although showing remarkable zero-shot and few-shot capabilities across a wide variety of tasks, Large Language Models (LLMs) are still not mature enough for off-the-shelf use in engineering design tasks. Organizations implementing model-based systems engineering practices into their product development processes can leverage on ontologies, models, and procedures to enhance LLMs applied to engineering design tasks. We present a methodology to integrate an Object-Process Methodology model of a space system into an LLMbased spacecraft design assistant and show a performance improvement, as compared to a conventional LLM. The benchmark is evaluated through subjective expert-assessed and an objective cosine-similarity-based criteria. The results motivate additional efforts in integrating Model-Based Systems Engineering practice into LLMs as means to improve their performance and reduce shortcomings such as hallucinations and black-box, untraceable behavior. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Design Assistant; Engineering Design; Large Language Models; Model-based Systems Engineering; Object-process Methodology; Systems Engineering},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Sadik2024149,
	author = {Sadik, Ahmed R. and Brulin, Sebastian and Olhofer, Markus},
	title = {Coding by Design: GPT-4 Empowers Agile Model Driven Development},
	year = {2024},
	journal = {International Conference on Model-Driven Engineering and Software Development},
	volume = {1},
	pages = {149 - 158},
	doi = {10.5220/0012356100003645},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190369863&doi=10.5220%2F0012356100003645&partnerID=40&md5=6d78d4c309cf1965bf07de35e525190f},
	abstract = {Generating code from a natural language using Large Language Models (LLMs) such as ChatGPT, seems groundbreaking. Yet, with more extensive use, it's evident that this approach has its own limitations. The inherent ambiguity of natural language proposes challenges to auto-generate synergistically structured artifacts that can be deployed. Model Driven Development (MDD) is therefore being highlighted in this research as a proper approach to overcome these challenges. Accordingly, we introduced an Agile Model- Driven Development (AMDD) approach that enhances code auto-generation using OpenAI's GPT-4. Our work emphasizes "Agility" as a significant contribution to the current MDD approach, particularly when the model undergoes changes or needs deployment in a different programming language. Thus, we presented a case-study showcasing a multi-agent simulation system of an Unmanned Vehicle Fleet (UVF). In the first and second layer of our proposed approach, we modelled the structural and behavioural aspects of the case-study using Unified Modeling Language (UML). In the next layer, we introduced two sets of meta-modelling constraints that minimize the model ambiguity. Object Constraints Language (OCL) is applied to fine-tune the code constructions details, while FIPA ontology is used to shape the communication semantics. Ultimately, GPT-4 is used to auto-generate code from the model in both Java and Python. The Java code is deployed within the JADE framework, while the Python code is deployed in PADE framework. Concluding our research, we engaged in a comprehensive evaluation of the generated code. From a behavioural standpoint, the auto-generated code not only aligned with the expected UML sequence diagram, but also added new behaviours that improved the interaction among the classes. Structurally, we compared the complexity of code derived from UML diagrams constrained solely by OCL to that influenced by both OCL and FIPA-ontology. Results showed that ontology-constrained model produced inherently more intricate code, however it remains manageable. Thus, other constraints can still be added to the model without passing the complexity high risk threshold. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai-empowered; Auto-generated Code; Cyclomatic Complexity; Gpt-4; Model Driven Development; Object Constraint Language; Ontology-constrained Class Diagram},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Ellsworth20247,
	author = {Ellsworth, Shannon},
	title = {Terms and references: Working definitions of terms and references},
	year = {2024},
	pages = {7 - 32},
	doi = {10.1016/B978-0-44-315991-6.00008-X},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189996763&doi=10.1016%2FB978-0-44-315991-6.00008-X&partnerID=40&md5=5830353ba60f12c76f8f36a0e6ef1011},
	abstract = {The words used to describe ethical computing concepts can be as controversial as the concepts themselves. To standardize language for shared understanding between the readers and authors, key terms and references used throughout this book are defined in this chapter. These definitions use descriptions and characterizations as seen by the authors and editors and are not meant to be exhaustive definitions. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Answer Set Programming; Artificial Ethical Agent; Artificial Intelligence (ai); Artificial Intelligence (ai) Ethical Risks; Artificial Intelligence (ai) Ethics; Artificial Moral Agent (ama); Artificial Phronesis (ap); Audit Trails; Auditing; Autonomous System Certification; Autonomous Underwater Vehicle (auv); Autonomous Vehicle Command Language (avcl); Bidirectional Encoder Representations From Transformers (bert); Bottom-up Artificial Intelligence (ai); Checkpoint Tasks; Competency; Compliance Assurance; Consequentialism; Curse Of Dimensionality; Defining Issues Test; Delphi; Deontic Cognitive Event Calculus (dcec); Deontological Ethics; Descriptive Ethics; Dimensions Of Autonomous Decision Making (dadms Or Dads); Elder Care Robotics; Emotion Detection; Epistemic Luck; Ethical Connotations; Ethical Hazard Analysis; Ethical Impact Agent (eia); Ethical Leniency; Ethical Mores; Ethical Reasoners; Ethical Reference Distribution; Ethics; Ethics-based Auditing; Event Calculus Framework; Explainable Artificial Intelligence (ai); Finite State Machine (fsm); First Principles; Generalized Outcome Assessment (goa); Governance; Gpt-2 Language Model; Habituation Effects; Human Ethos; Hume's Guillotine; Inner Speech; Interpretation-capable Reasoners; Kohlberg's Moral Stage Theory; Likert Scale Survey; Machine Ethics; Machine Learning; Machine Morality; Machine Wisdom; Markov Decision Process (mdp); Meaningful Human Control (mhc); Meaningful Human Involvement; Minimally Defeasible Argument; Mission Execution Ontology; Monte Carlo Simulation; Moral Foundations Questionnaire; Moral-conventional Transgression (mct); Morality; Natural Language Inference (nli); Neglect Tolerance (nt); Non-rt Control; Norm; Norm Grounding Problem; Normative Belief; Normative Knowledge; Ontology; Open-textured Terms; Paired T-test; Pearson Chi-square Test; Piaget's Observations And Theories Of Constructivist Moral Development; Plato's Cave; Prescriptive Ethics; Prisoner's Dilemma Game; Real-time (rt) Control; Reinforcement Learning; Repeated Measures Anova; Responsible Use Of Artificial Intelligence; Reward Hacking; Risk; Robot Consciousness; Robot Inner Dialog; Robot Trust; Rule Of Engagement (roe); Sentiment Analysis; Split–steal Game; Statutory Interpretive Reasoning; Technology Acceptance Model (tam); Technology Adoption Propensity; Thematic Analysis; Three Waves Of Ai According To Darpa; Top-down Artificial Intelligence (ai); Trolley Problem; Trust; Trust In Artificial Intelligence (ai); Trust In Robotics; Value Iteration; Verification And Validation; Veritic Epistemic Luck; Virtue Ethics; Wisdom Of Crowds (woc); Wizard Of Oz Design; Zero-shot Learning (zsl)},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Kosa202425,
	author = {Kosa, Victoria and Dobosevych, Oles and Ermolayev, Vadim},
	title = {Terminology Saturation Analysis: Refinements and Applications},
	year = {2024},
	journal = {Communications in Computer and Information Science},
	volume = {1810 CCIS},
	pages = {25 - 41},
	doi = {10.1007/978-3-031-53770-7_3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189532216&doi=10.1007%2F978-3-031-53770-7_3&partnerID=40&md5=421cb33eeaa2cc7b87b6f240baf2f738},
	abstract = {In this paper, we outline the results of our recent research on terminology saturation analysis (TSA) in subject domain-bounded textual corpora. We present the developed TSA method. We further report about the two use cases that proved the validity, efficiency, and effectiveness of TSA. Based on our experience of TSA use, we analyse the shortcomings of the method and figure out the ways to refinement and improvement. Further, we share our prognoses on how TSA could be used for: (i) generating quality datasets of minimal size for training large language models for performing better in scientific domains; (ii) iteratively constructing domain ontologies and knowledge graphs that representatively describe a subject domain, or topic; or (iii) detecting and predicting events based on the TSA of textual streams data. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Event Detection; Event Prediction; Large Language Model; Scientific Domain Ontology; Scientific Knowledge Graph; Terminological Saturation Analysis; Transfer Learning; Computational Linguistics; Deep Learning; Iterative Methods; Large Datasets; Ontology; Terminology; Domain Ontologies; Event Prediction; Events Detection; Knowledge Graphs; Language Model; Large Language Model; Scientific Domain Ontology; Scientific Knowledge; Scientific Knowledge Graph; Terminological Saturation Analyse; Transfer Learning; Knowledge Graph},
	keywords = {Computational linguistics; Deep learning; Iterative methods; Large datasets; Ontology; Terminology; Domain ontologies; Event prediction; Events detection; Knowledge graphs; Language model; Large language model; Scientific domain ontology; Scientific knowledge; Scientific knowledge graph; Terminological saturation analyse; Transfer learning; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2024,
	title = {IRCDL 2024 - Proceedings of the 20th Conference on Information and Research Science Connecting to Digital and Library Science},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3643},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188207790&partnerID=40&md5=0931328b4827c15960530931add73c43},
	abstract = {The proceedings contain 23 papers. The topics discussed include: building and exploiting a web of machine-readable scientific facts to make discoveries; publishing CoreKB facts as nanopublications; exploiting large language models to train automatic detectors of sensitive data; evaluating differential privacy approaches for information retrieval; evaluation of expressing without asserting approaches in RDF. the case of conjectures; a holistic ontology for digital libraries; a tool for empowering symbol detection through technological integration in library science. a case study on the Voynich manuscript; Ontofest: an ontology to integrate and retrieve data from the Locarno film festival archives; a novel methodology for topic identification in hadith; and research and teaching public communication of science and technology on digital data. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Stathis2024107,
	author = {Stathis, Georgios and Biagioni, Giulia and De Graaf, Klaas Andries and Trantas, Athanasios and van den Herik, Jaap},
	title = {The Value of Proactive Data for Intelligent Contracts},
	year = {2024},
	journal = {Lecture Notes in Networks and Systems},
	volume = {803},
	pages = {107 - 125},
	doi = {10.1007/978-981-99-7569-3_10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187647502&doi=10.1007%2F978-981-99-7569-3_10&partnerID=40&md5=240b3fb6f214463f7605964c0e67ce1e},
	abstract = {Intelligent Contracts (iContracts) is a new branch of research at the intersection of AI and law. It has many challenges, among which including the quality of data used. In our research we focus on generating and including quality Proactive Control Data (PCD) to improve iContracts, which is a novel research scope in literature. Our scope is defined by the main challenge in regards to emerging legal technologies. Currently, the legal system is more reactive than proactive, leading to high consequential legal costs. By shifting the focus to proactiveness, we discuss and improve upon the available methodologies (Bow-Tie Method and Logocratic Method) and technologies (Ontology Engineering, Software Engineering and Large Language Models [LLMs]) to demonstrate a higher degree of proactiveness in iContracts. Our results are threefold. First, we prove that the generation of PCD is possible with the development of a prototype that leverages the foundations of the Bow-Tie Method. Second, we demonstrate that the impact of PCD on contract drafting is significant, as the explicit inclusion of PCD in prompt engineering alters significantly the content of an LLM-drafted contract. Third, we show how the quality of PCD can be assessed and improved upon with the application of the Logocratic Method. The discussion highlights the feasibility of the research with available technologies. Ultimately, the implementation of our research depends on organisational considerations and resource allocation. We conclude that the generation of PCD is feasible, their impact on contract drafting is significant and their quality assessment is both possible and novel. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bow-tie Method; Contract Automation; Intelligent Contracts; Large Language Models; Legal Technology; Logocratic Method; Ontology Engineering; Preventive/proactive Law; Software Engineering; Computational Linguistics; Contracts; Software Engineering; Bow-tie Methods; Contract Automation; Control Data; Intelligent Contract; Language Model; Large Language Model; Legal Technology; Logocratic Method; Ontology Engineering; Preventive/proactive Law; Ontology},
	keywords = {Computational linguistics; Contracts; Software engineering; Bow-tie methods; Contract automation; Control data; Intelligent contract; Language model; Large language model; Legal technology; Logocratic method; Ontology engineering; Preventive/proactive law; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Martin2024,
	author = {Martin, James Lee and Zanuri, Muhammad Nur Arif and Sockalingam, Muthu Kumar and Andersen, Eric},
	title = {LLMs, Embeddings and Indexing Pipelines to Enable Natural Language Searching on Upstream Datasets},
	year = {2024},
	pages = {},
	doi = {10.2523/IPTC-23626-EA},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187576256&doi=10.2523%2FIPTC-23626-EA&partnerID=40&md5=f42b03c7a253a9087b673637da29d10f},
	abstract = {Large Language Models (LLMs) are attracting an enormous amount of interest at the moment in many domains. Their general nature, and ability to "understand" natural language, has already stimulated multiple areas of research at our company. Here we successfully demonstrate a Natural Language querying system, which is able to search a large repository of unstructured exploration data. The system supports follow up querying on the returned results, plus automatic summarization of content. The system is integrated into our novel end-to-end data-mining platform, which continuously mines our unstructured exploration data for new changes and indexes the results. Important in our method are the enrichment processes that occur prior to use of the LLM. Our approach avoids usual "chunking" techniques, which in our experience results in inferior results, especially in the multiple domain areas of Exploration. By integrating our novel ontology-model AI in the enrichment of the initial Index, we drastically boost the performance of search resulting from the LLM steps. In order to perform the search, key parts of our unstructured data, plus the query itself, need to be transformed into a vector form. This is performed using the embedding feature of the LLM. For this work, we had around 500,000 embeddings to calculate. To improve performance these were indexed in a leading Analytics Engine as a vector object, allowing fast search via cosine or Euclidian similarity. A custom dashboard was made to allow fresh searches of the vector datastore to be returned for further analysis. Our current search time across 500,000 embeddings is under 20 milli-seconds. Our custom dashboard returns the top matches for further interrogation and analysis. This includes follow-up Natural Language question support on the returned matches for summarization tasks and other customised querying. Since our exploration-specific, ontology model is able to tag each piece of data with over 40 exploration-specific labels, we are able to cross-examine the LLM returned results with the tags. Agreement on a range of queries - ranging from targeted, highly specific questions to general, open-ended queries - was surprisingly good. Natural Language based querying of our unstructured data is opening a whole new approach to data discovery in our company. Tailoring it to the exploration domain has required specific domain expertise and a novel ontology-model be used to ensure relevant prompts and query results. Obtaining search results quickly has also required expertise and fine-tuning. Future directions include ingesting more data, scaling the support infrastructure and further capability enhancement. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Mining; Gasoline; Natural Language Processing Systems; Ontology; Search Engines; Embeddings; Exploration Data; Follow Up; General Nature; Language Model; Model Embedding; Multiple Areas; Natural Languages; Ontology Model; Unstructured Data},
	keywords = {Data mining; Gasoline; Natural language processing systems; Ontology; Search engines; Embeddings; Exploration data; Follow up; General nature; Language model; Model embedding; Multiple areas; Natural languages; Ontology model; Unstructured data},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Han2024,
	author = {Han, Lifeng and Gladkoff, Serge and Erofeev, Gleb and Sorokina, Irina and Galiano, Betty and Nenadic, Goran},
	title = {Neural machine translation of clinical text: an empirical investigation into multilingual pre-trained language models and transfer-learning},
	year = {2024},
	journal = {Frontiers in Digital Health},
	volume = {6},
	pages = {},
	doi = {10.3389/fdgth.2024.1211564},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187113854&doi=10.3389%2Ffdgth.2024.1211564&partnerID=40&md5=44ade8897abe58fe6e53e705997201f4},
	abstract = {Clinical text and documents contain very rich information and knowledge in healthcare, and their processing using state-of-the-art language technology becomes very important for building intelligent systems for supporting healthcare and social good. This processing includes creating language understanding models and translating resources into other natural languages to share domain-specific cross-lingual knowledge. In this work, we conduct investigations on clinical text machine translation by examining multilingual neural network models using deep learning such as Transformer based structures. Furthermore, to address the language resource imbalance issue, we also carry out experiments using a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs). The experimental results on three sub-tasks including (1) clinical case (CC), (2) clinical terminology (CT), and (3) ontological concept (OC) show that our models achieved top-level performances in the ClinSpEn-2022 shared task on English-Spanish clinical domain data. Furthermore, our expert-based human evaluations demonstrate that the small-sized pre-trained language model (PLM) outperformed the other two extra-large language models by a large margin in the clinical domain fine-tuning, which finding was never reported in the field. Finally, the transfer learning method works well in our experimental setting using the WMT21fb model to accommodate a new language space Spanish that was not seen at the pre-training stage within WMT21fb itself, which deserves more exploitation for clinical knowledge transformation, e.g. to investigate into more languages. These research findings can shed some light on domain-specific machine translation development, especially in clinical and healthcare fields. Further research projects can be carried out based on our work to improve healthcare text analytics and knowledge transformation. Our data is openly available for research purposes at: https://github.com/HECTA-UoM/ClinicalNMT. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Knowledge Transformation; Clinical Text Translation; Large Language Model; Multilingual Pre-trained Language Model; Neural Machine Translation; Spanish-english Translation; Transfer Learning; Article; Artificial Neural Network; Controlled Study; Deep Learning; Diagnosis; Human; Language Model; Large Language Model; Transfer Of Learning},
	keywords = {article; artificial neural network; controlled study; deep learning; diagnosis; human; language model; large language model; transfer of learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Colucci Cante2024597,
	author = {Colucci Cante, Luigi and Di Martino, Beniamino D.I. and Graziano, Mariangela and Branco, Dario and Pezzullo, Gennaro Junior},
	title = {Automated Storytelling Technologies for Cultural Heritage},
	year = {2024},
	journal = {Lecture Notes on Data Engineering and Communications Technologies},
	volume = {193},
	pages = {597 - 606},
	doi = {10.1007/978-3-031-53555-0_57},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186402642&doi=10.1007%2F978-3-031-53555-0_57&partnerID=40&md5=8e989a3e22e9dc9b89d4b8b0e74f3a81},
	abstract = {The awareness of the importance of communicating cultural heritage through innovative and automated means is steadily increasing. This awareness is fueled by the need to promote cultural heritage knowledge in accessible and engaging ways for an increasingly digital audience. This article aims to examine in detail the various methodologies used in the creation of automated storytelling for historical cultural heritage communication. In particular, attention is given to models based on large language models, such as those derived from advanced neural networks, which demonstrate significant potential in generating rich and engaging narratives. Another key aspect of the analysis concerns the use of chatbots in the context of automated storytelling. The possibilities offered by interactive conversation managed by virtual agents are explored, which can enhance the user experience through personalized dialogues and contextualized information. Additionally, the article focuses on various tools dedicated to automatic storytelling writing, evaluating their effectiveness and versatility in the specific context of historical cultural heritage. These tools are essential to facilitate the creative process and ensure narrative coherence. A significant part of the analysis addresses automated storytelling models based on processes, exploring how the sequence of events and concepts can be managed automatically to construct meaningful and relevant stories from a historical perspective. Furthermore, applications of ontologies and other tools are examined with the aim of improving the structure and understanding of narratives, ensuring effective linkage between different elements of historical cultural heritage. The overall goal of the article is to provide a comprehensive view of the current landscape of automated storytelling techniques, highlighting the still-open challenges and potential future development directions in this field. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatbots; Creative Process; Cultural Heritages; Historical Perspective; Language Model; Model-based Opc; Neural-networks; Sequence Of Events; Users' Experiences; Virtual Agent; Automation},
	keywords = {Chatbots; Creative process; Cultural heritages; Historical perspective; Language model; Model-based OPC; Neural-networks; Sequence of events; Users' experiences; Virtual agent; Automation},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14}
}

@ARTICLE{Li202451,
	author = {Li, Fuwang},
	title = {A semantic retrieval model of social media data based on statistical theory},
	year = {2024},
	journal = {International Journal of Web Based Communities},
	volume = {20},
	number = {1-2},
	pages = {51 - 62},
	doi = {10.1504/IJWBC.2024.136657},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185529620&doi=10.1504%2FIJWBC.2024.136657&partnerID=40&md5=f9ce32fc90c733bc9b7c1ca86b6689c9},
	abstract = {Aiming at the problems of low retrieval accuracy and efficiency in semantic retrieval model of social media data, this paper studies semantic retrieval model of social media data based on statistical theory. Statistical theory and ontology of semantic retrieval information of social media data are analysed to complete the labelling process of retrieval information. The semantic retrieval model of social media data is constructed by calculating the similarity of semantic distance and information amount and using statistical theory. Experimental results show that the recall rate of the proposed method is as high as 94%, and the accuracy is as high as 92%, both higher than other methods, and the retrieval time is only 18.2 s. Therefore, the semantic retrieval effect of social media data is good, and the semantic retrieval accuracy and efficiency of social media data are effectively improved. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Retrieval Model; Semantic Retrieval; Social Media Data; Statistical Language Model; Statistical Theory; Information Retrieval; Natural Language Processing Systems; Semantics; Social Networking (online); Statistics; Labelings; Ontology's; Retrieval Accuracy; Retrieval Efficiency; Retrieval Models; Semantic Distance; Semantic Retrieval; Social Media Datum; Statistical Language Modelling; Statistical Theory; Efficiency},
	keywords = {Information retrieval; Natural language processing systems; Semantics; Social networking (online); Statistics; Labelings; Ontology's; Retrieval accuracy; Retrieval efficiency; Retrieval models; Semantic distance; Semantic retrieval; Social media datum; Statistical language modelling; Statistical theory; Efficiency},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Palagin2024111,
	author = {Palagin, Alexsandr V. and Kaverinsky, Vladislav and Malakhov, K. S. and Petrenko, M.},
	title = {Fundamentals of the Integrated Use of Neural Network and Ontolinguistic Paradigms: A Comprehensive Approach},
	year = {2024},
	journal = {Cybernetics and Systems Analysis},
	volume = {60},
	number = {1},
	pages = {111 - 123},
	doi = {10.1007/s10559-024-00652-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184410537&doi=10.1007%2Fs10559-024-00652-z&partnerID=40&md5=5eee2d464391bd8e27ea55c2e02e2334},
	abstract = {The paper presents an integrated approach that combines neural network and ontolinguistic paradigms. The method encompasses methodological underpinnings, information technology, and the MedRehabBot system. Collectively, they embody the core principles of meta-learning and structured prompts, ultimately enhancing the efficiency of information system interaction with Chatbots and information retrieval rooted in ontologies. The method also offers the flexibility to adapt the MedRehabBot system for utilization within different Large Language Model (LLM) systems. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatbot; Chatgpt; Llm-system; Medrehabbot; Ontological Engineering; Ontology; Ontology-driven Information System; Prompt Engineering; Transdisciplinary Scientific Research; Information Systems; Information Use; Linguistics; Search Engines; Chatbots; Chatgpt; Language Model; Large Language Model-system; Medrehabbot; Modelling Systems; Ontological Engineering; Ontology's; Ontology-driven Information System; Prompt Engineering; Scientific Researches; Transdisciplinary Scientific Research; Ontology},
	keywords = {Information systems; Information use; Linguistics; Search engines; Chatbots; ChatGPT; Language model; Large language model-system; Medrehabbot; Modelling systems; Ontological engineering; Ontology's; Ontology-driven information system; Prompt engineering; Scientific researches; Transdisciplinary scientific research; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@ARTICLE{Chen2024102,
	author = {Chen, Xi and Li, Cheng and Wang, Ziyuan and Zhou, Yixin and Chu, Ming},
	title = {Computational screening of biomarkers and potential drugs for arthrofibrosis based on combination of sequencing and large nature language model},
	year = {2024},
	journal = {Journal of Orthopaedic Translation},
	volume = {44},
	pages = {102 - 113},
	doi = {10.1016/j.jot.2023.11.002},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182903243&doi=10.1016%2Fj.jot.2023.11.002&partnerID=40&md5=5cca2645fea52a298c493f7fabd9dec2},
	abstract = {Background: Arthrofibrosis (AF) is a fibrotic joint disease resulting from excessive collagen production and fibrous scar formation after total knee arthroplasty (TKA). This devastating complication may cause consistent pain and dramatically reduction of functionality. Unfortunately, the conservative treatments to prevent the AF in the early stage are largely unknown due to the lack of specific biomarkers and reliable therapeutic targets. Methods: In this study, we extracted1782 fibrosis related genes (FRGs) from 373,461published literature based on the large natural language processing models (ChatGPT) and intersected with the 2750 differential expressed genes (DEGs) from mRNA microarray (GSE135854). A total of 311 potential AF biomarker genes (PABGs) were obtained and functional analysis were performed including gene ontology (GO) annotation and the Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analyses. Subsequently, we accomplished validation in AF animal models with immobilization of the unilateral knee joints of 16 rabbits for 1-week, 2-weeks, 3-weeks and 4-weeks. Finally, we tested the biomarkers in a retrospective cohort enrolled 35 AF patients and 35 control group patients. Results: We identified G-protein-coupled receptor 17 (GPR17) as a reliable therapeutic biomarker for AF diagnosis with higher AUC (0.819) in the ROC curve. A total of 21 potential drugs targeted to GPR17 were screened. Among them, pranlukast and montelukast have achieved therapeutic effect in animal models. In addition, we established an online AF database for data integration (https://chenxi2023.shinyapps.io/afdbv1). Conclusions: These results unveiling therapeutic biomarkers for AF diagnosis, and provide potential drugs for clinical treatment. The translational potential of this article: Our study demonstrated that GPR17 holds significant promise as a potential biomarker and therapeutic target for arthrofibrosis. Moreover, pranlukast and montelukast targeted to GPR17 that could be instrumental in the treatment of AF. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Af; Biomarker; Gpr17; Macrophage; Stiff Knee; Carboxypeptidase; Chemokine Receptor Ccr4; Collagen; Cryptochrome; Cystic Fibrosis Transmembrane Conductance Regulator; Epidermal Growth Factor Receptor; Hydroxyproline; Montelukast; Pranlukast; Smad Protein; Axin; Biological Marker; Carboxypeptidase; Chemokine Receptor Ccr4; Collagen; Cryptochrome; Cystic Fibrosis Transmembrane Conductance Regulator; Epidermal Growth Factor Receptor; G Protein Coupled Receptor; Hydroxyproline; Immunoglobulin Enhancer Binding Protein; Monocyte Chemotactic Protein 1; Montelukast; Pranlukast; Smad Protein; Tumor Necrosis Factor; Adult; Aged; Amino Acid Sequence; Animal Experiment; Arthrofibrosis; Article; Bioinformatics; Body Mass; Controlled Study; Female; Fibrosis; Gene Expression; Gene Ontology; Glycosylation; Human; Immune Response; Immunohistochemistry; Knee Joint; Machine Learning; Major Clinical Study; Male; Molecular Docking; Nonhuman; Pathway Enrichment Analysis; Reverse Transcription Polymerase Chain Reaction; Rna Isolation; Sensitivity And Specificity; Signal Transduction; Total Knee Arthroplasty},
	keywords = {axin; biological marker; carboxypeptidase; chemokine receptor CCR4; collagen; cryptochrome; cystic fibrosis transmembrane conductance regulator; epidermal growth factor receptor; G protein coupled receptor; hydroxyproline; immunoglobulin enhancer binding protein; monocyte chemotactic protein 1; montelukast; pranlukast; Smad protein; tumor necrosis factor; adult; aged; amino acid sequence; animal experiment; arthrofibrosis; Article; bioinformatics; body mass; controlled study; female; fibrosis; gene expression; gene ontology; glycosylation; human; immune response; immunohistochemistry; knee joint; machine learning; major clinical study; male; molecular docking; nonhuman; pathway enrichment analysis; reverse transcription polymerase chain reaction; RNA isolation; sensitivity and specificity; signal transduction; total knee arthroplasty},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access}
}

@ARTICLE{Jiang20241474,
	author = {Jiang, Yiwei and de Raedt, Maarten and Deleu, Johannes and Demeester, Thomas and Develder, Chris},
	title = {Few-shot out-of-scope intent classification: analyzing the robustness of prompt-based learning},
	year = {2024},
	journal = {Applied Intelligence},
	volume = {54},
	number = {2},
	pages = {1474 - 1496},
	doi = {10.1007/s10489-023-05215-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181448600&doi=10.1007%2Fs10489-023-05215-x&partnerID=40&md5=a6ce32d88f0e3931355357d99b402fa3},
	abstract = {Out-of-scope (OOS) intent classification is an emerging field in conversational AI research. The goal is to detect out-of-scope user intents that do not belong to a predefined intent ontology. However, establishing a reliable OOS detection system is challenging due to limited data availability. This situation necessitates solutions rooted in few-shot learning techniques. For such few-shot text classification tasks, prompt-based learning has been shown more effective than conventionally finetuned large language models with a classification layer on top. Thus, we advocate for exploring prompt-based approaches for OOS intent detection. Additionally, we propose a new evaluation metric, the Area Under the In-scope and Out-of-Scope Characteristic curve (AU-IOC). This metric addresses the shortcomings of current evaluation standards for OOS intent detection. AU-IOC provides a comprehensive assessment of a model’s dual performance capacities: in-scope classification accuracy and OOS recall. Under this new evaluation method, we compare our prompt-based OOS detector against 3 strong baseline models by exploiting the metadata of intent annotations, i.e., intent description. Our study found that our prompt-based model achieved the highest AU-IOC score across different data regimes. Further experiments showed that our detector is insensitive to a variety of intent descriptions. An intriguing finding shows that for extremely low data settings (1- or 5-shot), employing a naturally phrased prompt template boosts the detector’s performance compared to rather artificially structured template patterns. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Dialogue Intent Classification; Few-shot Learning; Outlier/novelty Detection; Prompt-based Models; Learning Systems; Text Processing; Detection System; Dialog Intent Classification; Few-shot Learning; Intent Detection; Limited Data; Novelty Detection; Ontology's; Outlier/novelty Detection; Performance; Prompt-based Model; Classification (of Information)},
	keywords = {Learning systems; Text processing; Detection system; Dialog intent classification; Few-shot learning; Intent detection; Limited data; Novelty detection; Ontology's; Outlier/novelty detection; Performance; Prompt-based model; Classification (of information)},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Zhang20243867,
	author = {Zhang, Liyuan and Jiang, Yongquan and Yang, Yan},
	title = {GNNGO3D: Protein Function Prediction Based on 3D Structure and Functional Hierarchy Learning},
	year = {2024},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	volume = {36},
	number = {8},
	pages = {3867 - 3878},
	doi = {10.1109/TKDE.2023.3331005},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177076809&doi=10.1109%2FTKDE.2023.3331005&partnerID=40&md5=4ed50ffe473b7ceacec0ea7af8f594d2},
	abstract = {Protein sequences accumulate in large quantities, and the traditional method of annotating protein function by experiment has been unable to bridge the gap between annotated proteins and unannotated proteins. Machine learning-based protein function prediction is an effective approach to solve this problem. Most of the existing methods only use the protein sequence but ignore the three-dimensional structure which is closely related to the protein function. And the hierarchy of protein functions is not adequately considered. To solve this problem, we propose a graph neural network (GNNGO3D) that combines the three-dimensional structure and functional hierarchy learning. GNNGO3D simultaneously uses three kinds of information: protein sequence, tertiary structure, and hierarchical relationship of protein function to predict protein function. The novelty of GNNGO3D lies in that it integrates the learning of functional level information into the method of predicting protein function by using tertiary structure information, fully learning the relationship between protein functions, and helping to better predict protein function. Experimental results show that our method is superior to existing methods for predicting protein function based on sequence and structure. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontology; Graph Neural Networks; Language Model; Machine Learning; Protein Function Prediction; Feature Extraction; Gene Ontology; Graph Neural Networks; Job Analysis; Learning Systems; Proteins; Three Dimensional Displays; Convolutional Neural Network; Features Extraction; Gene Ontology; Language Model; Machine-learning; Ontology's; Protein Function Prediction; Protein Sequences; Task Analysis; Three-dimensional Display; Forecasting},
	keywords = {Feature extraction; Gene Ontology; Graph neural networks; Job analysis; Learning systems; Proteins; Three dimensional displays; Convolutional neural network; Features extraction; Gene ontology; Language model; Machine-learning; Ontology's; Protein function prediction; Protein sequences; Task analysis; Three-dimensional display; Forecasting},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Preetha Evangeline2024251,
	author = {Preetha Evangeline, D. and Malathi, S. and AnandhaKumar, Palanisamy},
	title = {Automatic programming (source code generator) based on an ontological model},
	year = {2024},
	journal = {Advances in Computers},
	volume = {132},
	pages = {251 - 272},
	doi = {10.1016/bs.adcom.2023.08.008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174733400&doi=10.1016%2Fbs.adcom.2023.08.008&partnerID=40&md5=4f8cadb72c3a170f72787d92de3c62e5},
	abstract = {Source AI is an AI-powered tool that can generate code in any programming language from any human language description. It can also simplify, find errors and fix them and debug your code. Automatic code generation capabilities continue to evolve within programming languages, IDEs and tools that work at compile time. This coding technique has proliferated because it can reduce mundane programming grunt work, and developers have found that it improves turnaround times and accuracy. Auto generated code usually becomes a hindrance for developers who want to tweak it later on Teams should plan to restrict these tools to only certain parts of the SDLC, such as where they can act as facilitators in smaller, less complex situations. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Language Model; Natural Language Processing; Ontological Modeling; Software Engineering; Source Code},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Gao2024,
	author = {Gao, Jialiang and Peng, Peng and Lu, Feng and Claramunt, Christophe and Qiu, Peiyuan and Xu, Yang},
	title = {Mining tourist preferences and decision support via tourism-oriented knowledge graph},
	year = {2024},
	journal = {Information Processing and Management},
	volume = {61},
	number = {1},
	pages = {},
	doi = {10.1016/j.ipm.2023.103523},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173611224&doi=10.1016%2Fj.ipm.2023.103523&partnerID=40&md5=34ec759435444c84ccbcf44cf55ca81d},
	abstract = {Currently, tourism management research is focused on comprehending the fluctuating tourist preferences and devising targeted development strategies through extensive analysis of heterogenous user-generated contents. However, given the online reviews of attractions involve overabundant mixed and intangible dimensions, the widely-used unsupervised text mining could be incomplete or inaccurate. Furthermore, the existing literature typically restricted to the certain types of attractions within several tourist destinations and origins, can hardly guarantee comprehensive insights. To overcome these limitations, the study proposes a novel knowledge-graph-driven framework, involving the systematic construction as well as the thorough investigation and inference of a tourism-oriented knowledge graph (TKG). Following the ontology of domain expertise, 11,296,716 structured triplets of multifaceted knowledge about 1,174,034 tourists and 20,481 attractions within all 340 city-level destinations across China are extracted from multi-source text corpus by the transferring learning on pre-training language model with 43.64–50.65 % accuracy enhancement. In virtue of TKG, a comprehensive decision-support system can be established, which bifurcates into two distinct modes of knowledge application: symbolic query and distributed reasoning. Through the implementation of multiple spatiotemporal analyses via SPARQL queries on TKG, the distribution regularities of tourist preference, causal interpretations, and their effects on destination development can be progressively detected. Refining the distributed representations of objects by injecting abundant contextual knowledge from TKG can significantly enhance the downstream inferential tasks, such as tourist demand prediction and attraction competitive intelligence. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Decision-making Support; Information Extraction; Knowledge Reasoning; Spatiotemporal Analysis; Tourism Knowledge Graph; Competition; Competitive Intelligence; Data Mining; Decision Support Systems; Knowledge Graph; Tourism; Decision Making Support; Decision Supports; Development Strategies; Information Extraction; Knowledge Graphs; Knowledge Reasoning; Management Research; Spatiotemporal Analysis; Tourism Knowledge Graph; Tourism Management; Decision Making},
	keywords = {Competition; Competitive intelligence; Data mining; Decision support systems; Knowledge graph; Tourism; Decision making support; Decision supports; Development strategies; Information extraction; Knowledge graphs; Knowledge reasoning; Management research; Spatiotemporal analysis; Tourism knowledge graph; Tourism management; Decision making},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 29; All Open Access; Bronze Open Access}
}

@ARTICLE{Faiz202411133,
	author = {Faiz, Mehwish and Khan, Saad Jawaid and Azim, Choudhry Fahad and Ejaz, Nazia},
	title = {Disclosing the locale of transmembrane proteins within cellular alcove by machine learning approach: systematic review and meta analysis},
	year = {2024},
	journal = {Journal of Biomolecular Structure and Dynamics},
	volume = {42},
	number = {20},
	pages = {11133 - 11148},
	doi = {10.1080/07391102.2023.2260490},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172448189&doi=10.1080%2F07391102.2023.2260490&partnerID=40&md5=a69a4561098b8a3543e051a77ee01b91},
	abstract = {Protein subcellular localization is a promising research question in Proteomics and associated fields, including Biological Sciences, Biomedical Engineering, Computational Biology, Bioinformatics, Proteomics, Artificial Intelligence, and Biophysics. However, computational techniques are preferred to explore this attribute for a massive number of proteins. The byproduct of this conjunction yields diversified location identifiers of proteins. These protein subcellular localization identifiers are unique regarding the database used, organisms, Machine Learning Technique, and accuracy. Despite the availability of these identifiers, the majority of the work has been done on the subcellular localization of proteins and, less work has been done specifically on locations of transmembrane proteins. This systematic review accounts for computational techniques implemented on transmembrane protein localization. Moreover, a literature search on PubMed, Science Direct, and IEEE Databases disclosed no systematic review or meta-analysis on the cell’s transmembrane protein locale. A Systematic review was formed under the guidelines of PRISMA by using Science Direct, PubMed, and IEEE Databases. Journal publications from 2000 to 2023 were taken into consideration and screened. This review has focused only on computational studies rather than experimental techniques. 1004 studies were reviewed and were categorized as relevant and non-relevant according to inclusion and exclusion criteria. All the screening was done through Endnote after importing citations. This systematic review characterizes the gap in targeting the locale of the transmembrane protein and will aid researchers in exploring its new horizons. Communicated by Ramaswamy H. Sarma. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Accuracy; Deep Learning; Location; Machine Learning; Protein; Membrane Proteins; Membrane Protein; Accuracy; Amino Acid Composition; Amino Acid Sequence; Artificial Intelligence; Artificial Neural Network; Cells; Cellular Alcove; Convolutional Neural Network; Data Base; Feature Extraction; Gene Ontology; Human; Ieee Database; Information; K Nearest Neighbor; Large Language Model; Machine Learning; Medline; Meta Analysis; Preferred Reporting Items For Systematic Reviews And Meta-analyses; Protein Localization; Protein Protein Interaction; Proteomics; Random Forest; Review; Sciencedirect; Screening; Sequential Evolutionary Information; Support Vector Machine; Systematic Review; Bioinformatics; Cell Membrane; Metabolism; Procedures; Cell Membrane; Computational Biology; Humans; Machine Learning; Membrane Proteins; Proteomics},
	keywords = {membrane protein; accuracy; amino acid composition; amino acid sequence; artificial intelligence; artificial neural network; cells; cellular alcove; convolutional neural network; data base; feature extraction; gene ontology; human; IEEE Database; information; k nearest neighbor; large language model; machine learning; Medline; meta analysis; Preferred Reporting Items for Systematic Reviews and Meta-Analyses; protein localization; protein protein interaction; proteomics; random forest; Review; ScienceDirect; screening; Sequential evolutionary information; support vector machine; systematic review; bioinformatics; cell membrane; metabolism; procedures; Cell Membrane; Computational Biology; Humans; Machine Learning; Membrane Proteins; Proteomics},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Shah-Mohammadi202439,
	author = {Shah-Mohammadi, Fatemeh and Finkelstein, Joseph},
	title = {Comparative Analysis of Rule-Based and Large Language Model-Based Approaches in Addressing Variability in Clinical Outcome Reporting},
	year = {2024},
	pages = {39 - 45},
	doi = {10.1109/IEMCON62851.2024.11093511},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105013686114&doi=10.1109%2FIEMCON62851.2024.11093511&partnerID=40&md5=12b4c2a2fd16fd5c87a7c19a6b4dd54d},
	abstract = {In clinical trials, varied terminologies and definitions often obscure the clarity and consistency needed to interpret results effectively. The ability to standardize clinical outcome reports and align semantically similar outcomes is crucial in healthcare and research, as inconsistencies can impede the comparability of trial results, complicating metaanalyses and informed decision-making. This research focuses on minimizing variability in the reporting of outcome measures through a comparative analysis of rule-based and advanced language modeling techniques. The rule-based method employs established ontologies, while the language model-based approach utilizes large language models. Findings indicate a low linkage of outcomes to traditional rule-based ontology, particularly for three-word outcomes, and underscore large language models' efficacy in recognizing semantically similar outcomes across varying word counts. This supports the critical role of large language models in harmonizing outcome data, reducing redundancies, and improving data interoperability in clinical research contexts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Trial; Gpt; Large Language Model; Outcome Alignment; Sbert; Semantic Variability; Clinical Research; Computational Linguistics; Decision Making; Interoperability; Medical Applications; Ontology; Semantics; Terminology; Clinical Trial; Comparative Analyzes; Gpt; Language Model; Large Language Model; Model Based Approach; Outcome Alignment; Rule Based; Sbert; Semantic Variability; Modeling Languages},
	keywords = {Clinical research; Computational linguistics; Decision making; Interoperability; Medical applications; Ontology; Semantics; Terminology; Clinical trial; Comparative analyzes; GPT; Language model; Large language model; Model based approach; Outcome alignment; Rule based; SBERT; Semantic variability; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Huettemann2024,
	author = {Huettemann, Sebastian and Mueller, Roland M. and Larsen, Kai Rune T. and Dinter, Barbara},
	title = {A Framework for Ontology-Based Knowledge Synthesis from Research Articles},
	year = {2024},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010822471&partnerID=40&md5=f1a01b8dd7bcb90c47c6de099c5e0db9},
	abstract = {We present a framework for ontology-based knowledge synthesis from research articles to support researchers in conducting literature reviews and gaining comprehensive insights into the state of the Information Systems discipline. Building on calls for an academic knowledge infrastructure, we performed a design science research project to (1) develop a conceptual framework incorporating the semantic annotation of research articles based on a domain ontology, (2) provide a process model, a data model, and an operations model that can guide the development of tools to support literature reviews, and (3) evaluate this framework within a proof-of-concept implementation. We evaluated the prototype against manually labeled abstracts and large language model-based tools. We further tested its practical application in semi-automated literature reviews. The results indicate that the proposed framework can support researchers in knowledge extraction and synthesis when analyzing large volumes of articles while saving time and effort. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Extraction; Knowledge Infrastructure; Large Language Models; Literature Synthesis; Ontological Meta-analysis; Data Mining; Information Systems; Information Use; Knowledge Organization; Learning Systems; Ontology; Knowledge Extraction; Knowledge Infrastructure; Knowledge Synthesis; Language Model; Large Language Model; Literature Reviews; Literature Synthesis; Meta-analysis; Ontological Meta-analyze; Ontology-based; Semantics},
	keywords = {Data mining; Information systems; Information use; Knowledge organization; Learning systems; Ontology; Knowledge extraction; Knowledge infrastructure; Knowledge synthesis; Language model; Large language model; Literature reviews; Literature synthesis; Meta-analysis; Ontological meta-analyze; Ontology-based; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Soularidis20247,
	author = {Soularidis, Andreas and Kotis, Konstantinos I. and Lamolle, Myriam and Mejdoul, Zakaria and Lortal, Gaëlle and Vouros, George A.},
	title = {LLM-Assisted Generation of SWRL Rules from Natural Language},
	year = {2024},
	pages = {7 - 12},
	doi = {10.1109/AIxDKE63520.2024.00008},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008498105&doi=10.1109%2FAIxDKE63520.2024.00008&partnerID=40&md5=5e63e58a9654eba4991ac8cb40693cc7},
	abstract = {Recently, Large Language Models (LLMs) have attracted great attention due to their remarkable performance in human-like text generation and reasoning skills (although their memory and hallucination problems still remain key issues to tackle more efficiently). LLMs have been applied to various application domains, including Knowledge Graph (KG) generation, question and answering over KGs and text-to-SPARQL translation. In this work, we investigate the capabilities of LLMs in text-to-SWRL translation, i.e., translation of Natural Language (NL) rules into Semantic Web Rule Language (SWRL) rules, put in the context of an industrial Ontology Engineering (OE) environment called GLUON, presenting our first experimental results. The aim of this work is to identify the level of automation that is adequate for the LLM to generate well-formed SWRL rules, towards the development of an LLM-based framework, as a plugin to the GLUON OE environment. In this direction we leverage and combine the reasoning capabilities of GPT-4o model, the Retrieval-Augmented Generation (RAG) technology, and prompt engineering. We employ quantitative and qualitative metrics to evaluate the generated SWRL rules, focusing on the correct syntax and the level of human intervention. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models (llm); Ontology Engineering; Retrieval-augmented Generation (rag); Swrl; Computational Linguistics; Knowledge Management; Natural Language Processing Systems; Translation (languages); Engineering Environment; Language Model; Large Language Model; Natural Languages; Ontology Engineering; Performance; Retrieval-augmented Generation; Rules Languages; Semantic Web Rule Language; Semantic Web Rules; Ontology},
	keywords = {Computational linguistics; Knowledge management; Natural language processing systems; Translation (languages); Engineering environment; Language model; Large language model; Natural languages; Ontology engineering; Performance; Retrieval-augmented generation; Rules languages; Semantic web rule language; Semantic Web rules; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zaeifi202428,
	author = {Zaeifi, Mehrnoosh and Mosallanezhad, Ahmadreza and Bansal, Srividya},
	title = {Deeper and Deeper: A Lightweight Semi-Supervised Deep Reinforcement Adaptive Learning-Based Ontology Alignment},
	year = {2024},
	pages = {28 - 35},
	doi = {10.1109/AIxDKE63520.2024.00012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008494417&doi=10.1109%2FAIxDKE63520.2024.00012&partnerID=40&md5=029e5805fb55f5d2645c27de5b5fe5f4},
	abstract = {Ontology alignment, also known as ontology matching, is pivotal for addressing semantic heterogeneity on the Semantic Web. Essentially, it entails linking entities across different ontologies or knowledge graphs in order to resolve ambiguity and enhance the interoperability of data. While various techniques exist, many still rely on rule-based or logic-based approaches, often requiring human intervention and domain specificity. Despite these challenges, ontology alignment remains crucial for seamlessly integrating disparate knowledge sources and facilitating effective data integration. In this paper, we tackle the limitations of current ontology alignment models by introducing a novel, lightweight, semi-supervised deep reinforcement learning model called Deep Reinforcement Adaptive Learning for Ontology Alignment (DRAL-OA). The DRAL-OA method incorporates both syntactic and structural information into the training phase. In addition, this approach is semi-supervised, utilizing a portion of the training data and automatically generating the rest, which reduces the need for human intervention. Moreover, DRAL-OA uses non-domain-specific language models to ensure broad applicability and reduce the need for extensive domain expertise. We evaluate our proposed approach using two datasets from the Ontology Alignment Evaluation Initiative (OAEI). In our experiments, we have shown that the proposed model can achieve high-quality alignments with F-measures on par with other state-of-the-art systems, all while maintaining a very short runtime and a compact model size. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Ontology Alignment; Reinforcement Learning; Semantic Web; Alignment; Data Integration; Deep Learning; Deep Reinforcement Learning; Knowledge Graph; Learning Algorithms; Ontology; Semantic Web; Supervised Learning; Syntactics; Adaptive Learning; Human Intervention; Knowledge Graphs; Ontology Alignment; Ontology Graphs; Ontology Matching; Reinforcement Learnings; Semantic-web; Semantics Heterogeneity; Semi-supervised; Reinforcement Learning},
	keywords = {Alignment; Data integration; Deep learning; Deep reinforcement learning; Knowledge graph; Learning algorithms; Ontology; Semantic Web; Supervised learning; Syntactics; Adaptive learning; Human intervention; Knowledge graphs; Ontology alignment; Ontology graphs; Ontology matching; Reinforcement learnings; Semantic-Web; Semantics Heterogeneity; Semi-supervised; Reinforcement learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Belbekri2024,
	author = {Belbekri, Adel and Bouarroudj, Wissem and Benchikha, Fouzia},
	title = {A Two-Stage GAN Oversampling: Integrating GPT-3 and DBpedia for Named Entity Recognition Datasets},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3973},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105008219200&partnerID=40&md5=242158af9237c457f948bb604cc56bad},
	abstract = {Oversampling is a technique used to adjust the class distribution of a dataset, particularly to address imbalance between different classes or categories. This paper presents a novel oversampling method for named entity recognition (NER) datasets using a generative approach. Our method leverages the power of the GPT-3 large language models in combination with the DBpedia knowledge graphs to create high-quality synthetic examples for underrepresented entity classes. The process starts by analyzing the dataset to identify entity types that require balancing. For each such entity, we explore the DBpedia knowledge graph to find similar or equivalent concepts using ontological relationships. These concepts are then used to create GPT-3 prompts, guiding them to generate contextually appropriate examples of the type of target entity. This process is repeated until a balanced distribution across all entity classes is achieved. Our approach aims to address the common challenge of class imbalance in NER datasets while maintaining semantic coherence and linguistic diversity in the generated examples. We evaluate the effectiveness of this method on several benchmark NER datasets and discuss its potential impact on model performance and generalization. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Generative Large Language Models; Knowledge Graphs; Named Entity Recognition; Oversampling; Computational Linguistics; Knowledge Graph; Knowledge Management; Large Datasets; Semantics; Class Distributions; Dbpedia; Different Class; Generative Large Language Model; High Quality; Knowledge Graphs; Language Model; Named Entity Recognition; Over Sampling; Power; Benchmarking},
	keywords = {Computational linguistics; Knowledge graph; Knowledge management; Large datasets; Semantics; Class distributions; Dbpedia; Different class; Generative large language model; High quality; Knowledge graphs; Language model; Named entity recognition; Over sampling; Power; Benchmarking},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Xie2024133,
	author = {Xie, Jiangcun and Li, Ren and Yang, Jianxi and Xiao, Qiao},
	title = {Ontology Embeddings for Subsumption Prediction Based on Graph Language Model},
	year = {2024},
	pages = {133 - 137},
	doi = {10.1109/DSInS64146.2024.10992171},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007428875&doi=10.1109%2FDSInS64146.2024.10992171&partnerID=40&md5=2b1f28e91fc4db4db48fd3da725f8872},
	abstract = {With the growing importance of knowledge graphs in artificial intelligence, accurately modeling hierarchical relationships in ontologies has become a critical issue in knowledge representation learning. To address this, this paper proposes an ontology embedding framework based on graph language models, named GLMSubs, aimed at enhancing the prediction of subclass relationships. The GLMSubs framework adopts a two-stage strategy of 'multi-semantic view partitioning' and 'advanced training of graph language models'. Initially, it deconstructs the ontology's concepts, attributes, and instance information into five types of semantic views, such as class hierarchy view and class-attribute relationship view, through a multi-view partitioning mechanism, comprehensively capturing information from different semantic dimensions. Subsequently, the framework employs graph language models for joint training on the multi-view data to obtain embeddings that integrate both semantic and structural information. Experiments on datasets such as FoodOn and GO validate the effectiveness of GLMSubs, demonstrating that its performance in class hierarchy relationship prediction tasks significantly surpasses existing methods. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Language Models; Knowledge Graph; Knowledge Representation Learning; Owl Ontology; Image Representation; Embeddings; Graph Language Model; Graph Languages; Knowledge Graphs; Knowledge Representation Learning; Knowledge-representation; Language Model; Ontology's; Owl Ontologies; Semantic Views; Knowledge Graph},
	keywords = {Image representation; Embeddings; Graph language model; Graph languages; Knowledge graphs; Knowledge representation learning; Knowledge-representation; Language model; Ontology's; OWL ontologies; Semantic views; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Capshaw2024,
	author = {Capshaw, Riley and Blomqvist, Eva},
	title = {LINTEXT: A Visual Tool for Exploring and Modeling Knowledge in Text Documents},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3967},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006925553&partnerID=40&md5=8445ff346393a09f7d1bd79907c48606},
	abstract = {A large part of knowledge is commonly encoded into text documents. While extracting this information into a Knowledge Graph (KG) is a common approach, it suffers from challenges when texts are added, removed, or changed, or when the schema of the intended KG changes. Instead we advocate an approach where text and models evolve together in an interactive manner. We present LINTEXT, a system accompanying a published method which allows users to jointly explore and model the information held within text documents. The modeling is accomplished by specifying fill-in-the-blank prompts along with some metadata which are then recorded as specifications for simple relations that can be used to generate an ontology. The exploration aspect is accomplished by having the system complete each prompt with entities identified from the text and presenting the completions as a ranked list to the user, allowing users to verify the quality of the extracted triples. By elevating the development of the ontology to a visual and interactive level, it has an immediate text connection and users can be more certain that the documents they wish to model contain the information they wish to extract or query. Additionally, our system is designed to support the development of relation extraction (RE) pipelines underlying the document analysis, with a particular focus on supporting methods for improving vector representations of the extracted entities. To this end, users can choose to analyze documents from pre-annotated RE data sets to understand how changes in different elements of the pipeline affect the results. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Document-level Relation Extraction; Entity Embedding; Interactive Knowledge Modeling; Knowledge Graphs; Machine Reading; Masked Language Models; Document-level Relation Extraction; Embeddings; Entity Embedding; Interactive Knowledge Modeling; Knowledge Graphs; Knowledge Model; Language Model; Machine Reading; Masked Language Model; Relation Extraction; Knowledge Graph},
	keywords = {Document-level relation extraction; Embeddings; Entity embedding; Interactive knowledge modeling; Knowledge graphs; Knowledge model; Language model; Machine reading; Masked language model; Relation extraction; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lu2024,
	author = {Lu, Qiuhao and Wen, Andrew and Nguyen, Thien Huu and Liu, Hongfang},
	title = {Enhancing Clinical Relevance of Pretrained Language Models Through Integration of External Knowledge: Case Study on Cardiovascular Diagnosis From Electronic Health Records},
	year = {2024},
	journal = {JMIR AI},
	volume = {3},
	pages = {},
	doi = {10.2196/56932},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006921223&doi=10.2196%2F56932&partnerID=40&md5=882b3e2f5778e06e6aa30306fb00d89d},
	abstract = {Background: Despite their growing use in health care, pretrained language models (PLMs) often lack clinical relevance due to insufficient domain expertise and poor interpretability. A key strategy to overcome these challenges is integrating external knowledge into PLMs, enhancing their adaptability and clinical usefulness. Current biomedical knowledge graphs like UMLS (Unified Medical Language System), SNOMED CT (Systematized Medical Nomenclature for Medicine–Clinical Terminology), and HPO (Human Phenotype Ontology), while comprehensive, fail to effectively connect general biomedical knowledge with physician insights. There is an equally important need for a model that integrates diverse knowledge in a way that is both unified and compartmentalized. This approach not only addresses the heterogeneous nature of domain knowledge but also recognizes the unique data and knowledge repositories of individual health care institutions, necessitating careful and respectful management of proprietary information. Objective: This study aimed to enhance the clinical relevance and interpretability of PLMs by integrating external knowledge in a manner that respects the diversity and proprietary nature of health care data. We hypothesize that domain knowledge, when captured and distributed as stand-alone modules, can be effectively reintegrated into PLMs to significantly improve their adaptability and utility in clinical settings. Methods: We demonstrate that through adapters, small and lightweight neural networks that enable the integration of extra information without full model fine-tuning, we can inject diverse sources of external domain knowledge into language models and improve the overall performance with an increased level of interpretability. As a practical application of this methodology, we introduce a novel task, structured as a case study, that endeavors to capture physician knowledge in assigning cardiovascular diagnoses from clinical narratives, where we extract diagnosis-comment pairs from electronic health records (EHRs) and cast the problem as text classification. Results: The study demonstrates that integrating domain knowledge into PLMs significantly improves their performance. While improvements with ClinicalBERT are more modest, likely due to its pretraining on clinical texts, BERT (bidirectional encoder representations from transformer) equipped with knowledge adapters surprisingly matches or exceeds ClinicalBERT in several metrics. This underscores the effectiveness of knowledge adapters and highlights their potential in settings with strict data privacy constraints. This approach also increases the level of interpretability of these models in a clinical context, which enhances our ability to precisely identify and apply the most relevant domain knowledge for specific tasks, thereby optimizing the model’s performance and tailoring it to meet specific clinical needs. Conclusions: This research provides a basis for creating health knowledge graphs infused with physician knowledge, marking a significant step forward for PLMs in health care. Notably, the model balances integrating knowledge both comprehensively and selectively, addressing the heterogeneous nature of medical knowledge and the privacy needs of health care institutions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Adapters; Data Privacy; Ehr; Electronic Health Record; Electronic Health Records; Healthcare; Healthcare Data; Healthcare Institution; Healthcare Institutions; Heterogeneous; Knowledge Integration; Medical Knowledge; Methodology; Physician; Physician Reasoning; Physicians; Pre-trained Language Models; Proprietary Information; Text Classification},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Svátek2024,
	author = {Svátek, Vojtech and Ŝváb-Zamazal, Ondr̂ej Ř.Ej and Haniková, Kateřina and Chudán, David and Saeedizade, Mohammad Javad and Blomqvist, Eva},
	title = {Welcome, newborn entity! on handling newly generated entities in ontology transformation},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3967},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006920019&partnerID=40&md5=a57ad8bb2d6f629d92714d9b77231eb1},
	abstract = {Modeling can be seen both as an engineering and a design task. Even when clear requirements are available for an ontology modeling endevour, many times the requirements can be solved in several different ways. Even further, when requirements change and evolve, a certain modeling style, or pattern, may no longer be the best choice. However, refactoring an ontology due to such changes in requirements, or due to the desire to align better with other external ontologies or data sets, is a complex and tedious process, also requiring extensive expertise. Attempting to automate part of the ontology transformation process, by identifying typical transformation patterns, and creating tool support for their semi-automated application, is therefore an important research topic. One specific sub-task of such automation is the naming of new entities (e.g. classes or properties) that are generated through the pattern application. In this paper we discuss the need for such automated naming support, and show the feasibility of introducing Large Language Models (LLMs) for taking the automation one step further. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Entity Naming; Large Language Models; Ontology Transformation; Owl; Best Choice; Design Tasks; Entity Naming; Language Model; Large Language Model; Ontology Model; Ontology Transformation; Ontology's; Owl; Requirements Change; Requirements Engineering},
	keywords = {Best choice; Design tasks; Entity naming; Language model; Large language model; Ontology model; Ontology transformation; Ontology's; OWL; Requirements change; Requirements engineering},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Huang2024,
	author = {Huang, Yiwen and Karabulut, Erkan and Degeler, Victoria},
	title = {Large Language Model for Ontology Learning in Drinking Water Distribution Network Domain},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3967},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006919338&partnerID=40&md5=481aaad47de18ac4cf4a517059109b8b},
	abstract = {Currently, most ontologies are created manually, which is time-consuming and labour-intensive. Meanwhile, the advanced capabilities of Large Language Models (LLMs) have proven beneficial in various domains, significantly improving the efficiency of text processing and text generation. Therefore, this paper focuses on the use of LLMs for ontology learning. It uses a manual ontology construction method as a basis to facilitate the LLMs for ontology learning. The proposed approach is based on Retrieval Augmented Generation (RAG), and passed queries to LLMs are based upon the manual ontology method - UPON Lite ontology. Two different variants of LLMs have been experimented with, and they all demonstrate the capability of ontology learning to varying degrees. This approach shows promising initial results in the direction of (semi-) automated ontology learning using LLMs and makes the ontology construction process easier for people without prior domain expertise.The final ontology was evaluated by the domain expert and ranked according to the defined criteria. Based on the evaluation results, the final ontology could be used as a base version, but it requires further fine-tuning by domain experts to ensure its accuracy and completeness. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Drinking Water Distribution Network; Llm; Ontology Learning; Domain Experts; Drinking Water Distribution Networks; Labour-intensive; Language Model; Large Language Model; Network Domains; Ontology Construction; Ontology Learning; Ontology's; Text-processing; Ontology},
	keywords = {Domain experts; Drinking water distribution networks; Labour-intensive; Language model; Large language model; Network domains; Ontology construction; Ontology learning; Ontology's; Text-processing; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Georgakopoulos2024,
	author = {Georgakopoulos, Antonios and Van Ossenbruggen, Jacco and Stork, Lise},
	title = {From Text to Knowledge: Leveraging LLMs and RAG for Relationship Extraction in Ontologies and Thesauri},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3967},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006913033&partnerID=40&md5=ea70a64b726d1dc660804bf58efadf08},
	abstract = {Ontologies, vocabularies, and thesauri provide a shared conceptualisation for a domain. Manually maintaining and updating such knowledge systems when knowledge changes, does not scale for large domains, such as in biomedicine. Recently, large language models (LLMs) have been increasingly used as tools in knowledge engineering processes, offering new possibilities for the automatic creation and maintenance of knowledge systems. This work explores how LLMs can be leveraged for the automated extension of such knowledge systems. Specifically, we build on the DRAGON-AI framework, which integrates Retrieval-Augmented Generation (RAG) to provide LLMs with access to external knowledge sources for more faithful outputs. We investigate the ability of the framework to predict relationships between a given knowledge system and a novel concept. We do so for both an ontology and a thesaurus, and analyse the impact of (i) enriching prompts with contextual information as well as more clear instructions, (ii) an alternative retrieval approach, and (iii) using a conversational model versus an instruction-following model. The results show superior quality in the ontology generations for all models and approaches compared to the thesaurus. The two models show varied performance across the different experiment configurations with only the conversational model showing notably improved performance, in terms of F1, for the ontology with the custom retrieval approach. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Dragon-ai; Knowledge Engineering; Large Language Models; Ontology; Prompting; Rag; Thesaurus; Conversational Model; Dragon-ai; Knowledge System; Language Model; Large Language Model; Ontology's; Performance; Prompting; Relationship Extraction; Retrieval-augmented Generation; Knowledge Acquisition},
	keywords = {Conversational model; DRAGON-AI; Knowledge system; Language model; Large language model; Ontology's; Performance; Prompting; Relationship extraction; Retrieval-augmented generation; Knowledge acquisition},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {EKAW-PDWT 2024 - Joint Proceedings of Posters, Demos, Workshops, and Tutorials of the 24th International Conference on Knowledge Engineering and Knowledge Management, co-located with 24th International Conference on Knowledge Engineering and Knowledge Management, EKAW 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3967},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006909895&partnerID=40&md5=88c7731008b0ac94338bcc8e3160f32a},
	abstract = {The proceedings contain 45 papers. The topics discussed include: named entity recognition for digitized archival documents in German; welcome, newborn entity! on handling newly generated entities in ontology transformation; evaluating large language model literature reviews in interdisciplinary science: a systems biology perspective; leveraging meta-modelling language for ontology structuring and validation; taxonomy for patent classification: a step towards intelligent patent analytics; towards synthesizing e-mail conversations as part of knowledge work datasets; automatic ontology term typing by LLMs: the impact of prompt and ontology variation; on the role of preprocessing on matching tables to knowledge graphs; and unlocking historical knowledge: a semantic web approach to medieval notarial document analysis. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Godest2024,
	author = {Godest, Frederic and El Alaoui, M. and Richet, Victor and Plana, Robert and Benmiloud-Bechet, Lies and Bossu, Jean François and Malhomme, Olivier},
	title = {Enhanced LLM for smart Knowledge Management in nuclear industry},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3967},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006908739&partnerID=40&md5=7a211c0fd5dc2faa3dc8e621cf9c59c0},
	abstract = {The nuclear industry, characterized by its large-scale projects and intricate processes, requires robust knowledge management (KM) strategies. Traditional KM approaches, such as training, documentation, and expert networks, have been employed to address this need . Documentation, though, has challenged by its volume, the outcome of this approach. However, the advent of AI and Large Language Models (LLMs) has opened new avenues for KM innovation. This paper explores the integration of CurieLM, a domain-specific LLM, with a nuclear ontology to improve the quality of knowledge retrieval and answers generation. By automatically expanding the input context through ontology-driven enrichment, our approach aims to address the shortcomings of existing KM methods, offering a scalable and efficient solution for the nuclear industry's unique challenges. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Management; Large Language Model; Nuclear Industry; Ontology; Knowledge Transfer; Ontology; Domain Specific; Expert Networks; Knowledge Management Strategy; Language Model; Large Language Model; Large-scale Projects; Management Innovation; Ontology's; Traditional Knowledge; Training Network; Knowledge Organization},
	keywords = {Knowledge transfer; Ontology; Domain specific; Expert networks; Knowledge management strategy; Language model; Large language model; Large-scale projects; Management innovation; Ontology's; Traditional knowledge; Training network; Knowledge organization},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Fathallah2024,
	author = {Fathallah, Nadeen and Staab, Steffen and Algergawy, Alsayed},
	title = {LLMs4Life: Large Language Models for Ontology Learning in Life Sciences},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3967},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006905423&partnerID=40&md5=f37af3ab894f04b53e3fc593ac858616},
	abstract = {Ontology learning in complex domains, such as life sciences, poses significant challenges for current Large Language Models (LLMs). Existing LLMs struggle to generate ontologies with multiple hierarchical levels, rich interconnections, and comprehensive class coverage due to constraints on the number of tokens they can generate and inadequate domain adaptation. To address these issues, we extend the NeOn-GPT pipeline for ontology learning using LLMs with advanced prompt engineering techniques and ontology reuse to enhance the generated ontologies' domain-specific reasoning and structural depth. Our work evaluates the capabilities of LLMs in ontology learning in the context of highly specialized and complex domains such as life science domains. To assess the logical consistency, completeness, and scalability of the generated ontologies, we use the AquaDiva ontology developed and used in the collaborative research center AquaDiva 1 as a case study. Our evaluation shows the viability of LLMs for ontology learning in specialized domains, providing solutions to longstanding limitations in model performance and scalability. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Life Science Domain; Neon-gpt; Ontology Learning; Computer Simulation Languages; Engineering Education; Hierarchical Systems; Linguistics; Problem Oriented Languages; Reusability; Scalability; 'current; Complex Domains; Hierarchical Level; Language Model; Large Language Model; Life Science Domain; Life-sciences; Neon-gpt; Ontology Learning; Ontology's; Ontology},
	keywords = {Computer simulation languages; Engineering education; Hierarchical systems; Linguistics; Problem oriented languages; Reusability; Scalability; 'current; Complex domains; Hierarchical level; Language model; Large language model; Life science domain; Life-sciences; Neon-GPT; Ontology learning; Ontology's; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bhattacharya2024,
	author = {Bhattacharya, Upal and de Boer, Maaike H.T. and Sosnovsky, Sergey A.},
	title = {Automatic Ontology Term Typing by LLMs: The Impact of Prompt and Ontology Variation},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3967},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006903401&partnerID=40&md5=ed7b3771dc19477cc1c99014f80d8788},
	abstract = {Large Language Models (LLMs) have been applied to a wide variety of ontology engineering tasks. Building on initial progress, further research is needed to explore potential effects of variation over model-specific and ontology-specific factors. We perform a preliminary study on the ability of an LLM to perform term typing using only its own knowledge through concept retrieval and analyse the effect of domain contextualisation, ontology structure and popularity of ontologies on performance. Our findings suggest that LLMs are reasonably adept at identifying correct individual to concept assertions but are less capable of inferring concept hierarchies when used in a zero-shot setting. Domain contextualisation can enhance performance for structurally complex and less-popular ontologies. Our analysis furthers hints at ontology popularity improving concept retrievability while complexity in terms of structural depth and dispersion makes it difficult for LLMs to identify assertions. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Individual Assertion; Llm; Ontology Evaluation; Ontology Learning; Term Typing; Automatic Ontology; Contextualization; Individual Assertion; Language Model; Large Language Model; Ontology Evaluations; Ontology Learning; Ontology's; Performance; Term Typing; Ontology},
	keywords = {Automatic ontology; Contextualization; Individual assertion; Language model; Large language model; Ontology evaluations; Ontology learning; Ontology's; Performance; Term typing; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Santos2024,
	author = {Santos, Joana and Silva, Nuno and Ferreira, Carlos Abreu and Gama, João M.P.},
	title = {Online News Classification Using Large Language Models with Semantic Enrichment},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3967},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006892349&partnerID=40&md5=0d6fcb08b9545ec43f3ec026c0ba8d5b},
	abstract = {This paper addresses a critical gap in applying semantic enrichment for online news text classification using large language models (LLMs) in fast-paced newsroom environments. While LLMs excel in static text classification tasks, they struggle in real-time scenarios where news topics and narratives evolve rapidly. The dynamic nature of news, with frequent introductions of new concepts and events, challenges pre-trained models, which often fail to adapt quickly to changes. Additionally, the potential of ontology-based semantic enrichment to enhance model adaptability in these contexts has been underexplored. To address these challenges, we propose a novel supervised news classification system that incorporates semantic enrichment to enhance real-time adaptability. This approach bridges the gap between static language models and the dynamic nature of modern newsrooms. The system operates on an adaptive prequential learning framework, continuously assessing model performance on incoming data streams to simulate real-time newsroom decision-making. It supports diverse content formats - text, images, audio, and video - and multiple languages, aligning with the demands of digital journalism. We explore three strategies for deploying LLMs in this dynamic environment: using pre-trained models directly, fine-tuning classifier layers while freezing the initial layers to accommodate new data, and continuously fine-tuning the entire model using real-time feedback combined with data selected based on specified criteria to enhance adaptability and learning over time. These approaches are evaluated incrementally as new data is introduced, reflecting real-time news cycles. Our findings demonstrate that ontology-based semantic enrichment consistently improves classification performance, enabling models to adapt effectively to emerging topics and evolving contexts. This study highlights the critical role of semantic enrichment, prequential evaluation, and continuous learning in building robust and adaptive news classification systems capable of thriving in the rapidly evolving digital news landscape. By augmenting news content with third-party ontology-based knowledge, our system provides deeper contextual understanding, enabling LLMs to navigate emerging topics and shifting narratives more effectively. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Llm; News Classification; Nlp; Semantic Enrichment; Supervised Online Learning; Labeled Data; Language Model; Large Language Model; News Classification; Online Learning; Online News; Ontology-based; Real- Time; Semantic Enrichment; Supervised Online Learning; Text Classification; Supervised Learning},
	keywords = {Labeled data; Language model; Large language model; News classification; Online learning; Online news; Ontology-based; Real- time; Semantic enrichment; Supervised online learning; Text classification; Supervised learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Munzir2024838,
	author = {Munzir, Syed Ilyas and Hier, Daniel B. and Oommen, Chelsea and Carrithers, Michael D.},
	title = {A Large Language Model Outperforms Other Computational Approaches to the High-Throughput Phenotyping of Physician Notes},
	year = {2024},
	journal = {AMIA Annual Symposium proceedings},
	volume = {2024},
	pages = {838 - 846},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006856874&partnerID=40&md5=00e2cd51f40024511128007e7c1523d8},
	abstract = {High-throughput phenotyping, the automated mapping of patient signs and symptoms to standardized ontology concepts, is essential for realizing value from electronic health records (EHR) in support of precision medicine. Despite technological advances, high-throughput phenotyping remains a challenge. This study compares three computational approaches to high-throughput phenotyping: a large language model (LLM) incorporating generative AI, a deep learning (DL) approach utilizing span categorization, and a machine learning (ML) approach with word embeddings. The LLM approach that implemented GPT-4 demonstrated superior performance, suggesting that large language models are poised to become the preferred method for high-throughput phenotyping ofphysician notes. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine},
	author_keywords = {Artificial Intelligence; Comparative Study; Deep Learning; Electronic Health Record; Human; Language; Large Language Model; Machine Learning; Natural Language Processing; Phenotype; Artificial Intelligence; Deep Learning; Electronic Health Records; Humans; Language; Large Language Models; Machine Learning; Natural Language Processing; Phenotype},
	keywords = {artificial intelligence; comparative study; deep learning; electronic health record; human; language; large language model; machine learning; natural language processing; phenotype; Artificial Intelligence; Deep Learning; Electronic Health Records; Humans; Language; Large Language Models; Machine Learning; Natural Language Processing; Phenotype},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Libro2024,
	author = {Libro, Mario and Gelain, Stefano and Fummi, Franco},
	title = {Enhancing Trustworthiness and Formalization in the Construction Industry with Modeling Languages and Ontologies},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3960},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105006594317&partnerID=40&md5=5176c3af32d51014af53a087e7e9fec1},
	abstract = {The construction industry is rapidly evolving with the advent of new technologies. Despite this progress, large-scale projects remain plagued by complexity and diverse operations that hinder automation and productivity, leading to inefficiencies and lower profitability. These challenges are further amplified by traditional practices and the insufficient adoption of modern technologies. This research focuses on the adoption of Model-based System Engineering (MBSE), particularly on enriching System Modeling Language (SysML) models with ontologies. By leveraging ontologies as a comprehensive knowledge base that encapsulates essential details about the construction industry domain, providing formal support and enabling automated reasoning to verify model consistency. The proposed methodology aims at providing significant improvements in system design robustness and operational reliability, offering an effective solution for the automation and efficiency challenges in the industries. The introduction of the new SysML v2 standard promises to further enhance the integration between SysML models and ontologies, laying the groundwork for adapting the proposed methodology within the construction sector. Future research will explore the application of the methodology in the cement-based production industry, the modeling of legal contracts in SysML, and the integration of SysML models with blockchain technology to automate smart contract generation and enhance traceability in industrial operations. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Representation; Model-based Systems Engineering; Ontologies; Cements; Industrial Research; Smart Contract; System Of Systems; Automated Reasoning; Formalisation; Knowledge-representation; Language Model; Large-scale Projects; Model-based System Engineerings; Modern Technologies; Ontology's; Research Focus; System Models; Construction Industry},
	keywords = {Cements; Industrial research; Smart contract; System of systems; Automated reasoning; Formalisation; Knowledge-representation; Language model; Large-scale projects; Model-based system engineerings; Modern technologies; Ontology's; Research focus; System models; Construction industry},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2024,
	title = {PI-AI 2024 - Proceedings of the 2nd Workshop on Public Interest AI, co-located with the German Conference on AI, KI 2024},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3958},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105005722302&partnerID=40&md5=81417525bb85517fb054ba1fef784bf5},
	abstract = {The proceedings contain 7 papers. The topics discussed include: generating synthetic satellite imagery for rare objects: an empirical comparison of models and metrics; harnessing probabilistic logic programming for the transparent local modelling of infectious diseases; towards a semantic format for FIM: supporting towards a semantic format for FIM: supporting German public services using the GerPS-FIM-Microverse ontology pipeline; data-driven modeling of combined sewer systems for urban sustainability: an empirical evaluation; GPT-NL: towards a public interest large language model; and Weizenbaum’s legacy in the era of public interest AI. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Balaadich202474,
	author = {Balaadich, Youness and Jakimi, Abdeslam},
	title = {Toward an Advanced Rural Tourism Ontology for Enhancing Visitor Experiences in Morocco's Draa-Tafilalet Region},
	year = {2024},
	pages = {74 - 79},
	doi = {10.1109/ICCTA64612.2024.10974777},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004877195&doi=10.1109%2FICCTA64612.2024.10974777&partnerID=40&md5=c4c94baef32ba16ec29205c6437cd8f2},
	abstract = {Integrating ontologies in tourism applications represents a transformative step toward achieving a structured digital transformation to enhance the competitive edge of tourism destinations. In Morocco's tourism sector, particularly in the culturally and naturally rich Draa-Tafilalet region, implementing an ontology is essential to organize diverse tourism-related data and provide a more personalized and accessible visitor experience. Despite Morocco's significant efforts, the tourism industry continues to face persistent challenges in delivering customized and accessible information aligned with tourists' preferences and needs. To address this issue, this study develops comprehensive ontology using a semi-Automated methodology that combines natural language processing, language models, and expert validation. The ontology encapsulates attractions, services, and visitor preferences specific to the region. The main objective is to modernize and enhance the informational structure of the tourism sector, facilitating better navigation for tourists and supporting the strategic promotion of the region's assets. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Digital Transformation; Ontology; Personalized Experiences; Rural Tourism; Leisure; Natural Language Processing Systems; Ontology; Competitive Edges; Customized Information; Digital Transformation; Ontology's; Personalized Experience; Rural Tourisms; Tourism Application; Tourism Industry; Tourism Sectors; Visitor Experiences; Tourism},
	keywords = {Leisure; Natural language processing systems; Ontology; Competitive edges; Customized information; Digital transformation; Ontology's; Personalized experience; Rural tourisms; Tourism application; Tourism industry; Tourism sectors; Visitor experiences; Tourism},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Trokanas2024,
	author = {Trokanas, Nikos and Bussemaker, Madeleine J. and Antonis, Kokossis and Cecelja, Franjo V.},
	title = {Combining Ontologies and Llms to Evaluate and Improve Engineering Curricula},
	year = {2024},
	volume = {2024-October},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003717626&partnerID=40&md5=d3d9ccb4574f351841fee9e5e5ee9804},
	abstract = {This research introduces an innovative framework to evaluate and enhance engineering curricula by integrating domain-specific ontologies with Large Language Models (LLMs). As engineering fields continuously evolve, curricula must keep pace with the rapid advancement of technologies and industry needs. Traditional curriculum evaluation methods rely on static criteria and limited interdisciplinary perspectives, often failing to capture the complexities of modern engineering requirements. This study addresses these challenges by creating a more adaptive, comprehensive, and data-driven approach to evaluating and updating engineering programs. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Engineering Research; Requirements Engineering; Curriculum Evaluation; Data-driven Approach; Domain-specific Ontologies; Engineering Curriculum; Engineering Fields; Evaluation Methods; Industry Needs; Language Model; Modern Engineering; Ontology's; Engineering Education},
	keywords = {Engineering research; Requirements engineering; Curriculum evaluation; Data-driven approach; Domain-specific ontologies; Engineering curriculum; Engineering fields; Evaluation methods; Industry needs; Language model; Modern engineering; Ontology's; Engineering education},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{David202416,
	author = {David, Robert and Kernerman, Anna and Kernerman, Ilan and Ferranti, Nicolas and Siani, Assaf},
	title = {Multilingual Word Sense Disambiguation for Semantic Annotations: Fusing Knowledge Graphs, Lexical Resources, and Large Language Models},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3950},
	pages = {16 - 22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002726059&partnerID=40&md5=af4445a9d3da5f982a12a2f368f14c47},
	abstract = {Knowledge models, constructed from vocabularies and ontologies, establish a formal basis to enable semantic annotations, which can support retrieval use cases in the context of Retrieval Augmented Generation (RAG) systems. In such a scenario, we face the challenges of word sense disambiguation (WSD), multiword expressions (MWE), and multilinguality (of models and content) in the retrieval process. For WSD and MWE, there is a need for contextual knowledge to differentiate word senses of expressions in the content. For multilinguality, we aim for systems which support content that comes in a mix of languages, as well as querying across languages. To support both goals, we propose a combination of knowledge models, multilingual linguistic data (including lexicographic resources) and large language models (LLMs). Via dictionaries with additional lexical information for multiple languages, we implement cross-language queries, and with the integration of LLMs we use these quality language resources to drive multilingual disambiguation for Graph RAG systems. In this paper, we present research carried out jointly by Semantic Web Company and Lexicala by K Dictionaries, including our approach and methodology along with preliminary results of our experiments on converging language resources, knowledge graphs, and large language models. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Rag; Knowledge Graphs; Language Resources; Large Language Models; Multilingual; Semantic Annotation; Word Sense Disambiguation; Glossaries; Nonbibliographic Retrieval Systems; Query Languages; Semantics; Wsdl; Generation Systems; Graph Retrieval Augmented Generation; Knowledge Graphs; Knowledge Model; Language Model; Language Resources; Large Language Model; Multilingual; Semantic Annotations; Word Sense Disambiguation; Knowledge Graph},
	keywords = {Glossaries; Nonbibliographic retrieval systems; Query languages; Semantics; WSDL; Generation systems; Graph retrieval augmented generation; Knowledge graphs; Knowledge model; Language model; Language resources; Large language model; Multilingual; Semantic annotations; Word Sense Disambiguation; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Baidya20246913,
	author = {Baidya, Anushuya and Do, Tuyen and Gnimpieba Zohim, Etienne Zohim},
	title = {Toward Fine-Tuning Large Language Models in Ontology of Microbial Phenotypes Construction},
	year = {2024},
	pages = {6913 - 6920},
	doi = {10.1109/BIBM62325.2024.10947604},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002721969&doi=10.1109%2FBIBM62325.2024.10947604&partnerID=40&md5=31245c00f760e59fd3acd1b111a5c028},
	abstract = {Ontologies are crucial for organizing domainspecific knowledge in biomedical fields, but their manual construction is time-consuming. This study explores the automation of ontology learning using large language models (LLMs) like BERT, RoBERTa, and DistilBERT, focusing on the Ontology of Microbial Phenotypes (OMP). We investigate three key tasks: (1) entity extraction, (2) relation extraction between entities, and (3) ontology verification. These tasks align with broader applications in biomedical annotation and named entity recognition (NER) by enabling the identification and structuring of key terms and relationships within microbial phenotypes. We evaluate LLMs in two scenarios: baseline performance using pre-Trained models and fine-Tuned performance after training on OMP-specific data. Our approach integrates spaCy for entity extraction, Llama 2 for relation identification, and LLMs for ontology verification. Experiments reveal that fine-Tuned models significantly improve accuracy, precision, recall, and F1 scores, particularly for ontology verification. This research highlights the potential of LLMs to enhance ontology learning and support related biomedical applications like biofilm analysis, annotation, and NER, while emphasizing the value of expert curation. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Fine-tuning; Large Language Models; Microbial Phenotypes; Ontology Learning; Ontology Verification; Microorganisms; Entity Extractions; Fine Tuning; Language Model; Large Language Model; Microbial Phenotype; Microbials; Named Entity Recognition; Ontology Learning; Ontology Verification; Ontology's; Ontology},
	keywords = {Microorganisms; Entity extractions; Fine tuning; Language model; Large language model; Microbial phenotype; Microbials; Named entity recognition; Ontology learning; Ontology verification; Ontology's; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Garigliotti202423,
	author = {Garigliotti, Darío},
	title = {Retrieval-Augmented Generation for Query Target Type Identification},
	year = {2024},
	journal = {CEUR Workshop Proceedings},
	volume = {3950},
	pages = {23 - 30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002709216&partnerID=40&md5=83aec9594ccc26341748d3c5cf8cd724},
	abstract = {The paradigm shift unleashed by Entity-Oriented Search still characterizes a vast space of the dynamics with which users engage in digital information access, from Web search to e-commerce and social networks. The progress in research around Entity Retrieval tasks has in particular shown the convenience of incorporating type-based information for entities in their methods to provide relevant answers to queries. As types are typically accessible in an ontology of reference within the knowledge base where their assigned entities live, automatically identifying query target types is a relevant problem to tackle. In this work, we propose to address the task of Query Target Type Identification by assessing the capabilities of Large Language Models that have recently shown widespread success. Our experimentation with methods based on Retrieval-Augmented Generation over a purposely built test collection from the literature challenges a well-established closed LLM by presenting it with entity type information from a resource within the core in hubbing Linked Open Data. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Query Languages; Digital Information; E- Commerces; Entity Retrieval; Information Access; Language Model; Ontology's; Paradigm Shifts; Target Type; Test Collection; Web Searches; Structured Query Language},
	keywords = {Query languages; Digital information; E- commerces; Entity retrieval; Information access; Language model; Ontology's; Paradigm shifts; Target type; Test Collection; Web searches; Structured Query Language},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Roetzer2024365,
	author = {Roetzer, Florian and Gobel, Kai and Liebetreu, Maximilian and Strommer, Stephan},
	title = {Knowledge Graph Extraction from Retrieval-Augmented Generator: An Application in Aluminium Die Casting},
	year = {2024},
	journal = {Proceedings of the International Conference on Informatics in Control, Automation and Robotics},
	volume = {2},
	pages = {365 - 376},
	doi = {10.5220/0012951000003822},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002126872&doi=10.5220%2F0012951000003822&partnerID=40&md5=6b242aa1820fb9997209a66ed948d924},
	abstract = {We present a novel, efficient, and scalable approach for generating knowledge graphs (KGs) tailored to specific competency questions, leveraging large language model (LLM)-based retrieval-augmented generation (RAG) as a source of high-quality text data. Our method utilises a predefined ontology and defines two agents: The first agent extracts entities and triplets from the text corpus maintained by the RAG, while the second agent merges similar entities based on labels and descriptions, using embedding functions and LLM reasoning. This approach does not require fine-tuning or additional AI training, and relies solely on off-the-shelf technologies. Additionally, due to the use of RAG, the method can be used with a text corpus of arbitrary size. We applied our method to the high-pressure die casting domain, focusing on defects and their causes. In the absence of annotated datasets, manual evaluation of the resulting KGs showed over 90% precision in entity extraction and around 70% precision in triplet extraction, the main source of error being the RAG itself. Our findings suggest that this method can significantly aid in the rapid generation of customised KGs for specific applications. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {High-pressure Die Casting; Knowledge Graph Extraction; Knowledge Graph Generation; Large Language Model; Retrieval-augmented Generation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ying2024457,
	author = {Ying, Zhao and Weiyu, Chen and Longlong, Liao and Jie, Liu},
	title = {Masked Theme-Specific Named Entity Recognition Assisted with Large Language Models},
	year = {2024},
	pages = {457 - 462},
	doi = {10.1109/ICOCO62848.2024.10928264},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002040147&doi=10.1109%2FICOCO62848.2024.10928264&partnerID=40&md5=77736378fcbefcbc8d49f4bd57f5128a},
	abstract = {Existing Named Entity Recognition (NER) methods are required to label relevant samples and train the concrete NER models. Due to the specification of theme-specific documents, these NER models are considerably hard to identify potential theme-specific entities. To address this challenge, we propose an effective two-stage approach of masked theme-specific NER associated with Large Language Models (LLMs), which uses the unsupervised mechanism rather than the supervised one. The approach involves theme-specific entity ontology construction and masked NER in heterogeneous documents. The first stage is associated with LLMs and Wikipedia category pages, and the second one is implemented with the masked NER based on the created ontology in the first stage. Extensive experimental results suggest that the proposed masked NER can precisely locate the known entities in the theme-specific entity ontology while improving the accuracy of NER in the remaining text. Compared to the mainstream NER frameworks such as spaCy 3, the masked NER can identify more valid entities in the input Markdown text and continuously use the newly detected unknown entities to update the created ontology. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Heterogeneous Documents; Large Language Model; Named Entity Recognition; Natural Language Processing; Theme-specific Entity Ontology; Modeling Languages; Natural Language Processing Systems; Heterogeneous Documents; Language Model; Language Processing; Large Language Model; Named Entity Recognition; Natural Language Processing; Natural Languages; Ontology's; Recognition Models; Theme-specific Entity Ontology; Ontology},
	keywords = {Modeling languages; Natural language processing systems; Heterogeneous documents; Language model; Language processing; Large language model; Named entity recognition; Natural language processing; Natural languages; Ontology's; Recognition models; Theme-specific entity ontology; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Qi2024553,
	author = {Qi, Jiewei and Luo, Ling and Yang, Zhihao and Wang, Jian and Lin, Hongfei},
	title = {Ontology Information-augmented Human Phenotype Concept Recognition; 基于本体信息增强的人类表型概念识别},
	year = {2024},
	volume = {1},
	pages = {553 - 567},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001920502&partnerID=40&md5=f29df4d6aca5a1a8df5c00376ca713d2},
	abstract = {Automatic phenotype concept recognition from text is of great significance for disease analysis. Existing ontology-driven phenotype concept recognition methods mainly utilize concepts and synonyms information within the ontology, without fully considering the rich information of the ontology. To address this issue, this paper proposes an ontology-enhanced method for human phenotype concept recognition, which utilizes advanced large language models for data augmentation and designs a deep learning model enhanced with ontology vectors to improve concept recognition performance. Experimental results on the GSC+ and ID-68 datasets demonstrate that the proposed method effectively leverages the rich ontology information to enhance the performance of baseline models, achieving state-of-the-art results. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Text Mining; Human Phenotype Ontology; Ontology Information Enhancement; Deep Learning; Ontology; Biomedical Text Minings; Concept Recognition; Disease Analysis; Human Phenotype Ontology; Language Model; Ontology Information Enhancement; Ontology's; Performance; Recognition Methods; Computational Linguistics},
	keywords = {Deep learning; Ontology; Biomedical text minings; Concept recognition; Disease analysis; Human phenotype ontology; Language model; Ontology information enhancement; Ontology's; Performance; Recognition methods; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Nguyen2024169,
	author = {Nguyen, Tam N.},
	title = {Towards human digital twins for healthcare agent-based modeling in the Metaverse},
	year = {2024},
	pages = {169 - 194},
	doi = {10.1016/B978-0-443-21996-2.00013-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001780564&doi=10.1016%2FB978-0-443-21996-2.00013-9&partnerID=40&md5=d3f3e75bd6d874f284706f81681ca635},
	abstract = {Agent-based modeling (ABM) has been increasingly used to model complex real-life issues, such as informing prompt COVID-19 response policies. ABM represents subsystems and their entities as agents while employing flexible rules to describe complex relationships and interactions among the agents. The Metaverse, with its sophisticated agents like digital twins (DTs) and human digital twins (HDTs), can significantly boost ABM performance. However, current cognitive architectures are not ready for HDTs use in the Metaverse. Here we show that extending current digital cognitive architectures is a crucial first step towards building more robust HDTs. We introduce Cybonto, a novel ontology that packages 108 psychology constructs and thousands of related paths based on 20 time-tested psychology theories. Using 20 network science centrality algorithms, we rank the Cybonto psychology constructs by their influences, identifying the top 10 constructs: behavior, arousal, goals, perception, self-efficacy, circumstances, evaluating, behavior-controllability, knowledge, and intentional modality. These findings confirm the need for specific extensions of current digital cognitive architectures in preparation for future HDTs in the Metaverse. Additionally, Cybonto can be used to develop cognitive evaluation metrics for large language models, moving the field forward in terms of practical applications and theoretical advancements. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Agent-based Modeling; Cognitive Computing; Human Digital Twins; Metaverse; Smart Agents},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hier2024,
	author = {Hier, Daniel B. and Munzir, Syed Ilyas and Stahlfeld, Anne E. and Obafemi-Ajayi, Tayo and Carrithers, Michael D.},
	title = {High-Throughput Phenotyping of Clinical Text Using Large Language Models},
	year = {2024},
	pages = {},
	doi = {10.1109/BHI62660.2024.10913712},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001381121&doi=10.1109%2FBHI62660.2024.10913712&partnerID=40&md5=1014a8c6a7e3962918cf865392f856d4},
	abstract = {High-throughput phenotyping automates the mapping of patient signs to standardized concepts, such as those in Human Phenotype Ontology (HPO), a process critical to precision medicine. We evaluated the automated phenotyping of clinical summaries from the Online Mendelian Inheritance in Man (OMIM) database using a large language model. Various APIs were used to automate text retrieval, sign identification, categorization, and normalization. GPT-4 outperformed GPT-3.5Turbo in identifying, categorizing, and normalizing signs, achieving concordance with manual annotators comparable to concordance between manual annotators. While GPT-4 demonstrates high accuracy in sign identification and categorization, limitations remain in sign normalization, particularly in retrieving the correct HPO ID for a normalized term. Methods such as retrieval-augmented generation, changes in pre-training, and additional fine-tuning may help address these limitations. The combination of APIs with large language models presents a promising approach for high-throughput phenotyping of free text. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Gpt-4; High-throughput; Hpo; Large Language Model; Natural Language Processing; Neurology; Omim; Phenotype; Brain Mapping; Modeling Languages; Neurology; Ontology; Gpt-4; High-throughput; Human Phenotype Ontology; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Online Mendelian Inheritance In Man; Ontology's; Phenotype; Natural Language Processing Systems},
	keywords = {Brain mapping; Modeling languages; Neurology; Ontology; GPT-4; High-throughput; Human phenotype ontology; Language model; Language processing; Large language model; Natural language processing; Natural languages; Online mendelian inheritance in man; Ontology's; Phenotype; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Jenny Kalaiarasi2024,
	author = {Jenny Kalaiarasi, S. and Nimala, K. K.},
	title = {Enhancing E-Commerce Product Recommendations Using LLMs and Transformer-Based Deep Learning Architectures},
	year = {2024},
	pages = {},
	doi = {10.1109/ICSES63760.2024.10910646},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001377718&doi=10.1109%2FICSES63760.2024.10910646&partnerID=40&md5=d40f3671da28d737f32400eebfa79f26},
	abstract = {The integration of large language models with deep learning architectures brought an evolutionary revolution in the context of product recommendation systems because of several limitations of traditional methods, such as collaborative and content-based filtering. In this paper, a novel framework for product recommendation is proposed by integrating large language models jointly with deep learning. The contribution of large language models is that it adds semantic understanding capability to the predictive power provided through neural networks. Domain ontologies will be used in this hybrid model to enhance the accuracy and personalization of recommendations, considering complex user preferences and product attributes in e-commerce platforms. It takes a state-of-the-art pre-trained LLM, such as Llama-3, as input and generates personalized embeddings of users based on history, item descriptions, and contextual information. In this work, the Transformer architecture has been used in re-fining and ranking the products for relevance using attention mechanisms that select the most important features in each recommendation task. Besides, knowledge distillation will be used to conduct the small and efficient student model training process. The distilled model receives soft predictions that involve the teacher LLM, which greatly reduces computational overhead but preserves high recommendation accuracy. Eventually, the framework will be evaluated on a real-world e-commerce dataset to explore how it increases the click-through rate, purchase rate, and user's engagement compared to the traditional systems. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Collaborative And Content-based Filtering; Domain Ontologies; Llama-3; Pre-trained Llm; Contrastive Learning; Deep Learning; Electronic Commerce; Personnel Training; Recommender Systems; Semantics; Wiener Filtering; Collaborative-based Filtering; Content Based Filtering; Domain Ontologies; E- Commerces; Language Model; Learning Architectures; Llama-3; Pre-trained Llm; Product Recommendation; Product Recommendation System; Collaborative Filtering},
	keywords = {Contrastive Learning; Deep learning; Electronic commerce; Personnel training; Recommender systems; Semantics; Wiener filtering; Collaborative-based filtering; Content based filtering; Domain ontologies; E- commerces; Language model; Learning architectures; Llama-3; Pre-trained LLM; Product recommendation; Product recommendation system; Collaborative filtering},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Sharma2024970,
	author = {Sharma, Simple and Panda, Supriya P.},
	title = {Performance Metrics Analysis for Deep Learning Models},
	year = {2024},
	pages = {970 - 976},
	doi = {10.1109/ICAICCIT64383.2024.10912181},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001342750&doi=10.1109%2FICAICCIT64383.2024.10912181&partnerID=40&md5=392d5ba6a475465d31ae6185e73a804b},
	abstract = {Evaluating an Information Retrieval (IR) model is a multi-faceted process that requires selecting the right quantitative metrics to assess performance. The selection of evaluation metrics in IR depends on specific tasks, relevant standards, and desired characteristics. To thoroughly assess an IR system's performance, a combination of metrics is necessary. This study explores the effectiveness of deep learning (DL) models in semantic and personalized information retrieval (SIR), focusing on BERT and other large language models (LLMs) that provide context-sensitive embeddings and advanced language comprehension capabilities. Through a detailed analysis, this paper demonstrates how DL models can significantly enhance accuracy and relevance in IR, using evaluation metrics critical for assessing model performance. Metrics like recall, precision, and F1-score are key in capturing model accuracy and coverage; for example, a recall of 1.0 indicates complete retrieval of relevant instances, while a precision of 0.6 reflects a 60% accuracy in positive predictions, balancing these measures at an F1-score of 46.15%. Ranking and prioritization metrics, such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG), evaluated at 91.67% and 95.1%, respectively highlight the models' capabilities in prioritizing relevant results effectively. In personalized and semantic search, selecting the right evaluation metrics is essential to maximize DL model potential and improve the user experience. Recent LLM advancements, like GPT-4, are instrumental in capturing nuanced meanings and understanding complex user queries, which enhances the accuracy of search engines. Continuous optimization of these models through tailored metrics can improve IR systems' contextual relevance and accuracy, fostering greater user satisfaction. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Deep Learning Models; Evaluation; Metrics; Ontology; Personalized Ir; Semantic Ir; Transformer Models; User Profiling; Adversarial Machine Learning; Data Accuracy; Deep Learning; Deep Reinforcement Learning; Federated Learning; Information Retrieval; Semantics; Bert; Deep Learning Model; Evaluation; Learning Models; Metric; Ontology's; Personalized Information Retrieval; Semantic Information Retrieval; Transformer Modeling; User's Profiling; Contrastive Learning},
	keywords = {Adversarial machine learning; Data accuracy; Deep learning; Deep reinforcement learning; Federated learning; Information retrieval; Semantics; BERT; Deep learning model; Evaluation; Learning models; Metric; Ontology's; Personalized information retrieval; Semantic information retrieval; Transformer modeling; User's profiling; Contrastive Learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Juneja20241506,
	author = {Juneja, Amit and Morshed, Mahir and Harvill, John B. and Hasegawa-Johnson, Mark A. and Blondelle, Henri},
	title = {Title: Is the human-in-the-loop necessary to accurately apply large language models for enterprise decision-making?},
	year = {2024},
	journal = {SEG Technical Program Expanded Abstracts},
	volume = {2024-August},
	pages = {1506 - 1510},
	doi = {10.1190/image2024-4101715.1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001157228&doi=10.1190%2Fimage2024-4101715.1&partnerID=40&md5=caad97ed92b7454a77635c1c0c0e8baa},
	abstract = {The widespread use of Large Language Models (LLMs) in generic chatbots has significantly altered user expectations in Intelligence Document Processing (IDP), particularly within the energy sector where historical data access is vital. Yet, adapting LLMs to specific energy domain ontologies through fine-tuning, facilitated by collaboration between subject matter experts (SMEs) and advanced interfaces, is essential for accurately capturing structured knowledge and ensuring trustworthiness in decision-making processes. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Cloud Computing; Processing; Data Handling; Ontology; Petroleum Geology; Chatbots; Cloud-computing; Data Access; Document-processing; Energy Sector; Enterprise Decision-making; Historical Data; Human-in-the-loop; Language Model; User Expectations; Petroleum Prospecting},
	keywords = {Data handling; Ontology; Petroleum geology; Chatbots; Cloud-computing; Data access; Document-processing; Energy sector; Enterprise decision-making; Historical data; Human-in-the-loop; Language model; User expectations; Petroleum prospecting},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Barron20241669,
	author = {Barron, Ryan C. and Grantcharov, Vesselin and Wanna, Selma and Eren, Maksim Ekin and Bhattarai, Manish and Solovyev, Nicholas and Tompkins, George H. and Nicholas, Charles K. and Rasmussen, Kim Ø. and Matuszek, Cynthia},
	title = {Domain-Specific Retrieval-Augmented Generation Using Vector Stores, Knowledge Graphs, and Tensor Factorization},
	year = {2024},
	pages = {1669 - 1676},
	doi = {10.1109/ICMLA61862.2024.00258},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000862118&doi=10.1109%2FICMLA61862.2024.00258&partnerID=40&md5=023e5b34ed49a05e38f7a8cf11d66825},
	abstract = {Large Language Models (LLMs) are pre-trained on large-scale corpora and excel in numerous general natural language processing (NLP) tasks, such as question answering (QA). Despite their advanced language capabilities, when it comes to domain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations, knowledge cut-offs, and lack of knowledge attributions. Additionally, fine tuning LLMs' intrinsic knowledge to highly specific domains is an expensive and time consuming process. The retrieval-augmented generation (RAG) process has recently emerged as a method capable of optimization of LLM responses, by referencing them to a predetermined ontology. It was shown that using a Knowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into account relevant sub-graphs that preserve the information in a structured manner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM framework, that integrates RAG with KG and a vector store (VS) that store factual domain specific information. Importantly, to avoid hallucinations in the KG, we build these highly domain-specific KGs and VSs without the use of LLMs, but via NLP, data mining, and nonnegative tensor factorization with automatic model selection. Pairing our RAG with a domain-specific: (i) KG (containing structured information), and (ii) VS (containing unstructured information) enables the development of domain-specific chat-bots that attribute the source of information, mitigate hallucinations, lessen the need for fine-tuning, and excel in highly domain-specific question answering tasks. We pair SMART-SLIC with chain-of-thought prompting agents. The framework is designed to be generalizable to adapt to any specific or specialized domain. In this paper, we demonstrate the question answering capabilities of our framework on a corpus of scientific publications on malware analysis and anomaly detection. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Agents; Artificial Intelligence; Knowledge Graph; Natural Language Processing; Non-negative Tensor Factorization; Retrieval Augmented Generation; Topic Modeling; Anomaly Detection; Domain Knowledge; Knowledge Graph; Modeling Languages; Natural Language Processing Systems; Non-negative Matrix Factorization; Ontology; Photomapping; Problem Oriented Languages; Question Answering; Domain Specific; Knowledge Graphs; Language Processing; Natural Language Processing; Natural Languages; Non Negatives; Non-negative Tensor Factorization; Retrieval Augmented Generation; Tensor Factorization; Topic Modeling; Tensors},
	keywords = {Anomaly detection; Domain Knowledge; Knowledge graph; Modeling languages; Natural language processing systems; Non-negative matrix factorization; Ontology; Photomapping; Problem oriented languages; Question answering; Domain specific; Knowledge graphs; Language processing; Natural language processing; Natural languages; Non negatives; Non-negative tensor factorization; Retrieval augmented generation; Tensor factorization; Topic Modeling; Tensors},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Menaha2024,
	author = {Menaha, R. and Abilaash, R. and Mohanram, N. P. and Unnikrishnan, Akash and Sukumar, S.},
	title = {Drug Pills Identification System using Google Gemini LLM: A Generative AI approach},
	year = {2024},
	pages = {},
	doi = {10.1109/ICERCS63125.2024.10895371},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000646321&doi=10.1109%2FICERCS63125.2024.10895371&partnerID=40&md5=4660581d49f6bd296c77a2902b2be5b0},
	abstract = {Generative AI is emerging as a disruptive force in the healthcare industry, bringing novel solutions ranging from drug development and clinical decision support to personalized patient care. This study is focused on drug discovery using the Generative AI model. In this paper, a system is proposed for providing drug descriptions from drug pill images. The system is implemented by utilizing Large Language Models (LLMs) in combination with computer vision to detect and provide detailed information about drugs from pill images. In the proposed system, the identification process begins by taking the medicinal drug pills and their cover images. Then, the image is converted into binary values using a standard built-in function. In addition, the target language for providing audio descriptions about the drugs is also used. Then, the Google Gemini LLM model is customized by using binary values of the image, target language, and ontology-based prompt engineering. As a result, the LLM model provides drug descriptions in text. Then, the textual description of the drug is converted into the target language audio format by using the Google Text to Speech Converter. The system is experimented by using 807 medicinal drug images which are collected from web resources. The performance of the system is measured by using accuracy. The system achieved an accuracy of 95.04% which is a little higher when compared with the current state-of-the-art model. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Drug Pill Identification; Generative Ai; Large Language Models; Medical Text Analysis; Multimodal Learning; Pharmaceutical Image Analysis; Drug Pill Identification; Generative Ai; Image Analyze; Image-analysis; Language Model; Large Language Model; Medical Text Analyze; Multi-modal Learning; Pharmaceutical Image Analyze; Text Analysis},
	keywords = {Drug pill identification; Generative AI; Image analyze; Image-analysis; Language model; Large language model; Medical text analyze; Multi-modal learning; Pharmaceutical image analyze; Text analysis},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ye2024,
	author = {Ye, Yanpeng and Ren, Jie and Wang, Shaozhou and Wan, Yuwei and Razzak, Muhammad Imran and Hoex, Bram and Wang, Haofeng Fen and Xie, Tong and Zhang, Wenjie},
	title = {Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model},
	year = {2024},
	journal = {Advances in Neural Information Processing Systems},
	volume = {37},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000530840&partnerID=40&md5=481a305ec752594f6bad0a9fb71a84f2},
	abstract = {Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges to the efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques integrated with large language models to extract and systematically organize a decade's worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated science knowledge graphs. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Hu2024,
	author = {Hu, Bozhen and Tan, Cheng and Xu, Yongjie and Gao, Zhangyang and Xia, Jun and Wu, Lirong and Li, Stan Ziqing},
	title = {ProtGO: Function-Guided Protein Modeling for Unified Representation Learning},
	year = {2024},
	journal = {Advances in Neural Information Processing Systems},
	volume = {37},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000508851&partnerID=40&md5=47c66d93b1dd5da19b33200f6e5e8113},
	abstract = {Protein representation learning is indispensable for various downstream applications of artificial intelligence for bio-medicine research, such as drug design and function prediction. However, achieving effective representation learning for proteins poses challenges due to the diversity of data modalities involved, including sequence, structure, and function annotations. Despite the impressive capabilities of large language models in biomedical text modelling, there remains a pressing need for a framework that seamlessly integrates these diverse modalities, particularly focusing on the three critical aspects of protein information: sequence, structure, and function. Moreover, addressing the inherent data scale differences among these modalities is essential. To tackle these challenges, we introduce ProtGO, a unified model that harnesses a teacher network equipped with a customized graph neural network (GNN) and a Gene Ontology (GO) encoder to learn hybrid embeddings. Notably, our approach eliminates the need for additional functions as input for the student network, which shares the same GNN module. Importantly, we utilize a domain adaptation method to facilitate distribution approximation for guiding the training of the teacher-student framework. This approach leverages distributions learned from latent representations to avoid the alignment of individual samples. Benchmark experiments highlight that ProtGO significantly outperforms state-of-the-art baselines, clearly demonstrating the advantages of the proposed unified framework. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Lo2024,
	author = {Lo, Andy and Jiang, Albert Qiaochu and Li, Wenda and Jamnik, Mateja},
	title = {End-to-End Ontology Learning with Large Language Models},
	year = {2024},
	journal = {Advances in Neural Information Processing Systems},
	volume = {37},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000479241&partnerID=40&md5=f517a883bd55f9054bd7e3cb20e0ebd1},
	abstract = {Ontologies are useful for automatic machine processing of domain knowledge as they represent it in a structured format. Yet, constructing ontologies requires substantial manual effort. To automate part of this process, large language models (LLMs) have been applied to solve various subtasks of ontology learning. However, this partial ontology learning does not capture the interactions between subtasks. We address this gap by introducing OLLM, a general and scalable method for building the taxonomic backbone of an ontology from scratch. Rather than focusing on subtasks, like individual relations between entities, we model entire subcomponents of the target ontology by finetuning an LLM with a custom regulariser that reduces overfitting on high-frequency concepts. We introduce a novel suite of metrics for evaluating the quality of the generated ontology by measuring its semantic and structural similarity to the ground truth. In contrast to standard syntax-based metrics, our metrics use deep learning techniques to define more robust distance measures between graphs. Both our quantitative and qualitative results on Wikipedia show that OLLM outperforms subtask composition methods, producing more semantically accurate ontologies while maintaining structural integrity. We further demonstrate that our model can be effectively adapted to new domains, like arXiv, needing only a small number of training examples. Our source code and datasets are available at https://github.com/andylolu2/ollm. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Sivasubramaniam2024,
	author = {Sivasubramaniam, Sithursan and Osei-Akoto, Cedric and Zhang, Yi and Stockinger, Kurt and Furst, Jonathan},
	title = {SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark},
	year = {2024},
	journal = {Advances in Neural Information Processing Systems},
	volume = {37},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000461190&partnerID=40&md5=e5573743e399514c1c1b995370dc5377},
	abstract = {Electronic health records (EHRs) are stored in various database systems with different database models on heterogeneous storage architectures, such as relational databases, document stores, or graph databases. These different database models have a big impact on query complexity and performance. While this has been a known fact in database research, its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far. In this paper, we present SM3-Text-to-Query, the first multi-model medical Text-to-Query benchmark based on synthetic patient data from Synthea, following the SNOMED-CT taxonomy-a widely used knowledge graph ontology covering medical terminology. SM3-Text-to-Query provides data representations for relational databases (PostgreSQL), document stores (MongoDB), and graph databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four popular query languages, namely SQL, MQL, Cypher, and SPARQL. We systematically and manually develop 408 template questions, which we augment to construct a benchmark of 10K diverse natural language question/query pairs for these four query languages (40K pairs overall). On our dataset, we evaluate several common in-context-learning (ICL) approaches for a set of representative closed and open-source LLMs. Our evaluation sheds light on the trade-offs between database models and query languages for different ICL strategies and LLMs. Last, SM3-Text-to-Query is easily extendable to additional query languages or real, standard-based patient databases. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Stoller202426,
	author = {Stoller, Aaron and Schacht, Chris},
	title = {Composition Naturalized},
	year = {2024},
	journal = {Education and Culture},
	volume = {40},
	number = {1},
	pages = {26 - 50},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000388703&partnerID=40&md5=5a7312996f08bd40b01bf1f7ed77f1dd},
	abstract = {The emergence of Large Language Models has exposed composition studies’ long-standing commitment to Cartesian assumptions that position writing as a nonmaterial, distinctly human activity. This paper develops a naturalized theory of composition grounded in Deweyan pragmatic naturalism that dissolves the nature/culture dualism embedded in contemporary theory and practice. We advance an eco-ontological account that understands compositional activity as emerging from within the matrix of animal behavior and introduce “compositional viability” to theorize how writing functions as a biosemiotic tool for environmental reconstruction. This framework yields three pedagogical implications: attending to somaesthetics, cultivating writerly habits, and orienting composition toward viable action. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Biosemiotics; Composition Studies; Composition Theory; Deweyan Pragmatism; Somaesthetics; Writing Pedagogy},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Vasilakis202498,
	author = {Vasilakis, Yannis and Bittner, Rachel M. and Pauwels, Johan},
	title = {Evaluation of pretrained language models on music understanding},
	year = {2024},
	pages = {98 - 106},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000198786&partnerID=40&md5=b3ffa97cdd828a5eaab201d79f636b67},
	abstract = {Music-text multimodal systems have enabled new approaches to Music Information Research (MIR) applications such as audio-to-text and text-to-audio retrieval, text-based song generation, and music captioning. Despite the reported success, little effort has been put into evaluating the musical knowledge of Large Language Models (LLM). In this paper, we demonstrate that LLMs suffer from 1) prompt sensitivity, 2) inability to model negation (e.g. “rock song without guitar”), and 3) sensitivity towards the presence of specific words. We quantified these properties as a triplet-based accuracy, evaluating the ability to model the relative similarity of labels in a hierarchical ontology. We leveraged the Audioset ontology to generate triplets consisting of an anchor, a positive (relevant) label, and a negative (less relevant) label for the genre and instruments sub-tree. We evaluated the triplet-based musical knowledge for six general-purpose Transformer-based models. The triplets obtained through this methodology required filtering, as some were difficult to judge and therefore relatively uninformative for evaluation purposes. Despite the relatively high accuracy reported, inconsistencies are evident in all six models, suggesting that off-the-shelf LLMs need adaptation to music before use. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computer Music; Musical Instruments; Trees (mathematics); Audio Retrieval; Information Research; Language Model; Multimodal System; Music Information; Music Understanding; New Approaches; Ontology's; Research Applications; Rock Song; Ontology},
	keywords = {Computer music; Musical instruments; Trees (mathematics); Audio retrieval; Information research; Language model; Multimodal system; Music information; Music understanding; New approaches; Ontology's; Research applications; Rock song; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kenyon2024166,
	author = {Kenyon, Shaun R. and Gough Kenyon, Sheila M. and McColl, Callum},
	title = {Pythia - An LLM-Driven Automated Platform That Uses Established Consensus-Building Techniques To Capture And Synthesise The Wisdom Of A Panel Of Renowned Authorities On Space Engineering},
	year = {2024},
	journal = {Proceedings of the International Astronautical Congress, IAC},
	pages = {166 - 177},
	doi = {10.52202/078376-0016},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000168862&doi=10.52202%2F078376-0016&partnerID=40&md5=c2d402b6525cc27a9870f44df2ae87b1},
	abstract = {All space missions share common features such as requirements, risks (and mitigation strategies), and an agreed understanding between the stakeholders on the level of product assurance to be applied. The relationship between product assurance effort, mission risk and total mission costs are well understood (Wertz et al [1]). International Space Engineering Standards such as NASA’s NPR series, NASA centre guides, and ECSS exist to codify known best practices [2, 3]. These may or may not be complied to, depending on the risk, schedule and budget appetite of mission stakeholders [4]. What is less well understood is the nuanced understanding of the different ways standards can be interpreted, and applied to mission development processes [5]. This understanding is currently built organically through the experience of engineers as they progress in their careers. We propose an automated, intelligent system”Pythia” that can capture the knowledge, experience and opinions of senior space engineers with widely diverse experiences and perspectives, and synthesise them using the established consensus building Delphi methodology [21, 7] into a formally-defined knowledge graph database. This database can then be queried, analysed and deployed for a myriad of applications. Development of the prototype system is discussed, and we present insights on the challenges encountered, and the future plans for further development of the system. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Engineering Standards; Generative Ai; Knowledge Graphs; Knowledge Management; Ontologies; Budget Control; Cost Engineering; Knowledge Graph; Requirements Engineering; Building Techniques; Common Features; Consensus Buildings; Engineering Standards; Generative Ai; Knowledge Graphs; Ontology's; Product Assurance; Risk Strategies; Space Missions; Nasa},
	keywords = {Budget control; Cost engineering; Knowledge graph; Requirements engineering; Building techniques; Common features; Consensus buildings; Engineering standards; Generative AI; Knowledge graphs; Ontology's; Product assurance; Risk strategies; Space missions; NASA},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hou20241776,
	author = {Hou, Lijuan and Qin, Hanyan and Zhang, Xiankun and Zhang, Yiying},
	title = {Protein Function Prediction Based on the Pretrained Language Model ESM2 and Graph Convolutional Networks},
	year = {2024},
	pages = {1776 - 1781},
	doi = {10.1109/ISPA63168.2024.00242},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000150716&doi=10.1109%2FISPA63168.2024.00242&partnerID=40&md5=7b023aac5b19e341e1b0ae98ba0e6cf7},
	abstract = {Understanding protein function is crucial for comprehending life at the molecular level. Currently, less than 0.1% of proteins having experimental GO annotations. Traditional experimental methods are time-consuming and expensive. To narrow this gap, employing accurate and efficient computational methods can fill the void in automated protein function prediction (AFP). We have developed a new method for predicting protein function using sequence and predicted structural information. We use the large-scale pretrained language model ESM2 to pretrain protein sequences and an encoder to capture contextual information. Using the 3D structural data of proteins generated by AlphaFold2, combined with the sequence, as input for the Graph Convolutional Neural Network, to infer the probabilities of Gene Ontology (GO) annotations for the proteins. Compared to earlier methods, our model achieves better performance. Evaluations on the human dataset show AUPR improvements of 9%, 9.4%, and 20.7% in the BP, MF, and CC branches, respectively, demonstrating that our model is an effective tool for predicting protein function. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Alphafold2; Esm2; Gene Ontology; Graph Pooling; Protein Function Prediction; Convolutional Neural Networks; Graph Neural Networks; Prediction Models; Alphafold2; Convolutional Networks; Esm2; Gene Ontology; Gene Ontology Annotations; Graph Pooling; Language Model; Prediction-based; Protein Function Prediction; Protein Functions; Gene Ontology},
	keywords = {Convolutional neural networks; Graph neural networks; Prediction models; Alphafold2; Convolutional networks; ESM2; Gene ontology; Gene ontology annotations; Graph pooling; Language model; Prediction-based; Protein function prediction; Protein functions; Gene Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Neuhaus2023399,
	author = {Neuhaus, Fabian},
	title = {Ontologies in the era of large language models - a perspective},
	year = {2023},
	journal = {Applied Ontology},
	volume = {18},
	number = {4},
	pages = {399 - 407},
	doi = {10.3233/AO-230072},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182277245&doi=10.3233%2FAO-230072&partnerID=40&md5=863560862d60903db161beed3567eb0e},
	abstract = {The potential of large language models (LLM) has captured the imagination of the public and researchers alike. In contrast to previous generations of machine learning models, LLMs are general-purpose tools, which can communicate with humans. In particular, they are able to define terms and answer factual questions based on some internally represented knowledge. Thus, LLMs support functionalities that are closely related to ontologies. In this perspective article, I will discuss the consequences of the advent of LLMs for the field of applied ontology. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bard; Chatgpt; Copilot; Large Language Model; Ontology Development; Computational Linguistics; Am Generals; Bard; Chatgpt; Copilot; Language Model; Large Language Model; Machine Learning Models; Ontology Development; Ontology's; Ontology},
	keywords = {Computational linguistics; Am generals; Bard; ChatGPT; Copilot; Language model; Large language model; Machine learning models; Ontology development; Ontology's; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16}
}

@ARTICLE{Harvey202337,
	author = {Harvey, D. and Ruchayskiy, Oleg and Boyarsky, A. and Solovyov, V. and Magalich, Andrii and Romaniukov, A. and Puhach, D. and Arekhta, O.},
	title = {Prophy: An automated reviewer finder to improve the efficiency, diversity and quality of reviews},
	year = {2023},
	journal = {Information Services and Use},
	volume = {44},
	number = {1},
	pages = {37 - 42},
	doi = {10.3233/ISU-230196},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180974142&doi=10.3233%2FISU-230196&partnerID=40&md5=2f916ec7fd47db726a16eac97d7a2cee},
	abstract = {Peer review is under pressure. Without fair, transparent and efficient peer review we cannot ensure the right proposals get funded and the correct manuscripts get published. In the era of Open Access, which is driving an exponential increase in the number of submitted publications, how we carry out peer review is becoming increasingly important and how we find reviewers is coming under scrutiny. The current methods are slow and produce bias pools of reviewers. As such we need an improved way. At Prophy we have developed a state-of-the-art referee finder that can find experts to review any manuscript from any scientific field in seconds. Then through post-processing filters we can find appropriate candidate referees who are most likely to review a paper, whilst highlighting important conflicts of interest through our complex citation networks. These methods can ensure fair and independent experts who can review interdisciplinary papers from any discipline. These methods are being delivered through APIs and the editorial workflow of editors ensure the right people get access to these tools. Finally, as large-language models improve, so does Prophy and as such we will be looking to drive real innovation in this area in years to come. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Machine Learning; Ontology; Peer-review; Referee Finder; Semantic Ai; Machine Learning; 'current; Exponential Increase; Machine-learning; Ontology's; Openaccess; Peer Review; Referee Finder; Scientific Fields; Semantic Ai; State Of The Art; Semantics},
	keywords = {Machine learning; 'current; Exponential increase; Machine-learning; Ontology's; OpenAccess; Peer review; Referee finder; Scientific fields; Semantic AI; State of the art; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Zhou2023408,
	author = {Zhou, Yifan and Ding, Yizhou and Dong, Yuwu and He, Hao},
	title = {Ontology-Semantic Alignment On Contrastive Video-Language Model for Multimodel Video Retrieval Task},
	year = {2023},
	pages = {408 - 413},
	doi = {10.1145/3638584.3638635},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188340399&doi=10.1145%2F3638584.3638635&partnerID=40&md5=5e779a8b72afae89e57ee2fd7e13d62c},
	abstract = {Contrastive Learning-based models have shown impressive performance in text-image retrieval tasks. However, when applied in video retrieval, traditional contrastive learning strategies have faced challenges in achieving satisfactory results due to redundancy of video contents. We discern several potential reasons: (1)Current methodologies sometimes overlook the significant information imbalance between videos and query text, specifically neglecting the in-depth textual representation of the content within the videos. (2) Current video matching methodologies typically focus on cross-model alignment at general entity similarity level, without specific consideration for how entity pair preferences and similarity properties affect the task at hand. (3) Previous vectorized retrieval based on video content features have been somewhat flawed. They primarily focused on aligning overall features without having an video content tags feature for meaningful feature discrimination. Considering the shortcomings identified in the mentioned three aspects, we propose an ontology semantic labels augments retrieval model and introduce a method to integrate video ontology semantic labels into the contrastive learning framework. In particular, we have developed ontology semantic descriptions about entities encompassing both human figures and textual elements within the videos. Subsequently, we conducted training and testing on the CMIVQA dataset to assess the performance of our approach. The experimental results show that employing fine-grained ontology labels as sample pairs for contrastive learning leads to an increased level of precision in video retrieval tasks. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Multimodal Alignment; Ontology Description; Video Content Understanding; Alignment; Image Retrieval; Learning Systems; Semantics; Statistical Tests; Video Recording; 'current; Multi-modal; Multimodal Alignment; Ontology Description; Ontology Semantics; Ontology's; Performance; Video Content Understanding; Video Contents; Video Retrieval; Ontology},
	keywords = {Alignment; Image retrieval; Learning systems; Semantics; Statistical tests; Video recording; 'current; Multi-modal; Multimodal alignment; Ontology description; Ontology semantics; Ontology's; Performance; Video content understanding; Video contents; Video retrieval; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Rula20239,
	author = {Rula, Anisa and D’Souza, Jennifer},
	title = {Procedural Text Mining with Large Language Models},
	year = {2023},
	pages = {9 - 16},
	doi = {10.1145/3587259.3627572},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180376354&doi=10.1145%2F3587259.3627572&partnerID=40&md5=fe60e37435eb652a9486e6e942682d1b},
	abstract = {Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDF text in an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Capture; Knowledge Representation; Computational Linguistics; Deep Learning; Learning Systems; Natural Language Processing Systems; Zero-shot Learning; Context Learning; In Contexts; Knowledge Capture; Knowledge-representation; Language Model; Language Processing; Large-scales; Learning Settings; Natural Languages; Text-mining; Knowledge Representation},
	keywords = {Computational linguistics; Deep learning; Learning systems; Natural language processing systems; Zero-shot learning; Context learning; In contexts; Knowledge capture; Knowledge-representation; Language model; Language processing; Large-scales; Learning settings; Natural languages; Text-mining; Knowledge representation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Schneider2023114,
	author = {Schneider, Florian and Dash, Sarthak and Bagchi, Sugato and Mihindukulasooriya, Nandana and Gliozzo, Alfio Massimiliano},
	title = {NLFOA: Natural Language Focused Ontology Alignment},
	year = {2023},
	pages = {114 - 121},
	doi = {10.1145/3587259.3627560},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180373581&doi=10.1145%2F3587259.3627560&partnerID=40&md5=0d266e49be4e21c8f0d0cb58b443bb60},
	abstract = {For Ontology Alignment (OA), the task is to align semantically equivalent concepts and relations from different ontologies. This task plays a crucial role in many downstream tasks and applications in academia and industry. Since manually aligning ontologies is inefficient and costly, numerous approaches exist to do this automatically. However, most approaches are tailored to specific domains, are rule-based systems or based on feature engineering, and require external knowledge. The most recent advances in the field of OA rely on the widely proven effectiveness of pre-trained language models to represent the human-generated language that describes the entities in an ontology. However, these approaches additionally require sophisticated algorithms or Graph Neural Networks to exploit an ontology's graphical structure to achieve state-of-the-art performance. In this work, we present NLFOA, or Natural Language Focused Ontology Alignment, which purely focuses on the natural language contained in ontologies to process the ontology's semantics as well as graphical structure. An evaluation of our approach on common OA datasets shows superior results when finetuning with only a small number of training samples. Additionally, it demonstrates strong results in a zero-shot setting which could be employed in an active learning setup to reduce human labor when manually aligning ontologies significantly. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology Alignment Sentence Transformers Zero-shot; Graph Neural Networks; Knowledge Graph; Semantics; Zero-shot Learning; Down-stream; External Knowledge; Feature Engineerings; Graphical Structures; Language Model; Natural Languages; Ontology Alignment; Ontology Alignment Sentence Transformer Zero-shot; Ontology's; Rules Based Systems; Ontology},
	keywords = {Graph neural networks; Knowledge graph; Semantics; Zero-shot learning; Down-stream; External knowledge; Feature engineerings; Graphical structures; Language model; Natural languages; Ontology alignment; Ontology alignment sentence transformer zero-shot; Ontology's; Rules based systems; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Hertling2023131,
	author = {Hertling, Sven and Paulheim, Heiko},
	title = {OLaLa: Ontology Matching with Large Language Models},
	year = {2023},
	pages = {131 - 139},
	doi = {10.1145/3587259.3627571},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180367190&doi=10.1145%2F3587259.3627571&partnerID=40&md5=84b1b2a1f72d77e7a6792fea290eeabe},
	abstract = {Ontology (and more generally: Knowledge Graph) Matching is a challenging task where information in natural language is one of the most important signals to process. With the rise of Large Language Models, it is possible to incorporate this knowledge in a better way into the matching pipeline. A number of decisions still need to be taken, e.g., how to generate a prompt that is useful to the model, how information in the KG can be formulated in prompts, which Large Language Model to choose, how to provide existing correspondences to the model, how to generate candidates, etc. In this paper, we present a prototype that explores these questions by applying zero-shot and few-shot prompting with multiple open Large Language Models to different tasks of the Ontology Alignment Evaluation Initiative (OAEI). We show that with only a handful of examples and a well-designed prompt, it is possible to achieve results that are en par with supervised matching systems which use a much larger portion of the ground truth. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Entity Resolution; Large Language Model; Ontology Matching; Computational Linguistics; Knowledge Graph; Zero-shot Learning; Entity Resolutions; Graph Matchings; Knowledge Graphs; Language Model; Large Language Model; Matchings; Natural Languages; Ontology Alignment; Ontology Matching; Ontology's; Ontology},
	keywords = {Computational linguistics; Knowledge graph; Zero-shot learning; Entity resolutions; Graph matchings; Knowledge graphs; Language model; Large language model; Matchings; Natural languages; Ontology alignment; Ontology matching; Ontology's; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 51; All Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Gagliardi2023,
	author = {Gagliardi, Isabella and Artese, Maria Teresa},
	title = {Ensemble-Based Short Text Similarity: An Easy Approach for Multilingual Datasets Using Transformers and WordNet in Real-World Scenarios},
	year = {2023},
	journal = {Big Data and Cognitive Computing},
	volume = {7},
	number = {4},
	pages = {},
	doi = {10.3390/bdcc7040158},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180507850&doi=10.3390%2Fbdcc7040158&partnerID=40&md5=77775467c16dbdbbad220a454064f81d},
	abstract = {When integrating data from different sources, there are problems of synonymy, different languages, and concepts of different granularity. This paper proposes a simple yet effective approach to evaluate the semantic similarity of short texts, especially keywords. The method is capable of matching keywords from different sources and languages by exploiting transformers and WordNet-based methods. Key features of the approach include its unsupervised pipeline, mitigation of the lack of context in keywords, scalability for large archives, support for multiple languages and real-world scenarios adaptation capabilities. The work aims to provide a versatile tool for different cultural heritage archives without requiring complex customization. The paper aims to explore different approaches to identifying similarities in 1- or n-gram tags, evaluate and compare different pre-trained language models, and define integrated methods to overcome limitations. Tests to validate the approach have been conducted using the QueryLab portal, a search engine for cultural heritage archives, to evaluate the proposed pipeline. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Ensemble Methods; Pretrained Language Models; Querylab; Semantic Textual Similarity; Transformers; Wordnet; Computational Linguistics; Ontology; Pipelines; Search Engines; Cultural Heritages; Ensemble Methods; Language Model; Pretrained Language Model; Querylab; Real-world Scenario; Semantic Textual Similarity; Textual Similarities; Transformer; Wordnet; Semantics},
	keywords = {Computational linguistics; Ontology; Pipelines; Search engines; Cultural heritages; Ensemble methods; Language model; Pretrained language model; Querylab; Real-world scenario; Semantic textual similarity; Textual similarities; Transformer; Wordnet; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Lotz2023,
	author = {Lotz, Jeffrey C. and Ropella, Glen E.P. and Anderson, Paul E. and Yang, Qian and Hedderich, Michael A. and Bailey, Jeannie F. and Hunt, Carver Anthony},
	title = {An exploration of knowledge-organizing technologies to advance transdisciplinary back pain research},
	year = {2023},
	journal = {JOR Spine},
	volume = {6},
	number = {4},
	pages = {},
	doi = {10.1002/jsp2.1300},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177452510&doi=10.1002%2Fjsp2.1300&partnerID=40&md5=15ba32d9cdcf28d405ab162c6a48b40a},
	abstract = {Chronic low back pain (LBP) is influenced by a broad spectrum of patient-specific factors as codified in domains of the biopsychosocial model (BSM). Operationalizing the BSM into research and clinical care is challenging because most investigators work in silos that concentrate on only one or two BSM domains. Furthermore, the expanding, multidisciplinary nature of BSM research creates practical limitations as to how individual investigators integrate current data into their processes of generating impactful hypotheses. The rapidly advancing field of artificial intelligence (AI) is providing new tools for organizing knowledge, but the practical aspects for how AI may advance LBP research and clinical are beginning to be explored. The goals of the work presented here are to: (1) explore the current capabilities of knowledge integration technologies (large language models (LLM), similarity graphs (SGs), and knowledge graphs (KGs)) to synthesize biomedical literature and depict multimodal relationships reflected in the BSM, and; (2) highlight limitations, implementation details, and future areas of research to improve performance. We demonstrate preliminary evidence that LLMs, like GPT-3, may be useful in helping scientists analyze and distinguish cLBP publications across multiple BSM domains and determine the degree to which the literature supports or contradicts emergent hypotheses. We show that SG representations and KGs enable exploring LBP's literature in novel ways, possibly providing, trans-disciplinary perspectives or insights that are currently difficult, if not infeasible to achieve. The SG approach is automated, simple, and inexpensive to execute, and thereby may be useful for early-phase literature and narrative explorations beyond one's areas of expertise. Likewise, we show that KGs can be constructed using automated pipelines, queried to provide semantic information, and analyzed to explore trans-domain linkages. The examples presented support the feasibility for LBP-tailored AI protocols to organize knowledge and support developing and refining trans-domain hypotheses. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Biopsychosocial Model; Chronic Low Back Pain; Knowledge Graphs; Article; Artificial Intelligence; Biopsychosocial Model; Chronic Pain; Data Classification; Human; Hypothesis; Information Processing; Interdisciplinary Research; Knowledge; Knowledge Graph; Large Language Model; Low Back Pain; Medical Literature; Musculoskeletal System; Ontology; Preliminary Data; Scientist; Similarity Graph},
	keywords = {Article; artificial intelligence; biopsychosocial model; chronic pain; data classification; human; hypothesis; information processing; interdisciplinary research; knowledge; knowledge graph; large language model; low back pain; medical literature; musculoskeletal system; ontology; preliminary data; scientist; similarity graph},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Ibtehaz2023,
	author = {Ibtehaz, Nabil and Kagaya, Yuki and Kihara, Daisuke},
	title = {Domain-PFP allows protein function prediction using function-aware domain embedding representations},
	year = {2023},
	journal = {Communications Biology},
	volume = {6},
	number = {1},
	pages = {},
	doi = {10.1038/s42003-023-05476-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175591644&doi=10.1038%2Fs42003-023-05476-9&partnerID=40&md5=93ffabcf43aca868b8a4bd5f33193827},
	abstract = {Domains are functional and structural units of proteins that govern various biological functions performed by the proteins. Therefore, the characterization of domains in a protein can serve as a proper functional representation of proteins. Here, we employ a self-supervised protocol to derive functionally consistent representations for domains by learning domain-Gene Ontology (GO) co-occurrences and associations. The domain embeddings we constructed turned out to be effective in performing actual function prediction tasks. Extensive evaluations showed that protein representations using the domain embeddings are superior to those of large-scale protein language models in GO prediction tasks. Moreover, the new function prediction method built on the domain embeddings, named Domain-PFP, substantially outperformed the state-of-the-art function predictors. Additionally, Domain-PFP demonstrated competitive performance in the CAFA3 evaluation, achieving overall the best performance among the top teams that participated in the assessment. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Protein; Gene Ontology; Language; Learning; Gene Ontology; Language; Learning; Proteins},
	keywords = {protein; gene ontology; language; learning; Gene Ontology; Language; Learning; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Wang202318129,
	author = {Wang, Haibo},
	title = {A novel algorithm for the construction of fast English sentence retrieval model using a combination of ontology and advanced machine learning techniques},
	year = {2023},
	journal = {Soft Computing},
	volume = {27},
	number = {23},
	pages = {18129 - 18146},
	doi = {10.1007/s00500-023-09224-3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171833456&doi=10.1007%2Fs00500-023-09224-3&partnerID=40&md5=4b881f366b8dbc39edf3fddadc141bc4},
	abstract = {These days, exploring information retrieval models is one of the most essential aspects of English sentence retrieval research. These models are driven by diverse retrieval mechanisms that offer varying similarity calculations and directly influence the final result ranking. However, despite decades of work due to technical constraints, deep semantic analysis has been challenging. This gap emphasizes the importance of a precise semantic understanding of information acquisition through learning approaches. Based on the above opening, this paper establishes a fast retrieval model of English sentences based on the statistical language model (SLM). First, the proposed method utilizes SLM to extract significant feature words from the corpus. These feature words are identified by analyzing co-occurrence patterns and frequency distributions within the standard. Second, it employs the N-gram model to calculate the probabilities of word occurrences based on their contextual dependencies. This framework represents feature words and their associated probabilities in a structured manner by capturing the intricate nuances of language semantics. Third, the model integrates ontology to bridge the gap between human language and machine understanding by enabling the mapping natural language expressions to conceptual entities. Finally, the suggested model retrieves English sentences through semantic matching by leveraging the comprehensive semantic framework and ontology-based search. The experimental study revealed that the proposed model demonstrated an impressive retrieval ratio of 98.5% by outperforming existing models in the comparison. Moreover, these results show that the proposed algorithm performs better than the Term Frequency-Inverse Document Frequency (TF-IDF) algorithm, and the accuracy of this algorithm is improved by 7.52% compared with TF-IDF. When the labelled corpus is very small and the unlabeled corpus is relatively large, the algorithm enhances the classifier’s performance by 12.6%. This shows that the algorithm used in this paper reduces the influence of the synonym processing stage on the overall performance while retaining the advantages of high precision and accuracy of calculation results. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {English Sentences; Machine Learning; N-gram Model; Quick Retrieval; Semantic Similarity; Tf-idf Algorithm; Computational Linguistics; Learning Algorithms; Natural Language Processing Systems; Ontology; Semantics; Text Processing; English Sentences; Feature Words; Frequency Algorithms; Machine-learning; N-gram Models; Quick Retrieval; Retrieval Models; Semantic Similarity; Term Frequency-inverse Document Frequency Algorithm; Term Frequencyinverse Document Frequency (tf-idf); Machine Learning},
	keywords = {Computational linguistics; Learning algorithms; Natural language processing systems; Ontology; Semantics; Text processing; English sentences; Feature words; Frequency algorithms; Machine-learning; N-gram models; Quick retrieval; Retrieval models; Semantic similarity; Term frequency-inverse document frequency algorithm; Term frequencyinverse document frequency (TF-IDF); Machine learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Wang2023,
	author = {Wang, Yihao and Wegner, Philipp and Domingo-Fernández, Daniel and Tom Kodamullil, Alpha T.},
	title = {Multi-ontology embeddings approach on human-aligned multi-ontologies representation for gene-disease associations prediction},
	year = {2023},
	journal = {Heliyon},
	volume = {9},
	number = {11},
	pages = {},
	doi = {10.1016/j.heliyon.2023.e21502},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175320058&doi=10.1016%2Fj.heliyon.2023.e21502&partnerID=40&md5=d25f9bcae243bd5d2f6527ab157fce44},
	abstract = {Objectives: Knowledge graphs and ontologies in the biomedical domain provide rich contextual knowledge for a variety of challenges. Employing that for knowledge-driven NLP tasks such as gene-disease association prediction represents a promising way to increase the predictive power of a model. Methods: We investigated the power of infusing the embedding of two aligned ontologies as prior knowledge to the NLP models. We evaluated the performance of different models on some large-scale gene-disease association datasets and compared it with a model without incorporating contextualized knowledge (BERT). Results: The experiments demonstrated that the knowledge-infused model slightly outperforms BERT by creating a small number of bridges. Thus, indicating that incorporating cross-references across ontologies can enhance the performance of base models without the need for more complex and costly training. However, further research is needed to explore the generalizability of the model. We expected that adding more bridges would bring further improvement based on the trend we observed in the experiments. In addition, the use of state-of-the-art knowledge graph embedding methods on a joint graph from connecting OGG and DOID with bridges also yielded promising results. Conclusion: Our work shows that allowing language models to leverage structured knowledge from ontologies does come with clear advantages in the performance. Besides, the annotation stage brought out in this paper is constrained in reasonable complexity. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Multi-ontology; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Dong2023452,
	author = {Dong, Hang and Chen, Jiaoyan and He, Yuan and Liu, Yinan and Horrocks, Ian},
	title = {Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking},
	year = {2023},
	pages = {452 - 462},
	doi = {10.1145/3583780.3615036},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174562523&doi=10.1145%2F3583780.3615036&partnerID=40&md5=20f7b8c1544fa043c1e714259d87c583},
	abstract = {Discovering entity mentions that are out of a Knowledge Base (KB) from texts plays a critical role in KB maintenance, but has not yet been fully explored. The current methods are mostly limited to the simple threshold-based approach and feature-based classification, and the datasets for evaluation are relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL) method which can identify mentions that do not have corresponding KB entities by matching them to a special NIL entity. To better utilize BERT, we propose new techniques including NIL entity representation and classification, with synonym enhancement. We also apply KB Pruning and Versioning strategies to automatically construct out-of-KB datasets from common in-KB EL datasets. Results on five datasets of clinical notes, biomedical publications, and Wikipedia articles in various domains show the advantages of BLINKout over existing methods to identify out-of-KB mentions for the medical ontologies, UMLS, SNOMED CT, and the general KB, WikiData. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Ontologies; Entity Linking; Knowledge Base Enrichment; Language Models; Wikidata; Knowledge Based Systems; Ontology; 'current; Biomedical Ontologies; Entity Linking; Feature-based Classification; Knowledge Base Enrichment; Knowledge Base Maintenance; Language Model; Matchings; Simple++; Wikidata; Classification (of Information)},
	keywords = {Knowledge based systems; Ontology; 'current; Biomedical ontologies; Entity linking; Feature-based classification; Knowledge base enrichment; Knowledge base maintenance; Language model; Matchings; Simple++; Wikidata; Classification (of information)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Dong20235316,
	author = {Dong, Hang and Chen, Jiaoyan and He, Yuan and Horrocks, Ian},
	title = {Ontology Enrichment from Texts: A Biomedical Dataset for Concept Discovery and Placement},
	year = {2023},
	pages = {5316 - 5320},
	doi = {10.1145/3583780.3615126},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174561342&doi=10.1145%2F3583780.3615126&partnerID=40&md5=d24b47301d0cb2995bb22b110d3b6d24},
	abstract = {Mentions of new concepts appear regularly in texts and require automated approaches to harvest and place them into Knowledge Bases (KB), e.g., ontologies and taxonomies. Existing datasets suffer from three issues, (i) mostly assuming that a new concept is pre-discovered and cannot support out-of-KB mention discovery; (ii) only using the concept label as the input along with the KB and thus lacking the contexts of a concept label; and (iii) mostly focusing on concept placement w.r.t a taxonomy of atomic concepts, instead of complex concepts, i.e., with logical operators. To address these issues, we propose a new benchmark, adapting MedMentions dataset (PubMed abstracts) with SNOMED CT versions in 2014 and 2017 under the Diseases subcategory and the broader categories of Clinical finding, Procedure, and Pharmaceutical/biologic product. We provide usage on the evaluation with the dataset for out-of-KB mention discovery and concept placement, adapting recent Large Language Model based methods. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Ontologies; Concept Placement; Entity Linking; Language Models; Ontology Enrichment; Snomed Ct; Text Mining; Computational Linguistics; Large Dataset; Natural Language Processing Systems; Ontology; Automated Approach; Biomedical Ontologies; Concept Discoveries; Concept Placement; Entity Linking; Knowledge Taxonomies; Language Model; Ontology Enrichment; Snomed-ct; Text-mining; Taxonomies},
	keywords = {Computational linguistics; Large dataset; Natural language processing systems; Ontology; Automated approach; Biomedical ontologies; Concept discoveries; Concept placement; Entity linking; Knowledge taxonomies; Language model; Ontology enrichment; SNOMED-CT; Text-mining; Taxonomies},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Bednar202315,
	author = {Bednar, Peter and Sarnovský, Martin and Vanko, Jakub Ivan},
	title = {Cognitive Architecture for Process industries},
	year = {2023},
	pages = {15 - 20},
	doi = {10.1145/3624486.3624489},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175423044&doi=10.1145%2F3624486.3624489&partnerID=40&md5=f7048fe9c5dcfba972e0b4e95f166bb9},
	abstract = {This paper introduces a Cross-Sectorial Big Data Processing platform which provides tools for the semantic modelling of the data analytical processes and for the automatic generation of data analysis scripts for solving the described problems. The main contribution of this paper is the cognitive component for the automatic extraction of the task definition from the narrative description of the problem based on the Large Language Models (LLMs). We have evaluated the proposed method on five problems from the different domains and found that the automatic extraction of the task definition can have promising results that can be applied to full-automatic data analytics. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Analytics; Large Language Models; Ontologies; Computational Linguistics; Data Handling; Extraction; Semantics; Analytical Process; Automatic Extraction; Cognitive Architectures; Data Analytics; Language Model; Large Language Model; Ontology's; Process Industries; Processing Platform; Semantic Modelling; Data Analytics},
	keywords = {Computational linguistics; Data handling; Extraction; Semantics; Analytical process; Automatic extraction; Cognitive architectures; Data analytics; Language model; Large language model; Ontology's; Process industries; Processing platform; Semantic modelling; Data Analytics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ma20231390,
	author = {Ma, Kai and Tian, Miao and Tan, Yongjian and Qiu, Qinjun and Xie, Zhong and Huang, Rong},
	title = {Ontology-Based BERT Model for Automated Information Extraction from Geological Hazard Reports},
	year = {2023},
	journal = {Journal of Earth Science},
	volume = {34},
	number = {5},
	pages = {1390 - 1405},
	doi = {10.1007/s12583-022-1724-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174464909&doi=10.1007%2Fs12583-022-1724-z&partnerID=40&md5=0c0f50d88a102cc4ff43000aea7d6911},
	abstract = {Geological knowledge can provide support for knowledge discovery, knowledge inference and mineralization predictions of geological big data. Entity identification and relationship extraction from geological data description text are the key links for constructing knowledge graphs. Given the lack of publicly annotated datasets in the geology domain, this paper illustrates the construction process of geological entity datasets, defines the types of entities and interconceptual relationships by using the geological entity concept system, and completes the construction of the geological corpus. To address the shortcomings of existing language models (such as Word2vec and Glove) that cannot solve polysemous words and have a poor ability to fuse contexts, we propose a geological named entity recognition and relationship extraction model jointly with Bidirectional Encoder Representation from Transformers (BERT) pretrained language model. To effectively represent the text features, we construct a BERT- bidirectional gated recurrent unit network (BiGRU)-conditional random field (CRF)-based architecture to extract the named entities and the BERT-BiGRU-Attention-based architecture to extract the entity relations. The results show that the F1-score of the BERT-BiGRU-CRF named entity recognition model is 0.91 and the F1-score of the BERT-BiGRU-Attention relationship extraction model is 0.84, which are significant performance improvements when compared to classic language models (e.g., word2vec and Embedding from Language Models (ELMo)). © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert Model; Knowledge Graph; Name Entity Recognition; Ontology; Relation Extraction; Hazard Assessment; Knowledge Based System; Language; Mineralization; Prediction},
	keywords = {hazard assessment; knowledge based system; language; mineralization; prediction},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24}
}

@ARTICLE{Yang2023,
	author = {Yang, Yangrui and Chen, Sisi and Zhu, Yaping and Zhu, Hao and Chen, Zhigang},
	title = {Knowledge graph empowerment from knowledge learning to graduation requirements achievement},
	year = {2023},
	journal = {PLOS ONE},
	volume = {18},
	number = {10 October},
	pages = {},
	doi = {10.1371/journal.pone.0292903},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174193764&doi=10.1371%2Fjournal.pone.0292903&partnerID=40&md5=06c146996ded7051491027bb8fd12438},
	abstract = {A deep understanding of the relationship between the knowledge acquired and the graduation requirements is essential for students to precisely meet the graduation requirements and to become human resources with specific knowledge, skills and professionalism. In this paper, we define the ontology layer of the knowledge graph by deeply analyzing the relationship between graduation requirement, course and knowledge. Based on the implementation of the concept of Outcome Based Education, we use Knowledge extraction, fusion, reasoning techniques to construct a hierarchical knowledge graph with the main line of "knowledge- course-graduation requirements. In the process of knowledge extraction, in order to alleviate the huge labor overhead brought by traditional extraction methods, this paper adopts a transfer learning method to extract triadic knowledge using the multi-task framework EERJE, Finally, knowledge reasoning was also performed with the help of LLM to further expand the knowledge scope. The comprehensiveness, correctness and relatedness of the data were evaluated through the experiment, and the F1 value of the ternary group extraction was 87.76%, the accuracy rate of entity classification was 85.42%, the data coverage was more comprehensive, and the results showed that the data quality was better, and the knowledge graph constructed in this way can fully optimize the organization and management of teaching resources, help students intuitively and comprehensively grasp the correlation and difference between graduation requirements and various knowledge points, and let the Students can carry out personalized independent learning through the navigation mode of knowledge graph, strengthen their weak links, and complete the relevant graduation requirements, which effectively improves the degree of students' graduation requirements achievement. This new paradigm of knowledge graph enabled teaching is of reference significance for engineering education majors to improve the degree of graduation requirements achievement. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Achievement; Article; Data Quality; Education; Empowerment; Extraction; Human; Human Experiment; Learning; Ontology; Reasoning; Teaching; Transfer Of Learning; Automated Pattern Recognition; Educational Status; Student; Achievement; Educational Status; Humans; Learning; Pattern Recognition, Automated; Students},
	keywords = {achievement; article; data quality; education; empowerment; extraction; human; human experiment; learning; ontology; reasoning; teaching; transfer of learning; automated pattern recognition; educational status; student; Achievement; Educational Status; Humans; Learning; Pattern Recognition, Automated; Students},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Xu2023,
	author = {Xu, Justin and Mazwi, Mjaye L. and Johnson, Alistair E.w.},
	title = {AnnoDash, a clinical terminology annotation dashboard},
	year = {2023},
	journal = {JAMIA Open},
	volume = {6},
	number = {3},
	pages = {},
	doi = {10.1093/jamiaopen/ooad046},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166011912&doi=10.1093%2Fjamiaopen%2Fooad046&partnerID=40&md5=ea59edaa020122d0f81d34c5ab813774},
	abstract = {Background: Standard ontologies are critical for interoperability and multisite analyses of health data. Nevertheless, mapping concepts to ontologies is often done with generic tools and is labor-intensive. Contextualizing candidate concepts within source data is also done in an ad hoc manner. Methods and Results: We present AnnoDash, a flexible dashboard to support annotation of concepts with terms from a given ontology. Text-based similarity is used to identify likely matches, and large language models are used to improve ontology ranking. A convenient interface is provided to visualize observations associated with a concept, supporting the disambiguation of vague concept descriptions. Time-series plots contrast the concept with known clinical measurements. We evaluated the dashboard qualitatively against several ontologies (SNOMED CT, LOINC, etc.) by using MIMIC-IV measurements. The dashboard is web-based and step-by-step instructions for deployment are provided, simplifying usage for nontechnical audiences. The modular code structure enables users to extend upon components, including improving similarity scoring, constructing new plots, or configuring new ontologies. Conclusion: AnnoDash, an improved clinical terminology annotation tool, can facilitate data harmonizing by promoting mapping of clinical data. AnnoDash is freely available at https://github.com/justin13601/AnnoDash (https://doi.org/10.5281/zenodo.8043943). © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Annotation; Clinical Concepts; Natural Language Processing; Ontology; Software; Article; Human; Human Experiment; Logical Observation Identifiers Names And Codes; Natural Language Processing; Nomenclature; Ontology; Software; Systematized Nomenclature Of Medicine; Time Series Analysis},
	keywords = {article; human; human experiment; Logical Observation Identifiers Names and Codes; natural language processing; nomenclature; ontology; software; Systematized Nomenclature of Medicine; time series analysis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Zhao2023755,
	author = {Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Yang, Yumeng and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
	title = {Protein Function Prediction with Functional and Topological Knowledge of Gene Ontology},
	year = {2023},
	journal = {IEEE Transactions on Nanobioscience},
	volume = {22},
	number = {4},
	pages = {755 - 762},
	doi = {10.1109/TNB.2023.3278033},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160248186&doi=10.1109%2FTNB.2023.3278033&partnerID=40&md5=10e5af9770b802276bd9466926faf7d6},
	abstract = {Gene Ontology (GO) is a widely used bioinformatics resource for describing biological processes, molecular functions, and cellular components of proteins. It covers more than 5000 terms hierarchically organized into a directed acyclic graph and known functional annotations. Automatically annotating protein functions by using GO-based computational models has been an area of active research for a long time. However, due to the limited functional annotation information and complex topological structures of GO, existing models cannot effectively capture the knowledge representation of GO. To solve this issue, we present a method that fuses the functional and topological knowledge of GO to guide protein function prediction. This method employs a multi-view GCN model to extract a variety of GO representations from functional information, topological structure, and their combinations. To dynamically learn the significance weights of these representations, it adopts an attention mechanism to learn the final knowledge representation of GO. Furthermore, it uses a pre-trained language model (i.e., ESM-1b) to efficiently learn biological features for each protein sequence. Finally, it obtains all predicted scores by calculating the dot product of sequence features and GO representation. Our method outperforms other state-of-the-art methods, as demonstrated by the experimental results on datasets from three different species, namely Yeast, Human and Arabidopsis. Our proposed method's code can be accessed at: https://github.com/Candyperfect/Master. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontology; Multi-view Gcn; Pre-trained Language Model; Protein Function Prediction; Protein; Proteins; Biological Systems; Computational Linguistics; Directed Graphs; Forecasting; Gene Ontology; Genes; Modeling Languages; Amino-acids; Annotation; Biological System Modeling; Features Extraction; Gene Ontology; Language Model; Multi-view Gcn; Multi-views; Pre-trained Language Model; Predictive Models; Protein Function Prediction; Protein Sequences; Proteins; Protein; Arabidopsis; Bioinformatics; Gene Ontology; Genetics; Human; Metabolism; Molecular Genetics; Procedures; Semantics; Computational Biology; Humans; Molecular Sequence Annotation; Semantics},
	keywords = {Biological systems; Computational linguistics; Directed graphs; Forecasting; Gene Ontology; Genes; Modeling languages; Amino-acids; Annotation; Biological system modeling; Features extraction; Gene ontology; Language model; Multi-view GCN; Multi-views; Pre-trained language model; Predictive models; Protein function prediction; Protein sequences; Proteins; protein; Arabidopsis; bioinformatics; gene ontology; genetics; human; metabolism; molecular genetics; procedures; semantics; Computational Biology; Humans; Molecular Sequence Annotation; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Chen20232569,
	author = {Chen, Jiaoyan and He, Yuan and Geng, Yuxia and Jimeńez-Ruiz, Ernesto and Dong, Hang and Horrocks, Ian},
	title = {Contextual semantic embeddings for ontology subsumption prediction},
	year = {2023},
	journal = {World Wide Web},
	volume = {26},
	number = {5},
	pages = {2569 - 2591},
	doi = {10.1007/s11280-023-01169-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85156109122&doi=10.1007%2Fs11280-023-01169-9&partnerID=40&md5=a100ea0dbca26b847c8cf4881ae388de},
	abstract = {Automating ontology construction and curation is an important but challenging task in knowledge engineering and artificial intelligence. Prediction by machine learning techniques such as contextual semantic embedding is a promising direction, but the relevant research is still preliminary especially for expressive ontologies in Web Ontology Language (OWL). In this paper, we present a new subsumption prediction method named BERTSubs for classes of OWL ontology. It exploits the pre-trained language model BERT to compute contextual embeddings of a class, where customized templates are proposed to incorporate the class context (e.g., neighbouring classes) and the logical existential restriction. BERTSubs is able to predict multiple kinds of subsumers including named classes from the same ontology or another ontology, and existential restrictions from the same ontology. Extensive evaluation on five real-world ontologies for three different subsumption tasks has shown the effectiveness of the templates and that BERTSubs can dramatically outperform the baselines that use (literal-aware) knowledge graph embeddings, non-contextual word embeddings and the state-of-the-art OWL ontology embeddings. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Ontology Alignment; Ontology Embedding; Owl; Pre-trained Language Model; Subsumption Prediction; Artificial Intelligence; Birds; Computational Linguistics; Embeddings; Ontology; Semantics; Bert; Language Model; Ontology Alignment; Ontology Embedding; Ontology In Web Ontology Language; Ontology's; Pre-trained Language Model; Subsumption Prediction; Web Ontology Language (owl); Forecasting},
	keywords = {Artificial intelligence; Birds; Computational linguistics; Embeddings; Ontology; Semantics; BERT; Language model; Ontology alignment; Ontology embedding; Ontology in web ontology language; Ontology's; Pre-trained language model; Subsumption prediction; Web ontology language (OWL); Forecasting},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 29; All Open Access}
}

@ARTICLE{Jha20233215,
	author = {Jha, Kanchan and Saha, Sriparna and Karmakar, Sourav},
	title = {Prediction of Protein-Protein Interactions Using Vision Transformer and Language Model},
	year = {2023},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	volume = {20},
	number = {5},
	pages = {3215 - 3225},
	doi = {10.1109/TCBB.2023.3248797},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149378413&doi=10.1109%2FTCBB.2023.3248797&partnerID=40&md5=43c3ab66118d4203a64a9722a58a4a4c},
	abstract = {The knowledge of protein-protein interaction (PPI) helps us to understand proteins' functions, the causes and growth of several diseases, and can aid in designing new drugs. The majority of existing PPI research has relied mainly on sequence-based approaches. With the availability of multi-omics datasets (sequence, 3D structure) and advancements in deep learning techniques, it is feasible to develop a deep multi-modal framework that fuses the features learned from different sources of information to predict PPI. In this work, we propose a multi-modal approach utilizing protein sequence and 3D structure. To extract features from the 3D structure of proteins, we use a pre-trained vision transformer model that has been fine-tuned on the structural representation of proteins. The protein sequence is encoded into a feature vector using a pre-trained language model. The feature vectors extracted from the two modalities are fused and then fed to the neural network classifier to predict the protein interactions. To showcase the effectiveness of the proposed methodology, we conduct experiments on two popular PPI datasets, namely, the human dataset and the S. cerevisiae dataset. Our approach outperforms the existing methodologies to predict PPI, including multi-modal approaches. We also evaluate the contributions of each modality by designing uni-modal baselines. We perform experiments with three modalities as well, having gene ontology as the third modality. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Language Model; Protein-protein Interaction; Vision Transformer; Protein; Proteins; Computational Linguistics; Deep Learning; Forecasting; Gene Ontology; Three Dimensional Displays; 3d Structure; Amino-acids; Features Extraction; Language Model; Protein Sequences; Protein-protein Interactions; Three-dimensional Display; Transformer; Vision Transformer; Proteins; Protein; Amino Acid Sequence; Artificial Neural Network; Chemistry; Human; Metabolism; Multiomics; Saccharomyces Cerevisiae; Amino Acid Sequence; Humans; Multiomics; Neural Networks, Computer},
	keywords = {Computational linguistics; Deep learning; Forecasting; Gene Ontology; Three dimensional displays; 3D Structure; Amino-acids; Features extraction; Language model; Protein sequences; Protein-protein interactions; Three-dimensional display; Transformer; Vision transformer; Proteins; protein; amino acid sequence; artificial neural network; chemistry; human; metabolism; multiomics; Saccharomyces cerevisiae; Amino Acid Sequence; Humans; Multiomics; Neural Networks, Computer},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Suhag2023,
	author = {Suhag, Anju and Kidd, Jennifer J.J. and McGath, Meghan and Rajesh, Raeshmma and Gelfinbein, Joseph and Cacace, Nicole and Monteleone, Berrin and Chavez, Martin R.},
	title = {ChatGPT: a pioneering approach to complex prenatal differential diagnosis},
	year = {2023},
	journal = {American Journal of Obstetrics and Gynecology MFM},
	volume = {5},
	number = {8},
	pages = {},
	doi = {10.1016/j.ajogmf.2023.101029},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162070624&doi=10.1016%2Fj.ajogmf.2023.101029&partnerID=40&md5=f6befb7d952f412c3533aa024f3e9cfd},
	abstract = {This commentary examines how ChatGPT can assist healthcare teams in the prenatal diagnosis of rare and complex cases by creating a differential diagnoses based on deidentified clinical findings, while also acknowledging its limitations. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Chatgpt-4; Complex Prenatal Disorders; Generative Pre-trained Transformer Language Model; Human Phenotype Ontology; Neonatal Genetic Disorders; Online Mendelian Inheritance In Man; Prenatal Diagnosis; Article; Clinical Decision Making; Clinical Effectiveness; Differential Diagnosis; Genetic Screening; Health Equity; Human; Medical Ethics; Phenotype; Prediction; Prenatal Diagnosis; Female; Patient Care Team; Pregnancy; Diagnosis, Differential; Female; Humans; Patient Care Team; Pregnancy; Prenatal Diagnosis},
	keywords = {Article; clinical decision making; clinical effectiveness; differential diagnosis; genetic screening; health equity; human; medical ethics; phenotype; prediction; prenatal diagnosis; female; patient care team; pregnancy; Diagnosis, Differential; Female; Humans; Patient Care Team; Pregnancy; Prenatal Diagnosis},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11}
}

@ARTICLE{Ding2023,
	author = {Ding, Ling and Chen, Xiaojun and Wei, Jian and Xiang, Yang},
	title = {MABERT: Mask-Attention-Based BERT for Chinese Event Extraction},
	year = {2023},
	journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
	volume = {22},
	number = {7},
	pages = {},
	doi = {10.1145/3597455},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167671193&doi=10.1145%2F3597455&partnerID=40&md5=f47ecc15b073f93af2041073df00f15e},
	abstract = {Event extraction is an essential but challenging task in information extraction. This task has considerably benefited from pre-trained language models, such as BERT. However, when it comes to the trigger-word mismatch problem in languages without natural delimiters, existing methods ignore the complement of lexical information to BERT. In addition, the inherent multi-role noise problem could limit the performance of methods when one sentence contains multiple events. In this article, we propose a Mask-Attention-based BERT (MABERT) framework for Chinese event extraction to address the above problems. Firstly, in order to avoid trigger-word mismatch and integrate lexical features into BERT layers directly, a mask-attention-based transformer augmented with two mask matrices is devised to replace the original one in BERT. By the mask-attention-based transformer, the character sequence interacts with external lexical semantics sufficiently and keeps its structure information at the same time. Moreover, against the multi-role noise problem, we make use of event type information from representation and classification, two aspects to enrich entity features, where type markers and event-schema-based mask matrix are proposed. Experimental results on the widely used ACE2005 dataset show the effectiveness of our proposed MABERT on Chinese event extraction task compared with other state-of-the-art methods. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Additional Key Words And Phrasesevent Extraction; Event Ontology; Event Type Markers; Mask-attention-based Transformer; Classification (of Information); Semantics; Additional Key Word And Phrasesevent Extraction; Event Ontology; Event Type Marker; Event Types; Events Extractions; Key Words; Language Model; Mask-attention-based Transformer; Matrix; Noise Problems; Noise Pollution},
	keywords = {Classification (of information); Semantics; Additional key word and phrasesevent extraction; Event ontology; Event type marker; Event Types; Events extractions; Key words; Language model; Mask-attention-based transformer; matrix; Noise problems; Noise pollution},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{Li2023216,
	author = {Li, Na and Kteich, Hanane and Bouraoui, Zied and Schockaert, Steven},
	title = {Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models},
	year = {2023},
	pages = {216 - 226},
	doi = {10.1145/3539618.3591667},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168689425&doi=10.1145%2F3539618.3591667&partnerID=40&md5=356be0186e0d1776943b63ff78969615},
	abstract = {Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence from the neighbourhood structure of the contextualised word embeddings. The second strategy instead relies on a distant supervision signal from ConceptNet. Our experimental results show that the resulting vectors substantially outperform existing concept embeddings in predicting the semantic properties of concepts, with the ConceptNet-based strategy achieving the best results. These findings are furthermore confirmed in a clustering task and in the downstream task of ontology completion. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Commonsense Knowledge; Contrastive Learning; Language Models; Word Embedding; Computational Linguistics; Embeddings; Semantics; Commonsense Knowledge; Conceptnet; Contrastive Learning; Down-stream; Language Model; Property; Semantic Properties; Word Embedding; Word Vectors; Vectors},
	keywords = {Computational linguistics; Embeddings; Semantics; Commonsense knowledge; ConceptNet; Contrastive learning; Down-stream; Language model; Property; Semantic properties; Word embedding; Word vectors; Vectors},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Zheng2023,
	author = {Zheng, Rongtao and Huang, Zhijian and Deng, Lei},
	title = {Large-scale predicting protein functions through heterogeneous feature fusion},
	year = {2023},
	journal = {Briefings in Bioinformatics},
	volume = {24},
	number = {4},
	pages = {},
	doi = {10.1093/bib/bbad243},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165521785&doi=10.1093%2Fbib%2Fbbad243&partnerID=40&md5=bbda34a15fce66db5a4f788fd5011a5c},
	abstract = {As the volume of protein sequence and structure data grows rapidly, the functions of the overwhelming majority of proteins cannot be experimentally determined. Automated annotation of protein function at a large scale is becoming increasingly important. Existing computational prediction methods are typically based on expanding the relatively small number of experimentally determined functions to large collections of proteins with various clues, including sequence homology, protein–protein interaction, gene co-expression, etc. Although there has been some progress in protein function prediction in recent years, the development of accurate and reliable solutions still has a long way to go. Here we exploit AlphaFold predicted three-dimensional structural information, together with other non-structural clues, to develop a large-scale approach termed PredGO to annotate Gene Ontology (GO) functions for proteins. We use a pre-trained language model, geometric vector perceptrons and attention mechanisms to extract heterogeneous features of proteins and fuse these features for function prediction. The computational results demonstrate that the proposed method outperforms other state-of-the-art approaches for predicting GO functions of proteins in terms of both coverage and accuracy. The improvement of coverage is because the number of structures predicted by AlphaFold is greatly increased, and on the other hand, PredGO can extensively use non-structural information for functional prediction. Moreover, we show that over 205 000 (∼100%) entries in UniProt for human are annotated by PredGO, over 186 000 (∼90%) of which are based on predicted structure. The webserver and database are available at http://predgo.denglab.org/. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Mining; Feature Fusion; Graph Neural Network; Protein Function Prediction; Protein; Proteins; Article; Attention; Data Mining; Gene Ontology; Human; Human Experiment; Language; Perceptron; Prediction; Protein Function; Structure Activity Relation; Amino Acid Sequence; Artificial Neural Network; Bioinformatics; Chemistry; Factual Database; Procedures; Protein Database; Protein; Amino Acid Sequence; Computational Biology; Databases, Factual; Databases, Protein; Humans; Neural Networks, Computer; Proteins},
	keywords = {article; attention; data mining; gene ontology; human; human experiment; language; perceptron; prediction; protein function; structure activity relation; amino acid sequence; artificial neural network; bioinformatics; chemistry; factual database; procedures; protein database; protein; Amino Acid Sequence; Computational Biology; Databases, Factual; Databases, Protein; Humans; Neural Networks, Computer; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@CONFERENCE{Alam202316395,
	author = {Alam, Firoj and Dalvi, Fahim and Durrani, Nadir and Sajjad, Hassan and Khan, Abdul Rafae and Xu, Jia},
	title = {ConceptX: A Framework for Latent Concept Analysis},
	year = {2023},
	volume = {37},
	pages = {16395 - 16397},
	doi = {10.1609/aaai.v37i13.27057},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159861519&doi=10.1609%2Faaai.v37i13.27057&partnerID=40&md5=a26faeea794b745e6d010ce409c04470},
	abstract = {The opacity of deep neural networks remains a challenge in deploying solutions where explanation is as important as precision. We present ConceptX, a human-in-the-loop framework for interpreting and annotating latent representational space in pre-trained Language Models (pLMs). We use an unsupervised method to discover concepts learned in these models and enable a graphical interface for humans to generate explanations for the concepts. To facilitate the process, we provide auto-annotations of the concepts (based on traditional linguistic ontologies). Such annotations enable development of a linguistic resource that directly represents latent concepts learned within deep NLP models. These include not just traditional linguistic concepts, but also task-specific or sensitive concepts (words grouped based on gender or religious connotation) that helps the annotators to mark bias in the model. The framework consists of two parts (i) concept discovery and (ii) annotation platform. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Neural Networks; Auto-annotation; Concept Analysis; Concept Discoveries; Concept-based; Graphical Interface; Human-in-the-loop; Language Model; Linguistic Ontology; Linguistic Resources; Unsupervised Method; Linguistics},
	keywords = {Deep neural networks; Auto-annotation; Concept analysis; Concept discoveries; Concept-based; Graphical interface; Human-in-the-loop; Language model; Linguistic ontology; Linguistic resources; Unsupervised method; Linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Corrado2023330,
	author = {Corrado, Mario and Giliberti, Vincenzo and Gozzi, Manuel and Lanzolla, Vincenzo and Vetere, Guido and Zurlo, Domenico},
	title = {Assisting the Assistant: obot for Voice Customer Support},
	year = {2023},
	journal = {Frontiers in Artificial Intelligence and Applications},
	volume = {368},
	pages = {330 - 339},
	doi = {10.3233/FAIA230096},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171471256&doi=10.3233%2FFAIA230096&partnerID=40&md5=72f4209ead4f5ad39085c487ba5e6ebd},
	abstract = {Despite recent advances in automation, customer support still requires a substantial amount of human intervention through voice channels. With the aim of improving the work of human assistants, we developed a collaborative bot (cobot) to help them in the process of handling customer voice interactions. The cobot is a reasoning agent that starts from loading background customer data into a dynamic knowledge graph. Then it captures the audio stream of the conversation, converts it to text in real time, analyzes the blocks of conversation with neural technologies and 'thinks' about the results. Assistants can also supply data to the cobot, based on the information they gather from the ongoing conversation. The reasoning agent provides information and action suggestions to the human assistant by applying heuristics on data collected from both automatic and human sources, based on a task and domain-specific conceptual models (ontologies). While designing a prototypical solution for utility services in Italy, we are faced with many problems, including spontaneous speech understanding, factual and linguistic knowledge representation, and efficient heuristic reasoning. We adopted a standards-based approach and experimented with open source reasoners and publicly available language models. The paper presents preliminary findings and outlines the system design, with focus on the interplay of neural language processing and logic reasoning. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Collaborative Bots; Knowledge Graph; Natural Language Understanding; Ontologies; Virtual Assistants; Atoms; Botnet; Knowledge Graph; Sales; Collaborative Bot; Customer Data; Customer Support; Human Intervention; Knowledge Graphs; Natural Language Understanding; Ontology's; Virtual Assistants; Voice Channels; Voice Interaction; Ontology},
	keywords = {Atoms; Botnet; Knowledge graph; Sales; Collaborative bot; Customer data; Customer support; Human intervention; Knowledge graphs; Natural language understanding; Ontology's; Virtual assistants; Voice channels; Voice interaction; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Nebbia202367,
	author = {Nebbia, Giacomo and Kovashka, Adriana},
	title = {Hypernymization of named entity-rich captions for grounding-based multi-modal pretraining},
	year = {2023},
	pages = {67 - 75},
	doi = {10.1145/3591106.3592223},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163577770&doi=10.1145%2F3591106.3592223&partnerID=40&md5=35bad7362c4bdf582bb23180ec056d5e},
	abstract = {Named entities are ubiquitous in text that naturally accompanies images, especially in domains such as news or Wikipedia articles. In previous work, named entities have been identified as a likely reason for low performance of image-text retrieval models pretrained on Wikipedia and evaluated on named entities-free benchmark datasets. Because they are rarely mentioned, named entities could be challenging to model. They also represent missed learning opportunities for self-supervised models: the link between named entity and object in the image may be missed by the model, but it would not be if the object were mentioned using a more common term. In this work, we investigate hypernymization as a way to deal with named entities for pretraining grounding-based multi-modal models and for fine-tuning on open-vocabulary detection. We propose two ways to perform hypernymization: (1) a "manual"pipeline relying on a comprehensive ontology of concepts, and (2) a "learned"approach where we train a language model to learn to perform hypernymization. We run experiments on data from Wikipedia and from The New York Times. We report improved pretraining performance on objects of interest following hypernymization, and we show the promise of hypernymization on open-vocabulary detection, specifically on classes not seen during training. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Grounding; Hypernymization; Named Entities; Open-vocabulary Detection; Natural Language Processing Systems; Hypernymization; Image Texts; Multi-modal; Named Entities; News Articles; Open-vocabulary Detection; Performance; Pre-training; Wikipedia; Wikipedia Articles; Benchmarking},
	keywords = {Natural language processing systems; Hypernymization; Image texts; Multi-modal; Named entities; News articles; Open-vocabulary detection; Performance; Pre-training; Wikipedia; Wikipedia articles; Benchmarking},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Baker202360,
	author = {Baker, Bernadette M. and Mills, Kathy A. and McDonald, Peter D. and Wang, Liang},
	title = {AI, Concepts of Intelligence, and Chatbots: The "Figure of Man," the Rise of Emotion, and Future Visions of Education},
	year = {2023},
	journal = {Teachers College Record},
	volume = {125},
	number = {6},
	pages = {60 - 84},
	doi = {10.1177/01614681231191291},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208671333&doi=10.1177%2F01614681231191291&partnerID=40&md5=1a9bbdefd47f4f38371bc836a18db2c0},
	abstract = {Background: Artificial intelligence (AI) applications have been implemented across all levels of education, with the rapid developments of chatbots and AI language models, like ChatGPT, demonstrating the urgent need to conceptualize the key debates and their implications for a new era of learning and assessment. This adoption occurs in a context where AI is dramatically remapping "the human,"the purposes of schooling, and pedagogy. Focus of Study: The paper examines how different formulations of "human"became interwoven with the sliding signifier of "intelligence"through a series of violent exclusions, and how the shifting contour of "intelligence"produces uneven and unjust ontological scales undergirding both education and AI fields. Its purpose is to engage the education research community in dialogue about biases, the nature of ethics, and decision-making concerning AI in education. Research Design: This paper adapts a historical-philosophical method. It traces the effects of colonialism and racialization within humanism's emergence through Sylvia Wynter's historiography of "figure of Man,"especially via the invention of "intelligence,"which has linked education and computer science. It also investigates themes central to modern education such as justice, equity, and in/exclusion through a philosophical examination of the ontological scales of "human."Conclusions: After outlining how "intelligence"has shifted from reason-as-morality to concepts of natural intelligence, we argue that current examples of AI in Education (AIEd), like classroom chatbots and social agents, constitute an intermediary point in the arc toward a new computational superintelligence-the emergence of man3- illustrating the opportunities, risks, and ethical issues in pedagogical applications based on emotion. We outline three differing visions of AIEd's future, concluding with a series of provocations (onto-epistemological, practice-based, and purposes of schooling) that exceed such models and that, given rapid innovations in machine learning, require urgent consideration from multiple stakeholders. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence (ai); Chatbot; Emotion; Figure Of Man; Intelligence},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32}
}

@ARTICLE{Liu20233873,
	author = {Liu, Feifei and Liu, Mingtong and Li, Meiting and Xin, Yuwei and Gao, Dongping and Wu, Jun and Zhu, Jia'An},
	title = {Automatic knowledge extraction from Chinese electronic medical records and rheumatoid arthritis knowledge graph construction},
	year = {2023},
	journal = {Quantitative Imaging in Medicine and Surgery},
	volume = {13},
	number = {6},
	pages = {3873 - 3890},
	doi = {10.21037/qims-22-1158},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162134405&doi=10.21037%2Fqims-22-1158&partnerID=40&md5=976bcd9f12fb0a371fa17d9918eee7e5},
	abstract = {Background: Knowledge graphs are a powerful tool for organizing knowledge, processing information and integrating scattered information, effectively visualizing the relationships among entities and supporting further intelligent applications. One of the critical tasks in building knowledge graphs is knowledge extraction. The existing knowledge extraction models in the Chinese medical domain usually require high-quality and large-scale manually labeled corpora for model training. In this study, we investigate rheumatoid arthritis (RA)-related Chinese electronic medical records (CEMRs) and address the automatic knowledge extraction task with a small number of annotated samples from CEMRs, from which an authoritative RA knowledge graph is constructed. Methods: After constructing the domain ontology of RA and completing manual labeling, we propose the MC-bidirectional encoder representation from transformers-bidirectional long short-term memory-conditional random field (BERT-BiLSTM-CRF) model for the named entity recognition (NER) task and the MC-BERT + feedforward neural network (FFNN) model for the entity extraction task. The pretrained language model (MC-BERT) is trained with many unlabeled medical data and fine-tuned using other medical domain datasets. We apply the established model to automatically label the remaining CEMRs, and then an RA knowledge graph is constructed based on the entities and entity relations, a preliminary assessment is conducted, and an intelligent application is presented. Results: The proposed model achieved better performance than that of other widely used models in knowledge extraction tasks, with mean F1 scores of 92.96% in entity recognition and 95.29% in relation extraction. This study preliminarily confirmed that using a pretrained medical language model could solve the problem that knowledge extraction from CEMRs requires a large number of manual annotations. An RA knowledge graph based on the above identified entities and extracted relations from 1,986 CEMRs was constructed. Experts verified the effectiveness of the constructed RA knowledge graph. Conclusions: In this paper, an RA knowledge graph based on CEMRs was established, the processes of data annotation, automatic knowledge extraction, and knowledge graph construction were described, and a preliminary assessment and an application were presented. The study demonstrated the viability of a pretrained language model combined with a deep neural network for knowledge extraction tasks from CEMRs based on a small number of manually annotated samples. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Electronic Medical Records (emrs); Entity Recognition; Entity Relation Extraction; Knowledge Graph; Rheumatoid Arthritis (ra); Article; Artificial Intelligence; China; Classification Algorithm; Clinical Effectiveness; Conditional Random Field; Data Visualization; Deep Neural Network; Electronic Medical Record; Feature Extraction; Feed Forward Neural Network; Human; Information Processing; Knowledge; Knowledge Base; Knowledge Graph; Long Short Term Memory Network; Mathematical Model; Natural Language Processing; Preliminary Data; Recurrent Neural Network; Rheumatoid Arthritis; Semantics},
	keywords = {Article; artificial intelligence; China; classification algorithm; clinical effectiveness; conditional random field; data visualization; deep neural network; electronic medical record; feature extraction; feed forward neural network; human; information processing; knowledge; knowledge base; knowledge graph; long short term memory network; mathematical model; natural language processing; preliminary data; recurrent neural network; rheumatoid arthritis; semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Badenes-Olmedo2023,
	author = {Badenes-Olmedo, Carlos and Corcho, Oscar},
	title = {Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature},
	year = {2023},
	journal = {Journal of Biomedical Informatics},
	volume = {142},
	pages = {},
	doi = {10.1016/j.jbi.2023.104382},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160010635&doi=10.1016%2Fj.jbi.2023.104382&partnerID=40&md5=76ee70157ffc4b3034eb4d0e4e617193},
	abstract = {The article presents a workflow to create a question-answering system whose knowledge base combines knowledge graphs and scientific publications on coronaviruses. It is based on the experience gained in modeling evidence from research articles to provide answers to questions in natural language. The work contains best practices for acquiring scientific publications, tuning language models to identify and normalize relevant entities, creating representational models based on probabilistic topics, and formalizing an ontology that describes the associations between domain concepts supported by the scientific literature. All the resources generated in the domain of coronavirus are available openly as part of the Drugs4COVID initiative, and can be (re)-used independently or as a whole. They can be exploited by scientific communities conducting research related to SARS-CoV-2/COVID-19 and also by therapeutic communities, laboratories, etc., wishing to find and understand relationships between symptoms, drugs, active ingredients and their documentary evidence. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Evidences; Knowledge Graphs; Ontology; Question-answering; Chloroquine; Lamivudine; Nicotine; Diseases; Knowledge Graph; Modeling Languages; Natural Language Processing Systems; Ontology; Case-studies; Coronaviruses; Evidence; Knowledge Graphs; Natural Languages; Ontology's; Question Answering; Question Answering Systems; Scientific Publications; Work-flows; Coronavirus; Chloroquine; Lamivudine; Neutralizing Antibody; Nicotine; Algorithm; Answering Service; Article; Cigarette Smoking; Coronavirus Disease 2019; Data Mining; Gene Ontology; Glycosylation; Health Care Personnel; Human; Knowledge; Knowledge Base; Language; Machine Learning; Middle East Respiratory Syndrome Coronavirus; Named Entity Recognition; Natural Language Processing; Ontology; Open Access Publishing; Pregnancy; Publication; Question; Questionnaire; Scientific Literature; Severe Acute Respiratory Syndrome Coronavirus 2; Smoke; Therapeutic Community; Tobacco Consumption; Traditional Medicine; Workflow; Automated Pattern Recognition; Covid-19; Humans; Pattern Recognition, Automated; Publications; Sars-cov-2},
	keywords = {Diseases; Knowledge graph; Modeling languages; Natural language processing systems; Ontology; Case-studies; Coronaviruses; Evidence; Knowledge graphs; Natural languages; Ontology's; Question Answering; Question answering systems; Scientific publications; Work-flows; Coronavirus; chloroquine; lamivudine; neutralizing antibody; nicotine; algorithm; answering service; Article; cigarette smoking; coronavirus disease 2019; data mining; gene ontology; glycosylation; health care personnel; human; knowledge; knowledge base; language; machine learning; Middle East respiratory syndrome coronavirus; named entity recognition; natural language processing; ontology; open access publishing; pregnancy; publication; question; questionnaire; scientific literature; Severe acute respiratory syndrome coronavirus 2; smoke; therapeutic community; tobacco consumption; traditional medicine; workflow; automated pattern recognition; COVID-19; Humans; Pattern Recognition, Automated; Publications; SARS-CoV-2},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Yuan2023,
	author = {Yuan, Qianmu and Xie, Junjie and Xie, Jiancong and Zhao, Huiying and Yang, Yuedong},
	title = {Fast and accurate protein function prediction from sequence through pretrained language model and homology-based label diffusion},
	year = {2023},
	journal = {Briefings in Bioinformatics},
	volume = {24},
	number = {3},
	pages = {},
	doi = {10.1093/bib/bbad117},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159762677&doi=10.1093%2Fbib%2Fbbad117&partnerID=40&md5=7207030eb836117491c555f4ffd01c7b},
	abstract = {Protein function prediction is an essential task in bioinformatics which benefits disease mechanism elucidation and drug target discovery. Due to the explosive growth of proteins in sequence databases and the diversity of their functions, it remains challenging to fast and accurately predict protein functions from sequences alone. Although many methods have integrated protein structures, biological networks or literature information to improve performance, these extra features are often unavailable for most proteins. Here, we propose SPROF-GO, a Sequence-based alignment-free PROtein Function predictor, which leverages a pretrained language model to efficiently extract informative sequence embeddings and employs self-attention pooling to focus on important residues. The prediction is further advanced by exploiting the homology information and accounting for the overlapping communities of proteins with related functions through the label diffusion algorithm. SPROF-GO was shown to surpass state-of-the-art sequence-based and even network-based approaches by more than 14.5, 27.3 and 10.1% in area under the precision-recall curve on the three sub-ontology test sets, respectively. Our method was also demonstrated to generalize well on non-homologous proteins and unseen species. Finally, visualization based on the attention mechanism indicated that SPROF-GO is able to capture sequence domains useful for function prediction. The datasets, source codes and trained models of SPROF-GO are available at https://github.com/biomed-AI/SPROF-GO. The SPROF-GO web server is freely available at http://bio-web1.nscc-gz.cn/app/sprof-go. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Label Diffusion; Pretrained Language Model; Protein Function Prediction; Sequence-based; Protein; Proteins; Protein; Algorithm; Bioinformatics; Gene Ontology; Metabolism; Procedures; Software; Algorithms; Computational Biology; Gene Ontology; Proteins; Software},
	keywords = {protein; algorithm; bioinformatics; gene ontology; metabolism; procedures; software; Algorithms; Computational Biology; Gene Ontology; Proteins; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 52; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Zaitoun20231127,
	author = {Zaitoun, Antonio and Sagi, Tomer and Hose, Katja},
	title = {Automated Ontology Evaluation: Evaluating Coverage and Correctness using a Domain Corpus},
	year = {2023},
	pages = {1127 - 1137},
	doi = {10.1145/3543873.3587617},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159640436&doi=10.1145%2F3543873.3587617&partnerID=40&md5=a923fb75bd24101af15c6d0fbf9fe4a2},
	abstract = {Ontologies conceptualize domains and are a crucial part of web semantics and information systems. However, re-using an existing ontology for a new task requires a detailed evaluation of the candidate ontology as it may cover only a subset of the domain concepts, contain information that is redundant or misleading, and have inaccurate relations and hierarchies between concepts. Manual evaluation of large and complex ontologies is a tedious task. Thus, a few approaches have been proposed for automated evaluation, ranging from concept coverage to ontology generation from a corpus. Existing approaches, however, are limited by their dependence on external structured knowledge sources, such as a thesaurus, as well as by their inability to evaluate semantic relationships. In this paper, we propose a novel framework to automatically evaluate the domain coverage and semantic correctness of existing ontologies based on domain information derived from text. The approach uses a domain-tuned named-entity-recognition model to extract phrasal concepts. The extracted concepts are then used as a representation of the domain against which we evaluate the candidate ontology's concepts. We further employ a domain-tuned language model to determine the semantic correctness of the candidate ontology's relations. We demonstrate our automated approach on several large ontologies from the oceanographic domain and show its agreement with a manual evaluation by domain experts and its superiority over the state-of-the-art. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Knowledge Engineering; Natural Language Processing; Ontology; Automation; Knowledge Engineering; Natural Language Processing Systems; Semantics; Automated Evaluation; Bert; Domain Concepts; Language Processing; Natural Language Processing; Natural Languages; Ontology Evaluations; Ontology's; Web Information; Web Semantics; Ontology},
	keywords = {Automation; Knowledge engineering; Natural language processing systems; Semantics; Automated evaluation; BERT; Domain concepts; Language processing; Natural language processing; Natural languages; Ontology evaluations; Ontology's; Web information; Web semantics; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Yang20231468,
	author = {Yang, Yuting and Lei, Wengqiang and Huang, Pei and Cao, Juan and Li, Jintao and Chua, Tat Seng},
	title = {A Dual Prompt Learning Framework for Few-Shot Dialogue State Tracking},
	year = {2023},
	pages = {1468 - 1477},
	doi = {10.1145/3543507.3583238},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159312513&doi=10.1145%2F3543507.3583238&partnerID=40&md5=42feac9ec106e0a4344963550e271d0b},
	abstract = {Dialogue State Tracking (DST) module is an essential component of task-oriented dialog systems to understand users' goals and needs. Collecting dialogue state labels including slots and values can be costly, requiring experts to annotate all (slot, value) information for each turn in dialogues. It is also difficult to define all possible slots and values in advance, especially with the wide application of dialogue systems in more and more new-rising applications. In this paper, we focus on improving DST module to generate dialogue states in circumstances with limited annotations and knowledge about slot ontology. To this end, we design a dual prompt learning framework for few-shot DST. The dual framework aims to explore how to utilize the language understanding and generation capabilities of pre-trained language models for DST efficiently. Specifically, we consider the learning of slot generation and value generation as dual tasks, and two kinds of prompts are designed based on this dual structure to incorporate task-related knowledge of these two tasks respectively. In this way, the DST task can be formulated as a language modeling task efficiently under few-shot settings. To evaluate the proposed framework, we conduct experiments on two task-oriented dialogue datasets. The results demonstrate that the proposed method not only outperforms existing state-of-the-art few-shot methods, but also can generate unseen slots. It indicates that DST-related knowledge can be probed from pre-trained language models and utilized to address low-resource DST efficiently with the help of prompt learning. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Dialogue State Tracking; Few-shot Learning; Prompt Learning; Learning Systems; Modeling Languages; Speech Processing; Dialog State Tracking; Dialogue Systems; Few-shot Learning; Language Model; Learning Frameworks; Prompt Learning; State Tracking; Task-oriented; Tracking Module; User Goals; Computational Linguistics},
	keywords = {Learning systems; Modeling languages; Speech processing; Dialog state tracking; Dialogue systems; Few-shot learning; Language model; Learning frameworks; Prompt learning; State tracking; Task-oriented; Tracking module; User goals; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Lopes2023,
	author = {Lopes, Alcides Gonçalves and Carbonera, Joel Lúis and Schmidt, Daniela and Garcia, Luan Fonseca and Rodrigues, Fabrício Henrique Henrique and Abel, Mara},
	title = {Using terms and informal definitions to classify domain entities into top-level ontology concepts: An approach based on language models},
	year = {2023},
	journal = {Knowledge-Based Systems},
	volume = {265},
	pages = {},
	doi = {10.1016/j.knosys.2023.110385},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148090819&doi=10.1016%2Fj.knosys.2023.110385&partnerID=40&md5=d28c6116a49c3e36f2e7d8c599830429},
	abstract = {The classification of domain entities into top-level ontology concepts remains an activity performed manually by an ontology engineer. Although some works focus on automating this task by applying machine-learning approaches using textual sentences as input, they require the existence of the domain entities in external knowledge resources, such as pre-trained embedding models. In this context, this work proposes an approach that combines the term representing the domain entity and its informal definition into a single text sentence without requiring external knowledge resources. Thus, we use this sentence as the input of a deep neural network that contains a language model as a layer. Also, we present a methodology used to extract two novel datasets from the OntoWordNet ontology based on Dolce-Lite and Dolce-Lite-Plus top-level ontologies. Our experiments show that by using the transformer-based language models, we achieve promising results in classifying domain entities into 82 top-level ontology concepts, with 94% regarding micro F1-score. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Language Model; Ontology Learning; Top-level Ontology; Computational Linguistics; Deep Neural Networks; Learning Systems; Multilayer Neural Networks; Domain Entities; Embeddings; External Knowledge; Knowledge Resource; Language Model; Machine Learning Approaches; Ontology Concepts; Ontology Learning; Ontology's; Top-level Ontology; Ontology},
	keywords = {Computational linguistics; Deep neural networks; Learning systems; Multilayer neural networks; Domain entities; Embeddings; External knowledge; Knowledge resource; Language model; Machine learning approaches; Ontology concepts; Ontology learning; Ontology's; Top-level ontology; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14}
}

@ARTICLE{Liu2023,
	author = {Liu, Yi and Elsworth, Benjamin L. and Gaunt, Tom R.},
	title = {Using language models and ontology topology to perform semantic mapping of traits between biomedical datasets},
	year = {2023},
	journal = {Bioinformatics},
	volume = {39},
	number = {4},
	pages = {},
	doi = {10.1093/bioinformatics/btad169},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152491445&doi=10.1093%2Fbioinformatics%2Fbtad169&partnerID=40&md5=5e7bd4c04bb8770573d460b45e4c5902},
	abstract = {Motivation: Human traits are typically represented in both the biomedical literature and large population studies as descriptive text strings. Whilst a number of ontologies exist, none of these perfectly represent the entire human phenome and exposome. Mapping trait names across large datasets is therefore time-consuming and challenging. Recent developments in language modelling have created new methods for semantic representation of words and phrases, and these methods offer new opportunities to map human trait names in the form of words and short phrases, both to ontologies and to each other. Here, we present a comparison between a range of established and more recent language modelling approaches for the task of mapping trait names from UK Biobank to the Experimental Factor Ontology (EFO), and also explore how they compare to each other in direct trait-to-trait mapping. Results: In our analyses of 1191 traits from UK Biobank with manual EFO mappings, the BioSentVec model performed best at predicting these, matching 40.3% of the manual mappings correctly. The BlueBERT-EFO model (finetuned on EFO) performed nearly as well (38.8% of traits matching the manual mapping). In contrast, Levenshtein edit distance only mapped 22% of traits correctly. Pairwise mapping of traits to each other demonstrated that many of the models can accurately group similar traits based on their semantic similarity. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Article; Biobank; Human; Human Experiment; Language; Ontology; Biological Ontology; Natural Language Processing; Phenotype; Semantics; Biological Ontologies; Humans; Language; Natural Language Processing; Phenotype; Semantics},
	keywords = {article; biobank; human; human experiment; language; ontology; biological ontology; natural language processing; phenotype; semantics; Biological Ontologies; Humans; Language; Natural Language Processing; Phenotype; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Amur2023,
	author = {Amur, Zaira Hassan and Kwang Hooi, Yew and Bhanbhro, Hina and Dahri, Kamran and Soomro, Gul Muhammad},
	title = {Short-Text Semantic Similarity (STSS): Techniques, Challenges and Future Perspectives},
	year = {2023},
	journal = {Applied Sciences (Switzerland)},
	volume = {13},
	number = {6},
	pages = {},
	doi = {10.3390/app13063911},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152052419&doi=10.3390%2Fapp13063911&partnerID=40&md5=dc2a49a775cb318ef8151393ec11ba06},
	abstract = {In natural language processing, short-text semantic similarity (STSS) is a very prominent field. It has a significant impact on a broad range of applications, such as question–answering systems, information retrieval, entity recognition, text analytics, sentiment classification, and so on. Despite their widespread use, many traditional machine learning techniques are incapable of identifying the semantics of short text. Traditional methods are based on ontologies, knowledge graphs, and corpus-based methods. The performance of these methods is influenced by the manually defined rules. Applying such measures is still difficult, since it poses various semantic challenges. In the existing literature, the most recent advances in short-text semantic similarity (STSS) research are not included. This study presents the systematic literature review (SLR) with the aim to (i) explain short sentence barriers in semantic similarity, (ii) identify the most appropriate standard deep learning techniques for the semantics of a short text, (iii) classify the language models that produce high-level contextual semantic information, (iv) determine appropriate datasets that are only intended for short text, and (v) highlight research challenges and proposed future improvements. To the best of our knowledge, we have provided an in-depth, comprehensive, and systematic review of short text semantic similarity trends, which will assist the researchers to reuse and enhance the semantic information. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Natural Language Processing; Semantic Similarity; Short Text; Stss},
	type = {Review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 35; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Yang2023,
	author = {Yang, Heng and Wang, Nan and Yang, Lina and Liu, Wei and Sili, Wang},
	title = {Research on the Automatic Subject-Indexing Method of Academic Papers Based on Climate Change Domain Ontology},
	year = {2023},
	journal = {Sustainability (Switzerland)},
	volume = {15},
	number = {5},
	pages = {},
	doi = {10.3390/su15053919},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149997585&doi=10.3390%2Fsu15053919&partnerID=40&md5=c281237047c3b56c8259756e1c89eac4},
	abstract = {It is important to classify academic papers in a fine-grained manner to uncover deeper implicit themes and semantics in papers for better semantic retrieval, paper recommendation, research trend prediction, topic analysis, and a series of other functions. Based on the ontology of the climate change domain, this study used an unsupervised approach to combine two methods, syntactic structure and semantic modeling, to build a framework of subject-indexing techniques for academic papers in the climate change domain. The framework automatically indexes a set of conceptual terms as research topics from the domain ontology by inputting the titles, abstracts and keywords of the papers using natural language processing techniques such as syntactic dependencies, text similarity calculation, pre-trained language models, semantic similarity calculation, and weighting factors such as word frequency statistics and graph path calculation. Finally, we evaluated the proposed method using the gold standard of manually annotated articles and demonstrated significant improvements over the other five alternative methods in terms of precision, recall and F1-score. Overall, the method proposed in this study is able to identify the research topics of academic papers more accurately, and also provides useful references for the application of domain ontologies and unsupervised data annotation. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Automatic Subject Indexing; Climate Change; Deep Mining; Ontology; Semantic; Academic Research; Climate Change; Conceptual Framework; Index Method; Language},
	keywords = {academic research; climate change; conceptual framework; index method; language},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@ARTICLE{Castell-Díaz2023,
	author = {Castell-Díaz, Javier and Miñarro-Giménez, Jose Antonio and Martínez-Costa, Catalina},
	title = {Supporting SNOMED CT postcoordination with knowledge graph embeddings},
	year = {2023},
	journal = {Journal of Biomedical Informatics},
	volume = {139},
	pages = {},
	doi = {10.1016/j.jbi.2023.104297},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149060154&doi=10.1016%2Fj.jbi.2023.104297&partnerID=40&md5=dbb6b9e9b76e3e84fa7982905453d84d},
	abstract = {SNOMED CT postcoordination is an underused mechanism that can help to implement advanced systems for the automatic extraction and encoding of clinical information from text. It allows defining non-existing SNOMED CT concepts by their relationships with existing ones. Manually building postcoordinated expressions is a difficult task. It requires a deep knowledge of the terminology and the support of specialized tools that barely exist. In order to support the building of postcoordinated expressions, we have implemented KGE4SCT: a method that suggests the corresponding SNOMED CT postcoordinated expression for a given clinical term. We leverage on the SNOMED CT ontology and its graph-like structure and use knowledge graph embeddings (KGEs). The objective of such embeddings is to represent in a vector space knowledge graph components (e.g. entities and relations) in a way that captures the structure of the graph. Then, we use vector similarity and analogies for obtaining the postcoordinated expression of a given clinical term. We obtained a semantic type accuracy of 98%, relationship accuracy of 90%, and analogy accuracy of 60%, with an overall completeness of postcoordination of 52% for the Spanish SNOMED CT version. We have also applied it to the English SNOMED CT version and outperformed state of the art methods in both, corpus generation for language model training for this task (improvement of 6% for analogy accuracy), and automatic postcoordination of SNOMED CT expressions, with an increase of 17% for partial conversion rate. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Embeddings; Ontology; Postcoordination; Snomed Ct; Graph Embeddings; Knowledge Graph; Knowledge Management; Semantics; Vector Spaces; Advanced Systems; Automatic Extraction; Clinical Terms; Encodings; Knowledge Graph Embedding; Knowledge Graphs; Ontology's; Postcoordination; Snomed-ct; Ontology; Article; Embedding; Human; Human Experiment; Language; Nomenclature; Ontology; Protein Expression; Systematized Nomenclature Of Medicine; Automated Pattern Recognition; Natural Language Processing; Semantics; Language; Natural Language Processing; Pattern Recognition, Automated},
	keywords = {Graph embeddings; Knowledge graph; Knowledge management; Semantics; Vector spaces; Advanced systems; Automatic extraction; Clinical terms; Encodings; Knowledge graph embedding; Knowledge graphs; Ontology's; Postcoordination; SNOMED-CT; Ontology; article; embedding; human; human experiment; language; nomenclature; ontology; protein expression; Systematized Nomenclature of Medicine; automated pattern recognition; natural language processing; semantics; Language; Natural Language Processing; Pattern Recognition, Automated},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Lyu2023,
	author = {Lyu, Kewei and Tian, Yu and Shang, Yong and Zhou, Tianshu and Yang, Ziyue and Liu, Qianghua and Yao, Xi and Zhang, Ping and Chen, Jianghua and Li, Jingsong},
	title = {Causal knowledge graph construction and evaluation for clinical decision support of diabetic nephropathy},
	year = {2023},
	journal = {Journal of Biomedical Informatics},
	volume = {139},
	pages = {},
	doi = {10.1016/j.jbi.2023.104298},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147538756&doi=10.1016%2Fj.jbi.2023.104298&partnerID=40&md5=c209c46ed49ec0410bc473201929091f},
	abstract = {Background: Many important clinical decisions require causal knowledge (CK) to take action. Although many causal knowledge bases for medicine have been constructed, a comprehensive evaluation based on real-world data and methods for handling potential knowledge noise are still lacking. Objective: The objectives of our study are threefold: (1) propose a framework for the construction of a large-scale and high-quality causal knowledge graph (CKG); (2) design the methods for knowledge noise reduction to improve the quality of the CKG; (3) evaluate the knowledge completeness and accuracy of the CKG using real-world data. Material and methods: We extracted causal triples from three knowledge sources (SemMedDB, UpToDate and Churchill's Pocketbook of Differential Diagnosis) based on rule methods and language models, performed ontological encoding, and then designed semantic modeling between electronic health record (EHR) data and the CKG to complete knowledge instantiation. We proposed two graph pruning strategies (co-occurrence ratio and causality ratio) to reduce the potential noise introduced by SemMedDB. Finally, the evaluation was carried out by taking the diagnostic decision support (DDS) of diabetic nephropathy (DN) as a real-world case. The data originated from a Chinese hospital EHR system from October 2010 to October 2020. The knowledge completeness and accuracy of the CKG were evaluated based on three state-of-the-art embedding methods (R-GCN, MHGRN and MedPath), the annotated clinical text and the expert review, respectively. Results: This graph included 153,289 concepts and 1,719,968 causal triples. A total of 1427 inpatient data were used for evaluation. Better results were achieved by combining three knowledge sources than using only SemMedDB (three models: area under the receiver operating characteristic curve (AUC): p < 0.01, F1: p < 0.01), and the graph covered 93.9 % of the causal relations between diseases and diagnostic evidence recorded in clinical text. Causal relations played a vital role in all relations related to disease progression for DDS of DN (three models: AUC: p > 0.05, F1: p > 0.05), and after pruning, the knowledge accuracy of the CKG was significantly improved (three models: AUC: p < 0.01, F1: p < 0.01; expert review: average accuracy: + 5.5 %). Conclusions: The results demonstrated that our proposed CKG could completely and accurately capture the abstract CK under the concrete EHR data, and the pruning strategies could improve the knowledge accuracy of our CKG. The CKG has the potential to be applied to the DDS of diseases. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Causal Knowledge; Diabetic Nephropathy; Electronic Health Record; Knowledge Graph; Thyrotropin; Decision Support Systems; Diagnosis; Diseases; Knowledge Graph; Modeling Languages; Ontology; Records Management; Semantics; Causal Knowledge; Decision Supports; Diabetic Nephropathy; Diagnostic Decisions; Electronic Health; Electronic Health Record; Health Records; Knowledge Graphs; Real-world; Three Models; Quality Control; Thyrotropin; Article; Cardiovascular Disease; Chronic Kidney Failure; Chronic Respiratory Tract Disease; Clinical Evaluation; Controlled Study; Decision Support System; Diabetes Mellitus; Diabetic Nephropathy; Diagnostic Test Accuracy Study; Differential Diagnosis; Disease Exacerbation; Electronic Health Record; Glomerulopathy; Human; Insulin Dependent Diabetes Mellitus; Kidney Disease; Knowledge; Machine Learning; Mathematical Phenomena; Metabolic Acidosis; Natural Language Processing; Noise Reduction; Non Insulin Dependent Diabetes Mellitus; Receiver Operating Characteristic; Sentiment Analysis; Automated Pattern Recognition; Clinical Decision Support System; Language; Semantics; Decision Support Systems, Clinical; Diabetes Mellitus; Diabetic Nephropathies; Humans; Language; Pattern Recognition, Automated},
	keywords = {Decision support systems; Diagnosis; Diseases; Knowledge graph; Modeling languages; Ontology; Records management; Semantics; Causal knowledge; Decision supports; Diabetic nephropathy; Diagnostic decisions; Electronic health; Electronic health record; Health records; Knowledge graphs; Real-world; Three models; Quality control; thyrotropin; Article; cardiovascular disease; chronic kidney failure; chronic respiratory tract disease; clinical evaluation; controlled study; decision support system; diabetes mellitus; diabetic nephropathy; diagnostic test accuracy study; differential diagnosis; disease exacerbation; electronic health record; glomerulopathy; human; insulin dependent diabetes mellitus; kidney disease; knowledge; machine learning; mathematical phenomena; metabolic acidosis; natural language processing; noise reduction; non insulin dependent diabetes mellitus; receiver operating characteristic; sentiment analysis; automated pattern recognition; clinical decision support system; language; semantics; Decision Support Systems, Clinical; Diabetes Mellitus; Diabetic Nephropathies; Humans; Language; Pattern Recognition, Automated},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21}
}

@ARTICLE{Tran2023,
	author = {Tran, Chau and Khadkikar, Siddharth and Porollo, Aleksey A.},
	title = {Survey of Protein Sequence Embedding Models},
	year = {2023},
	journal = {International Journal of Molecular Sciences},
	volume = {24},
	number = {4},
	pages = {},
	doi = {10.3390/ijms24043775},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149030808&doi=10.3390%2Fijms24043775&partnerID=40&md5=3297b81920f586164150449f0cceb830},
	abstract = {Derived from the natural language processing (NLP) algorithms, protein language models enable the encoding of protein sequences, which are widely diverse in length and amino acid composition, in fixed-size numerical vectors (embeddings). We surveyed representative embedding models such as Esm, Esm1b, ProtT5, and SeqVec, along with their derivatives (GoPredSim and PLAST), to conduct the following tasks in computational biology: embedding the Saccharomyces cerevisiae proteome, gene ontology (GO) annotation of the uncharacterized proteins of this organism, relating variants of human proteins to disease status, correlating mutants of beta-lactamase TEM-1 from Escherichia coli with experimentally measured antimicrobial resistance, and analyzing diverse fungal mating factors. We discuss the advances and shortcomings, differences, and concordance of the models. Of note, all of the models revealed that the uncharacterized proteins in yeast tend to be less than 200 amino acids long, contain fewer aspartates and glutamates, and are enriched for cysteine. Less than half of these proteins can be annotated with GO terms with high confidence. The distribution of the cosine similarity scores of benign and pathogenic mutations to the reference human proteins shows a statistically significant difference. The differences in embeddings of the reference TEM-1 and mutants have low to no correlation with minimal inhibitory concentrations (MIC). © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Natural Language Processing; Protein Annotation; Protein Language Model; Protein Sequence Embedding; Survey Of Embedding Models; Amino Acid; Aspartic Acid; Protein; Amino Acids; Proteins; Amino Acid; Aspartic Acid; Beta Lactamase Tem 1; Glutamic Acid Derivative; Protein Variant; Virulence Factor; Protein; Amino Acid Composition; Amino Acid Sequence; Article; Bacterial Virulence; Bacterium Colony; Benchmarking; Bioinformatics; Controlled Study; Embedding; Escherichia Coli; Gene Ontology; Human; Minimum Inhibitory Concentration; Natural Language Processing; Nonhuman; Reproduction; Saccharomyces Cerevisiae; Algorithm; Biology; Chemistry; Genetics; Algorithms; Amino Acid Sequence; Amino Acids; Computational Biology; Humans; Proteins},
	keywords = {amino acid; aspartic acid; beta lactamase TEM 1; glutamic acid derivative; protein variant; virulence factor; protein; amino acid composition; amino acid sequence; Article; bacterial virulence; bacterium colony; benchmarking; bioinformatics; controlled study; embedding; Escherichia coli; gene ontology; human; minimum inhibitory concentration; natural language processing; nonhuman; reproduction; Saccharomyces cerevisiae; algorithm; biology; chemistry; genetics; Algorithms; Amino Acid Sequence; Amino Acids; Computational Biology; Humans; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Zhao20231140,
	author = {Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
	title = {Improving Protein Function Prediction by Adaptively Fusing Information From Protein Sequences and Biomedical Literature},
	year = {2023},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	volume = {27},
	number = {2},
	pages = {1140 - 1148},
	doi = {10.1109/JBHI.2022.3221988},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142828087&doi=10.1109%2FJBHI.2022.3221988&partnerID=40&md5=ec8195343d0849d89fd38e4dd432e354},
	abstract = {Proteins are the main undertakers of life activities, and accurately predicting their biological functions can help human better understand life mechanism and promote the development of themselves. With the rapid development of high-throughput technologies, an abundance of proteins are discovered. However, the gap between proteins and function annotations is still huge. To accelerate the process of protein function prediction, some computational methods taking advantage of multiple data have been proposed. Among these methods, the deep-learning-based methods are currently the most popular for their capability of learning information automatically from raw data. However, due to the diversity and scale difference between data, it is challenging for existing deep learning methods to capture related information from different data effectively. In this paper, we introduce a deep learning method that can adaptively learn information from protein sequences and biomedical literature, namely DeepAF. DeepAF first extracts the two kinds of information by using different extractors, which are built based on pre-trained language models and can capture rudimentary biological knowledge. Then, to integrate those information, it performs an adaptive fusion layer based on a Cross-attention mechanism that considers the knowledge of mutual interactions between two information. Finally, based on the mixed information, DeepAF utilizes logistic regression to obtain prediction scores. The experimental results on the datasets of two species (i.e., Human and Yeast) show that DeepAF outperforms other state-of-the-art approaches. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Cross-attention Mechanism; Deep Learning; Multiple Data; Pre-trained Language Models; Protein Function Prediction; Protein; Proteins; Biological Systems; Data Mining; Deep Learning; Forecasting; Http; Modeling Languages; Semantics; Amino-acids; Attention Mechanisms; Biological System Modeling; Cross-attention Mechanism; Language Model; Multiple Data; Pre-trained Language Model; Predictive Models; Protein Engineering; Protein Function Prediction; Proteins; Ablation Therapy; Accuracy; Adaptive Fusion Module; Amino Acid Sequence; Area Under The Curve; Area Under The Precision Recall Curve; Article; Biomedicine; Computer Model; Controlled Study; Deep Learning; Diagnostic Test Accuracy Study; F Max Numeric Value; Gene Ontology; High Throughput Analysis; Human; Human Versus Nonhuman Data; Logistic Regression Analysis; Molecular Genetics; Nonhuman; Protein Function; Receiver Operating Characteristic; Sequence Homology; Statistical Concepts; Yeast; Metabolism; Saccharomyces Cerevisiae; Protein; Amino Acid Sequence; Humans},
	keywords = {Biological systems; Data mining; Deep learning; Forecasting; HTTP; Modeling languages; Semantics; Amino-acids; Attention mechanisms; Biological system modeling; Cross-attention mechanism; Language model; Multiple data; Pre-trained language model; Predictive models; Protein engineering; Protein function prediction; Proteins; ablation therapy; accuracy; adaptive fusion module; amino acid sequence; area under the curve; area under the precision recall curve; Article; biomedicine; computer model; controlled study; deep learning; diagnostic test accuracy study; f max numeric value; gene ontology; high throughput analysis; human; human versus nonhuman data; logistic regression analysis; molecular genetics; nonhuman; protein function; receiver operating characteristic; sequence homology; statistical concepts; yeast; metabolism; Saccharomyces cerevisiae; protein; Amino Acid Sequence; Humans},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Chen202368,
	author = {Chen, Zhi and Liu, Yuncong and Chen, Lu and Zhu, Su and Wu, Mengyue and Yu, Kai},
	title = {OPAL: Ontology-Aware Pretrained Language Model for End-to-End Task-Oriented Dialogue},
	year = {2023},
	journal = {Transactions of the Association for Computational Linguistics},
	volume = {11},
	pages = {68 - 84},
	doi = {10.1162/tacl_a_00534},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146952463&doi=10.1162%2Ftacl_a_00534&partnerID=40&md5=425f3608d97b9bcab124185e0c15e74e},
	abstract = {This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: Dialogue state tracker (DST) and response generator (RG). The dialogue state consists of the domain-slot-value triples, which are regarded as the user’s constraints to search the domain-related databases. The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inacces-sible. It prevents the development of the pretrained language model for the task-oriented dialogue. We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases. The first phase is to pretrain on large-scale contextual text data, where the structured information of the text is extracted by the information extracting tool. To bridge the gap between the pretraining method and downstream tasks, we design two pretraining tasks: ontology-like triple recovery and next-text generation, which simulates the DST and RG, respectively. The second phase is to fine-tune the pretrained model on the TOD data. The experimental re-sults show that our proposed method achieves an exciting boost and obtains competitive performance even without any TOD data on CamRest676 and MultiWOZ benchmarks. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmarking; Bridges; Computational Linguistics; Dialogue Models; End-to-end Task; Language Model; Large-scales; Modeling Task; Ontology's; Pre-training; Simple++; Task-oriented; Task-specific Modules; Ontology},
	keywords = {Benchmarking; Bridges; Computational linguistics; Dialogue models; End-to-end task; Language model; Large-scales; Modeling task; Ontology's; Pre-training; Simple++; Task-oriented; Task-specific modules; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Gupta202395,
	author = {Gupta, Akshay and Kumar, Suresh Britto Ramesh and Sreenivasa Kumar, P.},
	title = {Solving age-word problems using domain ontology and BERT},
	year = {2023},
	journal = {ACM International Conference Proceeding Series},
	pages = {95 - 103},
	doi = {10.1145/3570991.3571058},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146150779&doi=10.1145%2F3570991.3571058&partnerID=40&md5=d0090c8744668678f84b3af4111ec860},
	abstract = {An age word problem (ageWP) typically involves sentences that express relationships between the age of the agents and asks for the age of one of them. Automatically solving ageWPs is a challenging task as we need to tackle temporal relationships between the agent's ages, frame and solve the equations for the required unknowns. To the best of our knowledge, there exists only one ageWP dataset consisting of just 124 examples. The dataset is too small to employ a learning-based solver, mainly consisting of ageWPs with simple temporal relationships. To address this issue, in our earlier work, we designed a description-logic based ontology (ageWP-ont) for the domain of age word problems and utilized it to automatically generate a large number of ageWPs. Sentences in these ageWPs relate the ages of agents in a temporally complex manner. In this paper, we focus on solving these problems. We analyzed an existing learning-based solver of algebraic word-problems that uses a traditional machine learning approach and found that the solver can be adapted to our domain. But we found that this approach does not seem to perform well, perhaps due to the complex nature of the ageWPs. As we have the ontology of the domain on hand, we propose a new approach of utilizing it in the deep-learning based NLU component of the solution. We annotate parts of the ageWP sentences with class-names from ageWP-ont and train a BERT-based language model (LM) that learns to predict the instances for these classes in the given sentences. An RDF graph is populated with these values and serves as a concrete problem-specific instance of the ontology. The dataset for training the LM is automatically generated with the help of ageWP-ont. Finally, for the actual solving of a given ageWP, we make use of its RDF graph and employ Semantic Web Rule Language (SWRL) rules. We implemented the proposed system and achieved 68.8% accuracy. The work demonstrates that combining deep learning with ontologies can give impressive results. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Age-word Problem Solver; Bert; Owl-dl Ontology; Swrl; Data Description; Deep Learning; Resource Description Framework (rdf); Age-word Problem Solv; Bert; Ontology's; Owl-dl Ontology; Problem Solvers; Rules Languages; Semantic Web Rule Language; Semantic Web Rules; Temporal Relationships; Word Problem; Ontology},
	keywords = {Data description; Deep learning; Resource Description Framework (RDF); Age-word problem solv; BERT; Ontology's; OWL-DL ontology; Problem solvers; Rules languages; Semantic web rule language; Semantic Web rules; Temporal relationships; Word problem; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Xiong2023713,
	author = {Xiong, Xiong and Wang, Chen and Liu, Yunfei and Li, Shengyang},
	title = {Enhancing Ontology Knowledge for Domain-Specific Joint Entity and Relation Extraction},
	year = {2023},
	volume = {1},
	pages = {713 - 725},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200747438&partnerID=40&md5=f97561a7a7169541367129a05ea6eb64},
	abstract = {Pre-trained language models (PLMs) have been widely used in entity and relation extraction methods in recent years. However, due to the semantic gap between general-domain text used for pre-training and domain-specific text, these methods encounter semantic redundancy and domain semantics insufficiency when it comes to domain-specific tasks. To mitigate this issue, we propose a low-cost and effective knowledge-enhanced method to facilitate domain-specific semantics modeling in joint entity and relation extraction. Precisely, we use ontology and entity type descriptions as domain knowledge sources, which are encoded and incorporated into the downstream entity and relation extraction model to improve its understanding of domain-specific information. We construct a dataset called SSUIE-RE for Chinese entity and relation extraction in space science and utilization domain of China Manned Space Engineering, which contains a wealth of domain-specific knowledge. The experimental results on SSUIE-RE demonstrate the effectiveness of our method, achieving a 1.4% absolute improvement in relation F1 score over previous best approach. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Ontology; Semantics; Space Research; Domain Semantics; Domain Specific; Entity Extractions; Extraction Method; Language Model; Ontology's; Pre-training; Relation Extraction; Semantic Gap; Specific Tasks; Domain Knowledge},
	keywords = {Computational linguistics; Ontology; Semantics; Space research; Domain semantics; Domain specific; Entity extractions; Extraction method; Language model; Ontology's; Pre-training; Relation extraction; Semantic gap; Specific tasks; Domain Knowledge},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Ponnusamy2023,
	author = {Ponnusamy, Sangeeth Saagar and Helle, Philipp and Zindel, Andreas and Richter, Stefan and Schramm, Gerrit and Strobel, Carsten},
	title = {IBIS : An Interactive Virtual Assistant for System Engineers},
	year = {2023},
	pages = {},
	doi = {10.2514/6.2023-1960},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200255872&doi=10.2514%2F6.2023-1960&partnerID=40&md5=e055f07b0762f79b93f6a22718a388ed},
	abstract = {One of the key challenges in adopting a MBSE approach in industry is how to intuitively exploit system engineering knowledge expressed through descriptive models by different stakeholders. This paper presents a virtual assistant named IBIS in assisting system engineers to perform such activities over SysML models and data in multimodal manner through text, speech or VR. Development of an underlying system engineering ontology, SysML model transformation, training a domain specific language model and query techniques are presented along with its implementation in the RASA framework based toolchain. Results of two different case studies are briefly discussed followed by lessons learnt, challenges and possible avenues of future work. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Engineers; Descriptive Model; Domains Specific Languages; Engineering Knowledge; Engineering Ontologies; Language Model; Model Transformation; Multi-modal; System Engineers; Underlying Systems; Virtual Assistants; Problem Oriented Languages},
	keywords = {Engineers; Descriptive Model; Domains specific languages; Engineering knowledge; Engineering ontologies; Language model; Model transformation; Multi-modal; System engineers; Underlying systems; Virtual assistants; Problem oriented languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Babkin20231,
	author = {Babkin, Eduard A. and Ulitin, Boris I.},
	title = {Ontology-Based Evolution of Domain-Oriented Languages: Models, Methods and Tools for User Interface Design in General-Purpose Software Systems},
	year = {2023},
	pages = {1 - 144},
	doi = {10.1007/978-3-031-42202-7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196473959&doi=10.1007%2F978-3-031-42202-7&partnerID=40&md5=6abbde1e1bfcab28eab6efdb3123cf93},
	abstract = {This book focuses on the study of ontology-based models and methods used for the implementation of the evolution of external domain-specific languages (DSL), which are mainly intended for modelling the structure of human-machine interfaces. The primary goal of the approach is to increase the efficiency of support processes during the life cycle of general-purpose software systems. The book is structured in seven chapters. Chapter 1 presents the objectives and significance of the research, as well as a summary of the contents of the work. Chapter 2 analyses the existing classical DSL design and implementation methodology for modelling human-machine interfaces in the context of the lifecycle of general-purpose software systems. Next, chapter 3 is devoted to an analysis of existing methods and formalisms used in describing the structure of a DSL for modelling human-machine interfaces of software systems. Subsequently, chapter 4 provides a detailed description of the proposed new projection-based approach for developing such DSLs. Chapter 5 then describes the software implementation of the human-machine interface evolution based on an example of an external DSL in two domains. Eventually, chapter 6 analyses the application of the proposed projection approach for more complex systems, namely, decision support systems based on heterogeneous information of decision makers. The concluding chapter 7 summarizes the main results of the research and suggests further development paths and practical applications. The book is written for researchers in model-driven software development in general and in domain-specific language engineering in particular. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain-specific Languages; Formal Ontologies; Model Evolution; Model-driven Software Development; Model-to-model Transformations; User Interface Design; Application Programs; Artificial Intelligence; Decision Making; Engineering Research; Formal Methods; Human Computer Interaction; Large Scale Systems; Life Cycle; Man Machine Systems; Modeling Languages; Ontology; Problem Oriented Languages; Software Design; User Interfaces; Domains Specific Languages; Formal Ontology; General Purpose Software; Human Machine Interface; Model Evolution; Model To Model Transformation; Model-driven Software Development; Ontology-based; Software-systems; User Interface Designs; Decision Support Systems},
	keywords = {Application programs; Artificial intelligence; Decision making; Engineering research; Formal methods; Human computer interaction; Large scale systems; Life cycle; Man machine systems; Modeling languages; Ontology; Problem oriented languages; Software design; User interfaces; Domains specific languages; Formal ontology; General purpose software; Human Machine Interface; Model evolution; Model to model transformation; Model-Driven Software Development; Ontology-based; Software-systems; User interface designs; Decision support systems},
	type = {Book},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kancheva2023167,
	author = {Kancheva, Zara},
	title = {Incorporating Prepositions in the BulTreeBank WordNet},
	year = {2023},
	pages = {167 - 171},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195598066&partnerID=40&md5=377c32f2c470cb614655af5e38efc349},
	abstract = {A model for preposition incorporation in the BulTreeBank WordNet is presented which follows the model for presenting open class words in wordnets. An adapted semantic classification of prepositions is done on the base of Bulgarian grammars and the classes are used for synset categories. The good coverage of prepositions in the wordnet will be used for the aim of neural language models creation for Bulgarian. This extension of the wordnet improves its utility for semantic annotation. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology; Language Model; Model Creation; Semantic Annotations; Semantic Classification; Synsets; Wordnet; Semantics},
	keywords = {Ontology; Language model; Model creation; Semantic annotations; Semantic classification; Synsets; Wordnet; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Janz202360,
	author = {Janz, Arkadiusz and Maziarz, Marek},
	title = {WordNet-based Data Augmentation for Hybrid WSD Models},
	year = {2023},
	pages = {60 - 66},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195566716&partnerID=40&md5=cb9818cc303f4fc0a292b93697214e63},
	abstract = {Recent advances in Word Sense Disambiguation suggest neural language models can be successfully improved by incorporating knowledge base structure. Such class of models are called hybrid solutions. We propose a method of improving hybrid WSD models by harnessing data augmentation techniques and bilingual training. The data augmentation consist of structure augmentation using interlingual connections between wordnets and text data augmentation based on multilingual glosses and usage examples. We utilise language-agnostic neural model trained both with SemCor and Princeton WordNet gloss and example corpora, as well as with Polish WordNet glosses and usage examples. This augmentation technique proves to make well-known hybrid WSD architecture to be competitive, when compared to current State-of-the-Art models, even more complex. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Natural Language Processing Systems; Ontology; Augmentation Techniques; Base Structure; Bilinguals; Data Augmentation; Hybrid Solution; Language Model; Neural Modelling; Text Data; Word Sense Disambiguation; Wordnet; Knowledge Based Systems},
	keywords = {Natural language processing systems; Ontology; Augmentation techniques; Base structure; Bilinguals; Data augmentation; Hybrid solution; Language model; Neural modelling; Text data; Word Sense Disambiguation; Wordnet; Knowledge based systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Maziarz2023228,
	author = {Maziarz, Marek and Grabowski, Łukasz and Piotrowski, Tadeusz and Rudnicka, Ewa and Piasecki, Maciej},
	title = {Lexicalised and Non-lexicalized Multi-word Expressions in WordNet: a Cross-encoder Approach},
	year = {2023},
	pages = {228 - 234},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195565565&partnerID=40&md5=24f4fbe3c415340f377dd48c8a938d3d},
	abstract = {Focusing on recognition of multi-word expressions (MWEs), we address the problem of recording MWEs in WordNet. In fact, not all MWEs recorded in that lexical database could with no doubt be considered as lexicalised (e.g. elements of wordnet taxonomy, quantifier phrases, certain collocations). In this paper, we use a cross-encoder approach to improve our earlier method of distinguishing between lexicalised and non-lexicalised MWEs found in WordNet using custom-designed rule-based and statistical approaches. We achieve F1-measure for the class of lexicalised word combinations close to 80%, easily beating two baselines (random and a majority class one). Language model also proves to be better than a feature-based logistic regression model. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology; Feature-based; Language Model; Lexical Database; Logistic Regression Modeling; Multi-word Expressions; Rule-based Approach; Statistical Approach; Word Combinations; Wordnet; Signal Encoding},
	keywords = {Ontology; Feature-based; Language model; Lexical database; Logistic Regression modeling; Multi-word expressions; Rule-based approach; Statistical approach; Word combinations; Wordnet; Signal encoding},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Gonçalo Oliveira,202341,
	author = {Gonçalo Oliveira,, Hugo},
	title = {On the Acquisition of WordNet Relations in Portuguese from Pretrained Masked Language Models},
	year = {2023},
	pages = {41 - 49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195555934&partnerID=40&md5=d5237772e2122086d29528950f075a27},
	abstract = {This paper studies the application of pretrained BERT in the acquisition of synonyms, antonyms, hypernyms and hyponyms in Portuguese. Masked patterns indicating those relations were compiled with the help of a service for validating semantic relations, and then used for prompting three pretrained BERT models, one multilingual and two for Portuguese (base and large). Predictions for the masks were evaluated in two different test sets. Results achieved by the monolingual models are interesting enough for considering these models as a source for enriching wordnets, especially when predicting hypernyms of nouns. Previously reported performances on prediction were improved with new patterns and with the large model. When it comes to selecting the related word from a set of four options, performance is even better, but not enough for outperforming the selection of the most similar word, as computed with static word embeddings. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontology; Semantics; Embeddings; Hypernyms; Hyponyms; Language Model; Large Models; Performance; Related Word; Semantic Relations; Test Sets; Wordnet; Forecasting},
	keywords = {Ontology; Semantics; Embeddings; Hypernyms; Hyponyms; Language model; Large models; Performance; Related word; Semantic relations; Test sets; Wordnet; Forecasting},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Tufa202314,
	author = {Tufa, Wondimagegnhue Tsegaye and Beinborn, Lisa and Vossen, Piek},
	title = {A WordNet View on Crosslingual Contextualized Language Models},
	year = {2023},
	pages = {14 - 24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195551832&partnerID=40&md5=a00b00d198b265ba30cf5ecb3c700bc0},
	abstract = {WordNet is a database that represents relations between words and concepts as an abstraction of the contexts in which words are used. Contextualized language models represent words in contexts but leave the underlying concepts implicit. In this paper, we investigate how different layers of a pre-trained language model shape the abstract lexical relationship toward the actual contextual concept. Can we define the amount of contextualized concept forming needed given the abstracted representation of a word? Specifically, we consider samples of words with different polysemy profiles shared across three languages, assuming that words with a different polysemy profile require a different degree of concept shaping by context. We conduct probing experiments to investigate the impact of prior polysemy profiles on the representation in different layers. We analyze how contextualized models can approximate meaning through context and examine cross-lingual interference effects. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Ontology; Cross-lingual; Different Layers; In Contexts; Interference Effects; Language Model; Wordnet; Abstracting},
	keywords = {Computational linguistics; Ontology; Cross-lingual; Different layers; In contexts; Interference effects; Language model; Wordnet; Abstracting},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Mateiu2023226,
	author = {Mateiu, Patricia and Groza, Adrian Petru},
	title = {Ontology engineering with Large Language Models},
	year = {2023},
	pages = {226 - 229},
	doi = {10.1109/SYNASC61333.2023.00038},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193849216&doi=10.1109%2FSYNASC61333.2023.00038&partnerID=40&md5=82842416ae615f60e5776b5c7fd1ef98},
	abstract = {We tackle the task of enriching ontologies by automatically translating natural language (NL) into Description Logic (DL). Since Large Language Models (LLMs) are the best tools for translations, we fine-tuned a GPT-3 model to convert NL into OWL Functional Syntax. For fine-tuning, we designed pairs of sentences in NL and the corresponding translations. This training pairs cover various aspects from ontology engineering: instances, class subsumption, domain and range of relations, object properties relationships, disjoint classes, complements, or cardinality restrictions. The resulted axioms are used to enrich an ontology, in a human supervised manner. The developed tool is publicly provided as a Protégé plugin. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Fine-tuning; Large Language Models; Ontology Engineering; Protege Plugin; Computational Linguistics; Data Description; Translation (languages); Description Logic; Fine Tuning; Language Model; Large Language Model; Natural Languages; Ontology Engineering; Ontology's; Plug-ins; Protege; Protege Plugin; Ontology},
	keywords = {Computational linguistics; Data description; Translation (languages); Description logic; Fine tuning; Language model; Large language model; Natural languages; Ontology engineering; Ontology's; Plug-ins; Protege; Protege plugin; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Chari2023,
	author = {Chari, Shruthi},
	title = {An Ontology-Enabled Approach For User-Centered and Knowledge-Enabled Explanations of AI Systems},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3678},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192738787&partnerID=40&md5=482d059e9c4afaa0f9c82f4cf40c2ab1},
	abstract = {Explainable Artificial Intelligence (AI) focuses on helping humans understand the working of AI systems or their decisions and has been a cornerstone of AI for decades. Recent research in explainability has focused on explaining the workings of AI models or model explainability. There have also been several position statements and review papers detailing the needs of end-users for user-centered explainability but fewer implementations. Hence, this thesis seeks to bridge some gaps between model and user-centered explainability. We create an explanation ontology (EO) to represent literature-derived explanation types via their supporting components. We implement a knowledge-augmented question-answering (QA) pipeline to support contextual explanations in a clinical setting. Finally, we are implementing a system to combine explanations from different AI methods and data modalities. Within the EO, we can represent fifteen different explanation types, and we have tested these representations in six exemplar use cases. We find that knowledge augmentations improve the performance of base large language models in the contextualized QA, and the performance is variable across disease groups. In the same setting, clinicians also indicated that they prefer to see actionability as one of the main foci in explanations. In our explanations combination method, we plan to use similarity metrics to determine the similarity of explanations in a chronic disease detection setting. Overall, through this thesis, we design methods that can support knowledge-enabled explanations across different use cases, accounting for the methods in today's AI era that can generate the supporting components of these explanations and domain knowledge sources that can enhance them. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Explainable Ai; Knowledge-enabled Explanations; User-centered Explanations; Domain Knowledge; Natural Language Processing Systems; User Interfaces; Artificial Intelligence Systems; Explainable Artificial Intelligence; Knowledge-enabled Explanation; Ontology's; Performance; Question Answering; Recent Researches; User Knowledge; User-centered Explanation; User-centred; Ontology},
	keywords = {Domain Knowledge; Natural language processing systems; User interfaces; Artificial intelligence systems; Explainable artificial intelligence; Knowledge-enabled explanation; Ontology's; Performance; Question Answering; Recent researches; User knowledge; User-centered explanation; User-centred; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{de Oliveira Silveira2023,
	author = {de Oliveira Silveira, Alysson and Ray, Arindam and Ebrahimi, Mohammadreza and Bhattacherjee, Anol},
	title = {Automated Deductive Content Analysis of Text: A Deep Contrastive and Active Learning Based Approach},
	year = {2023},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192549295&partnerID=40&md5=376592d022243c2cfa96a101ca52efb4},
	abstract = {Content analysis traditionally involves human coders manually combing through text documents to search for relevant concepts and categories. However, this approach is time-intensive and not scalable, particularly for secondary data like social media content, news articles, or corporate reports. To address this problem, the paper presents an automated framework called Automated Deductive Content Analysis of Text (ADCAT) that uses deep learning-based semantic techniques, an ontology of validated construct measures, a large language model, human-in-the-loop disambiguation, and a novel augmentation-based weighted contrastive learning approach for improved language representations, to build a scalable approach for deductive content analysis. We demonstrate the effectiveness of the proposed approach to identify firm innovation strategies from their 10-K reports to obtain inferences reasonably close to human coding. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Content Analysis; Contrastive Learning; Deep Active Learning; Large Language Model; Sentence Transformers; Text Augmentation; Computational Linguistics; Deep Learning; Information Systems; Information Use; Learning Systems; Semantics; Active Learning; Content Analysis; Contrastive Learning; Deep Active Learning; Language Model; Large Language Model; Learning-based Approach; Sentence Transformer; Text Augmentation; Text Document; Automation},
	keywords = {Computational linguistics; Deep learning; Information systems; Information use; Learning systems; Semantics; Active Learning; Content analysis; Contrastive learning; Deep active learning; Language model; Large language model; Learning-based approach; Sentence transformer; Text augmentation; Text document; Automation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Trepczyñski202319,
	author = {Trepczyñski, Marcin},
	title = {RELIGION, THEOLOGY, AND PHILOSOPHICAL SKILLS OF LLM–POWERED CHATBOTS; RELIGIJA, TEOLOGIJA I FILOZOFSKE VJEŠTINE AUTOMATIZIRANIH PROGRAMA ZA ČAVRLJANJE (CHATBOTOVA) POGONJENIMA VELIKIM JEZIČNIM MODELIMA (LLM)},
	year = {2023},
	journal = {Disputatio Philosophica},
	volume = {25},
	number = {1},
	pages = {19 - 36},
	doi = {10.32701/dp.25.1.2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192366757&doi=10.32701%2Fdp.25.1.2&partnerID=40&md5=e7b8fd5bbff8016da64c3fc9f9a0e4bc},
	abstract = {In this study, I demonstrate how religion and theology can be useful for testing the performance of LLMs or LLM–powered chatbots, focusing on the measurement of philosophical skills. I present the results of testing four selected chatbots: ChatGPT, Bing, Bard, and Llama2. I utilize three examples of possible sources of inspiration from religion or theology: 1) the theory of the four senses of Scripture; 2) abstract theological statements; 3) an abstract logic formula derived from a religious text, to show that these sources are good materials for tasks that can effectively measure philosophical skills such as interpretation of a given fragment, creative deductive reasoning, and identification of ontological limitations. This approach enabled sensitive testing, revealing differences among the performances of the four chatbots. I also provide an example showing how we can create a benchmark to rate and compare such skills, using the assessment criteria and simplified scales to rate each chatbot with respect to each criterion. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatbots; Interpretation; Large Language Models; Philosophical Skills; Reasoning; Religion; Testing; Theology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access}
}

@ARTICLE{Lussier2023V1,
	author = {Lussier, Yves A.},
	title = {Enhancing Precision Medicine and Wellness with Computing and AI across Clinical, Imaging, Environmental, Multi-Omics, Wearable Sensors, and Socio-Cognitive Data},
	year = {2023},
	volume = {1-2},
	pages = {V1 - V1-8},
	doi = {10.1016/B978-0-12-824010-6.00082-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191915641&doi=10.1016%2FB978-0-12-824010-6.00082-4&partnerID=40&md5=4d62c292e1fc316284f2e0623cc64ee6},
	abstract = {The convergence of computational capabilities and data-driven methodologies, spurred by the onset of the Third Industrial Revolution (Digital Age) as well as their integration into robotic tools and medical implants, often referred to as the Fourth Industrial Revolution (Park, 2016), is instigating a substantive transformation in clinical decision support that is rapidly changing the economics and practice of healthcare (Sutton et al., 2020). Here we introduce the strengths, challenges, and future trajectories of computational medicine, informatics, and machine learning (ML) methods as applied in the realm of precision healthcare and wellness (Lee et al., 2018). Precision medicine and precision wellness, situated at the intersection of technological advancements in clinical decision-making methods that impact the use, reuse, transformations, and analysis of data ranging from the nanoscale to clinical and societal dimensions of measurements, is the focal point of our discussion. We approach this subject through a dual perspective: one being a clinical data-focused approach that incorporates biomedical informatics with syntactic and semantic methods of interoperability (Garde et al., 2007; Strasberg et al., 2021), as well as human-interpretable decision algorithm and the other being a bottom-up approach rooted in genomics, biophysical and multiscale methods that increasingly employ robust yet human-opaque ML analytics (Lussier and Li, 2012). We conclude with an exploration of the remaining challenges, prospective opportunities, and future directions that arise at the confluence of these multifaceted methodologies inclusive of artificial intelligence and large language models in medicine (Shehab et al., 2022). © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence And Medicine; Biomedical Informatics; Biomedical Ontologies And Terminologies; Biomedical Semantic Interoperability; Biomedical Syntactic Interoperability; Clinical Data Science; Clinical Decision Support; Clinical Explainability And Interpretability; Clinical Reasoning With Certainty And Uncertainty; Electronic Health Record; Genomic Medicine; Ml And Medicine; Multi-omics Medicine; N-of-1 Trials; Precision Diagnostic; Precision Medicine; Precision Therapeutic; Precision Wellness; Predictive Clinical Analytics; Single-subject Studies},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2023,
	title = {2nd Workshop on Pattern-Based Approaches to NLP in the Age of Deep Learning, Pan-DL 2023 - Proceedings of the Workshop},
	year = {2023},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187268237&partnerID=40&md5=dd246a3e33ea51f8d82587c640f1fd8a},
	abstract = {The proceedings contain 10 papers. The topics discussed include: nearest neighbor search over vectorized lexico-syntactic patterns for relation extraction from financial documents; LEAF: linguistically enhanced event temporal relation framework; a graph-guided reasoning approach for open-ended commonsense question answering; generating Irish text with a flexible plug-and-play architecture; findings talk - a lightweight method to generate unanswerable questions in English; symbolic planning and code generation for grounded dialogue; towards zero-shot frame semantic parsing with task agnostic ontologies and simple labels; co-evolving data-driven and NLU-driven synthesizers for generating code in domain growth and data scarcity; and complementary roles of inference and language models in QA. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Nanwani2023507,
	author = {Nanwani, Laksh and Agarwal, Anmol and Jain, Kanishk and Prabhakar, Raghav and Monis, Aaron and Mathur, Aditya and Jatavallabhula, Krishna Murthy and Abdul Hafez, A. H. and Gandhi, Vineet and Krishna, K. Madhava},
	title = {Instance-Level Semantic Maps for Vision Language Navigation},
	year = {2023},
	journal = {IEEE International Workshop on Robot and Human Communication, RO-MAN},
	pages = {507 - 512},
	doi = {10.1109/RO-MAN57019.2023.10309534},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187015327&doi=10.1109%2FRO-MAN57019.2023.10309534&partnerID=40&md5=7fc3a96f9a17fdcf211b87ad0e648a08},
	abstract = {Humans have a natural ability to perform semantic associations with the surrounding objects in the environment. This allows them to create a mental map of the environment, allowing them to navigate on-demand when given linguistic instructions. A natural goal in Vision Language Navigation (VLN) research is to impart autonomous agents with similar capabilities. Recent works take a step towards this goal by creating a semantic spatial map representation of the environment without any labeled data. However, their representations are limited for practical applicability as they do not distinguish between different instances of the same object. In this work, we address this limitation by integrating instance-level information into spatial map representation using a community detection algorithm and utilizing word ontology learned by large language models (LLMs) to perform open-set semantic associations in the mapping representation. The resulting map representation improves the navigation performance by two-fold (233%) on realistic language commands with instance-specific descriptions compared to the baseline. We validate the practicality and effectiveness of our approach through extensive qualitative and quantitative experiments. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Autonomous Agents; Semantics; Community Detection Algorithms; Labeled Data; Language Model; Map Representations; Mental Maps; On Demands; Ontology's; Semantic Associations; Semantic Map; Spatial Maps; Navigation},
	keywords = {Autonomous agents; Semantics; Community detection algorithms; Labeled data; Language model; Map representations; Mental maps; On demands; Ontology's; Semantic associations; Semantic map; Spatial maps; Navigation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Wilcock20231396,
	author = {Wilcock, Graham and Jokinen, Kristiina},
	title = {To Err Is Robotic; to Earn Trust, Divine: Comparing ChatGPT and Knowledge Graphs for HRI},
	year = {2023},
	journal = {IEEE International Workshop on Robot and Human Communication, RO-MAN},
	pages = {1396 - 1401},
	doi = {10.1109/RO-MAN57019.2023.10309510},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186995135&doi=10.1109%2FRO-MAN57019.2023.10309510&partnerID=40&md5=8870798640e40252a6abbd670835e31e},
	abstract = {The paper discusses two current approaches to conversational AI, using large language models and knowledge graphs, and compares types of errors that occur in human-robot interactions based on these approaches. It provides example dialogues and describes solutions to several error types including false implications, ontological errors, theory of mind errors, and handling of speech recognition errors. The paper addresses issues of particular concern for earning user trust. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Human Robot Interaction; Knowledge Graph; Speech Recognition; 'current; Error Theory; Error Types; Humans-robot Interactions; Knowledge Graphs; Language Model; Recognition Error; Theory Of Minds; Errors},
	keywords = {Human robot interaction; Knowledge graph; Speech recognition; 'current; Error theory; Error types; Humans-robot interactions; Knowledge graphs; Language model; Recognition error; Theory of minds; Errors},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@CONFERENCE{Tang20233893,
	author = {Tang, Yun and da Costa, Antonio Anastasio Bruto and Zhang, Xizhe and Irvine, Patrick and Khastgir, Siddartha and Jennings, Paul A.},
	title = {Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain},
	year = {2023},
	pages = {3893 - 3900},
	doi = {10.1109/ITSC57777.2023.10422308},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186526218&doi=10.1109%2FITSC57777.2023.10422308&partnerID=40&md5=eb8968513a3c5a2be5a84e95c070417e},
	abstract = {Engineering knowledge-based (or expert) systems require extensive manual effort and domain knowledge. As Large Language Models (LLMs) are trained using an enormous amount of cross-domain knowledge, it becomes possible to automate such engineering processes. This paper presents an empirical automation and semi-automation framework for domain knowledge distillation using prompt engineering and the LLM ChatGPT. We assess the framework empirically in the autonomous driving domain and present our key observations. In our implementation, we construct the domain knowledge ontology by 'chatting' with ChatGPT. The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. We, therefore, also develop a web-based distillation assistant enabling supervision and flexible intervention at runtime. We hope our findings and tools could inspire future research toward revolutionizing the engineering of knowledge-based systems across application domains. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Autonomous Driving; Domain Ontology Distillation; Large Language Model; Autonomous Vehicles; Computational Linguistics; Distillation; Ontology; Autonomous Driving; Cross-domain; Domain Knowledge; Domain Ontologies; Domain Ontology Distillation; Empirical Studies; Engineering Knowledge; Knowledge Experts; Language Model; Large Language Model; Domain Knowledge},
	keywords = {Autonomous vehicles; Computational linguistics; Distillation; Ontology; Autonomous driving; Cross-domain; Domain knowledge; Domain ontologies; Domain ontology distillation; Empirical studies; Engineering knowledge; Knowledge experts; Language model; Large language model; Domain Knowledge},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Reza2023,
	author = {Reza, Umayer and Hahmann, Torsten},
	title = {Populating and Refining an Ontology of Cellulose Materials with Terms from Scientific Publications: Extended Abstract},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3637},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185198621&partnerID=40&md5=4d34c677ca310493ac3dad77d6efc6a1},
	abstract = {Cellulose is a highly versatile biopolymer with numerous applications, such as paper and paperboard production, textiles, packaging, biofuels, and biomedical applications. Though, the scattered nature of cellulose knowledge with ambiguous terms and datasets presents significant obstacles to its optimal utilization. This project seeks to address these challenges by systematically accumulating scattered knowledge about cellulose, enabling it to be modifiable, extensible, and reusable. The objective of the project is to develop an automated system to extract relevant cellulosic terms from scientific publications which will show an improved performance in named entity classification by taking additional context and disambiguous information from an existing cellulose ontology. An incremental training process will be utilized to train a ScispaCy language model, which is specifically designed for analyzing scientific, clinical, and biomedical texts, in order to accomplish this task. The system will also generate new terms for the ontology by taking the existing ontology into account. Therefore, the proposed system will facilitate the extension of the ontology, while simultaneously benefiting from the ontology to enhance performance in named entity classification. By meeting these objectives, the project aims to contribute to the development of a sustainable bioproduct-based society by providing a resource of state-of-the-art knowledge in cellulose materials that can facilitate material science research. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Cellulose Ontology; Knowledge Graph; Named Entity Recognition; Scientific Publication; Abstracting; Automation; Biopolymers; Classification (of Information); Knowledge Graph; Medical Applications; Ontology; Reusability; Cellulose Materials; Cellulose Ontology; Extended Abstracts; Knowledge Graphs; Named Entity Classification; Named Entity Recognition; Ontology's; Paper And Paperboard; Performance; Scientific Publications; Cellulose; Classification; Documents; Materials; Packaging; Systems},
	keywords = {Abstracting; Automation; Biopolymers; Classification (of information); Knowledge graph; Medical applications; Ontology; Reusability; Cellulose materials; Cellulose ontology; Extended abstracts; Knowledge graphs; Named entity classification; Named entity recognition; Ontology's; Paper and paperboard; Performance; Scientific publications; Cellulose; Classification; Documents; Materials; Packaging; Systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Kosten20235272,
	author = {Kosten, Catherine and Cudre-Mauroux, Philippe and Stockinger, Kurt},
	title = {Spider4SPARQL: A Complex Benchmark for Evaluating Knowledge Graph Question Answering Systems},
	year = {2023},
	pages = {5272 - 5281},
	doi = {10.1109/BigData59044.2023.10386182},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184975243&doi=10.1109%2FBigData59044.2023.10386182&partnerID=40&md5=e9f9603deb6910323726ccf90e5ce308},
	abstract = {With the recent spike in the number and availability of Large Language Models (LLMs), it has become increasingly important to provide large and realistic benchmarks for evaluating Knowledge Graph Question Answering (KGQA) systems. So far the majority of benchmarks rely on pattern-based SPARQL query generation approaches. The subsequent natural language (NL) question generation is conducted through crowdsourcing or other automated methods, such as rule-based paraphrasing or NL question templates. Although some of these datasets are of considerable size, their pitfall lies in their pattern-based generation approaches, which do not always generalize well to the vague and linguistically diverse questions asked by humans in real-world contexts. In this paper, we introduce Spider4SPARQL -a new SPARQL benchmark dataset featuring 9,693 previously existing manually generated NL questions and 4,721 unique, novel, and complex SPARQL queries of varying complexity. In addition to the NL/SPARQL pairs, we also provide their corresponding 166 knowledge graphs and ontologies, which cover 138 different domains. Our complex benchmark enables novel ways of evaluating the strengths and weaknesses of modern KGQA systems. We evaluate the system with state-of-the-art KGQA systems as well as LLMs, which achieve only up to 45% execution accuracy, demonstrating that Spider4SPARQL is a challenging benchmark for future research. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmark For Question Answering Over Knowledge Graphs; Language Models; Performance Evaluation; Benchmarking; Computational Linguistics; Natural Language Processing Systems; Automated Methods; Benchmark For Question Answering Over Knowledge Graph; Complex Benchmark; Knowledge Graphs; Language Model; Natural Language Questions; Performances Evaluation; Query Generation; Question Answering; Question Answering Systems; Knowledge Graph},
	keywords = {Benchmarking; Computational linguistics; Natural language processing systems; Automated methods; Benchmark for question answering over knowledge graph; Complex benchmark; Knowledge graphs; Language model; Natural language questions; Performances evaluation; Query generation; Question Answering; Question answering systems; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Guo2023522,
	author = {Guo, Kuo and Li, Yifan and Chen, Hao and Shen, Hongbin and Yang, Yang},
	title = {Isoform Function Prediction Based on Heterogeneous Graph Attention Networks},
	year = {2023},
	pages = {522 - 527},
	doi = {10.1109/BIBM58861.2023.10386048},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184931633&doi=10.1109%2FBIBM58861.2023.10386048&partnerID=40&md5=fecc6275159961520398218e9f2701bb},
	abstract = {Isoforms refer to different mRNA molecules transcribed from the same gene, which can be translated into proteins with varying structures and functions. Predicting the functions of isoforms is an essential topic in bioinformatics as it can provide valuable insights into the intricate mechanisms of gene regulation and biological processes. Conventionally, gene function labels are standardized in Gene Ontology (GO) terms. However, traditional methods for predicting isoform function are largely limited by the absence of isoform-specific labels, sparse annotations, and the vast number of GO terms. To address these issues, we propose HANIso, a deep learning-based method for isoform function prediction. HANIso leverages a pretrained protein language model to extract features from protein sequences. It also integrates heterogeneous information, such as isoform sequence features, GO annotations, and isoform interaction data, using a Heterogeneous Graph Attention Network (HAN). This allows the model to learn the importance of different sources of information and their semantic relationships through the attention mechanism. Our method can predict function labels at both the gene level and isoform level. We conduct experiments on two species datasets, and the results demonstrate that our method outperforms existing methods on both AUROC and AUPRC. HANIso has the potential to overcome the limitations of traditional methods and provide a more accurate and comprehensive understanding of isoform function. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Alternative Splicing; Gene Ontology; Heterogeneous Graph Attention Network; Isoform Function Prediction; Protein Language Model; Bioinformatics; Computational Linguistics; Deep Learning; Forecasting; Genes; Proteins; Semantics; Alternative Splicing; Function Prediction; Gene Ontology; Gene Ontology Terms; Heterogeneous Graph; Heterogeneous Graph Attention Network; Isoform Function Prediction; Isoforms; Language Model; Protein Language Model; Gene Ontology},
	keywords = {Bioinformatics; Computational linguistics; Deep learning; Forecasting; Genes; Proteins; Semantics; Alternative splicing; Function prediction; Gene ontology; Gene ontology terms; Heterogeneous graph; Heterogeneous graph attention network; Isoform function prediction; Isoforms; Language model; Protein language model; Gene Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Yáñez-Romero2023104,
	author = {Yáñez-Romero, Fabio Antonio},
	title = {Entity Modelling through Ontologies and Large Language Models},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3625},
	pages = {104 - 110},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184831862&partnerID=40&md5=e7bfd8a5401e242aa037a5e8fd7e0a4e},
	abstract = {The aim of this paper is to present a line of research focused on improving the knowledge represented in natural language processing tasks through the use of ontologies, combining these with machine learning techniques. It is expected that with this kind of techniques it will be possible to fight against phenomena such as the hallucination present in current generative language models and to reach the state of the art in different tasks taking into account semantic knowledge. Initially, we will try to solve the problem of semantics in specific areas such as medicine, where the external knowledge that can be incorporated would help to provide knowledge that does not exist in unstructured data such as all ICD-10 codes. Therefore, we expect obtain enough conclusions to apply this methodology with other dominions. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Embeddings; Generative Language Models; Graph Neural Networks; Knowledge Bases; Knowledge Graphs; Natural Language Processing; Ontologies; Semantics; Computational Linguistics; Embeddings; Graph Neural Networks; Knowledge Graph; Modeling Languages; Natural Language Processing Systems; Ontology; Generative Language Model; Knowledge Base; Knowledge Graphs; Language Model; Language Processing; Natural Language Processing; Natural Languages; Ontology's; Semantics},
	keywords = {Computational linguistics; Embeddings; Graph neural networks; Knowledge graph; Modeling languages; Natural language processing systems; Ontology; Generative language model; Knowledge base; Knowledge graphs; Language model; Language processing; Natural language processing; Natural languages; Ontology's; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Procko2023147,
	author = {Procko, Tyler Thomas and Elvira, Timothy and Ochoa, Omar},
	title = {GPT-4: A Stochastic Parrot or Ontological Craftsman? Discovering Implicit Knowledge Structures in Large Language Models},
	year = {2023},
	pages = {147 - 154},
	doi = {10.1109/TransAI60598.2023.00043},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184808375&doi=10.1109%2FTransAI60598.2023.00043&partnerID=40&md5=1508a82f1a0c8b3376895ff708ffdb11},
	abstract = {Ontologies are representational artifacts that purport to accurately portray the aspect of reality under the purview of the ontologists laboring upon them. Ontologies exist in a spectrum of formality, from lexical thesauri to knowledge graphs, to collections of statements of first-order logic. The recent proliferation of Large Language Models (LLMs) has brought to bear interactive 'knowledge bases' with general awareness of most things. As ontologists create ontologies from their understanding of reality; and as LLMs, presumably, possess some 'understanding' of reality, embedded in their vector matrices corresponding to lexical terms from massive quantities of learned texts, a question is posed: what form of ontology can an LLM create when prompted about some novel facet of reality, without explicitly asking it for an ontology? I.e., will an LLM categorize things into bins, or a subsumption hierarchy, or perhaps something else? LLMs, as they are understood, respond when prompted with the most likely response, because they are predictors of next tokens, i.e., they are stochastic parrots. In any case, it is posited that, if prompted without any explicit request for an ontology, an LLM can produce an ontology of novel form, effectively granting insight into the 'understanding' an LLM has of the world, as all humans possess an understanding of the world that ontologies are based upon. This paper explores the use of the flagship LLM, GPT-4, in forming an ontology of a novel domain. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Gpt; Large Language Models; Ontology; Taxonomy; Birds; Computational Linguistics; Formal Logic; Stochastic Models; Stochastic Systems; First Order Logic; Gpt; Implicit Knowledge; Knowledge Graphs; Knowledge Structures; Language Model; Large Language Model; Ontology's; Spectra's; Stochastics; Ontology},
	keywords = {Birds; Computational linguistics; Formal logic; Stochastic models; Stochastic systems; First order logic; GPT; Implicit knowledge; Knowledge graphs; Knowledge structures; Language model; Large language model; Ontology's; Spectra's; Stochastics; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Taye2023,
	author = {Taye, Mohammad Mustafa and Abulail, Rawan Nassri and Al-Oudat, Mohammad Atallah},
	title = {An Ontology Learning Framework for unstructured Arabic Text},
	year = {2023},
	pages = {},
	doi = {10.1109/ISAS60782.2023.10391548},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184808063&doi=10.1109%2FISAS60782.2023.10391548&partnerID=40&md5=383c7dfc97308ff44961706ab31047f2},
	abstract = {Ontologies are widely regarded as valuable sources of semantics and interoperability in all artificially intelligent systems. Due to the rapid growth of unstructured data on the web, studying how to automatically get ontology from unstructured text is important. Therefore, ontology learning (OL) is an important process in the business world. It involves finding and extracting concepts from the text so that these concepts can be used for things such as information retrieval. Unfortunately, learning ontology is not easy for some reasons, and there has not been much research on how to automatically learn a domain-specific ontology from data.Ontology Studying Arabic text is not as developed as learning Latin text. There is almost no automated support for using Arabic literary knowledge in semantically enabled systems. Machine learning (ML) has proven beneficial in numerous fields, including text mining. By employing neural language models such as AraBERT, it is possible to obtain word embeddings as distributed word representations from textual input using machine learning. However, the application of machine learning to aid the development of Arabic ontology is largely unexplored. This research examines the performance of AraBERT for ontology learning tasks in Arabic. Early performance results as an application of Arabic ontology learning are promising. In this research, we provide a method for populating an existing ontology with instance information extracted from the input natural language text. This prototype has achieved an information extraction accuracy of 91%. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Arabic Ontology; Natural Language Processing (nlp); Ontology; Ontology Learning (ol); Semantic Representation; Semantic Web; Data Mining; Intelligent Systems; Interoperability; Learning Algorithms; Learning Systems; Machine Learning; Natural Language Processing Systems; Semantic Web; Arabic Ontology; Language Processing; Natural Language Processing; Natural Languages; Ontology Learning; Ontology's; Semantic Representation; Semantic-web; Ontology},
	keywords = {Data mining; Intelligent systems; Interoperability; Learning algorithms; Learning systems; Machine learning; Natural language processing systems; Semantic Web; Arabic ontology; Language processing; Natural language processing; Natural languages; Ontology learning; Ontology's; Semantic representation; Semantic-Web; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@CONFERENCE{Procko2023141,
	author = {Procko, Tyler Thomas and Ochoa, Omar and Elvira, Timothy},
	title = {Automatic Generation of BFO-Compliant Aristotelian Definitions in OWL Ontologies with GPT},
	year = {2023},
	pages = {141 - 146},
	doi = {10.1109/TransAI60598.2023.00042},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184805224&doi=10.1109%2FTransAI60598.2023.00042&partnerID=40&md5=7f05736a0f125cf5bfd0cf0230dd81c3},
	abstract = {Ontologies are representational artifacts that purport to accurately describe some aspect of reality, including the entities and the relations that hold between them. In computer science, ontologies are software artifacts containing the schematic structure for machine-readable knowledge, typically formed as a graph of subject-predicate-object triples, constrained through Description Logics. These resources and their relations are self-defining, i.e., some resource may be defined by considering all its stated relations. Resources are often attended with natural language annotations, that humans may read and interpret, such as labels and definitions. Many long-standing ontologies have useless lexical definitions that define resources cyclically, e.g., a FOAF: Person is simply defined as 'A person'. In Aristotelian terms, the definition of a thing should be reducible, by using terms simpler than itself, such that every definition can be unpacked up to the most general thing, which can only be defined by stating examples and use cases. This paper presents an innovative technique that leverages the Generative Pre-trained Transformer (GPT) large language model, GPT -4, for automatically generating Aristotelian definition annotations for OWL classes that engenders compliance with the Basic Formal Ontology standard. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bfo; Epistemology; Gpt; Linked Data; Ontology; Birds; Data Description; Ontology; Regulatory Compliance; Automatic Generation; Bfo; Description Logic; Epistemology; Generative Pre-trained Transformer; Linked Datum; Ontology's; Owl Ontologies; Schematic Structures; Software Artefacts; Linked Data},
	keywords = {Birds; Data description; Ontology; Regulatory compliance; Automatic Generation; BFO; Description logic; Epistemology; Generative pre-trained transformer; Linked datum; Ontology's; OWL ontologies; Schematic structures; Software artefacts; Linked data},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Zhao202316189,
	author = {Zhao, Jeffrey and Cao, Yuan and Gupta, Raghav and Lee, Harrison and Rastogi, Abhinav and Wang, Mingqiu and Soltau, Hagen and Shafran, Izhak and Wu, Yonghui},
	title = {ANYTOD: A Programmable Task-Oriented Dialog System},
	year = {2023},
	pages = {16189 - 16204},
	doi = {10.18653/v1/2023.emnlp-main.1006},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184803635&doi=10.18653%2Fv1%2F2023.emnlp-main.1006&partnerID=40&md5=92237efb484e93ce0aaa8b624b67e548},
	abstract = {We propose ANYTOD, an end-to-end, zero-shot task-oriented dialog (TOD) system capable of zero-shot adaptation onto unseen tasks or domains. We view TOD as a program executed by a language model (LM), where program logic and ontology is provided by a designer as a schema. To enable generalization to unseen schemas and programs without prior training, ANYTOD adopts a neuro-symbolic approach. A neural LM keeps track of events that occur during a conversation, and a symbolic program implementing dialog policy is executed to recommend actions ANYTOD should take. This approach drastically reduces data annotation and model training requirements, addressing the enduring challenge of rapidly adapting a TOD system to unseen tasks and domains. We demonstrate state-of-the-art results on STAR (Mehri and Eskenazi, 2021), ABCD (Chen et al., 2021) and SGD (Rastogi et al., 2020) benchmarks. We also demonstrate strong zero-shot transfer ability in low-resource settings, such as zero-shot transfer onto MultiWOZ (Budzianowski et al., 2018a). In addition, we release STARV2, an updated version of the STAR dataset with richer annotations, for benchmarking zero-shot task transfer for end-to-end TOD models. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Stars; Zero-shot Learning; Data Annotation; Dialogue Systems; End To End; Generalisation; Keep Track Of; Language Model; Model Training; Ontology's; Program Logic; Task-oriented; Computational Linguistics},
	keywords = {Stars; Zero-shot learning; Data annotation; Dialogue systems; End to end; Generalisation; Keep track of; Language model; Model training; Ontology's; Program logic; Task-oriented; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Sharma2023410,
	author = {Sharma, Hemendra Shanker and Ashish, Sharma},
	title = {Query Expansion Using Word Embedding, Ontology and Natural Language Processing},
	year = {2023},
	pages = {410 - 414},
	doi = {10.1109/SmartTechCon57526.2023.10391425},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184797811&doi=10.1109%2FSmartTechCon57526.2023.10391425&partnerID=40&md5=6c6f2d2806dbc26dc82d48bbb2557cf0},
	abstract = {Query Expansion (QE) is the art of reconstructing specific queries to expand validation presentation, especially in the data mining process in a requirement understanding environment. Expanding requirements is one of the techniques involved in finding information. In the search engine environment, the query extension includes the evaluation of the value of the construction and the extension of search queries to match new documents. In natural language processing (NLP), word embedding is a term used in textbook parsing, usually as a real-valued vector that encodes the meaning of adjacent words in the vector. It is assumed that the space will be analogous in meaning. Word embedding can be achieved using a set of language models and point literacy methods where vocabulary words or expressions are mapped to vectors of real numbers. For query expansion, one method used is natural language processing through word embedding. Other approaches are ontology, machine learning, and deep learning for automatic query expansion. This paper proposes a hybrid approach for query expansion by combining NLP and ontology through word embedding. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Mining; Information Retrieval; Natural Language Processing; Query Expansion; Word Embedding; Deep Learning; Embeddings; Natural Language Processing Systems; Ontology; Query Processing; Search Engines; Data Mining Process; Language Processing; Natural Language Processing; Natural Languages; Ontology Language; Ontology's; Query Expansion; Search Queries; Word Embedding; Data Mining},
	keywords = {Deep learning; Embeddings; Natural language processing systems; Ontology; Query processing; Search engines; Data mining process; Language processing; Natural language processing; Natural languages; Ontology language; Ontology's; Query expansion; Search queries; Word embedding; Data mining},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Yu2023707,
	author = {Yu, Lijun and Miao, Jin and Sun, Xiaoyu and Chen, Jiayi and Hauptmann, Alexander Georg and Dai, Hanjun and Wei, Wei},
	title = {DocumentNet: Bridging the Data Gap in Document Pre-Training},
	year = {2023},
	pages = {707 - 722},
	doi = {10.18653/v1/2023.emnlp-industry.66},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184665343&doi=10.18653%2Fv1%2F2023.emnlp-industry.66&partnerID=40&md5=7f2e5bd50ba4ebcadf55503a1248e7a4},
	abstract = {Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multimodal capabilities for VDER. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; 'current; Broad Application; Data Gap; Document Understanding; Entity Retrieval; Knowledge Transfer; Labeled Data; Pre-training; Privacy Constraints; Retrieval Models; Knowledge Management},
	keywords = {Computational linguistics; 'current; Broad application; Data gap; Document understanding; Entity retrieval; Knowledge transfer; Labeled data; Pre-training; Privacy constraints; Retrieval models; Knowledge management},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{2023,
	title = {ISWC-Posters-Demos-Industry 2023 - Proceedings of the ISWC 2023 Posters, Demos and Industry Tracks: From Novel Ideas to Industrial Practice, co-located with 22nd International Semantic Web Conference, ISWC 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3632},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184588070&partnerID=40&md5=b9cba7059d8f6b8243e55dd41e8288df},
	abstract = {The proceedings contain 70 papers. The topics discussed include: towards algebraic mapping operators for knowledge graph construction; economics assistant for robustness checks (EconARC): identifying confounders from causal knowledge graphs; exploring large language models for ontology alignment; correlating eye gaze with object to enrich cultural heritage knowledge graph; towards data integrity verification for more sustainable petroleum industry; towards statistical reasoning with ontology embeddings; schema.org: how is it used?; exploring large language models as a source of common-sense knowledge for robots; ASKG: an approach to enrich scholarly knowledge graphs through paper decomposition with deep learning; and towards preserving biodiversity using nature FIRST knowledge graph with crossovers. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Ocker2023,
	author = {Ocker, Felix and Deigmoeller, Joerg and Eggert, Julian P.},
	title = {Exploring Large Language Models as a Source of Common-Sense Knowledge for Robots},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3632},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184382211&partnerID=40&md5=9fb8e419f9ec5f7722d6c3c66bf1f13b},
	abstract = {Service robots need common-sense knowledge to help humans in everyday situations as it enables them to understand the context of their actions. However, approaches that use ontologies face a challenge because common-sense knowledge is often implicit, i.e., it is obvious to humans but not explicitly stated. This paper investigates if Large Language Models (LLMs) can fill this gap. Our experiments reveal limited effectiveness in the selective extraction of contextual action knowledge, suggesting that LLMs may not be sufficient on their own. However, the large-scale extraction of general, actionable knowledge shows potential, indicating that LLMs can be a suitable tool for efficiently creating ontologies for robots. This paper shows that the technique used for knowledge extraction can be applied to populate a minimalist ontology, showcasing the potential of LLMs in synergy with formal knowledge representation. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Common-sense Knowledge; Knowledge Extraction; Large Language Models; Robotics; Computational Linguistics; Extraction; Knowledge Representation; Robots; Commonsense Knowledge; Formal Knowledge Representations; Knowledge Extraction; Language Model; Large Language Model; Large-scales; Ontology's; Selective Extraction; Service Robots; Ontology},
	keywords = {Computational linguistics; Extraction; Knowledge representation; Robots; Commonsense knowledge; Formal knowledge representations; Knowledge extraction; Language model; Large language model; Large-scales; Ontology's; Selective extraction; Service robots; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Mihindukulasooriya2023,
	author = {Mihindukulasooriya, Nandana and Dash, Sarthak and Bagchi, Sugato and Chowdhury, Faisal and Gliozzo, Alfio Massimiliano and Farkash, Ariel and Glass, Michael R. and Gokhman, Igor and Hassanzadeh, Oktie and Pham, Nhan H.},
	title = {Unleashing the Potential of Data Lakes with Semantic Enrichment Using Foundation Models},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3632},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184379489&partnerID=40&md5=3575ea110a78ffde7449db418c24400b},
	abstract = {Nowadays most organizations are managing data lakes containing heterogeneous data from various sources. However, the lack of adequate metadata often transforms these data lakes into data swamps, making it challenging to locate relevant data for critical organizational tasks and consequently limiting their utility. Recent advancements in large language models and foundation models have enabled the automation of metadata generation using generative AI models and the use of generated metadata for mapping tabular data into semantically richer glossaries, taxonomies, or ontologies. In this talk, we will present a semantic enrichment process that generates table metadata such as descriptive table captions, tags, expanded column names, and column descriptions and then uses that information to map table columns to concepts in a given business glossary or an ontology. Furthermore, during this process, we represent both table metadata and business glossaries as knowledge graphs and connect them by mapping columns to business concepts. As a result, the enrichment process makes the data in data lakes more meaningful to the organization and enhances downstream tasks, including improved table search and discovery, efficient table joins, and advanced business analytics. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Lakes; Foundation Models; Knowledge Graph; Large Language Models; Semantic Enrichment; Computational Linguistics; Glossaries; Knowledge Graph; Lakes; Mapping; Ontology; Semantics; Business Glossary; Data Lake; Foundation Models; Heterogeneous Data; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Organisational; Semantic Enrichment; Metadata},
	keywords = {Computational linguistics; Glossaries; Knowledge graph; Lakes; Mapping; Ontology; Semantics; Business glossary; Data lake; Foundation models; Heterogeneous data; Knowledge graphs; Language model; Large language model; Ontology's; Organisational; Semantic enrichment; Metadata},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{He2023,
	author = {He, Yuan and Chen, Jiaoyan and Dong, Hang and Horrocks, Ian},
	title = {Exploring Large Language Models for Ontology Alignment},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3632},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184375684&partnerID=40&md5=43d321af25029eca4ea303b161851303},
	abstract = {This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot<sup>1</sup> performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Flan-t5; Gpt; Large Language Model; Ontology Alignment; Ontology Matching; Computational Linguistics; Concept Equivalences; Flan-t5; Gpt; Language Model; Large Language Model; Matchings; Ontology Alignment; Ontology Matching; Ontology's; Performance; Ontology},
	keywords = {Computational linguistics; Concept equivalences; Flan-t5; GPT; Language model; Large language model; Matchings; Ontology alignment; Ontology matching; Ontology's; Performance; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Schulhoff20234945,
	author = {Schulhoff, Sander and Pinto, Jeremy and Khan, Anaum and Bouchard, Louis François and Si, Chenglei and Anati, Svetlina and Tagliabue, Valen and Kost, Anson Liu and Carnahan, Christopher and Boyd-Graber, Jordan Lee},
	title = {Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition},
	year = {2023},
	pages = {4945 - 4977},
	doi = {10.18653/v1/2023.emnlp-main.302},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184261267&doi=10.18653%2Fv1%2F2023.emnlp-main.302&partnerID=40&md5=3ed198e1c49bc2159e79a6dd0f7844f2},
	abstract = {Large Language Models (LLMs) are deployed in interactive contexts with direct user engagement, such as chatbots and writing assistants. These deployments are vulnerable to prompt injection and jailbreaking (collectively, prompt hacking), in which models are manipulated to ignore their original instructions and follow potentially malicious ones. Although widely acknowledged as a significant security threat, there is a dearth of large-scale resources and quantitative studies on prompt hacking. To address this lacuna, we launch a global prompt hacking competition, which allows for free-form human input attacks. We elicit 600K+ adversarial prompts against three state-of-the-art LLMs. We describe the dataset, which empirically verifies that current LLMs can indeed be manipulated via prompt hacking. We also present a comprehensive taxonomical ontology of the types of adversarial prompts. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Natural Language Processing Systems; 'lacuna'; Chatbots; Freeforms; Global Scale; Language Model; Large-scales; Quantitative Study; Security Threats; State Of The Art; User Engagement; Personal Computing},
	keywords = {Computational linguistics; Natural language processing systems; 'lacuna'; Chatbots; Freeforms; Global scale; Language model; Large-scales; Quantitative study; Security threats; State of the art; User engagement; Personal computing},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 31; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Lu202315108,
	author = {Lu, Keming and Pan, Xiaoman and Song, Kaiqiang and Zhang, Hongming and Yu, Dong and Chen, Jianshu},
	title = {PIVOINE: Instruction Tuning for Open-world Entity Profiling},
	year = {2023},
	pages = {15108 - 15127},
	doi = {10.18653/v1/2023.findings-emnlp.1009},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183306822&doi=10.18653%2Fv1%2F2023.findings-emnlp.1009&partnerID=40&md5=4f3c8fd9ce9062faf96259d012bba848},
	abstract = {This work considers the problem of Open-world Entity Profiling, which is a sub-domain of Open-world Information Extraction (Open-world IE). Unlike the conventional closed-world IE, Open-world IE considers a more general situation where entities and relations could be beyond a predefined ontology. We seek to develop a large language model (LLM) that can perform Open-world Entity Profiling with instruction tuning to extract desirable entity profiles characterized by (possibly fine-grained) natural language instructions. In particular, we construct INSTRUCTOPENWIKI, a substantial instruction-tuning dataset for Open-world Entity Profiling enriched with a comprehensive corpus, extensive annotations, and diverse instructions. We finetune pretrained BLOOM models on INSTRUCTOPENWIKI and obtain PIVOINE, an LLM for Open-world Entity Profiling with strong instruction-following capabilities. Our experiments demonstrate that PIVOINE significantly outperforms traditional methods and ChatGPT-based baselines, displaying impressive generalization capabilities on both unseen instructions and out-of-ontology cases. Consequently, PIVOINE emerges as a promising solution to tackle the open-world challenge in entity profiling. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Fine Grained; General Situation; Generalization Capability; Language Model; Natural Languages; Ontology's; Open World; Subdomain; Ontology},
	keywords = {Computational linguistics; Fine grained; General situation; Generalization capability; Language model; Natural languages; Ontology's; Open world; Subdomain; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Zhou20236491,
	author = {Zhou, Wentao and Zhao, Jun and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
	title = {Inductive Relation Inference of Knowledge Graph Enhanced by Ontology Information},
	year = {2023},
	pages = {6491 - 6502},
	doi = {10.18653/v1/2023.findings-emnlp.431},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183302828&doi=10.18653%2Fv1%2F2023.findings-emnlp.431&partnerID=40&md5=cb568f6ab67a78a556f048ed603be3db},
	abstract = {The inductive inference of the knowledge graph aims to complete the potential relations between the new unknown entities in the graph. Most existing methods are based on entity-independent features such as graph structure information and relationship information to inference. However, the neighborhood of these new entities is often too sparse to obtain enough information to build these features effectively. In this work, we propose a knowledge graph inductive inference method that fuses ontology information. Based on the enclosing subgraph, we bring in feature embeddings of concepts corresponding to entities to learn the semantic information implicit in the ontology. Considering that the ontology information of entities may be missing, we build a type constraint regular loss to explicitly model the semantic connections between entities and concepts, and thus capture the missing concepts of entities. Experimental results show that our approach significantly outperforms large language models like ChatGPT on two benchmark datasets, YAGO21K-610 and DB45K-165, and improves the MRR metrics by 15.4% and 44.1%, respectively, when compared with the state-of-the-art methods. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Graphic Methods; Knowledge Graph; Large Datasets; Semantics; As Graph; Graph Structures; Inductive Inference; Inference Methods; Knowledge Graphs; Neighbourhood; Ontology's; Structure Information; Subgraphs; Unknown Entities; Ontology},
	keywords = {Computational linguistics; Graphic methods; Knowledge graph; Large datasets; Semantics; AS graph; Graph structures; Inductive inference; Inference methods; Knowledge graphs; Neighbourhood; Ontology's; Structure information; Subgraphs; Unknown entities; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Chen202314815,
	author = {Chen, Derek and Lee, Celine and Lu, Yunan and Rosati, Domenic and Yu, Zhou},
	title = {Mixture of Soft Prompts for Controllable Data Generation},
	year = {2023},
	pages = {14815 - 14833},
	doi = {10.18653/v1/2023.findings-emnlp.988},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183292253&doi=10.18653%2Fv1%2F2023.findings-emnlp.988&partnerID=40&md5=65ab0d48ade6c9dd358f5ceb95e8c575},
	abstract = {Large language models (LLMs) effectively generate fluent text when the target output follows natural language patterns. However, structured prediction tasks confine the output format to a limited ontology, causing even very large models to struggle since they were never trained with such restrictions in mind. The difficulty of using LLMs for direct prediction is exacerbated in few-shot learning scenarios, which commonly arise due to domain shift and resource limitations. We flip the problem on its head by leveraging the LLM as a tool for data augmentation rather than a model for direct prediction. Our proposed Mixture of Soft Prompts (MSP) serves as a parameter-efficient procedure for generating multi-attribute data in a controlled manner. Denoising mechanisms are further applied to improve the quality of synthesized data. Automatic metrics show our method is capable of producing diverse and natural text, while preserving label semantics. Moreover, MSP achieves state-of-the-art results on three benchmarks when compared against strong baselines. Our method offers an alternate data-centric approach for applying LLMs to complex prediction tasks. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Natural Language Processing Systems; Semantics; Data Generation; Direct Prediction; Fluents; Language Model; Large Models; Natural Language Patterns; Ontology's; Output Formats; Prediction Tasks; Structured Prediction; Forecasting},
	keywords = {Computational linguistics; Natural language processing systems; Semantics; Data generation; Direct prediction; Fluents; Language model; Large models; Natural language patterns; Ontology's; Output formats; Prediction tasks; Structured prediction; Forecasting},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Sevastjanova202311,
	author = {Sevastjanova, Rita and Vogelbacher, Simon and Spitz, Andreas and Keim, Daniel A. and El-Assady, Mennatallah},
	title = {Visual Comparison of Text Sequences Generated by Large Language Models},
	year = {2023},
	pages = {11 - 20},
	doi = {10.1109/VDS60365.2023.00007},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182731475&doi=10.1109%2FVDS60365.2023.00007&partnerID=40&md5=0dabfcf56503bb321f3d7cda24c54867},
	abstract = {Causal language models have emerged as the leading technology for automating text generation tasks. Although these models tend to produce outputs that resemble human writing, they still suffer from quality issues (e.g., social biases). Researchers typically use automatic analysis methods to evaluate the model limitations, such as statistics on stereotypical words. Since different types of issues are embedded in the model parameters, the development of automated methods that capture all relevant aspects remains a challenge. To tackle this challenge, we propose a visual analytics approach that supports the exploratory analysis of text sequences generated by causal language models. Our approach enables users to specify starting prompts and effectively groups the resulting text sequences. To this end, we leverage a unified, ontology-driven embedding space, serving as a shared foundation for the thematic concepts present in the generated text sequences. Visual summaries provide insights into various levels of granularity within the generated data. Among others, we propose a novel comparison visualization that slices the embedding space and represents the differences between two prompt outputs in a radial layout. We demonstrate the effectiveness of our approach through case studies, showcasing its potential to reveal model biases and other quality issues. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Causal Language Models; Prompt Output Comparison; Text Generation; Computational Linguistics; Visual Languages; Visualization; Automatic Analysis Method; Causal Language Model; Embeddings; Language Model; Leading Technology; Modeling Parameters; Prompt Output Comparison; Quality Issues; Text Generations; Visual Comparison},
	keywords = {Computational linguistics; Visual languages; Visualization; Automatic analysis method; Causal language model; Embeddings; Language model; Leading technology; Modeling parameters; Prompt output comparison; Quality issues; Text generations; Visual comparison},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Zuo2023834,
	author = {Zuo, Xu and Zhou, Yujia and Duke, Jon D. and Hripcsak, George M. and Shah, Nigam Haresh and Banda, Juan M. and Reeves, Ruth Madeleine and Miller, Timothy A. and Waitman, Lemuel Russell and Natarajan, Karthik},
	title = {Standardizing Multi-site Clinical Note Titles to LOINC Document Ontology: A Transformer-based Approach},
	year = {2023},
	journal = {AMIA Annual Symposium proceedings},
	volume = {2023},
	pages = {834 - 843},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182546900&partnerID=40&md5=685a6b791b7981d1ac091630bc9446e5},
	abstract = {The types of clinical notes in electronic health records (EHRs) are diverse and it would be great to standardize them to ensure unified data retrieval, exchange, and integration. The LOINC Document Ontology (DO) is a subset of LOINC that is created specifically for naming and describing clinical documents. Despite the efforts of promoting and improving this ontology, how to efficiently deploy it in real-world clinical settings has yet to be explored. In this study we evaluated the utility of LOINC DO by mapping clinical note titles collected from five institutions to the LOINC DO and classifying the mapping into three classes based on semantic similarity between note titles and LOINC DO codes. Additionally, we developed a standardization pipeline that automatically maps clinical note titles from multiple sites to suitable LOINC DO codes, without accessing the content of clinical notes. The pipeline can be initialized with different large language models, and we compared the performances between them. The results showed that our automated pipeline achieved an accuracy of 0.90. By comparing the manual and automated mapping results, we analyzed the coverage of LOINC DO in describing multi-site clinical note titles and summarized the potential scope for extension. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine},
	author_keywords = {Electronic Health Record; Human; Information Retrieval; Logical Observation Identifiers Names And Codes; Semantics; Electronic Health Records; Humans; Information Storage And Retrieval; Semantics},
	keywords = {electronic health record; human; information retrieval; Logical Observation Identifiers Names and Codes; semantics; Electronic Health Records; Humans; Information Storage and Retrieval; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Sahil202394,
	author = {Sahil, null and Kumar, Pratyush Sreenivasa},
	title = {Leveraging Biomedical Ontologies to Boost Performance of BERT-Based Models for Answering Medical MCQs},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3603},
	pages = {94 - 105},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182276176&partnerID=40&md5=9c3aed05231e20e028c4c4ce22a1cbd2},
	abstract = {Large-scale pretrained language models like BERT have shown promising results in various natural language processing tasks. However, these models do not benefit from the rich knowledge available in domain ontologies. In this work, we propose BioOntoBERT, a BERT-based model pretrained on multiple biomedical ontologies. We also introduce the Onto2Sen system to process various ontologies to generate lexical documents, such as entity names, synonyms and definitions, and concept relationship documents. We then incorporate these knowledge-rich documents during pretraining to enhance the model’s “understanding” of the biomedical concepts. We evaluate our model on the MedMCQA dataset, a multiple-choice question-answering benchmark for the medical domain. Our experiments show that BioOntoBERT outperforms the baseline model BERT, SciBERT, BioBERT and PubMedBERT. BioOntoBERT achieves this performance improvement by incorporating only 158MB of ontology-generated data on top of the BERT model during pretraining, just 0.75% of data used in pretraining PubMedBERT. Our results demonstrate the effectiveness of incorporating biomedical ontologies in pretraining language models for the medical domain. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Biomedical Ontologies; Medical Multiple Choice Question Answering; Computational Linguistics; Natural Language Processing Systems; Bert; Biomedical Ontologies; Language Model; Medical Domains; Medical Multiple Choice Question Answering; Multiple-choice Questions; Ontology's; Performance; Pre-training; Question Answering; Ontology},
	keywords = {Computational linguistics; Natural language processing systems; BERT; Biomedical ontologies; Language model; Medical domains; Medical multiple choice question answering; Multiple-choice questions; Ontology's; Performance; Pre-training; Question Answering; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Toonsi202313,
	author = {Toonsi, Sumyyah and Kafkas, Şenay and Hoehndorf, Robert},
	title = {Exploring the Use of Ontology Components for Distantly-Supervised Disease and Phenotype Named Entity Recognition},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3603},
	pages = {13 - 24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182257589&partnerID=40&md5=3cb52eeba3025ca91da3a5d2abb23bbb},
	abstract = {The lack of curated corpora is one of the major obstacles for Named Entity Recognition (NER). With the advancements in deep learning and development of robust language models, distant supervision utilizing weakly labelled data is often used to alleviate this problem. Previous approaches utilized weakly labeled corpora from Wikipedia or from the literature. However, to the best of our knowledge, none of them explored the use of the different ontology components for disease/phenotype NER under the distant supervision scheme. In this study, we explored whether different ontology components can be used to develop a distantly supervised disease/phenotype entity recognition model. We trained different models by considering ontology labels, synonyms, definitions, axioms and their combinations in addition to a model trained on literature. Results showed that content from the disease/phenotype ontologies can be exploited to develop a NER model performing at the state-of-the-art level. In particular, models that utilised both the ontology definitions and axioms showed competitive performance compared to the model trained on literature. This relieves the need of finding and annotating external corpora. Furthermore, models trained using ontology components made zero-shot predictions on the test datasets which were not observed by the models training on the literature based datasets. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Named Entity Recognition; Ontologies; Text Mining; Character Recognition; Deep Learning; Natural Language Processing Systems; Text Processing; Zero-shot Learning; Disease Phenotypes; Entity Recognition; Labeled Data; Language Model; Named Entity Recognition; Ontology's; Recognition Models; State Of The Art; Text-mining; Wikipedia; Ontology},
	keywords = {Character recognition; Deep learning; Natural language processing systems; Text processing; Zero-shot learning; Disease phenotypes; Entity recognition; Labeled data; Language model; Named entity recognition; Ontology's; Recognition models; State of the art; Text-mining; Wikipedia; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Samson2023320,
	author = {Samson, Briane Paul V. and Cheng, Charibeth K. and Chua, Unisse C. and Velasco, Dan John and Alba, Axel and Pelagio, Trisha Gail and Ramirez, Bryce Anthony and Bangonon, Robi Jeanne and Deticio, Christine and Gaw, Sharmaine},
	title = {Towards the Creation of the Filipino Wordnet: A Two-Way Approach},
	year = {2023},
	pages = {320 - 325},
	doi = {10.1109/IALP61005.2023.10336981},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181776075&doi=10.1109%2FIALP61005.2023.10336981&partnerID=40&md5=71d746a1e7a2496af927cc83ea23952d},
	abstract = {As databases of lexical information on words and their lexical relationships, WordNets are important for various downstream natural language processing applications. However, the construction of WordNets can be challenging, especially for low-resource languages such as Filipino. The existing Filipino WordNet has not been maintained, and lacks contextual information for identifying the evolution of word senses. In this study, we built a corpus of 5,370,667 unique tokens and used it to construct a Filipino WordNet via a two-way approach that combines natural language processing and network science. For the natural language processing approach, we utilized only two linguistic sources: our corpus and a RoBERTa-based language model that generates sentence embeddings. For the network science approach, we created a temporal-multiplex network that represents the co-occurrence of words, their semantic relationships, and their usage in different sources across time. We show that our proposed method can induce existing senses (30% of our validation data, as evaluated by matching with the senses from Princeton WordNet) and generate 9,549 semantic sets. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Word Co-occurrence Networks; Word Sense Disambiguation; Word Sense Induction; Wordnet Construction; Natural Language Processing Systems; Ontology; Co-occurrence Networks; Language Processing; Natural Languages; Two Ways; Word Co-occurrence; Word Co-occurrence Network; Word Sense Disambiguation; Word Sense Inductions; Wordnet; Wordnet Construction; Semantics},
	keywords = {Natural language processing systems; Ontology; Co-occurrence networks; Language processing; Natural languages; Two ways; Word co-occurrence; Word co-occurrence network; Word Sense Disambiguation; Word sense inductions; Wordnet; Wordnet construction; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Fatemi2023145386,
	author = {Fatemi, Bahareh and Rabbi, Fazle and Opdahl, Andreas L.},
	title = {Evaluating the Effectiveness of GPT Large Language Model for News Classification in the IPTC News Ontology},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {145386 - 145394},
	doi = {10.1109/ACCESS.2023.3345414},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181543202&doi=10.1109%2FACCESS.2023.3345414&partnerID=40&md5=013b0eaa020e69a49f3f6724cea13afc},
	abstract = {News classification plays a vital role in newsrooms, as it involves the time-consuming task of categorizing news articles and requires domain knowledge. Effective news classification is essential for categorizing and organizing a constant flow of information, serving as the foundation for subsequent tasks, such as news aggregation, monitoring, filtering, and organization. The automation of this process can significantly benefit newsrooms by saving time and resources. In this study, we explore the potential of the GPT large language model in a zero-shot setting for multi-class classification of news articles within the widely accepted International Press Telecommunications Council (IPTC) news ontology. The IPTC news ontology provides a structured framework for categorizing news, facilitating the efficient organization and retrieval of news content. By investigating the effectiveness of the GPT language model in this classification task, we aimed to understand its capabilities and potential applications in the news domain. This study was conducted as part of our ongoing research in the field of automated journalism. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Iptc Media Topics; Journalism; Large Language Models; News Classification; Classification (of Information); Computational Linguistics; Information Filtering; Job Analysis; Zero-shot Learning; Adaptation Models; Annotation; International Press Telecommunication Council Medium Topic; International Press Telecommunications Councils; Journalism; Language Model; Large Language Model; News Classification; Ontology's; Support Vectors Machine; Tag Clouds; Task Analysis; Ontology},
	keywords = {Classification (of information); Computational linguistics; Information filtering; Job analysis; Zero-shot learning; Adaptation models; Annotation; International press telecommunication council medium topic; International press telecommunications councils; Journalism; Language model; Large language model; News classification; Ontology's; Support vectors machine; Tag clouds; Task analysis; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Hertling2023170,
	author = {Hertling, Sven and Paulheim, Heiko},
	title = {OLaLa Results for OAEI 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3591},
	pages = {170 - 177},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180814848&partnerID=40&md5=b8d5fac66dc9350abb833bd534428cf1},
	abstract = {This paper presents the results of the OLaLa matching system participating in the OAEI 2023. The system is based on sentence-transformers as well as large language models. The former is used to generate correspondence candidates which is independent of any overlapping tokens because the comparison is only based on embeddings. To finally select the best mappings, a large language model is used to decide if two given textual representations of the source and target concept are equal or not. Based on positive and negative words that the LLM predicts, a confidence is extracted. Still, there are a lot of decisions that heavily influence the final result like (1) how can each concept be verbalized into text, (2) which prompt to use, and (3) which language model to choose. A lot of combinations were executed and the most useful one is submitted and packaged as a matching system. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graphs; Large Language Model; Ontology Matching; Knowledge Graph; Ontology; Embeddings; Knowledge Graphs; Language Model; Large Language Model; Matching System; Ontology Matching; Target Concept; Textual Representation; Computational Linguistics},
	keywords = {Knowledge graph; Ontology; Embeddings; Knowledge graphs; Language model; Large language model; Matching system; Ontology matching; Target concept; Textual representation; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Gosselin2023184,
	author = {Gosselin, Francis and Zouaq, Amal},
	title = {SORBETMatcher Results for OAEI 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3591},
	pages = {184 - 190},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180813182&partnerID=40&md5=69e5e5b6ea4a65476d218c8e74d08e91},
	abstract = {This paper presents the results of SORBETMatcher in the OAEI 2023 competition. SORBETMatcher is a schema matching system for both equivalence matching and subsumption matching. SORBETMatcher is largely based on SORBET Embeddings, a novel ontology embedding method that leverages large language models, random walks, and a regression loss to construct a latent space that encapsulates ontology structures. Despite recognizing certain limitations inherent in SORBET Embeddings, SORBETMatcher performed well in the OAEI competition. It emerged as the leading system in three out of the five subsumption matching challenges within the Bio-ML track, as well as in the equivalence matching problem involving ORDO-DOID. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Iswc-2023; Ontology Alignment; Representation Learning; Schema Matching; Embeddings; Semantic Web; Embedding Method; Iswc-2023; Language Model; Matching System; Matchings; Ontology Alignment; Ontology's; Representation Learning; Schema Matching; Ontology},
	keywords = {Embeddings; Semantic Web; Embedding method; ISWC-2023; Language model; Matching system; Matchings; Ontology alignment; Ontology's; Representation learning; Schema matching; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2023,
	title = {Scholarly QALD 2023 and SemREC 2023 - Joint Proceedings of 1st Scholarly QALD Challenge 2023 and 4th SeMantic Answer Type, Relation and Entity Prediction Tasks Challenge 2023, co-located with 22nd International Semantic Web Conference, ISWC 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3592},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180801778&partnerID=40&md5=c2ad5e26f2c121747a43d361e9de2083},
	abstract = {The proceedings contain 9 papers. The topics discussed include: when context matters: entity linking in the scholarly domain; NLQxform: a language model-based question to SPARQL transformer; a structure and content prompt-based method for knowledge graph question answering over scholarly data; leveraging LLMs in scholarly knowledge graph question answering; improving subgraph extraction algorithms for one-shot SPARQL query generation with large language models; PSYCHIC: a neuro-symbolic framework for knowledge graph question-answering grounding; BERTologyNavigator: advanced question answering with BERT-based semantics; enhanced GAT: expanding receptive field with meta path-guided RDF rules for two-hop connectivity; and evaluating different methods for semantic reasoning over ontologies. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Wang2023146,
	author = {Wang, Zhu},
	title = {AMD Results for OAEI 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3591},
	pages = {146 - 153},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180779638&partnerID=40&md5=5de524163e7e345de6e2c5ebd3bde262},
	abstract = {AgreementMakerDeep (AMD) is a new flexible and extensible ontology matching system. It exploits the contextual and structural information of ontologies by infusing knowledge to pre-trained masked language model, and then filter the output mappings using knowledge graph embedding techniques. AMD learns from classes and their relations between classes by constructing vector representations into the low dimensional embedding space with knowledge graph embedding methods. The results demonstrate that AMD achieves a competitive performance in many OAEI tracks, but AMD has limitations for property and instance matching. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Embedding; Large Language Model; Ontology Matching; Computational Linguistics; Graph Embeddings; Ontology; Vector Spaces; Contextual Information; Knowledge Graph Embedding; Knowledge Graphs; Language Model; Large Language Model; Matching System; Ontology Matching; Ontology's; Structural Information; Knowledge Graph},
	keywords = {Computational linguistics; Graph embeddings; Ontology; Vector spaces; Contextual information; Knowledge graph embedding; Knowledge graphs; Language model; Large language model; Matching system; Ontology matching; Ontology's; Structural information; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Peng202367,
	author = {Peng, Yiwen and Alam, Mehwish Afshar and Bonald, Thomas},
	title = {Ontology Matching using Textual Class Descriptions},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3591},
	pages = {67 - 72},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180776271&partnerID=40&md5=5115bca4fda57f89bac2412fb02a8506},
	abstract = {In this paper, we propose TEXTO, a TEXT-based Ontology matching system. This matcher leverages the rich semantic information of classes available in most ontologies by a combination of a pre-trained word embedding model and a pre-trained language model. Its performance is evaluated on the datasets of the OAEI Common Knowledge Graphs Track, augmented with the description of each class, and a new dataset based on the refreshed alignment of Schema.org and Wikidata. Our results demonstrate that TEXTO outperforms all state-of-art matchers in terms of precision, recall and F1 score. In particular, we show that almost perfect class alignment can be achieved using textual content only, excluding any structural information like the graph of classes or the instances of each class. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Language Models; Ontology Matching; Textual Information; Computational Linguistics; Knowledge Graph; Semantics; Common Knowledge; Embeddings; Knowledge Graphs; Language Model; Matching System; Ontology Matching; Ontology's; Performance; Semantics Information; Textual Information; Ontology},
	keywords = {Computational linguistics; Knowledge graph; Semantics; Common knowledge; Embeddings; Knowledge graphs; Language model; Matching system; Ontology matching; Ontology's; Performance; Semantics Information; Textual information; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Norouzi202361,
	author = {Norouzi, Sanaz Saki and Mahdavinejad, Mohammad Saeid and Hitzler, Pascal Al},
	title = {Conversational Ontology Alignment with ChatGPT},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3591},
	pages = {61 - 66},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180754943&partnerID=40&md5=8a06be5d71f54f617249262d138c8531},
	abstract = {This study evaluates the applicability and efficiency of ChatGPT for ontology alignment using a naive approach. ChatGPT’s output is compared to the results of the Ontology Alignment Evaluation Initiative 2022 campaign using conference track ontologies. This comparison is intended to provide insights into the capabilities of a conversational large language model when used in a naive way for ontology matching and to investigate the potential advantages and disadvantages of this approach. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatgpt; Large Language Models; Llm Behavior; Ontology Alignment; Ontology Matching; Prompt Engineering; Schema Matching; Computational Linguistics; Chatgpt; Language Model; Large Language Model; Llm Behavior; Ontology Alignment; Ontology Matching; Ontology's; Prompt Engineering; Schema Matching; Ontology},
	keywords = {Computational linguistics; ChatGPT; Language model; Large language model; LLM behavior; Ontology alignment; Ontology matching; Ontology's; Prompt engineering; Schema matching; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{Wang202337,
	author = {Wang, Zhu},
	title = {Contextualized Structural Self-supervised Learning for Ontology Matching},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3591},
	pages = {37 - 48},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180754259&partnerID=40&md5=3cffc4029e8fbfd6858798439a8e6d9b},
	abstract = {Ontology matching (OM) entails the identification of semantic relationships between concepts within two or more knowledge graphs (KGs) and serves as a critical step in integrating KGs from various sources. Recent advancements in deep OM models have harnessed the power of transformer-based language models and the advantages of knowledge graph embedding. Nevertheless, these OM models still face persistent challenges, such as a lack of reference alignments, runtime latency, and unexplored different graph structures within an end-to-end framework. In this study, we introduce a novel self-supervised learning OM framework with input ontologies, called LaKERMap. This framework capitalizes on the contextual and structural information of concepts by integrating implicit knowledge into transformers. Specifically, we aim to capture multiple structural contexts, encompassing both local and global interactions, by employing distinct training objectives. To assess our methods, we utilize the Bio-ML datasets and tasks. The findings from our innovative approach reveal that LaKERMap surpasses state-of-the-art systems in terms of alignment quality and inference time. Our models and codes are available here https://github.com/ellenzhuwang/lakermap. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Embedding; Ontology Matching; Self-supervised Learning; Graph Embeddings; Graphic Methods; Ontology; Semantics; Supervised Learning; Critical Steps; Knowledge Graph Embedding; Knowledge Graphs; Matching Models; Ontology Matching; Power; Relationship Between Concepts; Self-supervised Learning; Semantic Relationships; Knowledge Graph},
	keywords = {Graph embeddings; Graphic methods; Ontology; Semantics; Supervised learning; Critical steps; Knowledge graph embedding; Knowledge graphs; Matching models; Ontology matching; Power; Relationship between concepts; Self-supervised learning; Semantic relationships; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2023,
	title = {OM 2023 - Proceedings of the 18th International Workshop on Ontology Matching, co-located with the 22nd International Semantic Web Conference, ISWC 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3591},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180752538&partnerID=40&md5=a46172b5082beccb5fc1ca6c98e5880b},
	abstract = {The proceedings contain 23 papers. The topics discussed include: Truveta mapper: a zero-shot ontology alignment framework; the role of ontology matching in ontology network development; matching table metadata with business glossaries using large language models; contextualized structural self-supervised learning for ontology matching; evaluation toolkit for API and RDF alignment; conversational ontology alignment with ChatGPT; ontology matching using textual class descriptions; a simple standard for ontological mappings 2023: updates on data model, collaborations and tooling; repairing networks of ontologies using weakening and completing; towards a methodology for the semi-automatic generation of scientific knowledge graphs from XML documents; and combining word and sentence embeddings with alignment extension for property matching. © 2024 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Pliukhin2023,
	author = {Pliukhin, Dmitrii A. and Radyush, Daniil and Kovriguina, Liubov and Muromtsev, Dmitry I.},
	title = {Improving Subgraph Extraction Algorithms for One-Shot SPARQL Query Generation with Large Language Models},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3592},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180551320&partnerID=40&md5=d3d62c78b1b2b37274a9e88314393de0},
	abstract = {Question answering over scholarly knowledge graphs involves many challenges: complex graph patterns, long-tail distributed data, revision and evolution of the scholarly ontologies, and knowledge graphs incompleteness due to constant research dynamics. In this work, we present an LLM-based approach for SPARQL query generation over Open Research Knowledge Graph (ORKG) for the ISWC SciQA Challenge. Our approach proposes a couple of improvements to the recently published SPARQLGEN approach, that performs one-shot SPARQL query generation by augmenting Large Language Models (LLMs) with the relevant context within a single prompt. Similar to SPARQLGEN, we include heterogeneous data sources in the SPARQL generation prompt: a question itself, an RDF subgraph required to answer the question, and an example of a correct SPARQL query. In the current work, we focused on designing subgraph extraction algorithms, that are close to real-life scenarios of generative KGQA, and replaced the random choice of example question-query pair with similarity scoring. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Augmented Large Language Models; Knowledge Graphs Question Answering; Scholarly Knowledge Graphs; Sparql Query Generation; Subgraph Extraction; Computational Linguistics; Data Mining; Extraction; Graphic Methods; Natural Language Processing Systems; Query Processing; Resource Description Framework (rdf); Augmented Large Language Model; Knowledge Graph Question Answering; Knowledge Graphs; Language Model; Query Generation; Question Answering; Scholarly Knowledge Graph; Sparql Query Generation; Subgraph Extraction; Knowledge Graph},
	keywords = {Computational linguistics; Data mining; Extraction; Graphic methods; Natural language processing systems; Query processing; Resource Description Framework (RDF); Augmented large language model; Knowledge graph question answering; Knowledge graphs; Language model; Query generation; Question Answering; Scholarly knowledge graph; SPARQL query generation; Subgraph extraction; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Jiang2023,
	author = {Jiang, Longquan and Xi, Yan and Usbeck, Ricardo},
	title = {A Structure and Content Prompt-based Method for Knowledge Graph Question Answering over Scholarly Data},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3592},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180545725&partnerID=40&md5=f5ab3853af26941c54e4799988381362},
	abstract = {Answering scholarly questions is challenging without the help of query-based systems. Thus, we develop a divide-and-conquer approach based on a Large Language Model (LLM) for scholarly Knowledge Graph (KG) Question Answering (QA). Our system integrates the KG ontology into the LLM prompts and leverages a hybrid prompt learning strategy with both query structure and content. Our experiments suggest that given an ontology of a specific KG, LLMs are capable of automatically choosing the corresponding classes or predicates required to generate a target SPARQL query from a natural language question. Our approach shows state-of-the-art results over one scholarly KGQA dataset, namely sciQA [1]. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Kgqa; Large Language Model; Question Answering; Scholarly Kgqa; Computational Linguistics; Natural Language Processing Systems; Ontology; Divide-and-conquer Approach; Kgqa; Knowledge Graphs; Language Model; Large Language Model; Learning Strategy; Ontology's; Query Structures; Question Answering; Scholarly Kgqa; Knowledge Graph},
	keywords = {Computational linguistics; Natural language processing systems; Ontology; Divide-and-conquer approach; KGQA; Knowledge graphs; Language model; Large language model; Learning strategy; Ontology's; Query structures; Question Answering; Scholarly KGQA; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Wang2023133,
	author = {Wang, Zhaoyi and Zhang, Zhenyang and Qin, Jiaxin and Iwaihara, Mizuho},
	title = {SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging Semantic, Lexical, and Hierarchical Features},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14457 LNCS},
	pages = {133 - 148},
	doi = {10.1007/978-981-99-8085-7_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180152462&doi=10.1007%2F978-981-99-8085-7_12&partnerID=40&md5=4720f12d43de762209f92b9ea57dedb6},
	abstract = {Wikipedia articles are hierarchically organized through categories and lists, providing one of the most comprehensive and universal taxonomy, but its open creation is causing redundancies and inconsistencies. Assigning DBPedia classes to Wikipedia categories and lists can alleviate the problem, realizing a large knowledge graph which is essential for categorizing digital contents through entity linking and typing. However, the existing approach of CaLiGraph is producing incomplete and non-fine grained mappings. In this paper, we tackle the problem as ontology alignment, where structural information of knowledge graphs and lexical and semantic features of ontology class names are utilized to discover confident mappings, which are in turn utilized for finetuing pretrained language models in a distant supervision fashion. Our method SLHCat consists of two main parts: 1) Automatically generating training data by leveraging knowledge graph structure, semantic similarities, and named entity typing. 2) Finetuning and prompt-tuning of the pre-trained language model BERT are carried out over the training data, to capture semantic and syntactic properties of class names. Our model SLHCat is evaluated over a benchmark dataset constructed by annotating 3000 fine-grained CaLiGraph-DBpedia mapping pairs. SLHCat is outperforming the baseline model by a large margin of 25% in accuracy, offering a practical solution for large-scale ontology mapping. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Caligraph; Dbpedia; Distant Supervision; Knowledge Graph; Ontology Alignment; Wikipedia Categories And Lists; Computational Linguistics; Knowledge Graph; Knowledge Management; Ontology; Semantic Web; Semantics; Syntactics; Caligraph; Dbpedia; Distant Supervision; Fine Grained; Knowledge Graphs; Lexical Features; Ontology Alignment; Semantic Features; Wikipedia; Wikipedia Category And List; Mapping},
	keywords = {Computational linguistics; Knowledge graph; Knowledge management; Ontology; Semantic Web; Semantics; Syntactics; Caligraph; Dbpedia; Distant supervision; Fine grained; Knowledge graphs; Lexical features; Ontology alignment; Semantic features; Wikipedia; Wikipedia category and list; Mapping},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Tóthfalusi2023,
	author = {Tóthfalusi, Tamás and Varga, Eszter and Csiszar, Zoltan and Varga, Pal},
	title = {ML-Based Translation Methods for Protocols and Data Formats},
	year = {2023},
	pages = {},
	doi = {10.23919/CNSM59352.2023.10327850},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180009347&doi=10.23919%2FCNSM59352.2023.10327850&partnerID=40&md5=d5e743da68565427a9eb2070f266c8b0},
	abstract = {In order to exchange information between systems, the information must get encoded into a predefined data format, and it must be transferred in a protocol that the communicating parties have agreed upon. This works well if all parties follow the same protocol standard and use the same data description schemes. If systems use different data formats or protocols, then some sort of translation is required. Protocol and data format translation has been attempted previously through rule-based approaches, ontologies, and also by using machine learning (ML) techniques. Due to the current advances related to AI/ML methods, tools, and infrastructure, the accuracy and feasibility of 'translation' with ML-approaches improved significantly. This paper introduces a generic approach and methodology for translating data formats and protocols with ML-based methods and presents our initial results through JSON-XML and JSON-SenML translation. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Llm; Machine Learning; Natural Language Processing; Neural Machine Translation; Protocol Translation; Computational Linguistics; Learning Algorithms; Natural Language Processing Systems; Translation (languages); Description Schemes; Language Processing; Llm; Machine-learning; Natural Language Processing; Natural Languages; Protocol Translations; Rule-based Approach; System Use; Translation Method; Machine Learning},
	keywords = {Computational linguistics; Learning algorithms; Natural language processing systems; Translation (languages); Description schemes; Language processing; LLM; Machine-learning; Natural language processing; Natural languages; Protocol translations; Rule-based approach; System use; Translation method; Machine learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{2023,
	title = {15th International Conference on Knowledge Engineering and Ontology Development, KEOD 2023 as part of IC3K 2023 - Proceedings of the 15th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management},
	year = {2023},
	journal = {International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K - Proceedings},
	volume = {2},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179795882&partnerID=40&md5=59ccc157ab95d6847c06f927218c2061},
	abstract = {The proceedings contain 35 papers. The topics discussed include: EvdoGraph: a knowledge graph for the EVDOXUS textbook management service for Greek universities; development of an OWL ontology based on the function-oriented system architecture to support data synchronization between SysML and domain models; using paraphrasers to detect duplicities in ontologies; developing digital media service value creation by using emotion data; knowledge graphs alignment based on learning to rank methods; knowledge graphs extracted from medical appointment transcriptions: results generating triples relying on LLMs; an approach to developing ontology-based tools for event series analysis; and evaluating the perceived quality and functionality of demo models’ representations in the health domain. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2023,
	title = {KBC-LM + LM-KBC 2023 - Joint Proceedings of the 1st Workshop on Knowledge Base Construction from Pre-Trained Language Models and the 2nd Challenge on Language Models for Knowledge Base Construction, co-located with the 22nd International Semantic Web Conference, ISWC 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3577},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179793559&partnerID=40&md5=6d62091d318f31477226c8a50f92fc45},
	abstract = {The proceedings contain 15 papers. The topics discussed include: language models as knowledge bases for visual word sense disambiguation; extracting geographic knowledge from large language models: an experiment; do instruction-tuned large language models help with relation extraction?; towards ontology construction with language models; towards syntax-aware pretraining and prompt engineering for knowledge retrieval from large language models; can large language models generate salient negative statements?; limits of zero-shot probing on object prediction; knowledge-centric prompt composition for knowledge base construction from pre-trained language models; and enhancing knowledge base construction from pre-trained language models using prompt ensembles. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Händler202385,
	author = {Händler, Thorsten},
	title = {A Taxonomy for Autonomous LLM-Powered Multi-Agent Architectures},
	year = {2023},
	journal = {International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K - Proceedings},
	volume = {3},
	pages = {85 - 98},
	doi = {10.5220/0012239100003598},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179763690&doi=10.5220%2F0012239100003598&partnerID=40&md5=6b2498df865b3e79dbcb1f00c54d2daa},
	abstract = {Large language models (LLMs) have revolutionized the field of artificial intelligence, endowing it with sophisticated language understanding and generation capabilities. However, when faced with more complex and interconnected tasks that demand a profound and iterative thought process, LLMs reveal their inherent limitations. Autonomous LLM-powered multi-agent systems represent a strategic response to these challenges. While these architectures hold promising potential in amplifying AI capabilities, striking the right balance between different levels of autonomy and alignment remains the crucial challenge for their effective operation. This paper proposes a comprehensive multi-dimensional taxonomy, engineered to analyze how autonomous LLM-powered multi-agent systems balance the dynamic interplay between autonomy and alignment across various aspects inherent to architectural viewpoints such as goal-driven task management, agent composition, multi-agent collaboration, and context interaction. Our taxonomy aims to empower researchers, engineers, and AI practitioners to systematically analyze the architectural dynamics and balancing strategies employed by these increasingly prevalent AI systems. The exploratory taxonomic classification of selected representative LLM-powered multi-agent systems illustrates its practical utility and reveals potential for future research and development. An extended version of this paper is available on arXiv (Händler, 2023). © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Ai System Classification; Alignment; Architectural Viewpoints; Artificial Intelligence; Autonomous Agents; Context Interaction; Domain-ontology Diagram; Feature Diagram; Large Language Models (llms); Multi-agent Collaboration; Radar Chart; Software Architecture; Software-design Rationale; Taxonomy; Autonomous Agents; Computational Linguistics; Multi Agent Systems; Software Architecture; Software Design; Agent Collaboration; Ai System Classification; Ai Systems; Architectural Viewpoints; Context Interaction; Design Rationale; Domain Ontologies; Domain-ontology Diagram; Feature Diagrams; Language Model; Large Language Model; Multi Agent; Multi-agent Collaboration; Radar Chart; Software-design Rationale; System Classification; Taxonomies},
	keywords = {Autonomous agents; Computational linguistics; Multi agent systems; Software architecture; Software design; Agent collaboration; AI system classification; AI systems; Architectural viewpoints; Context interaction; Design rationale; Domain ontologies; Domain-ontology diagram; Feature diagrams; Language model; Large language model; Multi agent; Multi-agent collaboration; Radar chart; Software-design rationale; System classification; Taxonomies},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Funk2023,
	author = {Funk, Maurice and Hosemann, Simon and Jung, Jean Christoph and Lutz, Carsten},
	title = {Towards Ontology Construction with Language Models},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3577},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179559244&partnerID=40&md5=33969446bee8becffb8a2e0211cb3651},
	abstract = {We present a method for automatically constructing a concept hierarchy for a given domain by querying a large language model. We apply this method to various domains using OpenAI’s GPT 3.5. Our experiments indicate that LLMs can be of considerable help for constructing concept hierarchies. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Hierarchies; Language Model; Ontology Construction; Computational Linguistics},
	keywords = {Concept hierarchies; Language model; Ontology construction; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@CONFERENCE{Menad202373,
	author = {Menad, Safaa and Laddada, Wissame and Abdeddaïm, Saïd and Soualmia, Lina F.},
	title = {BioSTransformers for Biomedical Ontologies Alignment},
	year = {2023},
	journal = {International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K - Proceedings},
	volume = {2},
	pages = {73 - 84},
	doi = {10.5220/0012188600003598},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179555795&doi=10.5220%2F0012188600003598&partnerID=40&md5=d79a910980a3dfdd47be64c74fc445de},
	abstract = {This paper aims at describing the new siamese neural models that we have developed. They optimize a self supervised contrastive learning function on scientific biomedical literature articles. The results obtained on several benchmarks show that the proposed models are able to improve various biomedical tasks without examples (zero shot) and are comparable to biomedical transformers fine-tuned on supervised data specific to the problems addressed. Moreover, these new siamese models are exploited to align biomedical ontologies, demonstrating their semantic mapping capabilities. We then compare the different approaches of alignments that we have proposed. In conclusion, we propose a distinct methods and data sources that we evaluate and compare to validate our alignments. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Ontologies; Biomedical Texts; Language Models; Ontology Alignment; Siamese Neural Models; Transformers; Zero-shot Learning; Bioinformatics; Learning Systems; Ontology; Semantics; Biomedical Literature; Biomedical Ontologies; Biomedical Text; Language Model; Learning Functions; Neural Modelling; Ontology Alignment; Semantics Mappings; Siamese Neural Model; Transformer; Zero-shot Learning},
	keywords = {Bioinformatics; Learning systems; Ontology; Semantics; Biomedical literature; Biomedical ontologies; Biomedical text; Language model; Learning functions; Neural modelling; Ontology alignment; Semantics mappings; Siamese neural model; Transformer; Zero-shot learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@CONFERENCE{Lupu2023,
	author = {Lupu, Dan and Groza, Adrian Petru and Pease, Adam},
	title = {Cross-validation of Answers with SUMO and GPT},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3577},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179546948&partnerID=40&md5=19fa9a6d20f4be4ab6edf41544488940},
	abstract = {We have developed a tool for fact-checking in automated question answering based on four technologies: (i) the Suggested Upper Merged Ontology (SUMO) for knowledge representation, (ii) the Vampire theorem prover [1] for fact verification, (iii) WordNet for lexical semantics and (iv) GPT (Generative Pretrained Transformer) for concept learning and alignment. SUMO provides a structured representation of knowledge in an expressive logic, facilitating semantic understanding and analysis. Vampire serves as an automated reasoning tool to check the validity of facts and claims. WordNet and GPT contribute to concept learning and alignment, enhancing the system’s ability to interpret natural language (NL) expressions and align them with the underlying ontological representations. By combining these components, the proposed framework offers a robust solution for fact-checking, combating misinformation, and promoting informed decision-making. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Fake News; Foundational Language Models; Foundational Ontologies; Generative Pre-trained Transformers; Decision Making; Knowledge Representation; Semantics; Concept Alignments; Concept Learning; Cross Validation; Fake News; Foundational Language Model; Foundational Ontologies; Generative Pre-trained Transformer; Language Model; Ontology's; Wordnet; Ontology},
	keywords = {Decision making; Knowledge representation; Semantics; Concept alignments; Concept learning; Cross validation; Fake news; Foundational language model; Foundational ontologies; Generative pre-trained transformer; Language model; Ontology's; Wordnet; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Hristov2023519,
	author = {Hristov, Anton and Ivanov, Petar and Aksenova, Anna and Asamov, Tsvetan and Gyurov, Pavlin and Primov, Todor and Boytcheva, Svetla},
	title = {Clinical Text Classification to SNOMED CT Codes using Transformers Trained on Linked Open Medical Ontologies},
	year = {2023},
	journal = {International Conference Recent Advances in Natural Language Processing, RANLP},
	pages = {519 - 526},
	doi = {10.26615/978-954-452-092-2_057},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179181038&doi=10.26615%2F978-954-452-092-2_057&partnerID=40&md5=7a737a8b92178dc64aa2289c3c4a7443},
	abstract = {We present an approach for medical text coding with SNOMED CT. Our approach uses publicly available linked open data from terminologies and ontologies as training data for the algorithms. We claim that even small training corpora made of short text snippets can be used to train models for the given task. We propose a method based on transformers enhanced with clustering and filtering of the candidates. Further, we adopt a classical machine learning approach - support vector classification (SVC) using the transformer embeddings. The resulting approach proves to be more accurate than the predictions given by Large Language Models. We evaluate on a dataset generated from linked open data for SNOMED codes related to morphology and topography for four use cases. Our transformers-based approach achieves an F1-score of 0.82 for morphology and 0.99 for topography codes. Further, we validate the applicability of our approach in a clinical context using labelled real clinical data that are not used for model training. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Linked Data; Ontology; Open Data; Text Processing; Topography; Clinical Text Classifications; Linked Open Datum; Medical Ontology; Ontology's; Short Texts; Small Training; Snomed-ct; Text Snippets; Training Corpus; Training Data; Classification (of Information)},
	keywords = {Linked data; Ontology; Open Data; Text processing; Topography; Clinical text classifications; Linked open datum; Medical ontology; Ontology's; Short texts; Small training; SNOMED-CT; Text snippets; Training corpus; Training data; Classification (of information)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Bronze Open Access}
}

@CONFERENCE{Phokela20231846,
	author = {Phokela, Kanchanjot Kaur and Sikand, Samarth and Singi, Kapil and Dey, Kuntal and Sharma, Vibhu Saujanya and Kaulgud, Vikrant S.},
	title = {Smart Prompt Advisor: Multi-Objective Prompt Framework for Consistency and Best Practices},
	year = {2023},
	pages = {1846 - 1848},
	doi = {10.1109/ASE56229.2023.00019},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179012657&doi=10.1109%2FASE56229.2023.00019&partnerID=40&md5=2c0579db926c8993668155fac67ca11d},
	abstract = {Recent breakthroughs in Large Language Models (LLM), comprised of billions of parameters, have achieved the ability to unveil exceptional insight into a wide range of Natural Language Processing (NLP) tasks. The onus of the performance of these models lies in the sophistication and completeness of the input prompt. Minimizing the enhancement cycles of prompt with improvised keywords becomes critically important as it directly affects the time to market and cost of the developing solution. However, this process inevitably has a trade-off between the learning curve/proficiency of the user and completeness of the prompt, as generating such a solutions is an incremental process. In this paper, we have designed a novel solution and implemented it in the form of a plugin for Visual Studio Code IDE, which can optimize this trade-off, by learning the underlying prompt intent to enhance with keywords. This will tend to align with developers' collection of semantics while developing a secure code, ensuring parameter and local variable names, return expressions, simple pre and post-conditions. and basic control and data flow are met. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Deep Learning; Llm; Ontology; Prompt Engineering; Codes (symbols); Deep Learning; Economic And Social Effects; Natural Language Processing Systems; Best Practices; Language Model; Language Processing; Large Language Model; Multi Objective; Natural Languages; Ontology's; Prompt Engineering; Trade Off; Semantics},
	keywords = {Codes (symbols); Deep learning; Economic and social effects; Natural language processing systems; Best practices; Language model; Language processing; Large language model; Multi objective; Natural languages; Ontology's; Prompt engineering; Trade off; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@CONFERENCE{Bruno2023,
	author = {Bruno, Alessandro and Pipitone, Arianna and Manzotti, Riccardo and Augello, Agnese and Mazzeo, Pier Luigi and Vella, Filippo and Chella, Antonio},
	title = {AIxPAC 2023 - Preface to the 1st Workshop on Artificial Intelligence for Perception and Artificial Consciousness},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3563},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178666040&partnerID=40&md5=a6689066d6d770628df0ffc62ddc4042},
	abstract = {The AIxPAC workshop aims to bring together researchers from academia and industry to discuss the latest advancements in AI for perception and consciousness. The workshop features presentations from experts on the physicalist ontology of consciousness, artificial consciousness, colour perception, and computer vision. Some research questions are addressed in AIxPAC: Can a visual perception system be embedded into machines? How accurately does AI tackle visual attention processes? What is the relation between attention and consciousness? Can AI architectures and approaches be used to design Artificial Consciousness? What are the pros and cons of Large Language Models? The given research questions foster multidisciplinary contributions and several critical readings for the given topics. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Behavioral Research; Color Vision; Artificial Consciousness; Color Computers; Colour Perception; Language Model; Ontology's; Perception Systems; Research Questions; Visual Attention; Visual Perception; Artificial Intelligence},
	keywords = {Behavioral research; Color vision; Artificial consciousness; Color computers; Colour perception; Language model; Ontology's; Perception systems; Research questions; Visual Attention; Visual perception; Artificial intelligence},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Schulz202336,
	author = {Schulz, Stefan and Del-Pinto, Warren and Han, Lifeng and Kreuzthaler, Markus and Aghaei, Sareh and Nenadic, Goran},
	title = {Towards principles of ontology-based annotation of clinical narratives},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3603},
	pages = {36 - 47},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178585442&partnerID=40&md5=00f37d20b44ee07a2ff64d86ac809e06},
	abstract = {Despite the increasing availability of ontology-based semantic resources for biomedical content representation, large amounts of clinical data are in narrative form only. Therefore, many clinical information management tasks require to unlock this information using natural language processing (NLP). Clinical corpora annotated by humans are crucial resources. On the one hand, they are needed to train and domain-fine-tune language models with the goal to transform information from unstructured free text into an interoperable form. On the other hand, manually annotated corpora are indispensable for assessing the results of information extraction using NLP. Annotation quality is crucial. Therefore, detailed annotation guidelines are needed to define the form that extracted information should take, to prevent human annotators from making erratic annotation decisions and to guarantee a good inter-annotator agreement. Our hypothesis is that, to this end, human annotations (and subsequently machine annotations learned from human annotations) should (i) be based on ontological principles, and (ii) be consistent with existing clinical documentation standards. With the experience of several annotation projects, we highlight the need for sophisticated guidelines. We formulate a set of abstract principles on which such guidelines should be based, followed by examples of how to keep them, on the one hand, user-friendly and consistent, and on the other hand compatible with the international semantic standards SNOMED CT and FHIR, including their areas of overlap. We sketch the representation of the resulting representations in a knowledge graph as a state-of-the-art semantic representation paradigm, which can be enriched by additional content on A-Box and T-Box levels and on which symbolic and neural reasoning tasks can be applied. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Information Models; Electronic Health Records; Formal Ontologies; Natural Language Processing; Text Annotation Guidelines; Abstracting; Information Management; Information Theory; Information Use; Natural Language Processing Systems; Ontology; Clinical Information; Clinical Information Model; Electronic Health; Electronic Health Record; Formal Ontology; Health Records; Information Modeling; Language Processing; Natural Language Processing; Natural Languages; Text Annotation Guideline; Text Annotations; Semantics},
	keywords = {Abstracting; Information management; Information theory; Information use; Natural language processing systems; Ontology; Clinical information; Clinical information model; Electronic health; Electronic health record; Formal ontology; Health records; Information Modeling; Language processing; Natural language processing; Natural languages; Text annotation guideline; Text annotations; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Cadeddu2023,
	author = {Cadeddu, Andrea and Chessa, Alessandro and De Leo, Vincenzo and Fenu, Gianni and Motta, Enrico and Osborne, Francesco and Reforgiato Recupero, Diego and Salatino, Angelo Antonio and Secchi, Luca},
	title = {Enhancing Scholarly Understanding: A Comparison of Knowledge Injection Strategies in Large Language Models},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3559},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178584892&partnerID=40&md5=1a1e9863020e62bdd181f45745a1b8ee},
	abstract = {The use of transformer-based models like BERT for natural language processing has achieved remarkable performance across multiple domains. However, these models face challenges when dealing with very specialized domains, such as scientific literature. In this paper, we conduct a comprehensive analysis of knowledge injection strategies for transformers in the scientific domain, evaluating four distinct methods for injecting external knowledge into transformers. We assess these strategies in a single-label multi-class classification task involving scientific papers. For this, we develop a public benchmark based on 12k scientific papers from the AIDA knowledge graph, categorized into three fields. We utilize the Computer Science Ontology as our external knowledge source. Our findings indicate that most proposed knowledge injection techniques outperform the BERT baseline. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Classification Tasks; Feature Engineering; Knowledge Graphs; Natural Language Processing; Classification (of Information); Computational Linguistics; Natural Language Processing Systems; Bert; Classification Tasks; External Knowledge; Feature Engineerings; Knowledge Graphs; Language Model; Language Processing; Natural Language Processing; Natural Languages; Scientific Papers; Knowledge Graph},
	keywords = {Classification (of information); Computational linguistics; Natural language processing systems; BERT; Classification tasks; External knowledge; Feature engineerings; Knowledge graphs; Language model; Language processing; Natural language processing; Natural languages; Scientific papers; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{2023,
	title = {22nd International Conference on Web-based Learning, ICWL 2023},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14409 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178572378&partnerID=40&md5=ea68bd763d1123339ecfb53fa9001bf9},
	abstract = {The proceedings contain 16 papers. The special focus in this conference is on Web-based Learning. The topics include: Intended Learning Outcomes and Taxonomy Mapping at University Level; mixed Reality Learning Visualizations Using Knowledge Graphs; tracking the Adaptive Learning Process with Topics Ontology; prompting Large Language Models to Power Educational Chatbots; motivating Learners with Gamified Chatbot-Assisted Learning Activities; a Transfer Learning Approach Interaction in an Academic Consortium; AI4T: A Teacher’s Dashboard for Visual Rendering of Students’ Assignments in Massive Open Online Courses; corpus-Based Translation Pedagogy: A Preliminary Case Study; investigating Student Profiles Related to Academic Learning Achievement; graduation Project Monitoring Platform Based on a Personalized Supervision Plan; an Online Tutoring and Assessment System for Teaching Relational Algebra in Database Classes; automatic and Authentic eAssessment of Online Database Design Theory Assignments; from Classroom to Metaverse: A Study on Gamified Constructivist Teaching in Higher Education; exploring the Transformative Potential of Virtual Reality in History Education: A Scoping Review. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2023,
	title = {SWODCH 2023 - Proceedings of the International Workshop on Semantic Web and Ontology Design for Cultural Heritage, co-located with the International Semantic Web Conference 2023, ISWC 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3540},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178310394&partnerID=40&md5=4255b7ffbc7ce03459f146fbae0d9af7},
	abstract = {The proceedings contain 12 papers. The topics discussed include: Patterns2KG: JAMS pipeline for modeling music patterns; RCC8 for CIDOC CRM: semantic modeling of mereological and topological spatial relations in Notre-Dame de Paris; a data model for linked stage graph and the historical performing arts domain; an ontology to support decision-making in conservation and restoration interventions of cultural heritage; a comparative study of simple and complex art interpretations in linked open data using ICON ontology; an ontology for creating hypermedia stories over knowledge graphs; a data-driven approach to create an ontology of parliamentary work: case parliament of Finland on the semantic web; semantic data retrieval and integration for supporting artworks interpretation through integrative narrative networks; enhancing entity alignment between Wikidata and ArtGraph using LLMs; publishing and studying historical opera and music theatre performances on the semantic web: case OperaSampo 1830–1960; and why we need ontology-specific data portals: a case study for CIDOC-CRM. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Yu2023339,
	author = {Yu, Shuang and Huang, Tao and Liu, Mingyi and Wang, Zhongjie},
	title = {BEAR: Revolutionizing Service Domain Knowledge Graph Construction with LLM},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14419 LNCS},
	pages = {339 - 346},
	doi = {10.1007/978-3-031-48421-6_23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178217581&doi=10.1007%2F978-3-031-48421-6_23&partnerID=40&md5=eca2c6b924751bc0b3e460491f6a1498},
	abstract = {Knowledge graph (KG), as a novel knowledge storage approach, has been widely used in various domains. In the service computing community, researchers tried to harness the enormous potential of KG to tackle domain-specific tasks. However, the lack of an openly available service domain KG limits the in-depth exploration of KGs in domain-specific applications. Building a service domain KG primarily faces two challenges: first, the diversity and complexity of service domain knowledge, and second, the dispersion of domain knowledge and the lack of annotated data. These challenges discouraged costly investment in large, high-quality domain-specific KGs by researchers. In this paper, we present the construction of a service domain KG called BEAR. We design a comprehensive service domain knowledge ontology to automatically generate the prompts for the Large Language Model (LLM) and employ LLM to implement a zero-shot method to extract high-quality knowledge. A series of experiments are conducted to demonstrate the feasibility of graph construction process and showcase the richness of content available from BEAR. Currently, BEAR includes 133, 906 nodes, 169, 159 relations, and about 424, 000 factual knowledge as attributes, which is available through github.com/HTXone/BEAR. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Construction; Large Language Model; Service Domain Knowledge Graph; Service Domain Ontology; Computational Linguistics; Digital Storage; Domain Knowledge; Ontology; Zero-shot Learning; Domain Knowledge; Domain Ontologies; Graph Construction; Knowledge Graph Construction; Knowledge Graphs; Language Model; Large Language Model; Service Domain; Service Domain Knowledge Graph; Service Domain Ontology; Knowledge Graph},
	keywords = {Computational linguistics; Digital storage; Domain Knowledge; Ontology; Zero-shot learning; Domain knowledge; Domain ontologies; Graph construction; Knowledge graph construction; Knowledge graphs; Language model; Large language model; Service domain; Service domain knowledge graph; Service domain ontology; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14}
}

@ARTICLE{Quevedo20231,
	author = {Quevedo, Ernesto and Cerny, Tomas and Rodriguez, Alejandro and Rivas, Pablo and Yero, Jorge and Sooksatra, Korn and Zhakubayev, Alibek and Taibi, Davide},
	title = {Legal Natural Language Processing from 2015-2022: A Comprehensive Systematic Mapping Study of Advances and Applications},
	year = {2023},
	journal = {IEEE Access},
	pages = {1 - 1},
	doi = {10.1109/ACCESS.2023.3333946},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178053115&doi=10.1109%2FACCESS.2023.3333946&partnerID=40&md5=9f41c71b213c8fe94892597b1be43fad},
	abstract = {The surge in legal text production has amplified the workload for legal professionals, making many tasks repetitive and time-consuming. Furthermore, the complexity and specialized language of legal documents pose challenges not just for those in the legal domain but also for the general public. This emphasizes the potential role and impact of Legal Natural Language Processing (Legal NLP). Although advancements have been made in this domain, particularly after 2015 with the advent of Deep Learning and Large Language Models (LLMs), a systematic exploration of this progress until 2022 is nonexistent. In this research, we perform a Systematic Mapping Study (SMS) to bridge this gap.We aim to provide a descriptive statistical analysis of the Legal NLP research between 2015 and 2022. Categorize and sub-categorize primary publications based on their research problems. Identify limitations and areas of improvement in current research. Using a robust search methodology across four reputable indexers, we filtered 536 papers down to 75 pivotal articles. Our findings reveal the diverse methods employed for tasks such as Multiclass Classification, Summarization, and Question Answering in the Legal NLP field.We also highlight resources, challenges, and gaps in current methodologies and emphasize the need for curated datasets, ontologies, and a focus on inherent difficulties like data accessibility. As the legal sector gradually embraces Natural Language Processing (NLP), understanding the capabilities and limitations of Legal NLP becomes vital for ensuring efficient and ethical application. The research offers insights for both Legal NLP researchers and the broader legal community, advocating for continued advancements in automation while also addressing ethical concerns. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Deep Learning; Information Retrieval; Law; Legal-nlp; Natural Language Processing; Search Problems; Surveys; Systematic-mapping-study; Systematics; Task Analysis; Ethical Technology; Job Analysis; Mapping; Natural Language Processing Systems; Deep Learning; Language Processing; Law; Legal-natural Language Processing; Natural Language Processing; Natural Languages; Search Problem; Systematic; Systematic Mapping Studies; Task Analysis},
	keywords = {Ethical technology; Job analysis; Mapping; Natural language processing systems; Deep learning; Language processing; Law; Legal-natural language processing; Natural language processing; Natural languages; Search problem; Systematic; Systematic mapping studies; Task analysis},
	type = {Article},
	publication_stage = {aip},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Gold Open Access}
}

@ARTICLE{2023,
	title = {24th International Conference on Intelligent Data Engineering and Automated Learning, IDEAL 2023},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14404 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177843125&partnerID=40&md5=74934626a1d75f125e91ff8837b6771f},
	abstract = {The proceedings contain 49 papers. The special focus in this conference is on Intelligent Data Engineering and Automated Learning. The topics include: GPU-Based Acceleration of the Rao Optimization Algorithms: Application to the Solution of Large Systems of Nonlinear Equations; direct Determination of Operational Value-at-Risk Using Descriptive Statistics; using Deep Learning Models to Predict the Electrical Conductivity of the Influent in a Wastewater Treatment Plant; unsupervised Defect Detection for Infrastructure Inspection; Generating Adversarial Examples Using LAD; emotion Extraction from Likert-Scale Questionnaires: – An Additional Dimension to Psychology Instruments –; recent Applications of Pre-aggregation Functions; a Probabilistic Approach: Querying Web Resources in the Presence of Uncertainty; domain Adaptation in Transformer Models: Question Answering of Dutch Government Policies; complexity-Driven Sampling for Bagging; sustainable On-Street Parking Mapping with Deep Learning and Airborne Imagery; hebbian Learning-Guided Random Walks for Enhanced Community Detection in Correlation-Based Brain Networks; extracting Automatically a Domain Ontology from the “Book of Properties” of the Archbishop’s Table of Braga; language Models for Automatic Distribution of Review Notes in Movie Production; extracting Knowledge from Incompletely Known Models; threshold-Based Classification to Enhance Confidence in Open Set of Legal Texts; comparing Ranking Learning Algorithms for Information Retrieval Systems; analyzing the Influence of Market Event Correction for Forecasting Stock Prices Using Recurrent Neural Networks; measuring the Relationship Between the Use of Typical Manosphere Discourse and the Engagement of a User with the Pick-Up Artist Community; uniform Design of Experiments for Equality Constraints; a Pseudo-Label Guided Hybrid Approach for Unsupervised Domain Adaptation; globular Cluster Detection in M33 Using Multiple Views Representation Learning; segmentation of Brachial Plexus Ultrasound Images Based on Modified SegNet Model; Unsupervised Online Event Ranking for IT Operations; A Subgraph Embedded GIN with Attention for Graph Classification; a Machine Learning Approach to Predict Cyclists’ Functional Threshold Power; combining Regular Expressions and Supervised Algorithms for Clinical Text Classification. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Thant2023261,
	author = {Thant, Shin and Racharak, Teeradaj and Andrès, Frédéric},
	title = {BERT Fine-Tuning the Covid-19 Open Research Dataset for Named Entity Recognition},
	year = {2023},
	journal = {Communications in Computer and Information Science},
	volume = {1942 CCIS},
	pages = {261 - 275},
	doi = {10.1007/978-981-99-7969-1_19},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177820256&doi=10.1007%2F978-981-99-7969-1_19&partnerID=40&md5=9116aafc44812fd6dc4bdff7a9d8e8f1},
	abstract = {This study employs the widely used Large Language Model (LLM), BERT, to implement Named Entity Recognition (NER) on the CORD-19 biomedical literature corpus. By fine-tuning the pre-trained BERT on the CORD-NER dataset, the model gains the ability to comprehend the context and semantics of biomedical named entities. The refined model is then utilized on the CORD-19 to extract more contextually relevant and updated named entities. However, fine-tuning large datasets with LLMs poses a challenge. To counter this, two distinct sampling methodologies are proposed to apply on each dataset. First, for the NER task on the CORD-19, a Latent Dirichlet Allocation (LDA) topic modeling technique is employed. This maintains the sentence structure while concentrating on related content. Second, a straightforward greedy method is deployed to gather the most informative data of 25 entity types from the CORD-NER dataset. The study realizes its goals by demonstrating the content comprehension capability of BERT-based models without the necessity of supercomputers, and converting the document-level corpus into a source for NER data, enhancing data accessibility. The outcomes of this research can shed light on the potential progression of more sophisticated NLP applications across various sectors, including knowledge graph creation, ontology learning, and conversational AI. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Cord-19; Cord-ner; Dataset Sampling; Document Level Entity Extraction; Large Language Models; Ner; Computational Linguistics; Large Dataset; Natural Language Processing Systems; Semantics; Statistics; Bert; Cord-19; Cord-named Entity Recognition; Dataset Sampling; Document Level Entity Extraction; Entity Extractions; Language Model; Large Language Model; Named Entity Recognition; Supercomputers},
	keywords = {Computational linguistics; Large dataset; Natural language processing systems; Semantics; Statistics; BERT; CORD-19; CORD-named entity recognition; Dataset sampling; Document level entity extraction; Entity extractions; Language model; Large language model; Named entity recognition; Supercomputers},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Mihindukulasooriya2023247,
	author = {Mihindukulasooriya, Nandana and Tiwari, Sanju M. and Enguix, Carlos F. and Lata, Kusum},
	title = {Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14266 LNCS},
	pages = {247 - 265},
	doi = {10.1007/978-3-031-47243-5_14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177480752&doi=10.1007%2F978-3-031-47243-5_14&partnerID=40&md5=f601274a6ee385854119e6b6494454a7},
	abstract = {The recent advances in large language models (LLM) and foundation models with emergent capabilities have been shown to improve the performance of many NLP tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs can be used for KG construction or completion while existing KGs can be used for different tasks such as making LLM outputs explainable or fact-checking in Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to evaluate the capabilities of language models to generate KGs from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19 ontologies and 4,860 sentences. We define seven evaluation metrics to measure fact extraction performance, ontology conformance, and hallucinations by LLMs. Furthermore, we provide results for two baseline models, Vicuna-13B and Alpaca-LoRA-13B using automatic prompt generation from test cases. The baseline results show that there is room for improvement using both Semantic Web and Natural Language Processing techniques. Resource Type: Evaluation Benchmark Source Repo: https://github.com/cenguix/Text2KGBench DOI: https://doi.org/10.5281/zenodo.7916716 License: Creative Commons Attribution (CC BY 4.0) © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmark; Knowledge Graph; Knowledge Graph Generation; Large Language Models; Relation Extraction; Computational Linguistics; Extraction; Http; Natural Language Processing Systems; Ontology; Benchmark; Graph Generation; Knowledge Graph Generation; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Performance; Relation Extraction; Knowledge Graph},
	keywords = {Computational linguistics; Extraction; HTTP; Natural language processing systems; Ontology; Benchmark; Graph generation; Knowledge graph generation; Knowledge graphs; Language model; Large language model; Ontology's; Performance; Relation extraction; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 43}
}

@ARTICLE{Raggett202344,
	author = {Raggett, Dave},
	title = {Defeasible Reasoning with Knowledge Graphs},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14382 LNCS},
	pages = {44 - 51},
	doi = {10.1007/978-3-031-47745-4_4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177474902&doi=10.1007%2F978-3-031-47745-4_4&partnerID=40&md5=671697a28f7da6ff36f4d6c056adc6bc},
	abstract = {Human knowledge is subject to uncertainties, imprecision, incompleteness and inconsistencies. Moreover, the meaning of many everyday terms is dependent on the context. That poses a huge challenge for the Semantic Web. This paper introduces work on an intuitive notation and model for defeasible reasoning with imperfect knowledge, and relates it to previous work on argumentation theory. PKN is to N3 as defeasible reasoning is to deductive logic. Further work is needed on an intuitive syntax for describing reasoning strategies and tactics in declarative terms, drawing upon the AIF ontology for inspiration. The paper closes with observations on symbolic approaches in the era of large language models. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Argumentation Theory; Defeasible Reasoning; Knowledge Graphs; Argumentation Theory; Deductive Logic; Defeasible Reasoning; Further Works; Human Knowledge; Incompleteness And Inconsistency; Knowledge Graphs; Language Model; Ontology's; Uncertainty; Knowledge Graph},
	keywords = {Argumentation theory; Deductive logic; Defeasible reasoning; Further works; Human knowledge; Incompleteness and inconsistency; Knowledge graphs; Language model; Ontology's; Uncertainty; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Saba20233,
	author = {Saba, Walid S.},
	title = {Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14320 LNCS},
	pages = {3 - 19},
	doi = {10.1007/978-3-031-47262-6_1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177460103&doi=10.1007%2F978-3-031-47262-6_1&partnerID=40&md5=ea4695ea4f1cbff9514c4268ddcc2544},
	abstract = {In our opinion the exuberance surrounding the relative success of data-driven large language models (LLMs) is slightly misguided and for several reasons (i) LLMs cannot be relied upon for factual information since for LLMs all ingested text (factual or non-factual) was created equal; (ii) due to their subsymbolic nature, whatever ‘knowledge’ these models acquire about language will always be buried in billions of microfeatures (weights), none of which is meaningful on its own; and (iii) LLMs will often fail to make the correct inferences in several linguistic contexts (e.g., nominal compounds, copredication, quantifier scope ambiguities, intensional contexts). Since we believe the relative success of data-driven large language models (LLMs) is not a reflection on the symbolic vs. subsymbolic debate but a reflection on applying the successful strategy of a bottom-up reverse engineering of language at scale, we suggest in this paper applying the effective bottom-up strategy in a symbolic setting resulting in symbolic, explainable, and ontologically grounded language models. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Bottom-up Reverse Engineering Of Language; Language Agnostic Ontology; Symbolic Large Language Models; Computational Linguistics; Reverse Engineering; Stochastic Models; Stochastic Systems; Bottom Up; Bottom-up Reverse Engineering Of Language; Data Driven; Factual Information; Language Agnostic Ontology; Language Model; Ontology's; Stochastics; Sub-symbolic; Symbolic Large Language Model; Ontology},
	keywords = {Computational linguistics; Reverse engineering; Stochastic models; Stochastic systems; Bottom up; Bottom-up reverse engineering of language; Data driven; Factual information; Language agnostic ontology; Language model; Ontology's; Stochastics; Sub-symbolic; Symbolic large language model; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 22}
}

@ARTICLE{2023,
	title = {22nd International Semantic Web Conference, ISWC 2023},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14266 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177448280&partnerID=40&md5=5a6a808bf67cfb68607e7e3cb9533d1f},
	abstract = {The proceedings contain 58 papers. The special focus in this conference is on Semantic Web. The topics include: How is Your Knowledge Graph Used: Content-Centric Analysis of SPARQL Query Logs; iterative Geographic Entity Alignment with Cross-Attention; entity-Relation Distribution-Aware Negative Sampling for Knowledge Graph Embedding; negative Sampling with Adaptive Denoising Mixup for Knowledge Graph Embedding; comparison of Knowledge Graph Representations for Consumer Scenarios; a Comprehensive Study on Knowledge Graph Embedding over Relational Patterns Based on Rule Learning; Compact Encoding of Reified Triples Using HDTr; causal Inference-Based Debiasing Framework for Knowledge Graph Completion; Can ChatGPT Replace Traditional KBQA Models? An In-Depth Analysis of the Question Answering Performance of the GPT LLM Family; Dense Re-Ranking with Weak Supervision for RDF Dataset Search; mapping and Cleaning Open Commonsense Knowledge Bases with Generative Translation; integrating Knowledge Graph Embeddings and Pre-trained Language Models in Hypercomplex Spaces; LLMs4OL: Large Language Models for Ontology Learning; biomedical Knowledge Graph Embeddings with Negative Statements; knowledge Graph Enhanced Language Models for Sentiment Analysis; TemporalFC: A Temporal Fact Checking Approach over Knowledge Graphs; Assessing the Generalization Capabilities of Neural Machine Translation Models for SPARQL Query Generation; linking Tabular Columns to Unseen Ontologies; neural Multi-hop Logical Query Answering with Concept-Level Answers; ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs; Optimizing SPARQL Queries with SHACL; SORBET: A Siamese Network for Ontology Embeddings Using a Distance-Based Regression Loss and BERT; visualizing Mappings Between Pairwise Ontologies - An Empirical Study of Matrix and Linked Indented List in Their User Support During Class Mapping Creation and Evaluation; FeaBI: A Feature Selection-Based Framework for Interpreting KG Embeddings; CapsKG: Enabling Continual Knowledge Integration in Language Models for Automatic Knowledge Graph Completion; HAEE: Low-Resource Event Detection with Hierarchy-Aware Event Graph Embeddings; textual Entailment for Effective Triple Validation in Object Prediction. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Rodrigues2023249,
	author = {Rodrigues, Fabrício Henrique Henrique and Lopes, Alcides Gonçalves and Dos Santos, Nicolau O. and Garcia, Luan Fonseca and Carbonera, Joel Lúis and Abel, Mara},
	title = {On the Use of ChatGPT for Classifying Domain Terms According to Upper Ontologies},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14319 LNCS},
	pages = {249 - 258},
	doi = {10.1007/978-3-031-47112-4_24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177226095&doi=10.1007%2F978-3-031-47112-4_24&partnerID=40&md5=dca995c6f4355d84ee7c1ba5820fa9bd},
	abstract = {In this paper, we report an experiment to investigate the performance of ChatGPT in the task of classifying domain terms according to the categories of upper-level ontologies. The experiment consisted of (1) starting a conversation in ChatGPT with a contextual prompt listing the categories of an upper-level ontology along with their definitions, (2) submitting a follow-up prompt with a list of terms from a domain along with informal definitions, (3) asking ChatGPT to classify the terms according to the categories of the chosen upper-level ontology and explain its decision, and (4) comparing the answers of ChatGPT with the classification proposed by experts in the chosen ontology. Given the results, we evaluated the success rate of ChatGPT in performing the task and analyzed the cases of misclassification to understand the possible reasons underlying them. Based on that, we made some considerations about the extent to which we can employ ChatGPT as an assistant tool for the task of classifying domain terms into upper-level ontologies. For our experiment, we selected a set of 19 terms from the manufacturing domain that were gathered by the Industrial Ontologies Foundry (IOF) and for which there are informal textual definitions reflecting a community view of them. Also, as a baseline for comparison, we resorted to publicly available classifications of such terms according to DOLCE and BFO upper-level ontologies, which resulted from a thorough ontological analysis of those terms and informal definitions by experts in each of the ontologies. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Bfo; Chatgpt; Dolce; Industrial Ontologies Foundry; Iof; Large Language Models; Llms; Manufacturing; Ontologies; Term Classification; Foundries; Bfo; Chatgpt; Dolce; Industrial Ontology Foundry; Language Model; Large Language Model; Llm; Manufacturing; Ontology's; Term Classification; Ontology},
	keywords = {Foundries; BFO; ChatGPT; DOLCE; Industrial ontology foundry; Language model; Large language model; LLM; Manufacturing; Ontology's; Term classification; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{2023,
	title = {22nd International Semantic Web Conference, ISWC 2023},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14265 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177195187&partnerID=40&md5=8b40c4b8d53d8783594689275fc7e0ed},
	abstract = {The proceedings contain 58 papers. The special focus in this conference is on Semantic Web. The topics include: How is Your Knowledge Graph Used: Content-Centric Analysis of SPARQL Query Logs; iterative Geographic Entity Alignment with Cross-Attention; entity-Relation Distribution-Aware Negative Sampling for Knowledge Graph Embedding; negative Sampling with Adaptive Denoising Mixup for Knowledge Graph Embedding; comparison of Knowledge Graph Representations for Consumer Scenarios; a Comprehensive Study on Knowledge Graph Embedding over Relational Patterns Based on Rule Learning; Compact Encoding of Reified Triples Using HDTr; causal Inference-Based Debiasing Framework for Knowledge Graph Completion; Can ChatGPT Replace Traditional KBQA Models? An In-Depth Analysis of the Question Answering Performance of the GPT LLM Family; Dense Re-Ranking with Weak Supervision for RDF Dataset Search; mapping and Cleaning Open Commonsense Knowledge Bases with Generative Translation; integrating Knowledge Graph Embeddings and Pre-trained Language Models in Hypercomplex Spaces; LLMs4OL: Large Language Models for Ontology Learning; biomedical Knowledge Graph Embeddings with Negative Statements; knowledge Graph Enhanced Language Models for Sentiment Analysis; TemporalFC: A Temporal Fact Checking Approach over Knowledge Graphs; Assessing the Generalization Capabilities of Neural Machine Translation Models for SPARQL Query Generation; linking Tabular Columns to Unseen Ontologies; neural Multi-hop Logical Query Answering with Concept-Level Answers; ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs; Optimizing SPARQL Queries with SHACL; SORBET: A Siamese Network for Ontology Embeddings Using a Distance-Based Regression Loss and BERT; visualizing Mappings Between Pairwise Ontologies - An Empirical Study of Matrix and Linked Indented List in Their User Support During Class Mapping Creation and Evaluation; FeaBI: A Feature Selection-Based Framework for Interpreting KG Embeddings; CapsKG: Enabling Continual Knowledge Integration in Language Models for Automatic Knowledge Graph Completion; HAEE: Low-Resource Event Detection with Hierarchy-Aware Event Graph Embeddings; textual Entailment for Effective Triple Validation in Object Prediction. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Giglou2023408,
	author = {Giglou, Hamed Babaei and D’Souza, Jennifer and Auer, Sören},
	title = {LLMs4OL: Large Language Models for Ontology Learning},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14265 LNCS},
	pages = {408 - 427},
	doi = {10.1007/978-3-031-47240-4_22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177183557&doi=10.1007%2F978-3-031-47240-4_22&partnerID=40&md5=0304490aebf9532fcf8f18d22aa760bf},
	abstract = {We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text? To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS. The obtained empirical results show that foundational LLMs are not sufficiently suitable for ontology construction that entails a high degree of reasoning skills and domain expertise. Nevertheless, when effectively fine-tuned they just might work as suitable assistants, alleviating the knowledge acquisition bottleneck, for ontology construction. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Large Language Models; Llms; Ontologies; Ontology Learning; Prompt-based Learning; Prompting; Computational Linguistics; Data Mining; Learning Systems; Natural Language Processing Systems; Zero-shot Learning; Language Model; Language Patterns; Large Language Model; Ontology Construction; Ontology Learning; Ontology's; Prompt-based Learning; Prompting; Ontology},
	keywords = {Computational linguistics; Data mining; Learning systems; Natural language processing systems; Zero-shot learning; Language model; Language patterns; Large language model; Ontology construction; Ontology learning; Ontology's; Prompt-based learning; Prompting; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 67}
}

@ARTICLE{Menad202316,
	author = {Menad, Safaa and Laddada, Wissame and Abdeddaïm, Saïd and Soualmia, Lina F.},
	title = {New Siamese Neural Networks for Text Classification and Ontologies Alignment},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {13927 LNCS},
	pages = {16 - 29},
	doi = {10.1007/978-3-031-44355-8_2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177181193&doi=10.1007%2F978-3-031-44355-8_2&partnerID=40&md5=2f88ffa184c46c3f2ea03bd6aeeefa50},
	abstract = {Integrating heterogeneous and complementary data in clinical decision support systems (e.g., electronic health records, drug databases, scientific articles, etc.) could improve the accuracy of these systems. Based on this finding, the PreDiBioOntoL (Predicting Clinical Diagnosis by combining BioMedical Ontologies and Language Models) project aims at developing a computer-aided clinical and predictive diagnosis tool to help clinicians to better handle their patients. This tool will combine deep neural networks trained on heterogeneous data sources and biomedical ontologies. The first obtained results of PreDiBioOntoL are presented in this paper. We propose new siamese neural models (BioSTransformers and BioS-MiniLM) that embed texts to be compared in a vector space and then find their similarities. The models optimize an objective self-supervised contrastive learning function on articles from the scientific literature (MEDLINE bibliographic database) associated with their MeSH (Medical Subject Headings) keywords. The obtained results on several benchmarks show that the proposed models can solve different biomedical tasks without examples (zero-shot). These results are comparable to those of other biomedical transformers that are fine-tuned on supervised data specific to the problems being addressed. Moreover, we show in this paper how these new siamese models are exploited in order to semantically map entities from several biomedical ontologies. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Ontologies; Biomedical Texts; Contrastive Learning; Language Models; Ontology Alignment; Siamese Neural Networks; Transformers; Zero-shot Learning; Classification (of Information); Computational Linguistics; Computer Aided Diagnosis; Decision Support Systems; Deep Neural Networks; Information Services; Learning Systems; Medical Computing; Ontology; Text Processing; Biomedical Ontologies; Biomedical Text; Contrastive Learning; Heterogeneous Data; Language Model; Neural-networks; Ontology Alignment; Siamese Neural Network; Text Classification; Transformer; Vector Spaces},
	keywords = {Classification (of information); Computational linguistics; Computer aided diagnosis; Decision support systems; Deep neural networks; Information services; Learning systems; Medical computing; Ontology; Text processing; Biomedical ontologies; Biomedical text; Contrastive learning; Heterogeneous data; Language model; Neural-networks; Ontology alignment; Siamese neural network; Text classification; Transformer; Vector spaces},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Gosselin2023561,
	author = {Gosselin, Francis and Zouaq, Amal},
	title = {SORBET: A Siamese Network for Ontology Embeddings Using a Distance-Based Regression Loss and BERT},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14265 LNCS},
	pages = {561 - 578},
	doi = {10.1007/978-3-031-47240-4_30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177180876&doi=10.1007%2F978-3-031-47240-4_30&partnerID=40&md5=ed21d1f517c0dcc3e37a690e143f6bf1},
	abstract = {Ontology embedding methods have been popular in recent years, especially when it comes to representation learning algorithms for solving ontology-related tasks. Despite the impact of large language models on knowledge graphs’ related tasks, there has been less focus on adapting these models to construct ontology embeddings that are both semantically relevant and faithful to the ontological structure. In this paper, we present a novel ontology embedding method that encodes ontology classes into a pre-trained SBERT through random walks and then fine-tunes the embeddings using a distance-based regression loss. We benchmark our algorithm on four different datasets across two tasks and show the impact of transfer learning and our distance-based loss on the quality of the embeddings. Our results show that SORBET outperform state-of-the-art ontology embedding techniques for the performed tasks. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Ontology; Ontology Embedding; Representation Learning; Sentence Bert; Transfer Learning; Benchmarking; Knowledge Graph; Ontology; Bert; Distance-based; Embedding Method; Embeddings; Ontology Embedding; Ontology's; Representation Learning; Sentence Bert; Transfer Learning},
	keywords = {Benchmarking; Knowledge graph; Ontology; BERT; Distance-based; Embedding method; Embeddings; Ontology embedding; Ontology's; Representation learning; Sentence BERT; Transfer learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@CONFERENCE{2023,
	title = {SEMPDS 2023 - Proceedings of the Posters and Demo Track of the 19th International Conference on Semantic Systems, co-located with 19th International Conference on Semantic Systems, SEMANTiCS 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3526},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176562472&partnerID=40&md5=9fa6810968538af5fcc8f972422a755d},
	abstract = {The proceedings contain 9 papers. The topics discussed include: a framework generate, store, and publish FAIR data in experimental sciences; a mapping lifecycle for public procurement data; a toolset for normative interpretations in FLINT; developing a scalable benchmark for assessing large language models in knowledge graph engineering; enhancing interpretability of machine learning models over knowledge graphs; OntoAnon: an anonymizer for sharing ontology structure without data; SPARQLGEN: one-shot prompt-based approach for SPARQL query generation; and towards assessing FAIR research software best practices in an organization using RDF-star. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2023,
	title = {Proceedings of the AISB Convention 2023},
	year = {2023},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176314091&partnerID=40&md5=45b96d8c77e6cf260404b793a4cd50a9},
	abstract = {The proceedings contain 19 papers. The topics discussed include: a meta-semantics fit for large language models; holding large language models to account; intelligence, super-intelligence, superintelligence++, and ChatGPT: searching for substance amidst the hype; from epistemology to ethics of deep networks; my belief or Alexa’s? belief attribution and AI extension; who needs needy machine consciousness?; NLP based framework for recommending candidate ontologies for reuse; trust in cognitive models: understandability and computational Reliabilism; local minima drive communications in cooperative interaction; exploring the impact of external factors on ride-hailing demand: a predictive modelling approach; evolving time-dependent cognitive models; and switchable lightweight anti-symmetric processing (SLAP) with CNN outspeeds data augmentation by smaller sample - application in Gomoku reinforcement learning. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhang202368,
	author = {Zhang, Xinxin and Ding, Jingjia},
	title = {Technical Principles and Process Innovations of Generative Intelligent Publishing; 生成式智能出版的技术原理与流程革新},
	year = {2023},
	journal = {Documentation, Information and Knowledge},
	volume = {40},
	number = {5},
	pages = {68 - 76},
	doi = {10.13366/j.dik.2023.05.068},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176009936&doi=10.13366%2Fj.dik.2023.05.068&partnerID=40&md5=2e08c394d3c933b24758ff1b5d0cd7af},
	abstract = {[Purpose/Significance] This study aims to explore how generative artificial intelligence（AI）technology empowers the publishing industry to form the principles and process innovations of generative intelligent publishing. Based on the categorical structure of analytical intelligent publishing and generative intelligent publishing, this study investigates the issues of process innovations of generative intelligent publishing in four aspects of content production, content review, intelligent printing, and intelligent marketing. [Design/Methodology] From the perspective of ChatGPT, this study analyzes the ontological attribute definitions of ChatGPT in three aspects: generative AI, cognitive intelligence, and artificial general intelligence.It reveals three key technical principles: massive data advantage, reinforcement learning algorithm advantage, and continuous supercomputing power advantage. Finally, it discusses the process innovations of intelligent publishing based on generative AI. [Findings/Conclusion] The implications of generative AI in the publishing process innovations include the transition from professional generated content to AI generated content, the transition from manual editing to human-machine collaborative intelligent editing, to innovations in printing scenarios such as on-demand printing and intelligent printing factories based on generative AI, and to changes in marketing data, capabilities, efficiency and quality based on generative AI. [Originality/ Value] This paper defines the ontological results of generative AI, cognitive intelligence and artificial general intelligence stages for the products of optimizing language models for dialogue represented by ChatGPT. It proposes new characteristics, new laws and new ideas for generative intelligent publishing in the stages of planning, editing, printing and distribution and expands the research scope of digital publishing and intelligent publishing to some extent. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Aigc（artificial Intelligence Generated Content）; Chatgpt; Digital Publishing; Generative Ai; Intelligent Publishing; Metaverse Publishing; Smart Publishing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Baldazzi202386,
	author = {Baldazzi, Teodoro and Bellomarini, Luigi and Ceri, Stefano and Colombo, Andrea and Gentili, Andrea and Sallinger, Emanuel},
	title = {Fine-Tuning Large Enterprise Language Models via Ontological Reasoning},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14244 LNCS},
	pages = {86 - 94},
	doi = {10.1007/978-3-031-45072-3_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175989701&doi=10.1007%2F978-3-031-45072-3_6&partnerID=40&md5=83fb65e9b88906672e5f9f2504381329},
	abstract = {Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to diverse goals, thanks to task-specific training data. Task specificity should go hand in hand with domain orientation, that is, the specialization of an LLM to accurately address the tasks of a given realm of interest. However, models are usually fine-tuned over publicly available data or, at most, over ground data from databases, ignoring business-level definitions and domain experience. On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and augment such domain knowledge via ontological reasoning. With the goal of combining LLM flexibility with the domain orientation of EKGs, we propose a novel neurosymbolic architecture that leverages the power of ontological reasoning to build task- and domain-specific corpora for LLM fine-tuning. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graphs; Language Models; Ontological Reasoning; Computational Linguistics; Domain Knowledge; Ontology; Domain Knowledge; Domain Orientation; Fine Tuning; Ground Data; Knowledge Graphs; Language Model; Large Enterprise; Ontological Reasoning; Specialisation; Training Data; Knowledge Graph},
	keywords = {Computational linguistics; Domain Knowledge; Ontology; Domain knowledge; Domain orientation; Fine tuning; Ground data; Knowledge graphs; Language model; Large enterprise; Ontological reasoning; Specialisation; Training data; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@ARTICLE{Pitarch2023213,
	author = {Pitarch, Lucía},
	title = {Metaphor Processing in the Medical Domain via Linked Data and Language Models},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {13998 LNCS},
	pages = {213 - 223},
	doi = {10.1007/978-3-031-43458-7_40},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175983396&doi=10.1007%2F978-3-031-43458-7_40&partnerID=40&md5=be3f1f53946171aa5d47930d10cb0a6f},
	abstract = {This thesis proposes a hybrid approach that benefits from Natural Language Processing and Semantic Web technologies for computational metaphor processing. Metaphors are linguistic devices that enable us to perceive and express a concept in terms of another similar one. Designing systems that allow their explicit identification and interpretation can highly facilitate communication in sensitive and obscure contexts such as the medical one. This proposal seeks the identification, understanding, generation, and manipulation of metaphors while providing novel datasets and baselines to exploit Languages Models and Linked Data in the context of figurative knowledge. The developed methodologies will be validated by their application into a specific communication tool between cancer patients and healthcare professionals. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Llod; Medical Domain; Metaphors; Ontologies; Ptlms; Data Handling; Natural Language Processing Systems; Semantic Web; Hybrid Approach; Language Model; Language Processing; Linked Datum; Llod; Medical Domains; Metaphor; Natural Language Semantics; Ontology's; Ptlm; Linked Data},
	keywords = {Data handling; Natural language processing systems; Semantic Web; Hybrid approach; Language model; Language processing; Linked datum; LLOD; Medical domains; Metaphor; Natural language semantics; Ontology's; PTLM; Linked data},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2023,
	title = {20th Extended Semantic Web Conference, ESWC 2023},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {13998 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175965513&partnerID=40&md5=2f734909c53a66c228b68809319185d7},
	abstract = {The proceedings contain 93 papers. The special focus in this conference is on Extended Semantic Web. The topics include: AgriNER: An NER Dataset of Agricultural Entities for the Semantic Web; clayBot: Increasing Human-Centricity in Conversational Recommender Systems; mining Symbolic Rules to Explain Lung Cancer Treatments; GLENDA: Querying RDF Archives with Full SPARQL; Piloting Topic-Aware Research Impact Assessment Features in BIP! Services; explanation-Based Tool for Helping Data Producers to Reduce Privacy Risks; pathWays: Entity-Focused Exploration of Heterogeneous Data Graphs; a Geological Case Study on Semantically Triggered Processes; A System for Repairing $${\mathcal{E}\mathcal{L}}$$ Ontologies Using Weakening and Completing; sparnatural: A Visual Knowledge Graph Exploration Tool; a User Interface Model for Digital Humanities Research: Case BookSampo – Finnish Fiction Literature on the Semantic Web; modeling Grammars with Knowledge Representation Methods: Subcategorization as a Test Case; TRIC: A Triples Corrupter for Knowledge Graphs; ExeKGLib: Knowledge Graphs-Empowered Machine Learning Analytics; hannotate: Flexible Annotation for Text Analytics from Anywhere; study-Buddy: A Knowledge Graph-Powered Learning Companion for School Students; On the Problem of Automatically Aligning Indicators to SDGs; automating Benchmark Generation for Named Entity Recognition and Entity Linking; VRKG-CollaborativeExploration - Data-Driven Discussions in the Metaverse; FOO: An Upper-Level Ontology for the Forest Observatory; integrating Faceted Search with Data Analytic Tools in the User Interface of ParliamentSampo – Parliament of Finland on the Semantic Web; roXi: A Framework for Reactive Reasoning; SummaryGPT: Leveraging ChatGPT for Summarizing Knowledge Graphs; entity Typing with Triples Using Language Models; addressing the Scalability Bottleneck of Semantic Technologies at Bosch; Knowledge Injection to Counter Large Language Model (LLM) Hallucination; towards the Deployment of Knowledge Based Systems in Safety-Critical Systems; a Source-Agnostic Platform for Finding and Exploring Ontologies at Bosch; wisdom of the Sellers: Mining Seller Data for eCommerce Knowledge Graph Generation. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2023,
	title = {7th International Joint Conference on Rules and Reasoning, RuleML+RR 2023},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14244 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175961590&partnerID=40&md5=5fabca150cae87c9c284b4266445d17b},
	abstract = {The proceedings contain 17 papers. The special focus in this conference is on Rules and Reasoning. The topics include: Comparing State of the Art Rule-Based Tools for Information Extraction; a Case Study for Declarative Pattern Mining in Digital Forensics; semantic Role Assisted Natural Language Rule Formalization for Intelligent Vehicle; FreeCHR: An Algebraic Framework for CHR-Embeddings; explaining Optimal Trajectories; abstract Domains for Database Manipulating Processes; extracting Interpretable Hierarchical Rules from Deep Neural Networks’ Latent Space; extension of Regression Tsetlin Machine for Interpretable Uncertainty Assessment; GUCON: A Generic Graph Pattern Based Policy Framework for Usage Control Enforcement; combining Proofs for Description Logic and Concrete Domain Reasoning; notation3 as an Existential Rule Language; fine-Tuning Large Enterprise Language Models via Ontological Reasoning; layerwise Learning of Mixed Conjunctive and Disjunctive Rule Sets; analyzing Termination for Prev-Aware Fragments of Communicating Datalog Programs; marrying Query Rewriting and Knowledge Graph Embeddings. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Chizhikova202391,
	author = {Chizhikova, Mariia and Collado-Montañez, Jaime and Díaz-Galiano, Manuel Carlos and Ureña-López, L. Alfonso Alfonso and Martín-Valdivia, María Teresa},
	title = {Coming a Long Way with Pre-Trained Transformers and String Matching Techniques: Clinical Procedure Mention Recognition and Normalization Notebook for the BioASQ Lab at CLEF 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3497},
	pages = {91 - 101},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175624587&partnerID=40&md5=5aef651a5ee3c4aaee938315b386a4a9},
	abstract = {This paper covers the participation of the SINAI team in the MedProcNER shared task and the BioASQ workshop held on CLEF 2023. The main objective of this challenge is to create systems able to accurately detect and normalize clinical procedure mentions in electronic health Reports. For the named entity recognition (NER) sub-task we compare different ways of processing long sequences: sentence level token classification based on fine-tuning of a RoBERTa model pre-trained on biomedical and clinical data and employing different types of recurrent architectures that rely on non-trainable contextual word embeddings extracted from the same pre-trained language model. In the normalization sub-task, we perform a sequential process that combines literal string matching and embedding similarity search to link each entity found in the previous sub-task with a concept from the SNOMED-CT ontology. Our best-performing system achieves 0.7568 micro-averaged F1 score on the NER sub-task and 0.5267 on the NORM sub-task. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Clinical Nlp; Electronic Health Records; Named Entity Recognition; Natural Language Processing Systems; Clinical Nlp; Clinical Procedure; Electronic Health; Electronic Health Record; Embeddings; Health Records; Named Entity Recognition; Normalisation; String Matching; Subtask},
	keywords = {Natural language processing systems; Clinical NLP; Clinical procedure; Electronic health; Electronic health record; Embeddings; Health records; Named entity recognition; Normalisation; String matching; Subtask},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{2023,
	title = {Joint Workshop Proceedings of the 5th International Workshop on a Semantic Data Space for Transport, Sem4Tra 2023 and 2nd NLP4KGC: Natural Language Processing for Knowledge Graph Construction, co-located with the 19th International Conference on Semantic Systems, SEMANTiCS 2023},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3510},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175614197&partnerID=40&md5=891a8dcd63cc0e6fb6fa6ba6500c5bd1},
	abstract = {The proceedings contain 9 papers. The topics discussed include: policy patterns for usage control in data spaces; an interaction pattern ontology for data sharing about logistics activities; mobility profiles: a taxonomy for the standardization of mobility data spaces; towards aligning IoT data with domain-specific ontologies through semantic web technologies and NLP; probing large language models for scientific synonyms; effects of pretraining corpora on scientific relation extraction using BERT and SciBERT; framing few-shot knowledge graph completion with large language models; similar papers recommendation for research comparisons; and challenges of entity linking in KGQA datasets. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Han202331,
	author = {Han, Lifeng and Erofeev, Gleb and Sorokina, Irina and Gladkoff, Serge and Nenadic, Goran},
	title = {Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning},
	year = {2023},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	pages = {31 - 40},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175455609&partnerID=40&md5=b59ea0ec4995d72e9171365897a30c90},
	abstract = {Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks. This work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning. We carry out an experimental investigation using Meta-AI's MMPLMs "wmt21-dense24-wide-en-X and X-en (WMT21fb)"which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction. We fine-tune these MMPLMs towards English-Spanish language pair which did not exist at all in their original pre-trained corpora both implicitly and explicitly. We prepare carefully aligned clinical domain data for this fine-tuning, which is different from their original mixed domain knowledge. Our experimental result shows that the fine-tuning is very successful using just 250k well-aligned in-domain ENES segments for three sub-task translation testings: clinical cases, clinical terms, and ontology concepts. It achieves very close evaluation scores to another MMPLM NLLB from Meta-AI, which included Spanish as a high-resource setting in the pre-training. To the best of our knowledge, this is the first work on using MMPLMs towards clinical domain transferlearning NMT successfully for totally unseen languages during pre-training. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Computer Aided Language Translation; Domain Knowledge; Learning Systems; Natural Language Processing Systems; Transfer Learning; Well Testing; Domain Machines; Down-stream; Experimental Investigations; Fine Tuning; Language Model; Language Pairs; Machine Translation Models; Machine Translations; Pre-training; Machine Translation},
	keywords = {Computational linguistics; Computer aided language translation; Domain Knowledge; Learning systems; Natural language processing systems; Transfer learning; Well testing; Domain machines; Down-stream; Experimental investigations; Fine tuning; Language model; Language pairs; Machine translation models; Machine translations; Pre-training; Machine translation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Remy202373,
	author = {Remy, François and Khabibullina, Alfiya Marselevna and Demeester, Thomas},
	title = {Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning},
	year = {2023},
	pages = {73 - 80},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175233350&partnerID=40&md5=03002a6d9db081d565e9ea8059e4be94},
	abstract = {This paper shines a light on the potential of definition-based semantic models for detecting idiomatic and semi-idiomatic multiword expressions (MWEs) in clinical terminology. Our study focuses on biomedical entities defined in the UMLS ontology and aims to help prioritize the translation efforts of these entities. In particular, we develop an effective tool for scoring the idiomaticity of biomedical MWEs based on the degree of similarity between the semantic representations of those MWEs and a weighted average of the representation of their constituents. We achieve this using a biomedical language model trained to produce similar representations for entity names and their definitions, called BioLORD. The importance of this definition-based approach is highlighted by comparing the BioLORD model to two other state-of-the-art biomedical language models based on Transformer: SapBERT and CODER. Our results show that the BioLORD model has a strong ability to identify idiomatic MWEs, not replicated in other models. Our corpus-free idiomaticity estimation helps ontology translators to focus on more challenging MWEs. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Semantics; Terminology; Translation (languages); Clinical Terminology; Degree Of Similarity; Effective Tool; Idiomatics; Language Model; Multi-word Expressions; Ontology's; Semantic Modelling; Semantic Representation; Weighted Averages; Ontology},
	keywords = {Computational linguistics; Semantics; Terminology; Translation (languages); Clinical terminology; Degree of similarity; Effective tool; Idiomatics; Language model; Multi-word expressions; Ontology's; Semantic modelling; Semantic representation; Weighted averages; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2023,
	title = {17th Linguistic Annotation Workshop, LAW 2023 @ ACL 2023 - Proceedings of the Workshop},
	year = {2023},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174853831&partnerID=40&md5=2b0e4b658208802ef06b5dc623c5da85},
	abstract = {The proceedings contain 26 papers. The topics discussed include: medieval social media: manual and automatic annotation of byzantine Greek marginal writing; Orpheus came to his end by being struck by a thunderbolt": annotating events in mythological sequences; difficulties in handling mathematical expressions in universal dependencies; a dataset for physical and abstract plausibility and sources of human disagreement; an active learning pipeline for NLU error detection in conversational agents; multi-layered annotation of conversation-like narratives in German; crowdsourcing on sensitive data with privacy-preserving text rewriting; extending an event-type ontology: adding verbs and classes using fine-tuned LLMs suggestions; temporal and second language influence on intra-annotator agreement and stability in hate speech labelling; annotators-in-the-loop: testing a novel annotation procedure on Italian case law; and annotating decomposition in time: three approaches for again. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Straková202385,
	author = {Straková, Jana and Fučíková, Eva and Hajic, J. and Uresova, Zdenka},
	title = {Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned LLMs Suggestions},
	year = {2023},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	pages = {85 - 95},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174806193&partnerID=40&md5=8eeb6ee14d3a258140ee86884e7c6a1c},
	abstract = {In this project, we have investigated the use of advanced machine learning methods, specifically fine-tuned large language models, for pre-annotating data for a lexical extension task, namely adding descriptive words (verbs) to an existing (but incomplete, as of yet) ontology of event types. Several research questions have been focused on, from the investigation of a possible heuristics to provide at least hints to annotators which verbs to include and which are outside the current version of the ontology, to the possible use of the automatic scores to help the annotators to be more efficient in finding a threshold for identifying verbs that cannot be assigned to any existing class and therefore they are to be used as seeds for a new class. We have also carefully examined the correlation of the automatic scores with the human annotation. While the correlation turned out to be strong, its influence on the annotation proper is modest due to its near linearity, even though the mere fact of such pre-annotation leads to relatively short annotation times. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Learning Systems; 'current; Event Types; Human Annotations; Language Model; Machine Learning Methods; Ontology's; Research Questions; Ontology},
	keywords = {Learning systems; 'current; Event Types; Human annotations; Language model; Machine learning methods; Ontology's; Research questions; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Liu2023614,
	author = {Liu, Yang and Qin, Melissa Xiaohui and Wang, Long and Huang, Chao},
	title = {CCAE: A Corpus of Chinese-Based Asian Englishes},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14303 LNAI},
	pages = {614 - 626},
	doi = {10.1007/978-3-031-44696-2_48},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174746171&doi=10.1007%2F978-3-031-44696-2_48&partnerID=40&md5=c7d891852f92d22cd343a60871d61598},
	abstract = {Language models have been foundations in various scenarios of NLP applications, but it has not been well applied in language variety studies, even for the most popular language like English. This paper represents one of the few initial efforts to utilize the NLP technology in the paradigm of World Englishes, specifically in creating a multi-variety corpus for studying Asian Englishes. We present an overview of the CCAE — Corpus of Chinese-based Asian English, a suite of corpora comprising six Chinese-based Asian English varieties. It is based on 340 million tokens in 448 thousand web documents from six regions. The ontology of data would make the corpus a helpful resource with enormous research potential for Asian Englishes (especially for Chinese Englishes for which there has not been a publicly accessible corpus yet so far) and an ideal source for variety-specific language modeling and downstream tasks, thus setting the stage for NLP-based World Englishes studies. And preliminary experiments on this corpus reveal the practical value of CCAE. Finally, we make CCAE available at https://huggingface.co/datasets/CCAE/CCAE-Corpus. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Data-centric Ai; Language Model; Web Corpora; World English; Computational Linguistics; Http; Modeling Languages; Data Centric; Data-centric Ai; Language Model; Multi-varieties; Ontology's; Publicly Accessible; Research Potential; Web Corpora; Web Document; World English; Natural Language Processing Systems},
	keywords = {Computational linguistics; HTTP; Modeling languages; Data centric; Data-centric AI; Language model; Multi-varieties; Ontology's; Publicly accessible; Research potential; Web Corpora; Web document; World english; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Lefebvre20231,
	author = {Lefebvre, Clément and Stoehr, Niklas},
	title = {Rethinking the Event Coding Pipeline with Prompt Entailment},
	year = {2023},
	pages = {1 - 16},
	doi = {10.18653/v1/2023.fever-1.1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174564374&doi=10.18653%2Fv1%2F2023.fever-1.1&partnerID=40&md5=6c0ae0f2237792d073ff76096bdb8bcc},
	abstract = {For monitoring crises, political events are extracted from the news. The large amount of unstructured full-text event descriptions makes a case-by-case analysis unmanageable, particularly for low-resource humanitarian aid organizations. This creates a demand to classify events into event types, a task referred to as event coding. Typically, domain experts craft an event type ontology, annotators label a large dataset and technical experts develop a supervised coding system. In this work, we propose PR-ENT<sup>1</sup>, a new event coding approach that is more flexible and resource-efficient, while maintaining competitive accuracy: first, we extend an event description such as “Military injured two civilians” by a template, e.g. “People were [Z]” and prompt a pre-trained (cloze) language model to fill the slot Z. Second, we select suitable answer candidates Z<sup>∗</sup> = {“injured”, “hurt”...} by treating the event description as premise and the filled templates as hypothesis in a textual entailment task. In a final step, the selected answer candidate can be mapped to its corresponding event type. This allows domain experts to draft the codebook directly as labeled prompts and interpretable answer candidates. This human-in-the-loop process is guided by our codebook design tool<sup>2</sup>. We show that our approach is robust through several checks: perturbing the event description and prompt template, restricting the vocabulary and removing contextual information. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Case Analysis; Domain Experts; Event Description; Event Types; Humanitarian Aids; Large Amounts; Large Datasets; Ontology's; Political Events; Technical Experts; Large Dataset},
	keywords = {Computational linguistics; Case analysis; Domain experts; Event description; Event Types; Humanitarian aids; Large amounts; Large datasets; Ontology's; Political events; Technical experts; Large dataset},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{2023,
	title = {22nd China National Conference on Computational Linguistics, CCL 2023},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14232 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174467034&partnerID=40&md5=5cdf91a403c0853b7e7b9da82c4596f6},
	abstract = {The proceedings contain 28 papers. The special focus in this conference is on China National Conference on Computational Linguistics. The topics include: Improving Cascade Decoding with Syntax-Aware Aggregator and Contrastive Learning for Event Extraction; TERL: Transformer Enhanced Reinforcement Learning for Relation Extraction; P-MNER: Cross Modal Correction Fusion Network with Prompt Learning for Multimodal Named Entity Recognition; Self Question-Answering: Aspect Sentiment Triplet Extraction via a Multi-MRC Framework Based on Rethink Mechanism; enhancing Ontology Knowledge for Domain-Specific Joint Entity and Relation Extraction; FACT: A Dynamic Framework for Adaptive Context-Aware Translation; MCLS: A Large-Scale Multimodal Cross-Lingual Summarization Dataset; CHED: A Cross-Historical Dataset with a Logical Event Schema for Classical Chinese Event Detection; Training NLI Models Through Universal Adversarial Attack; ask to Understand: Question Generation for Multi-hop Question Answering; Revisiting k-NN for Fine-Tuning Pre-trained Language Models; adder Encoder for Pre-trained Language Model; exploring Accurate and Generic Simile Knowledge from Pre-trained Language Models; learnable Conjunction Enhanced Model for Chinese Sentiment Analysis; enhancing Implicit Sentiment Learning via the Incorporation of Part-of-Speech for Aspect-Based Sentiment Analysis; improving Affective Event Classification with Multi-perspective Knowledge Injection; adversarial Network with External Knowledge for Zero-Shot Stance Detection; case Retrieval for Legal Judgment Prediction in Legal Artificial Intelligence; sentBench: Comprehensive Evaluation of Self-Supervised Sentence Representation with Benchmark Construction; learning on Structured Documents for Conditional Question Answering; overcoming Language Priors with Counterfactual Inference for Visual Question Answering; rethinking Label Smoothing on Multi-Hop Question Answering; unsupervised Style Transfer in News Headlines via Discrete Style Space; lexical Complexity Controlled Sentence Generation for Language Learning; improving Zero-Shot Cross-Lingual Dialogue State Tracking via Contrastive Learning; document Information Extraction via Global Tagging. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Xiong2023237,
	author = {Xiong, Xiong and Wang, Chen and Liu, Yunfei and Li, Shengyang},
	title = {Enhancing Ontology Knowledge for Domain-Specific Joint Entity and Relation Extraction},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14232 LNAI},
	pages = {237 - 252},
	doi = {10.1007/978-981-99-6207-5_15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174443278&doi=10.1007%2F978-981-99-6207-5_15&partnerID=40&md5=d4c3cf19b6f7b2019151d1409845c3cd},
	abstract = {Pre-trained language models (PLMs) have been widely used in entity and relation extraction methods in recent years. However, due to the semantic gap between general-domain text used for pre-training and domain-specific text, these methods encounter semantic redundancy and domain semantics insufficiency when it comes to domain-specific tasks. To mitigate this issue, we propose a low-cost and effective knowledge-enhanced method to facilitate domain-specific semantics modeling in joint entity and relation extraction. Precisely, we use ontology and entity type descriptions as domain knowledge sources, which are encoded and incorporated into the downstream entity and relation extraction model to improve its understanding of domain-specific information. We construct a dataset called SSUIE-RE for Chinese entity and relation extraction in space science and utilization domain of China Manned Space Engineering, which contains a wealth of domain-specific knowledge. The experimental results on SSUIE-RE demonstrate the effectiveness of our method, achieving a 1.4% absolute improvement in relation F1 score over previous best approach. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Joint Entity And Relation Extraction; Knowledge Enhancement; Transformer; Domain Knowledge; Extraction; Semantics; Domain Specific; Entity Extractions; Extraction Method; Joint Entity And Relation Extraction; Knowledge Enhancement; Language Model; Ontology's; Relation Extraction; Semantic Gap; Transformer; Ontology},
	keywords = {Domain Knowledge; Extraction; Semantics; Domain specific; Entity extractions; Extraction method; Joint entity and relation extraction; Knowledge enhancement; Language model; Ontology's; Relation extraction; Semantic gap; Transformer; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Tornés2023184,
	author = {Tornés, Beatriz Martínez and Boros, Emanuela and Doucet, Antoine and Gomez-Krämer, Petra and Ogier, Jean Marc},
	title = {Detecting Forged Receipts with Domain-Specific Ontology-Based Entities & Relations},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14189 LNCS},
	pages = {184 - 199},
	doi = {10.1007/978-3-031-41682-8_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173581081&doi=10.1007%2F978-3-031-41682-8_12&partnerID=40&md5=e4ffbfcb8be95a8b7836f0ca5da443e6},
	abstract = {In this paper, we tackle the task of document fraud detection. We consider that this task can be addressed with natural language processing techniques. We treat it as a regression-based approach, by taking advantage of a pre-trained language model in order to represent the textual content, and by enriching the representation with domain-specific ontology-based entities and relations. We emulate an entity-based approach by comparing different types of input: raw text, extracted entities and a triple-based reformulation of the document content. For our experimental setup, we utilize the single freely available dataset of forged receipts, and we provide a deep analysis of our results in regard to the efficiency of our methods. Our findings show interesting correlations between the types of ontology relations (e.g., has_address, amounts_to), types of entities (product, company, etc.) and the performance of a regression-based language model that could help to study the transfer learning from natural language processing (NLP) methods to boost the performance of existing fraud detection systems. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Fraud Detection; Language Models; Ontology; Computational Linguistics; Crime; Natural Language Processing Systems; Document Contents; Domain-specific Ontologies; Fraud Detection; Language Model; Language Processing Techniques; Natural Languages; Ontology's; Ontology-based; Performance; Textual Content; Ontology},
	keywords = {Computational linguistics; Crime; Natural language processing systems; Document contents; Domain-specific ontologies; Fraud detection; Language model; Language processing techniques; Natural languages; Ontology's; Ontology-based; Performance; Textual content; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Kardos2023387,
	author = {Kardos, Péter and Farkas, Richárd},
	title = {Are These Descriptions Referring to the Same Entity or Just to Similar Ones?},
	year = {2023},
	journal = {IFIP Advances in Information and Communication Technology},
	volume = {676 IFIP},
	pages = {387 - 398},
	doi = {10.1007/978-3-031-34107-6_31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173557372&doi=10.1007%2F978-3-031-34107-6_31&partnerID=40&md5=5675fa04fd3762330f364b7c212975ad},
	abstract = {The Knowledge Graph matching task is to identify nodes in the two graphs that refer to the same concept. In this paper, we focus on the analysis of textual descriptions of the concepts. We employ neural language models as they can score well on text content similarity On the other hand, we show that the text similarity of entity descriptions does not equal to referring to the exact same entity. Our text-based multi-step system was among the top participants at the Knowledge Graph matching track of the Ontology Alignment Evaluation Initiative. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Graph Matching; Language Models; Nlp; Computational Linguistics; Content Similarity; Graph Matchings; Knowledge Graphs; Language Model; Multisteps; Step System; Text Content; Text Similarity; Textual Description; Two-graphs; Knowledge Graph},
	keywords = {Computational linguistics; Content similarity; Graph matchings; Knowledge graphs; Language model; Multisteps; STEP system; Text content; Text similarity; Textual description; Two-graphs; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{De Bellis2023181,
	author = {De Bellis, Alessandro and Biancofiore, Giovanni Maria and Anelli, Vito Walter and Narducci, Fedelucio and Di Noia, Tommaso and Ragone, Azzurra and Di Sciascio, Eugenio},
	title = {Semantic Interpretation of BERT embeddings with Knowledge Graphs},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3478},
	pages = {181 - 191},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173494150&partnerID=40&md5=d56e643e19b5d88dad9391e03ad488d0},
	abstract = {Pretrained language models have transformed the way we process natural languages, enhancing the performance of related systems. BERT has played a pivotal role in revolutionizing the field of Natural Language Processing (NLP). However, the deep learning framework behind BERT lacks interpretability. Recent research has focused on explaining the knowledge BERT acquires from the textual sources used for pre-training its linguistic model. In this study, we analyze the latent vector space produced by BERT's context-aware word embeddings. Our aim is to determine whether certain areas of the BERT vector space have an explicit meaning related to a Knowledge Graph (KG). Using the Link Prediction (LP) task, we demonstrate the presence of explicit and meaningful regions of the BERT vector space. Moreover, we establish links between BERT's vector space and specific ontology concepts in the KG by learning classification patterns. To the best of our knowledge, this is the first attempt to interpret BERT's learned linguistic knowledge through a KG by relying on its pre-trained context-aware word embeddings. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Knowledge Graphs; Natural Language Processing; Classification (of Information); Deep Learning; Graph Embeddings; Natural Language Processing Systems; Semantics; Vector Spaces; Vectors; Context-aware; Embeddings; Knowledge Graphs; Language Model; Language Processing; Natural Language Processing; Natural Languages; Performance; Semantic Interpretation; Knowledge Graph},
	keywords = {Classification (of information); Deep learning; Graph embeddings; Natural language processing systems; Semantics; Vector spaces; Vectors; Context-Aware; Embeddings; Knowledge graphs; Language model; Language processing; Natural language processing; Natural languages; Performance; Semantic interpretation; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Lenz2023263,
	author = {Lenz, Mirko and Bergmann, Ralph},
	title = {Case-Based Adaptation of Argument Graphs with WordNet and Large Language Models},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14141 LNAI},
	pages = {263 - 278},
	doi = {10.1007/978-3-031-40177-0_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172236578&doi=10.1007%2F978-3-031-40177-0_17&partnerID=40&md5=75c535542e2fe3e16ac1310ee94982ce},
	abstract = {Finding information online is hard, even more so once you get into the domain of argumentation. There have been developments around the specialized argumentation machines that incorporate structural features of arguments, but all current approaches share one pitfall: They operate on a corpora of limited sizes. Consequently, it may happen that a user searches for a rather general term like cost increases, but the machine is only able to serve arguments concerned with rent increases. We aim to bridge this gap by introducing approaches to generalize/specialize a found argument using a combination of WordNet and Large Language Models. The techniques are evaluated on a new benchmark dataset with diverse queries using our fully featured implementation. Both the dataset and the code are publicly available on GitHub. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Adaptation; Argumentation; Background Knowledge; Graphs; Natural Language Processing; Knowledge Graph; Natural Language Processing Systems; Ontology; Adaptation; Argumentation; Background Knowledge; Case Based; Graph; Language Model; Language Processing; Natural Language Processing; Natural Languages; Wordnet; Computational Linguistics},
	keywords = {Knowledge graph; Natural language processing systems; Ontology; Adaptation; Argumentation; Background knowledge; Case based; Graph; Language model; Language processing; Natural language processing; Natural languages; Wordnet; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@ARTICLE{Oduro-Afriyie202363,
	author = {Oduro-Afriyie, Joel and Jamil, Hasan M.},
	title = {Knowledge Graph Enabled Open-Domain Conversational Question Answering},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {14113 LNAI},
	pages = {63 - 76},
	doi = {10.1007/978-3-031-42935-4_6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172105045&doi=10.1007%2F978-3-031-42935-4_6&partnerID=40&md5=cc55e9b93f470342c76d191d7062ecb5},
	abstract = {With the advent of natural language enabled applications, there has been a growing appetite for conversational question answering systems. This demand is being largely satisfied with the help of such powerful language models as Open AI’s GPT models, Google’s BERT, and BigScience’s BLOOM. However, the astounding amount of training data and computing resources required to create such models is a huge challenge. Furthermore, for such systems, catering to multiple application domains typically requires the acquisition of even more training data. We discuss an alternative approach to the problem of open-domain conversational question answering by utilizing knowledge graphs to capture relevant information from a body of text in any domain. We achieve this by allowing the relations of the knowledge graphs to be drawn directly from the body of text being processed, rather than from a fixed ontology. By connecting this process with SPARQL queries generated from natural language questions, we demonstrate the foundations of an open-domain question answering system that requires no training and can switch domains flexibly and seamlessly. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graphs; Knowledge Representation; Natural Language Processing; Question Answering System; Graphic Methods; Natural Language Processing Systems; Google+; Knowledge Graphs; Knowledge-representation; Language Model; Language Processing; Natural Language Processing; Natural Languages; Question Answering; Question Answering Systems; Training Data; Knowledge Graph},
	keywords = {Graphic methods; Natural language processing systems; Google+; Knowledge graphs; Knowledge-representation; Language model; Language processing; Natural language processing; Natural languages; Question Answering; Question answering systems; Training data; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Hu20231309,
	author = {Hu, Minghao and Wang, Fang and Xu, Xiantao and Luo, Wei and Liu, Xiaopeng and Luo, Zhunchen and Tan, Yushan},
	title = {Two-stage open information extraction method for the defence technology field; 国防科技领域两阶段开放信息抽取方法},
	year = {2023},
	journal = {Qinghua Daxue Xuebao/Journal of Tsinghua University},
	volume = {63},
	number = {9},
	pages = {1309 - 1316},
	doi = {10.16511/j.cnki.qhdxxb.2023.21.010},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171629927&doi=10.16511%2Fj.cnki.qhdxxb.2023.21.010&partnerID=40&md5=1d1ee73d6dbcf96797667b0da819b281},
	abstract = {[Objective] The abundant information resources available on the internet about defense technology are of vital importance as data sources for obtaining high-value military intelligence. The aim of open information extraction in the field of defense technology is to extract structured triplets containing subject, predicate, object, and other arguments from the massive amount of information available on the internet. This technology has important implications for ontology induction and the construction of knowledge graphs in the defense technology domain. However, while information extraction experiments in the general domain yield good results, open information extraction in the defense technology domain faces several challenges, such as a lack of domain annotated data, arguments overlapping unadaptability, and unrecognizable long entities.[Methods] In this paper, an annotation strategy is proposed based on the entity boundaries, and an annotated dataset in the defense technology field combined with the experience of domain experts was constructed. Furthermore, a two-stage open information extraction method is proposed in the defense technology field that utilizes a pretrained language model-based sequence labeling algorithm to extract predicates and a multihead attention mechanism to learn the prediction of argument boundaries. In the first stage, the input sentence was converted into an input sequence < [CLS], input sentence [SEP] >, and the input sequence was encoded using a pretrained language model to obtain an implicit state representation of the input sequence. Based on this sentence representation, a conditional random field (CRF) layer was used to predict the position of the predicates, i. e., to predict the BIO labels of the words. In the second stage, the predicated predicates from the first stage were concatenated with the original sentence and converted into an input sequence < [CLS], predicate [SEP], and input sentence [SEP] >, which was encoded using a pretrained language model to obtain an implicit state representation of the input sequence. This representation was then fed to a multihead pointer network to predict the position of the argument. The predicted position was tagged with the actual position to calculate the cross-entropy loss function. Finally, the predicates and the arguments predicted by the predicate and argument extraction models were combined to obtain the complete triplet.[Results] The experimental results from the extensive experiments conducted on a self-built annotated dataset in the defense technology field reveal the following. (1) In predicate extraction, our method achieved a 3. 92% performance improvement in the Fl value as compared to LSTM methods and more than 10% performance improvement as compared to syntactic analysis methods. (2) In argument extraction, our method achieved a considerable performance improvement of more than 16% in the Fl value as compared to LSTM methods and about 11% in the Fl value as compared to the BERT + CRF method.[Conclusions] The proposed two-stage open information extraction method can overcome the challenge of arguments overlapping unadaptability and the difficulty of long-span entity extraction, thus improving the shortcomings of existing open information extraction methods. Extensive experimental analysis conducted on the self-built annotated dataset proved the effectiveness of the proposed method. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Defense Technology; Knowledge Graph; Open Information Extraction; Pretrained Language Model; Subject-verb-object Complement; Computational Linguistics; Forecasting; Information Retrieval; Knowledge Graph; Network Security; Annotated Datasets; Defence Technology; Information Extraction Methods; Input Sequence; Knowledge Graphs; Language Model; Open Information Extraction; Pretrained Language Model; Subject-verb-object Complement; Technology Fields; Random Processes},
	keywords = {Computational linguistics; Forecasting; Information retrieval; Knowledge graph; Network security; Annotated datasets; Defence technology; Information extraction methods; Input sequence; Knowledge graphs; Language model; Open information extraction; Pretrained language model; Subject-verb-object complement; Technology fields; Random processes},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2023,
	title = {34th International Conference on Database and Expert Systems Applications, DEXA 2023},
	year = {2023},
	journal = {Communications in Computer and Information Science},
	volume = {1872 CCIS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171574667&partnerID=40&md5=04f386e242620277ebf463165a3df6ec},
	abstract = {The proceedings contain 10 papers. The special focus in this conference is on . The topics include: Analyzing the Innovative Potential of Texts Generated by Large Language Models: An Empirical Evaluation; LocBERT: Improving Social Media User Location Prediction Using Fine-Tuned BERT; measuring Overhead Costs of Federated Learning Systems by Eavesdropping; a Context Ontology-Based Model to Mitigate Root Causes of Uncertainty in Cyber-Physical Systems; architecture for Self-protective Medical Cyber-Physical Systems; an Approach for Safe and Secure Software Protection Supported by Symbolic Execution; Towards Increasing Safety in Collaborative CPS Environments; an Intermediate Representation for Rewriting Cypher Queries. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Jousselme2023,
	author = {Jousselme, Anne Laure and de Villiers, Pieter Pieter and de Freitas, Allan and Blasch, Erik Philip and Dragos, Valentina and Pavlin, Gregor and Costa, Paulo Cesar G. and Laskey, Kathryn Blackmond and Laudy, Claire},
	title = {Uncertain about ChatGPT: enabling the uncertainty evaluation of large language models},
	year = {2023},
	pages = {},
	doi = {10.23919/FUSION52260.2023.10224086},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171522315&doi=10.23919%2FFUSION52260.2023.10224086&partnerID=40&md5=cb834d8d98c6e7b7e557e1debb154586},
	abstract = {ChatGPT, OpenAI's chatbot, has gained consider-able attention since its launch in November 2022, owing to its ability to formulate articulated responses to text queries and comments relating to seemingly any conceivable subject. As impressive as the majority of interactions with ChatGPT are, this large language model has a number of acknowledged shortcomings, which in several cases, may be directly related to how ChatGPT handles uncertainty. The objective of this paper is to pave the way to formal analysis of ChatGPT uncertainty handling. To this end, the ability of the Uncertainty Representation and Reasoning Framework (URREF) ontology is assessed, to support such analysis. Elements of structured experiments for reproducible results are identified. The dataset built varies Information Criteria of Correctness, Non-specificity, Self-confidence, Relevance and Inconsistency, and the Source Criteria of Reliability, Competency and Type. ChatGPT's answers are analyzed along Information Criteria of Correctness, Non-specificity and Self-confidence. Both generic and singular information are sequentially provided. The outcome of this preliminary study is twofold: Firstly, we validate that the experimental setup is efficient in capturing aspects of ChatGPT uncertainty handling. Secondly, we identify possible modifications to the URREF ontology that will be discussed and eventually implemented in URREF ontology Version 4.0 under development. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Information Quality; Large Language Models; Nlp; Ontology; Source Quality; Uncertainty Evaluation; Computational Linguistics; Quality Control; Uncertainty Analysis; Information Quality; Language Model; Large Language Model; Ontology's; Reasoning Framework; Source Quality; Uncertainty Evaluation; Uncertainty Handling; Uncertainty Reasoning; Uncertainty Representation; Ontology},
	keywords = {Computational linguistics; Quality control; Uncertainty analysis; Information quality; Language model; Large language model; Ontology's; Reasoning framework; Source quality; Uncertainty evaluation; Uncertainty handling; Uncertainty reasoning; Uncertainty representation; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Tamagnone20236219,
	author = {Tamagnone, Nicolò and Fekih, Selim and Contla, Ximena and Orozco, Nayid and Rekabsaz, Navid},
	title = {Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification},
	year = {2023},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	volume = {2023-August},
	pages = {6219 - 6227},
	doi = {10.24963/ijcai.2023/690},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170385846&doi=10.24963%2Fijcai.2023%2F690&partnerID=40&md5=10eb7be044c81b5006fac37de29cb2af},
	abstract = {Accurate and rapid situation analysis during humanitarian crises is critical to delivering humanitarian aid efficiently and is fundamental to humanitarian imperatives and the Leave No One Behind (LNOB) principle. This data analysis can highly benefit from language processing systems, e.g., by classifying the text data according to a humanitarian ontology. However, approaching this by simply fine-tuning a generic large language model (LLM) involves considerable practical and ethical issues, particularly the lack of effectiveness on data-sparse and complex subdomains, and the encoding of societal biases and unwanted associations. In this work, we aim to provide an effective and ethically-aware system for humanitarian data analysis. We approach this by (1) introducing a novel architecture adjusted to the humanitarian analysis framework, (2) creating and releasing a novel humanitarian-specific LLM called HumBERT, and (3) proposing a systematic way to measure and mitigate biases. Our results show the better performance of our approach on zero-shot and full-training settings in comparison with strong baseline models, while also revealing the existence of biases in the resulting LLMs. Utilizing a targeted counterfactual data augmentation approach, we significantly reduce these biases without compromising performance. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Handling; Information Analysis; Knowledge Management; Zero-shot Learning; Domain Knowledge; Fine Tuning; Humanitarian Aids; Language Model; Language Processing Systems; Ontology's; Performance; Practical Issues; Situation Analysis; Text Data; Domain Knowledge},
	keywords = {Data handling; Information analysis; Knowledge management; Zero-shot learning; Domain knowledge; Fine tuning; Humanitarian aids; Language model; Language processing systems; Ontology's; Performance; Practical issues; Situation analysis; Text data; Domain Knowledge},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Bronze Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Capshaw2023118,
	author = {Capshaw, Riley and Blomqvist, Eva},
	title = {Towards Tailored Knowledge Base Modeling using Masked Language Models},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3447},
	pages = {118 - 131},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169151570&partnerID=40&md5=afd9b4f845701ed93b92499a46ae2a3f},
	abstract = {We propose a methodology for leveraging aspects of ontology design principles to guide the use of a masked language model (MLM) as a query engine over raw text documents. By using targeted fill-in-the-blank-style prompts to define relations, we show how a domain expert could use BERT, a well-known MLM, to extract triples from unseen documents without any fine-tuning. We evaluate our proposed methodology using a modified document-level relation extraction task, highlighting early successes but also numerous areas that need improvement. Despite these shortcomings, we then discuss why we are still hopeful that this paves the way toward flexible text-based query engines which use collections of unstructured documents. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Document-level Relation Extraction; Knowledge Graphs; Masked Language Models; Ontologies; Computational Linguistics; Engines; Extraction; Knowledge Graph; Modeling Languages; Design Principles; Document-level Relation Extraction; Knowledge Base Modeling; Knowledge Graphs; Language Model; Masked Language Model; Ontology Design; Ontology's; Query Engines; Relation Extraction; Ontology},
	keywords = {Computational linguistics; Engines; Extraction; Knowledge graph; Modeling languages; Design Principles; Document-level relation extraction; Knowledge base modeling; Knowledge graphs; Language model; Masked language model; Ontology design; Ontology's; Query engines; Relation extraction; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Wu20233080,
	author = {Wu, Weiqi and Jiang, Chengyue and Jiang, Yong and Xie, Pengjun and Tu, Kewei},
	title = {Do PLMs Know and Understand Ontological Knowledge?},
	year = {2023},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	volume = {1},
	pages = {3080 - 3101},
	doi = {10.18653/v1/2023.acl-long.173},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168843299&doi=10.18653%2Fv1%2F2023.acl-long.173&partnerID=40&md5=13c4219292c596dcfbe9ffca3d7e73cc},
	abstract = {Ontological knowledge, which comprises classes and properties and their relationships, is integral to world knowledge. It is significant to explore whether Pretrained Language Models (PLMs) know and understand such knowledge. However, existing PLM-probing studies focus mainly on factual knowledge, lacking a systematic probing of ontological knowledge. In this paper, we focus on probing whether PLMs store ontological knowledge and have a semantic understanding of the knowledge rather than rote memorization of the surface form. To probe whether PLMs know ontological knowledge, we investigate how well PLMs memorize: (1) types of entities; (2) hierarchical relationships among classes and properties, e.g., Person is a subclass of Animal and Member of Sports Team is a subproperty of Member of; (3) domain and range constraints of properties, e.g., the subject of Member of Sports Team should be a Person and the object should be a Sports Team. To further probe whether PLMs truly understand ontological knowledge beyond memorization, we comprehensively study whether they can reliably perform logical reasoning with given knowledge according to ontological entailment rules. Our probing results show that PLMs can memorize certain ontological knowledge and utilize implicit knowledge in reasoning. However, both the memorizing and reasoning performances are less than perfect, indicating incomplete knowledge and understanding. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Ontology; Semantics; Domain Constraint; Factual Knowledge; Implicit Knowledge; Language Model; Logical Reasoning; Property; Range Constraints; Semantics Understanding; Surface Forms; World Knowledge; Sports},
	keywords = {Computational linguistics; Ontology; Semantics; Domain constraint; Factual knowledge; Implicit knowledge; Language model; Logical reasoning; Property; Range constraints; Semantics understanding; Surface forms; World knowledge; Sports},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{He20233439,
	author = {He, Yuan and Chen, Jiaoyan and Jimeńez-Ruiz, Ernesto and Dong, Hang and Horrocks, Ian},
	title = {Language Model Analysis for Ontology Subsumption Inference},
	year = {2023},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	pages = {3439 - 3453},
	doi = {10.18653/v1/2023.findings-acl.213},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168836722&doi=10.18653%2Fv1%2F2023.findings-acl.213&partnerID=40&md5=4afe0a08b8999a4f5d0ed7c0a0e33e1f},
	abstract = {Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose ONTOLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Open Source Software; Open Systems; Background Knowledge; Different Domains; Language Inference; Language Model; Modeling Analyzes; Natural Languages; Ontology's; Owl Ontologies; Research Interests; Simple++; Ontology},
	keywords = {Computational linguistics; Open source software; Open systems; Background knowledge; Different domains; Language inference; Language model; Modeling analyzes; Natural languages; Ontology's; OWL ontologies; Research interests; Simple++; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Khorashadizadeh2023132,
	author = {Khorashadizadeh, Hanieh and Mihindukulasooriya, Nandana and Tiwari, Sanju M. and Groppe, Jinghua and Groppe, Sven Thilo},
	title = {Exploring In-Context Learning Capabilities of Foundation Models for Generating Knowledge Graphs from Text},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3447},
	pages = {132 - 153},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168818600&partnerID=40&md5=f1d79f7173a4f5005baba416c9acc899},
	abstract = {Knowledge graphs can represent information about the real-world using entities and their relations in a structured and semantically rich manner and they enable a variety of downstream applications such as question-answering, recommendation systems, semantic search, and advanced analytics. However, at the moment, building a knowledge graph involves a lot of manual effort and thus hinders their application in some situations and the automation of this process might benefit especially for small organizations. Automatically generating structured knowledge graphs from a large volume of natural language is still a challenging task and the research on sub-tasks such as named entity extraction, relation extraction, entity and relation linking, and knowledge graph construction aims to improve the state of the art of automatic construction and completion of knowledge graphs from text. The recent advancement of foundation models with billions of parameters trained in a self-supervised manner with large volumes of training data that can be adapted to a variety of downstream tasks has helped to demonstrate high performance on a large range of Natural Language Processing (NLP) tasks. In this context, one emerging paradigm is in-context learning where a language model is used as it is with a prompt that provides instructions and some examples to perform a task without changing the parameters of the model using traditional approaches such as fine-tuning. This way, no computing resources are needed for re-training/fine-tuning the models and the engineering effort is minimal. Thus, it would be beneficial to utilize such capabilities for generating knowledge graphs from text. In this paper, grounded by several research questions, we explore the capabilities of foundation models such as ChatGPT to generate knowledge graphs from the knowledge it captured during pre-training as well as the new text provided to it in the prompt. The paper provides a qualitative analysis of a set of example outputs generated by a foundation model with the aim of knowledge graph construction and completion. The results demonstrate promising capabilities. Furthermore, we discuss the challenges and next steps for this research work. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Foundation Models; In-context Learning; Knowledge Graph Completion; Knowledge Graph Construction; Large Language Models; Ontology; Relation Extraction; Computational Linguistics; Data Mining; Engineering Education; Extraction; Foundations; Graphic Methods; Learning Systems; Natural Language Processing Systems; Semantics; Context Learning; Foundation Models; Graph Construction; In Contexts; In-context Learning; Knowledge Graph Completion; Knowledge Graph Construction; Knowledge Graphs; Language Model; Large Language Model; Ontology's; Relation Extraction; Knowledge Graph},
	keywords = {Computational linguistics; Data mining; Engineering education; Extraction; Foundations; Graphic methods; Learning systems; Natural language processing systems; Semantics; Context learning; Foundation models; Graph construction; In contexts; In-context learning; Knowledge graph completion; Knowledge graph construction; Knowledge graphs; Language model; Large language model; Ontology's; Relation extraction; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Zhang20233987,
	author = {Zhang, Qiangnu and Hu, Weibin and Xiong, Lingfeng and Wen, Jin and Wei, Teng and Yan, Lesen and Liu, Quan and Zhu, Siqi and Bai, Yu and Zeng, Yuandi},
	title = {IHGA: An interactive web server for large-scale and comprehensive discovery of genes of interest in hepatocellular carcinoma},
	year = {2023},
	journal = {Computational and Structural Biotechnology Journal},
	volume = {21},
	pages = {3987 - 3998},
	doi = {10.1016/j.csbj.2023.08.003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168805863&doi=10.1016%2Fj.csbj.2023.08.003&partnerID=40&md5=450c197fdfc7aa89063b1441919ced6e},
	abstract = {Mining gene expression data is valuable for discovering novel biomarkers and therapeutic targets in hepatocellular carcinoma (HCC). Although emerging data mining tools are available for pan-cancer–related gene data analysis, few tools are dedicated to HCC. Moreover, tools specifically designed for HCC have restrictions such as small data scale and limited functionality. Therefore, we developed IHGA, a new interactive web server for discovering genes of interest in HCC on a large-scale and comprehensive basis. Integrative HCC Gene Analysis (IHGA) contains over 100 independent HCC patient-derived datasets (with over 10,000 tissue samples) and more than 90 cell models. IHGA allows users to conduct a series of large-scale and comprehensive analyses and data visualizations based on gene mRNA levels, including expression comparison, correlation analysis, clinical characteristics analysis, survival analysis, immune system interaction analysis, and drug sensitivity analysis. This method notably enhanced the richness of clinical data in IHGA. Additionally, IHGA integrates artificial intelligence (AI)–assisted gene screening based on natural language models. IHGA is free, user-friendly, and can effectively reduce time spent during data collection, organization, and analysis. In conclusion, IHGA is competitive in terms of data scale, data diversity, and functionality. It effectively alleviates the obstacles caused by HCC heterogeneity to data mining work and helps advance research on the molecular mechanisms of HCC. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Mining; Gene Expression Profile; Hepatocellular Carcinoma; Prognosis; Web Tools; Sorafenib; Gene Expression; Natural Language Processing Systems; Sensitivity Analysis; Web Services; Data-mining Tools; Gene Analysis; Gene Expression Data; Gene Expression Profiles; Hepatocellular Carcinoma; Large-scales; Prognose; Therapeutic Targets; Web Servers; Web Tools; Data Mining; Messenger Rna; Solute Carrier Family 25 Member 15; Solute Carrier Protein; Sorafenib; Unclassified Drug; Article; Artificial Intelligence; Cellular Distribution; Clinical Feature; Comparative Study; Differential Expression Analysis; Differential Gene Expression; Dimensionality Reduction; Disease Free Survival; Drug Sensitivity; Gene Expression; Gene Expression Level; Gene Ontology; Gene Set Enrichment Analysis; Human; Immune System; Liver Cell Carcinoma; Overall Survival; Tumor Suppressor Gene; Urea Cycle},
	keywords = {Gene expression; Natural language processing systems; Sensitivity analysis; Web services; Data-mining tools; Gene analysis; Gene Expression Data; Gene expression profiles; Hepatocellular carcinoma; Large-scales; Prognose; Therapeutic targets; Web servers; Web tools; Data mining; messenger RNA; solute carrier family 25 member 15; solute carrier protein; sorafenib; unclassified drug; Article; artificial intelligence; cellular distribution; clinical feature; comparative study; differential expression analysis; differential gene expression; dimensionality reduction; disease free survival; drug sensitivity; gene expression; gene expression level; gene ontology; gene set enrichment analysis; human; immune system; liver cell carcinoma; overall survival; tumor suppressor gene; urea cycle},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Steinigen202360,
	author = {Steinigen, Daniel and Namysl, Marcin and Hepperle, Markus and Krekeler, Jan and Landgraf, Susanne},
	title = {Semantic Extraction of Key Figures and Their Properties From Tax Legal Texts Using Neural Models},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3441},
	pages = {60 - 71},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167811405&partnerID=40&md5=652decb6ef770251a2249e556e05c520},
	abstract = {Applying information extraction to legislative texts is a challenging task that requires a specification to distinguish the relevant parts from the less relevant parts of the text. Moreover, there is still a lack of appropriate language- and domain-specific data in the field of information extraction. This work investigates the extraction and modeling of key figures from legal texts. We introduce a universally applicable annotation scheme together with a semantic model for key figures and their logically connected properties in legal texts. Moreover, we release KeyFiTax, a dataset with key figures based on paragraphs of German tax acts manually annotated by tax experts together with a knowledge graph populated from these paragraphs based on our semantic model. Using our dataset, we also evaluate and compare state-of-the-art entity extraction models in terms of long entity spans and low-resource data. Furthermore, we present a transformer-based approach for relation extraction using entity markers to obtain a logical formulation of the key figures. Finally, we introduce task triggers for training a combined resource-efficient entity and relation extraction model. We make our dataset together with the semantic model and the knowledge graph, as well as the implementation of the entity and relation extraction approaches investigated in this work public. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Entity Extraction; German Datasets; Information Extraction; Knowledge Graphs; Language Models; Legal Texts; Ontologies; Relation Extraction; Tax Key Figures; Transformers; Information Retrieval; Knowledge Graph; Semantics; Entity Extractions; German Dataset; Information Extraction; Key Figures; Knowledge Graphs; Language Model; Legal Texts; Ontology's; Relation Extraction; Tax Key Figure; Transformer; Taxation},
	keywords = {Information retrieval; Knowledge graph; Semantics; Entity extractions; German dataset; Information extraction; Key figures; Knowledge graphs; Language model; Legal texts; Ontology's; Relation extraction; Tax key figure; Transformer; Taxation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2023,
	title = {NeSy 2023 - Proceedings of the 17th International Workshop on Neural-Symbolic Learning and Reasoning},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3432},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167418670&partnerID=40&md5=cb4edd11030180c6b763facb04fe7c02},
	abstract = {The proceedings contain 33 papers. The topics discussed include: a roadmap for neuro-argumentative learning; what's wrong with gradient-based complex query answering?; closing the neural-symbolic cycle: knowledge extraction, user intervention and distillation from convolutional neural networks; the challenge of learning symbolic representations; exploring mathematical conjecturing with large language models; learning logic constraints from demonstration; from axioms over graphs to vectors, and back again: evaluating the properties of graph-based ontology embeddings; neural-symbolic predicate invention: learning relational concepts from visual scenes; semantic interpretability of convolutional neural networks by taxonomy extraction; preliminary results on a state-driven method for rule construction in neural-symbolic reinforcement learning; and is the proof length a good indicator of hardness for reason-able embeddings?. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2023,
	title = {AAAI-MAKE 2023 - Proceedings of the AAAI 2023 Spring Symposium on Challenges Requiring the Combination of Machine Learning and Knowledge Engineering},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3433},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166525775&partnerID=40&md5=d0404ed875044b207b1de39cebd0e67d},
	abstract = {The proceedings contain 26 papers. The topics discussed include: a systematic and efficient approach to the design of modular hybrid ai systems; embedded to interpretive: a paradigm shift in knowledge discovery to represent dynamic knowledge; hybrid machine learning/knowledge base systems learning through natural language dialogue with deep learning models; robot behavior-tree-based task generation with large language models; an early warning system that combines machine learning and a rule-based approach for the prediction of cancer patients' unplanned visits; dynamic ontology matching challenge; ontology-driven enhancement of process mining with domain knowledge; towards hybrid dialog management strategies for a health coach chatbot; neuro-symbolic rule learning in real-world classification tasks; rule-based knowledge discovery via anomaly detection in tabular data; and leveraging RDF graphs, similarity metrics and network analysis for business process management. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Agrawal2023,
	author = {Agrawal, Garima and Pal, Kuntal Kumar and Deng, Yuli and Liu, Huan and Baral, Chitta R.},
	title = {AISecKG: Knowledge Graph Dataset for Cybersecurity Education},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3433},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166476137&partnerID=40&md5=6986cfb09ced2f49a0c07d5fcd81fc35},
	abstract = {Cybersecurity education is exceptionally challenging as it involves learning the complex attacks; tools and developing critical problem-solving skills to defend the systems. For a student or novice researcher in the cybersecurity domain, there is a need to design an adaptive learning strategy that can break complex tasks and concepts into simple representations. An AI-enabled automated cybersecurity education system can improve cognitive engagement and active learning. Knowledge graphs (KG) provide a visual representation in a graph that can reason and interpret from the underlying data, making them suitable for use in education and interactive learning. However, there are no publicly available datasets for the cybersecurity education domain to build such systems. The data is present as unstructured educational course material, Wiki pages, capture the flag (CTF) writeups, etc. Creating knowledge graphs from unstructured text is challenging without an ontology or annotated dataset. However, data annotation for cybersecurity needs domain experts. To address these gaps, we made three contributions in this paper. First, we propose an ontology for the cybersecurity education domain for students and novice learners. Second, we develop AISecKG, a triple dataset with cybersecurity-related entities and relations as defined by the ontology. This dataset can be used to construct knowledge graphs to teach cybersecurity and promote cognitive learning. It can also be used to build downstream applications like recommendation systems or self-learning question-answering systems for students. The dataset would also help identify malicious named entities and their probable impact. Third, using this dataset, we show a downstream application to extract custom-named entities from texts and educational material on cybersecurity. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Cybersecurity Education; Kg Dataset; Knowledge Base; Knowledge Graph; Language Model; Ontology; Cybersecurity; Knowledge Graph; Learning Systems; Natural Language Processing Systems; Students; Cyber Security; Cyber-security Educations; Downstream Applications; Education Domain; Knowledge Base; Knowledge Graph Dataset; Knowledge Graphs; Language Model; Ontology's; Ontology},
	keywords = {Cybersecurity; Knowledge graph; Learning systems; Natural language processing systems; Students; Cyber security; Cyber-security educations; Downstream applications; Education domain; Knowledge base; Knowledge graph dataset; Knowledge graphs; Language model; Ontology's; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Abdelmageed202362,
	author = {Abdelmageed, Nora and Löffler, Felicitas and König-Ries, Birgitta},
	title = {BiodivBERT: a Pre-Trained Language Model for the Biodiversity Domain},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3415},
	pages = {62 - 71},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165793172&partnerID=40&md5=8cca4477cb76fca65e4b133871920cbf},
	abstract = {Information Extraction in the Life Sciences is getting increasing attention due to the constantly growing amount of data and text. The advancements of deep learning models further accelerate this development. However, applying these models to domain-specific data is crucial as applied domains often require different entity type extractions than general ones. This paper introduces BiodivBERT, the first pre-trained language model for the biodiversity domain. We constructed two pre-training corpora (abstracts and abstracts + full text) based on a keyword search strategy from two leading publishers in the Life Sciences. In addition, we fine-tuned BiodivBERT on two downstream tasks, i.e., Named Entity Recognition (NER) and Relation Extraction (RE), using various state-of-the-art benchmarks. The results show that BiodivBERT outperforms the state-of-the-art approaches. Moreover, we discuss a potential application of BiodivBERT for ontology auto-population. We publicly release data and code for both pre-training and fine-tuning. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Biodiversity; Fine-tuning; Language Model; Pre-training; Abstracting; Computational Linguistics; Deep Learning; Search Engines; Bert; Domain Specific; Entity-types; Fine Tuning; Keyword Search; Language Model; Learning Models; Life-sciences; Pre-training; Training Corpus; Biodiversity},
	keywords = {Abstracting; Computational linguistics; Deep learning; Search engines; BERT; Domain specific; Entity-types; Fine tuning; Keyword search; Language model; Learning models; Life-sciences; Pre-training; Training corpus; Biodiversity},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Groza2023141,
	author = {Groza, Adrian Petru and Marginean, Anca Nicoleta and Delia Nicoara, Simona},
	title = {An ontology for Age-Related Macular Degeneration using ophthalmologists and language models},
	year = {2023},
	journal = {CEUR Workshop Proceedings},
	volume = {3415},
	pages = {141 - 142},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165747761&partnerID=40&md5=4afe6edcf9e1b17885effdbede323f78},
	abstract = {We aim to support monitoring of the current guidelines and scientific evidence in the management of Age-Related Macular Degeneration (AMD) in order to augment retinal specialists to develop a clinically oriented and consensual protocol for therapeutic approaches for AMD. First, we are engineering an ontology for AMD retinal condition using information from literature, related medical ontologies and domain knowledge from ophthalmologists. Second, we augment the knowledge engineer capabilities to populate and enrich the ontology using structured knowledge extracted from medical literature with the GPT-3 language model. Third, we perform reasoning to signal to the ophthalmologist differences or inconsistencies among different clinical studies, protocols or therapeutic approaches. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Age-related Macular Degeneration; Conflict Detection; Medical Ontologies; Reasoning; Computational Linguistics; Domain Knowledge; Ontology; 'current; Age-related Macular Degeneration; Condition; Conflict Detection; Language Model; Medical Domains; Medical Ontology; Ontology's; Reasoning; Scientific Evidence; Ophthalmology},
	keywords = {Computational linguistics; Domain Knowledge; Ontology; 'current; Age-related macular degeneration; Condition; Conflict detection; Language model; Medical domains; Medical ontology; Ontology's; Reasoning; Scientific evidence; Ophthalmology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Razzaq202374148,
	author = {Razzaq, Saad and Maqbool, Fahad and Ilyas, Muhammad and Jabeen, Hajira},
	title = {EvoRecipes: A Generative Approach for Evolving Context-Aware Recipes},
	year = {2023},
	journal = {IEEE Access},
	volume = {11},
	pages = {74148 - 74164},
	doi = {10.1109/ACCESS.2023.3296144},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165259066&doi=10.1109%2FACCESS.2023.3296144&partnerID=40&md5=4d706f9f3d01b30905522e9ecd973483},
	abstract = {Generative AI e.g. Large Language Models (LLMs) can be used to generate new recipes. However, LLMs struggle with more complex aspects like recipe semantics and process comprehension. Furthermore, LLMs have limited ability to account for user preferences since they are based on statistical patterns. As a result, these recipes may be invalid. Evolutionary algorithms inspired by the process of natural selection are optimization algorithms that use stochastic operators to generate new solutions. These algorithms can generate large number of solutions from the set of possible solution space. Moreover, these algorithms have the capability to incorporate user preferences in fitness function to generate novel recipes that are more aligned with the fitness objective. In this paper, we propose the EvoRecipes framework to generate novel recipes. The EvoRecipes framework utilizes both Genetic Algorithm and generative AI in addition to RecipeOn ontology, and RecipeKG knowledge graph. Genetic Algorithm explore the large solution space of encoded recipe solutions and are capable of incorporating user preferences, while LLMs are used to generate recipe text from encoded recipe solutions. EvoRecipes uses a population of context-aware recipe solutions from the RecipeKG knowledge graph. RecipeKG encodes recipes in RDF format using classes and properties as defined in the RecipeOn ontology. Moreover, to evaluate the alignment of EvoRecipe generated recipes with multiple intended objectives, we propose a fitness function that incorporates novelty, simplicity, visual appeal, and feasibility. Additionally, to evaluate the quality of the EvoRecipe generated recipes while considering the subjective nature of recipes, we conducted a survey using multi-dimensional metrics (i.e. contextual, procedural, and novelty). Results show that EvoRecipes generated recipes are novel, valid and incorporate user preferences. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Creativity; Food; Knowledge Graph; Ontology; Recipe; Recipe Evolution; Food Products; Function Evaluation; Knowledge Graph; Ontology; Quality Control; Resource Description Framework (rdf); Semantics; Stochastic Systems; User Interfaces; Computational Creativities; Creativity; Knowledge Graphs; Language Model; Ontology's; Recipe; Recipe Evolution; Resources Description Frameworks; Genetic Algorithms},
	keywords = {Food products; Function evaluation; Knowledge graph; Ontology; Quality control; Resource Description Framework (RDF); Semantics; Stochastic systems; User interfaces; Computational creativities; Creativity; Knowledge graphs; Language model; Ontology's; Recipe; Recipe evolution; Resources description frameworks; Genetic algorithms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Gold Open Access}
}

@ARTICLE{Palagin2023170,
	author = {Palagin, Alexsandr V. and Kaverinsky, Vladislav and Litvin, Anna A. and Malakhov, K. S.},
	title = {OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning},
	year = {2023},
	journal = {International Journal of Computing},
	volume = {22},
	number = {2},
	pages = {170 - 183},
	doi = {10.47839/ijc.22.2.3086},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165118737&doi=10.47839%2Fijc.22.2.3086&partnerID=40&md5=76cc2594c3a872be8e0b81d5fb9f1d63},
	abstract = {This research presents a comprehensive methodology for utilizing an ontology-driven structured prompts system in interplay with ChatGPT, a widely used large language model (LLM). The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT’s meta-learning capabilities. The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system, which collectively enhance the effectiveness and performance of chatbot systems. The implementation of this technology is demonstrated using the Ukrainian language within the domain of rehabilitation. By applying the proposed methodology, the OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses. The study highlights the versatility of the methodology, emphasizing its applicability not only to ChatGPT but also to other chatbot systems based on LLMs, such as Google’s Bard utilizing the PaLM 2 LLM. The underlying principles of meta-learning, structured prompts, and ontology-driven information retrieval form the core of the proposed methodology, enabling their adaptation and utilization in various LLM-based systems. This versatile approach opens up new possibilities for NLP and dialogue systems, empowering developers to enhance the performance and functionality of chatbot systems across different domains and languages. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Chatbot; Chatgpt; Composite Service; Meta-learning; Ontochatgpt; Ontology Engineering; Ontology-driven Information System; Prompt Engineering; Prompt-based Learning; Transdisciplinary Research},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{2023,
	title = {10th International Work-Conference on Bioinformatics and Biomedical Engineering, IWBBIO 2023},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {13920 LNBI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165106937&partnerID=40&md5=76ef7f8a8fe1798e0bbdb445d3f1770e},
	abstract = {The proceedings contain 78 papers. The special focus in this conference is on Bioinformatics and Biomedical Engineering. The topics include: Three-Dimensional Representation and Visualization of High-Grade and Low-Grade Glioma by Nakagami Imaging; speeding up Simulations for Radiotherapy Research by Means of Machine Learning; A Meta-Graph for the Construction of an RNA-Centered Knowledge Graph; Structural Analysis of RNA-Binding Protein EWSR1 Involved in Ewing’s Sarcoma Through Domain Assembly and Conformational Molecular Dynamics Studies; Pharmacoinformatic Analysis of Drug Leads for Alzheimer’s Disease from FDA-Approved Dataset Through Drug Repositioning Studies; motion Control of a Robotic Lumbar Spine Model; annotation-Free Identification of Potential Synteny Anchors; analysing Dose Parameters of Radiation Therapy Treatment Planning and Estimation of Their Influence on Secondary Cancer Risks; A Platform for the Radiomic Analysis of Brain FDG PET Images: Detecting Alzheimer’s Disease; identification of InhA-Inhibitors Interaction Fingerprints that Affect Residence Time; deep Learning for Automatic Electroencephalographic Signals Classification; evaluation of Homogeneity of Effervescent Tablets Containing Quercetin and Calcium Using X-ray Microtomography and Hyperspectral Analysis; the Effect of Biofeedback on Learning the Wheelie Position on Manual Wheelchair; preliminary Study on the Identification of Diseases by Electrocardiography Sensors’ Data; exploring Machine Learning Algorithms and Protein Language Models Strategies to Develop Enzyme Classification Systems; a System Biology and Bioinformatics Approach to Determine the Molecular Signature, Core Ontologies, Functional Pathways, Drug Compounds in Between Stress and Type 2 Diabetes; recent Advances in Discovery of New Tyrosine Kinase Inhibitors Using Computational Methods; the Coherent Multi-representation Problem with Applications in Structural Biology; computational Study of Conformational Changes in Intrinsically Disordered Regions During Protein-Protein Complex Formation; predicting and Detecting Coronary Heart Disease in Patients Using Machine Learning Method. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2023,
	title = {10th International Work-Conference on Bioinformatics and Biomedical Engineering, IWBBIO 2023},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {13919 LNBI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164953942&partnerID=40&md5=692fe1de08b91b0b044cf08fa9395509},
	abstract = {The proceedings contain 78 papers. The special focus in this conference is on Bioinformatics and Biomedical Engineering. The topics include: Three-Dimensional Representation and Visualization of High-Grade and Low-Grade Glioma by Nakagami Imaging; speeding up Simulations for Radiotherapy Research by Means of Machine Learning; A Meta-Graph for the Construction of an RNA-Centered Knowledge Graph; Structural Analysis of RNA-Binding Protein EWSR1 Involved in Ewing’s Sarcoma Through Domain Assembly and Conformational Molecular Dynamics Studies; Pharmacoinformatic Analysis of Drug Leads for Alzheimer’s Disease from FDA-Approved Dataset Through Drug Repositioning Studies; motion Control of a Robotic Lumbar Spine Model; annotation-Free Identification of Potential Synteny Anchors; analysing Dose Parameters of Radiation Therapy Treatment Planning and Estimation of Their Influence on Secondary Cancer Risks; A Platform for the Radiomic Analysis of Brain FDG PET Images: Detecting Alzheimer’s Disease; identification of InhA-Inhibitors Interaction Fingerprints that Affect Residence Time; deep Learning for Automatic Electroencephalographic Signals Classification; evaluation of Homogeneity of Effervescent Tablets Containing Quercetin and Calcium Using X-ray Microtomography and Hyperspectral Analysis; the Effect of Biofeedback on Learning the Wheelie Position on Manual Wheelchair; preliminary Study on the Identification of Diseases by Electrocardiography Sensors’ Data; exploring Machine Learning Algorithms and Protein Language Models Strategies to Develop Enzyme Classification Systems; a System Biology and Bioinformatics Approach to Determine the Molecular Signature, Core Ontologies, Functional Pathways, Drug Compounds in Between Stress and Type 2 Diabetes; recent Advances in Discovery of New Tyrosine Kinase Inhibitors Using Computational Methods; the Coherent Multi-representation Problem with Applications in Structural Biology; computational Study of Conformational Changes in Intrinsically Disordered Regions During Protein-Protein Complex Formation; predicting and Detecting Coronary Heart Disease in Patients Using Machine Learning Method. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2023,
	title = {20th International Conference on The Semantic Web, ESWC 2023},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {13870 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163374180&partnerID=40&md5=6395b58ba12bdb75e7f7f24ce3e5ccae},
	abstract = {The proceedings contain 41 papers. The special focus in this conference is on The Semantic Web. The topics include: NASTyLinker: NIL-Aware Scalable Transformer-Based Entity Linker; iSummary: Workload-Based, Personalized Summaries for Knowledge Graphs; neural Class Expression Synthesis; evaluating Language Models for Knowledge Base Completion; subsumption Prediction for E-Commerce Taxonomies; two-View Graph Neural Networks for Knowledge Graph Completion; GETT-QA: Graph Embedding Based T2T Transformer for Knowledge Graph Question Answering; Repairing EL Ontologies Using Weakening and Completing; activity Recommendation for Business Process Modeling with Pre-trained Language Models; a Comparative Study of Stream Reasoning Engines; RELD: A Knowledge Graph of Relation Extraction Datasets; IMKG: The Internet Meme Knowledge Graph; Describing and Organizing Semantic Web and Machine Learning Systems in the SWeMLS-KG; a Concise Ontology to Support Research on Complex, Multimodal Clinical Reasoning; lauNuts: A Knowledge Graph to Identify and Compare Geographic Regions in the European Union; HHT: An Approach for Representing Temporally-Evolving Historical Territories; an Upper Ontology for Modern Science Branches and Related Entities; k-Hub: A Modular Ontology to Support Document Retrieval and Knowledge Extraction in Industry 5.0; pyRDF2Vec: A Python Implementation and Extension of RDF2Vec; Boosting Knowledge Graph Generation from Tabular Data with RML Views; Join Ordering of SPARQL Property Path Queries; a Knowledge Graph of Contentious Terminology for Inclusive Representation of Cultural Heritage; LegalHTML: A Representation Language for Legal Acts; whyis 2: An Open Source Framework for Knowledge Graph Development and Research; Prototyping an End-User User Interface for the Solid Application Interoperability Specification Under GDPR; semReasoner - A High-Performance Knowledge Graph Store and Rule-Based Reasoner; LIS: A Knowledge Graph-Based Line Information System; combining Semantic Web and Machine Learning for Auditable Legal Key Element Extraction; understanding Customer Requirements: An Enterprise Knowledge Graph Approach; investigating Ontology-Based Data Access with GitHub. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Guraliuk202368,
	author = {Guraliuk, Andrii and Zakatnov, Dmytro and Lapaenko, Svitlana and Ahalets, Inna and Varaksina, Nataliia},
	title = {Integrative Technology for Creating Electronic Educational Resources},
	year = {2023},
	journal = {International Journal of Engineering Pedagogy},
	volume = {13},
	number = {3},
	pages = {68 - 79},
	doi = {10.3991/ijep.v13i3.36109},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162170720&doi=10.3991%2Fijep.v13i3.36109&partnerID=40&md5=ae9c4cfb0d8b02c31c7de1610e4f55f8},
	abstract = {To increase the efficiency of using ontologies in the educational process, we have developed the Ontos.xyz resource, which consists of an ontograph editor and a viewer that allows displaying both individual EERs and their collections. The editor allows creating an ontograph that describes the structure of the EER as well as the ability to assign to each node a context of all types supported by the browser, including html pages, web 2.0 resources, etc. The EER built in this way allows the integration of data from different sources with the ability to adapt to the conditions of any subject area, regardless of its specifics. Thus, the digital competencies of the EER creators remain relevant since they do not need to learn new software resources to work with individual ontology concepts (“ontograph nodes”). As a result of our research, we have developed a technology for using the ontological approach to create an EER. For its implementation, the Ontos.xyz resource was developed, which is an ontological graph editor. The ontograph allows for the primary visualization of the EER structure, interpreting its structural elements as nodes (vertices) of the graph and displaying the logic of transitions between structural elements in the form of directed links. With such an approach, the ontology is a kind of aggregator that ensures the integration of the semantic and technological approaches. Building an EER using the ontological approach requires appropriate technological support in terms of description languages, models, software tools, and systems. The proposed software solution, in contrast to its functional counterparts, focuses on use in the educational process. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Digital Competence; Digital Technologies; Electronic Educational Resources; Ontological Approach In Education},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Gold Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Lamri20231,
	author = {Lamri, Manal and Lyazid, Sabri},
	title = {XACML-based semantic rules language and ontological model for reconciling semantic differences of access control rules},
	year = {2023},
	journal = {International Journal of Ad Hoc and Ubiquitous Computing},
	volume = {43},
	number = {1},
	pages = {1 - 17},
	doi = {10.1504/IJAHUC.2023.130977},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162056340&doi=10.1504%2FIJAHUC.2023.130977&partnerID=40&md5=ebd18c9a7efd9122810fcd7092c0a5a6},
	abstract = {Internet of things interconnects increasing numbers of artefacts and individuals, allowing the setup of ambient intelligence systems in multi-domains (e.g., hospitals). Designing a suitable architectural framework is required to ensure the efficiency and sustainability of the implementation of such systems. Consolidating public services about citizens’ safety and authorisation decisions when a resource is accessed in an open-dynamic environment are the main challenges that can be highlighted. Therefore, the semantic heterogeneity between the local policies of the different domains is a crucial lock for implementing this process. Our approach goes beyond the semantic web languages’ weaknesses by combining the XACML-based security policy model with a semantic rules language developed during the European SembySem Project. Built on top of RDF(S), the proposed model aims to abstract the security implementation, reconcile semantic differences across multi-domain, and maintain the local security policy. Moreover, this model addresses the semantic heterogeneity of sensors’ data during knowledge-sharing. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Access Control; Authentication; Distributed Systems; Internet Of Things; Iot; Multi-domain; Ontology; Xacml; Authorization; Network Security; Ontology; Resource Description Framework (rdf); Security Systems; Sustainable Development; Distributed Systems; Iot; Language Model; Multi-domains; Ontology's; Rules Languages; Semantic Difference; Semantic Rules; Semantics Heterogeneity; Xacml; Internet Of Things},
	keywords = {Authorization; Network security; Ontology; Resource Description Framework (RDF); Security systems; Sustainable development; Distributed systems; IoT; Language model; Multi-domains; Ontology's; Rules languages; Semantic difference; Semantic rules; Semantics Heterogeneity; XACML; Internet of things},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Coca20231413,
	author = {Coca, Alexandru and Tseng, Bo Hsiang and Lin, Weizhe and Byrne, Bill},
	title = {More Robust Schema-Guided Dialogue State Tracking via Tree-Based Paraphrase Ranking},
	year = {2023},
	pages = {1413 - 1424},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159861522&partnerID=40&md5=7699f2521bdb409db846afd88e910cc8},
	abstract = {The schema-guided paradigm overcomes scalability issues inherent in building task-oriented dialogue (TOD) agents with static ontologies. Instead of operating on dialogue context alone, agents have access to hierarchical schemas containing task-relevant natural language descriptions. Fine-tuned language models excel at schema-guided dialogue state tracking (DST) but are sensitive to the writing style of the schemas. We explore methods for improving the robustness of DST models. We propose a framework<sup>1</sup> for generating synthetic schemas which uses tree-based ranking to jointly optimise lexical diversity and semantic faithfulness. The generalisation of strong baselines is improved when augmenting their training data with prompts generated by our framework, as demonstrated by marked improvements in average joint goal accuracy (JGA) and schema sensitivity (SS) on the SGD-X benchmark. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Hierarchical Schema; In-buildings; Language Description; Natural Languages; Ontology's; Scalability Issue; State Tracking; Task Relevant; Task-oriented; Tree-based; Semantics},
	keywords = {Computational linguistics; Hierarchical schema; In-buildings; Language description; Natural languages; Ontology's; Scalability issue; State tracking; Task relevant; Task-oriented; Tree-based; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Jiang2023340,
	author = {Jiang, Chengyue and Jiang, Yong and Wu, Weiqi and Zheng, Yuting and Xie, Pengjun and Tu, Kewei},
	title = {COMBO: A Complete Benchmark for Open KG Canonicalization},
	year = {2023},
	pages = {340 - 357},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159856213&partnerID=40&md5=c3b538d171161706c7ce8931aad8b94b},
	abstract = {Open knowledge graph (KG) consists of (subject, relation, object) triples extracted from millions of raw text. The subject and object noun phrases and the relation in open KG have severe redundancy and ambiguity and need to be canonicalized. Existing datasets for open KG canonicalization only provide gold entity-level canonicalization for noun phrases. In this paper, we present COMBO, a Complete Benchmark for Open KG canonicalization. Compared with existing datasets, we additionally provide gold canonicalization for relation phrases, gold ontology-level canonicalization for noun phrases, as well as source sentences from which triples are extracted. We also propose metrics for evaluating each type of canonicalization. On the COMBO dataset, we empirically compare previously proposed canonicalization methods as well as a few simple baseline methods based on pretrained language models. We find that properly encoding the phrases in a triple using pretrained language models results in better relation canonicalization and ontology-level canonicalization of the noun phrase. We release our dataset, baselines, and evaluation scripts at https://github.com/jeffchy/COMBO/tree/main. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Gold; Knowledge Graph; Ontology; Baseline Methods; Canonicalization; Encodings; Entity-level; Knowledge Graphs; Language Model; Modeling Results; Noun Phrase; Ontology's; Simple++; Computational Linguistics},
	keywords = {Gold; Knowledge graph; Ontology; Baseline methods; Canonicalization; Encodings; Entity-level; Knowledge graphs; Language model; Modeling results; Noun phrase; Ontology's; Simple++; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Chen2023,
	author = {Chen, Luming and Qi, Yifan and Wu, Aiping and Deng, Lizong and Jiang, Taijiao},
	title = {Mapping Chinese Medical Entities to the Unified Medical Language System},
	year = {2023},
	journal = {Health Data Science},
	volume = {3},
	pages = {},
	doi = {10.34133/hds.0011},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159234091&doi=10.34133%2Fhds.0011&partnerID=40&md5=152de4a1010b269822ca78d81f402032},
	abstract = {Background. Chinese medical entities have not been organized comprehensively due to the lack of well-developed terminology systems, which poses a challenge to processing Chinese medical texts for fine-grained medical knowledge representation. To unify Chinese medical terminologies, mapping Chinese medical entities to their English counterparts in the Unified Medical Language System (UMLS) is an efficient solution. However, their mappings have not been investigated sufficiently in former research. In this study, we explore strategies for mapping Chinese medical entities to the UMLS and systematically evaluate the mapping performance. Methods. First, Chinese medical entities are translated to English using multiple web-based translation engines. Then, 3 mapping strategies are investigated: (a) string-based, (b) semantic-based, and (c) string and semantic similarity combined. In addition, cross-lingual pretrained language models are applied to map Chinese medical entities to UMLS concepts without translation. All of these strategies are evaluated on the ICD10-CN, Chinese Human Phenotype Ontology (CHPO), and RealWorld datasets. Results. The linear combination method based on the SapBERT and term frequency-inverse document frequency bag-of-words models perform the best on all evaluation datasets, with 91.85%, 82.44%, and 78.43% of the top 5 accuracies on the ICD10-CN, CHPO, and RealWorld datasets, respectively. Conclusions. In our study, we explore strategies for mapping Chinese medical entities to the UMLS and identify a satisfactory linear combination method. Our investigation will facilitate Chinese medical entity normalization and inspire research that focuses on Chinese medical ontology development. © 2023 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{2023,
	title = {International Workshops which were held in conjunction with 27th European Symposium on Research in Computer Security, ESORICS 2022},
	year = {2023},
	journal = {Lecture Notes in Computer Science},
	volume = {13785 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151075395&partnerID=40&md5=f605f848da5685e910d3c01c9a0a0cf1},
	abstract = {The proceedings contain 39 papers. The special focus in this conference is on Research in Computer Security. The topics include: Towards a Security Impact Analysis Framework: A Risk-Based and MITRE Attack Approach; data Protection Officers’ Perspectives on Privacy Challenges in Digital Ecosystems; Rebooting IT Security Awareness – How Organisations Can Encourage and Sustain Secure Behaviours; towards Reverse Engineering of Industrial Physical Processes; solutions for Protecting the Space Ground Segments: From Risk Assessment to Emergency Response; modelling and Simulation of Railway Networks for Resilience Analysis; honeyChart: Automated Honeypot Management over Kubernetes; ComSEC: Secure Communications for Baggage Handling Systems; Coordinated Network Attacks on Microgrid Dispatch Function: An EPIC Case Study; methodology for Resilience Assessment for Rail Infrastructure Considering Cyber-Physical Threats; coverage-Guided Fuzzing of Embedded Systems Leveraging Hardware Tracing; Challenges and Pitfalls in Generating Representative ICS Datasets in Cyber Security Research; securing Cyber-Physical Spaces with Hybrid Analytics: Vision and Reference Architecture; a Precision Cybersecurity Workflow for Cyber-physical Systems: The IoT Healthcare Use Case; a Revisitation of Clausewitz’s Thinking from the Cyber Situational Awareness Perspective; examining 5G Technology-Based Applications for Military Communications; design of a Validation Model of the Cognitive State in Military Operations in Cyberspace; design and Validation of a Threat Model Based on Cyber Kill Chain Applied to Human Factors; the Cloud Continuum for Military Deployable Networks: Challenges and Opportunities; adversarial Attacks and Mitigations on Scene Segmentation of Autonomous Vehicles; ballot-Polling Audits of Instant-Runoff Voting Elections with a Dirichlet-Tree Model; non(c)esuch Ballot-Level Comparison Risk-Limiting Audits; why Is Online Voting Still Largely a Black Box?; connecting Incident Reporting Infrastructure to Election Day Proceedings; Council of Europe Guidelines on the Use of ICT in Electoral Processes; SAEOn: An Ontological Metamodel for Quantitative Security Assurance Evaluation; a Comparison-Based Methodology for the Security Assurance of Novel Systems; automation of Vulnerability Information Extraction Using Transformer-Based Language Models; preface. © 2025 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2023,
	title = {19th International Symposium on Distributed Computing and Artificial Intelligence, DCAI 2022},
	year = {2023},
	journal = {Lecture Notes in Networks and Systems},
	volume = {585 LNNS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149648475&partnerID=40&md5=125d0caa91cebde403c827214794f075},
	abstract = {The proceedings contain 22 papers. The special focus in this conference is on Distributed Computing and Artificial Intelligence. The topics include: Computer Vision: A Review on 3D Object Recognition; An IoUT-Based Platform for Managing Underwater Cultural Heritage; overview: Security in 5G Wireless Systems; a Study on the Application of Protein Language Models in the Analysis of Membrane Proteins; visualization for Infection Analysis and Decision Support in Hospitals; An Intelligent and Green E-healthcare Model for an Early Diagnosis of Medical Images as an IoMT Application; towards Highly Performant Context Awareness in the Internet of Things; adaptive System to Manage User Comfort Preferences and Conflicts at Everyday Environments; ML-Based Automation of Constraint Satisfaction Model Transformation and Solver Configuration; race Condition Error Detection in a Program Executed on a Device with Limited Memory Resources; the Impact of Covid-19 on Student Mental Health and Online Learning Experience; Threat Detection in URLs by Applying Machine Learning Algorithms<sup>*</sup> ; an Approach to Simulate Malware Propagation in the Internet of Drones; the Use of Corporate Architecture in Planning and Automation of Production Processes; Towards Ontology-Based End-to-End Domain-Oriented KBQA System; TFEEC: Turkish Financial Event Extraction Corpus; denial of Service Attack Detection Based on Feature Extraction and Supervised Techniques; automating the Implementation of Unsupervised Machine Learning Processes in Smart Cities Scenarios; Intelligent Model Hotel Energy Demand Forecasting by Means of LSTM and GRU Neural Networks; explainable Artificial Intelligence on Smart Human Mobility: A Comparative Study Approach. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Zhao2023,
	author = {Zhao, Ziyan and Zhang, Li and Lian, Xiaoli and Gao, Xiaoyun and Lv, Heyang and Shi, Lin},
	title = {ReqGen: Keywords-Driven Software Requirements Generation},
	year = {2023},
	journal = {Mathematics},
	volume = {11},
	number = {2},
	pages = {},
	doi = {10.3390/math11020332},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146761492&doi=10.3390%2Fmath11020332&partnerID=40&md5=bc2dd7225b96d41a7ce49f31b6d4a764},
	abstract = {Software requirements specification is undoubtedly critical for the whole software life-cycle. Currently, writing software requirements specifications primarily depends on human work. Although massive studies have been proposed to speed up the process via proposing advanced elicitation and analysis techniques, it is still a time-consuming and error-prone task, which needs to take domain knowledge and business information into consideration. In this paper, we propose an approach, named ReqGen, which can provide further assistance by automatically generating natural language requirements specifications based on certain given keywords. Specifically, ReqGen consists of three critical steps. First, keywords-oriented knowledge is selected from the domain ontology and is injected into the basic Unified pre-trained Language Model (UniLM) for domain fine-tuning. Second, a copy mechanism is integrated to ensure the occurrence of keywords in the generated statements. Finally, a requirements-syntax-constrained decoding is designed to close the semantic and syntax distance between the candidate and reference specifications. Experiments on two public datasets from different groups and domains show that ReqGen outperforms six popular natural language generation approaches with respect to the hard constraint of keywords’ (phrases’) inclusion, BLEU, ROUGE, and syntax compliance. We believe that ReqGen can promote the efficiency and intelligence of specifying software requirements. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Injection; Requirements Syntax; Software Requirements Generation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Gold Open Access}
}

@ARTICLE{Sun2023345,
	author = {Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
	title = {Minimising Biasing Word Errors for Contextual ASR with the Tree-Constrained Pointer Generator},
	year = {2023},
	journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
	volume = {31},
	pages = {345 - 354},
	doi = {10.1109/TASLP.2022.3224286},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144075835&doi=10.1109%2FTASLP.2022.3224286&partnerID=40&md5=c54f33bae9ada858f2c8487d8ec8cbb1},
	abstract = {Contextual knowledge is essential for reducing speech recognition errors on high-valued long-tail words. This paper proposes a novel tree-constrained pointer generator (TCPGen) component that enables end-to-end ASR models to bias towards a list of long-tail words obtained using external contextual information. With only a small overhead in memory use and computation cost, TCPGen can structure thousands of biasing words efficiently into a symbolic prefix-tree, and creates a neural shortcut between the tree and the final ASR output to facilitate the recognition of the biasing words. To enhance TCPGen, we further propose a novel minimum biasing word error (MBWE) loss that directly optimises biasing word errors during training, along with a biasing-word-driven language model discounting (BLMD) method during the test. All contextual ASR systems were evaluated on the public Librispeech audiobook corpus and the data from the dialogue state tracking challenges (DSTC) with the biasing lists extracted from the dialogue-system ontology. Consistent word error rate (WER) reductions were achieved with TCPGen, which were particularly significant on the biasing words with around 40% relative reductions in the recognition error rates. MBWE and BLMD further improved the effectiveness of TCPGen, and achieved more significant WER reductions on the biasing words. TCPGen also achieved zero-shot learning of words not in the audio training set with large WER reductions on the out-of-vocabulary words in the biasing list. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Contextual Speech Recognition; End-to-end; Language Model Discounting; Minimum Bayes' Risk; Pointer Generator; Computational Linguistics; Errors; Speech Communication; Speech Processing; Zero-shot Learning; Contextual Knowledge; Contextual Speech Recognition; End To End; Language Model; Language Model Discounting; Long Tail; Minimum Bayes Risk; Pointer Generator; Recognition Error; Word Error Rate Reductions; Speech Recognition},
	keywords = {Computational linguistics; Errors; Speech communication; Speech processing; Zero-shot learning; Contextual knowledge; Contextual speech recognition; End to end; Language model; Language model discounting; Long tail; Minimum bayes risk; Pointer generator; Recognition error; Word error rate reductions; Speech recognition},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Zhu2022,
	author = {Zhu, Yiheng and Zhang, Chengxin and Yu, Dongjun and Zhang, Yang},
	title = {Integrating unsupervised language model with triplet neural networks for protein gene ontology prediction},
	year = {2022},
	journal = {PLOS Computational Biology},
	volume = {18},
	number = {12},
	pages = {},
	doi = {10.1371/journal.pcbi.1010793},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144617184&doi=10.1371%2Fjournal.pcbi.1010793&partnerID=40&md5=2cb8075a3294e5e993a7b7e7b4824597},
	abstract = {Accurate identification of protein function is critical to elucidate life mechanisms and design new drugs. We proposed a novel deep-learning method, ATGO, to predict Gene Ontology (GO) attributes of proteins through a triplet neural-network architecture embedded with pre-trained language models from protein sequences. The method was systematically tested on 1068 non-redundant benchmarking proteins and 3328 targets from the third Critical Assessment of Protein Function Annotation (CAFA) challenge. Experimental results showed that ATGO achieved a significant increase of the GO prediction accuracy compared to the state-of-the-art approaches in all aspects of molecular function, biological process, and cellular component. Detailed data analyses showed that the major advantage of ATGO lies in the utilization of pre-trained transformer language models which can extract discriminative functional pattern from the feature embeddings. Meanwhile, the proposed triplet network helps enhance the association of functional similarity with feature similarity in the sequence embedding space. In addition, it was found that the combination of the network scores with the complementary homology-based inferences could further improve the accuracy of the predicted models. These results demonstrated a new avenue for high-accuracy deep-learning function prediction that is applicable to large-scale protein function annotations from sequence alone. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Computational Linguistics; Deep Learning; Embeddings; Forecasting; Genes; Network Architecture; Proteins; Gene Ontology; Identification Of Proteins; Language Model; Learning Methods; Neural Network Architecture; Neural-networks; Protein Function Annotation; Protein Functions; Protein Genes; Protein Sequences; Gene Ontology; Accuracy; Animal Experiment; Arabidopsis; Article; Controlled Study; Deep Learning; Diagnostic Test Accuracy Study; Gene Ontology; Gene Product; Human; Language Model; Mouse; Nerve Cell Network; Nonhuman; Prediction; Probability; Protein Function; Sensitivity And Specificity; Sequence Homology; Validation Process; Biology; Genetics; Language; Metabolism; Procedures; Protein; Computational Biology; Language; Neural Networks, Computer},
	keywords = {Computational linguistics; Deep learning; Embeddings; Forecasting; Genes; Network architecture; Proteins; Gene ontology; Identification of proteins; Language model; Learning methods; Neural network architecture; Neural-networks; Protein function annotation; Protein functions; Protein genes; Protein sequences; Gene Ontology; accuracy; animal experiment; Arabidopsis; Article; controlled study; deep learning; diagnostic test accuracy study; gene ontology; gene product; human; language model; mouse; nerve cell network; nonhuman; prediction; probability; protein function; sensitivity and specificity; sequence homology; validation process; biology; genetics; language; metabolism; procedures; protein; Computational Biology; Language; Neural Networks, Computer},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 41; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Wang2022,
	author = {Wang, Zichen and Combs, Steven A. and Brand, Ryan M. and Calvo, Miguel Romero and Xu, Panpan and Price, George and Golovach, Nataliya and Salawu, Emmanuel Oluwatobi and Wise, Colby J. and Ponnapalli, Sri Priya},
	title = {LM-GVP: an extensible sequence and structure informed deep learning framework for protein property prediction},
	year = {2022},
	journal = {Scientific Reports},
	volume = {12},
	number = {1},
	pages = {},
	doi = {10.1038/s41598-022-10775-y},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128972187&doi=10.1038%2Fs41598-022-10775-y&partnerID=40&md5=2c3151115c16d4a5c9c6ce78653bcb53},
	abstract = {Proteins perform many essential functions in biological systems and can be successfully developed as bio-therapeutics. It is invaluable to be able to predict their properties based on a proposed sequence and structure. In this study, we developed a novel generalizable deep learning framework, LM-GVP, composed of a protein Language Model (LM) and Graph Neural Network (GNN) to leverage information from both 1D amino acid sequences and 3D structures of proteins. Our approach outperformed the state-of-the-art protein LMs on a variety of property prediction tasks including fluorescence, protease stability, and protein functions from Gene Ontology (GO). We also illustrated insights into how a GNN prediction head can inform the fine-tuning of protein LMs to better leverage structural information. We envision that our deep learning framework will be generalizable to many protein property prediction problems to greatly accelerate protein engineering and drug development. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Protein; Amino Acid Sequence; Chemistry; Language; Amino Acid Sequence; Deep Learning; Language; Neural Networks, Computer; Proteins},
	keywords = {protein; amino acid sequence; chemistry; language; Amino Acid Sequence; Deep Learning; Language; Neural Networks, Computer; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 57}
}

@ARTICLE{Nourani20223744,
	author = {Nourani, Esmaeil and Asgari, Ehsaneddin and McHardy, Alice Carolyn and Kaazempur-Mofrad, Mohammad Reza},
	title = {TripletProt: Deep Representation Learning of Proteins Based On Siamese Networks},
	year = {2022},
	journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	volume = {19},
	number = {6},
	pages = {3744 - 3753},
	doi = {10.1109/TCBB.2021.3108718},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144974462&doi=10.1109%2FTCBB.2021.3108718&partnerID=40&md5=3826a8f3a9386e049cd885e29680fba4},
	abstract = {Pretrained representations have recently gained attention in various machine learning applications. Nonetheless, the high computational costs associated with training these models have motivated alternative approaches for representation learning. Herein we introduce TripletProt, a new approach for protein representation learning based on the Siamese neural networks. Representation learning of biological entities which capture essential features can alleviate many of the challenges associated with supervised learning in bioinformatics. The most important distinction of our proposed method is relying on the protein-protein interaction (PPI) network. The computational cost of the generated representations for any potential application is significantly lower than comparable methods since the length of the representations is significantly smaller than that in other approaches. TripletProt offers great potentials for the protein informatics tasks and can be widely applied to similar tasks. We evaluate TripletProt comprehensively in protein functional annotation tasks including sub-cellular localization (14 categories) and gene ontology prediction (more than 2000 classes), which are both challenging multi-class, multi-label classification machine learning problems. We compare the performance of TripletProt with the state-of-the-art approaches including a recurrent language model-based approach (i.e., UniRep), as well as a protein-protein interaction (PPI) network and sequence-based method (i.e., DeepGO). Our TripletProt showed an overall improvement of F1 score in the above mentioned comprehensive functional annotation tasks, solely relying on the PPI network. Availability: The source code and datasets are available at https://github.com/EsmaeilNourani/TripletProt. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein Representation Learning; Siamese Networks; Triplet Loss; Protein; Proteins; Bioinformatics; Classification (of Information); Deep Learning; Gene Ontology; Biological Entities; Computational Costs; Functional Annotation; Machine Learning Applications; Neural-networks; New Approaches; Protein Representation Learning; Protein-protein Interaction Networks; Siamese Network; Triplet Loss; Proteins; Protein; Language; Metabolism; Protein Analysis; Software; Language; Neural Networks, Computer; Protein Interaction Maps; Software},
	keywords = {Bioinformatics; Classification (of information); Deep learning; Gene Ontology; Biological entities; Computational costs; Functional annotation; Machine learning applications; Neural-networks; New approaches; Protein representation learning; Protein-protein interaction networks; Siamese network; Triplet loss; Proteins; protein; language; metabolism; protein analysis; software; Language; Neural Networks, Computer; Protein Interaction Maps; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Kabir2022,
	author = {Kabir, Anowarul and Shehu, Amarda},
	title = {GOProFormer: A Multi-Modal Transformer Method for Gene Ontology Protein Function Prediction},
	year = {2022},
	journal = {Biomolecules},
	volume = {12},
	number = {11},
	pages = {},
	doi = {10.3390/biom12111709},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142652245&doi=10.3390%2Fbiom12111709&partnerID=40&md5=148cd310824fd9289964d3d22e9176fa},
	abstract = {Protein Language Models (PLMs) are shown to be capable of learning sequence representations useful for various prediction tasks, from subcellular localization, evolutionary relationships, family membership, and more. They have yet to be demonstrated useful for protein function pre-diction. In particular, the problem of automatic annotation of proteins under the Gene Ontology (GO) framework remains open. This paper makes two key contributions. It debuts a novel method that leverages the transformer architecture in two ways. A sequence transformer encodes protein sequences in a task-agnostic feature space. A graph transformer learns a representation of GO terms while respecting their hierarchical relationships. The learned sequence and GO terms representations are combined and utilized for multi-label classification, with the labels corresponding to GO terms. The method is shown superior over recent representative GO prediction methods. The second major contribution in this paper is a deep investigation of different ways of constructing training and testing datasets. The paper shows that existing approaches under-or over-estimate the generalization power of a model. A novel approach is proposed to address these issues, resulting in a new benchmark dataset to rigorously evaluate and compare methods and advance the state-of-the-art. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontology; Multi-modal Transformer; Protein Function; Protein; Proteins; Agnostic; Amino Acid Sequence; Article; Gene Ontology; Gene Sequence; Human; Human Experiment; Multilabel Classification; Prediction; Protein Function; Genetics; Metabolism; Protein; Amino Acid Sequence; Gene Ontology; Proteins},
	keywords = {Agnostic; amino acid sequence; article; gene ontology; gene sequence; human; human experiment; multilabel classification; prediction; protein function; genetics; metabolism; protein; Amino Acid Sequence; Gene Ontology; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 18; All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Meng20224446,
	author = {Meng, Zaiqiao and Okhmatovskaia, Anya and Polleri, Maxime and Shen, Yannan and Powell, Guido Antonio and Fu, Zihao and Ganser, Iris and Zhang, Meiru and King, Nicholas B. and Buckeridge, David Llewellyn},
	title = {BioCaster in 2021: automatic disease outbreaks detection from global news media},
	year = {2022},
	journal = {Bioinformatics},
	volume = {38},
	number = {18},
	pages = {4446 - 4448},
	doi = {10.1093/bioinformatics/btac497},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140395378&doi=10.1093%2Fbioinformatics%2Fbtac497&partnerID=40&md5=8dccce952a97547856d868f80afc838c},
	abstract = {Summary: BioCaster was launched in 2008 to provide an ontology-based text mining system for early disease detection from open news sources. Following a 6-year break, we have re-launched the system in 2021. Our goal is to systematically upgrade the methodology using state-of-the-art neural network language models, whilst retaining the original benefits that the system provided in terms of logical reasoning and automated early detection of infectious disease outbreaks. Here, we present recent extensions such as neural machine translation in 10 languages, neural classification of disease outbreak reports and a new cloud-based visualization dashboard. Furthermore, we discuss our vision for further improvements, including combining risk assessment with event semantics and assessing the risk of outbreaks with multi-granularity. We hope that these efforts will benefit the global public health community. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Mining; Epidemic; Health Survey; Procedures; Semantics; Data Mining; Disease Outbreaks; Population Surveillance; Semantics},
	keywords = {data mining; epidemic; health survey; procedures; semantics; Data Mining; Disease Outbreaks; Population Surveillance; Semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 11; All Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Mishra2022193,
	author = {Mishra, Divyansh Shankar and Agarwal, Abhinav and Swathi, B. P. and Akshay, K. C.},
	title = {Natural language query formalization to SPARQL for querying knowledge bases using Rasa},
	year = {2022},
	journal = {Progress in Artificial Intelligence},
	volume = {11},
	number = {3},
	pages = {193 - 206},
	doi = {10.1007/s13748-021-00271-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120888895&doi=10.1007%2Fs13748-021-00271-1&partnerID=40&md5=da87f565dbb18ee707f102b1a1eeaae0},
	abstract = {The idea of data to be semantically linked and the subsequent usage of this linked data with modern computer applications has been one of the most important aspects of Web 3.0. However, the actualization of this aspect has been challenging due to the difficulties associated with building knowledge bases and using formal languages to query them. In this regard, SPARQL, a recursive acronym for standard query language and protocol for Linked Open Data and Resource Description Framework databases, is a most popular formal querying language. Nonetheless, writing SPARQL queries is known to be difficult, even for experts. Natural language query formalization, which involves semantically parsing natural language queries to their formal language equivalents, has been an essential step in overcoming this steep learning curve. Recent work in the field has seen the usage of artificial intelligence (AI) techniques for language modelling with adequate accuracy. This paper discusses a design for creating a closed domain ontology, which is then used by an AI-powered chat-bot that incorporates natural language query formalization for querying linked data using Rasa for entity extraction after intent recognition. A precision–recall analysis is performed using in-built Rasa tools in conjunction with our own testing parameters, and it is found that our system achieves a precision of 0.78, recall of 0.79 and F1-score of 0.79, which are better than the current state of the art. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Natural Language Query Formalization; Nlu; Ontology; Rasa; Sparql; Data Handling; Formal Languages; Linked Data; Modeling Languages; Natural Language Processing Systems; Open Data; Query Languages; Formalisation; Natural Language Queries; Natural Language Query Formalization; Nlu; Ontology's; Rasa; Resources Description Frameworks; Sparql; Standard Query Languages; Web 3.0; Ontology},
	keywords = {Data handling; Formal languages; Linked data; Modeling languages; Natural language processing systems; Open Data; Query languages; Formalisation; Natural language queries; Natural language query formalization; NLU; Ontology's; Rasa; Resources description frameworks; SPARQL; Standard query languages; Web 3.0; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Gao20221191,
	author = {Gao, Jialiang and Lu, Feng and Peng, Peng and Xu, Yang},
	title = {Construction of Tourism Attraction Knowledge Graph Based on Web Text and Transfer Learning; 基于网络文本迁移学习的旅游知识图谱构建},
	year = {2022},
	journal = {Wuhan Daxue Xuebao (Xinxi Kexue Ban)/Geomatics and Information Science of Wuhan University},
	volume = {47},
	number = {8},
	pages = {1191 - 1219},
	doi = {10.13203/j.whugis20220120},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138141244&doi=10.13203%2Fj.whugis20220120&partnerID=40&md5=d681ee709b1c81368c826c6f6dfb7678},
	abstract = {Objectives The rapid development of information and communication technology has facilitated the online tourism service and massive web text, which provides a new opportunity for tourism sector planning and personalized recommendation. However, owing to the characteristics of semantic vagueness and low signal-to-noise ratio, the web text is difficult to get utilized directly. Therefore, how to integrate the technologies of knowledge engineering, natural language processing and machine learning, so as to form a formalized domain knowledge graph from abundant tourism text, has attracted much attention. Methods This paper proposes a tourism knowledge graph construction method based on tourism domain ontology and transfer learning. Firstly, the ontology of tourist attractions is defined based on the domain specifications and standards, which support a comprehensive and systematic description of the semantic characteristics of attractions. Secondly, a transfer learning method is adopted to transform the pre-training language model into a customized knowledge extractor to acquire knowledge triples accurately from web text, which is integrated with the scattered tourism-related information including tourist check-ins and POI (point of interest) attributes to build a systematic knowledge graph. Results Experimental results show that the proposed knowledge extractor improves the accuracy (average area under the curve) and integrity (the number of sematic characteristics) of acquisition of sematic knowledge by 50.7% and 670%, respectively, compared with the common LDA (latent Dirichlet allocation) model. The constructed knowledge graph of tourist attractions contained 77 039 entities, 16 types of relationship, and total 10 971 810 triples. Conclusions Through the unified organization paradigm of triplet knowledge, the study realizes the fusion and integration of multi-source heterogeneous tourism data, and addresses the potential systemic risk in the decision-making process based on a single data source. It is argued that the constructed knowledge graph can fully capture the real tourism scene, support in-depth analysis of tourist behaviors and demands at different scales and granularities, and provide decision support for sustainable developments of tourist destinations. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Tourism Management; Transfer Learning; Web Text Mining; Decision Making; Decision Support Systems; Domain Knowledge; Engineering Education; Knowledge Acquisition; Learning Algorithms; Learning Systems; Machine Learning; Natural Language Processing Systems; Ontology; Semantics; Signal To Noise Ratio; Statistics; Graph-based; Information And Communication Technologies; Knowledge Graphs; Tourism Management; Tourism Services; Tourist Attractions; Transfer Learning; Web Text Mining; Web Texts; Web Transfers; Knowledge Graph; Knowledge Based System; Machine Learning; Signal-to-noise Ratio; Tourism Development; Tourism Management; Tourist Destination},
	keywords = {Decision making; Decision support systems; Domain Knowledge; Engineering education; Knowledge acquisition; Learning algorithms; Learning systems; Machine learning; Natural language processing systems; Ontology; Semantics; Signal to noise ratio; Statistics; Graph-based; Information and Communication Technologies; Knowledge graphs; Tourism management; Tourism services; Tourist attractions; Transfer learning; Web text mining; Web texts; Web transfers; Knowledge graph; knowledge based system; machine learning; signal-to-noise ratio; tourism development; tourism management; tourist destination},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{2022,
	title = {Proceedings of the Working Conference on Advanced Visual Interfaces, AVI 2022},
	year = {2022},
	journal = {ACM International Conference Proceeding Series},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132251397&partnerID=40&md5=f8ad670d3200af9fd34f6dcdf57dd0bc},
	abstract = {The proceedings contain 89 papers. The topics discussed include: follow your nose: history frames the future; extended reality and passengers of the future; soma design – intertwining aesthetics, ethics and movement; a comparative study on single-handed keyboards on large-screen mobile devices; RepliGES and GEStory: visual tools for systematizing and consolidating knowledge on user-defined gestures; composites: a tangible interaction paradigm for visual data analysis in design practice; impending success or failure? an investigation of gaze-based user predictions during interaction with ontology visualizations; extended UTAUT model to analyze the acceptance of virtual assistant’s recommendations using interactive visualizations; context-situated visualization of biclusters to aid decisions: going beyond subspaces with parallel coordinates; and interactive clustering and high-recall information retrieval using language models. © 2022 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Quamar2022319,
	author = {Quamar, Abdul and Efthymiou, Vasilis and Lei, Chuan and Özcan, Fatma},
	title = {Natural Language Interfaces to Data},
	year = {2022},
	journal = {Foundations and Trends in Databases},
	volume = {11},
	number = {4},
	pages = {319 - 414},
	doi = {10.1561/1900000078},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146640292&doi=10.1561%2F1900000078&partnerID=40&md5=95241022232971a28c840314f33518fa},
	abstract = {Recent advances in natural language understanding and processing have resulted in renewed interest in natural language interfaces to data, which provide an easy mechanism for non-technical users to access and query the data. While early systems evolved from keyword search and focused on simple factual queries, the complexity of both the input sentences as well as the generated SQL queries has evolved over time. More recently, there has also been a lot of focus on using conversational interfaces for data analytics, empowering a line of business owners and non-technical users with quick insights into the data. There are three main challenges in natural language querying: (1) identifying the entities involved in the user utterance, (2) connecting the different entities in a meaningful way over the underlying data source to interpret user intents, and finally (3) generating a structured query in the form of SQL or SPARQL. There are two main approaches in the literature for interpreting a user’s natural language query. Rule-based systems make use of semantic indices, ontologies, and knowledge graphs to identify the entities in the query, understand the intended relationships between those entities, and utilize grammars to generate the target queries. With the advances in deep learning-based language models, there have been many text-to-SQL approaches that try to interpret the query holistically using deep learning models. Hybrid approaches that utilize both rule-based techniques as well as deep learning models are also emerging by combining the strengths of both approaches. Conversational interfaces are the next natural step to one-shot natural language querying by exploiting query context between multiple turns of conversation for disambiguation. In this monograph, we review the background technologies that are used in natural language interfaces, and survey the different approaches to natural language querying. We also describe conversational interfaces for data analytics and discuss several benchmarks used for natural language querying research and evaluation. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Deep Learning; Knowledge Graph; Natural Language Processing Systems; Search Engines; Semantics; Conversational Interface; Data Analytics; Keyword Search; Language Processing; Learning Models; Natural Language Interfaces; Natural Language Understanding; Natural Languages; Non-technical Users; Simple++; Data Analytics},
	keywords = {Computational linguistics; Deep learning; Knowledge graph; Natural language processing systems; Search engines; Semantics; Conversational interface; Data analytics; Keyword search; Language processing; Learning models; Natural language interfaces; Natural language understanding; Natural languages; Non-technical users; Simple++; Data Analytics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Rees2022168,
	author = {Rees, Tobias},
	title = {Non-Human Words: On GPT-3 as a Philosophical Laboratory},
	year = {2022},
	journal = {Daedalus},
	volume = {151},
	number = {2},
	pages = {168 - 182},
	doi = {10.1162/DAED_a_01908},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129134607&doi=10.1162%2FDAED_a_01908&partnerID=40&md5=d66c255dac0ca9ed0034286e55829b3d},
	abstract = {In this essay, I investigate the effect of OpenAI’s GPT-3 on the modern concept of the human (as alone capable of reason and language) and of machines (as devoid of reason and language). I show how GPT-3 and other transformer-based language models give rise to a new, structuralist concept of language, implicit in which is a new understanding of human and machine that unfolds far beyond the reach of the categories we have inherited from the past. I try to make compelling the argument that AI companies like OpenAI, Google, Facebook, or Microsoft effectively are philosophical laboratories (insofar as they disrupt the old concepts/ontologies we live by) and I ask what it would mean to build AI products from the perspective of the philosophical disruptions they provoke: can we liberate AI from the concept of the human we inherited from the past? © 2022 Elsevier B.V., All rights reserved.},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 21; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Ye2022778,
	author = {Ye, Hongbin and Zhang, Ningyu and Deng, Shumin and Chen, Xiang and Chen, Hui and Xiong, Feiyu and Chen, Xi and Chen, Huajun},
	title = {Ontology-enhanced Prompt-tuning for Few-shot Learning},
	year = {2022},
	pages = {778 - 787},
	doi = {10.1145/3485447.3511921},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128259903&doi=10.1145%2F3485447.3511921&partnerID=40&md5=0ad2ba24916dffec719dafe07e56c2bd},
	abstract = {Few-shot Learning (FSL) is aimed to make predictions based on a limited number of samples. Structured data such as knowledge graphs and ontology libraries has been leveraged to benefit the few-shot setting in various tasks. However, the priors adopted by the existing methods suffer from challenging knowledge missing, knowledge noise, and knowledge heterogeneity, which hinder the performance for few-shot learning. In this study, we explore knowledge injection for FSL with pre-trained language models and propose ontology-enhanced prompt-tuning (OntoPrompt). Specifically, we develop the ontology transformation based on the external knowledge graph to address the knowledge missing issue, which fulfills and converts structure knowledge to text. We further introduce span-sensitive knowledge injection via a visible matrix to select informative knowledge to handle the knowledge noise issue. To bridge the gap between knowledge and text, we propose a collective training algorithm to optimize representations jointly. We evaluate our proposed OntoPrompt in three tasks, including relation extraction, event extraction, and knowledge graph completion, with eight datasets. Experimental results demonstrate that our approach can obtain better few-shot performance than baselines. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Event Extraction; Few-shot Learning; Knowledge Graph Completion; Ontology; Prompt-tuning; Relation Extraction; Extraction; Knowledge Management; Ontology; Events Extractions; Few-shot Learning; Knowledge Graph Completion; Knowledge Graphs; Number Of Samples; Ontology's; Performance; Prediction-based; Prompt-tuning; Relation Extraction; Knowledge Graph},
	keywords = {Extraction; Knowledge management; Ontology; Events extractions; Few-shot learning; Knowledge graph completion; Knowledge graphs; Number of samples; Ontology's; Performance; Prediction-based; Prompt-tuning; Relation extraction; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 69; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Brandes20222102,
	author = {Brandes, Nadav and Ofer, Dan and Peleg, Yam and Rappoport, Nadav and Linial, Michal},
	title = {ProteinBERT: a universal deep-learning model of protein sequence and function},
	year = {2022},
	journal = {Bioinformatics},
	volume = {38},
	number = {8},
	pages = {2102 - 2110},
	doi = {10.1093/bioinformatics/btac020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128725695&doi=10.1093%2Fbioinformatics%2Fbtac020&partnerID=40&md5=317284e13f734d2b12fd67e6a0510a8d},
	abstract = {Summary: Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme combines language modeling with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to long sequences. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains near state-of-the-art performance, and sometimes exceeds it, on multiple benchmarks covering diverse protein properties (including protein structure, post-translational modifications and biophysical attributes), despite using a far smaller and faster model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Amino Acid Sequence; Article; Deep Learning; Gene Ontology; Human; Human Experiment; Language Test; Prediction; Protein Function; Protein Processing; Protein Structure; Chemistry; Language; Natural Language Processing; Protein; Amino Acid Sequence; Deep Learning; Language; Natural Language Processing; Proteins},
	keywords = {amino acid sequence; article; deep learning; gene ontology; human; human experiment; language test; prediction; protein function; protein processing; protein structure; chemistry; language; natural language processing; protein; Amino Acid Sequence; Deep Learning; Language; Natural Language Processing; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 501; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Hsu2022223,
	author = {Hsu, Hao Hsuan and Huang, Nenfu},
	title = {Xiao-Shih: A Self-Enriched Question Answering Bot With Machine Learning on Chinese-Based MOOCs},
	year = {2022},
	journal = {IEEE Transactions on Learning Technologies},
	volume = {15},
	number = {2},
	pages = {223 - 237},
	doi = {10.1109/TLT.2022.3162572},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127504665&doi=10.1109%2FTLT.2022.3162572&partnerID=40&md5=c40a12601b7bac535006b76426c54dc6},
	abstract = {This article introduces Xiao-Shih, the first intelligent question answering bot on Chinese-based massive open online courses (MOOCs). Question answering is critical for solving individual problems. However, instructors on MOOCs must respond to many questions, and learners must wait a long time for answers. To address this issue, Xiao-Shih integrates many novel natural language processing and machine learning approaches to achieve state-of-the-art performance. Furthermore, Xiao-Shih has a built-in self-enriched mechanism for expanding the knowledge base through open community-based question answering. This article proposes a novel approach, known as spreading question similarity (SQS), which iterates similar keywords on our keyword networks to find duplicate questions. Compared with BERT, an advanced neural language model, the results showed that SQS outperforms BERT on recall and accuracy above a prediction probability threshold of 0.8. After training, Xiao-Shih achieved a perfect correct rate. Furthermore, Xiao-Shih outperforms Jill Watson 1.0, which is a noted question answering bot, on answer rate with the self-enriched mechanism. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Answer Selection; Machine Learning (ml); Massive Open Online Courses (moocs); Natural Language Processing (nlp); Ontologies; Question Answering Bot; Question Retrieval; Bit Error Rate; Computer Aided Instruction; E-learning; Knowledge Based Systems; Learning Systems; Natural Language Processing Systems; Answer Selection; Bit-error Rate; Chatbots; Machine Learning Approaches; Machine-learning; Massive Open Online Course; Ontology's; Question Answering; Question Answering Bot; Question Retrieval; Learning Algorithms},
	keywords = {Bit error rate; Computer aided instruction; E-learning; Knowledge based systems; Learning systems; Natural language processing systems; Answer selection; Bit-error rate; Chatbots; Machine learning approaches; Machine-learning; Massive open online course; Ontology's; Question Answering; Question answering bot; Question retrieval; Learning algorithms},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19}
}

@ARTICLE{Ezaldeen2022,
	author = {Ezaldeen, Hadi and Misra, Rachita and Bisoy, Sukant Kishoro and Alatrash, Rawaa and Priyadarshini, Rojalina K.},
	title = {A hybrid E-learning recommendation integrating adaptive profiling and sentiment analysis},
	year = {2022},
	journal = {Journal of Web Semantics},
	volume = {72},
	pages = {},
	doi = {10.1016/j.websem.2021.100700},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122815302&doi=10.1016%2Fj.websem.2021.100700&partnerID=40&md5=89be71aa0389b85afbeee21826e5dc24},
	abstract = {This research proposes a novel framework named Enhanced e-Learning Hybrid Recommender System (ELHRS) that provides an appropriate e-content with the highest predicted ratings corresponding to the learner's particular needs. To accomplish this, a new model is developed to deduce the Semantic Learner Profile automatically. It adaptively associates the learning patterns and rules depending on the learner's behavior and the semantic relations computed in the semantic matrix that mutually links e-learning materials and terms. Here, a semantic-based approach for term expansion is introduced using DBpedia and WordNet ontologies. Further, various sentiment analysis models are proposed and incorporated as a part of the recommender system to predict ratings of e-learning resources from posted text reviews utilizing fine-grained sentiment classification on five discrete classes. Qualitative Natural Language Processing (NLP) methods with tailored-made Convolutional Neural Network (CNN) are developed and evaluated on our customized dataset collected for a specific domain and a public dataset. Two improved language models are introduced depending on Skip-Gram (S-G) and Continuous Bag of Words (CBOW) techniques. In addition, a robust language model based on hybridization of these couple of methods is developed to derive better vocabulary representation, yielding better accuracy 89.1% for the CNN-Three-Channel-Concatenation model. The suggested recommendation methodology depends on the learner's preferences, other similar learners’ experience and background, deriving their opinions from the reviews towards the best learning resources. This assists the learners in finding the desired e-content at the proper time. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Adaptive Profiling; Convolutional Neural Network; Fine-grained Sentiment Analysis; Hybrid E-learning Recommendation; Semantic Learner Profile; Word Embeddings; Computational Linguistics; Convolution; Convolutional Neural Networks; E-learning; Learning Systems; Ontology; Recommender Systems; Semantic Web; Semantics; Adaptive Profiling; Convolutional Neural Network; E - Learning; Embeddings; Fine Grained; Fine-grained Sentiment Analyse; Hybrid E-learning Recommendation; Learner Profiles; Semantic Learner Profile; Sentiment Analysis; Word Embedding},
	keywords = {Computational linguistics; Convolution; Convolutional neural networks; E-learning; Learning systems; Ontology; Recommender systems; Semantic Web; Semantics; Adaptive profiling; Convolutional neural network; E - learning; Embeddings; Fine grained; Fine-grained sentiment analyse; Hybrid E-learning recommendation; Learner profiles; Semantic learner profile; Sentiment analysis; Word embedding},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 67}
}

@CONFERENCE{Shishaev2022,
	author = {Shishaev, Maxim and Dikovitsky, Vladimir and Pimeshkov, Vadim K.},
	title = {Application of neural network language models based on distributive semantics for ontological modeling of the domain},
	year = {2022},
	journal = {Journal of Physics: Conference Series},
	volume = {2182},
	number = {1},
	pages = {},
	doi = {10.1088/1742-6596/2182/1/012033},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127688801&doi=10.1088%2F1742-6596%2F2182%2F1%2F012033&partnerID=40&md5=1bccb672abdd2af02cbc269748429a40},
	abstract = {The article discusses the technology of automated formation of SKOS-ontologies for semantic modeling of the subject area, based on natural language texts analysis. The technology is based on neural network and distributive (vector) language models. A brief description of the content and formulation of the problem of extracting concepts and relations from natural language texts is given, the results of constructing a neural network classifier of SKOS relations using the Glove vector model, as well as an example of using the technology to construct a fragment of an applied SKOS ontology are given. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Modeling Languages; Semantic Web; Semantics; Area-based; Language Model; Model-based Opc; Natural Languages Texts; Network Language; Neural-networks; Ontological Modeling; Ontology's; Semantic Modelling; Text Analysis; Ontology},
	keywords = {Computational linguistics; Modeling languages; Semantic Web; Semantics; Area-based; Language model; Model-based OPC; Natural languages texts; Network language; Neural-networks; Ontological modeling; Ontology's; Semantic modelling; Text analysis; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Gold Open Access}
}

@CONFERENCE{Zhang2022274,
	author = {Zhang, Xiangliang and Jia, Yangli and Zhang, Zhenling and Kang, Qi and Zhang, Yongchen and Jia, Hongling},
	title = {Improving End-to-End Biomedical Question Answering System},
	year = {2022},
	journal = {ACM International Conference Proceeding Series},
	pages = {274 - 279},
	doi = {10.1145/3532213.3532254},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134385469&doi=10.1145%2F3532213.3532254&partnerID=40&md5=082e8620708be4a77b7f8c7b77e5b462},
	abstract = {Biomedical question answering refers to extracting an answer based on given questions and related documents. Existing biomedical question answering research either focuses on a specific stage, such as machine reading comprehension, or uses traditional rule-based methods and ontology with complex construction processes. In this paper, we demonstrate the application of simple but powerful neural-based approaches in improving the end-to-end biomedical question answering system. We employ the BM25-based documents retriever, BERT-based neural ranker, and an answer extraction stage using the BioBERT pre-trained language model. In view of the lack of sufficient training data in the biomedical domain, domain adaptation and data augmentation are adopted to address the question answering task, so as to further reinforce the system performance. Based on our self-built standard large-volume retrieve corpus and neural ranker corpus, we get competitive results on BioASQ8b. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Question Answering; Data Augmentation; Document Re-ranking; Natural Language Processing Systems; Biomedical Question Answering; Complex Construction; Data Augmentation; Document Re-ranking; End To End; Ontology's; Question Answering Systems; Re-ranking; Reading Comprehension; Rule-based Method; Artificial Intelligence},
	keywords = {Natural language processing systems; Biomedical question answering; Complex construction; Data augmentation; Document re-ranking; End to end; Ontology's; Question answering systems; Re-ranking; Reading comprehension; Rule-based method; Artificial intelligence},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Ünsal2022227,
	author = {Ünsal, Serbülent and Ataş, Heval and Albayrak, Muammer and Turhan, Kemal and Acar, Aybar C. and Dogan, Tunca},
	title = {Learning functional properties of proteins with language models},
	year = {2022},
	journal = {Nature Machine Intelligence},
	volume = {4},
	number = {3},
	pages = {227 - 245},
	doi = {10.1038/s42256-022-00457-9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126907351&doi=10.1038%2Fs42256-022-00457-9&partnerID=40&md5=f7ec26149037fb55d623a80dd4a6448b},
	abstract = {Data-centric approaches have been used to develop predictive methods for elucidating uncharacterized properties of proteins; however, studies indicate that these methods should be further improved to effectively solve critical problems in biomedicine and biotechnology, which can be achieved by better representing the data at hand. Novel data representation approaches mostly take inspiration from language models that have yielded ground-breaking improvements in natural language processing. Lately, these approaches have been applied to the field of protein science and have displayed highly promising results in terms of extracting complex sequence–structure–function relationships. In this study we conducted a detailed investigation over protein representation learning by first categorizing/explaining each approach, subsequently benchmarking their performances on predicting: (1) semantic similarities between proteins, (2) ontology-based protein functions, (3) drug target protein families and (4) protein–protein binding affinity changes following mutations. We evaluate and discuss the advantages and disadvantages of each method over the benchmark results, source datasets and algorithms used, in comparison with classical model-driven approaches. Finally, we discuss current challenges and suggest future directions. We believe that the conclusions of this study will help researchers to apply machine/deep learning-based representation techniques to protein data for various predictive tasks, and inspire the development of novel methods. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Binding Energy; Biochemistry; Computational Linguistics; Learning Systems; Natural Language Processing Systems; Ontology; Semantics; Breakings; Complex Sequences; Critical Problems; Data Representations; Data-centric Approaches; Functional Properties; Language Model; Predictive Methods; Property; Protein Science; Proteins},
	keywords = {Binding energy; Biochemistry; Computational linguistics; Learning systems; Natural language processing systems; Ontology; Semantics; Breakings; Complex sequences; Critical problems; Data representations; Data-centric approaches; Functional properties; Language model; Predictive methods; Property; Protein science; Proteins},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 123}
}

@ARTICLE{Zhao2022,
	author = {Zhao, Chenguang and Liu, Tong and Wang, Zheng},
	title = {PANDA2: Protein function prediction using graph neural networks},
	year = {2022},
	journal = {NAR Genomics and Bioinformatics},
	volume = {4},
	number = {1},
	pages = {},
	doi = {10.1093/nargab/lqac004},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125146439&doi=10.1093%2Fnargab%2Flqac004&partnerID=40&md5=427a32f0bc265724b8a98410aa7dbc9c},
	abstract = {High-throughput sequencing technologies have generated massive protein sequences, but the annotations of protein sequences highly rely on the low-throughput and expensive biological experiments. Therefore, accurate and fast computational alternatives are needed to infer functional knowledge from protein sequences. The gene ontology (GO) directed acyclic graph (DAG) contains the hierarchical relationships between GO terms but is hard to be integrated into machine learning algorithms for functional predictions. We developed a deep learning system named PANDA2 to predict protein functions, which used the cutting-edge graph neural network to model the topology of the GO DAG and integrated the features generated by transformer protein language models. Compared with the top 10 methods in CAFA3, PANDA2 ranked first in cellular component ontology (CCO), tied first in biological process ontology (BPO) but had a higher coverage rate, and second in molecular function ontology (MFO). Compared with other recently-developed cutting-edge predictors DeepGOPlus, GOLabeler, and DeepText2GO, and benchmarked on another independent dataset, PANDA2 ranked first in CCO, first in BPO, and second in MFO. PANDA2 can be freely accessed from http://dna.cs.miami.edu/PANDA2/. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Article; Deep Learning; Directed Acyclic Graph; Gene Ontology; Human; Human Experiment; Language; Prediction; Protein Function},
	keywords = {article; deep learning; directed acyclic graph; gene ontology; human; human experiment; language; prediction; protein function},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Deepak2022,
	author = {Deepak, Gerard and Surya, Deepak Ak and Trivedi, Ishdutt and Kumar, Ayush A. and Lingampalli, Amrutha and Vijayan, Santhana},
	title = {An artificially intelligent approach for automatic speech processing based on triune ontology and adaptive tribonacci deep neural networks},
	year = {2022},
	journal = {Computers and Electrical Engineering},
	volume = {98},
	pages = {},
	doi = {10.1016/j.compeleceng.2022.107736},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123759109&doi=10.1016%2Fj.compeleceng.2022.107736&partnerID=40&md5=00b2fa11bf3b2bab8418e0121919bba1},
	abstract = {Automatic Speech Recognition systems have become essential for an independent automation during the present-day era. A hybrid approach for Automatic Speech Recognition, the TriNNOnto has been proposed in this paper which, integrates different approaches like Language Model integrated with dynamic Triune Ontology generation scheme, Acoustic Model and Feature modelling are hybridised based on the Tribonacci based Deep Neural Network, which decides upon the number of layers depending on the size of the samples and their count. The dynamic generation of Ontologies based on the language models and triune ontology for automatic speech recognition is quite novel. The strategies for feature extraction as and the Tribonacci based deep neural network, based the dynamic adjustment of the number of layers using Tribonacci series contributes towards novelty as well as enhances the performance of speech recognition. The proposed strategy has been evaluated for two datasets and an accuracy of 98.15% and 95.18%, have been achieved for the CMUKids and the TIMIT datasets, respectively with low word error rates. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Acoustic Model; Automatic Speech Recognition; Tribonacci Deep Neural Network; Computational Linguistics; Deep Neural Networks; Modeling Languages; Multilayer Neural Networks; Speech Processing; Speech Recognition; Acoustics Model; Automatic Speech Processing; Automatic Speech Recognition; Automatic Speech Recognition System; Hybrid Approach; Language Model; Number Of Layers; Ontology Generation; Ontology's; Tribonacci Deep Neural Network; Ontology},
	keywords = {Computational linguistics; Deep neural networks; Modeling languages; Multilayer neural networks; Speech processing; Speech recognition; Acoustics model; Automatic speech processing; Automatic speech recognition; Automatic speech recognition system; Hybrid approach; Language model; Number of layers; Ontology generation; Ontology's; Tribonacci deep neural network; Ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@CONFERENCE{Tu2022343,
	author = {Tu, Tao and Loreaux, Eric and Chesley, Emma and Lelkes, Ádám D. and Gamble, Paul G. and Bellaiche, Mathias and Seneviratne, Martin G. and Chen, Mingjun},
	title = {Automated LOINC standardization using pre-Trained large language models},
	year = {2022},
	journal = {Proceedings of Machine Learning Research},
	volume = {193},
	pages = {343 - 355},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171421517&partnerID=40&md5=35b85b4aba9ca95f5e71956acbfd651e},
	abstract = {Harmonization of local source concepts to standard clinical terminologies is a prerequisite for multi-center data aggregation and sharing. Challenges in automating the mapping process stem from the idiosyncratic source encoding schemes adopted by different health systems and the lack of large publicly available training data. In this study, we aim to develop a scalable and generalizable machine learning tool to facilitate standardizing laboratory observations to the Logical Observation Identifiers Names and Codes (LOINC). Specifically, we leverage the contextual embedding from pre-Trained T5 models and propose a two-stage fine-Tuning strategy based on contrastive learning to enable learning in a few-shot setting without manual feature engineering. Our method utilizes unlabeled general LOINC ontology and data augmentation to achieve high accuracy on retrieving the most relevant LOINC targets when limited amount of labeled data are available. We further show that our model generalizes well to unseen targets. Taken together, our approach shows great potential to reduce manual effort in LOINC standardization and can be easily extended to mapping other terminologies. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Contrastive Learning; Data Standardization; Large Language Model; Loinc; Medical Entity Linking; Sentence Embedding; T5; Computational Linguistics; Mapping; Natural Language Processing Systems; Standardization; Terminology; Code Standardizations; Contrastive Learning; Data Standardization; Embeddings; Language Model; Large Language Model; Logical Observation Identifiers Names And Codes; Medical Entity Linking; Sentence Embedding; T5},
	keywords = {Computational linguistics; Mapping; Natural language processing systems; Standardization; Terminology; Code standardizations; Contrastive learning; Data standardization; Embeddings; Language model; Large language model; Logical observation identifiers names and codes; Medical entity linking; Sentence embedding; T5},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Vukovic2022564,
	author = {Vukovic, Renato and Heck, Michael and Ruppik, Benjamin M. and van Niekerk, Carel and Zibrowius, Marcus and Gašić, Milica},
	title = {Dialogue Term Extraction using Transfer Learning and Topological Data Analysis},
	year = {2022},
	pages = {564 - 581},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164899247&partnerID=40&md5=41e2c25984990065c1da37e15a9feede},
	abstract = {Goal oriented dialogue systems were originally designed as a natural language interface to a fixed data-set of entities that users might inquire about, further described by domain, slots and values. As we move towards adaptable dialogue systems where knowledge about domains, slots and values may change, there is an increasing need to automatically extract these terms from raw dialogues or related non-dialogue data on a large scale. In this paper, we take an important step in this direction by exploring different features that can enable systems to discover realizations of domains, slots and values in dialogues in a purely data-driven fashion. The features that we examine stem from word embeddings, language modelling features, as well as topological features of the word embedding space. To examine the utility of each feature set, we train a seed model based on the widely used MultiWOZ data-set. Then, we apply this model to a different corpus, the Schema-Guided Dialogue data-set. Our method outperforms the previously proposed approach that relies solely on word embeddings. We also demonstrate that each of the features is responsible for discovering different kinds of content. We believe our results warrant further research towards ontology induction, and continued harnessing of topological data analysis for dialogue and natural language processing research. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Data Handling; Data Mining; Modeling Languages; Natural Language Processing Systems; Speech Processing; Topology; Data Set; Dialogue Systems; Embeddings; Goal-oriented; Large-scales; Learning Data; Natural Language Interfaces; Term Extraction; Topological Data Analysis; Transfer Learning},
	keywords = {Computational linguistics; Data handling; Data mining; Modeling languages; Natural language processing systems; Speech processing; Topology; Data set; Dialogue systems; Embeddings; Goal-oriented; Large-scales; Learning data; Natural language interfaces; Term extraction; Topological data analysis; Transfer learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@CONFERENCE{Zhang2022396,
	author = {Zhang, Xiang and Yu, Bruce X.B. and Liu, Yan and Chen, Gong and Ng, George Wing Yiu and Chia, Namhung and So, Eric Hang Kwong and So, Szesze and Cheung, Victor K.L.},
	title = {Conversational System for Clinical Communication Training Supporting User-defined Tasks},
	year = {2022},
	pages = {396 - 403},
	doi = {10.1109/TALE54877.2022.00071},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163868207&doi=10.1109%2FTALE54877.2022.00071&partnerID=40&md5=8e0d6e8e834a1f7349304e355cbee063},
	abstract = {Effective clinical communication is essential for delivering safe and high-quality patient care, especially in emergent cases. Standard communication protocols have been developed to improve communication accuracy and efficiency. However, traditional training and evaluation require substantial manpower and time, which can be infeasible during public crises when training is most needed. This research aims to facilitate autonomous, low-cost, adaptive clinical communication training via artificial intelligence (AI)-powered techniques. We propose a conversational system for clinical communication training supporting user-defined tasks. Two data augmentation (DA) methods, term replacement and context expansion, are proposed to allow non-professional users to create Al models with a small number of samples. Equipped with biomedical ontology and pre-trained language models, our system is able to simulate clinical communication scenarios, provide timely evaluation, and adapt to new tasks with minimal editing. Various experiments demonstrate that our proposed algorithms can achieve satisfactory performance using a small amount of training data. Real-world practice in local hospitals shows that our system can provide expert-level evaluation and deliver effective clinical communication training. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Autonomous Communication Training; Clinical Communication; Conversational System; Human-computer Interaction; Clinical Research; User Interfaces; Autonomous Communication Training; Autonomous Communications; Clinical Communications; Communications Protocols; Communications Training; Conversational Systems; High Quality; Low-costs; Patient Care; Public Crisis; Human Computer Interaction},
	keywords = {Clinical research; User interfaces; Autonomous communication training; Autonomous communications; Clinical communications; Communications protocols; Communications training; Conversational systems; High quality; Low-costs; Patient care; Public crisis; Human computer interaction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Gnehm202214,
	author = {Gnehm, Ann Sophie and Bühlmann, Eva and Buchs, Helen and Clematide, Simon},
	title = {Fine-Grained Extraction and Classification of Skill Requirements in German-Speaking Job Ads},
	year = {2022},
	pages = {14 - 24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154569390&partnerID=40&md5=749bddd84e60db193aad9f4d9e8882bd},
	abstract = {Monitoring the development of labor market skill requirements is an information need that is more and more approached by applying text mining methods to job advertisement data. We present an approach for fine-grained extraction and classification of skill requirements from German-speaking job advertisements. We adapt pre-trained transformer-based language models to the domain and task of computing meaningful representations of sentences or spans. By using context from job advertisements and the large ESCO domain ontology we improve our similarity-based unsupervised multi-label classification results. Our best model achieves a mean average precision of 0.969 on the skill class level. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Classification (of Information); Computational Linguistics; Text Processing; Best Model; Classification Results; Domain Ontologies; Fine Grained; Labour Market; Language Model; Mining Methods; Multi-label Classifications; Skill Requirements; Text-mining; Extraction},
	keywords = {Classification (of information); Computational linguistics; Text processing; Best model; Classification results; Domain ontologies; Fine grained; Labour market; Language model; Mining methods; Multi-label classifications; Skill requirements; Text-mining; Extraction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@ARTICLE{Pokharel202281,
	author = {Pokharel, Suresh and Sidorov, Evgenii and Caragea, Doina and Dukka, Bahadur K.C.},
	title = {NLP-based Encoding Techniques for Prediction of Post-translational Modification Sites and Protein Functions},
	year = {2022},
	pages = {81 - 128},
	doi = {10.1142/9789811258589_0004},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153650412&doi=10.1142%2F9789811258589_0004&partnerID=40&md5=636d4f9e1c314c4df78286fc92922223},
	abstract = {With advancements in sequencing and proteomics approaches, computational functional annotation of proteins is becoming increasingly crucial. Among these annotations, prediction of post-translational modification (PTM) sites and prediction of function given a protein sequence are two very important problems. Recently, there have been several breakthroughs in Natural Language Processing (NLP) area. Consequently, we have observed an increase in the application of NLP-based techniques in the field of protein bioinformatics. In this chapter, we review various NLP-based encoding techniques for representation of protein sequences. Especially, we classify these approaches based on local/sparse encodings, distributed representation encodings, context-independent word embeddings, contextual word embedding and recent language models based pre-trained encodings. We summarize some of the recent approaches that make use of these NLP-based encodings for the prediction of various types of protein PTM sites and protein functions based on Gene Ontology (GO). Finally, we provide an outlook on possible future research directions for the NLP-based approaches for PTM sites and protein function predictions. © 2023 Elsevier B.V., All rights reserved.},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{Lopes2022180,
	author = {Lopes, Alcides Gonçalves and Carbonera, Joel Lúis and Abel, Mara},
	title = {Learning Domain Ontologies Based On Top-Level Ontology Concepts Using Language Models And Informal Definitions},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3346},
	pages = {180 - 185},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150217899&partnerID=40&md5=6161d8c3f141c3af53e3a240d368bb42},
	abstract = {Ontology development is a challenging task that encompasses many time-consuming activities. One of these activities is the classification of the domain entities (concepts and instances) according to top-level concepts. This activity is usually performed manually by an ontology engineer. However, when the set of entities increases in size, associating each domain entity to the proper top-level ontological concept becomes challenging and requires a high level of expertise in both the target domain and ontology engineering. In this context, this work describes an approach for learning domain ontologies based on top-level ontology concepts using informal definitions as input. In our approach, we used informal definitions of the domain entities as text input of a language model that predicts their proper top-level concepts. Also, we present a methodology to extract datasets from existing domain ontologies to evaluate the proposed approach. Our experiments show that we have promising results in classifying domain entities into top-level ontology concepts. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Neural Network; Ontology Learning; Text Classification; Classification (of Information); Computational Linguistics; Ontology; Text Processing; Domain Entities; Domain Ontologies; Language Model; Ontology Concepts; Ontology Development; Ontology Learning; Ontology's; Ontology-based; Target Domain; Text Classification; Deep Neural Networks},
	keywords = {Classification (of information); Computational linguistics; Ontology; Text processing; Domain entities; Domain ontologies; Language model; Ontology concepts; Ontology development; Ontology learning; Ontology's; Ontology-based; Target domain; Text classification; Deep neural networks},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lal20221204,
	author = {Lal, Yash Kumar and Tandon, Niket and Aggarwal, Tanvi and Liu, Horace and Chambers, Nathanael and Mooney, Raymond J. and Balasubramanian, Niranjan},
	title = {Using Commonsense Knowledge to Answer Why-Questions},
	year = {2022},
	pages = {1204 - 1219},
	doi = {10.18653/v1/2022.emnlp-main.79},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149443386&doi=10.18653%2Fv1%2F2022.emnlp-main.79&partnerID=40&md5=dcc078b8d5204fc1c6b91970822cb8bf},
	abstract = {Answering questions in narratives about why events happened often requires commonsense knowledge external to the text. What aspects of this knowledge are available in large language models? What aspects can be made accessible via external commonsense resources? We study these questions in the context of answering questions in the TELLMEWHY dataset using COMET as a source of relevant commonsense relations. We analyze the effects of model size (T5 variants and GPT-3) along with methods of injecting knowledge (COMET) into these models. Results show that the largest models, as expected, yield substantial improvements over base models and injecting external knowledge helps models of all sizes. We also find that the format in which knowledge is provided is critical, and that smaller models benefit more from larger amounts of knowledge. Finally, we develop an ontology of knowledge types and analyze the relative coverage of the models across these categories. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Base Models; Commonsense Knowledge; External Knowledge; Help Model; Knowledge Analysis; Language Model; Large Amounts; Large Models; Model Size; Ontology's},
	keywords = {Base models; Commonsense knowledge; External knowledge; HELP model; Knowledge analysis; Language model; Large amounts; Large models; Model size; Ontology's},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Mahapatra2022942,
	author = {Mahapatra, Aniruddha and Nangi, Sharmila Reddy and Garimella, Aparna and Natarajan, Anandhavelu},
	title = {Entity Extraction in Low Resource Domains with Selective Pre-training of Large Language Models},
	year = {2022},
	pages = {942 - 951},
	doi = {10.18653/v1/2022.emnlp-main.61},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149442616&doi=10.18653%2Fv1%2F2022.emnlp-main.61&partnerID=40&md5=5798bd308b5efe55edbd2b289e444b48},
	abstract = {Transformer-based language models trained on large natural language corpora have been very useful in downstream entity extraction tasks. However, they often result in poor performances when applied to domains that are different from those they are pretrained on. Continued pretraining using unlabeled data from target domains can help improve the performances of these language models on the downstream tasks. However, using all of the available unlabeled data for pretraining can be time-intensive; also, it can be detrimental to the performance of the downstream tasks, if the unlabeled data is not aligned with the data distribution for the target tasks. Previous works employed external supervision in the form of ontologies for selecting appropriate data samples for pretraining, but external supervision can be quite hard to obtain in low-resource domains. In this paper, we introduce effective ways to select data from unlabeled corpora of target domains for language model pretraining to improve the performances in target entity extraction tasks. Our data selection strategies do not require any external supervision. We conduct extensive experiments for the task of named entity recognition (NER) on seven different domains and show that language models pretrained on target domain unlabeled data obtained using our data selection strategies achieve better performances compared to those using data selection strategies in previous works that use external supervision. We also show that these pretrained language models using our data selection strategies outperform those pretrained on all of the available unlabeled target domain data. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Data Reduction; Data Selection Strategies; Down-stream; Entity Extractions; Language Model; Natural Languages; Performance; Pre-training; Resource Domains; Target Domain; Unlabeled Data; Extraction},
	keywords = {Computational linguistics; Data reduction; Data selection strategies; Down-stream; Entity extractions; Language model; Natural languages; Performance; Pre-training; Resource domains; Target domain; Unlabeled data; Extraction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Reid2022964,
	author = {Reid, Machel and Zhong, Victor and Gururangan, Suchin S. and Zettlemoyer, Luke S.},
	title = {M2D2: A Massively Multi-Domain Language Modeling Dataset},
	year = {2022},
	pages = {964 - 975},
	doi = {10.18653/v1/2022.emnlp-main.63},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148744117&doi=10.18653%2Fv1%2F2022.emnlp-main.63&partnerID=40&md5=a50a4597e3c3e81b2ca71e3a410c7a3b},
	abstract = {We present M2D2, a fine-grained, massively multi-domain corpus for studying domain adaptation in language models (LMs). M2D2 consists of 8.5B tokens and spans 145 domains extracted from Wikipedia and Semantic Scholar. Using ontologies derived from Wikipedia and ArXiv categories, we organize the domains in each data source into 22 groups. This two-level hierarchy enables the study of relationships between domains and their effects on in- and out-of-domain performance after adaptation. We also present a number of insights into the nature of effective domain adaptation in LMs, as examples of the new types of studies M2D2 enables. To improve in-domain performance, we show the benefits of adapting the LM along a domain hierarchy; adapting to smaller amounts of fine-grained domain-specific data can lead to larger in-domain performance gains than larger amounts of weakly relevant data. We further demonstrate a tradeoff between in-domain specialization and out-of-domain generalization within and across ontologies, as well as a strong correlation between out-of-domain performance and lexical overlap between domains. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Modeling Languages; Semantics; Data-source; Domain Adaptation; Domain Language; Effective Domains; Fine Grained; Language Model; Multi-domains; Ontology's; Performance; Wikipedia; Ontology},
	keywords = {Computational linguistics; Modeling languages; Semantics; Data-source; Domain adaptation; Domain language; Effective domains; Fine grained; Language model; Multi-domains; Ontology's; Performance; Wikipedia; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{2022,
	title = {DL4KG 2022 - Proceedings of the Workshop on Deep Learning for Knowledge Graphs, co-located with the 21st International Semantic Web Conference, ISWC 2022},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3342},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148639377&partnerID=40&md5=aaf2f60ce082c5b909bcf1bee0114eac},
	abstract = {The proceedings contain 10 papers. The topics discussed include: towards a question answering system over temporal knowledge graph embedding; transformer-based subject entity detection in Wikipedia listings; improving language model predictions via prompts enriched with knowledge graphs; knowledge graph embeddings for link prediction: beware of semantics!; neuro-symbolic learning for dealing with sparsity in cultural heritage image archives: an empirical journey; bilingual question answering over DBpedia abstracts through machine translation and BERT; a closer look at sum-based embeddings for knowledge graphs containing procedural knowledge; knowledge graph embeddings for causal relation prediction; multi-label classification using BERT and knowledge graphs with a limited training dataset; and CosmOntology: creating an ontology of the cosmos. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Wullschleger20221950,
	author = {Wullschleger, Pascal and Lionetti, Simone and Daly, Donnacha and Volpe, Francesca and Caro, Grégoire},
	title = {Auto-Regressive Self-Attention Models for Diagnosis Prediction on Electronic Health Records},
	year = {2022},
	pages = {1950 - 1956},
	doi = {10.1109/BigData55660.2022.10020339},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147928928&doi=10.1109%2FBigData55660.2022.10020339&partnerID=40&md5=cbff75806de935dcc3174e3308a2f471},
	abstract = {Insufficient data and data lacking the diversity to represent the general public is a common challenge when modelling diagnosis prediction. We consider a much larger and more diverse database of commercial Electronic Health Records than what is prevalent in the literature. We formulate a simplified version of diagnosis prediction that focuses on major developments in medical histories of patients. To this end, we leverage Auto-Regressive Self-Attention models that have seen promising applications in language modelling and extend them to incorporate ontological representations of medical codes. Additionally, we include time-intervals between diagnoses into the attention calculation. We evaluate models and baselines at different levels of diagnostic granularity and our results suggest that using very detailed clinical classifications does not significantly degrade performance, possibly allowing their use in practice. Our model outperforms all baselines and we suggest that leveraging the ontology for generating diagnosis representations is mostly helpful for rare diagnoses. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Diagnosis Prediction; Electronic Health Record; Ontological Representation; Transformer; Diagnosis; Ehealth; Medical Computing; Modeling Languages; Ontology; Records Management; Attention Model; Auto-regressive; Diagnose Prediction; Electronic Health; Electronic Health Record; General Publics; Health Records; Ontological Representation; Public Is; Transformer; Forecasting},
	keywords = {Diagnosis; eHealth; Medical computing; Modeling languages; Ontology; Records management; Attention model; Auto-regressive; Diagnose prediction; Electronic health; Electronic health record; General publics; Health records; Ontological representation; Public IS; Transformer; Forecasting},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Deepak202264,
	author = {Deepak, Gerard and Ayush Kumar, A. and Sheeba Priyadarshini, J. and Singh, Divyanshu Pratap},
	title = {Descriptive Answer Evaluation using NLP Processes Integrated with Strategically Constructed Semantic Skill Ontologies},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3335},
	pages = {64 - 76},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147696836&partnerID=40&md5=44a8dbf4219e99a4043c52da94fd57d5},
	abstract = {The world is moving towards an online methodology of education. One of the key challenges is the assessment of questions which do not have a definite answer and have several correct answers. To solve this problem, and for quality evaluation of descriptive answers online, an automatic evaluation methodology is proposed in this work. A language model is modelled from the expected answer key, and entity graphs are generated from the ontology modelled using the input answer to be evaluated. Natural Language Processing (NLP) techniques like Stemming, Summarization, and Polarity Analysis are integrated in this work with Ontologies for the efficient evaluation of descriptive answers. Several challenges which come across evaluating descriptive answers are discussed in this chapter, and they have been solved in order to obtain a dynamic and robust evaluating system. Finally, the system is evaluated using a user-feedback methodology comprising a panel of 100 students and 100 professors. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {E-learning; Keyword Extraction; Natural Language Processing; Online Evaluation; Ontology; E-learning; Natural Language Processing Systems; Quality Control; Semantics; Automatic Evaluation; Creative Commons; E - Learning; Keywords Extraction; Language Processing; Natural Language Processing; Natural Languages; On-line Evaluation; Ontology's; Quality Evaluation; Ontology},
	keywords = {E-learning; Natural language processing systems; Quality control; Semantics; Automatic evaluation; Creative Commons; E - learning; Keywords extraction; Language processing; Natural language processing; Natural languages; On-line evaluation; Ontology's; Quality evaluation; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Zhao202290,
	author = {Zhao, Yingwen and Yang, Zhihao and Hong, Yongkai and Wang, Lei and Zhang, Yin and Lin, Hongfei and Wang, Jian},
	title = {Adaptive Multi-view Graph Convolutional Network for Gene Ontology Annotations of Proteins},
	year = {2022},
	pages = {90 - 93},
	doi = {10.1109/BIBM55620.2022.9995517},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146713546&doi=10.1109%2FBIBM55620.2022.9995517&partnerID=40&md5=ac2039ebebc24d2aaae96a61c5280a42},
	abstract = {Gene Ontology (GO) containing a set of standard concepts (or terms) is launched to unify the functional descriptions of proteins. Developing computational models based on GO to automatically annotate protein functions has been a longstanding active research area. In this paper, we propose a novel method to adaptively fuse functional and topological information between GO Terms. Our method is composed of a pre-trained language model for encoding protein sequences and an adaptive multi-view graph convolutional network (Multi-view GCN) for representing GO terms. Particularly, the Multi-view GCN considers multiple views from functional information, topological structures, and their combinations, and extracts multiple corresponding representations of GO terms. Then, an attention mechanism is applied to adaptively learn the importance weights of these representations. Finally, the predicted scores are calculated by using a dot product between protein sequence features and GO term representations. Experimental results on the datasets of two species (i.e., Human and Yeast) show that our method outperforms other state-of-the-art methods. The code of our proposed method is available at: https://github.com/Candyperfect/Master. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Adaptive Multi-view Graph Convolutional Network; Deep Learning; Gene Ontology Terms; Protein Function Prediction; Bioinformatics; Convolution; Convolutional Neural Networks; Deep Learning; Genes; Proteins; Topology; Adaptive Multi-view Graph Convolutional Network; Convolutional Networks; Functional Information; Gene Ontology; Gene Ontology Annotations; Gene Ontology Terms; Multi-views; Protein Function Prediction; Protein Sequences; Gene Ontology},
	keywords = {Bioinformatics; Convolution; Convolutional neural networks; Deep learning; Genes; Proteins; Topology; Adaptive multi-view graph convolutional network; Convolutional networks; Functional information; Gene ontology; Gene ontology annotations; Gene ontology terms; Multi-views; Protein function prediction; Protein sequences; Gene Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Choi202251,
	author = {Choi, Kyudam and Lee, Yurim and Kim, Wonil},
	title = {GCL-GO: A novel sequence-based hierarchy-aware method for protein function prediction},
	year = {2022},
	pages = {51 - 56},
	doi = {10.1109/BIBM55620.2022.9994917},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146642218&doi=10.1109%2FBIBM55620.2022.9994917&partnerID=40&md5=3c3103760b6bccffd29c52ac32885d60},
	abstract = {Experimental protein functional annotation does not cover rapidly-expanding protein sequences. Sequence-based methods, one of the computational methods, have been developed for extending functional annotations to fast-growing sequence databases. We propose a novel sequence-based hierarchy-aware method, namely GCL-GO. GCL-GO applies a protein language model to represent sequences, applies graph contrastive learning to represent GO terms, and then predicts protein functions by combining these two features. By contrasting the GO graph and semantic features of GO terms, GCL-GO has generalizability and scalability by accurately embedding the features of GO terms while relying less on training data. We also suggest GCL-GO+, which combines a sequence similarity-based method with GCLGO, to improve performance. GCL-GO+ outperforms sequence-based competing methods on both the CAFA3 and the TALE datasets. Furthermore, GCL-GO and GCL-GO+ demonstrate functional generalization and scalability potential by having the best performance on new GO terms or on GO terms annotated infrequently in the training dataset. Our code is available in https://github.com/kch38896/GCL-GO © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Gene Ontology; Graph Constructive Learning; Protein Function Prediction; Protein Language Model; Bioinformatics; Computational Linguistics; Gene Ontology; Learning Systems; Scalability; Semantics; Constructive Learning; Functional Annotation; Gene Ontology; Go Terms; Graph Constructive Learning; Language Model; Protein Function Prediction; Protein Language Model; Protein Sequences; Sequence Database; Proteins},
	keywords = {Bioinformatics; Computational linguistics; Gene Ontology; Learning systems; Scalability; Semantics; Constructive learning; Functional annotation; Gene ontology; GO terms; Graph constructive learning; Language model; Protein function prediction; Protein language model; Protein sequences; Sequence database; Proteins},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Sai Sharath202229,
	author = {Sai Sharath, Japa and Green, Sarah},
	title = {Question Answering over Knowledge Base with Variational Auto-Encoder},
	year = {2022},
	pages = {29 - 36},
	doi = {10.1109/BigMM55396.2022.00012},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146491190&doi=10.1109%2FBigMM55396.2022.00012&partnerID=40&md5=9bd9f667a43f8136cddbc4fcac4b4146},
	abstract = {Knowledge Base is a semantic data repos-itory made available over the web. Knowledge bases are multi-relational graphs consisting of entities and relations representing facts about the world. These facts follow a controlled vocabulary that drives the knowledge base, often called ontology. Entities are the graph nodes, while relations are the edges connecting the nodes. While knowledge base attention prolifer-ates, extracting information from these resources is challenging. One of the promising approaches is Question Answering systems. The Question Answering over Knowledge Base(KB-QA) system aims to provide a natural language interface to pose a query to the KB. QA systems use embeddings to map the posed Question to the facts stored in the knowledge bases. QA systems can be effectively improved using large transformer models. These transformer models are computationally expensive, which prevents their use in real-world applications. To solve this, we started our study with the use-case of Knowledge Base Question Answering systems. In KB-QA systems, representing the entities and relationships play a pivotal role in extracting the answer to the posed query. Traditional KB embedding techniques represent entities/relations as vectors, mapping them in different semantic spaces and ignoring the subtle cor-relation. A pre-training language model for encoding the KB facts will preserve the semantic and contextual correlation. While learning linguistic knowledge, these models also store relational knowledge in the training data. These Language Models are often trained on a massive corpus addressing the Out-Of- Vocabulary problem. In this paper, we incorporate a co-embedding model for knowledge base embedding, which learns low-dimensional representations of entities and relations in the same semantic space. We propose a Variational Auto-Encoder, an efficient transformer architecture that represents knowledge base representations as Gaussian distributions to address the issue of neglecting uncertainty. In addition, our method has high quality and interpretability advantages compared with previous methods. Our experimental results show the effectiveness and the superiority of the VAE approach for question-answering systems on knowledge bases over other well-known pre-trained embedding methods. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Encoder; Kbqa; Knowledge Base Question Answering; Language Model; Multi-head Attention; Transformers; Vae; Computational Linguistics; Data Mining; Digital Storage; Knowledge Based Systems; Natural Language Processing Systems; Query Processing; Semantics; Signal Encoding; Vector Spaces; Bert; Encoder; Kbqa; Knowledge Base Question Answering; Language Model; Multi-head Attention; Qa System; Question Answering; Transformer; Vae; Embeddings},
	keywords = {Computational linguistics; Data mining; Digital storage; Knowledge based systems; Natural language processing systems; Query processing; Semantics; Signal encoding; Vector spaces; Bert; Encoder; KBQA; Knowledge base question answering; Language model; Multi-head attention; QA system; Question Answering; Transformer; VAE; Embeddings},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Vela-Tambo2022158,
	author = {Vela-Tambo, Javier and Gracia, Jorge Delgado},
	title = {Cross-lingual ontology matching with CIDER-LM: Results for OAEI 2022},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3324},
	pages = {158 - 165},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146435434&partnerID=40&md5=e449ddfa3492be2a82ecb051e87721c1},
	abstract = {In this paper, the CIDER-LM cross-lingual matching system is presented, as well as the results it achieved during the OAEI (Ontology Alignment Evaluation Initiative) 2022 campaign. This is the first appearance of CIDER-LM in OAEI where it only participated in MultiFarm, the track for cross-lingual ontology alignment evaluation. The matching system uses a pre-trained multilingual language model based on transformers, fine-tuned using the openly available portion of the MultiFarm dataset. The model calculates the vector embeddings of the labels associated to every ontology entity and its context. The confidence degree between matching entities is computed as the cosine similarity between their associated embeddings. CIDER-LM is novel in the use of multilingual language models for cross-lingual ontology matching. Its initial version obtained promising results in the OAEI'22 MultiFarm track, attaining a modest precision but the best overall performance in recall. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Cider-lm; Cross-lingual Ontology Matching; Language Models; Natural Language Processing; Ontology Alignment; Sentence-bert; Transformers; Computational Linguistics; Natural Language Processing Systems; Ontology; Petroleum Reservoir Evaluation; Cider-lm; Cross-lingual; Cross-lingual Ontology Matching; Language Model; Language Processing; Natural Language Processing; Natural Languages; Ontology Alignment; Ontology Matching; Sentence-bert; Transformer; Embeddings},
	keywords = {Computational linguistics; Natural language processing systems; Ontology; Petroleum reservoir evaluation; CIDER-LM; Cross-lingual; Cross-lingual ontology matching; Language model; Language processing; Natural language processing; Natural languages; Ontology alignment; Ontology matching; Sentence-BERT; Transformer; Embeddings},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Gosselin2022202,
	author = {Gosselin, Francis and Zouaq, Amal},
	title = {SEBMatcher Results for OAEI 2022},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3324},
	pages = {202 - 209},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146435366&partnerID=40&md5=304b1763207ce915a1271adf09143f62},
	abstract = {This paper presents the results of the Structural Embeddings with BERT Matcher (SEBMatcher) in the OAEI 2022 competition. SEBMatcher is a novel schema matching system that employs a 2 step approach: An unsupervised pretraining of a Masked Language Modeling BERT fed with random walks, followed by a supervised training of a BERT for sequence classification fed with positive and negative mappings. This is the first year of participation in the OAEI for SEBMatcher and it has obtained promising results in participating tracks. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Iswc-2022; Ontology Alignment; Representation Learning; Schema Matching; Embeddings; Iswc-2022; Language Model; Matching System; Ontology Alignment; Pre-training; Random Walk; Representation Learning; Schema Matching; Supervised Trainings; Modeling Languages},
	keywords = {Embeddings; ISWC-2022; Language model; Matching system; Ontology alignment; Pre-training; Random Walk; Representation learning; Schema matching; Supervised trainings; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@CONFERENCE{Kardos2022216,
	author = {Kardos, Péter and Szántó, Zsolt and Farkas, Richárd},
	title = {WomboCombo Results for OAEI 2022},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3324},
	pages = {216 - 219},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146433159&partnerID=40&md5=bbbd84da2da1bec2e3e25835a887d9a9},
	abstract = {This paper presents the results of the WomboCombo Matcher in the Ontology Alignment Evaluation Initiative (OAEI) 2022. WomboCombo is an ontology matching tool that finds node pairs starting out from simpler exact string matching based steps through more complex neural Language Model based steps. We also train a classifier to differentiate between entities with the same and entities with similar meaning. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Language Models; Ontology Alignment; Ontology Mathching; Computational Linguistics; Language Model; Matching Tools; Model-based Opc; Node Pairs; Ontology Alignment; Ontology Matching; Ontology Mathching; Ontology's; Simple++; String Matching; Ontology},
	keywords = {Computational linguistics; Language model; Matching tools; Model-based OPC; Node pairs; Ontology alignment; Ontology matching; Ontology mathching; Ontology's; Simple++; String matching; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Wang2022145,
	author = {Wang, Zhu},
	title = {AMD Results for OAEI 2022},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3324},
	pages = {145 - 152},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146423755&partnerID=40&md5=5d40e36481c36d7e9d14b4536f18bebe},
	abstract = {AgreementMakerDeep (AMD) is a new flexible and extensible ontology matching system. It exploits the contextual and structural information of ontologies by infusing knowledge to pre-trained masked language model, and then filter the output mappings using knowledge graph embedding techniques. AMD learns from classes and their relations between classes by constructing vector representations into the low dimensional embedding space with knowledge graph embedding methods. The results demonstrate that AMD achieves a competitive performance in many OAEI tracks, but AMD has limitations for property and instance matching. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph Embedding; Ontology Matching; Pre-train Language Model; Computational Linguistics; Graph Embeddings; Ontology; Vector Spaces; Contextual Information; Knowledge Graph Embedding; Knowledge Graphs; Language Model; Matching System; Ontology Matching; Ontology's; Pre-train Language Model; Structural Information; Knowledge Graph},
	keywords = {Computational linguistics; Graph embeddings; Ontology; Vector spaces; Contextual information; Knowledge graph embedding; Knowledge graphs; Language model; Matching system; Ontology matching; Ontology's; Pre-train language model; Structural information; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{Vasantharajan20221482,
	author = {Vasantharajan, Charangan and Tun, Kyaw Zin and Ho, Thinga and Jain, Sparsh and Tong, Rong and Siong, Chng Eng},
	title = {MedBERT: A Pre-trained Language Model for Biomedical Named Entity Recognition},
	year = {2022},
	pages = {1482 - 1488},
	doi = {10.23919/APSIPAASC55919.2022.9980157},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146303653&doi=10.23919%2FAPSIPAASC55919.2022.9980157&partnerID=40&md5=f6cceacbfce834cc156367a72b5018ce},
	abstract = {This paper introduces MedBERT, a new pre-trained transformer-based model for biomedical named entity recognition. MedBERT is trained with 57.46M tokens collected from biomedical-related data sources, i.e. datasets acquired from N2C2, BioNLP, CRAFT challenges, and biomedical-related articles crawled from Wikipedia. We validate the effectiveness of MedBERT by comparing it with four publicly available pre-trained models on ten biomedical datasets from BioNLP and CRAFT shared tasks. Our experimental results show that models fine-tuned on MedBERT achieve state-of-the-art performance in nine datasets that predict Protein, Gene, Chemical, Cellular/Component, Gene Ontology, and Taxonomy entities. Specifically, the model achieved an average of 84.04% F1-micro score on ten test sets from BioNLP and CRAFT challenges with an improvement of 3.7% and 7.83% as compared to models that were fine-tuned on BioBERT and Bio_ClinicalBERT, respectively. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Genes; Biomedical Named Entity Recognition; Cellular Components; Data-source; Gene Ontology; Language Model; Protein Genes; State-of-the-art Performance; Test Sets; Wikipedia; Gene Ontology},
	keywords = {Genes; Biomedical named entity recognition; Cellular components; Data-source; Gene ontology; Language model; Protein genes; State-of-the-art performance; Test sets; Wikipedia; Gene Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Wang2022117,
	author = {Wang, Yetian and Kühn, Ramona and Allen Harris, Randy Allen and Mitrović, Jelena and Granitzer, Michael},
	title = {Towards a Unified Multilingual Ontology for Rhetorical Figures},
	year = {2022},
	journal = {International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K - Proceedings},
	volume = {2},
	pages = {117 - 127},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146198439&partnerID=40&md5=d4a279f8942e8f7de2fb77de944e797e},
	abstract = {Formal ontologies for rhetorical figures have been developed to improve the computational detection for different applications in the area of Natural Language Processing, such as hate speech and fake news detection, argumentation mining, and sentiment analysis. The existing ontologies all model different aspects of rhetorical figures, thus creating a variety of formalisms and in the worst case, creating incompatibilities and contradictory representations. In this paper, we focus on figures of perfect lexical repetition and their representation in three ontologies in three different languages: The Ploke ontology, the Serbian RetFig, and the German GRhOOT ontology. We combine those ontologies to benefit from synergy effects and create a multilingual, coherent, robust, and modular ontology for rhetorical figures of perfect lexical repetition. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Rhetoric; Knowledge Representation; Language Modelling; Ontology; Rhetorical Figures; Modeling Languages; Ontology; Sentiment Analysis; Computational Detection; Computational Rhetoric; Formal Ontology; Knowledge-representation; Language Model; Language Processing; Multilingual Ontologies; Natural Languages; Ontology's; Rhetorical Figure; Knowledge Representation},
	keywords = {Modeling languages; Ontology; Sentiment analysis; Computational detection; Computational rhetoric; Formal ontology; Knowledge-representation; Language model; Language processing; Multilingual ontologies; Natural languages; Ontology's; Rhetorical figure; Knowledge representation},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{2022,
	title = {Proceedings of the Language Resources and Evaluation Conference, LREC 2022 Workshop on 4th Financial Narrative Processing Workshop, FNP 2022},
	year = {2022},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145890269&partnerID=40&md5=7f6b6d4b18f1eea389afada276db881a},
	abstract = {The proceedings contain 24 papers. The topics discussed include: FinRAD: financial readability assessment dataset - 13,000+ definitions of financial terms for measuring readability; discovering financial hypernyms by prompting masked language models; sentiment classification by incorporating background knowledge from financial ontologies; detecting causes of stock price rise and decline by machine reading comprehension with BERT; multilingual text summarization on financial documents; extractive and abstractive summarization methods for financial narrative summarization in English, Spanish and Greek; DiMSum: Distributed and multilingual summarization of financial narratives; transformer-based models for long document summarization in financial domain; and DCU-Lorcan at FinCausal 2022: span-based causality extraction from financial documents using pre-trained language models. © 2023 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Maudslay202290,
	author = {Maudslay, Rowan Hall and Teufel, Simone},
	title = {Homonymy Information for English WordNet},
	year = {2022},
	pages = {90 - 98},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145881520&partnerID=40&md5=7d6804fd4c3843c66e32af494ae8f6f5},
	abstract = {A widely acknowledged shortcoming of WordNet is that it lacks a distinction between word meanings which are systematically related (polysemy), and those which are coincidental (homonymy). Several previous works have attempted to fill this gap, by inferring this information using computational methods. We revisit this task, and exploit recent advances in language modelling to synthesise homonymy annotation for Princeton WordNet. Previous approaches treat the problem using clustering methods; by contrast, our method works by linking WordNet to the Oxford English Dictionary, which contains the information we need. To perform this alignment, we pair definitions based on their proximity in an embedding space produced by a Transformer model. Despite the simplicity of this approach, our best model attains an F1 of .97 on an evaluation set that we annotate. The outcome of our work is a high-quality homonymy annotation layer for Princeton WordNet, which we release. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Homonymy; Oxford English Dictionary; Polysemy; Wordnet; Ontology; Best Model; Clustering Methods; Embeddings; Homonymy; Language Model; Oxford English Dictionary; Polysemy; Transformer Modeling; Word Meaning; Wordnet; Modeling Languages},
	keywords = {Ontology; Best model; Clustering methods; Embeddings; Homonymy; Language model; Oxford english dictionary; Polysemy; Transformer modeling; Word meaning; Wordnet; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Hudeček20221286,
	author = {Hudeček, Vojtěch and Schaub, Léon Paul and Štancl, Daniel and Paroubek, Patrick and Dušek, Ondřej Ř.},
	title = {DIASER: A Unifying View On Task-oriented Dialogue Annotation},
	year = {2022},
	pages = {1286 - 1296},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144429858&partnerID=40&md5=30e1105684ac700b0a26577bf4d5b1fe},
	abstract = {Every model is only as strong as the data that it is trained on. In this paper, we present a new dataset, obtained by merging four publicly available annotated corpora for task-oriented dialogues in several domains (MultiWOZ 2.2, CamRest676, DSTC2 and Schema-Guided Dialogue Dataset). This way, we assess the feasibility of providing a unified ontology and annotation schema covering several domains with a relatively limited effort. We analyze the characteristics of the resulting dataset along three main dimensions: language, information content and performance. We focus on aspects likely to be pertinent for improving dialogue success, e.g. dialogue consistency. Furthermore, to assess the usability of this new corpus, we thoroughly evaluate dialogue generation performance under various conditions with the help of two prominent recent end-to-end dialogue models: MarCo and GPT-2. These models were selected as popular open implementations representative of the two main dimensions of dialogue modelling. While we did not observe a significant gain for dialogue state tracking performance, we show that using more training data from different sources can improve language modelling capabilities and positively impact dialogue flow (consistency). In addition, we provide the community with one of the largest open dataset for machine learning experiments. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Annotated Corpora; Resource Merging; Task Oriented Dialog; Modeling Languages; Annotated Corpus; Dialogue Generations; Dialogue Models; Information Contents; Language Informations; Ontology's; Performance; Resource Merging; Task Oriented Dialog; Task-oriented; Merging},
	keywords = {Modeling languages; Annotated corpus; Dialogue generations; Dialogue models; Information contents; Language informations; Ontology's; Performance; Resource merging; Task oriented dialog; Task-oriented; Merging},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Chizhikova20224040,
	author = {Chizhikova, Anastasia and Murzakhmetov, Sanzhar B. and Serikov, Oleg A. and Shavrina, T. O. and Burtsev, Mikhail S.},
	title = {Attention Understands Semantic Relations},
	year = {2022},
	pages = {4040 - 4050},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144341878&partnerID=40&md5=8e9ab0e2b9708c47b3448aae0cfa3fdb},
	abstract = {Today, natural language processing heavily relies on pre-trained large language models. Even though such models are criticised for poor interpretability, they still yield state-of-the-art solutions for a wide range of very different tasks. While many probing studies have been conducted to measure the models awareness of grammatical knowledge, semantic probing is less popular. In this work, we introduce a probing pipeline to study how semantic relations are represented in transformer language models. We show that in this task, attention scores express the information about relations similar to the layers' output activations despite their lesser ability to represent surface cues. This supports the hypothesis that attention mechanisms focus not only on syntactic relational information but semantic as well. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Bertology; Explainable Ai (xai); Knowledge Probing; Language Models Interpretation; Ontology Extraction; Semantic Probing; Artificial Intelligence; Computational Linguistics; Natural Language Processing Systems; Bertology; Explainable Ai (xai); Knowledge Probing; Language Model; Language Model Interpretation; Model Interpretations; Natural Languages; Ontology Extraction; Semantic Probing; Semantic Relations; Semantics},
	keywords = {Artificial intelligence; Computational linguistics; Natural language processing systems; Bertology; Explainable AI (XAI); Knowledge probing; Language model; Language model interpretation; Model interpretations; Natural languages; Ontology Extraction; Semantic probing; Semantic relations; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Winter2022363,
	author = {Winter, Benjamin and Figueroa, Alexei and Löser, Alexander and Gers, Felix Alexander and Siu, Amy},
	title = {KIMERA: Injecting Domain Knowledge into Vacant Transformer Heads},
	year = {2022},
	pages = {363 - 373},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144338453&partnerID=40&md5=d03ede477eb4fc2d2bcd290832cd6795},
	abstract = {Training transformer language models requires vast amounts of text and computational resources. This drastically limits the usage of these models in niche domains for which they are not optimized, or where domain-specific training data is scarce. We focus here on the clinical domain because of its limited access to training data in common tasks, while structured ontological data is often readily available. Recent observations in model compression of transformer models show optimization potential in improving the representation capacity of attention heads. We propose KIMERA (Knowledge Injection via Mask Enforced Retraining of Attention) for detecting, retraining and instilling attention heads with complementary structured domain knowledge. Our novel multi-task training scheme effectively identifies and targets individual attention heads that are least useful for a given downstream task and optimizes their representation with information from structured data. KIMERA generalizes well, thereby building the basis for an efficient fine-tuning. KIMERA achieves significant performance boosts on seven datasets in the medical domain in Information Retrieval and Clinical Outcome Prediction settings. We apply KIMERA to BERT-base to evaluate the extent of the domain transfer and also improve on the already strong results of BioBERT in the clinical domain. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Language Modelling; Neural Language Representation Models; Semi-supervised; Statistical And Machine Learning Methods; Weakly-supervised And Unsupervised Learning; Computational Linguistics; Domain Knowledge; Supervised Learning; Domain Knowledge; Language Model; Machine Learning Methods; Neural Language Representation Model; Representation Model; Semi-supervised; Statistical Learning Methods; Supervised And Unsupervised Learning; Training Data; Weakly-supervised And Unsupervised Learning; Modeling Languages},
	keywords = {Computational linguistics; Domain Knowledge; Supervised learning; Domain knowledge; Language model; Machine learning methods; Neural language representation model; Representation model; Semi-supervised; Statistical learning methods; Supervised and unsupervised learning; Training data; Weakly-supervised and unsupervised learning; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@CONFERENCE{Lin2022270,
	author = {Lin, Hsienchin and Geishauser, Christian and Feng, Shutong and Lubis, Nurul and van Niekerk, Carel and Heck, Michael and Gašić, Milica},
	title = {GenTUS: Simulating User Behaviour and Language in Task-oriented Dialogues with Generative Transformers},
	year = {2022},
	pages = {270 - 282},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144185855&partnerID=40&md5=25549a534957562f77ef50d72c5bb905},
	abstract = {User simulators (USs) are commonly used to train task-oriented dialogue systems (DSs) via reinforcement learning. The interactions often take place on semantic level for efficiency, but there is still a gap from semantic actions to natural language, which causes a mismatch between training and deployment environment. Incorporating a natural language generation (NLG) module with USs during training can partly deal with this problem. However, since the policy and NLG of USs are optimised separately, these simulated user utterances may not be natural enough in a given context. In this work, we propose a generative transformer-based user simulator (GenTUS). GenTUS consists of an encoder-decoder structure, which means it can optimise both the user policy and natural language generation jointly. GenTUS generates both semantic actions and natural language utterances, preserving interpretability and enhancing language variation. In addition, by representing the inputs and outputs as word sequences and by using a large pre-trained language model we can achieve generalisability in feature representation. We evaluate GenTUS with automatic metrics and human evaluation. Our results show that GenTUS generates more natural language and is able to transfer to an unseen ontology in a zero-shot fashion. In addition, its behaviour can be further shaped with reinforcement learning opening the door to training specialised user simulators. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Behavioral Research; Computational Linguistics; Learning Systems; Natural Language Processing Systems; Semantics; Speech Processing; Zero-shot Learning; Dialogue Systems; Encoder-decoder; Natural Language Generation; Natural Languages; Policy Language; Reinforcement Learnings; Semantic Action; Semantic Levels; Task-oriented; User Behaviors; Reinforcement Learning},
	keywords = {Behavioral research; Computational linguistics; Learning systems; Natural language processing systems; Semantics; Speech processing; Zero-shot learning; Dialogue systems; Encoder-decoder; Natural language generation; Natural languages; Policy language; Reinforcement learnings; Semantic action; Semantic levels; Task-oriented; User behaviors; Reinforcement learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@CONFERENCE{Zhang2022,
	author = {Zhang, Ningyu and Bi, Zhen and Liang, Xiaozhuan and Cheng, Siyuan and Hong, Haosen and Deng, Shumin and Zhang, Qiang and Lian, Jiazhang and Chen, Huajun},
	title = {ONTOPROTEIN: PROTEIN PRETRAINING WITH GENE ONTOLOGY EMBEDDING},
	year = {2022},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143658405&partnerID=40&md5=54737ec3da09c6780ecdbec1ab64c76e},
	abstract = {Self-supervised protein language models have proved their effectiveness in learning the proteins representations. With the increasing computational power, current protein language models pre-trained with millions of diverse sequences can advance the parameter scale from million-level to billion-level and achieve remarkable improvement. However, those prevailing approaches rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better protein representations. We argue that informative biology knowledge in KGs can enhance protein representation with external knowledge. In this work, we propose OntoProtein, the first general framework that makes use of structure in GO (Gene Ontology) into protein pre-training models. We construct a novel large-scale knowledge graph that consists of GO and its related proteins, and gene annotation texts or protein sequences describe all nodes in the graph. We propose novel contrastive learning with knowledge-aware negative sampling to jointly optimize the knowledge graph and protein embedding during pre-training. Experimental results show that OntoProtein can surpass state-of-the-art methods with pre-trained protein language models in TAPE benchmark and yield better performance compared with baselines in protein-protein interaction and protein function prediction. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Benchmarking; Computational Linguistics; Gene Ontology; Genes; Graph Embeddings; Knowledge Graph; Computational Power; Embeddings; External Knowledge; Gene Ontology; Knowledge Graphs; Language Model; Power Current; Pre-training; Structured Knowledge; Training Model; Proteins},
	keywords = {Benchmarking; Computational linguistics; Gene Ontology; Genes; Graph embeddings; Knowledge graph; Computational power; Embeddings; External knowledge; Gene ontology; Knowledge graphs; Language model; Power current; Pre-training; Structured knowledge; Training model; Proteins},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 28}
}

@ARTICLE{Martín-Moncunill2022234,
	author = {Martín-Moncunill, David and Sicília, Miguel Angel and Gonzalez-Garcia, Lino and Rodríguez, Diego Rubén},
	title = {On Contrasting YAGO with GPT-J: An Experiment for Person-Related Attributes},
	year = {2022},
	journal = {Communications in Computer and Information Science},
	volume = {1686 CCIS},
	pages = {234 - 245},
	doi = {10.1007/978-3-031-21422-6_17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142770683&doi=10.1007%2F978-3-031-21422-6_17&partnerID=40&md5=c7f24f63f68d03ee14f39670d13ed585},
	abstract = {Language models (LMs) trained or large text corpora have demonstrated their superior performance in different language related tasks in the last years. These models automatically implicitly incorporate factual knowledge that can be used to complement existing Knowledge Graphs (KGs) that in most cases are structured from human curated databases. Here we report an experiment that attempts to gain insights about the extent to which LMs can generate factual information as that present in KGs. Concretely, we have tested such process using the English Wikipedia subset of YAGO and the GPT-J model for attributes related to individuals. Results show that the generation of correct factual information depends on the generation parameters of the model and are unevenly balanced across diverse individuals. Further, the LM can be used to populate further factual information, but it requires intermediate parsing to correctly map to KG attributes. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Factual Information; Gpt-j; Language Models; Ontology; Yago; Computational Linguistics; Factual Information; Factual Knowledge; Gain Insight; Gpt-j; Knowledge Graphs; Language Model; Ontology's; Performance; Text Corpora; Yago; Knowledge Graph},
	keywords = {Computational linguistics; Factual information; Factual knowledge; Gain insight; GPT-J; Knowledge graphs; Language model; Ontology's; Performance; Text corpora; YAGO; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Ilievski2022178,
	author = {Ilievski, Filip and Pujara, Jay and Shenoy, Kartik},
	title = {Does Wikidata Support Analogical Reasoning?},
	year = {2022},
	journal = {Communications in Computer and Information Science},
	volume = {1686 CCIS},
	pages = {178 - 191},
	doi = {10.1007/978-3-031-21422-6_13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142732519&doi=10.1007%2F978-3-031-21422-6_13&partnerID=40&md5=6298ded8121b36e10706918128427fc9},
	abstract = {Analogical reasoning methods have been built over various resources, including commonsense knowledge bases, lexical resources, language models, or their combination. While the wide coverage of knowledge about entities and events make Wikidata a promising resource for analogical reasoning across situations and domains, Wikidata has not been employed for this task yet. In this paper, we investigate whether the knowledge in Wikidata supports analogical reasoning. Specifically, we study whether relational knowledge is modeled consistently in Wikidata, observing that relevant relational information is typically missing or modeled in an inconsistent way. Our further experiments show that Wikidata can be used to create data for analogy classification, but this requires much manual effort. To facilitate future work that can support analogies, we discuss key desiderata, and devise a set of metrics to guide an automatic method for extracting analogies from Wikidata. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Analogical Reasoning; Ontologies; User Experience; Wikidata; Artificial Intelligence; Analogical Reasoning; Automatic Method; Commonsense Knowledge Basis; Language Model; Lexical Resources; Ontology's; Reasoning Methods; Users' Experiences; Wikidata; Ontology},
	keywords = {Artificial intelligence; Analogical reasoning; Automatic method; Commonsense knowledge basis; Language model; Lexical resources; Ontology's; Reasoning methods; Users' experiences; Wikidata; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Kühn20224001,
	author = {Kühn, Ramona and Mitrović, Jelena and Granitzer, Michael},
	title = {GRhOOT: Ontology of Rhetorical Figures in German},
	year = {2022},
	pages = {4001 - 4010},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142653768&partnerID=40&md5=bfa9460da53065c27bf39e7ab03953d9},
	abstract = {GRhOOT, the German RhetOrical OnTology, is a domain ontology of 110 rhetorical figures in the German language. The overall goal of building an ontology of rhetorical figures in German is not only the formal representation of different rhetorical figures, but also allowing for their easier detection, thus improving sentiment analysis, argument mining, detection of hate speech and fake news, machine translation, and many other tasks in which recognition of non-literal language plays an important role. The challenge of building such ontologies lies in classifying the figures and assigning adequate characteristics to group them, while considering their distinctive features. The ontology of rhetorical figures in the Serbian language was used as a basis for our work. Besides transferring and extending the concepts of the Serbian ontology, we ensured completeness and consistency by using description logic and SPARQL queries. Furthermore, we show a decision tree to identify figures and suggest a usage scenario on how the ontology can be utilized to collect and annotate data. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Information Extraction; Information Retrieval; Knowledge Discovery/representation; Language Modelling; Ontologies; Semantic Web; Semantics; Computational Linguistics; Data Description; Decision Trees; Information Retrieval; Modeling Languages; Ontology; Sentiment Analysis; Speech Recognition; Domain Ontologies; Formal Representations; German Language; Information Extraction; Knowledge Discovery/representation; Language Model; Machine Translations; Ontology's; Semantic-web; Semantic Web},
	keywords = {Computational linguistics; Data description; Decision trees; Information retrieval; Modeling languages; Ontology; Sentiment analysis; Speech recognition; Domain ontologies; Formal representations; German language; Information extraction; Knowledge discovery/representation; Language model; Machine translations; Ontology's; Semantic-Web; Semantic Web},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@CONFERENCE{2022,
	title = {EKAW-C 2022 - Companion Proceedings of the 23rd International Conference on Knowledge Engineering and Knowledge Management},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3256},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142451123&partnerID=40&md5=a6e2b1932609d3af4ab4902a99ecf785},
	abstract = {The proceedings contain 11 papers. The topics discussed include: experiment maker: a tool to create experiments with GPT-3 Easily; CrowdIQ: an ontology for crowdsourced information quality assessments; automated identification of flaky builds using knowledge graphs; ATONTE: towards a new methodology for seed ontology development from texts and experts; a step toward semantic content negotiation; FAIR ontologies, FAIR ontology alignments; extracting structured knowledge from Dutch legal texts: a rule-based approach; knowledge-based legal document retrieval: a case study on Italian civil court decisions; ITALIAN-LEGAL-BERT: a pre-trained transformer language model for Italian law; and public procurement fraud detection and artificial intelligence techniques: a literature review. © 2022 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2022,
	title = {21st International Semantic Web Conference, ISWC 2022},
	year = {2022},
	journal = {Lecture Notes in Computer Science},
	volume = {13489 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141665889&partnerID=40&md5=2619b3fcbc43fc9d8d86e4bcafe7f820},
	abstract = {The proceedings contain 48 papers. The special focus in this conference is on International Semantic Web. The topics include: H<sup>2</sup>TNE : Temporal Heterogeneous Information Network Embedding in Hyperbolic Spaces; facing Changes: Continual Entity Alignment for Growing Knowledge Graphs; Mapping Relational Database Constraints to SHACL; POSO: A Generic Positioning System Ontology; each Snapshot to Each Space: Space Adaptation for Temporal Knowledge Graph Completion; efficient Dependency Analysis for Rule-Based Ontologies; heterogeneous Graph Neural Network with Hypernetworks for Knowledge Graph Embedding; MultPAX: Keyphrase Extraction Using Language Models and Knowledge Graphs; RT-KGD: Relation Transition Aware Knowledge-Grounded Dialogue Generation; Faithful Embeddings for EL<sup>+ +</sup> Knowledge Bases; LoGNet: Local and Global Triple Embedding Network; an Analysis of Content Gaps Versus User Needs in the Wikidata Knowledge Graph; Repairing SHACL Constraint Violations Using Answer Set Programming; entity Type Prediction Leveraging Graph Walks and Entity Descriptions; Strabo 2: Distributed Management of Massive Geospatial RDF Datasets; Controlled Query Evaluation in OWL 2 QL: A “Longest Honeymoon” Approach; a Survey of Syntactic Modelling Structures in Biomedical Ontologies; HybridFC: A Hybrid Fact-Checking Approach for Knowledge Graphs; GNNQ: A Neuro-Symbolic Approach to Query Answering over Incomplete Knowledge Graphs; Radar Station: Using KG Embeddings for Semantic Table Interpretation and Entity Disambiguation; enhancing Document-Level Relation Extraction by Entity Knowledge Injection; CRNet: Modeling Concurrent Events over Temporal Knowledge Graph; LODChain: Strengthen the Connectivity of Your RDF Dataset to the Rest LOD Cloud; WDV: A Broad Data Verbalisation Dataset Built from Wikidata; machine Learning-Friendly Biomedical Datasets for Equivalence and Subsumption Ontology Matching; The DLCC Node Classification Benchmark for Analyzing Knowledge Graph Embeddings; μKG : A Library for Multi-source Knowledge Graph Embeddings and Applications; IMGT-KG: A Knowledge Graph for Immunogenetics; REBench: Microbenchmarking Framework for Relation Extraction Systems; WDBench: A Wikidata Graph Query Benchmark. © 2022 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Lin2022,
	author = {Lin, Ruiming and Cheng, Lianglun and Wang, Tao and Deng, Jianfeng},
	title = {Trans-SBLGCN: A Transfer Learning Model for Event Logic Knowledge Graph Construction of Fault Diagnosis},
	year = {2022},
	pages = {},
	doi = {10.1109/IJCNN55064.2022.9892075},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140756470&doi=10.1109%2FIJCNN55064.2022.9892075&partnerID=40&md5=2a5f27922df5f74a673f6496463c1200},
	abstract = {Taking fault diagnosis corpus as the research object, an event logic knowledge graph construction method is proposed in this paper. Firstly, we propose a data labeling strategy based on a constructed event logic ontology model, then collect large-scale robot transmission system fault diagnosis corpus, and label part of the data according to the strategy. Secondly, we propose a transfer learning model called Trans-SBLGCN for event argument entity and event argument relation joint extraction. A language model is trained based on large-scale unlabeled fault diagnosis corpus and transferred to a model based on stacked bidirectional long short term memory (BiLSTM) and bidirectional graph convolutional network (BiGCN). Experimental results show that the method is superior to other methods. Finally, an event logic knowledge graph of robot transmission system fault diagnosis is constructed to provide decision support for autonomous robot transmission system fault diagnosis. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Bigcn; Event Logic Knowledge Graph; Fault Diagnosis; Knowledge Joint Extraction; Computer Circuits; Decision Support Systems; Entropy; Extraction; Fault Detection; Learning Systems; Robots; Bidirectional Graph Convolutional Network; Convolutional Networks; Event Logic; Event Logic Knowledge Graph; Faults Diagnosis; Knowledge Graphs; Knowledge Joint Extraction; System Faults; Transfer Learning; Transmission Systems; Failure Analysis},
	keywords = {Computer circuits; Decision support systems; Entropy; Extraction; Fault detection; Learning systems; Robots; Bidirectional graph convolutional network; Convolutional networks; Event logic; Event logic knowledge graph; Faults diagnosis; Knowledge graphs; Knowledge joint extraction; System faults; Transfer learning; Transmission systems; Failure analysis},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{2022,
	title = {26th International Conference on Enterprise Design, Operations, and Computing, EDOC 2022},
	year = {2022},
	journal = {Lecture Notes in Computer Science},
	volume = {13585 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140458018&partnerID=40&md5=b88ff60b1f80dda8ea9cb6ff58f75cc9},
	abstract = {The proceedings contain 15 papers. The special focus in this conference is on Enterprise Design, Operations, and Computing. The topics include: Extracting Business Process Entities and Relations from Text Using Pre-trained Language Models and In-Context Learning; discovering Sound Free-Choice Workflow Nets with Non-block Structures; shape Your Process: Discovering Declarative Business Processes from Positive and Negative Traces Taking into Account User Preferences; Semi-automated Test Migration for BPMN-Based Process-Driven Applications; splitting Quantum-Classical Scripts for the Generation of Quantum Workflows; a Multi-level Cyber-Security Reference Model in Support of Vulnerability Analysis; security Ontologies: A Systematic Literature Review; model-Based Construction of Enterprise Architecture Knowledge Graphs; enterprise Architecture Management Support for Digital Transformation Projects in Very Large Enterprises: A Case Study at a European Mobility Provider; interoperability of Digital Government Services: A Brazilian Reference Architecture Model to Promote Communication, Management, and Reuse of Solutions; Modeling, Executing and Monitoring IoT-Driven Business Rules with BPMN and DMN: Current Support and Challenges; Enhanced Transformation of BPMN Models with Cancellation Features; next-Activity Prediction for Non-stationary Processes with Unseen Data Variability. © 2022 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Yorsh202228,
	author = {Yorsh, Uladzislau and Behr, Alexander S. and Kockmann, Norbert and Holeňa, Martin},
	title = {Text-to-Ontology Mapping via Natural Language Processing Models},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3226},
	pages = {28 - 34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139875540&partnerID=40&md5=6da54e850bc7b9fa8ff112f3ca724e7b},
	abstract = {The paper presents work in progress attempting to solve a text-to-ontology mapping problem. While ontologies are being created as formal specifications of shared conceptualizations of application domains, different users often create different ontologies to represent the same domain. For better reasoning about concepts in scientific papers, it is desired to pick the ontology which best matches concepts present in the input text. We have started to automatize this process and attack the problem by utilizing state-of-the-art NLP tools and neural networks. Given a specific set of ontologies, we experiment with different training pipelines for NLP machine learning models with the aim to construct representative embeddings for the text-to-ontology matching task. We assess the final result through visualizing the latent space and exploring the mappings between an input text and ontology classes. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Fasttext; Language Models; Matching Text To Ontologies; Text Analysis; Learning Algorithms; Mapping; Natural Language Processing Systems; Bert; Fasttext; Language Model; Language Processing; Matching Text To Ontology; Matchings; Natural Languages; Ontology Mapping; Ontology's; Text Analysis; Ontology},
	keywords = {Learning algorithms; Mapping; Natural language processing systems; BERT; Fasttext; Language model; Language processing; Matching text to ontology; Matchings; Natural languages; Ontology mapping; Ontology's; Text analysis; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2022,
	title = {ITAT 2022 - Proceedings of the 22nd Conference Information Technologies - Applications and Theory},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3226},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139823193&partnerID=40&md5=126454825f97dab2a3a66868c14e6e9b},
	abstract = {The proceedings contain 26 papers. The topics discussed include: deep transfer learning of traversability assessment for heterogeneous robots; on the incremental construction of deep rule theories; text-to-ontology mapping via natural language processing models; tams: text augmentation using most similar synonyms for SMS spam filtering; using artificial neural networks to determine ontologies most relevant to scientific texts; learning to segment from object sizes; beyond sensor data analysis: unexpected challenges in a honeybee monitoring project; image classifier with dynamic set of known classes; analysis of the semantic vector space induced by a neural language model and a corpus; on reducing automata and their normalizations; and keyphrase extraction from Slovak court decisions. © 2022 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2022,
	title = {25th International Conference on Text, Speech, and Dialogue, TSD 2022},
	year = {2022},
	journal = {Lecture Notes in Computer Science},
	volume = {13502 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139079686&partnerID=40&md5=b5fcd4fe86d99f718d79f3d50e8485d8},
	abstract = {The proceedings contain 43 papers. The special focus in this conference is on Text, Speech, and Dialogue. The topics include: Linear Transformations for Cross-lingual Sentiment Analysis; TOKEN Is a MASK: Few-shot Named Entity Recognition with Pre-trained Language Models; use of Machine Learning Methods in the Assessment of Programming Assignments; ontology-Aware Biomedical Relation Extraction; balancing the Style-Content Trade-Off in Sentiment Transfer Using Polarity-Aware Denoising; a Self-Evaluating Architecture for Describing Data; a Novel Hybrid Framework to Enhance Zero-shot Classification; attention-Based Model for Accurate Stance Detection; OPTICS: Automatic MT Evaluation Based on Optimal Transport by Integration of Contextual Representations and Static Word Embeddings; DaFNeGE: Dataset of French Newsletters with Graph Representation and Embedding; exploration of Multi-corpus Learning for Hate Speech Classification in Low Resource Scenarios; on the Importance of Word Embedding in Automated Harmful Information Detection; BERT-based Classifiers for Fake News Detection on Short and Long Texts with Noisy Data: A Comparative Analysis; can a Machine Generate a Meta-Review? How Far Are We?; autoblog 2021: The Importance of Language Models for Spontaneous Lecture Speech; Transformer-Based Automatic Speech Recognition of Formal and Colloquial Czech in MALACH Project; wakeword Detection Under Distribution Shifts; end-to-End Parkinson’s Disease Detection Using a Deep Convolutional Recurrent Network; lexical Bundle Variation in Business Actors’ Public Communications; 50 Shades of Gray: Effect of the Color Scale for the Assessment of Speech Disorders; ANTILLES: An Open French Linguistically Enriched Part-of-Speech Corpus; sub 8-Bit Quantization of Streaming Keyword Spotting Models for Embedded Chipsets; detection of Prosodic Boundaries in Speech Using Wav2Vec 2.0; text-to-Text Transfer Transformer Phrasing Model Using Enriched Text Input; Lexicon-based vs. Lexicon-free ASR for Norwegian Parliament Speech Transcription; on Comparison of Phonetic Representations for Czech Neural Speech Synthesis; the Influence of Dataset Partitioning on Dysfluency Detection Systems; going Beyond the Cookie Theft Picture Test: Detecting Cognitive Impairments Using Acoustic Features. © 2022 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Dordiuk20221,
	author = {Dordiuk, Vladislav and Demicheva, Ekaterina I. and Polanco Espino, Fernando Jonathan and Ushenin, Konstantin S.},
	title = {Natural language processing for clusterization of genes according to their functions},
	year = {2022},
	pages = {1 - 4},
	doi = {10.1109/CSGB56354.2022.9865330},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138478040&doi=10.1109%2FCSGB56354.2022.9865330&partnerID=40&md5=6c4eac4e8e9a5b8004ace8541bc5ae8b},
	abstract = {There are hundreds of methods for analysis of data obtained in mRNA-sequencing. The most of them are focused on small number of genes. In this study, we propose an approach that reduces the analysis of several thousand genes to analysis of several clusters. The list of genes is enriched with information from open databases. Then, the descriptions are encoded as vectors using the pretrained language model (BERT) and some text processing approaches. The encoded gene function pass through the dimensionality reduction and clusterization. Aiming to find the most efficient pipeline, 180 cases of pipeline with different methods in the major pipeline steps were analyzed. The performance was evaluated with clusterization indexes and expert review of the results. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Clusterization; Differential Gene Expression Analysis; Gene Expression; Gene Ontology; Natural Language Processing; Semantic Analysis; Gene Ontology; Natural Language Processing Systems; Pipelines; Semantics; Text Processing; Bert; Clusterization; Differential Gene Expression Analyse; Differential Gene Expressions; Gene Expression Analysis; Gene Ontology; Genes Expression; Language Processing; Natural Language Processing; Natural Languages; Semantic Analysis; Gene Expression},
	keywords = {Gene Ontology; Natural language processing systems; Pipelines; Semantics; Text processing; BERT; Clusterization; Differential gene expression analyse; Differential gene expressions; Gene expression analysis; Gene ontology; Genes expression; Language processing; Natural language processing; Natural languages; Semantic analysis; Gene expression},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Dou20227250,
	author = {Dou, Yao and Forbes, Maxwell and Koncel-Kedziorski, Rik and Smith, Noah A. and Choi, Yejin},
	title = {Is GPT-3 Text Indistinguishable from Human Text? SCARECROW: A Framework for Scrutinizing Machine Text},
	year = {2022},
	journal = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
	volume = {1},
	pages = {7250 - 7274},
	doi = {10.18653/v1/2022.acl-long.501},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138310305&doi=10.18653%2Fv1%2F2022.acl-long.501&partnerID=40&md5=ded90dd6f93fc25ef05b48706e1d868b},
	abstract = {Modern neural language models can produce remarkably fluent and grammatical text. So much, in fact, that recent work by Clark et al. (2021) has reported that conventional crowdsourcing can no longer reliably distinguish between machine-authored (GPT-3) and human-authored writing. As errors in machine generations become ever subtler and harder to spot, it poses a new challenge to the research community for robust machine text evaluation. We propose a new framework called SCARECROW for scrutinizing machine text via crowd annotation. To support the broad range of real machine errors that can be identified by laypeople, the ten error categories of SCARECROW-such as redundancy, commonsense errors, and incoherence -are identified through several rounds of crowd annotation experiments without a predefined ontology. We then use SCARECROW to collect over 41k error spans in human-written and machine-generated paragraphs of English language news text. We isolate factors for detailed analysis, including parameter count, training data, and various decoding-time configurations. Our approach successfully quantifies measurable gaps between human authored text and generations from models of several sizes, including fourteen configurations of GPT-3. In addition, our analysis unveils new insights, with detailed rationales provided by laypeople, e.g., that the commonsense capabilities have been improving with larger models while math capabilities have not, and that the choices of simple decoding hyperparameters can make remarkable differences on the perceived quality of machine text. We release our training material, annotation toolkit and dataset at https://yao-dou.github.io/scarecrow/. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Decoding; Quality Control; English Languages; Fluents; Language Model; Large Models; Machine Errors; Ontology's; Research Communities; Text Evaluation; Time Configuration; Training Data; Errors},
	keywords = {Computational linguistics; Decoding; Quality control; English languages; Fluents; Language model; Large models; Machine errors; Ontology's; Research communities; Text evaluation; Time configuration; Training data; Errors},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 81; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Rezayi20225150,
	author = {Rezayi, Saed and Liu, Zhengliang and Wu, Zihao and Dhakal, Chandra K. and Ge, Bao and Zhen, Chen and Liu, Tianming and Li, Sheng},
	title = {AgriBERT: Knowledge-Infused Agricultural Language Models for Matching Food and Nutrition},
	year = {2022},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	pages = {5150 - 5156},
	doi = {10.24963/ijcai.2022/715},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137944663&doi=10.24963%2Fijcai.2022%2F715&partnerID=40&md5=2ad321c7d448b113ce52793a93cd56b4},
	abstract = {Pretraining domain-specific language models remains an important challenge which limits their applicability in various areas such as agriculture. This paper investigates the effectiveness of leveraging food related text corpora (e.g., food and agricultural literature) in pretraining transformer-based language models. We evaluate our trained language model, called AgriBERT, on the task of semantic matching, i.e., establishing mapping between food descriptions and nutrition data, which is a long-standing challenge in the agricultural domain. In particular, we formulate the task as an answer selection problem, fine-tune the trained language model with the help of an external source of knowledge (e.g., FoodOn ontology), and establish a baseline for this task. The experimental results reveal that our language model substantially outperforms other language models and baselines in the task of matching food description and nutrition. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Agriculture; Artificial Intelligence; Computational Linguistics; Problem Oriented Languages; Semantics; Domains Specific Languages; External Sources; Language Model; Matchings; Ontology's; Pre-training; Selection Problems; Semantic Matching; Text Corpora; Nutrition},
	keywords = {Agriculture; Artificial intelligence; Computational linguistics; Problem oriented languages; Semantics; Domains specific languages; External sources; Language model; Matchings; Ontology's; Pre-training; Selection problems; Semantic matching; Text corpora; Nutrition},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32; All Open Access; Bronze Open Access}
}

@ARTICLE{Ataei2022,
	author = {Ataei, Sima and Butler, Greg},
	title = {Predicting the specific substrate for transmembrane transport proteins using BERT language model},
	year = {2022},
	pages = {},
	doi = {10.1109/CIBCB55180.2022.9863051},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137911398&doi=10.1109%2FCIBCB55180.2022.9863051&partnerID=40&md5=bd7172d9fc50e43ab2b8b5b773690981},
	abstract = {Transmembrane transport proteins play a vital role in cells' metabolism by the selective passage of substrates through the cell membrane. Metabolic network reconstruction requires transport reactions that describe the specific substrate transported as well as the metabolic reactions of enzyme catalysis. In this paper, we apply BERT (Bidirectional Encoder Representations from Transformers) language model for protein sequences to predict one of 12 specific substrates. Our UniProt-ICAT-100 dataset is automatically constructed from UniProt using the ChEBI and GO ontologies to identify 4,112 proteins transporting 12 inorganic anion or cation substrates. We classified this dataset using three different models including Logistic Regression with an MCC of 0.81 and accuracy of 97.5%; Feed-forward Neural Networks classifier with an MCC of 0.88 and accuracy of 98.5%. Our third model utilizes a Fine-tuned BERT language model to predict the specific substrate with an MCC of 0.95 and accuracy of 99.3% on an independent test set. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert Model; Chebi Ontology; Classification; Gene Ontology; Specific Substrate Prediction; Transport Protein; Classification (of Information); Cytology; Feedforward Neural Networks; Forecasting; Metabolism; Proteins; Substrates; Bidirectional Encoder Representation From Transformer Model; Chebi Ontology; Gene Ontology; Language Model; Ontology's; Specific Substrate Prediction; Specific Substrates; Transformer Modeling; Transmembrane Transport; Transport Proteins; Gene Ontology},
	keywords = {Classification (of information); Cytology; Feedforward neural networks; Forecasting; Metabolism; Proteins; Substrates; Bidirectional encoder representation from transformer model; ChEBI ontology; Gene ontology; Language model; Ontology's; Specific substrate prediction; Specific substrates; Transformer modeling; Transmembrane transport; Transport proteins; Gene Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Vassileva202274,
	author = {Vassileva, Sylvia and Boytcheva, Svetla and Koychev, Ivan Kolev},
	title = {Tool for Automatic Annotation of Clinical Texts in Bulgarian – BGMedAnno},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3191},
	pages = {74 - 88},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137180843&partnerID=40&md5=292dd65ad5c1835f17a3714c310e1f8c},
	abstract = {This paper describes the design of BGMedAnno: an automatic annotation tool for clinical text in Bulgarian. The proposed solution combines classical rule-based and dictionary-based approaches for name entity recognition (NER) with more advanced deep learning methods to identify different categories of medical terms, as well as some nested objects. The following categories of medical terms are currently identified: symptoms, complaints, diagnoses, anatomical organs and systems, risk factors, and family history. In addition, the negation relation and its scope are recognized. The location relation was modeled for connecting different categories of symptoms and complaints to anatomical organs and systems. All identified concepts were normalized to medical standard classifications and ontologies like UMLS, MESH, ICD-10 and mapped into the concepts of the linked open data Knowledge Graph WikiData. The proposed approach for automatic medical terms and their relations recognition shows high accuracy. The rule-based method shows an F1-score of 75%, while the trained BERT-based model presents an F1 score of 73%. Although the BERT model performs slightly worse on the test set, observations show that it finds objects in sentences that are not covered by the rule-based method. For the object linking task, the developed method based on the BERT language model shows a 61% F1 result, significantly outperforming direct string comparison, which achieves an F1 result of 45%. The developed user interface allows direct application of the annotation tool for individual texts. The service API outputs data in JSON format, which enables interoperability with other systems and can be used to process large collections of clinical data. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Annotation Tools; Deep Learning; Health Informatics; Machine Learning; Natural Language Processing; Application Programming Interfaces (api); Deep Learning; Diagnosis; Knowledge Graph; Learning Algorithms; Learning Systems; Medical Informatics; Natural Language Processing Systems; Open Data; User Interfaces; Annotation Tool; Automatic Annotation; Health Informatics; Language Processing; Machine-learning; Medical Terms; Natural Language Processing; Natural Languages; Rule-based Method; Interoperability},
	keywords = {Application programming interfaces (API); Deep learning; Diagnosis; Knowledge graph; Learning algorithms; Learning systems; Medical informatics; Natural language processing systems; Open Data; User interfaces; Annotation tool; Automatic annotation; Health informatics; Language processing; Machine-learning; Medical terms; Natural language processing; Natural languages; Rule-based method; Interoperability},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Bernik2022235,
	author = {Bernik, Matic and Tovornik, Robert and Fabjan, Borut and Marco-Ruiz, Luis},
	title = {DIAGÑOZA: a Natural Language Processing Tool for Automatic Annotation of Clinical Free Text with SNOMED-CT},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3180},
	pages = {235 - 243},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136923785&partnerID=40&md5=4cf4df7dc8e51e9de377d9e05548b129},
	abstract = {Secondary use of clinical data needs to deal with large amounts of free text present in Electronic Health Records. DIAGÑOZA is a tool that combines the advantages of statistical Natural Language Processing with the high performance of a Vector Database (VDB) for query answering. This paper presents the approach followed in the BioASQ-DisTEMIST subtasks for disease identification in free text and their annotation with the SNOMED-CT terminology. DIAGÑOZA relies on pretrained Spanish language models for tokenization, Part-of-speech tagging, and partially entity extraction. The text entities resulting from the NLP pipeline are vectorized by FastText. In addition, vector embeddings of SNOMED-CT fully specified names, definitions and synonyms are normalized and stored in the VDB. This allows the VDB to estimate similarities between the entity embeddings and SNOMED-CT concept embeddings to perform matching between them. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Named Entity Recognition; Natural Language Processing; Ontology Linking; Semantic Indexing; Snomed-ct; Spanish; Terminologies; Computational Linguistics; Embeddings; Natural Language Processing Systems; Query Processing; Semantics; Text Processing; Free Texts; Language Processing; Named Entity Recognition; Natural Language Processing; Natural Languages; Ontology Linking; Ontology's; Semantic Indexing; Snomed-ct; Spanish; Terminology},
	keywords = {Computational linguistics; Embeddings; Natural language processing systems; Query processing; Semantics; Text processing; Free texts; Language processing; Named entity recognition; Natural language processing; Natural languages; Ontology linking; Ontology's; Semantic indexing; SNOMED-CT; Spanish; Terminology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Piat2022760,
	author = {Piat, Guilhem and Semmar, Nasredine and Allauzen, Alexandre and Essafi, Hassane and Tourille, Julien},
	title = {Enriching Contextualized Representations with Biomedical Ontologies: Extending KnowBert to UMLS},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {507 LNNS},
	pages = {760 - 773},
	doi = {10.1007/978-3-031-10464-0_52},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135084777&doi=10.1007%2F978-3-031-10464-0_52&partnerID=40&md5=3996864d1af20f372c8a832580533b63},
	abstract = {Currently, biomedical document processing is mostly human work. Software solutions which attempt to alleviate this burden exist but generally do not perform well enough to be helpful in many applications. Concurrently, there exist projects which organize concepts in the biomedical field. Therefore, we seek to leverage existing structured knowledge resources to improve biomedical language modeling. In this paper, we provide an implementation integrating the UMLS knowledgebase into a BERT-based language model, aiming to improve its performance in biomedical Named Entity Recognition. To achieve this, we extend KnowBert, a recently developed technique for integrating knowledge into language models. Preliminary results reveal the challenges of applying KnowBert to the biomedical domain given the number and subtlety of different concepts in UMLS. Going forward, addressing these challenges and combining this with other approaches such as BioBERT may help expand the range of usefully automatable biomedical language processing tasks. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Neural Networks; Biomedical Informatics; Information Extraction; Knowledge Based Systems; Knowledge Representation; Machine Learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@ARTICLE{Lomov2022337,
	author = {Lomov, Pavel A. and Malozemova, Marina and Shishaev, Maxim},
	title = {Extracting Relations from NER-Tagged Sentences for Ontology Learning},
	year = {2022},
	journal = {Lecture Notes in Networks and Systems},
	volume = {502 LNNS},
	pages = {337 - 344},
	doi = {10.1007/978-3-031-09076-9_31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135040942&doi=10.1007%2F978-3-031-09076-9_31&partnerID=40&md5=c41a0291643a6e3d057fb866e2c40e2c},
	abstract = {This paper is a continuation of the research on training and using a neural-network language model for ontology learning. The paper deals with the problem of analyzing domain natural language texts and extracting possible relations between concepts from them. A brief overview of existing approaches to solving this problem is presented. A procedure for extracting relations between concepts based on the analysis of the sentence syntax dependency tree is proposed. The results of evaluating the efficiency of extracting relations using this procedure are considered. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Nlp; Ontology Learning; Relation Extraction},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2022,
	title = {27th International Conference on Applications of Natural Language to Information Systems, NLDB 2022},
	year = {2022},
	journal = {Lecture Notes in Computer Science},
	volume = {13286 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132992786&partnerID=40&md5=1627bb169c4355f8ec959e1c9a9121a3},
	abstract = {The proceedings contain 48 papers. The special focus in this conference is on Applications of Natural Language to Information Systems. The topics include: Using Meaning Instead of Words to Track Topics; A BERT-Based Approach for Multilingual Discourse Connective Detection; studying the Role of Named Entities for Content Preservation in Text Style Transfer; automatically Computing Connotative Shifts of Lexical Items; the Case of Imperfect Negation Cues: A Two-Step Approach for Automatic Negation Scope Resolution; evaluating Gender Bias in Film Dialogue; investigating Topic-Agnostic Features for Authorship Tasks in Spanish Political Speeches; using Language Models for Classifying the Party Affiliation of Political Texts; detecting Vaccine Skepticism on Twitter Using Heterogeneous Information Networks; profiling Fake News Spreaders on Twitter: A Clickbait and Linguistic Feature Based Scheme; A Framework for False Negative Detection in NER/NEL; Network Analysis of German COVID-19 Related Discussions on Telegram; Automatic Mapping of Quranic Ontologies Using RML and Cellfie Plugin; slot Filling for Extracting Reskilling and Upskilling Options from the Web; extracting Domain Terms from Data Model Elements: Task Overview in One Implementation; a Text Structural Analysis Model for Address Extraction; a Decade of Legal Argumentation Mining: Datasets and Approaches; predicting Argument Density from Multiple Annotations; A BERT-Based Model for Question Answering on Construction Incident Reports; convolutional Graph Neural Networks for Hate Speech Detection in Data-Poor Settings; RUTA:MED – Dual Workflow Medical Speech Transcription Pipeline and Editor; Preprocessing Requirements Documents for Automatic UML Modelling; linguistically Enhanced Text to Sign Gloss Machine Translation; automated Bug Triaging in a Global Software Development Environment: An Industry Experience; extracting and Understanding Call-to-actions of Push-Notifications; classifying Documents by Viewpoint Using Word2Vec and Support Vector Machines; metric Learning and Adaptive Boundary for Out-of-Domain Detection; identifying Fake News in Brazilian Portuguese; an Ensemble Approach for Dutch Cross-Domain Hate Speech Detection. © 2022 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Shivashankar2022,
	author = {Shivashankar, Kanchan and Benmaarouf, Khaoula and Steinmetz, Nadine},
	title = {Reaching out for the Answer: Answer Type Prediction},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3119},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129222847&partnerID=40&md5=273cbb1f6d1a1f25d7db43ea27670c54},
	abstract = {Natural language is complex and similar statements can be expressed in various ways. Therefore, understanding natural language is an ongoing challenge in the field of Question Answering. To make the process from the question to the answer, the QA pipeline can be broken down to several sub-steps, as e.g. prediction of the potential type of the answer, entity linking and detection of references for properties relevant for the formal query. The SMART Task challenge, co-located with ISWC 2021, focussed on two of these sub-tasks: relation linking and answer type prediction. With this paper, we present our approach for the latter task. Our solution is a two-staged process combining two separate multi-label classification tasks. For the answer category prediction, we utilize the RoBERTa language model. Questions predicted as resource questions are then further classified regarding the concrete answer types - a list of ontology classes - utilizing the BERT language model. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Answertype Prediction; Question Answering; Semantic Web; Classification (of Information); Computational Linguistics; Natural Language Processing Systems; Query Processing; Semantic Web; Answertype Prediction; Broken Down; Co-located; Language Model; Natural Languages; Property; Question Answering; Semantic-web; Subtask; Type Predictions; Forecasting},
	keywords = {Classification (of information); Computational linguistics; Natural language processing systems; Query processing; Semantic Web; Answertype prediction; Broken down; Co-located; Language model; Natural languages; Property; Question Answering; Semantic-Web; Subtask; Type predictions; Forecasting},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Argüello Casteleiro2022108,
	author = {Argüello Casteleiro, Mercedes and Henson, C. and Maroto, Nava and Li, S. and Des-Diz, Julio and Fernandez-Prieto, Maria Jesus and Peters, Simon and Furmston, Tim and Sevillano Torrado, Carlos and Maseda-Fernandez, Diego},
	title = {MetaMap versus BERT models with explainable active learning: Ontology-based experiments with prior knowledge for COVID-19},
	year = {2022},
	journal = {CEUR Workshop Proceedings},
	volume = {3127},
	pages = {108 - 117},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128899680&partnerID=40&md5=1c31de031ed995c8019e4f2dcca31a30},
	abstract = {Emergence of the Coronavirus 2019 Disease has highlighted further the need for timely support for clinicians as they manage severely ill patients. We combine Semantic Web technologies with Deep Learning for Natural Language Processing with the aim of converting human-readable best evidence/ practice for COVID-19 into that which is computer-interpretable. We present the results of experiments with 1212 clinical ideas (medical terms and expressions) from two UK national healthcare services specialty guides for COVID-19 and three versions of two BMJ Best Practice documents for COVID-19. The paper seeks to recognise and categorise clinical ideas, performing a Named Entity Recognition (NER) task, with an ontology providing extra terms as context and describing the intended meaning of categories understandable by clinicians. The paper investigates: 1) the performance of classical NER using MetaMap versus NER with fine-tuned BERT models; 2) the integration of both NER approaches using a lightweight ontology developed in close collaboration with senior doctors; and 3) the easy interpretation by junior doctors of the main classes from the ontology once populated with NER results. We report the NER performance and the observed agreement for human audits. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Covid-19; Deep Learning For Natural Language Processing; Ontologies; Static Embeddings; Transformer-based Language Models; Coronavirus; Deep Learning; Natural Language Processing Systems; Active Learning; Covid-19; Deep Learning For Natural Language Processing; Embeddings; Language Model; Named Entity Recognition; Ontology's; Performance; Static Embedding; Transformer-based Language Model; Ontology},
	keywords = {Coronavirus; Deep learning; Natural language processing systems; Active Learning; COVID-19; Deep learning for natural language processing; Embeddings; Language model; Named entity recognition; Ontology's; Performance; Static embedding; Transformer-based language model; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Dheenadhayalan2022315,
	author = {Dheenadhayalan, R. and Deepak, Gerard},
	title = {OntoReqC: An Ontology Focused Integrative Approach for Classification of Software Requirements},
	year = {2022},
	journal = {Lecture Notes on Data Engineering and Communications Technologies},
	volume = {106},
	pages = {315 - 324},
	doi = {10.1007/978-981-16-8403-6_28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127711545&doi=10.1007%2F978-981-16-8403-6_28&partnerID=40&md5=1f2e5663ad5f3aa97e85b0228bfff2b1},
	abstract = {Software products start to develop based on the client requirements that the software demands and classified requirements can result in well-developed software. This paper proposes the OntoReqC framework, a requirements classifier that has an accuracy of 95.49%. OntoReqC amalgamates ontologies, domain markers and is optimized with the ant colony optimization algorithm. This model obtains functional and nonfunctional ontologies from the unified modeling language models and software requirement artifact using OntoCollab and Protégé. Domain markers are derived from software requirements specification document using linked open data cloud for domain enrichment making this model knowledge-based. Ontologies and domain markers are combined to get content development network which is then semantically aligned with requirement terms obtained from preprocessed requirements dataset. Ant colony optimization has been applied to the results, producing the classified requirements. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain Knowledge; Knowledge Centric Approach; Ontology; Requirement Engineering; Requirements Classification; Domain Knowledge; Open Data; Open Systems; Requirements Engineering; Unified Modeling Language; Ant Colony Optimization Algorithms; Classifieds; Client Requirement; Domain Knowledge; Knowledge Centric Approach; Ontology's; Requirement Engineering; Requirements Classifications; Software Products; Software Requirements; Ontology},
	keywords = {Domain Knowledge; Open Data; Open systems; Requirements engineering; Unified Modeling Language; Ant Colony Optimization algorithms; Classifieds; Client requirement; Domain knowledge; Knowledge centric approach; Ontology's; Requirement engineering; Requirements classifications; Software products; Software requirements; Ontology},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lim2022401,
	author = {Lim, Chae-gyun and Lee, Dongkun and Lee, Youngjun and Choi, Ho-jin},
	title = {Knowledge Management Approach for Memory Components Based on User-friendly Conversational System},
	year = {2022},
	pages = {401 - 403},
	doi = {10.1109/BigComp54360.2022.00091},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127595721&doi=10.1109%2FBigComp54360.2022.00091&partnerID=40&md5=e09f2a1f2d9935ec3fc6a8970190314f},
	abstract = {Due to the recent technological development and the growth of computing resources, there are various studies that apply large-scale language models in the field of natural language processing such as conversational systems. Also, there are researches that attempt to maintain a conversation flow and naturally lead a dialogue by treating the contextual information exchanged with users from the perspective of knowledge. In this paper, we propose a method for managing various contextual information such as chat history, situations, and preferred topics based on a knowledge base and generating a conversation customized for the specific user. We design a schema of memory components to deal with the user's contextual information so that implement a Web-based conversational system, which is friendly come to those users. It is expected that these systems using the user-specific memory components will be helpful in such domains like education or customer consultation. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Conversation History; Conversational System; Memory Component; Ontology-based Approach; Knowledge Based Systems; Knowledge Management; Natural Language Processing Systems; Component Based; Computing Resource; Contextual Information; Conversation History; Conversational Systems; Memory Component; Ontology-based; Ontology-based Approach; Technological Development; User Friendly; Ontology},
	keywords = {Knowledge based systems; Knowledge management; Natural language processing systems; Component based; Computing resource; Contextual information; Conversation history; Conversational systems; Memory component; Ontology-based; Ontology-based approach; Technological development; User friendly; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Lai2022,
	author = {Lai, Boqiao and Xu, Jinbo},
	title = {Accurate protein function prediction via graph attention networks with predicted structure information},
	year = {2022},
	journal = {Briefings in Bioinformatics},
	volume = {23},
	number = {1},
	pages = {},
	doi = {10.1093/bib/bbab502},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126708344&doi=10.1093%2Fbib%2Fbbab502&partnerID=40&md5=4d1d79ebb18988845b7d9c7c6dd46338},
	abstract = {Experimental protein function annotation does not scale with the fast-growing sequence databases. Only a tiny fraction (<0.1%) of protein sequences has experimentally determined functional annotations. Computational methods may predict protein function very quickly, but their accuracy is not very satisfactory. Based upon recent breakthroughs in protein structure prediction and protein language models, we develop GAT-GO, a graph attention network (GAT) method that may substantially improve protein function prediction by leveraging predicted structure information and protein sequence embedding. Our experimental results show that GAT-GO greatly outperforms the latest sequence- and structure-based deep learning methods. On the PDB-mmseqs testset where the train and test proteins share <15% sequence identity, our GAT-GO yields Fmax (maximum F-score) 0.508, 0.416, 0.501, and area under the precision-recall curve (AUPRC) 0.427, 0.253, 0.411 for the MFO, BPO, CCO ontology domains, respectively, much better than the homology-based method BLAST (Fmax 0.117, 0.121, 0.207 and AUPRC 0.120, 0.120, 0.163) that does not use any structure information. On the PDB-cdhit testset where the training and test proteins are more similar, although using predicted structure information, our GAT-GO obtains Fmax 0.637, 0.501, 0.542 for the MFO, BPO, CCO ontology domains, respectively, and AUPRC 0.662, 0.384, 0.481, significantly exceeding the just-published method DeepFRI that uses experimental structures, which has Fmax 0.542, 0.425, 0.424 and AUPRC only 0.313, 0.159, 0.193. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Gene Ontology; Graph Attention Networks; Machine Learning; Protein Function Prediction; Amino Acid Sequence; Article; Attention Network; Deep Learning; Embedding; Gene Ontology; Human; Human Experiment; Language; Machine Learning; Prediction; Protein Domain; Protein Function; Protein Structure; Recall},
	keywords = {amino acid sequence; article; attention network; deep learning; embedding; gene ontology; human; human experiment; language; machine learning; prediction; protein domain; protein function; protein structure; recall},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 76; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Boguslav2021,
	author = {Boguslav, Mayla R. and Hailu, Negacy D. and Bada, Michael A. and Baumgartner, William A. and Hunter, Lawrence E.},
	title = {Concept recognition as a machine translation problem},
	year = {2021},
	journal = {BMC Bioinformatics},
	volume = {22},
	pages = {},
	doi = {10.1186/s12859-021-04141-4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121442101&doi=10.1186%2Fs12859-021-04141-4&partnerID=40&md5=3684d19525c5217369742f7817280643},
	abstract = {Background: Automated assignment of specific ontology concepts to mentions in text is a critical task in biomedical natural language processing, and the subject of many open shared tasks. Although the current state of the art involves the use of neural network language models as a post-processing step, the very large number of ontology classes to be recognized and the limited amount of gold-standard training data has impeded the creation of end-to-end systems based entirely on machine learning. Recently, Hailu et al. recast the concept recognition problem as a type of machine translation and demonstrated that sequence-to-sequence machine learning models have the potential to outperform multi-class classification approaches. Methods: We systematically characterize the factors that contribute to the accuracy and efficiency of several approaches to sequence-to-sequence machine learning through extensive studies of alternative methods and hyperparameter selections. We not only identify the best-performing systems and parameters across a wide variety of ontologies but also provide insights into the widely varying resource requirements and hyperparameter robustness of alternative approaches. Analysis of the strengths and weaknesses of such systems suggest promising avenues for future improvements as well as design choices that can increase computational efficiency with small costs in performance. Results: Bidirectional encoder representations from transformers for biomedical text mining (BioBERT) for span detection along with the open-source toolkit for neural machine translation (OpenNMT) for concept normalization achieve state-of-the-art performance for most ontologies annotated in the CRAFT Corpus. This approach uses substantially fewer computational resources, including hardware, memory, and time than several alternative approaches. Conclusions: Machine translation is a promising avenue for fully machine-learning-based concept recognition that achieves state-of-the-art results on the CRAFT Corpus, evaluated via a direct comparison to previous results from the 2019 CRAFT shared task. Experiments illuminating the reasons for the surprisingly good performance of sequence-to-sequence methods targeting ontology identifiers suggest that further progress may be possible by mapping to alternative target concept representations. All code and models can be found at: https://github.com/UCDenver-ccp/Concept-Recognition-as-Translation. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Resources; Concept Recognition; Machine Translation; Named Entity Normalization; Named Entity Recognition; Classification (of Information); Computational Efficiency; Computational Linguistics; Computer Aided Language Translation; Efficiency; Learning Algorithms; Machine Learning; Natural Language Processing Systems; Ontology; Automated Assignments; Computational Resources; Concept Recognition; Hyper-parameter; Machine Translations; Named Entity Normalizations; Named Entity Recognition; Ontology's; Performance; State Of The Art; Machine Translation},
	keywords = {Classification (of information); Computational efficiency; Computational linguistics; Computer aided language translation; Efficiency; Learning algorithms; Machine learning; Natural language processing systems; Ontology; Automated assignments; Computational resources; Concept recognition; Hyper-parameter; Machine translations; Named entity normalizations; Named entity recognition; Ontology's; Performance; State of the art; Machine translation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Gold Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Pourreza-Shahri2021,
	author = {Pourreza-Shahri, Morteza and Kahanda, Indika},
	title = {Deep semi-supervised learning ensemble framework for classifying co-mentions of human proteins and phenotypes},
	year = {2021},
	journal = {BMC Bioinformatics},
	volume = {22},
	number = {1},
	pages = {},
	doi = {10.1186/s12859-021-04421-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117368463&doi=10.1186%2Fs12859-021-04421-z&partnerID=40&md5=35b67b0318cb8e7b9fb9e381f771e525},
	abstract = {Background: Identifying human protein-phenotype relationships has attracted researchers in bioinformatics and biomedical natural language processing due to its importance in uncovering rare and complex diseases. Since experimental validation of protein-phenotype associations is prohibitive, automated tools capable of accurately extracting these associations from the biomedical text are in high demand. However, while the manual annotation of protein-phenotype co-mentions required for training such models is highly resource-consuming, extracting millions of unlabeled co-mentions is straightforward. Results: In this study, we propose a novel deep semi-supervised ensemble framework that combines deep neural networks, semi-supervised, and ensemble learning for classifying human protein-phenotype co-mentions with the help of unlabeled data. This framework allows the ability to incorporate an extensive collection of unlabeled sentence-level co-mentions of human proteins and phenotypes with a small labeled dataset to enhance overall performance. We develop PPPredSS, a prototype of our proposed semi-supervised framework that combines sophisticated language models, convolutional networks, and recurrent networks. Our experimental results demonstrate that the proposed approach provides a new state-of-the-art performance in classifying human protein-phenotype co-mentions by outperforming other supervised and semi-supervised counterparts. Furthermore, we highlight the utility of PPPredSS in powering a curation assistant system through case studies involving a group of biologists. Conclusions: This article presents a novel approach for human protein-phenotype co-mention classification based on deep, semi-supervised, and ensemble learning. The insights and findings from this work have implications for biomedical researchers, biocurators, and the text mining community working on biomedical relationship extraction. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Relationship Extraction; Deep Learning; Ensemble Learning; Human Phenotype Ontology; Protein Phenotype Relationships; Semi-supervised Learning; Deep Neural Networks; Extraction; Learning Algorithms; Natural Language Processing Systems; Recurrent Neural Networks; Biomedical Relationship Extraction; Deep Learning; Ensemble Learning; Human Phenotype Ontology; Human Proteins; Ontology's; Protein Phenotype Relationship; Relationship Extraction; Semi-supervised; Semi-supervised Learning; Proteins; Data Mining; Human; Phenotype; Supervised Machine Learning; Data Mining; Humans; Neural Networks, Computer; Phenotype; Supervised Machine Learning},
	keywords = {Deep neural networks; Extraction; Learning algorithms; Natural language processing systems; Recurrent neural networks; Biomedical relationship extraction; Deep learning; Ensemble learning; Human phenotype ontology; Human proteins; Ontology's; Protein phenotype relationship; Relationship extraction; Semi-supervised; Semi-supervised learning; Proteins; data mining; human; phenotype; supervised machine learning; Data Mining; Humans; Neural Networks, Computer; Phenotype; Supervised Machine Learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Littmann2021,
	author = {Littmann, Maria and Heinzinger, Michael and Dallago, Christian and Olenyi, Tobias and Rost, Burkhard},
	title = {Embeddings from deep learning transfer GO annotations beyond homology},
	year = {2021},
	journal = {Scientific Reports},
	volume = {11},
	number = {1},
	pages = {},
	doi = {10.1038/s41598-020-80786-0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099475205&doi=10.1038%2Fs41598-020-80786-0&partnerID=40&md5=9fd44e1badc3e945004dbfaef0b01eb8},
	abstract = {Knowing protein function is crucial to advance molecular and medical biology, yet experimental function annotations through the Gene Ontology (GO) exist for fewer than 0.5% of all known proteins. Computational methods bridge this sequence-annotation gap typically through homology-based annotation transfer by identifying sequence-similar proteins with known function or through prediction methods using evolutionary information. Here, we propose predicting GO terms through annotation transfer based on proximity of proteins in the SeqVec embedding rather than in sequence space. These embeddings originate from deep learned language models (LMs) for protein sequences (SeqVec) transferring the knowledge gained from predicting the next amino acid in 33 million protein sequences. Replicating the conditions of CAFA3, our method reaches an F<inf>max</inf> of 37 ± 2%, 50 ± 3%, and 57 ± 2% for BPO, MFO, and CCO, respectively. Numerically, this appears close to the top ten CAFA3 methods. When restricting the annotation transfer to proteins with < 20% pairwise sequence identity to the query, performance drops (F<inf>max</inf> BPO 33 ± 2%, MFO 43 ± 3%, CCO 53 ± 2%); this still outperforms naïve sequence-based transfer. Preliminary results from CAFA4 appear to confirm these findings. Overall, this new concept is likely to change the annotation of proteins, in particular for proteins from smaller families or proteins with intrinsically disordered regions. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Amino Acid; Protein; Amino Acids; Proteins; Amino Acid; Protein; Amino Acid Sequence; Biology; Chemistry; Gene Ontology; Human; Machine Learning; Molecular Genetics; Procedures; Sequence Analysis; Sequence Homology; Software; Amino Acid Sequence; Amino Acids; Computational Biology; Deep Learning; Gene Ontology; Humans; Machine Learning; Molecular Sequence Annotation; Proteins; Sequence Analysis, Protein; Sequence Homology, Amino Acid; Software},
	keywords = {amino acid; protein; amino acid sequence; biology; chemistry; gene ontology; human; machine learning; molecular genetics; procedures; sequence analysis; sequence homology; software; Amino Acid Sequence; Amino Acids; Computational Biology; Deep Learning; Gene Ontology; Humans; Machine Learning; Molecular Sequence Annotation; Proteins; Sequence Analysis, Protein; Sequence Homology, Amino Acid; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 111; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Both2021940,
	author = {Both, Maximilian Alexander and Müller, Jochen S. and Diedrich, Christian},
	title = {Automated mapping of semantically heterogeneous I4.0 asset administration shells by methods of natural language processing; Automatisierte Abbildung semantisch heterogener I4.0-Verwaltungsschalen durch Methoden des Natural Language Processing},
	year = {2021},
	journal = {At-Automatisierungstechnik},
	volume = {69},
	number = {11},
	pages = {940 - 951},
	doi = {10.1515/auto-2021-0050},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119898872&doi=10.1515%2Fauto-2021-0050&partnerID=40&md5=aeb1bc89b43be784ac5c93710c183763},
	abstract = {Systems in the field of Industrie 4.0 should be able to interact with each other in an interoperable manner. In order for this to be realized automatically, they must be semantically interoperable. For this purpose, the current Industrie 4.0 research approach to the interaction of systems focuses on a semantically homogeneous language space. In this paper, we present a method that extends the current approach to include heterogeneous semantics. Mapping unknown vocabularies to a target ontology enables the interactions of semantically heterogeneous Industrie 4.0 asset administration shells. The mapping is based on methods from the field of Natural Language Processing. For this purpose, language models pre-trained on ISO standards and sentence embeddings are combined. This leads to a promising accuracy in the created evaluation dataset, which contains different semantics for identification and design submodels of the Pumpe 4.0 project. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Asset Administration Shell; Industrie 4.0; Natrual Language Processing; Semantic Interoperability},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Kuwana2021,
	author = {Kuwana, Ayato and Oba, Atsushi and Sawai, Ranto and Paik, Incheon},
	title = {Automatic taxonomy classification by pretrained language model},
	year = {2021},
	journal = {Electronics (Switzerland)},
	volume = {10},
	number = {21},
	pages = {},
	doi = {10.3390/electronics10212656},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118191099&doi=10.3390%2Felectronics10212656&partnerID=40&md5=f1be38ec305cb1a32392fad19275bccd},
	abstract = {In recent years, automatic ontology generation has received significant attention in information science as a means of systemizing vast amounts of online data. As our initial attempt of ontology generation with a neural network, we proposed a recurrent neural network-based method. However, updating the architecture is possible because of the development in natural language processing (NLP). By contrast, the transfer learning of language models trained by a large, unlabeled corpus has yielded a breakthrough in NLP. Inspired by these achievements, we propose a novel workflow for ontology generation comprising two-stage learning. Our results showed that our best method improved accuracy by over 12.5%. As an application example, we applied our model to the Stanford Question Answering Dataset to show ontology generation in a real field. The results showed that our model can generate a good ontology, with some exceptions in the real field, indicating future research directions to improve the quality. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Automation; Natural Language Processing (nlp); Ontology; Pretrained Model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Gold Open Access}
}

@CONFERENCE{Hagen2021,
	author = {Hagen, Morten and Arora, Piyush and Ghosh, Rahul and Thomas, Dawn and Joshi, Salil Rajeev},
	title = {Class-Based Order-Independent Models of Natural Language for Bayesian Auto-Complete Inference},
	year = {2021},
	journal = {ACM International Conference Proceeding Series},
	pages = {},
	doi = {10.1145/3486001.3486240},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118311810&doi=10.1145%2F3486001.3486240&partnerID=40&md5=8353ee86553d55e437d6216fe420e442},
	abstract = {We introduce a model for auto-complete of general queries via Bayesian inference. To that end, we address three issues: First, the problem of predicting a word given previous words in a text. Usually, the context words are treated as a directional sequence. In our approach, we introduce a set-based class language model with order-independence, modeling the context words as a set of classes. Second, towards the task of predicting the next word's class based on the classes of previous words plus an incomplete word prefix, we present a Bayesian framework that incorporates the set-based class language model in conjunction with an ontology. Third, regarding the auto-complete problem, we provide complete query suggestions via abstract class-space search which determines similar historical queries that contain the classes of previous words plus the next word's predicted class. Subsequently, we apply the model to auto-complete inference in a system setting, in which users can access data via natural language queries. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Auto-complete; Bayesian Inference; Class-based Language Model; Order-independence; Abstracting; Bayesian Networks; Computational Linguistics; Modeling Languages; Natural Language Processing Systems; Auto-complete; Bayesian Inference; Class-based; Class-based Language Model; Context-word; Independent Model; Language Model; Natural Languages; Order Independents; Order-independence; Inference Engines},
	keywords = {Abstracting; Bayesian networks; Computational linguistics; Modeling languages; Natural language processing systems; Auto-complete; Bayesian inference; Class-based; Class-based language model; Context-word; Independent model; Language model; Natural languages; Order independents; Order-independence; Inference engines},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Koutsomitropoulos202137,
	author = {Koutsomitropoulos, Dimitrios A.},
	title = {Validating Ontology-based Annotations of Biomedical Resources using Zero-shot Learning},
	year = {2021},
	pages = {37 - 43},
	doi = {10.1145/3486713.3486730},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153276770&doi=10.1145%2F3486713.3486730&partnerID=40&md5=060ed14908f0050ad88b2f9343cad592},
	abstract = {Authoritative thesauri in the form of web ontologies offer a sound representation of domain knowledge and can act as a reference point for automated semantic tagging. On the other hand, current language models achieve to capture contextualized semantics of text corpora and can be leveraged towards this goal. We present an approach for injecting subject annotations using query term expansion against such ontologies in the biomedical domain. For the user to have an indication of the usefulness of these suggestions we further propose an online method for validating the quality of annotations using NLI models such as BART and XLM-R. To circumvent training barriers posed by very large label sets and scarcity of data we rely on zero-shot classification and show that semantic matching can contribute above-average thematic annotations. Also, a web-based validation service can be attractive for human curators vs. the overhead of pretraining large, domain-tailored classification models. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Biomedical Indexing; Classification; Language Models; Machine Learning; Mesh; Semantic Matching; Thesaurus; Classification (of Information); Computational Linguistics; Domain Knowledge; Ontology; Semantics; Zero-shot Learning; Biomedical Indexing; Biomedical Resource; Domain Knowledge; Language Model; Machine-learning; Mesh; Ontology-based Annotation; Reference Points; Semantic Matching; Web Ontology; Thesauri},
	keywords = {Classification (of information); Computational linguistics; Domain Knowledge; Ontology; Semantics; Zero-shot learning; Biomedical indexing; Biomedical resource; Domain knowledge; Language model; Machine-learning; MeSH; Ontology-based annotation; Reference points; Semantic matching; Web ontology; Thesauri},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Razouk2021113,
	author = {Razouk, Houssam and Kern, Roman and Mischitz, Martin and Moser, Josef and Memic, Mirhad and Liu, Lan and Burmer, Christian and Safont-Andreu, Anna},
	title = {AI-based knowledge management system for risk assessment and root cause analysis in semiconductor industry},
	year = {2021},
	pages = {113 - 129},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184845907&partnerID=40&md5=86e37c4dd77cf4422984f248fe078272},
	abstract = {Due to the increasing technical complexity of products and market pressure, the demands in the semiconductor industry are rising with respect to quality, performance, and time to market. Root cause analysis and risk assessment are crucial elements for success in fulfilling these demands. As a result, there is an ever-growing number of technical documents, which potentially contain valuable information serving as a base to inform development and production. Experts need to cope with this large number of technical documents, for example, to generate new hypotheses to identify possible root causes of deviations or potential risks in the ramp-up and production phase of new products. Unfortunately, most of the technical documents are unstructured, making processing them even more tedious. New advances in computer science, specifically artificial intelligence (AI), open the door for a higher degree of automation of knowledge management tools to support experts. Knowledge bases such as knowledge graphs allow for representing complex information but need to be created for each domain. Novel state-of-the-art graph embedding algorithms showed promising results in complementing knowledge bases with new relations. Complementary to knowledge base completion, language models trained on large textual corpora have demonstrated their ability to capture complex semantics. This paper proposes a new expert system concept for failure root cause analysis and risk assessment in the semiconductor industry, which leverages the advanced graph embeddings in combination with language models. The main challenges in this setting are the type of relations of interest, which are causal, and the language being used, which is highly domain-specific. Thus, we devised AI for consistency improvement of the data, predicting new links, and information extraction from unstructured data. The information extraction is conducted by levaraging domain specific ontologies and by focusing on presence of causal language. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Consistency Improvement; Convolutional Neural Network; Expert System; Information Extraction; Knowledge Graph; Knowledge Representation; Link Prediction; Machine Learning; Natural Language Processing; Recurrent Neural Network; Root Cause Analysis And Risk Assessment; Semiconductor Industry; Text Classification},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Russell-Rose2021127,
	author = {Russell-Rose, Tony and Gooch, Phil and Kruschwitz, Udo},
	title = {Interactive query expansion for professional search applications},
	year = {2021},
	journal = {Business Information Review},
	volume = {38},
	number = {3},
	pages = {127 - 137},
	doi = {10.1177/02663821211034079},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111158744&doi=10.1177%2F02663821211034079&partnerID=40&md5=416a8dcafd4db29d033e72a7e3b53936},
	abstract = {Knowledge workers (such as healthcare information professionals, patent agents and recruitment professionals) undertake work tasks where search forms a core part of their duties. In these instances, the search task is often complex and time-consuming and requires specialist expert knowledge to formulate accurate search strategies. Interactive features such as query expansion can play a key role in supporting these tasks. However, generating query suggestions within a professional search context requires that consideration be given to the specialist, structured nature of the search strategies they employ. In this paper, we investigate a variety of query expansion methods applied to a collection of Boolean search strategies used in a variety of real-world professional search tasks. The results demonstrate the utility of context-free distributional language models and the value of using linguistic cues to optimise the balance between precision and recall. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Information Retrieval; Machine Learning; Natural Language Processing; Ontologies; Professional Search; Query Expansion},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{He2021267,
	author = {He, Keqing and Yan, Yuanmeng and Xu, Weiran},
	title = {From context-aware to knowledge-aware: Boosting OOV tokens recognition in slot tagging with background knowledge},
	year = {2021},
	journal = {Neurocomputing},
	volume = {445},
	pages = {267 - 275},
	doi = {10.1016/j.neucom.2021.01.134},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103687677&doi=10.1016%2Fj.neucom.2021.01.134&partnerID=40&md5=b2df1bb4946e5b20a0b74f55c16cd1c7},
	abstract = {Neural-based context-aware models for slot tagging tasks in language understanding have achieved state-of-the-art performance, especially deep contextualized models, such as ELMo, BERT. However, the presence of out-of-vocab (OOV) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this paper, we propose a novel knowledge-aware slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. Besides, we use multi-level graph attention to explicitly reason via lexical relations. We aim to leverage both linguistic regularities covered by deep language models (LM) and high-quality background knowledge derived from curated knowledge bases (KB). Consequently, our model could infer rare and unseen words in the test dataset by incorporating contextual semantics learned from the training dataset and lexical relations from ontology. The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets. We also show through detailed analysis that incorporating background knowledge effectively alleviates issues of data scarcity. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Background Knowledge; Contextual Representation; Knowledge Integration; Multi-level Graph Attention; Slot Tagging; Statistical Tests; Background Knowledge; Context-aware; Context-aware Models; Contextual Representation; Knowledge Integration; Language Understanding; Lexical Relations; Multi-level Graph Attention; Slot Tagging; State-of-the-art Performance; Semantics; Article; Attention; Human; Human Experiment; Knowledge Base; Language; Ontology; Semantics},
	keywords = {Statistical tests; Background knowledge; Context-Aware; Context-aware models; Contextual representation; Knowledge integration; Language understanding; Lexical relations; Multi-level graph attention; Slot tagging; State-of-the-art performance; Semantics; article; attention; human; human experiment; knowledge base; language; ontology; semantics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8}
}

@CONFERENCE{Choudhary2021,
	author = {Choudhary, Rishabh and Doboli, Simona and Minai, Ali A.},
	title = {A Comparative Study of Methods for Visualizable Semantic Embedding of Small Text Corpora},
	year = {2021},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	volume = {2021-July},
	pages = {},
	doi = {10.1109/IJCNN52387.2021.9534250},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116441915&doi=10.1109%2FIJCNN52387.2021.9534250&partnerID=40&md5=076e82432db0576c5c7c6b98d7138f7b},
	abstract = {Text embedding has recently emerged as a very useful and successful method for semantic representation. Following initial word-level embedding methods such as Latent Semantic Analysis (LSA) and topic-based bag-of-words approaches like Latent Dirichlet Allocation (LDA), the focus has turned to language models and text encoders implemented as neural networks - ranging from word-level models to those embedding whole documents. The distinctive feature of these models is their ability to infer semantic spaces at all levels based purely on data, with no need for complexities such as syntactic analysis or ontology building. Many of these models are available pre-trained on enormous amounts of data, providing downstream applications with general-purpose semantic spaces. In particular, embedding models at the sentence level or higher are most useful in applications because the meaning of text only becomes clear at that level. Most text embedding methods produce text embeddings in high-dimensional spaces, with a dimensionality ranging from a few hundred to thousands. However, it is often useful to visualize semantic spaces in very low dimension, which requires the use of dimensionality reduction methods. It is not clear what language models and what method of dimensionality reduction would work well in these cases. In this paper, we compare four text embedding methods in combination with three methods of dimensionality reduction to map three related real-world datasets comprising textual descriptions of items in a particular domain (sports) to a 2-dimensional semantic visualization space. The results provide several insights into the utility of these methods for data of this type. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Language Models; Semantic Spaces; Semantic Visualization; Text Embedding; Computational Linguistics; Information Retrieval; Natural Language Processing Systems; Semantics; Statistics; Syntactics; Visualization; Comparatives Studies; Dimensionality Reduction; Embedding Method; Embeddings; Language Model; Semantic Embedding; Semantic Space; Semantic Visualization; Text Embedding; Word Level},
	keywords = {Computational linguistics; Information retrieval; Natural language processing systems; Semantics; Statistics; Syntactics; Visualization; Comparatives studies; Dimensionality reduction; Embedding method; Embeddings; Language model; Semantic embedding; Semantic Space; Semantic visualization; Text embedding; Word level},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@CONFERENCE{Neji20211717,
	author = {Neji, Sameh and Chenaina, Tarek and Shoeb, Abdullah M. and Ayed, Leila Jemni Ben},
	title = {HIR: A hybrid IR ranking model},
	year = {2021},
	pages = {1717 - 1722},
	doi = {10.1109/COMPSAC51774.2021.00256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115711808&doi=10.1109%2FCOMPSAC51774.2021.00256&partnerID=40&md5=394c57fba9408bd0aaaff2e781f17ef6},
	abstract = {The aim of information retrieval (IR) process is to respond effectively to user queries by retrieving information that are better meets their expectations. A gap could exist between user information needs and his/her defined context as the level of the user's expertise in the search domain is directly influencing the query richness. The information retrieval system (IRS) must be intelligent enough to identify information needs and respond effectively to meet the expected needs, regardless the level of user's expertise level. This process is difficult and it remains a major and open challenge in the domain of IR. IR models integrate many sources to achieve an effective retrieval system. Semantic IR is an environment in which semantic techniques were applied to sort the documents according to their degree of relevance to the query. The present work proposes a hybrid model to rank documents. The proposed model is based on a query likelihood language model and the semantic similarity between concepts to assess the relevance between query-document pairs. Concepts were extracted by projection on WordNet ontology then word sense disambiguation was conducted. A semantic index was built to validate the proposed model. The conducted empirical experiments show that the proposed model is outperformed the compared benchmarks in the measured IR metrics. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Conceptual Model; Language Model; Query-document Relevance; Semantic Information Retrieval; Semantic Similarity; Application Programs; Information Retrieval; Learning To Rank; Natural Language Processing Systems; Ontology; Semantics; Degree Of Relevance; Empirical Experiments; Query Documents; Retrieval Systems; Semantic Similarity; Semantic Techniques; User Information Need; Word Sense Disambiguation; Search Engines},
	keywords = {Application programs; Information retrieval; Learning to rank; Natural language processing systems; Ontology; Semantics; Degree of relevance; Empirical experiments; Query documents; Retrieval systems; Semantic similarity; Semantic techniques; User information need; Word Sense Disambiguation; Search engines},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Shannon202186,
	author = {Shannon, George J. and Rayapati, Nagasharath and Corns, Steven Michael and Wunsch, Donald C.},
	title = {Comparative study using inverse ontology cogency and alternatives for concept recognition in the annotated National Library of Medicine database},
	year = {2021},
	journal = {Neural Networks},
	volume = {139},
	pages = {86 - 104},
	doi = {10.1016/j.neunet.2021.01.018},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102081027&doi=10.1016%2Fj.neunet.2021.01.018&partnerID=40&md5=b89d446e813eb093fdb06e0aff6108d5},
	abstract = {This paper introduces inverse ontology cogency, a concept recognition process and distance function that is biologically-inspired and competitive with alternative methods. The paper introduces inverse ontology cogency as a new alternative method. It is a novel distance measure used in selecting the optimum mapping between ontology-specified concepts and phrases in free-form text. We also apply a multi-layer perceptron and text processing method for named entity recognition as an alternative to recurrent neural network methods. Automated named entity recognition, or concept recognition, is a common task in natural language processing. Similarities between confabulation theory and existing language models are discussed. This paper provides comparisons to MetaMap from the National Library of Medicine (NLM), a popular tool used in medicine to map free-form text to concepts in a medical ontology. The NLM provides a manually annotated database from the medical literature with concepts labeled, a unique, valuable source of ground truth, permitting comparison with MetaMap performance. Comparisons for different feature set combinations are made to demonstrate the effectiveness of inverse ontology cogency for entity recognition. Results indicate that using both inverse ontology cogency and corpora cogency improved concept recognition precision 20% over the best published MetaMap results. This demonstrates a new, effective approach for identifying medical concepts in text. This is the first time cogency has been explicitly invoked for reasoning with ontologies, and the first time it has been used on medical literature where high-quality ground truth is available for quality assessment. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Cogent Confabulation; Concept Recognition; Language Model; Natural Language Processing; Character Recognition; Computational Linguistics; Inverse Problems; Ontology; Processing; Recurrent Neural Networks; Text Processing; Cogent Confabulation; Concept Recognition; Freeforms; Language Modeling; Language Processing; Medical Literatures; Named Entity Recognition; National Library Of Medicines; Natural Languages; Ontology's; Natural Language Processing Systems; Article; Comparative Study; Intermethod Comparison; Inverse Ontology Cogency; Mathematical Model; Measurement Precision; Medical Ontology; Multilayer Perceptron; Natural Language Processing; Probabilistic Neural Network; Recurrent Neural Network; Word Processing; Biological Ontology; Brain Cortex; Factual Database; Human; Library; Nerve Cell Network; Physiology; United States; Biological Ontologies; Cerebral Cortex; Databases, Factual; Humans; National Library Of Medicine (u.s.); Natural Language Processing; Nerve Net; Neural Networks, Computer},
	keywords = {Character recognition; Computational linguistics; Inverse problems; Ontology; Processing; Recurrent neural networks; Text processing; Cogent confabulation; Concept recognition; Freeforms; Language modeling; Language processing; Medical literatures; Named entity recognition; National library of medicines; Natural languages; Ontology's; Natural language processing systems; Article; comparative study; intermethod comparison; inverse ontology cogency; mathematical model; measurement precision; medical ontology; multilayer perceptron; natural language processing; probabilistic neural network; recurrent neural network; word processing; biological ontology; brain cortex; factual database; human; library; nerve cell network; physiology; United States; Biological Ontologies; Cerebral Cortex; Databases, Factual; Humans; National Library of Medicine (U.S.); Natural Language Processing; Nerve Net; Neural Networks, Computer},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Bronze Open Access}
}

@ARTICLE{Paley2021119,
	author = {Paley, Andrew and Li Zhao, Andong Luis Li and Pack, Harper and Servantez, Sergio and Adler, Rachel F. and Sterbentz, Marko and Pah, Adam R. and Schwartz, David L. and Barrie, Cameron and Einarsson, Alexander},
	title = {From data to information: Automating data science to explore the U.S. court system},
	year = {2021},
	pages = {119 - 128},
	doi = {10.1145/3462757.3466100},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112351799&doi=10.1145%2F3462757.3466100&partnerID=40&md5=93fcf3bc0c87509d5a54c2d0c3116e34},
	abstract = {The U.S. court system is the nation's arbiter of justice, tasked with the responsibility of ensuring equal protection under the law. But hurdles to information access obscure the inner workings of the system, preventing stakeholders - from legal scholars to journalists and members of the public - from understanding the state of justice in America at scale. There is an ongoing data access argument here: U.S. court records are public data and should be freely available. But open data arguments represent a half-measure; what we really need is open information. This distinction marks the difference between downloading a zip file containing a quarter-million case dockets and getting the real-time answer to a question like "Are pro se parties more or less likely to receive fee waivers?"To help bridge that gap, we introduce a novel platform and user experience that provides users with the tools necessary to explore data and drive analysis via natural language statements. Our approach leverages an ontology configuration that adds domain-relevant data semantics to database schemas to provide support for user guidance and for search and analysis without user-entered code or SQL. The system is embodied in a "natural-language notebook"user experience, and we apply this approach to the space of case docket data from the U.S. federal court system. Additionally, we provide detail on the collection, ingestion and processing of the dockets themselves, including early experiments in the use of language modeling for docket entry classification with an initial focus on motions. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Analytics; Information Extraction; Natural Language Processing; Notebook Interface; Visualization; Air Navigation; Artificial Intelligence; Data Science; Digital Storage; Modeling Languages; Natural Language Processing Systems; Semantics; User Experience; Data Semantics; Database Schemas; Drive Analysis; Federal Court System; Information Access; Language Model; Legal Scholars; Natural Languages; Open Data},
	keywords = {Air navigation; Artificial intelligence; Data Science; Digital storage; Modeling languages; Natural language processing systems; Semantics; User experience; Data semantics; Database schemas; Drive analysis; Federal court system; Information access; Language model; Legal scholars; Natural languages; Open Data},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@CONFERENCE{Nguyen2021302,
	author = {Nguyen, Nhu Khoa and Boros, Emanuela and Lejeune, Gaël and Doucet, Antoine and Delahaut, Thierry},
	title = {L3i_LBPAM at the FinSim-2 task: Learning Financial Semantic Similarities with Siamese Transformers},
	year = {2021},
	number = {03-06-21},
	pages = {302 - 306},
	doi = {10.1145/3442442.3451384},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107616016&doi=10.1145%2F3442442.3451384&partnerID=40&md5=db0a52085ace00498e4906fdb5d9e2d2},
	abstract = {In this paper, we present the different methods proposed for the FinSIM-2 Shared Task 2021 on Learning Semantic Similarities for the Financial domain. The main focus of this task is to evaluate the classification of financial terms into corresponding top-level concepts (also known as hypernyms) that were extracted from an external ontology. We approached the task as a semantic textual similarity problem. By relying on a siamese network with pre-Trained language model encoders, we derived semantically meaningful term embeddings and computed similarity scores between them in a ranked manner. Additionally, we exhibit the results of different baselines in which the task is tackled as a multi-class classification problem. The proposed methods outperformed our baselines and proved the robustness of the models based on textual similarity siamese network. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Hypernym Detection; Semantic Similarities; Siamese Networks; Finance; Semantic Web; Financial Domains; Hypernym Detection; Hypernyms; Language Model; Learning Semantics; Ontology's; Semantic Similarity; Siamese Network; Task Learning; Textual Similarities; Semantics},
	keywords = {Finance; Semantic Web; Financial domains; Hypernym detection; Hypernyms; Language model; Learning semantics; Ontology's; Semantic similarity; Siamese network; Task learning; Textual similarities; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Final Open Access; Green Open Access}
}

@ARTICLE{Shuang2021181,
	author = {Shuang, Kai and Gu, Mengyu and Li, Rui and Loo, Jonathan Kok Keng and Su, Sen},
	title = {Interactive POS-aware network for aspect-level sentiment classification},
	year = {2021},
	journal = {Neurocomputing},
	volume = {420},
	pages = {181 - 196},
	doi = {10.1016/j.neucom.2020.08.013},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092335288&doi=10.1016%2Fj.neucom.2020.08.013&partnerID=40&md5=bc143a454ff5a2d85e626030e1b48a29},
	abstract = {Existing aspect-level sentiment-classification models completely rely on the learning from given datasets. However, these are easily misled by biased samples, resulting in learning some ill-suited rules that limit their potential. The information of some specific part-of-speech (POS) categories often indicates the word sentiment polarity, which can be introduced as prior knowledge to facilitate prediction of the model. Accordingly, we propose an interactive POS-aware network (IPAN) that explicitly introduces the POS information as reliable guidance to assist the model in accurately predicting sentiment polarity. We distinguish the information of different POS categories using a POS-filter gate and reinforce the features extracted from adjectives, adverbs, and verbs via a POS-highlighting attention mechanism. This enables the model to concentrate on the words that contain significant sentiment orientations and to obtain the most practical learning experience. To emphasize the target information, we construct a target-context gate that enables the interaction of the target information with contexts; consequently, the model considerably focuses on target-related sentiment features. The experiments on SemEval2014 and Twitter datasets verify that our IPAN consistently outperforms the current state-of-the-art methods. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Aspect-level Sentiment Classification; Attention Mechanism; Gating Mechanism; Part-of-speech; Conceptnet; Emosenticnet; Ontosenticnet; Opinionfinder; Python; Senticnet; Sentiwordnet; Wordnet; Learning Systems; Predictive Analytics; Attention Mechanisms; Learning Experiences; Part Of Speech; Prior Knowledge; Sentiment Classification; Sentiment Features; State-of-the-art Methods; Target Information; Classification (of Information); Accuracy; Affect; Affective Computing; Article; Aspect Level Sentiment Analysis; Aspect Level Sentiment Classification; Association; Attention; Bayesian Learning; Bidirectional Long Short Term Memory; Big Data; Book; Classification; Classification Algorithm; Computer Model; Convolutional Neural Network; Data Analysis; Decision Making; Decision Tree; Deep Learning; Embedding Layer; Emotional Intelligence; Emotional Orientation; Emotionality; Fuzzy Clustering; Gated Linear Unit; Gated Recurrent Unit; Gated Relu Unit; Gated Tanh Relu Unit; Grammar; Human; Interactive Part Of Speech Aware Network; Knowledge; Language Development; Language Network; Logistic Regression Analysis; Multi Grained Attention Network; Natural Language Processing; Online Social Network; Ontology; Orientation; Part Of Speech Filter Gate Mechanism; Part Of Speech Highlighting Attention Mechanism; Pre Trained Language Model; Prediction; Priority Journal; Psychologic Assessment; Psychological Model; Recurrent Neural Network; Reliability; Sarcasm Detection; Semantic Feature Extraction Layer; Semantic Orientation; Semantics; Sentiment Expression; Sentiment Lexicon; Social Data Analysis; Software; Statistical Analysis; Statistical Bias; Statistical Model; Statistical Parameters; Support Vector Machine; Target Context Gating Mechanism; Task Performance; Thinking; Transfer Capsule Network; Trend Study; Validity; Word Emotion Association; Word Processing; Word Sense Disambiguation; Word Sentiment Polarity},
	keywords = {Learning systems; Predictive analytics; Attention mechanisms; Learning experiences; Part Of Speech; Prior knowledge; Sentiment classification; Sentiment features; State-of-the-art methods; Target information; Classification (of information); accuracy; affect; affective computing; Article; aspect level sentiment analysis; aspect level sentiment classification; association; attention; Bayesian learning; bidirectional long short term memory; big data; book; classification; classification algorithm; computer model; convolutional neural network; data analysis; decision making; decision tree; deep learning; embedding layer; emotional intelligence; emotional orientation; emotionality; fuzzy clustering; gated linear unit; gated recurrent unit; gated ReLU unit; gated Tanh Relu unit; grammar; human; interactive part of speech aware network; knowledge; language development; language network; logistic regression analysis; multi grained attention network; natural language processing; online social network; ontology; orientation; part of speech filter gate mechanism; part of speech highlighting attention mechanism; pre trained language model; prediction; priority journal; psychologic assessment; psychological model; recurrent neural network; reliability; sarcasm detection; semantic feature extraction layer; semantic orientation; semantics; sentiment expression; sentiment lexicon; social data analysis; software; statistical analysis; statistical bias; statistical model; statistical parameters; support vector machine; target context gating mechanism; task performance; thinking; transfer capsule network; trend study; validity; word emotion association; word processing; word sense disambiguation; word sentiment polarity},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 30; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Kleinert2021,
	author = {Kleinert, Matthias and Venkatarathinam, Narasimman and Helmke, Hartmut and Ohneiser, Oliver and Strake, Maximilian and Fingscheidt, Tim},
	title = {Easy Adaptation of Speech Recognition to Different Air Traffic Control Environments using the DeepSpeech Engine},
	year = {2021},
	journal = {SESAR Innovation Days},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160763592&partnerID=40&md5=749537e5c83156feb7527ebd2fa34e62},
	abstract = {Nowadays, recognizing and understanding human speech is quite popular through systems like Alexa®, the Google Assistant or Siri®. Speech also plays a major role in air traffic control (ATC) as voice communication between air traffic controllers (ATCos) and pilots is essential for ensuring safe and efficient air traffic. This communication is still analogue and ATCos are forced to enter the same communication content again into digital systems with additional input devices. Automatic speech recognition (ASR) is a solution to automate this digitization process and an important necessity in optimizing ATCo workflow. This paper investigates the applicability of DeepSpeech, an open source, easy to adapt, end-to-end speech recognition engine from the Mozilla Corporation, as a speech-to-text solution for ATC speech. Different training approaches such as training a model from scratch and adapting a model pre-trained on non-ATC speech are explored. Model adaptation is performed by employing techniques such as fine-tuning, transfer learning, and layer freezing. Furthermore, the effect of employing an additional language model in conjunction with the end-to-end trained model is evaluated and shown to lead to a considerable relative improvement of 61% in word error rate. Overall, a word error rate of 6.0% is achieved on voice recordings from operational and simulation environment of different airspaces, resulting in command recognition rates between 85% and 97%. The achieved results show that DeepSpeech is a highly relevant solution for ATC-speech, especially when considering that it includes easy to use adaptation mechanisms also for non-experts in speech recognition. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Air Traffic Control; Asr; Atc; Automatic Speech Recognition; Deepspeech; Domain Adaptation; Ontology; Air Navigation; Character Recognition; Control Towers; Digital Devices; Engines; Information Systems; Information Use; Learning Systems; Speech Recognition; Air Traffic Controller; Automatic Speech Recognition; Control Environment; Deepspeech; Domain Adaptation; End To End; Human Speech; Ontology's; Word Error Rate; Air Traffic Control},
	keywords = {Air navigation; Character recognition; Control towers; Digital devices; Engines; Information systems; Information use; Learning systems; Speech recognition; Air traffic controller; Automatic speech recognition; Control environment; Deepspeech; Domain adaptation; End to end; Human speech; Ontology's; Word error rate; Air traffic control},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Yu2021,
	author = {Yu, Tao and Zhang, Rui and Polozov, Oleksandr and Meek, Christopher A. and Awadallah, Ahmed Hassan},
	title = {SCORE: PRE-TRAINING FOR CONTEXT REPRESENTATION IN CONVERSATIONAL SEMANTIC PARSING},
	year = {2021},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145975337&partnerID=40&md5=9948ac088c1850f8e3fc9f8d7fa064ad},
	abstract = {Conversational Semantic Parsing (CSP) is the task of converting a sequence of natural language queries to formal language (e.g., SQL, SPARQL) that can be executed against a structured ontology (e.g. databases, knowledge bases). To accomplish this task, a CSP system needs to model the relation between the unstructured language utterance and the structured ontology while representing the multi-turn dynamics of the dialog. Pre-trained language models (LMs) are the state-of-the-art for various natural language processing tasks. However, existing pre-trained LMs that use language modeling training objectives over free-form text have limited ability to represent natural language references to contextual structural data. In this work, we present SCORE, a new pre-training approach for CSP tasks designed to induce representations that capture the alignment between the dialogue flow and the structural context. We demonstrate the broad applicability of SCORE to CSP tasks by combining SCORE with strong base systems on four different tasks (SPARC, COSQL, MWOZ, and SQA). We show that SCORE can improve the performance over all these base systems by a significant margin and achieves state-of-the-art results on three of them. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Modeling Languages; Natural Language Processing Systems; Ontology; Semantics; Base Systems; Context Representation; Conversational Semantics; Language Model; Natural Language Queries; Natural Languages; Ontology's; Pre-training; Semantic Parsing; State Of The Art; Formal Languages},
	keywords = {Computational linguistics; Modeling languages; Natural language processing systems; Ontology; Semantics; Base systems; Context representation; Conversational semantics; Language model; Natural language queries; Natural languages; Ontology's; Pre-training; Semantic parsing; State of the art; Formal languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 42}
}

@CONFERENCE{Liu20214228,
	author = {Liu, Fangyu and Shareghi, Ehsan and Meng, Zaiqiao and Basaldella, Marco and Collier, Nigel H.},
	title = {Self-Alignment Pretraining for Biomedical Entity Representations},
	year = {2021},
	pages = {4228 - 4238},
	doi = {10.18653/v1/2021.naacl-main.334},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137694463&doi=10.18653%2Fv1%2F2021.naacl-main.334&partnerID=40&md5=4f7af8cdb84fd0b6320b990c42bf4f1d},
	abstract = {Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SAPBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SAPBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BIOBERT, SCIBERT and PUBMEDBERT, our pretraining scheme proves to be both effective and robust. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Semantics; Biomedical Domain; Entity-level; Fine Grained; Language Model; Model Entities; Pre-training; Self-align; Self-alignment; Semantic Relationships; State Of The Art; Hybrid Systems},
	keywords = {Computational linguistics; Semantics; Biomedical domain; Entity-level; Fine grained; Language model; Model entities; Pre-training; Self-align; Self-alignment; Semantic relationships; State of the art; Hybrid systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 206; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Zhang2021466,
	author = {Zhang, Ruichuan and El-Gohary, Nora M.},
	title = {Semantic Representation Learning and Information Integration of BIM and Regulations},
	year = {2021},
	pages = {466 - 473},
	doi = {10.1061/9780784483893.058},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132574650&doi=10.1061%2F9780784483893.058&partnerID=40&md5=59d0014d5794c71f69ad5c998c38d269},
	abstract = {Automated checking of the compliance of building information modeling (BIM)-based building designs with relevant codes and regulations requires bridging the semantic gap between the Industry Foundation Classes (IFC) schema and the natural language. In most of the existing automated compliance checking (ACC) systems, the integration of the IFC schema and natural language is realized through hardcoding or predefined rules, ontologies, or dictionaries. These methods require intensive manual engineering effort and are often rigid and difficult to generalize. There is, thus, a need for an automated and meanwhile flexible and generalizable information integration method. To address this need, this paper leverages transformer-based language models to learn the semantic representations of concepts in the building information models (BIMs) and regulatory documents. An automated IFC-regulatory information integration approach based on these learned semantic representations is proposed. The preliminary experimental results show that the proposed approach achieved promising performance-an accuracy of 80%-on integrating IFC and regulatory concepts. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Automation; Compliance Control; Information Retrieval; Information Theory; Modeling Languages; Semantics; Structural Design; Automated Compliance Checking; Building Design; Building Information Modelling; Hardcoding; Information Integration; Model-based Opc; Natural Languages; Schema Language; Semantic Gap; Semantic Representation; Architectural Design},
	keywords = {Automation; Compliance control; Information retrieval; Information theory; Modeling languages; Semantics; Structural design; Automated compliance checking; Building design; Building Information Modelling; Hardcoding; Information integration; Model-based OPC; Natural languages; Schema language; Semantic gap; Semantic representation; Architectural design},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{Huang2021360,
	author = {Huang, Tao and Liang, Mengyi and Yang, Huali and Li, Zhi and Yu, Tao and Hu, Shengze},
	title = {Context-aware Knowledge Tracing Integrated with The Exercise Representation and Association in Mathematics},
	year = {2021},
	pages = {360 - 366},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128945679&partnerID=40&md5=28d4796ab16ba2ccac385b1c790c1d20},
	abstract = {Influenced by Covid-19, online learning has become one of the most important forms of education in the world. In the era of intelligent education, knowledge tracing(KT) can provide excellent technical support for individualized teaching. For online learning, we come up with a new knowledge tracing method that integrates mathematical exercise representation and association of exercise(ERAKT). In the aspect of exercise representation, we represent the multi-dimensional features of the exercises, such as formula, text and associated concept, by using ontology replacement method, language model and embedding technology, so we can obtain the unified internal representation of exercise. Besides, we utilize the bidirectional long short memory neural network to acquire the association between exercises, so as to predict his performance in future exercise. Extensive experiments on a real dataset clearly proved the effectiveness of ERAKT method, they also verified that adding multi-dimensional features and exercise association can indeed improve the accuracy of prediction. © 2023 Elsevier B.V., All rights reserved.},
	author_keywords = {Context-aware; Exercise Representation; Knowledge Tracing; Context-aware; Exercise Representation; Intelligent Educations; Knowledge Tracings; Multi Dimensional; Online Learning; Ontology's; Replacement Methods; Technical Support; Tracing Method},
	keywords = {Context-Aware; Exercise representation; Intelligent educations; Knowledge tracings; Multi dimensional; Online learning; Ontology's; Replacement methods; Technical support; Tracing method},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12}
}

@ARTICLE{Moramarco2021881,
	author = {Moramarco, Francesco and Jurić, Damir and Savkov, Aleksandar and Flann, Jack and Lehl, Maria and Boda, Kristian and Grafen, Tessa and Zhelezniak, Vitalii and Gohil, Sunir and Korfiatis, Alex Papadopoulos},
	title = {Towards more patient friendly clinical notes through language models and ontologies},
	year = {2021},
	journal = {AMIA Annual Symposium proceedings},
	volume = {2021},
	pages = {881 - 890},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126899303&partnerID=40&md5=59940d80d0f0030018ed7e72c26c432a},
	abstract = {Clinical notes are an efficient way to record patient information but are notoriously hard to decipher for non-experts. Automatically simplifying medical text can empower patients with valuable information about their health, while saving clinicians time. We present a novel approach to automated simplification of medical text based on word frequencies and language modelling, grounded on medical ontologies enriched with layman terms. We release a new dataset of pairs of publicly available medical sentences and a version of them simplified by clinicians. Also, we define a novel text simplification metric and evaluation framework, which we use to conduct a large-scale human evaluation of our method against the state of the art. Our method based on a language model trained on medical forum data generates simpler sentences while preserving both grammar and the original meaning, surpassing the current state of the art. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine},
	author_keywords = {Electronic Health Record; Human; Language; Linguistics; Natural Language Processing; Electronic Health Records; Humans; Language; Linguistics; Natural Language Processing},
	keywords = {electronic health record; human; language; linguistics; natural language processing; Electronic Health Records; Humans; Language; Linguistics; Natural Language Processing},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@CONFERENCE{Eggleston20215597,
	author = {Eggleston, Chloe and Abramson, Jeremy},
	title = {Woolery: Extending Frame Semantics to Structured Documents},
	year = {2021},
	pages = {5597 - 5601},
	doi = {10.1109/BigData52589.2021.9671788},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125339948&doi=10.1109%2FBigData52589.2021.9671788&partnerID=40&md5=c4091df453aa66f2878efe5e01e91714},
	abstract = {This paper presents Woolery, a system for semantic annotation and mapping of structured documents (such as JSON key-value pairs) to FrameNet. Implemented as a graphical interface, Woolery provides an annotator with a guided means to map keys in a JSON document to FrameNet elements, without the need for extensive knowledge of FrameNet's semantic structures. Candidate frame elements are identified via a search across FrameNet's internal representations, or via mapping keys to their potential WordNet synsets. Final element selection is automated via a pretrained language model. Initial results are promising, with the model giving an overall accuracy of 77.8% when labeling frames across a diverse corpus of JSON document schemas. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Annotation; Computational Semantics; Framenet; Json; Lexical Databases; Natural Language Processing; Ontology Alignment; Mapping; Natural Language Processing Systems; Ontology; Annotation; Computational Semantics; Frame Semantics; Framenet; Json; Lexical Database; Ontology Alignment; Semantic Annotations; Semantics Mappings; Structured Document; Semantics},
	keywords = {Mapping; Natural language processing systems; Ontology; Annotation; Computational semantics; Frame semantics; FrameNet; JSON; Lexical database; Ontology alignment; Semantic annotations; Semantics mappings; Structured document; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Skorupa Parolin20211449,
	author = {Skorupa Parolin, Erick and Hu, Yibo and Khan, Latifur Rahman and Osorio, Javier and Brandt, Patrick T. and D'Orazio, Vito J.},
	title = {CoMe-KE: A New Transformers Based Approach for Knowledge Extraction in Conflict and Mediation Domain},
	year = {2021},
	pages = {1449 - 1459},
	doi = {10.1109/BigData52589.2021.9672080},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125338805&doi=10.1109%2FBigData52589.2021.9672080&partnerID=40&md5=e4f62b651a33a919e3bbd5fdbfd41b9c},
	abstract = {Knowledge discovery and extraction approaches attract special attention across industries and areas moving toward the 5V Era. In the political and social sciences, scholars and governments dedicate considerable resources to develop intelligent systems for monitoring, analyzing and predicting conflicts and affairs involving political entities across the globe. Such systems rely on background knowledge from external knowledge bases, that conflict experts commonly maintain manually. The high costs and extensive human efforts associated with updating and extending these repositories often compromise their correctness of. Here we introduce CoMe-KE (Conflict and Mediation Knowledge Extractor) to extend automatically knowledge bases about conflict and mediation events. We explore state-of-the-art natural language models to discover new political entities, their roles and status from news. We propose a distant supervised method and propose an innovative zero-shot approach based on a dynamic hypothesis procedure. Our methods leverage pre-trained models through transfer learning techniques to obtain excellent results with no need for a labeled data. Finally, we demonstrate the superiority of our method through a comprehensive set of experiments involving two study cases in the social sciences domain. CoMe-KE significantly outperforms the existing baseline, with (on average) double of the performance retrieving new political entities. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Cameo; Knowledge Base Construction; Knowledge Extraction; Link And Graph Mining; Natural Language Processing; Ontologies; Semantic-based Data Mining; Transfer-learning; Web Search And Mining; Behavioral Research; Extraction; Intelligent Systems; Knowledge Based Systems; Learning Systems; Natural Language Processing Systems; Semantics; Cameo; Graph Mining; Knowledge Extraction; Knowledge-base Construction; Link Mining; Ontology's; Semantic-based Data Mining; Transfer Learning; Web Mining; Web Searches; Data Mining},
	keywords = {Behavioral research; Extraction; Intelligent systems; Knowledge based systems; Learning systems; Natural language processing systems; Semantics; CAMEO; Graph mining; Knowledge extraction; Knowledge-base construction; Link mining; Ontology's; Semantic-based data mining; Transfer learning; Web Mining; Web searches; Data mining},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@CONFERENCE{Chen20214687,
	author = {Chen, Catherine and Lin, Kevin Qinhong and Klein, Dan},
	title = {Constructing Taxonomies from Pretrained Language Models},
	year = {2021},
	pages = {4687 - 4700},
	doi = {10.18653/v1/2021.naacl-main.373},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125028771&doi=10.18653%2Fv1%2F2021.naacl-main.373&partnerID=40&md5=98e416eedc7b0ab3e8fd89bb10a055d8},
	abstract = {We present a method for constructing taxonomic trees (e.g., WORDNET) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the maximum spanning tree of this graph. We train our model on subtrees sampled from WORDNET, and test on non-overlapping WORDNET subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of English WORDNET, the model achieves 66.7 ancestor F<inf>1</inf>, a 20.0% relative increase over the previous best published result on this task. In addition, we convert the original English dataset into nine other languages using OPEN MULTILINGUAL WORDNET and extend our results across these languages. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Ontology; Graph Optimization Problems; Improve Performance; Language Model; Likelihood Score; Maximum Spanning-tree; Sub Trees; Taxonomic Trees; Trees (mathematics)},
	keywords = {Computational linguistics; Ontology; Graph optimization problems; Improve performance; Language model; Likelihood score; Maximum spanning-tree; Sub trees; Taxonomic trees; Trees (mathematics)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 19; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Garcia-Olano20213547,
	author = {Garcia-Olano, Diego and Onoe, Yasumasa and Baldini, Ioana and Ghosh, Joydeep and Wallace, Byron C. and Varshney, Kush R.},
	title = {Biomedical Interpretable Entity Representations},
	year = {2021},
	pages = {3547 - 3561},
	doi = {10.18653/v1/2021.findings-acl.311},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123917151&doi=10.18653%2Fv1%2F2021.findings-acl.311&partnerID=40&md5=f28d61f59e1426aae1b8c1bc081262a8},
	abstract = {Pre-trained language models induce dense entity representations that offer strong performance on entity-centric NLP tasks, but such representations are not immediately interpretable. This can be a barrier to model uptake in important domains such as biomedicine. There has been recent work on general interpretable representation learning (Onoe and Durrett, 2020), but these domain-agnostic representations do not readily transfer to the important domain of biomedicine. In this paper, we create a new entity type system and training set from a large corpus of biomedical texts by mapping entities to concepts in a medical ontology, and from these to Wikipedia pages whose categories are our types. From this mapping we derive Biomedical Interpretable Entity Representations (BIERs), in which dimensions correspond to fine-grained entity types, and values are predicted probabilities that a given entity is of the corresponding type. We propose a novel method that exploits BIER's final sparse and intermediate dense representations to facilitate model and entity type debugging. We show that BIERs achieve strong performance in biomedical tasks including named entity disambiguation and entity label classification, and we provide error analysis to highlight the utility of their interpretability, particularly in low-supervision settings. Finally, we provide our induced 68K biomedical type system, the corresponding 37 million triples of derived data used to train BIER models and our best performing model. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Classification (of Information); Computational Linguistics; Natural Language Processing Systems; Biomedical Text; Domain Agnostics; Entity-types; Interpretable Representation; Language Model; Large Corpora; Model Uptakes; Performance; Training Sets; Type Systems; Mapping},
	keywords = {Classification (of information); Computational linguistics; Natural language processing systems; Biomedical text; Domain agnostics; Entity-types; Interpretable representation; Language model; Large corpora; Model uptakes; Performance; Training sets; Type systems; Mapping},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Balaraman2021239,
	author = {Balaraman, Vevake and Sheikhalishahi, Seyedmostafa and Magnini, Bernardo},
	title = {Recent Neural Methods on Dialogue State Tracking for Task-Oriented Dialogue Systems: A Survey},
	year = {2021},
	pages = {239 - 251},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123619722&partnerID=40&md5=9bf2b9e13969ff2967d92fcf56749672},
	abstract = {This paper aims at providing a comprehensive overview of recent developments in dialogue state tracking (DST) for task-oriented conversational systems. We introduce the task, the main datasets that have been exploited as well as their evaluation metrics, and we analyze several proposed approaches. We distinguish between static ontology DST models, which predict a fixed set of dialogue states, and dynamic ontology models, which can predict dialogue states even when the ontology changes. We also discuss the model's ability to track either single or multiple domains and to scale to new domains, both in terms of knowledge transfer and zero-shot learning. We cover a period from 2013 to 2020, showing a significant increase of multiple domain methods, most of them utilizing pre-trained language models. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Management; Speech Processing; Zero-shot Learning; Conversational Systems; Dialogue Systems; Dynamic Ontologies; Evaluation Metrics; Fixed Sets; Multiple Domains; Ontology's; State Tracking; Task-oriented; Tracking Models; Ontology},
	keywords = {Knowledge management; Speech processing; Zero-shot learning; Conversational systems; Dialogue systems; Dynamic ontologies; Evaluation metrics; Fixed sets; Multiple domains; Ontology's; State tracking; Task-oriented; Tracking models; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 48}
}

@CONFERENCE{Velichkov20211448,
	author = {Velichkov, Boris and Vassileva, Sylvia and Gerginov, Simeon and Kraychev, Boris and Ivanov, Ivaylo and Ivanov, Philip and Koychev, Ivan Kolev and Boytcheva, Svetla},
	title = {Comparative Analysis of Fine-tuned Deep Learning Language Models for ICD-10 classification task for Bulgarian Language},
	year = {2021},
	journal = {International Conference Recent Advances in Natural Language Processing, RANLP},
	pages = {1448 - 1454},
	doi = {10.26615/978-954-452-072-4_162},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123617023&doi=10.26615%2F978-954-452-072-4_162&partnerID=40&md5=280bc3c0b71bc874b05b29144d73ef7f},
	abstract = {The task of automatic diagnosis encoding into standard medical classifications and ontologies is of great importance in medicine - both to support the daily tasks of physicians in the preparation and reporting of clinical documentation, and for automatic processing of clinical reports. In this paper, we investigate the application and performance of different deep learning transformers for automatic encoding in ICD-10 of clinical texts in Bulgarian. The comparative analysis attempts to find which approach is more efficient to be used for fine-tuning of pre-trained BERT family transformer to deal with a specific domain terminology on a rare language such as Bulgarian. On the one hand, we use SlavicBERT and MultiligualBERT models, which are pre-trained for a common vocabulary in Bulgarian but lack medical terminology. On the other hand, we compare them to BioBERT, ClinicalBERT, SapBERT, BlueBERT models, which are pretrained for medical terminology in English, but lack training for language models in Bulgarian, and vocabulary in Cyrillic. In our research study, all BERT models are fine-tuned with additional medical texts in Bulgarian and then applied to the classification task for encoding medical diagnoses in Bulgarian into ICD-10 codes. A big corpus of diagnoses in Bulgarian annotated with ICD-10 codes is used for the classification task. Such an analysis gives a good idea of which of the models would be suitable for tasks of a similar type and domain. The experiments and evaluation results show that both approaches have comparable accuracy. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Deep Learning; Diagnosis; Encoding (symbols); Signal Encoding; Automatic Diagnosis; Classification Tasks; Clinical Documentation; Comparative Analyzes; Daily Tasks; Language Model; Learning Languages; Medical Classification; Medical Ontology; Medical Terminologies; Terminology},
	keywords = {Computational linguistics; Deep learning; Diagnosis; Encoding (symbols); Signal encoding; Automatic diagnosis; Classification tasks; Clinical documentation; Comparative analyzes; Daily tasks; Language model; Learning languages; Medical classification; Medical ontology; Medical terminologies; Terminology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1; All Open Access; Bronze Open Access}
}

@CONFERENCE{Shu2021549,
	author = {Shu, Lei and Guo, Yiluan and Wang, Huiping and Zhang, Xuetao and Hu, Renfen},
	title = {The Construction and Application of Ancient Chinese Corpus with Word Sense Annotation; 古汉语词义标注语料库的构建及应用研究},
	year = {2021},
	pages = {549 - 563},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123395920&partnerID=40&md5=95a9ff9bb5b8277c5248f392faaed29d},
	abstract = {In ancient Chinese, monosyllabic words are dominant, and polysemy is very common, which brings a certain challenge for modern people to understand the meaning of ancient Chinese. Based on the linguistic facts reflected in traditional dictionaries and corpora, this paper designs the principles of semantic division of polysemous words in ancient Chinese, and arranges the knowledge of commonly used monosyllabic words in ancient Chinese, so as to annotate the meaning of polysemous words. So far, the corpus contains 38,700 labeled data with a scale of more than 1,176,000 characters, which enriches the language resources in the field of ancient Chinese. Experiments show that the accuracy of automatic word sense disambiguation based on the corpus with the BERT language model achieves about 80%. Furthermore, this paper explores the application of the corpus built and the word sense disambiguation technology in the fields of language ontology research and dictionary compilation by taking the diachronic evolution analysis of word meaning and the induction of sense families as examples. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Ancient Chinese; Corpus; Word Sense Annotation; Word Sense Disambiguation; Computational Linguistics; Natural Language Processing Systems; Ancient Chinese; Chinese Corpus; Corpus; Labeled Data; Language Resources; Monosyllabic Words; Polysemous Word; Word Sense; Word Sense Annotation; Word Sense Disambiguation; Semantics},
	keywords = {Computational linguistics; Natural language processing systems; Ancient chinese; Chinese corpus; Corpus; Labeled data; Language resources; Monosyllabic words; Polysemous word; Word sense; Word sense annotation; Word Sense Disambiguation; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{Kossack2021193,
	author = {Kossack, Daniel and Borg, Niklas and Knorr, Leon and Portisch, Jan Philipp},
	title = {TOM Matcher Results for OAEI 2021},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {3063},
	pages = {193 - 198},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122692672&partnerID=40&md5=166ae259e2baee385eae401561937313},
	abstract = {This paper presents the matching system TOM together with its results in the Ontology Alignment Evaluation Initiative 2021 (OAEI 2021). This is the first participation of TOM in the OAEI. Very recently, transformers achieved remarkable results in the natural language processing community on a variety of tasks. The TOM match- ing system exploits a zero-shot transformer-based language model to cal- culate confidences for each instance. The matcher uses the pre-trained transformer model paraphrase-TinyBERT-L6-v2.3 © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Language Mod- Els; Ontology Alignment; Ontology Matching; Transformers; Natural Language Processing Systems; Language Mod- Els; Language Model; Matching System; Ontology Alignment; Ontology Matching; Transformer; Transformer Modeling; Ontology},
	keywords = {Natural language processing systems; Language mod- els; Language model; Matching system; Ontology alignment; Ontology matching; Transformer; Transformer modeling; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@CONFERENCE{Hertling202113,
	author = {Hertling, Sven and Portisch, Jan Philipp and Paulheim, Heiko},
	title = {Matching with Transformers in MELT},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {3063},
	pages = {13 - 24},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122652126&partnerID=40&md5=9cc2e930a5549028dc272c54d43b20a6},
	abstract = {One of the strongest signals for automated matching of on- tologies and knowledge graphs are the textual descriptions of the con- cepts. The methods that are typically applied (such as character- or token-based comparisons) are relatively simple, and therefore do not cap- ture the actual meaning of the texts. With the rise of transformer-based language models, text comparison based on meaning (rather than lexical features) is possible. In this paper, we model the ontology matching task as classification problem and present approaches based on transformer models. We further provide an easy to use implementation in the MELT framework which is suited for ontology and knowledge graph matching. We show that a transformer-based filter helps to choose the correct cor- respondences given a high-recall alignment and already achieves a good result with simple alignment post-processing methods.3 © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Matcher Optimization; Ontology Matching; Transformers; Knowledge Graph; Automated Matching; Knowledge Graphs; Matcher Optimization; Matchings; Ontology Matching; Optimisations; Simple++; Strong Signal; Textual Description; Transformer; Ontology},
	keywords = {Knowledge graph; Automated matching; Knowledge graphs; Matcher optimization; Matchings; Ontology matching; Optimisations; Simple++; Strong signal; Textual description; Transformer; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{2021,
	title = {ISWC-DC 2021 - Proceedings of the Doctoral Consortium at ISWC 2021, co-located with 20th International Semantic Web Conference, ISWC 2021},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {3005},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121138470&partnerID=40&md5=92cbd7294c560464733a0be7f4f8bf49},
	abstract = {The proceedings contain 9 papers. The topics discussed include: conceptual components and ontology design patterns extraction across different ontologies; leveraging literals for knowledge graph embeddings; knowledge augmented language models for causal question answering; knowledge graph for explainable cyber physical systems: a case study in smart energy grids; optimal transport methods for aligning knowledge graph triples with natural language in unsupervised settings; connecting the dots: transparent FAIRification of restricted data; advancing on the linked open university context: a Cuban linked open university; auditable semantic web machine learning systems; and assessing candidate ontologies for reuse. © 2021 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lomov2021669,
	author = {Lomov, Pavel A. and Malozemova, Marina and Shishaev, Maxim},
	title = {Data Augmentation in Training Neural-Network Language Model for Ontology Population},
	year = {2021},
	journal = {Lecture Notes in Networks and Systems},
	volume = {231 LNNS},
	pages = {669 - 679},
	doi = {10.1007/978-3-030-90321-3_55},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120694922&doi=10.1007%2F978-3-030-90321-3_55&partnerID=40&md5=18800da28ed17f70214321051d1a7b61},
	abstract = {This paper is a continuation of the research focused on solving the problem of ontology population using training on an automatically generated training set and the subsequent use of a neural-network language model for analyzing texts in order to discover new concepts to add to the ontology. The article is devoted to the text data augmentation - increasing the size of the training set by modification of its samples. Along with this, a solution to the problem of clarifying concepts (i.e. adjusting their boundaries in sentences), which were found during the automatic formation of the training set, is considered. A brief overview of existing approaches to text data augmentation, as well as approaches to extracting so-called nested named entities (nested NER), is presented. A procedure is proposed for clarifying the boundaries of the discovered concepts of the training set and its augmentation for subsequent training a neural-network language model in order to identify new concepts of ontology in the domain texts. The results of the experimental evaluation of the trained model and the main directions of further research are considered. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Augmentation; Neural Network; Ontology Population},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Janz2021223,
	author = {Janz, Arkadiusz and Piasecki, Maciej and Wątorski, Piotr},
	title = {Neural language models vs wordnet-based semantically enriched representation in CST relation recognition},
	year = {2021},
	pages = {223 - 233},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119991567&partnerID=40&md5=427263c24a6951f7ec810e8c68008c81},
	abstract = {Neural language models, including transformer-based models, that are pre-trained on very large corpora became a common way to represent text in various tasks, including recognition of textual semantic relations, e.g. Cross-document Structure Theory. Pre-trained models are usually fine tuned to downstream tasks and the obtained vectors are used as an input for deep neural classifiers. No linguistic knowledge obtained from resources and tools is utilised. In this paper we compare such universal approaches with a combination of rich graph-based linguistically motivated sentence representation and a typical neural network classifier applied to a task of recognition of CST relation in Polish. The representation describes selected levels of the sentence structure including description of lexical meanings on the basis of the wordnet (plWordNet) synsets and connected SUMO concepts. The obtained results show that in the case of difficult relations and medium size training corpus semantically enriched text representation leads to significantly better results. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Character Recognition; Computational Linguistics; Graphic Methods; Semantics; Cross-document Structure Theories; Down-stream; Graph-based; Language Model; Large Corpora; Linguistic Knowledge; Neural Classifiers; Semantic Relations; Universal Approach; Wordnet; Ontology},
	keywords = {Character recognition; Computational linguistics; Graphic methods; Semantics; Cross-document structure theories; Down-stream; Graph-based; Language model; Large corpora; Linguistic knowledge; Neural classifiers; Semantic relations; Universal approach; Wordnet; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{2021,
	title = {MLSMKG 2021 - Machine Learning with Symbolic Methods and Knowledge Graphs, co-located with European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2021},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {2997},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119414380&partnerID=40&md5=b361c68abb209a7ba6dc32e95e6c33f2},
	abstract = {The proceedings contain 4 papers. The topics discussed include: ontology-based N-ball concept embeddings informing few-shot image classification; contextual graph representation learning in text disambiguation; contextual language models for knowledge graph completion; and on refining BERT contextualized embeddings using semantic lexicons. © 2021 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Peng20213851,
	author = {Peng, Baolin and Zhu, Chenguang and Zeng, Michael and Gao, Jianfeng},
	title = {Data augmentation for spoken language understanding via pretrained language models},
	year = {2021},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
	volume = {5},
	pages = {3851 - 3855},
	doi = {10.21437/Interspeech.2021-117},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119209279&doi=10.21437%2FInterspeech.2021-117&partnerID=40&md5=95f449241113bcdc7f9ab9b3b398350e},
	abstract = {The training of spoken language understanding (SLU) models often faces the problem of data scarcity. In this paper, we put forward a data augmentation method using pretrained language models to boost the variability and accuracy of generated utterances. Furthermore, we investigate and propose solutions to two previously overlooked semi-supervised learning scenarios of data scarcity in SLU: i) Rich-in-Ontology: ontology information with numerous valid dialogue acts is given; ii) Rich-in-Utterance: a large number of unlabelled utterances are available. Empirical results show that our method can produce synthetic training data that boosts the performance of language understanding models in various scenarios. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Augmentation; Pretraining; Rich-in-ontology; Rich-in-utterance; Spoken Language Understanding; Computational Linguistics; Speech Communication; Speech Recognition; Supervised Learning; Augmentation Methods; Data Augmentation; Data Scarcity; Language Model; Ontology's; Pre-training; Rich-in-ontology; Rich-in-utterance; Semi-supervised Learning; Spoken Language Understanding; Ontology},
	keywords = {Computational linguistics; Speech communication; Speech recognition; Supervised learning; Augmentation methods; Data augmentation; Data scarcity; Language model; Ontology's; Pre-training; Rich-in-ontology; Rich-in-utterance; Semi-supervised learning; Spoken language understanding; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Sainz202144,
	author = {Sainz, Oscar and Rigau, German},
	title = {Ask2Transformers: Zero-shot domain labelling with pre-trained language models},
	year = {2021},
	pages = {44 - 52},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119149541&partnerID=40&md5=02af5dddcf0ffae9926edf8de4a38bdb},
	abstract = {In this paper we present a system that exploits different pre-trained Language Models for assigning domain labels to WordNet synsets without any kind of supervision. Furthermore, the system is not restricted to use a particular set of domain labels. We exploit the knowledge encoded within different off-the-shelf pre-trained Language Models and task formulations to infer the domain label of a particular WordNet definition. The proposed zero-shot system achieves a new state-of-the-art on the English dataset used in the evaluation. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Labelings; Language Model; State Of The Art; Synsets; Wordnet; Ontology},
	keywords = {Computational linguistics; Labelings; Language model; State of the art; Synsets; Wordnet; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 14}
}

@CONFERENCE{Marcińczuk2021207,
	author = {Marcińczuk, Michał and Gniewkowski, Mateusz and Walkowiak, Tomasz and Bȩdkowski, Marcin},
	title = {Text document clustering: Wordnet vs. TF-IDF vs. word embeddings},
	year = {2021},
	pages = {207 - 214},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118276608&partnerID=40&md5=efec21f689647df23a10ea47d8433c6c},
	abstract = {In the paper, we deal with the problem of unsupervised text document clustering for the Polish language. Our goal is to compare the modern approaches based on language modeling (doc2vec and BERT) with the classical ones, i.e., TF-IDF and wordnet-based. The experiments are conducted on three datasets containing qualification descriptions. The experiments’ results showed that wordnet-based similarity measures could compete and even outperform modern embedding-based approaches. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Cluster Analysis; Computational Linguistics; Embeddings; Information Retrieval; Modeling Languages; Language Model; Similarity Measure; Text Document Clustering; Wordnet; Ontology},
	keywords = {Cluster analysis; Computational linguistics; Embeddings; Information retrieval; Modeling languages; Language model; Similarity measure; Text Document Clustering; Wordnet; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 24}
}

@ARTICLE{2021,
	title = {18th Extended Semantic Web Conference, ESWC 2021},
	year = {2021},
	journal = {Lecture Notes in Computer Science},
	volume = {12739 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115854881&partnerID=40&md5=f10b32b4cb6c00f4ac62076dc9d2e24e},
	abstract = {The proceedings contain 38 papers. The special focus in this conference is on Semantic Web. The topics include: RaiseWikibase: Fast Inserts into the BERD Instance; do Judge an Entity by Its Name! Entity Typing Using Language Models; towards a Domain-Agnostic Computable Policy Tool; towards an Evaluation Framework for Expressive Stream Reasoning; schema-Backed Visual Queries over Europeana and Other Linked Data Resources; CLiT: Combining Linking Techniques for Everyone; SANTé: A Light-Weight End-to-End Semantic Search Framework for RDF Data; Coverage-Based Summaries for RDF KBs; named Entity Recognition as Graph Classification; scikit-learn Pipelines Meet Knowledge Graphs: The Python kgextension Package; exploiting Transitivity for Entity Matching; the Nuremberg Address Knowledge Graph; SaGe-Path: Pay-as-you-go SPARQL Property Path Queries Processing Using Web Preemption; ontology for Informatics Research Artifacts; non-named Entities – The Silent Majority; unsupervised Relation Extraction Using Sentence Encoding; evoKGsim+: A Framework for Tailoring Knowledge Graph-Based Similarity for Supervised Learning; extraction of Union and Intersection Axioms from Biomedical Text; implementing Informed Consent with Knowledge Graphs; improving Decision Making Using Semantic Web Technologies; SLURP: An Interactive SPARQL Query Planner; ontological Formalisation of Mathematical Equations for Phenomic Data Exploitation; Identifying Events from Streams of RDF-Graphs Representing News and Social Media Messages; Towards Visually Intelligent Agents (VIA): A Hybrid Approach; using Knowledge Graphs for Machine Learning in Smart Home Forecasters; stigmergic Multi-Agent Systems in the Semantic Web of Things; towards an Ontology for Propaganda Detection in News Articles; a Virtual Knowledge Graph for Enabling Defect Traceability and Customer Service Analytics; constructing Micro Knowledge Graphs from Technical Support Documents. © 2021 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2021,
	title = {CEUR Workshop Proceedings},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {2949},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115766018&partnerID=40&md5=dd5fcf88abaae6bf9ede44647bcc1d5a},
	abstract = {The proceedings contain 9 papers. The topics discussed include: capturing complex heritage knowledge; digital knowledge presentation within the national gallery, London; an approach to semantic representation and modeling in the development of De Rerum Natura digital exhibition; describing and searching food content in historical images with the ChIA vocabulary; TTProfiler: computing types and terms profiles of assertional knowledge graphs; masked language model entity matching for cultural heritage data; towards the representation of claims in ontologies for the digital humanities; modelling lexicographic resources using CIDOC-CRM, FRBRoo and Ontolex-Lemon; and the IMAGO project: towards a knowledge base of medieval and renaissance geographical works. © 2021 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Nagy2021477,
	author = {Nagy, George},
	title = {Near-Perfect Relation Extraction from Family Books},
	year = {2021},
	journal = {Lecture Notes in Computer Science},
	volume = {12823 LNCS},
	pages = {477 - 491},
	doi = {10.1007/978-3-030-86334-0_31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115309611&doi=10.1007%2F978-3-030-86334-0_31&partnerID=40&md5=ee6cb6aebef893f74d8afd36ef1a131a},
	abstract = {Precise sequence constraints are proposed to accelerate information extraction from a class of “semi-structured” documents that includes hundreds of thousands of digitized genealogical records. While Named Entity Recognition (NER) and Named Relation Recognition (NRR) on free-running text lack universally applicable solutions, under these constraints generalized template-matching can accomplish both. Interactive information extraction is demonstrated on three digitized books. The book-text tokens are first labeled according to their role (e.g. Head, Spouse, or Birthdate), then pairs of labeled entities are combined into labeled relations (e.g. &lt;Head [Spouse]&gt;, or &lt;Spouse [Birthdate]&gt;). Accurate NRR is ensured by high-precision (&gt;99%) NER. On semi-structured text the proposed NRR algorithm produces only valid relations from correctly labeled entities. About three hours of user interaction and a few minutes of laptop run time suffice to produce database- or ontology-ready input from a new book. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Information Extraction; Language Models; Text Analysis; Character Recognition; Natural Language Processing Systems; Template Matching; High-precision; Information Extraction; Language Model; Named Entity Recognition; Recognition Algorithm; Relation Extraction; Runtimes; Semi-structured Documents; Semi-structured Text; User Interaction; Information Retrieval},
	keywords = {Character recognition; Natural language processing systems; Template matching; High-precision; Information extraction; Language model; Named entity recognition; Recognition algorithm; Relation extraction; Runtimes; Semi-structured documents; Semi-structured text; User interaction; Information retrieval},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{2021,
	title = {CSW 2021 - Proceedings of the 2nd Crowd Science Workshop: Trust, Ethics, and Excellence in Crowdsourced Data Management at Scale, co-located with 47th International Conference on Very Large Data Bases, VLDB 2021},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {2932},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113784744&partnerID=40&md5=3122d3f7ed9363cef694f0361df24467},
	abstract = {The proceedings contain 8 papers. The topics discussed include: VLDB 2021 crowd science challenge on aggregating crowdsourced audio transcriptions; a multi-objective clustering ensemble approach for crowdsourced clustering; crowdsourcing of parallel corpora: the case of style transfer for detoxification; training-induced class imbalance in crowdsourced data; aggregation of crowdsourced ontology-based item descriptions by hierarchical voting; finer granularity means better data: a crowdsourcing lab experiment; fine-tuning pre-trained language model for crowdsourced texts aggregation; and noisy text sequences aggregation as a summarization subtask. © 2021 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2021,
	title = {34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021},
	year = {2021},
	journal = {Lecture Notes in Computer Science},
	volume = {12798 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112728065&partnerID=40&md5=76cbd81a41c738fdf562798b2cb458a7},
	abstract = {The proceedings contain 106 papers. The special focus in this conference is on Industrial, Engineering and Other Applications of Applied Intelligent Systems. The topics include: Semantic Technologies Towards Missing Values Imputation; an Improved Integer Programming Formulation for Inferring Chemical Compounds with Prescribed Topological Structures; automatic Classification for Ontology Generation by Pretrained Language Model; deep Learning Architecture for Topological Optimized Mechanical Design Generation with Complex Shape Criterion; towards Increasing Open Data Adoption Through Stream Data Integration and Imputation; Computational Ontology and BIM Technology in Data-Driven Indoor Route Planning; Collaborative Maintenance of EDOAL Alignments in VocBench; DIKG2: A Semantic Data Integration Approach for Knowledge Graphs Generation from Web Forms; ontology-Based Resume Searching System for Job Applicants in Information Technology; an Approach to Expressing Metamodels’ Semantics in a Concept System; Birth-Death MCMC Approach for Multivariate Beta Mixture Models in Medical Applications; intelligent Asthma Self-management System for Personalised Weather-Based Healthcare Using Machine Learning; Deep Forecasting of COVID-19: Canadian Case Study; COVID-19 Genome Analysis Using Alignment-Free Methods; Deep Efficient Neural Networks for Explainable COVID-19 Detection on CXR Images; towards Efficient Discovery of Periodic-Frequent Patterns in Columnar Temporal Databases; predicting Psychological Distress from Ecological Factors: A Machine Learning Approach; configuration Model of Employee Competences in a Social Media Team; the Extended Graph Generalization as a Representation of the Metamodels’ Extensional Layer; wawPart: Workload-Aware Partitioning of Knowledge Graphs; analysis of Sentimental Behaviour over Social Data Using Machine Learning Algorithms; a Large-Scale Dataset for Hate Speech Detection on Vietnamese Social Media Texts; Determining 2-Optimality Consensus for DNA Structure. © 2021 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Oba2021210,
	author = {Oba, Atsushi and Paik, Incheon and Kuwana, Ayato},
	title = {Automatic Classification for Ontology Generation by Pretrained Language Model},
	year = {2021},
	journal = {Lecture Notes in Computer Science},
	volume = {12798 LNAI},
	pages = {210 - 221},
	doi = {10.1007/978-3-030-79457-6_18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112704288&doi=10.1007%2F978-3-030-79457-6_18&partnerID=40&md5=5cb2f6fa7ec2a1f6545b7a90c3c7f634},
	abstract = {In recent years, for systemizing enormous information on the Internet, ontology that organizes knowledge through a hierarchical structure of concepts has received a large amount of attention in spatiotemporal information science. However, constructing ontology manually requires a large amount of time and deep knowledge of the target field. Consequently, automating ontology generation from raw text corpus is required to meet the ontology demand. As an initial attempt of ontology generation with a neural network, a recurrent neural N = network (RNN)-based method is proposed. However, updating the architecture is possible because of the development in natural language processing (NLP). In contrast, the transfer learning of language models trained by a large unlabeled corpus such as bidirectional encoder representations from transformers (BERT) has yielded a breakthrough in NLP. Inspired by these achievements, to apply transfer learning of language models, we propose a novel workflow for ontology generation consisting of two-stage learning. This paper provides a quantitative comparison between the proposed method and the existing methods. Our result showed that our best method improved accuracy by over 12.5%. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Automation; Natural Language Processing (nlp); Ontology; Pretrained Model; Computational Linguistics; Intelligent Systems; Natural Language Processing Systems; Ontology; Recurrent Neural Networks; Transfer Learning; Automatic Classification; Deep Knowledge; Hierarchical Structures; Language Model; Natural Language Processing; Ontology Generation; Quantitative Comparison; Spatiotemporal Information; Learning Systems},
	keywords = {Computational linguistics; Intelligent systems; Natural language processing systems; Ontology; Recurrent neural networks; Transfer learning; Automatic classification; Deep knowledge; Hierarchical structures; Language model; NAtural language processing; Ontology generation; Quantitative comparison; Spatiotemporal information; Learning systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{2021,
	title = {34th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2021},
	year = {2021},
	journal = {Lecture Notes in Computer Science},
	volume = {12799 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112698250&partnerID=40&md5=4d1c08f6d141ae7febbfc433cd6995c8},
	abstract = {The proceedings contain 106 papers. The special focus in this conference is on Industrial, Engineering and Other Applications of Applied Intelligent Systems. The topics include: Semantic Technologies Towards Missing Values Imputation; an Improved Integer Programming Formulation for Inferring Chemical Compounds with Prescribed Topological Structures; automatic Classification for Ontology Generation by Pretrained Language Model; deep Learning Architecture for Topological Optimized Mechanical Design Generation with Complex Shape Criterion; towards Increasing Open Data Adoption Through Stream Data Integration and Imputation; Computational Ontology and BIM Technology in Data-Driven Indoor Route Planning; Collaborative Maintenance of EDOAL Alignments in VocBench; DIKG2: A Semantic Data Integration Approach for Knowledge Graphs Generation from Web Forms; ontology-Based Resume Searching System for Job Applicants in Information Technology; an Approach to Expressing Metamodels’ Semantics in a Concept System; Birth-Death MCMC Approach for Multivariate Beta Mixture Models in Medical Applications; intelligent Asthma Self-management System for Personalised Weather-Based Healthcare Using Machine Learning; Deep Forecasting of COVID-19: Canadian Case Study; COVID-19 Genome Analysis Using Alignment-Free Methods; Deep Efficient Neural Networks for Explainable COVID-19 Detection on CXR Images; towards Efficient Discovery of Periodic-Frequent Patterns in Columnar Temporal Databases; predicting Psychological Distress from Ecological Factors: A Machine Learning Approach; configuration Model of Employee Competences in a Social Media Team; the Extended Graph Generalization as a Representation of the Metamodels’ Extensional Layer; wawPart: Workload-Aware Partitioning of Knowledge Graphs; analysis of Sentimental Behaviour over Social Data Using Machine Learning Algorithms; a Large-Scale Dataset for Hate Speech Detection on Vietnamese Social Media Texts; Determining 2-Optimality Consensus for DNA Structure. © 2021 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Edwards20211,
	author = {Edwards, Aleksandra and Rogers, David and Camacho-Collados, José and de Ribaupierre, Hélène and Preece, Alun D.},
	title = {Predicting themes within complex unstructured texts: A case study on safeguarding reports},
	year = {2021},
	journal = {CEUR Workshop Proceedings},
	volume = {2918},
	pages = {1 - 10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112074833&partnerID=40&md5=e08c8e39254682103a3b0987866a9412},
	abstract = {Text classification typically requires large amounts of labelled training data; however, the acquisition of high volumes of labelled datasets is often expensive or unfeasible, especially for highly-specialised domains for which both training data (documents) and access to subject-matter expertise (for labelling) is limited. Language models pre-trained on large amounts of text corpora provide state-of-the-art performance against most standard natural language processing (NLP) benchmarks, including text classification. However, their prevalence over more traditional linear classifiers and domain-based approaches have not been investigated fully. In this paper, we address the combination of state-of-the-art deep learning and classification methods and provide an insight into what combination of methods fit the needs of small, domain-specific, and terminologicallyrich corpora. We focus on a real-world scenario related to a collection of safeguarding reports comprising learning experiences and reflections on tackling serious incidents involving children and vulnerable adults. Our aim is to automatically identify the main themes in a safeguarding report using three main types of classification. Our results show that for a very small amount of data a simple linear classifier outperforms state-of-the-art language models. Further, we show that the performance of classifiers is more affected by the size of the training data rather than the amount of context given. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Language Models; Small Domain Corpus; Text Classification; Benchmarking; Computational Linguistics; Deep Learning; Information Retrieval Systems; Large Dataset; Ontology; Sentiment Analysis; Classification Methods; Learning Experiences; Linear Classifiers; Natural Language Processing; Performance Of Classifier; Real-world Scenario; State-of-the-art Performance; Text Classification; Classification (of Information)},
	keywords = {Benchmarking; Computational linguistics; Deep learning; Information retrieval systems; Large dataset; Ontology; Sentiment analysis; Classification methods; Learning experiences; Linear classifiers; NAtural language processing; Performance of classifier; Real-world scenario; State-of-the-art performance; Text classification; Classification (of information)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Jiang2021793,
	author = {Jiang, Yuxin and Shou, Ziyi and Wang, Qijun and Wu, Hao and Lin, Fangzhen},
	title = {XRJL-HKUST at SemEval-2021 Task 4: WordNet-Enhanced Dual Multi-head Co-Attention for Reading Comprehension of Abstract Meaning},
	year = {2021},
	pages = {793 - 798},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108801413&partnerID=40&md5=16aa0de2660bcd562593c1610078ae35},
	abstract = {This paper presents our submitted system to SemEval 2021 Task 4: Reading Comprehension of Abstract Meaning. Our system uses a large pre-trained language model as the encoder and an additional dual multi-head co-attention layer to strengthen the relationship between passages and question-answer pairs, following the current state-of-the-art model DUMA. The main difference is that we stack the passage-question and question-passage attention modules instead of calculating parallelly to simulate re-considering process. We also add a layer normalization module to improve the performance of our model. Furthermore, to incorporate our known knowledge about abstract concepts, we retrieve the definitions of candidate answers from WordNet and feed them to the model as extra inputs. Our system, called WordNet-enhanced DUal Multi-head Co-Attention (WN-DUMA), achieves 86.67% and 89.99% accuracy on the official blind test set of subtask 1 and subtask 2 respectively. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Semantics; 'current; Art Model; Language Model; Normalisation; Question-answer Pairs; Reading Comprehension; State Of The Art; Subtask; System Use; Wordnet; Ontology},
	keywords = {Semantics; 'current; ART model; Language model; Normalisation; Question-answer pairs; Reading comprehension; State of the art; Subtask; System use; Wordnet; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Li20211063,
	author = {Li, Shuyang and Cao, Jin and Sridhar, Mukund Harakere and Zhu, Henghui and Li, Shangwen and Hamza, Wael and McAuley, Julian J.},
	title = {Zero-shot generalization in dialog state tracking through generative question answering},
	year = {2021},
	pages = {1063 - 1074},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107277854&partnerID=40&md5=8d594107df9044a13b93d2040d7e3f56},
	abstract = {Dialog State Tracking (DST), an integral part of modern dialog systems, aims to track user preferences and constraints (slots) in task-oriented dialogs. In real-world settings with constantly changing services, DST systems must generalize to new domains and unseen slot types. Existing methods for DST do not generalize well to new slot names and many require known ontologies of slot types and values for inference. We introduce a novel ontology-free framework that supports natural language queries for unseen constraints and slots in multi-domain task-oriented dialogs. Our approach is based on generative question-answering using a conditional language model pre-trained on substantive English sentences. Our model improves joint goal accuracy in zero-shot domain adaptation settings by up to 9% (absolute) over the previous state-of-the-art on the MultiWOZ 2.1 dataset. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Ontology; Domain Adaptation; English Sentences; Language Model; Multi-domain Tasks; Natural Language Queries; Question Answering; Real World Setting; State Of The Art; Natural Language Processing Systems},
	keywords = {Computational linguistics; Ontology; Domain adaptation; English sentences; Language model; Multi-domain tasks; Natural language queries; Question Answering; Real world setting; State of the art; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 32}
}

@ARTICLE{Gonzalez-Garcia2021116,
	author = {Gonzalez-Garcia, Lino and García-Barriocanal, Elena and Sicília, Miguel Angel},
	title = {Entity Linking as a Population Mechanism for Skill Ontologies: Evaluating the Use of ESCO and Wikidata},
	year = {2021},
	journal = {Communications in Computer and Information Science},
	volume = {1355 CCIS},
	pages = {116 - 122},
	doi = {10.1007/978-3-030-71903-6_12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104849893&doi=10.1007%2F978-3-030-71903-6_12&partnerID=40&md5=944956e046d5861008c02f265cd51994},
	abstract = {Ontologies or databases describing occupations in terms of competences or skills are an important resource for a number of applications. Exploiting large knowledge graphs thus becomes a promising direction to update those ontologies with entities of the latter, which may be updated faster, especially in the case of crowd-sourced resources. Here we report a first assessment of the potential of that strategy matching knowledge elements in ESCO to Wikidata using NER and document similarity models available at the Spacy NLP libraries. Results show that the approach may be effective, but the use of pre-trained language models and the short texts included with entities (labels and descriptions) does not result in sufficient quality for a fully automated process. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Entity Linking; Esco; Skill Ontologies; Automation; Knowledge Representation; Metadata; Semantics; Document Similarity; Fully Automated; Knowledge Graphs; Language Model; Short Texts; Ontology},
	keywords = {Automation; Knowledge representation; Metadata; Semantics; Document similarity; Fully automated; Knowledge graphs; Language model; Short texts; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@ARTICLE{Balaraman2021866,
	author = {Balaraman, Vevake and Magnini, Bernardo},
	title = {Domain-Aware Dialogue State Tracker for Multi-Domain Dialogue Systems},
	year = {2021},
	journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
	volume = {29},
	pages = {866 - 873},
	doi = {10.1109/TASLP.2021.3054309},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100500450&doi=10.1109%2FTASLP.2021.3054309&partnerID=40&md5=12198f262ed6a6e21de55d25965957df},
	abstract = {In task-oriented dialogue systems the dialogue state tracker component (DST) is responsible for predicting the current state of the dialogue based on the dialogue history and the user utterance. Current DST approaches rely on a predefined domain ontology, a fact that limits their effective usage for large scale conversational agents, where the DST constantly needs to be interfaced with ever-increasing services and APIs. Focused towards overcoming this drawback, we propose a domain-aware dialogue state tracker, that is completely data-driven and it is modeled to predict for dynamic service schemas, including zero-shot domains. Unlike approaches that propose separate models for prediction of intents, requested slots, slot status, categorical slots and non-categorical slots, we propose a single model in an end-to-end architecture. The proposed model utilizes domain and slot information to extract both domain and slot specific representations from a given dialogue, and then uses such representations to predict the values of the corresponding slot in a given domain. Integrating this mechanism with pretrained language models, our approach can effectively learn semantic relations and effectively perform transfer learning between domains or zero-shot tracking for domains not present in training. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Dialogue State Tracking; End-to-end; Multi-domain Dialogue Systems; Zero-shot Tracking; Forecasting; Semantics; Speech Processing; Transfer Learning; Conversational Agents; Dialogue Systems; Domain Ontologies; Dynamic Services; Language Model; Semantic Relations; Single Models; Task-oriented; Speech Recognition},
	keywords = {Forecasting; Semantics; Speech processing; Transfer learning; Conversational agents; Dialogue systems; Domain ontologies; Dynamic services; Language model; Semantic relations; Single models; Task-oriented; Speech recognition},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Chhaya20211,
	author = {Chhaya, Bharvi and Jafer, Shafagh},
	title = {Usability analysis of scenario-based framework for domain-specific language model},
	year = {2021},
	volume = {1 PartF},
	pages = {1 - 11},
	doi = {10.2514/6.2021-0787},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100294229&doi=10.2514%2F6.2021-0787&partnerID=40&md5=4e0d89eb67df979fa559662532b1e2f2},
	abstract = {Domain-Specific Languages (DSLs) support higher levels of abstractions than general-purpose modeling languages and are closer to the problem domain than they are to the implementation domain. Currently, a lot of external research and collaboration between a modeling and simulation (M&S) expert and a domain expert is required to understand the specifics of domain structure and behavior to create a DSL. The Domain-Specific Scenario (DoSS) framework proposes the use of scenarios in natural language, which are currently used in requirements engineering and testing, as the basis for developing the domain model iteratively. In order to use natural-language scenarios to drive development, these scenarios need to be suitably broken down to extract information about the functionality of the language and domain model. A cited advantage of this approach is that it is a promising alternative for developing a DSL without excessive prior domain knowledge. A thorough analysis of this approach can assist in reaching a conclusion about the practicality and applicability of the proposed solution. In order to substantiate this claim, this paper presents a case study detailing the benefits obtained from using such an approach compared to the current standard in terms of two important metrics: a) time taken, and b) prior knowledge required. For this study, a student researcher was tasked with creating a metamodel for aviation using only the scenarios presented in the paper. They had no prior knowledge of flight training scenarios and had not seen an ontology or metamodel for it. The final metamodel is compared to the metamodel obtained by a researcher with previous domain modeling and aviation ontology modeling experience to see the value in the framework when used by someone who is not a domain or modeling and simulation expert. The resulting domain model was almost identical to that created by a domain expert, showing that the DoSS framework does not rely upon technical knowledge to generate domain models. A log of the time taken and effort required for this user also showed that the tasks were not considered tedious or taxing. The paper discusses the results and the challenges of the approach and the limitations of the results obtained. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Aviation; Digital Subscriber Lines; Domain Knowledge; Modeling Languages; Natural Language Processing Systems; Ontology; Domain Experts; Domain Model; Domain Specific; Domains Specific Languages; Language Model; Meta Model; Model And Simulation; Natural Languages; Prior-knowledge; Usability Analysis; Problem Oriented Languages},
	keywords = {Aviation; Digital subscriber lines; Domain Knowledge; Modeling languages; Natural language processing systems; Ontology; Domain experts; Domain model; Domain specific; Domains specific languages; Language model; Meta model; Model and simulation; Natural languages; Prior-knowledge; Usability analysis; Problem oriented languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Denisov2021256,
	author = {Denisov, Mikhail and Anikin, Anton V. and Sychev, Oleg A. and Katyshev, Alexander},
	title = {Program execution comprehension modelling for algorithmic languages learning using ontology-based techniques},
	year = {2021},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1184},
	pages = {256 - 269},
	doi = {10.1007/978-981-15-5859-7_25},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091916449&doi=10.1007%2F978-981-15-5859-7_25&partnerID=40&md5=9234b5f6925d2075a35bcd25507fa9f6},
	abstract = {In this paper, we propose an ontology-based approach to model a program execution comprehension so to be able to explain to the novice programmer the essence of his/her error. We have studied the algorithmic languages model operating with actions and basic control structures (“sequence,” “branching,” and “looping”) and designed the rules to capture any deviation from the permissible. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Algorithmic Language; Comprehension; E-learning; Explanation; Ontology; Modeling Languages; Ontology; Basic Control Structures; Novice Programmer; Ontology-based; Program Execution; Algorithmic Languages},
	keywords = {Modeling languages; Ontology; Basic control structures; Novice programmer; Ontology-based; Program execution; Algorithmic languages},
	type = {Book chapter},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Mehndiratta2021329,
	author = {Mehndiratta, Akanksha and Asawa, Krishna},
	title = {Non-goal oriented dialogue agents: state of the art, dataset, and evaluation},
	year = {2021},
	journal = {Artificial Intelligence Review},
	volume = {54},
	number = {1},
	pages = {329 - 357},
	doi = {10.1007/s10462-020-09848-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086088700&doi=10.1007%2Fs10462-020-09848-z&partnerID=40&md5=0fc2ac4d1bbbd82c647734d0bec8c7f9},
	abstract = {Dialogue agent, a derivative of intelligent agent in the field of computational linguistics, is a computer program that is capable of generating responses and performing conversation in natural language. The field of computational linguistics is flourishing due to the intensive growth of dialogue agents; the most potential one is providing voice controlled smart personal assistant service for handsets and homes. The agents are usable, accessible but perform task-related short conversations. Non-goal-oriented dialogue agents are designed to imitate extended human–human conversations, also called as chit-chat, to provide the consumer with a satisfactory experience on the conversation quality. The design of such agents is primarily defined by a language model, unlike goal-oriented dialogue agents that employees slot based or ontology-based frameworks, hence most of the methods are data-driven. This paper surveys the current state of the art of non-goal-oriented dialogue systems specifically data-driven methods, the most prevalent being deep learning. This paper aims at (a) providing an insight of recent methods and architectures proposed for building context and modeling response along with a comprehensive review of the state of the art (b) examine the type of data set and evaluation methods available (c) present the challenges and limitation that the recent models, dataset and evaluation methods constitute. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Dialogue Agent; Dialogue Management Systems; Language Modeling; Machine Learning; Natural Language Processing; Deep Learning; Ontology; Speech Processing; Data-driven Methods; Dialogue Systems; Evaluation Methods; Natural Languages; Ontology-based; Personal Assistants; State Of The Art; Voice-controlled; Computational Linguistics},
	keywords = {Deep learning; Ontology; Speech processing; Data-driven methods; Dialogue systems; Evaluation methods; Natural languages; Ontology-based; Personal assistants; State of the art; Voice-controlled; Computational linguistics},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Colas2021,
	author = {Colas, Anthony M. and Sadeghian, Ali and Wang, Yue and Wang, Daisy Zhe},
	title = {EventNarrative: A Large-scale Event-centric Dataset for Knowledge Graph-to-Text Generation},
	year = {2021},
	journal = {Advances in Neural Information Processing Systems},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000453796&partnerID=40&md5=cce382eb32b4149f8f93f09b9ea65b89},
	abstract = {We introduce EventNarrative, a knowledge graph-to-text dataset from publicly available open-world knowledge graphs. Given the recent advances in event-driven Information Extraction (IE), and that prior research on graph-to-text only focused on entity-driven KGs, this paper focuses on event-centric data. However, our data generation system can still be adapted to other types of KG data. Existing large-scale datasets in the graph-to-text area are non-parallel, meaning there is a large disconnect between the KGs and text. The datasets that have a paired KG and text, are small scale and manually generated or generated without a rich ontology, making the corresponding graphs sparse. Furthermore, these datasets contain many unlinked entities between their KG and text pairs. EventNarrative consists of approximately 230,000 graphs and their corresponding natural language text, six times larger than the current largest parallel dataset. It makes use of a rich ontology, all the KGs entities are linked to the text, and our manual annotations confirm a high data quality. Our aim is two-fold: to help break new ground in event-centric research where data is lacking and to give researchers a well-defined, large-scale dataset in order to better evaluate existing and future knowledge graph-to-text models. We also evaluate two types of baselines on EventNarrative: a graph-to-text specific model and two state-of-the-art language models, which previous work has shown to be adaptable to the knowledge graph-to-text domain. © 2025 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Assimilation; Spatio-temporal Data; Data Generation System; Event-driven; Knowledge Graphs; Large Scale Events; Large-scale Datasets; Ontology's; Open World; Text Areas; Text Generations; World Knowledge; Knowledge Graph},
	keywords = {Data assimilation; Spatio-temporal data; Data generation system; Event-driven; Knowledge graphs; Large scale events; Large-scale datasets; Ontology's; Open world; Text areas; Text generations; World knowledge; Knowledge graph},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12}
}

@ARTICLE{Wu2020177,
	author = {Wu, Saisai and Zhou, Ailian and Xie, Nengfu and Liang, Xiaohe and Wang, Huijuan and Li, Xiaoyu and Chen, Guipeng},
	title = {Construction of visualization domain-specific knowledge graph of crop diseases and pests based on deep learning; 基于深度学习的作物病虫害可视化知识图谱构建},
	year = {2020},
	journal = {Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering},
	volume = {36},
	number = {24},
	pages = {177 - 185},
	doi = {10.11975/j.issn.1002-6819.2020.24.021},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102952395&doi=10.11975%2Fj.issn.1002-6819.2020.24.021&partnerID=40&md5=7bfd76d3407f494599e751c4f0e1949d},
	abstract = {The knowledge graph describes the concepts, entities, and their relationships in the objective world in a structured form. It has a better ability to organize, manage, and understand massive amounts of information, and can structure heterogeneous knowledge in the field. It can be widely used in medical, biological, financial, etc. In view of the current situation in the field of crop diseases and insect pests, there are multiple relationship pairs between the same entity and multiple entities, multi-source heterogeneous data, poor aggregation ability, low utilization, and the possibility of knowledge sharing. Combining Natural Language Processing (NLP) and text mining technologies, this study focused on data acquisition, ontology construction, knowledge extraction, and knowledge storage, researched on the construction of crops diseases and insect pests knowledge graph based on deep learning. Firstly, this study used the Scrapy crawler framework of the Python programming language to crawl data from web pages related to crop diseases and insect pests, and performed data cleaning and data supplementation through data preprocessing methods. Secondly, according to the characteristics of the domain corpus, the Protégé ontology construction tool was used to complete the semi-automatic construction of the crop diseases and insect pests ontology predefined the set of properties and relations and set the corresponding domains and ranges. Then, based on the ontology, the rule method was used to extract semi-structured knowledge, and the deep learning method was used to extract unstructured knowledge. In the process of unstructured knowledge extraction, a text annotation mode "Main_Entity+Relation+BIESO" (ME+R+BIESO) adapted to the domain corpus was also proposed. Based on a predefined set of relationships, entities and relationships were simultaneously annotated, it contained entity and relationship information at the same time, and directly modeling the triples instead of separately modeling entities and relationships. The corresponding triples were also directly obtained through analysis, which not only saved at least half of the cost of labeling but also realized the joint extraction of entity relations and solved the problem of overlapping relation extraction. And this study used the Bidirectional Encoder Representation from Transformers (BERT)- Bi-directional Long-Short Term Memory (BiLSTM)+ Conditional Random Field (CRF) end-to-end model to experiment on the crop diseases and insect pests dataset. First, this study used the BERT pre-training language model to encode words, extracted text features, and used the generated vector as the input of the BiLSTM layer; BiLSTM integrated contextual information into the model at the same time, and performed bidirectional encoding to achieve effective prediction of label sequences; finally, this study used the CRF module to decode the output result of BiLSTM, and the label transition probability and constraint conditions were obtained through training and learning, and the entity label category of each character was obtained. The experimental results showed that the precision was 94.06%, the recall was 89.02%, and the F1 value reached 91.34%, which was much better than the pipeline method and classic models such as BiLSTM+CRF and Convolutional Neural Networks (CNN)+BiLSTM+CRF in the joint extraction method. The joint extraction of entity relations based on this annotation mode not only improved the efficiency and accuracy of annotation but also solved the problem of overlapping relations in the corpus. Finally, the extracted knowledge was stored in the graph database to realize the visual display of the knowledge graph and deep knowledge mining and reasoning. Combined the deep learning technology to realize the semi-automatic construction of the knowledge graph, which was of great significance for the detection of crop diseases and insect pests, forecasting and early warning, and the establishment of prevention models in the intelligent production system. It could provide a high-quality knowledge base for crop diseases and insect pests question answering systems, recommendation systems, search engines, and other applications, which could be effectively applied to crop variety selection, pest prevention and control, and fertilization and irrigation. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Crops; Deep Learning; Diseases And Pests; Joint Extraction Of Entity And Relation; Knowledge Graph; Models; Convolutional Neural Networks; Crops; Data Acquisition; Digital Storage; Disease Control; Encoding (symbols); Extraction; Graph Databases; Graphic Methods; Information Management; Knowledge Based Systems; Knowledge Representation; Learning Systems; Linguistics; Natural Language Processing Systems; Ontology; Problem Oriented Languages; Quality Control; Random Processes; Search Engines; Signal Encoding; Text Mining; Websites; Conditional Random Field; Domain-specific Knowledge; Forecasting And Early Warnings; Heterogeneous Knowledge; Natural Language Processing; Python Programming Language; Question Answering Systems; Transition Probabilities; Deep Learning},
	keywords = {Convolutional neural networks; Crops; Data acquisition; Digital storage; Disease control; Encoding (symbols); Extraction; Graph Databases; Graphic methods; Information management; Knowledge based systems; Knowledge representation; Learning systems; Linguistics; Natural language processing systems; Ontology; Problem oriented languages; Quality control; Random processes; Search engines; Signal encoding; Text mining; Websites; Conditional random field; Domain-specific knowledge; Forecasting and early warnings; Heterogeneous Knowledge; NAtural language processing; Python programming language; Question answering systems; Transition probabilities; Deep learning},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 41}
}

@ARTICLE{Du2020,
	author = {Du, Zhihua and He, Yufeng and Li, Jianqiang and Uversky, Vladimir N.},
	title = {DeepAdd: Protein function prediction from k-mer embedding and additional features},
	year = {2020},
	journal = {Computational Biology and Chemistry},
	volume = {89},
	pages = {},
	doi = {10.1016/j.compbiolchem.2020.107379},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091774895&doi=10.1016%2Fj.compbiolchem.2020.107379&partnerID=40&md5=bcfed79ce41cb7484ed8a5a33b62c4d7},
	abstract = {With the application of new high throughput sequencing technology, a large number of protein sequences is becoming available. Determination of the functional characteristics of these proteins by experiments is an expensive endeavor that requires a lot of time. Furthermore, at the organismal level, such kind of experimental functional analyses can be conducted only for a very few selected model organisms. Computational function prediction methods can be used to fill this gap. The functions of proteins are classified by Gene Ontology (GO), which contains more than 40,000 classifications in three domains, Molecular Function (MF), Biological Process (BP), and Cellular Component (CC). Additionally, since proteins have many functions, function prediction represents a multi-label and multi-class problem. We developed a new method to predict protein function from sequence. To this end, natural language model was used to generate word embedding of sequence and learn features from it by deep learning, and additional features to locate every protein. Our method uses the dependencies between GO classes as background information to construct a deep learning model. We evaluate our method using the standards established by the Computational Assessment of Function Annotation (CAFA) and have noticeable improvement over several algorithms, such as FFPred, DeepGO, GoFDR and other methods compared on the CAFA3 datasets. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Convolution Neural Network; Natural Language Process; Protein Function Prediction; Protein-protein Interaction Network; Sequence Similarity Profile; Protein; Proteins; Deep Learning; Embeddings; Forecasting; Natural Language Processing Systems; Proteins; Background Information; Cellular Components; Computational Functions; Function Prediction; Functional Characteristics; Multi-class Problems; Natural Language Model; Protein Function Prediction; Learning Systems; Protein; Algorithm; Amino Acid Sequence; Chemistry; Protein Analysis; Protein Database; Algorithms; Amino Acid Sequence; Databases, Protein; Deep Learning; Protein Interaction Maps},
	keywords = {Deep learning; Embeddings; Forecasting; Natural language processing systems; Proteins; Background information; Cellular components; Computational functions; Function prediction; Functional characteristics; Multi-class problems; Natural language model; Protein function prediction; Learning systems; protein; algorithm; amino acid sequence; chemistry; protein analysis; protein database; Algorithms; Amino Acid Sequence; Databases, Protein; Deep Learning; Protein Interaction Maps},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 27}
}

@CONFERENCE{Jiang20208366,
	author = {Jiang, Chen and Dehghan, Masood and Jägersand, Martin},
	title = {Understanding contexts inside robot and human manipulation tasks through vision-language model and ontology system in video streams},
	year = {2020},
	journal = {IEEE International Conference on Intelligent Robots and Systems},
	pages = {8366 - 8372},
	doi = {10.1109/IROS45743.2020.9340905},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094109055&doi=10.1109%2FIROS45743.2020.9340905&partnerID=40&md5=8f2b1e4a2ac15acd484c559e19070193},
	abstract = {Manipulation tasks in daily life, such as pouring water, unfold through human intentions. Being able to process contextual knowledge from these Activities of Daily Living (ADLs) over time can help us understand manipulation intentions, which are essential for an intelligent robot to transition smoothly between various manipulation actions. In this paper, to model the intended concepts of manipulation, we present a vision dataset under a strictly constrained knowledge domain for both robot and human manipulations, where manipulation concepts and relations are stored by an ontology system in a taxonomic manner. Furthermore, we propose a scheme to generate a combination of visual attentions and an evolving knowledge graph filled with commonsense knowledge. Our scheme works with real-world camera streams and fuses an attention-based Vision-Language model with the ontology system. The experimental results demonstrate that the proposed scheme can successfully represent the evolution of an intended object manipulation procedure for both robots and humans. The proposed scheme allows the robot to mimic human-like intentional behaviors by watching real-time videos. We aim to develop this scheme further for real-world robot intelligence in Human-Robot Interaction. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Agricultural Robots; Behavioral Research; Computational Linguistics; Computer Hardware Description Languages; Intelligent Robots; Knowledge Representation; Ontology; Video Streaming; Activities Of Daily Living (adls); Commonsense Knowledge; Contextual Knowledge; Human Manipulation; Knowledge Domains; Manipulation Task; Object Manipulation; Robot Intelligences; Social Robots},
	keywords = {Agricultural robots; Behavioral research; Computational linguistics; Computer hardware description languages; Intelligent robots; Knowledge representation; Ontology; Video streaming; Activities of daily living (ADLs); Commonsense knowledge; Contextual knowledge; Human manipulation; Knowledge domains; Manipulation task; Object manipulation; Robot intelligences; Social robots},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9}
}

@CONFERENCE{Fu2020,
	author = {Fu, Taotao and Kong, Dezhen and Jin, Yilin and Jin, Canghong and Zheng, Min and Chen, Bing and Zhu, Danyan},
	title = {Criminal Punishment Prediction Based on Fuzzy Document Vector Model},
	year = {2020},
	journal = {Journal of Physics: Conference Series},
	volume = {1631},
	number = {1},
	pages = {},
	doi = {10.1088/1742-6596/1631/1/012111},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092466379&doi=10.1088%2F1742-6596%2F1631%2F1%2F012111&partnerID=40&md5=789b580d7b455db48e247f3e03d20cf7},
	abstract = {The drug addict problem is more and more critical to the world during current periods, analysing the criminal process and predicting the final punishment from judgments report is an interesting and important task. Existing studies on text analysis and language model supply methods based on special feature selection and ontology models generation which need external knowledge by human experts. In this paper, however, we creatively leveraged such text data onto prediction in the public judgments without human business. We propose a combined framework to capture the prediction problem by considering both valued based rules and fuzzy document models. This framework contains the complete process as: information extraction, term fuzzy and document vector regression. We setup an experiment on a real-world dataset and compare our model with traditional classification and regression methods. The results show that our model outperforms than others by both RMSE and R squared measures. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Classification (of Information); Crime; Feature Extraction; Forecasting; Regression Analysis; Document Model; Document Vectors; External Knowledge; Language Model; Prediction Problem; Prediction-based; R-squared Measures; Regression Method; Artificial Intelligence},
	keywords = {Classification (of information); Crime; Feature extraction; Forecasting; Regression analysis; Document model; Document vectors; External knowledge; Language model; Prediction problem; Prediction-based; R-squared measures; Regression method; Artificial intelligence},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Gold Open Access}
}

@CONFERENCE{Han20203064,
	author = {Han, Fred X. and Niu, Di and Chen, Haolan and Guo, Weidong and Yan, Shengli and Long, Bowei},
	title = {Meta-Learning for Query Conceptualization at Web Scale},
	year = {2020},
	pages = {3064 - 3073},
	doi = {10.1145/3394486.3403357},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090410958&doi=10.1145%2F3394486.3403357&partnerID=40&md5=7a5ce2b26678f3c43e9f882c8da80a43},
	abstract = {Concepts naturally constitute an abstraction for fine-grained entities and knowledge in the open domain. They enable search engines and recommendation systems to enhance user experience by discovering high-level abstraction of a search query and the user intent behind it. In this paper, we study the problem of query conceptualization, which is to find the most appropriate matching concepts for any given search query from a large pool of pre-defined concepts. We propose a coarse-to-fine approach to first reduce the search space for each query through a shortlisting scheme and then identify the matching concepts using pre-trained language models, which are meta-tuned to our query-concept matching task. Our shortlisting scheme involves using a GRU-based Relevant Words Generator (RWG) to first expand and complete the context of the given query and then shortlisting the candidate concepts through a scoring mechanism based on word overlaps. To accurately identify the most appropriate matching concepts for a query, even when the concepts may have zero verbatim overlaps with the query, we meta-fine-tune a BERT pairwise text-matching model under the Reptile meta-learning algorithm, which achieves zero-shot transfer learning on the conceptualization problem. Our two-stage framework can be trained with data completely derived from a search click graph, without requiring any human labelling efforts. For evaluation, we have constructed a large click graph based on more than $7$ million instances of the click history recorded in Tencent QQ browser and performed the query conceptualization task based on a large ontology with $159,148$ unique concepts. Results from a range of evaluation methods, including an offline evaluation procedure on the click graph, human evaluation, online A/B testing and case studies, have demonstrated the superiority of our approach over a number of competitive pre-trained language models and fine-tuned neural network baselines. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Conceptualization; Information Retrieval; Meta-learning; Query Analysis; Computational Linguistics; Data Mining; Graphic Methods; Query Processing; Search Engines; Transfer Learning; User Experience; Coarse To Fine; Evaluation Methods; High-level Abstraction; Human Evaluation; Human Labelling; Mechanism-based; Offline Evaluation; Search Queries; Learning Algorithms},
	keywords = {Computational linguistics; Data mining; Graphic methods; Query processing; Search engines; Transfer learning; User experience; Coarse to fine; Evaluation methods; High-level abstraction; Human evaluation; Human labelling; Mechanism-based; Offline evaluation; Search queries; Learning algorithms},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@ARTICLE{Tao2020,
	author = {Tao, Jie and Zhou, Lina},
	title = {A Weakly Supervised WordNet-Guided Deep Learning Approach to Extracting Aspect Terms from Online Reviews},
	year = {2020},
	journal = {ACM Transactions on Management Information Systems},
	volume = {11},
	number = {3},
	pages = {},
	doi = {10.1145/3399630},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095110914&doi=10.1145%2F3399630&partnerID=40&md5=eb6c06d5c7bda7e850a061c3ad1b5d1c},
	abstract = {The unstructured nature of online reviews makes it inefficient and inconvenient for prospective consumers to research and use in support of purchase decision making. The aspects of products provide a fine-grained meaningful perspective for understanding and organizing review texts. Traditional aspect term extraction approaches rely on discrete language models that treat words in isolation. Despite that continuous-space language models have demonstrated promise in addressing a wide range of problems, their application in aspect term extraction faces significant challenges. For instance, existing continuous-space language models typically require large collections of labeled data, which remain difficult to obtain in many domains. More importantly, previous methods are largely data driven but overlook the role of human knowledge in guiding model development. To address these limitations, this study designs and develops weakly supervised WordNet-guided deep learning to aspect term extraction. The approach draws on deep-level semantic information from WordNet to guide not only the selection representative seed terms but also the pruning of aspect candidate terms. The weak supervision is provided by a very small set of labeled data. We conduct a comprehensive evaluation of the proposed method using both direct and indirect methods. The evaluation results with Yelp restaurant reviews demonstrate that our proposed method consistently outperforms all baseline methods including discrete models and the state-of-the-art continuous-space language models for aspect term extraction across both direct and indirect evaluations. The research findings have broad research, technical, and practical implications for various stakeholders of online reviews. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Aspect Term Extraction; Continuous-space Language Model; Deep Learning; Semantic Knowledge; Text Analytics; Computational Linguistics; Decision Making; E-learning; Extraction; Labeled Data; Ontology; Semantics; Comprehensive Evaluation; Continuous Spaces; Direct And Indirect Methods; Evaluation Results; Learning Approach; Model Development; Restaurant Reviews; Semantic Information; Deep Learning},
	keywords = {Computational linguistics; Decision making; E-learning; Extraction; Labeled data; Ontology; Semantics; Comprehensive evaluation; Continuous spaces; Direct and indirect methods; Evaluation results; Learning approach; Model development; Restaurant reviews; Semantic information; Deep learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10}
}

@CONFERENCE{Jiang20201447,
	author = {Jiang, Chen and Jägersand, Martin},
	title = {Bridging Visual Perception with Contextual Semantics for Understanding Robot Manipulation Tasks},
	year = {2020},
	journal = {IEEE International Conference on Automation Science and Engineering},
	volume = {2020-August},
	pages = {1447 - 1452},
	doi = {10.1109/CASE48305.2020.9216770},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094164796&doi=10.1109%2FCASE48305.2020.9216770&partnerID=40&md5=ecdffea342d89820ff110175eca432ec},
	abstract = {Understanding manipulation scenarios allows intelligent robots to plan for appropriate actions to complete a manipulation task successfully. It is essential for intelligent robots to semantically interpret manipulation knowledge by describing entities, relations and attributes in a structural manner. In this paper, we propose an implementing framework to generate high-level conceptual dynamic knowledge graphs from video clips. A combination of a Vision-Language model and an ontology system, in correspondence with visual perception and contextual semantics, is used to represent robot manipulation knowledge with Entity-Relation-Entity (E-R-E) and Entity-Attribute-Value (E-A-V) tuples. The proposed method is flexible and well-versed. Using the framework, we present a case study where robot performs manipulation actions in a kitchen environment, bridging visual perception with contextual semantics using the generated dynamic knowledge graphs. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Representation; Semantics; Vision; Visual Languages; Attribute Values; Contextual Semantics; Knowledge Graphs; Language Model; Manipulation Task; Ontology System; Robot Manipulation; Visual Perception; Intelligent Robots},
	keywords = {Knowledge representation; Semantics; Vision; Visual languages; Attribute values; Contextual semantics; Knowledge graphs; Language model; Manipulation task; Ontology system; Robot manipulation; Visual perception; Intelligent robots},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4; All Open Access; Green Accepted Open Access; Green Open Access}
}

@ARTICLE{Chang2020160,
	author = {Chang, David and Hong, Woo-suk and Taylor, Richard Andrew},
	title = {Generating contextual embeddings for emergency department chief complaints},
	year = {2020},
	journal = {JAMIA Open},
	volume = {3},
	number = {2},
	pages = {160 - 166},
	doi = {10.1093/jamiaopen/ooaa022},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103742846&doi=10.1093%2Fjamiaopen%2Fooaa022&partnerID=40&md5=29d816c313681b5aaaaf8562df14b961},
	abstract = {Objective: We learn contextual embeddings for emergency department (ED) chief complaints using Bidirectional Encoder Representations from Transformers (BERT), a state-of-the-art language model, to derive a compact and computationally useful representation for free-text chief complaints. Materials and methods: Retrospective data on 2.1 million adult and pediatric ED visits was obtained from a large healthcare system covering the period of March 2013 to July 2019. A total of 355 497 (16.4%) visits from 65 737 (8.9%) patients were removed for absence of either a structured or unstructured chief complaint. To ensure adequate training set size, chief complaint labels that comprised less than 0.01%, or 1 in 10 000, of all visits were excluded. The cutoff threshold was incremented on a log scale to create seven datasets of decreasing sparsity. The classification task was to predict the provider-assigned label from the free-text chief complaint using BERT, with Long Short-Term Memory (LSTM) and Embeddings from Language Models (ELMo) as baselines. Performance was measured as the Top-k accuracy from k ¼ 1:5 on a hold-out test set comprising 5% of the samples. The embedding for each free-text chief complaint was extracted as the final 768-dimensional layer of the BERT model and visualized using t-distributed stochastic neighbor embedding (t-SNE). Results: The models achieved increasing performance with datasets of decreasing sparsity, with BERT outperforming both LSTM and ELMo. The BERT model yielded Top-1 accuracies of 0.65 and 0.69, Top-3 accuracies of 0.87 and 0.90, and Top-5 accuracies of 0.92 and 0.94 on datasets comprised of 434 and 188 labels, respectively. Visualization using t-SNE mapped the learned embeddings in a clinically meaningful way, with related concepts embedded close to each other and broader types of chief complaints clustered together. Discussion: Despite the inherent noise in the chief complaint label space, the model was able to learn a rich representation of chief complaints and generate reasonable predictions of their labels. The learned embeddings accurately predict provider-assigned chief complaint labels and map semantically similar chief complaints to nearby points in vector space. Conclusion: Such a model may be used to automatically map free-text chief complaints to structured fields and to assist the development of a standardized, data-driven ontology of chief complaints for healthcare institutions. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Chief Complaint; Emergency Medicine; Machine Learning; Natural Language Processing; Adult; Article; Child; Embedding; Emergency Medicine; Emergency Ward; Female; Health Care System; Human; Machine Learning; Male; Natural Language Processing; Noise; Ontology; Prediction; Retrospective Study; Short Term Memory; Stochastic Model},
	keywords = {adult; article; child; embedding; emergency medicine; emergency ward; female; health care system; human; machine learning; male; natural language processing; noise; ontology; prediction; retrospective study; short term memory; stochastic model},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Sai Sharath2020,
	author = {Sai Sharath, Japa and Rekabdar, Banafsheh},
	title = {Question Answering over Knowledge Base using Language Model Embeddings},
	year = {2020},
	journal = {Proceedings of the International Joint Conference on Neural Networks},
	pages = {},
	doi = {10.1109/IJCNN48605.2020.9206698},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093833070&doi=10.1109%2FIJCNN48605.2020.9206698&partnerID=40&md5=b38f01ccb83edf9fb6b1001f74af82d3},
	abstract = {Knowledge Base, represents facts about the world, often in some form of subsumption ontology, rather than implicitly, embedded in procedural code, the way a conventional computer program does. While there is a rapid growth in knowledge bases, it poses a challenge of retrieving information from them. Knowledge Base Question Answering is one of the promising approaches for extracting substantial knowledge from Knowledge Bases. Unlike web search, Question Answering over a knowledge base gives accurate and concise results, provided that natural language questions can be understood and mapped precisely to an answer in the knowledge base. However, some of the existing embedding-based methods for knowledge base question answering systems ignore the subtle correlation between the question and the Knowledge Base (e.g., entity types, relation paths, and context) and suffer from the Out Of Vocabulary problem. In this paper, we focused on using a pre-trained language model for the Knowledge Base Question Answering task. Firstly, we used Bert base uncased for the initial experiments. We further fine-tuned these embeddings with a twoway attention mechanism from the knowledge base to the asked question and from the asked question to the knowledge base answer aspects. Our method is based on a simple Convolutional Neural Network architecture with a Multi-Head Attention mechanism to represent the asked question dynamically in multiple aspects. Our experimental results show the effectiveness and the superiority of the Bert pre-trained language model embeddings for question answering systems on knowledge bases over other well-known embedding methods. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Bert; Kbqa; Knowledge Base Question Answering; Language Model; Multi-head Attention; Computational Linguistics; Convolutional Neural Networks; Embeddings; Information Retrieval; Natural Language Processing Systems; Network Architecture; Attention Mechanisms; Conventional Computers; Embedding Method; Natural Language Questions; Procedural Codes; Question Answering; Question Answering Systems; Question Answering Task; Knowledge Based Systems},
	keywords = {Computational linguistics; Convolutional neural networks; Embeddings; Information retrieval; Natural language processing systems; Network architecture; Attention mechanisms; Conventional computers; Embedding method; Natural language questions; Procedural codes; Question Answering; Question answering systems; Question Answering Task; Knowledge based systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 10; All Open Access; Green Accepted Open Access; Green Open Access}
}

@CONFERENCE{Alamsyah202032,
	author = {Alamsyah, Andry and Bastikarana, Rafa Syafiq and Ramadhanti, Alya Rysda and Widiyanesti, Sri},
	title = {Recognizing Personality from Social Media Linguistic Cues: A Case Study of Brand Ambassador Personality},
	year = {2020},
	pages = {32 - 35},
	doi = {10.1109/ICoICT49345.2020.9166221},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091032378&doi=10.1109%2FICoICT49345.2020.9166221&partnerID=40&md5=5bbc0e86df816c44ea4c031a13b59e60},
	abstract = {The burgeoning need of a brand ambassador (BA) as a company representative begin to rise in recent year. The phenomena followed by the increase of method to select the most suitable BA. The universal way of selecting one appropriate ambassador is by understanding their personality, therefore, measurement of a BA personality considered as one way to characterize a company credibility. This research proposes to design a method of measuring the BA personality from their social media data in Bahasa Indonesia. We enrich the methodology to measure human personality using the ontology modeling approach. The ontology model constructed under the ngram language model which provides a rapid and effective way of measuring a BA personality. The results of a BA personality measurement allow the utilization to portray of how an ambassador represent their brand and interact with their customer. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Brand Ambassador; Ontology; Personality Measurement; Ontology; Indonesia; N-gram Language Models; Ontology Model; Social Media; Social Media Datum; Social Networking (online)},
	keywords = {Ontology; Indonesia; N-gram language models; Ontology model; Social media; Social media datum; Social networking (online)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Wang2020868,
	author = {Wang, Chengyu and He, Xiaofeng and Gong, Xueqing and Zhou, Aoying Yingn},
	title = {Word Embedding Projection Models for Hypernymy Relation Prediction; 面向上下位关系预测的词嵌入投影模型},
	year = {2020},
	journal = {Jisuanji Xuebao/Chinese Journal of Computers},
	volume = {43},
	number = {5},
	pages = {868 - 883},
	doi = {10.11897/SP.J.1016.2020.00868},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089896380&doi=10.11897%2FSP.J.1016.2020.00868&partnerID=40&md5=c7542bcd16aee344db4d7e69dc729626},
	abstract = {A hypernymy ("is-a") relation is an important concept in the field of Natural Language Processing (NLP) and computational linguistics. This type of semantic relations is often used to describe the subordination relation between two semantic concepts, such as "(dog, animal)", "(rose, plant)" and "(sofa, furniture)". The accurate extraction and prediction of hypernymy relations from massive text corpora is extremely important for mining the inherent hierarchy among semantic concepts and entities, as well as building large-scale semantic networks, ontologies, knowledge graphs and other knowledge-intensive information systems. This task is also beneficial to a variety of downstream NLP tasks, including natural language inference, personalized recommendation, query understanding and so on. Most traditional hypernymy prediction algorithms rely on relatively fixed language patterns, such as the Hearst patterns in English. These approaches have several potential drawbacks such as the low coverage of relations in texts and the high degree of manual intervention required to train these machine learning models. In addition, the textual patterns used for hypernymy extraction are highly correlated with the characteristics of the target language itself. For languages with low regularity in text expressions such as Chinese, pattern-based methods are not sufficiently accurate. Distributional models for hypernymy prediction are more precise and can avoid the occurrence sparsity problem of concepts, but likely to suffer from the "lexical memorization" problem. With the rapid development of deep learning techniques in NLP, word embeddings which learned from neural language models are frequently employed to model the semantic relations between words, without a lot of linguistic knowledge. Especially, word embedding projection models learn how to map the embeddings of hyponyms to those of their hypernyms, modeling the representations of hypernymy relations in the embedding space explicitly. In view of existing classical and latest research, this paper introduces the development process and the latest breakthrough of word embedding projection models, in order to predict hypernymy relations accurately. We give a unified mathematical framework of these models and discuss how these models are developed, including the improvements of projection learning based on deep iterative, transductive and adversarial learning. Specifically, iterative learning methods consider the situation where hypernymy relations from different domains have diverse representations, and employ iterative, semi-supervised learning technique to learn multiple projection matrices from the embeddings of hyponyms to hypernyms. Transductive models learn the projection matrices of hypernymy and non-hypernymy relations at the same time, and consider the semantic differences between concepts in the training and testing sets. Because there are a large number of hypernymy relations in modern taxonomies, deep adversarial models learn neural network-based projection models over taxonomies and training sets, and train adversarial classifiers to make the two neural networks to learn from each other. In the experiments, we evaluate all these projection learning models under a unified framework, including multiple general-domain and domain-specific benchmark datasets in English and Chinese languages. We also compare the advantages and disadvantages of these projection learning models under different learning circumstances. Finally, the future research directions of this work are discussed, which focus on domain-specific and long-tail hypernymy prediction. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Chinese Language Characteristics; Hypernymy Relation; Relation Extraction; Word Embedding; Word Embedding Projection Model; Computational Linguistics; Deep Learning; Embeddings; Extraction; Forecasting; Iterative Methods; Knowledge Representation; Large Scale Systems; Natural Language Processing Systems; Neural Networks; Semantics; Semi-supervised Learning; Taxonomies; Text Mining; Distributional Models; Future Research Directions; Machine Learning Models; Mathematical Frameworks; Natural Language Processing; Personalized Recommendation; Prediction Algorithms; Semi-supervised Learning Techniques; Learning Systems},
	keywords = {Computational linguistics; Deep learning; Embeddings; Extraction; Forecasting; Iterative methods; Knowledge representation; Large scale systems; Natural language processing systems; Neural networks; Semantics; Semi-supervised learning; Taxonomies; Text mining; Distributional models; Future research directions; Machine learning models; Mathematical frameworks; NAtural language processing; Personalized recommendation; Prediction algorithms; Semi-supervised learning techniques; Learning systems},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Strodthoff20202401,
	author = {Strodthoff, Nils and Wagner, Patrick and Wenzel, Markus A. and Samek, Wojciech},
	title = {UDSMProt: Universal deep sequence models for protein classification},
	year = {2020},
	journal = {Bioinformatics},
	volume = {36},
	number = {8},
	pages = {2401 - 2409},
	doi = {10.1093/bioinformatics/btaa003},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084028155&doi=10.1093%2Fbioinformatics%2Fbtaa003&partnerID=40&md5=d00a19dbc68a649f35d9cbf4c71448b2},
	abstract = {Motivation: Inferring the properties of a protein from its amino acid sequence is one of the key problems in bioinformatics. Most state-of-The-Art approaches for protein classification are tailored to single classification tasks and rely on handcrafted features, such as position-specific-scoring matrices from expensive database searches. We argue that this level of performance can be reached or even be surpassed by learning a task-Agnostic representation once, using self-supervised language modeling, and transferring it to specific tasks by a simple fine-Tuning step. Results: We put forward a universal deep sequence model that is pre-Trained on unlabeled protein sequences from Swiss-Prot and fine-Tuned on protein classification tasks. We apply it to three prototypical tasks, namely enzyme class prediction, gene ontology prediction and remote homology and fold detection. The proposed method performs on par with state-of-The-Art algorithms that were tailored to these specific tasks or, for two out of three tasks, even outperforms them. These results stress the possibility of inferring protein properties from the sequence alone and, on more general grounds, the prospects of modern natural language processing methods in omics. Moreover, we illustrate the prospects for explainable machine learning methods in this field by selected case studies. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Protein; Proteins; Protein; Algorithm; Amino Acid Sequence; Genetics; Protein Database; Software; Algorithms; Amino Acid Sequence; Databases, Protein; Proteins; Software},
	keywords = {protein; algorithm; amino acid sequence; genetics; protein database; software; Algorithms; Amino Acid Sequence; Databases, Protein; Proteins; Software},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 142; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Loureiro20203514,
	author = {Loureiro, Daniel and Camacho-Collados, José},
	title = {Don't neglect the obvious: On the role of unambiguous words in word sense disambiguation},
	year = {2020},
	pages = {3514 - 3520},
	doi = {10.18653/v1/2020.emnlp-main.283},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104584462&doi=10.18653%2Fv1%2F2020.emnlp-main.283&partnerID=40&md5=6bf21185139d79fc97022b146a02ef03},
	abstract = {State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models. This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet). At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora. In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus. We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD. © 2024 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Natural Language Processing Systems; 'current; Language Model; Large Corpora; Power; Propagation Method; Sense Inventories; Simple Method; State-of-the-art Methods; Word Sense Disambiguation; Wordnet; Ontology},
	keywords = {Computational linguistics; Natural language processing systems; 'current; Language model; Large corpora; Power; Propagation method; Sense inventories; SIMPLE method; State-of-the-art methods; Word Sense Disambiguation; Wordnet; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 9; All Open Access; Green Accepted Open Access; Green Open Access; Hybrid Gold Open Access}
}

@ARTICLE{Yin2020150,
	author = {Yin, Yanqin and Sun, Xiaodong and Lv, Huanhuan and Wang, Pikun and Ma, Hongwei and Yang, Dongqiang},
	title = {Using semantic relationships to enhance neural word embeddings},
	year = {2020},
	pages = {150 - 154},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099555187&partnerID=40&md5=f64a66d5529d633541c5d7559984c145},
	abstract = {Neural language models have significantly improved current natural language understanding tasks. However, distributional semantics, derived from neural language models is less competitive in computing semantic relatedness or similarity than other taxonomy-based methods. Although current researches seek to exploit the handcrafted semantic knowledge in ontology to improve distributional semantics, they often ignore distinguishing different functions of semantic relationships in updating or retrofitting neural word embeddings. This paper proposes retrofitting neural word embedding through semantic relationships encoded in semantic networks such as WordNet and Roget's thesaurus. We employ the hypernym/hyponym relationships to modify the asymmetric distance measure in retrofitting neural embeddings, which can fully transfer the hierarchical semantic information contained in semantic networks. In the evaluation with the gold-standard data sets, our method achieved the Spearman correlation value of 0.80, which is about 8% higher than the state-of-the-art methods in the literature. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Neural Network Word Embeddings; Semantic Relationship; Semantic Similarity; Computational Linguistics; Embeddings; Knowledge Representation; Natural Language Processing Systems; Ontology; Retrofitting; Asymmetric Distances; Distributional Semantics; Natural Language Understanding; Semantic Information; Semantic Relatedness; Semantic Relationships; Spearman Correlation; State-of-the-art Methods; Semantics},
	keywords = {Computational linguistics; Embeddings; Knowledge representation; Natural language processing systems; Ontology; Retrofitting; Asymmetric distances; Distributional semantics; Natural language understanding; Semantic information; Semantic relatedness; Semantic relationships; Spearman correlation; State-of-the-art methods; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Sankhe2020243,
	author = {Sankhe, Pranav and Khabiri, Elham and Agrawal, Bhavna and Li, Yingjie},
	title = {TableCNN: Deep learning framework for learning tabular data},
	year = {2020},
	journal = {CEUR Workshop Proceedings},
	volume = {2788},
	pages = {243 - 244},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099345699&partnerID=40&md5=90b31eab978be5a5697715dd5d1bdcaf},
	abstract = {Databases and tabular data are among the most common and rapidly growing resources. But many of these are poorly annotated (lack sufficient metadata), and are filled with domain specific jargon and alpha-numeric codes. Because of the domain specific jargon, no pre-trained language model could be applied readily to encode the cell content. We propose a deep learning based framework, TableCNN, that encodes the semantics of the surrounding cells to predict the meaning of the columns. We propose application of Byte Pair Encoding (BPE)[5] to create tokens for each cell and treat each cell as a phrase of existing tokens. Once tokenized, we process it with a CNN network to develop a classifier. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Cells; Cytology; Encoding (symbols); Ontology; Byte-pair Encoding; Cnn Network; Domain Specific; Language Model; Learning Frameworks; Numeric Codes; Tabular Data; Deep Learning},
	keywords = {Cells; Cytology; Encoding (symbols); Ontology; Byte-pair encoding; CNN network; Domain specific; Language model; Learning frameworks; Numeric codes; Tabular data; Deep learning},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Lomov2020919,
	author = {Lomov, Pavel A. and Malozemova, Marina and Shishaev, Maxim},
	title = {Training and Application of Neural-Network Language Model for Ontology Population},
	year = {2020},
	journal = {Advances in Intelligent Systems and Computing},
	volume = {1295},
	pages = {919 - 926},
	doi = {10.1007/978-3-030-63319-6_85},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098202098&doi=10.1007%2F978-3-030-63319-6_85&partnerID=40&md5=aa6941ffee156c7c0eb4e1d516573e82},
	abstract = {This paper considers one of the subtasks of ontology learning - the ontology population, which implies the extension of existing ontology by new instances without changing the ontology structure. A brief overview of existing ontology learning approaches and their software implementations is presented. A highly automated technology for ontology population based on training and application of the neural network language model to identify and extract potential instances of ontology classes from domain texts is proposed. The main stages of its application, as well as the results of its experimental evaluation and the main directions of its further improvement are considered. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Neural Network; Ontology Learning; Ontology Population; Computational Linguistics; Computational Methods; Intelligent Systems; Ontology; Petroleum Reservoir Evaluation; Software Engineering; Automated Technology; Experimental Evaluation; Its Applications; Network Language; Ontology Learning; Ontology Population; Software Implementation; Subtasks; Neural Networks},
	keywords = {Computational linguistics; Computational methods; Intelligent systems; Ontology; Petroleum reservoir evaluation; Software engineering; Automated technology; Experimental evaluation; ITS applications; Network language; Ontology learning; Ontology Population; Software implementation; Subtasks; Neural networks},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@CONFERENCE{Bourgonje20201,
	author = {Bourgonje, Peter and Breit, Anna and Khvalchik, Maria and Mireles, Víctor and Moreno-Schneider, Julián and Revenko, Artem and Rehm, Georg},
	title = {Automatic induction of named entity classes from legal text corpora},
	year = {2020},
	journal = {CEUR Workshop Proceedings},
	volume = {2722},
	pages = {1 - 11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097612512&partnerID=40&md5=ea4e40c0f632531ffc652cd381ee475e},
	abstract = {Named Entity Recognition tools and datasets are widely used. The standard pre-trained models, however often do not cover specific application needs as these models are too generic. We introduce a methodology to automatically induce fine-grained classes of named entities for the legal domain. Specifically, given a corpus which has been annotated with instances of coarse entity classes, we show how to induce fine-grained, domain specific (sub-)classes. The method relies on predictions of the masked tokens generated by a pre-trained language model. These predictions are then collected and clustered. The clusters are then taken as the new candidate classes. We develop an implementation of the introduced method and experiment with a large legal corpus in German language that is manually annotated with almost 54,000 named entities. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Knowledge Discovery; Language Model; Named Entity Recognition; Ontology Induction; Data Handling; Linked Data; Automatic Induction; Domain Specific; German Language; Language Model; Legal Corpus; Legal Domains; Named Entities; Named Entity Recognition; Natural Language Processing Systems},
	keywords = {Data handling; Linked data; Automatic induction; Domain specific; German language; Language model; Legal corpus; Legal domains; Named entities; Named entity recognition; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{2020,
	title = {CIKMW 2020 - Proceedings of the CIKM 2020 Workshops, co-located with 29th ACM International Conference on Information and Knowledge Management, CIKM 2020},
	year = {2020},
	journal = {CEUR Workshop Proceedings},
	volume = {2699},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097556916&partnerID=40&md5=5e090c0120680084e593d3315190f170},
	abstract = {The proceedings contain 46 papers. The topics discussed include: guided-LIME: structured sampling based hybrid approach towards explaining blackbox machine learning models; now you see me (CME): concept-based model extraction; more is not always better: the negative impact of a-box materialization on RDF2VEC knowledge graph embeddings; symbolic vs sub-symbolic ai methods: friends or enemies?; creative storytelling with language models and knowledge graphs; simplifying architecture search for graph neural network; an adaptive semantic stream reasoning framework for deep neural networks; neuro-symbolic visual reasoning for multimedia event processing: overview, prospects and challenges; and leveraging a domain ontology in (neural) learning from heterogeneous data. © 2020 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2020,
	title = {19th International Semantic Web Conference, ISWC 2020},
	year = {2020},
	journal = {Lecture Notes in Computer Science},
	volume = {12507 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096607437&partnerID=40&md5=0382e28c1777a71f751341c184247a84},
	abstract = {The proceedings contain 81 papers. The special focus in this conference is on Semantic Web. The topics include: Enhancing Online Knowledge Graph Population with Semantic Knowledge; Extending SPARQL with Similarity Joins; exCut: Explainable Embedding-Based Clustering over Knowledge Graphs; cost- and Robustness-Based Query Optimization for Linked Data Fragments; GeoSPARQL+: Syntax, Semantics and System for Integrated Querying of Graph, Raster and Vector Data; funMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation; KnowlyBERT - Hybrid Query Answering over Language Models and Knowledge Graphs; Generating Referring Expressions from RDF Knowledge Graphs for Data Linking; prevalence and Effects of Class Hierarchy Precompilation in Biomedical Ontologies; PNEL: Pointer Network Based End-To-End Entity Linking over Knowledge Graphs; tab2Know: Building a Knowledge Base from Tables in Scientific Papers; Deciding SHACL Shape Containment Through Description Logics Reasoning; rule-Guided Graph Neural Networks for Recommender Systems; leveraging Semantic Parsing for Relation Linking over Knowledge Bases; NABU – Multilingual Graph-Based Neural RDF Verbalizer; fantastic Knowledge Graph Embeddings and How to Find the Right Space for Them; LM4KG: Improving Common Sense Knowledge Graphs with Language Models; SHACL Satisfiability and Containment; contextual Propagation of Properties for Knowledge Graphs: A Sentence Embedding Based Approach; In-Database Graph Analytics with Recursive SPARQL; explainable Link Prediction for Emerging Entities in Knowledge Graphs; from Syntactic Structure to Semantic Relationship: Hypernym Extraction from Definitions by Recurrent Neural Networks Using the Part of Speech Information; focused Query Expansion with Entity Cores for Patient-Centric Health Search; generating Expressive Correspondences: An Approach Based on User Knowledge Needs and A-Box Relation Discovery. © 2020 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2020,
	title = {19th International Semantic Web Conference, ISWC 2020},
	year = {2020},
	journal = {Lecture Notes in Computer Science},
	volume = {12506 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096579820&partnerID=40&md5=2daf75e2d474ad964eb1d745a3e949b0},
	abstract = {The proceedings contain 81 papers. The special focus in this conference is on Semantic Web. The topics include: Enhancing Online Knowledge Graph Population with Semantic Knowledge; Extending SPARQL with Similarity Joins; exCut: Explainable Embedding-Based Clustering over Knowledge Graphs; cost- and Robustness-Based Query Optimization for Linked Data Fragments; GeoSPARQL+: Syntax, Semantics and System for Integrated Querying of Graph, Raster and Vector Data; funMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation; KnowlyBERT - Hybrid Query Answering over Language Models and Knowledge Graphs; Generating Referring Expressions from RDF Knowledge Graphs for Data Linking; prevalence and Effects of Class Hierarchy Precompilation in Biomedical Ontologies; PNEL: Pointer Network Based End-To-End Entity Linking over Knowledge Graphs; tab2Know: Building a Knowledge Base from Tables in Scientific Papers; Deciding SHACL Shape Containment Through Description Logics Reasoning; rule-Guided Graph Neural Networks for Recommender Systems; leveraging Semantic Parsing for Relation Linking over Knowledge Bases; NABU – Multilingual Graph-Based Neural RDF Verbalizer; fantastic Knowledge Graph Embeddings and How to Find the Right Space for Them; LM4KG: Improving Common Sense Knowledge Graphs with Language Models; SHACL Satisfiability and Containment; contextual Propagation of Properties for Knowledge Graphs: A Sentence Embedding Based Approach; In-Database Graph Analytics with Recursive SPARQL; explainable Link Prediction for Emerging Entities in Knowledge Graphs; from Syntactic Structure to Semantic Relationship: Hypernym Extraction from Definitions by Recurrent Neural Networks Using the Part of Speech Information; focused Query Expansion with Entity Cores for Patient-Centric Health Search; generating Expressive Correspondences: An Approach Based on User Knowledge Needs and A-Box Relation Discovery. © 2020 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2020,
	title = {18th Russian Conference on Artificial Intelligence, RCAI 2020},
	year = {2020},
	journal = {Lecture Notes in Computer Science},
	volume = {12412 LNAI},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092166935&partnerID=40&md5=07a0a730936442338113d98bcb0a6637},
	abstract = {The proceedings contain 35 papers. The special focus in this conference is on Artificial Intelligence. The topics include: Incremental structure-evolving intelligent systems with advanced interpretational properties; on analytical solutions to the problems of maintaining local consistency; abduction with estimates for statements in fuzzy propositional logic; complex graphs in the modeling of multi-agent systems: From goal-resource networks to fuzzy metagraphs; automating hierarchical subject index construction for scientific documents; automatic labelling of genre-specific collections for word sense disambiguation in russian; revealing implicit relations in russian legal texts; detection of social media users who lead a healthy lifestyle; the influence of different methods on the quality of the russian-tatar neural machine translation; ontology-controlled geometric solver; the combined method of automated knowledge acquisition from various sources: The features of development and experimental research of the temporal version; application of the bert language model for sentiment analysis of social network posts; keyword extraction approach based on probabilistic-entropy, graph, and neural network methods; method of selecting experts based on analysis of large unstructured data and their relations; the study of argumentative relations in popular science discourse; method for detecting text markers of depression and depressiveness; distributional models in the task of hypernym discovery; analysis of the persuasiveness of argumentation in popular science texts; intelligent information search method based on a compositional ontological approach; format and usage model of security patterns in ontology-driven threat modelling; assessment of the technological process condition based on the assembly of deep recurrent neural networks; intelligent systems with restricted autonomy. © 2020 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Caselli2020443,
	author = {Caselli, Ashley and Daponte, Vincenzo and Falquet, Gilles and Métral, Claudine},
	title = {A rule language model for subsurface data refinement},
	year = {2020},
	pages = {443 - 452},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091041936&partnerID=40&md5=4476f2481036cc7209ea8932734ebc91},
	abstract = {The automated regulation compliance checking in the domain of Architecture, Engineering, and Construction (AEC) still presents many challenges to face. The completeness and correctness of the input data affect the results of the compliance checking. Refinement operations on data represented through knowledge graphs can help the assessment of these data properties. In this work, a model to represent rules holding complex constraints is proposed and conveyed through an ontology. This model aims to overcome existing expressiveness limitations and to provide a conceptual tool to represent both compliance regulations and domain experts' knowledge encoded through heuristics. These heuristics are used to drive the refinement process needed prior the compliance checking. The proposed model is intended as the first step towards an automatic execution process of both types of rule instances. The model has been assessed by representing regulations issued by Swiss entities and domain experts' heuristics targeting data of an area of the city of Geneva. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Digital Storage; Intelligent Computing; Knowledge Acquisition; Knowledge Representation; Architecture , Engineering , And Constructions; Complex Constraints; Compliance Checking; Compliance Regulations; Execution Process; Refinement Operations; Refinement Process; Regulation Compliance; Compliance Control},
	keywords = {Digital storage; Intelligent computing; Knowledge acquisition; Knowledge representation; Architecture , engineering , and constructions; Complex constraints; Compliance checking; Compliance regulations; Execution process; Refinement operations; Refinement process; Regulation compliance; Compliance control},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Neves2020,
	author = {Neves, André and Lamurias, Andre and Couto, Francisco M.},
	title = {Biomedical question answering using extreme multi- label classification and ontologies in the multilingual panorama},
	year = {2020},
	journal = {CEUR Workshop Proceedings},
	volume = {2619},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088226546&partnerID=40&md5=cfc80cfe003db2a08e81087741a08736},
	abstract = {Deep learning models achieve state-of-the-art results in Natural Language Processing (NLP) tasks, such as Question Answering (QA), across different domains, mostly thanks to pre-trained language models such as BERT [1]. However, there is a lack of models designed for NLP tasks in the multilingual panorama, especially in specific domains such as the biomedical sciences, mostly due to the lack of datasets available in non-English languages. In this short paper, we propose the development of a QA system using stateof- the-art deep learning models and combining it with a deep learning Extreme Multi-Label Classification (XMLC) solution along with ontologies, in order to improve the results achieved by the model. The proposed model shall be able to answer biomedical questions in English, Spanish and Portuguese. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Extreme Multi-label Classification; Multilingual; Ontologies; Question Answering; Classification (of Information); Deep Learning; Indexing (of Information); Information Retrieval; Learning Systems; Ontology; Semantics; Biomedical Question Answering; Biomedical Science; Different Domains; Multi Label Classification; Natural Language Processing; Non-english Languages; Question Answering; State Of The Art; Natural Language Processing Systems},
	keywords = {Classification (of information); Deep learning; Indexing (of information); Information retrieval; Learning systems; Ontology; Semantics; Biomedical question answering; Biomedical science; Different domains; Multi label classification; NAtural language processing; Non-English languages; Question Answering; State of the art; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2020,
	title = {16th IFIP WG 12.5 International Conference on Artificial Intelligence Applications and Innovations, AIAI 2020},
	year = {2020},
	journal = {IFIP Advances in Information and Communication Technology},
	volume = {583 IFIP},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086257799&partnerID=40&md5=3f6d516f7ee8c53d88621fd61c87b559},
	abstract = {The proceedings contain 75 papers. The special focus in this conference is on Artificial Intelligence Applications and Innovations. The topics include: Manifold learning for innovation funding: Identification of potential funding recipients; network aggregation to enhance results derived from multiple analytics; PolicyCLOUD: Analytics as a service facilitating efficient data-driven public policy management; demand forecasting of short life cycle products using data mining techniques; Arbitrary scale super-resolution for brain MRI images; knowledge-based fusion for image tampering localization; Transfer learning using convolutional neural network architectures for brain tumor classification from MRI images; a novel learning automata-based strategy to generate melodies from chordal inputs; graph neural networks to advance anticancer drug design; Boosted ensemble learning for anomaly detection in 5G RAN; optimizing self-organizing lists-on-lists using transitivity and pursuit-enhanced object partitioning; task-projected hyperdimensional computing for multi-task learning; cross-domain authorship attribution using pre-trained language models; indoor localization with multi-objective selection of Radiomap models; STDP plasticity in TRN within hierarchical spike timing model of visual information processing; Tensor-based CUDA optimization for ANN inferencing using parallel acceleration on embedded GPU; the random neural network in price predictions; joint multi-object detection and segmentation from an untrimmed video; robust 3D detection in traffic scenario with tracking-based coupling system; Automated MeSH indexing of biomedical literature using contextualized word representations; machine learning for cognitive load classification – A case study on contact-free approach; knowledge-based management and reasoning on cultural and natural touristic routes; ontological foundations of modelling security policies for logical analytics; RDF reasoning on large ontologies: A study on cultural heritage and wikidata. © 2020 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{2020,
	title = {MS-AMLV 2019 - Proceedings of the Masters Symposium on Advances in Data Mining, Machine Learning, and Computer Vision},
	year = {2020},
	journal = {CEUR Workshop Proceedings},
	volume = {2566},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085864024&partnerID=40&md5=b8c30564860fc9b64aa43f6a872f16a4},
	abstract = {The proceedings contain 12 papers. The topics discussed include: 3D reconstruction of 2D sign language dictionaries; generation of memes to engage audience in social media; investigation of complex data distributions for their efficient generation; image recommendation for Wikipedia articles; toward a theoretical framework of terminological saturation for ontology learning from texts; parameterizing human speech generation; building a feature taxonomy of the terms extracted from a text collection; toward language modeling for Ukrainian language; context-based question-answering system for the Ukrainian language; topological approach to Wikipedia page recommendation; and determining sentiment and important properties of Ukrainian language user reviews. © 2020 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Elsayed2020318,
	author = {Elsayed, Mo'men and Elkashef, Nermeen and Hassan, Yasser F.},
	title = {Mapping UML sequence diagram into the web ontology language OWL},
	year = {2020},
	journal = {International Journal of Advanced Computer Science and Applications},
	volume = {11},
	number = {5},
	pages = {318 - 326},
	doi = {10.14569/IJACSA.2020.0110542},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085750191&doi=10.14569%2FIJACSA.2020.0110542&partnerID=40&md5=dacf94d2c3df74115304e43f4f01d73c},
	abstract = {In this paper, we propose a new mapping technique from the OMG's UML modeling language into the Web Ontology Language (OWL) to serve the Semantic Web. UML (Unified Modeling Language) is widely accepted and used as a standardized modeling language in Object-Oriented Analysis (OOA) and Design (OOD) approach by domain experts to model real-world objects in software development. On the other hand, the conceptualization, which is represented in OWL, is designed to process the content of information rather than just present the information. Therefore, the matter of migrating UML to OWL is becoming an energetic research domain. OWL (Web Ontology Language) is a Semantic Web language designed for defining ontologies on the Web. An ontology is a formal specification naming and definition of shared data. This technique describes how to map UML Models into OWL and allows us to keep semantic of UML sequence diagrams such as messages, the sequence of messages, guard invariant, etc. to make data of UML sequence diagrams machine-readable. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Mapping; Ontology; Owl; Sequence Diagram; Uml; Unified Modeling Language; Web Ontology Language; Birds; Mapping; Object Oriented Programming; Ontology; Semantic Web; Software Design; Analysis Approach; Design Approaches; Language Model; Mapping Techniques; Object Oriented Analysis And Design; Ontology's; Sequence Diagrams; Standardized Models; Unified Modeling Language-sequence Diagrams; Web Ontology Language (owl); Unified Modeling Language},
	keywords = {Birds; Mapping; Object oriented programming; Ontology; Semantic Web; Software design; Analysis approach; Design approaches; Language model; Mapping techniques; Object oriented analysis and design; Ontology's; Sequence diagrams; Standardized models; Unified modeling language-sequence diagrams; Web ontology language (OWL); Unified Modeling Language},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6; All Open Access; Gold Open Access}
}

@ARTICLE{MacAvaney2020238,
	author = {MacAvaney, Sean and Cohan, Arman and Goharian, Nazli and Filice, Ross W.},
	title = {Ranking significant discrepancies in clinical reports},
	year = {2020},
	journal = {Lecture Notes in Computer Science},
	volume = {12036 LNCS},
	pages = {238 - 245},
	doi = {10.1007/978-3-030-45442-5_30},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084187747&doi=10.1007%2F978-3-030-45442-5_30&partnerID=40&md5=328854849a8bd728bb85e6440c560068},
	abstract = {Medical errors are a major public health concern and a leading cause of death worldwide. Many healthcare centers and hospitals use reporting systems where medical practitioners write a preliminary medical report and the report is later reviewed, revised, and finalized by a more experienced physician. The revisions range from stylistic to corrections of critical errors or misinterpretations of the case. Due to the large quantity of reports written daily, it is often difficult to manually and thoroughly review all the finalized reports to find such errors and learn from them. To address this challenge, we propose a novel ranking approach, consisting of textual and ontological overlaps between the preliminary and final versions of reports. The approach learns to rank the reports based on the degree of discrepancy between the versions. This allows medical practitioners to easily identify and learn from the reports in which their interpretation most substantially differed from that of the attending physician (who finalized the report). This is a crucial step towards uncovering potential errors and helping medical practitioners to learn from such errors, thus improving patient-care in the long run. We evaluate our model on a dataset of radiology reports and show that our approach outperforms both previously-proposed approaches and more recent language models by 4.5% to 15.4%. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Artificial Intelligence; Computer Science; Computers; Health Concerns; Language Model; Medical Errors; Medical Practitioner; Potential Errors; Radiology Reports; Ranking Approach; Reporting Systems; Errors},
	keywords = {Artificial intelligence; Computer science; Computers; Health concerns; Language model; Medical errors; Medical practitioner; Potential errors; Radiology reports; Ranking approach; Reporting systems; Errors},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Loureiro2020230,
	author = {Loureiro, Daniel and Jorge, Alípio Mário},
	title = {MedLinker: Medical entity linking with neural representations and dictionary matching},
	year = {2020},
	journal = {Lecture Notes in Computer Science},
	volume = {12036 LNCS},
	pages = {230 - 237},
	doi = {10.1007/978-3-030-45442-5_29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084175170&doi=10.1007%2F978-3-030-45442-5_29&partnerID=40&md5=08ff0d67af79695b475d1186a7c95f34},
	abstract = {Progress in the field of Natural Language Processing (NLP) has been closely followed by applications in the medical domain. Recent advancements in Neural Language Models (NLMs) have transformed the field and are currently motivating numerous works exploring their application in different domains. In this paper, we explore how NLMs can be used for Medical Entity Linking with the recently introduced MedMentions dataset, which presents two major challenges: (1) a large target ontology of over 2M concepts, and (2) low overlap between concepts in train, validation and test sets. We introduce a solution, MedLinker, that addresses these issues by leveraging specialized NLMs with Approximate Dictionary Matching, and show that it performs competitively on semantic type linking, while improving the state-of-the-art on the more fine-grained task of concept linking (+4 F1 on MedMentions main task). © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Bioinformatics; Entity Linking; Neural Language Models; Large Dataset; Semantics; Statistical Tests; Dictionary Matching; Different Domains; Language Model; Medical Domains; Natural Language Processing; Neural Representations; Semantic Types; State Of The Art; Natural Language Processing Systems},
	keywords = {Large dataset; Semantics; Statistical tests; Dictionary matching; Different domains; Language model; Medical domains; NAtural language processing; Neural representations; Semantic types; State of the art; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 13}
}

@CONFERENCE{Rutkowski2020118,
	author = {Rutkowski, Szymon and Rychlik, Piotr and Mykowiecka, A.},
	title = {Estimating senses with sets of lexically related words for Polish word sense disambiguation},
	year = {2020},
	pages = {118 - 124},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082494401&partnerID=40&md5=b8d1d7cfea5aeb2a554dc61c6170eb07},
	abstract = {We propose a new algorithm for word sense disambiguation, exploiting data from a WordNet with many types of lexical relations, such as plWordNet for Polish. In this method, sense probabilities in context are approximated with a language model. To estimate the likelihood of a sense appearing amidst the word sequence, the token being disambiguated is substituted with words related lexically to the given sense or words appearing in its WordNet gloss. We test this approach on a set of sense-annotated Polish sentences with a number of neural language models. Our best setup achieves the accuracy score of 55.12% (72.02% when first senses are excluded), up from 51.77% of an existing PageRank-based method. While not exceeding the first (often meaning most frequent) sense baseline in the standard case, this encourages further research on combining WordNet data with neural models. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Ontology; In Contexts; Language Model; Lexical Relations; Neural Models; Plwordnet; Polish Words; Related Word; Word Sense Disambiguation; Natural Language Processing Systems},
	keywords = {Computational linguistics; Ontology; In contexts; Language model; Lexical relations; Neural models; plWordNet; Polish words; Related word; Word Sense Disambiguation; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Loureiro20205682,
	author = {Loureiro, Daniel and Jorge, Alípio Mário},
	title = {Language modelling makes sense: Propagating representations through wordNet for full-coverage word sense disambiguation},
	year = {2020},
	pages = {5682 - 5691},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081104855&partnerID=40&md5=5bfebd7b2a47454afe92d5b35a50d8c0},
	abstract = {Contextual embeddings represent a new generation of semantic representations learned from Neural Language Modelling (NLM) that addresses the issue of meaning conflation hampering traditional word embeddings. In this work, we show that contextual embeddings can be used to achieve unprecedented gains in Word Sense Disambiguation (WSD) tasks. Our approach focuses on creating sense-level embeddings with full-coverage of WordNet, and without recourse to explicit knowledge of sense distributions or task-specific modelling. As a result, a simple Nearest Neighbors (k-NN) method using our representations is able to consistently surpass the performance of previous systems using powerful neural sequencing models. We also analyse the robustness of our approach when ignoring part-of-speech and lemma features, requiring disambiguation against the full sense inventory, and revealing shortcomings to be improved. Finally, we explore applications of our sense embeddings for concept-level analyses of contextual embeddings and their respective NLMs. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Embeddings; Modeling Languages; Nearest Neighbor Search; Ontology; Semantics; Concept Levels; Explicit Knowledge; Language Modelling; Nearest Neighbors; Part Of Speech; Semantic Representation; Sense Inventories; Word-sense Disambiguation; Natural Language Processing Systems},
	keywords = {Computational linguistics; Embeddings; Modeling languages; Nearest neighbor search; Ontology; Semantics; Concept levels; Explicit knowledge; Language modelling; Nearest neighbors; Part Of Speech; Semantic representation; Sense inventories; Word-sense disambiguation; Natural language processing systems},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 100}
}

@ARTICLE{Gromann202029,
	author = {Gromann, Dagmar},
	title = {Neural language models for the multilingual, transcultural, and multimodal Semantic Web},
	year = {2020},
	journal = {Semantic Web},
	volume = {11},
	number = {1},
	pages = {29 - 39},
	doi = {10.3233/SW-190373},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079061830&doi=10.3233%2FSW-190373&partnerID=40&md5=040ed581774761b0fad84d0e21d273f1},
	abstract = {A vision of a truly multilingual Semantic Web has found strong support with the Linguistic Linked Open Data community. Standards, such as OntoLex-Lemon, highlight the importance of explicit linguistic modeling in relation to ontologies and knowledge graphs. Nevertheless, there is room for improvement in terms of automation, usability, and interoperability. Neural Language Models have achieved several breakthroughs and successes considerably beyond Natural Language Processing (NLP) tasks and recently also in terms of multimodal representations. Several paths naturally open up to port these successes to the Semantic Web, from automatically translating linguistic information associated with structured knowledge resources to multimodal question-answering with machine translation. Language is also an important vehicle for culture, an aspect that deserves considerably more attention. Building on existing approaches, this article envisions joint forces between Neural Language Models and Semantic Web technologies for multilingual, transcultural, and multimodal information access and presents open challenges and opportunities in this direction. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Cross-linguistic Modeling; Multilingual Representations; Neural Networks},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Parisi2020355,
	author = {Parisi, Andrew},
	title = {Atomic ontology},
	year = {2020},
	journal = {Synthese},
	volume = {197},
	number = {1},
	pages = {355 - 379},
	doi = {10.1007/s11229-018-1725-8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042586680&doi=10.1007%2Fs11229-018-1725-8&partnerID=40&md5=313e21a95f6732914e2eab4cc39af7da},
	abstract = {The aim of this article is to offer a method for determining the ontological commitments of a formalized theory. The second section shows that determining the consequence relation of a language model-theoretically entails that the ontology of a theory is tied very closely to the variables that feature in that theory. The third section develops an alternative way of determining the ontological commitments of a theory given a proof-theoretic account of the consequence relation for the language that theory is in. It is shown that the proof-theoretic account of ontological commitment does not entail that the ontological commitments of a theory depend on the variables of that theory. The last section of the article discusses how this account of ontological commitment can be used in other philosophical projects such as Wright’s (Frege’s conception of numbers as objects, Aberdeen University Press, Aberdeen, 1983) abstractionism. The article concludes with a discussion of the upshots of adopting the proof-theoretic account of ontological commitment for ontology generally. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Ontological Commitment; Proof-theory; Quantification; Quine’s Dictum},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@ARTICLE{Abdulhameed2019,
	author = {Abdulhameed, Tiba Zaki and Zitouni, Imed and Abdel-Qader, Ikhlas M.},
	title = {WASF-VEC: Topology-based word embedding for modern standard Arabic and Iraqi dialect ontology},
	year = {2019},
	journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
	volume = {19},
	number = {2},
	pages = {},
	doi = {10.1145/3345517},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077799973&doi=10.1145%2F3345517&partnerID=40&md5=850aa1ab29c6d9496d84aed188ea68cc},
	abstract = {Word clustering is a serious challenge in low-resource languages. Since words that share semantics are expected to be clustered together, it is common to use a feature vector representation generated from a distributional theory-based word embedding method. The goal of this work is to utilize Modern Standard Arabic (MSA) for better clustering performance of the low-resource Iraqi vocabulary. We began with a new Dialect Fast Stemming Algorithm (DFSA) that utilizes the MSA data. The proposed algorithm achieved 0.85 accuracy measured by the F1 score. Then, the distributional theory-based word embedding method and a new simple, yet effective, feature vector named Wasf-Vec word embedding are tested. Wasf-Vec word representation utilizes a word’s topology features. The difference between Wasf-Vec and distributional theory-based word embedding is that Wasf-Vec captures relations that are not contextually based. The embedding is followed by an analysis of how the dialect words are clustered within other MSA words. The analysis is based on the word semantic relations that are well supported by solid linguistic theories to shed light on the strong and weak word relation representations identified by each embedding method. The analysis is handled by visualizing the feature vector in two-dimensional (2D) space. The feature vectors of the distributional theory-based word embedding method are plotted in 2D space using the t-sne algorithm, while the Wasf-Vec feature vectors are plotted directly in 2D space. A word’s nearest neighbors and the distance-histograms of the plotted words are examined. For validation purpose of the word classification used in this article, the produced classes are employed in Class-based Language Modeling (CBLM). Wasf-Vec CBLM achieved a 7% lower perplexity (pp) than the distributional theory-based word embedding method CBLM. This result is significant when working with low-resource languages. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {2d Visualizing; Arabic Language; Class-based Language Modeling; Dialect; Morphology; Orthographic; Phonology; Topology; Word Embedding; Words Classification; Words Features; Words Ontology; Classification (of Information); Computational Linguistics; Modeling Languages; Morphology; Natural Language Processing Systems; Ontology; Semantics; Topology; Vector Spaces; Vectors; 2d Visualizing; Arabic Languages; Class-based Language Model; Dialect; Orthographic; Phonology; Word Embedding; Words Features; Embeddings},
	keywords = {Classification (of information); Computational linguistics; Modeling languages; Morphology; Natural language processing systems; Ontology; Semantics; Topology; Vector spaces; Vectors; 2D visualizing; Arabic languages; Class-based language model; Dialect; Orthographic; Phonology; Word embedding; Words features; Embeddings},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 5}
}

@ARTICLE{Koroleva2019,
	author = {Koroleva, Anna and Kamath, Sanjay and Paroubek, Patrick},
	title = {Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations},
	year = {2019},
	journal = {Journal of Biomedical Informatics: X},
	volume = {4},
	pages = {},
	doi = {10.1016/j.yjbinx.2019.100058},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074260516&doi=10.1016%2Fj.yjbinx.2019.100058&partnerID=40&md5=5dac8e4a547e838ad8847edeefd7fc83},
	abstract = {Background: Outcomes are variables monitored during a clinical trial to assess the impact of an intervention on humans’ health.Automatic assessment of semantic similarity of trial outcomes is required for a number of tasks, such as detection of outcome switching (unjustified changes of pre-defined outcomes of a trial) and implementation of Core Outcome Sets (minimal sets of outcomes that should be reported in a particular medical domain). Objective: We aimed at building an algorithm for assessing semantic similarity of pairs of primary and reported outcomes.We focused on approaches that do not require manually curated domain-specific resources such as ontologies and thesauri. Methods: We tested several approaches, including single measures of similarity (based on strings, stems and lemmas, paths and distances in an ontology, and vector representations of phrases), classifiers using a combination of single measures as features, and a deep learning approach that consists in fine-tuning pre-trained deep language representations.We tested language models provided by BERT (trained on general-domain texts), BioBERT and SciBERT (trained on biomedical and scientific texts, respectively).We explored the possibility of improving the results by taking into account the variants for referring to an outcome (e.g.the use of a measurement tool name instead on the outcome name; the use of abbreviations).We release an open corpus with annotation for similarity of pairs of outcomes. Results: Classifiers using a combination of single measures as features outperformed the single measures, while deep learning algorithms using BioBERT and SciBERT models outperformed the classifiers.BioBERT reached the best F-measure of 89.75%.The addition of variants of outcomes did not improve the results for the best-performing single measures nor for the classifiers, but it improved the performance of deep learning algorithms: BioBERT achieved an F-measure of93.38%. Conclusions: Deep learning approaches using pre-trained language representations outperformed other approaches for similarity assessment of trial outcomes, without relying on any manually curated domain-specific resources (ontologies and other lexical resources). Addition of variants of outcomes further improved the performance of deep learning algorithms. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Deep Learning; Natural Language Processing; Pre-trained Language Representations; Semantic Similarity; Spin Detection; Trial Outcomes; Deep Learning; Medical Applications; Natural Language Processing Systems; Ontology; Semantics; Natural Language Processing; Pre-trained Language Representations; Semantic Similarity; Spin Detection; Trial Outcomes; Learning Algorithms; Algorithm; Article; Classifier; Clinical Assessment; Deep Learning; Human; Human Experiment; Natural Language Processing; Ontology},
	keywords = {Deep learning; Medical applications; Natural language processing systems; Ontology; Semantics; NAtural language processing; Pre-trained language representations; Semantic similarity; Spin detection; Trial outcomes; Learning algorithms; algorithm; article; classifier; clinical assessment; deep learning; human; human experiment; natural language processing; ontology},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 17; All Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Bikmullina2019,
	author = {Bikmullina, Ilsiyar Ildarovna and Barkov, I. A.},
	title = {Instrumentation and Control System for Test Bench},
	year = {2019},
	pages = {},
	doi = {10.1109/FarEastCon.2019.8934256},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078014886&doi=10.1109%2FFarEastCon.2019.8934256&partnerID=40&md5=fc9156db8da97a10d47699e51a955daa},
	abstract = {The main problem of the article is the lack of automated structural synthesis of information systems based on the semantic relations of subject area. As presented in the article, in technology creation software system expert developing the domain ontology with help user-friendly the program interfaces. This program is a mechanism for automatically formalizing domain description and automatically synthesizing the class diagram in the Unified Modeling Language. In the article discusses the method for automatic design based on automated synthesis of Unified Modeling Language models of the application program to control the test stand. The method of its use in solving the problems of designing an instrumentation and control system for test bench of an unmanned aerial vehicle is described. Expert inputs a domain ontology, then system automatically formalizes domain description and synthesizes the class diagram in the Unified Modeling Language. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {An Unmanned Aerial Vehicle; Domain; Ontology; Synthesis Of Class Diagrams; Uml; Antennas; Application Programs; Computer Simulation Languages; Control Systems; Encoding (symbols); Flight Dynamics; Ontology; Semantics; Software Testing; Unmanned Aerial Vehicles (uav); User Interfaces; Automated Synthesis; Class Diagrams; Domain; Domain Description; Domain Ontologies; Instrumentation And Control System; Semantic Relations; Structural Synthesis; Unified Modeling Language},
	keywords = {Antennas; Application programs; Computer simulation languages; Control systems; Encoding (symbols); Flight dynamics; Ontology; Semantics; Software testing; Unmanned aerial vehicles (UAV); User interfaces; Automated synthesis; Class diagrams; domain; Domain description; Domain ontologies; Instrumentation and control system; Semantic relations; Structural synthesis; Unified Modeling Language},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Selvaretnam20191397,
	author = {Selvaretnam, Bhawani and Belkhatir, Mohammed},
	title = {Coupled intrinsic and extrinsic human language resource-based query expansion},
	year = {2019},
	journal = {Knowledge and Information Systems},
	volume = {60},
	number = {3},
	pages = {1397 - 1426},
	doi = {10.1007/s10115-018-1267-x},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053540207&doi=10.1007%2Fs10115-018-1267-x&partnerID=40&md5=6d29f6f828d550759696cf75a6fd2355},
	abstract = {Poor information retrieval performance has often been attributed to the query-document vocabulary mismatch problem which is defined as the difficulty for human users to formulate precise natural language queries that are in line with the vocabulary of the documents deemed relevant to a specific search goal. To alleviate this problem, query expansion processes are applied in order to spawn and integrate additional terms to an initial query. This requires accurate identification of main query concepts to ensure the intended search goal is duly emphasized and relevant expansion concepts are extracted and included in the enriched query. Natural language queries have intrinsic linguistic properties such as parts-of-speech labels and grammatical relations which can be utilized in determining the intended search goal. Additionally, extrinsic language-based resources such as ontologies are needed to suggest expansion concepts semantically coherent with the query content. We present here a query expansion framework which capitalizes on both linguistic characteristics of user queries and ontology resources for query constituent encoding, expansion concept extraction and concept weighting. A thorough empirical evaluation on real-world datasets validates our approach against unigram language model, relevance model and a sequential dependence-based technique. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Concept Weighting; Concept-based Query Segmentation; Human Language Processing; Ontology Processing; Query Expansion; Information Retrieval; Natural Language Processing Systems; Ontology; Syntactics; Concept Weighting; Concept-based; Empirical Evaluations; Grammatical Relations; Human Language Processing; Linguistic Properties; Natural Language Queries; Query Expansion; Expansion},
	keywords = {Information retrieval; Natural language processing systems; Ontology; Syntactics; Concept weighting; Concept-based; Empirical evaluations; Grammatical relations; Human language processing; Linguistic properties; Natural language queries; Query expansion; Expansion},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3; All Open Access}
}

@CONFERENCE{Atighetchi201997,
	author = {Atighetchi, Michael and Simidchieva, Borislava I. and Olejnik, Katarzyna},
	title = {Security Requirements Analysis - A Vision for an Automated Toolchain},
	year = {2019},
	pages = {97 - 104},
	doi = {10.1109/QRS-C.2019.00031},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073878314&doi=10.1109%2FQRS-C.2019.00031&partnerID=40&md5=3c2f671a12010bb7f1b17deadc26c00d},
	abstract = {As the organizations move towards more fully embracing agile software development and DevOps for mission-critical systems, the need to continuously and rapidly assess cyber security requirements will become increasingly important. Current security assessment tools currently focus on performing Cyber Vulnerability Analysis (CVA) on designs and software artifacts rather than requirements, missing critical steps to vet requirements and support design decisions early in the development lifecycle or as requirements change. This paper outlines the main concepts underlying a toolchain that would enable software developers to analyze cyber security requirements based on existing natural language requirement descriptions, without burdening the developer with learning new languages, models, or ontologies. The main contribution of this paper is a discussion of the concept of operations for automated security requirement analysis together with a discussion of the main functional components required to implement an automated analysis toolchain. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Automated Analysis; Cyber Analysis And Quantification; Requirement Engineering; Automation; Computer Software Selection And Evaluation; Cryptography; Life Cycle; Software Design; Software Reliability; Agile Software Development; Automated Analysis; Concept Of Operations; Cyber Analysis And Quantification; Functional Components; Mission Critical Systems; Natural Language Requirements; Requirement Engineering; C (programming Language)},
	keywords = {Automation; Computer software selection and evaluation; Cryptography; Life cycle; Software design; Software reliability; Agile software development; Automated analysis; Concept of operations; Cyber analysis and quantification; Functional components; Mission critical systems; Natural language requirements; Requirement engineering; C (programming language)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{Oba201970,
	author = {Oba, Atsushi and Paik, Incheon},
	title = {Extraction of Taxonomic Relation of Complex Terms by Recurrent Neural Network},
	year = {2019},
	pages = {70 - 72},
	doi = {10.1109/ICCC.2019.00024},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072805221&doi=10.1109%2FICCC.2019.00024&partnerID=40&md5=591e93ef7983cea2c19244cdefb6f174},
	abstract = {In recent years, while the Internet has brought various technological evolutions, a lot of ontology is required to organize and systemize knowledge, and its generation is necessary. Especially, classification of hypernym-hyponym relation which describes taxonomy of ontology has received a lot of attention. As a method to automate the generation, word embedding based method was proposed recently. Although the method enabled high accuracy classification by using semantics, it does not correspond to complex term consisting of multiple words. Based on this background, in this paper, we proposed a new model combined word embedding and Recurrent Neural Network(RNN), evaluated the classification performance with data extracted from WordNet. For the result, it is indicated that the RNN approach is more effective and general for ontology generation. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Natural Language Processing; Ontological Classification; Recurrent Neural Network; Recurrent Neural Network Language Model; Word Embedding; Word2vector; Complex Networks; Embeddings; Natural Language Processing Systems; Ontology; Semantics; Classification Performance; High-accuracy; Natural Language Processing; Ontology Generation; Recurrent Neural Network (rnn); Technological Evolution; Word Embedding; Word2vector; Recurrent Neural Networks},
	keywords = {Complex networks; Embeddings; Natural language processing systems; Ontology; Semantics; Classification performance; High-accuracy; NAtural language processing; Ontology generation; Recurrent neural network (RNN); Technological evolution; Word Embedding; Word2Vector; Recurrent neural networks},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@CONFERENCE{Gaur2019514,
	author = {Gaur, Manas and Kursuncu, Ugur and Sheth, Amit P. and Alambo, Amanuel and Thirunarayan, Krishnaprasad and Welton, Randon S. and Sain, Joy Prakash and Kavuluru, Ramakanth and Pathak, Jyotishman},
	title = {Knowledge-aware assessment of severity of suicide risk for early intervention},
	year = {2019},
	pages = {514 - 525},
	doi = {10.1145/3308558.3313698},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066888757&doi=10.1145%2F3308558.3313698&partnerID=40&md5=4089fc812fa82af5d7aaa75783a1c714},
	abstract = {Mental health illness such as depression is a significant risk factor for suicide ideation, behaviors, and attempts. A report by Substance Abuse and Mental Health Services Administration (SAMHSA) shows that 80% of the patients suffering from Borderline Personality Disorder (BPD) have suicidal behavior, 5-10% of whom commit suicide. While multiple initiatives have been developed and implemented for suicide prevention, a key challenge has been the social stigma associated with mental disorders, which deters patients from seeking help or sharing their experiences directly with others including clinicians. This is particularly true for teenagers and younger adults where suicide is the second highest cause of death in the US. Prior research involving surveys and questionnaires (e.g. PHQ-9) for suicide risk prediction failed to provide a quantitative assessment of risk that informed timely clinical decision-making for intervention. Our interdisciplinary study concerns the use of Reddit as an unobtrusive data source for gleaning information about suicidal tendencies and other related mental health conditions afflicting depressed users. We provide details of our learning framework that incorporates domain-specific knowledge to predict the severity of suicide risk for an individual. Our approach involves developing a suicide risk severity lexicon using medical knowledge bases and suicide ontology to detect cues relevant to suicidal thoughts and actions. We also use language modeling, medical entity recognition and normalization and negation detection to create a dataset of 2181 redditors that have discussed or implied suicidal ideation, behavior, or attempt. Given the importance of clinical knowledge, our gold standard dataset of 500 redditors (out of 2181) was developed by four practicing psychiatrists following the guidelines outlined in Columbia Suicide Severity Rating Scale (C-SSRS), with the pairwise annotator agreement of 0.79 and group-wise agreement of 0.73. Compared to the existing four-label classification scheme (no risk, low risk, moderate risk, and high risk), our proposed C-SSRS-based 5-label classification scheme distinguishes people who are supportive, from those who show different severity of suicidal tendency. Our 5-label classification scheme outperforms the state-of-the-art schemes by improving the graded recall by 4.2% and reducing the perceived risk measure by 12.5%. Convolutional neural network (CNN) provided the best performance in our scheme due to the discriminative features and use of domain-specific knowledge resources, in comparison to SVM-L that has been used in the state-of-the-art tools over similar dataset. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Behavioral Research; Clinical Research; Decision Making; Health Care; Health Risks; Modeling Languages; Natural Language Processing Systems; Neural Networks; Surveys; World Wide Web; Clinical Decision Making; Convolutional Neural Network; Discriminative Features; Domain-specific Knowledge; Inter-disciplinary Studies; Mental Health Services; Quantitative Assessments; State-of-the-art Scheme; Risk Assessment},
	keywords = {Behavioral research; Clinical research; Decision making; Health care; Health risks; Modeling languages; Natural language processing systems; Neural networks; Surveys; World Wide Web; Clinical decision making; Convolutional neural network; Discriminative features; Domain-specific knowledge; Inter-disciplinary studies; Mental health services; Quantitative assessments; State-of-the-art scheme; Risk assessment},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 132}
}

@CONFERENCE{Gromann2019,
	author = {Gromann, Dagmar and Declerck, Thierry},
	title = {Towards the detection and formal representation of semantic shifts in inflectional morphology},
	year = {2019},
	journal = {OpenAccess Series in Informatics},
	volume = {70},
	pages = {},
	doi = {10.4230/OASIcs.LDK.2019.21},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068066655&doi=10.4230%2FOASIcs.LDK.2019.21&partnerID=40&md5=71d8ae48235faac7b63edd66186ce8b1},
	abstract = {Semantic shifts caused by derivational morphemes is a common subject of investigation in language modeling, while inflectional morphemes are frequently portrayed as semantically more stable. This study is motivated by the previously established observation that inflectional morphemes can be just as variable as derivational ones. For instance, the English plural “-s” can turn the fabric silk into the garments of a jockey, silks. While humans know that silk in this sense has no plural, it takes more for machines to arrive at this conclusion. Frequently utilized computational language resources, such as WordNet, or models for representing computational lexicons, like OntoLex-Lemon, have no descriptive mechanism to represent such inflectional semantic shifts. To investigate this phenomenon, we extract word pairs of different grammatical number from WordNet that feature additional senses in the plural and evaluate their distribution in vector space, i.e., pre-trained word2vec and fastText embeddings. We then propose an extension of OntoLex-Lemon to accommodate this phenomenon that we call inflectional morpho-semantic variation to provide a formal representation accessible to algorithms, neural networks, and agents. While the exact scope of the problem is yet to be determined, this first dataset shows that it is not negligible. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Embeddings; Formal Lexical Modeling; Inflectional Morphology; Semantic Shift; Citrus Fruits; Embeddings; Morphology; Ontology; Semantics; Silk; Vector Spaces; Computational Languages; Formal Representations; Inflectional Morphemes; Know-that; Language Model; Word-pairs; Wordnet; Modeling Languages},
	keywords = {Citrus fruits; Embeddings; Morphology; Ontology; Semantics; Silk; Vector spaces; Computational languages; Formal representations; Inflectional morphemes; Know-that; Language model; Word-pairs; Wordnet; Modeling languages},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 6}
}

@CONFERENCE{Eckart2019,
	author = {Eckart, Thomas and Bosch, Sonja E. and Goldhahn, Dirk and Quasthoff, Uwe and Klimek, Bettina},
	title = {Translation-based dictionary alignment for under-resourced Bantu languages},
	year = {2019},
	journal = {OpenAccess Series in Informatics},
	volume = {70},
	pages = {},
	doi = {10.4230/OASIcs.LDK.2019.17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068051098&doi=10.4230%2FOASIcs.LDK.2019.17&partnerID=40&md5=67070355a83a6c00258e24ceb53b1c7a},
	abstract = {Despite a large number of active speakers, most Bantu languages can be considered as under- or less-resourced languages. This includes especially the current situation of lexicographical data, which is highly unsatisfactory concerning the size, quality and consistency in format and provided information. Unfortunately, this does not only hold for the amount and quality of data for monolingual dictionaries, but also for their lack of interconnection to form a network of dictionaries. Current endeavours to promote the use of Bantu languages in primary and secondary education in countries like South Africa show the urgent need for high-quality digital dictionaries. This contribution describes a prototypical implementation for aligning Xhosa, Zimbabwean Ndebele and Kalanga language dictionaries based on their English translations using simple string matching techniques and via WordNet URIs. The RDF-based representation of the data using the Bantu Language Model (BLM) and – partial – references to the established WordNet dataset supported this process significantly. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Bantu Languages; Cross-language Dictionary Alignment; Linguistic Linked Data; Translation; Under-resourced Languages; Linguistics; Ontology; Cross Languages; Current Situation; Digital Dictionaries; Primary And Secondary Education; Prototypical Implementation; Quality Of Data; String Matching; Under-resourced Languages; Translation (languages)},
	keywords = {Linguistics; Ontology; Cross languages; Current situation; Digital dictionaries; Primary and secondary education; Prototypical implementation; Quality of data; String matching; Under-resourced languages; Translation (languages)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2}
}

@CONFERENCE{Deshpande2019350,
	author = {Deshpande, Ameet and Jegadeesan, Monisha},
	title = {Leveraging ontological knowledge for neural language models},
	year = {2019},
	journal = {ACM International Conference Proceeding Series},
	pages = {350 - 353},
	doi = {10.1145/3297001.3297059},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061124631&doi=10.1145%2F3297001.3297059&partnerID=40&md5=eb6cd6bb2704ada593387b3ea955d453},
	abstract = {Neural Language Models such as Word2Vec and GloVe have been shown to encode semantic relatedness between words. Improvements in unearthing these embeddings can ameliorate performance in numerous downstream applications such as sentiment analysis, question answering, and dialogue generation. Lexical ontologies such as WordNet are known to supply information about semantic similarity rather than relatedness. Further, extracting word embeddings from small corpora is daunting for data-hungry neural networks. This work shows how methods that conflate Word2Vec and Ontologies can achieve better performance, reduce training time and help adapt to domains with a minimum amount of data. © 2022 Elsevier B.V., All rights reserved.},
	author_keywords = {Domain-transfer; Hierarchy; Ontology; Word Vectors; Computational Linguistics; Embeddings; Knowledge Management; Semantics; Sentiment Analysis; Dialogue Generations; Domain Transfers; Downstream Applications; Hierarchy; Question Answering; Semantic Relatedness; Semantic Similarity; Word Vectors; Ontology},
	keywords = {Computational linguistics; Embeddings; Knowledge management; Semantics; Sentiment analysis; Dialogue generations; Domain transfers; Downstream applications; Hierarchy; Question Answering; Semantic relatedness; Semantic similarity; Word vectors; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2019,
	title = {17th Russian Conference on Artificial Intelligence, RCAI 2019},
	year = {2019},
	journal = {Communications in Computer and Information Science},
	volume = {1093},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075725741&partnerID=40&md5=964ff28499a5c06f5e9be34780275414},
	abstract = {The proceedings contain 30 papers. The special focus in this conference is on Russian Conference on Artificial Intelligence. The topics include: Ontology-Driven Processing of Unstructured Text; towards Automated Identification of Technological Trajectories; development of the Lexicon of Argumentation Indicators; a Technique to Pre-trained Neural Network Language Model Customization to Software Development Domain; predicting Personality Traits from Social Network Profiles; extraction of Cognitive Operations from Scientific Texts; evolutionary Design of Fuzzy Systems Based on Multi-objective Optimization and Dempster-Shafer Schemes; randomized General Indices for Evaluating Damage Through Malefactor Social Engineering Attacks; multiclass Classification Based on the Convolutional Fuzzy Neural Networks; redistributing Animats Between Groups; algebraic Bayesian Networks: Naïve Frequentist Approach to Local Machine Learning Based on Imperfect Information from Social Media and Expert Estimates; inference Methods for One Class of Systems with Many Fuzzy Inputs; chaotic Phenomena in Collective Behavior; data Collection and Preparation of Training Samples for Problem Diagnosis of Vision Pathologies; using Ontology Engineering Methods for Organizing the Information Interaction Between Relational Databases; technology of Personalized Preventive Recommendation Formation Based on Disease Risk Assessment; constructing Features of Competence-Oriented Specialist Models Based on Tutoring Integrated Expert Systems Technology; UAV Trajectory Tracking Based on Local Interpolating Splines and Optimal Choice of Knots; spatial Clustering Based on Analysis of Big Data in Digital Marketing; ontology-Based Approach to Organizing the Support for the Analysis of Argumentation in Popular Science Discourse; hierarchical Reinforcement Learning with Clustering Abstract Machines; ontological Approach to Providing Intelligent Support for Solving Compute-Intensive Problems on Supercomputers. © 2019 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Joshi2019135,
	author = {Joshi, Aditya and Karimi, Sarvnaz and Sparks, Ross Stewart and Paris, Cecile L. and Macintyre, C. Raina},
	title = {A comparison of word-based and context-based representations for classification problems in health informatics},
	year = {2019},
	pages = {135 - 141},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074169639&partnerID=40&md5=4d73790a9ea974011bbbc9a695023c59},
	abstract = {Distributed representations of text can be used as features when training a statistical classifier. These representations may be created as a composition of word vectors or as context-based sentence vectors. We compare the two kinds of representations (word versus context) for three classification problems: influenza infection classification, drug usage classification and personal health mention classification. For statistical classifiers trained for each of these problems, context-based representations based on ELMo, Universal Sentence Encoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe and the two adapted using the MESH ontology. There is an improvement of 2-4% in the accuracy when these context-based representations are used instead of word-based representations. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Computational Linguistics; Health; Medical Informatics; Text Processing; Context-based; Distributed Representation; Health Informatics; Language Model; Ontology's; Personal Health; Statistical Classifier; Word Vectors; Classification (of Information)},
	keywords = {Computational linguistics; Health; Medical informatics; Text processing; Context-based; Distributed representation; Health informatics; Language model; Ontology's; Personal health; Statistical classifier; Word vectors; Classification (of information)},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 12}
}

@CONFERENCE{Holter201933,
	author = {Holter, Ole Magnus and Myklebust, Erik Bryhn and Chen, Jiaoyan and Jimeńez-Ruiz, Ernesto},
	title = {Embedding OWL ontologies with OWL2Vec},
	year = {2019},
	journal = {CEUR Workshop Proceedings},
	volume = {2456},
	pages = {33 - 36},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073241176&partnerID=40&md5=8fe1afa76330a66bd3611af8d50f5046},
	abstract = {In this paper, we present a preliminary study to compute embeddings for OWL 2 ontologies by projecting the ontology axioms into a graph and performing (random) walks over the ontology graph to create a corpus of sentences. This corpus is then given to a neural language model to create concept embeddings. The conducted preliminary evaluation shows promising results. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Birds; Embeddings; Language Model; Ontology Axioms; Ontology Graphs; Owl Ontologies; Ontology},
	keywords = {Birds; Embeddings; Language model; Ontology axioms; Ontology graphs; OWL ontologies; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 15}
}

@CONFERENCE{Murtazina201963,
	author = {Murtazina, M. S. and Avdeenko, Tatiana Vladimirovna},
	title = {The detection of conflicts in the requirements specification based on an ontological model and a production rule system},
	year = {2019},
	journal = {CEUR Workshop Proceedings},
	volume = {2416},
	pages = {63 - 73},
	doi = {10.18287/1613-0073-2019-2416-63-73},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070974831&doi=10.18287%2F1613-0073-2019-2416-63-73&partnerID=40&md5=fc4560c2046ee31a5b06f353a6f54a97},
	abstract = {The paper presents an approach to organizing the detection of conflicts between requirements based on an ontological model and a system of production rules. Requirements in the form of plain text are converted to instances of OWL ontologies for analysis. There are three basic elements “subject”, “action” and “object” in the requirements. These elements become classes of the ontology. The object properties between instances of the same classes are defined in the ontology. In the system of rules it is determined that one of four types of relations can be established between a pair of the requirements: isConflict, isDuplicate, isInterconnect, isNotInterconnect. We develop the software product in the Python language for building and applying production rules system for classes and property objects of the ontology. Protégé 5.2 is used to work with the ontology. Also Python library PySwip and development environment SWI-Prolog 7.6.4 are used in the work. The paper also considers the issues of extracting requirements ontology instances from the automated processing results of textual requirements. The UDPipe with Russian language model is used for text processing. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Computer Software; High Level Languages; Nanotechnology; Text Processing; Automated Processing; Development Environment; Ontological Modeling; Requirements Ontology; Requirements Specifications; Russian Languages; Software Products; Types Of Relations; Ontology},
	keywords = {Computer software; High level languages; Nanotechnology; Text processing; Automated processing; Development environment; Ontological modeling; Requirements ontology; Requirements specifications; Russian languages; Software products; Types of relations; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@ARTICLE{Teslya2019745,
	author = {Teslya, Nikolay N. and Savosin, Sergey V.},
	title = {Matching Ontologies with Word2Vec-Based Neural Network},
	year = {2019},
	journal = {Lecture Notes in Computer Science},
	volume = {11619 LNCS},
	pages = {745 - 756},
	doi = {10.1007/978-3-030-24289-3_55},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069184246&doi=10.1007%2F978-3-030-24289-3_55&partnerID=40&md5=d7683e583c3eea1f884afcad90f4bc01},
	abstract = {To date, knowledge engineering researchers have proposed a large number of ontology matching methods. In this paper, to solve the ontology mapping problem, it is proposed to use a vector language model based on the Word2Vec statistical model set. The vector model is implemented using a neural network based on the TensorFlow framework. The peculiarity of the method is the use of the baseline pre-trained Wor2Vec model based on texts from the English Wikipedia and Google News, which is consistently extended on the basis of relationships specific to the ontologies being matched. This approach allows the use of semantics of the language bypassing situations in which, due to the form of a word or a specific term, it is impossible to find a correspondence of ontology concepts. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Neural Network; Ontology; Ontology Matching; Word2vec; Neural Networks; Semantics; Language Model; Model-based Opc; Ontology Concepts; Ontology Mapping; Ontology Matching; Statistical Modeling; Vector Models; Word2vec; Ontology},
	keywords = {Neural networks; Semantics; Language model; Model-based OPC; Ontology concepts; Ontology mapping; Ontology matching; Statistical modeling; Vector models; Word2Vec; Ontology},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 7}
}

@ARTICLE{Gentile2019131,
	author = {Gentile, Anna Lisa and Gruhl, Daniel and Ristoski, Petar and Welch, Steve R.},
	title = {Explore and exploit. Dictionary expansion with human-in-the-loop},
	year = {2019},
	journal = {Lecture Notes in Computer Science},
	volume = {11503 LNCS},
	pages = {131 - 145},
	doi = {10.1007/978-3-030-21348-0_9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066815840&doi=10.1007%2F978-3-030-21348-0_9&partnerID=40&md5=d0cf9be8663cb348d875ea0279190370},
	abstract = {Many Knowledge Extraction systems rely on semantic resources - dictionaries, ontologies, lexical resources - to extract information from unstructured text. A key for successful information extraction is to consider such resources as evolving artifacts and keep them up-to-date. In this paper, we tackle the problem of dictionary expansion and we propose a human-in-the-loop approach: we couple neural language models with tight human supervision to assist the user in building and maintaining domain-specific dictionaries. The approach works on any given input text corpus and is based on the explore and exploit paradigm: starting from a few seeds (or an existing dictionary) it effectively discovers new instances (explore) from the text corpus as well as predicts new potential instances which are not in the corpus, i.e. “unseen”, using the current dictionary entries (exploit). We evaluate our approach on five real-world dictionaries, achieving high accuracy with a rapid expansion rate. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Data Mining; Expansion; Information Retrieval; Domain Specific; Extract Informations; Human Supervision; Human-in-the-loop; Knowledge Extraction; Lexical Resources; Semantic Resources; Unstructured Texts; Semantic Web},
	keywords = {Data mining; Expansion; Information retrieval; Domain specific; Extract informations; Human supervision; Human-in-the-loop; Knowledge extraction; Lexical resources; Semantic resources; Unstructured texts; Semantic Web},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 16}
}

@CONFERENCE{2019,
	title = {CEUR Workshop Proceedings},
	year = {2019},
	journal = {CEUR Workshop Proceedings},
	volume = {2367},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066505321&partnerID=40&md5=f6f353615b9151741448e8940e20f24d},
	abstract = {The proceedings contain 13 papers. The topics discussed include: good premises retrieval via a two-stage argument retrieval model; visualization of data from online participation projects; language models for next-track music recommendation; autoencoders for next-track recommendation; monitoring thread-related resource demands of a multi-tenant in-memory database in a cloud environment; execution strategies for compute intensive queries in particle physics; towards performance prediction for stream processing applications; how to become a (throughput) billionaire: the stream processing engine PipeFabric; and towards an ontology-driven evolutionary programming system for answering natural language queries against RDF data. © 2019 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Bradshaw201967,
	author = {Bradshaw, Stephen and O’Riordan, Colm and Bradshaw, Daragh},
	title = {Constructing language models from online forms to aid better document representation for more effective clustering},
	year = {2019},
	journal = {Communications in Computer and Information Science},
	volume = {976},
	pages = {67 - 81},
	doi = {10.1007/978-3-030-15640-4_4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064010888&doi=10.1007%2F978-3-030-15640-4_4&partnerID=40&md5=fd9b0935a996cc7f1985704aa8253cba},
	abstract = {Clustering is the practice of finding tacit patterns in datasets by grouping the corpus by similarity. When clustering documents this is achieved by converting the corpus into a numeric format and applying clustering techniques to this new format. Values are assigned to terms based on their frequency within a particular document, against their general occurrence in the corpus. One obstacle in achieving this aim is as a result of the polysemic nature of terms. That is words having multiple meanings; each intended meaning only being discernible when examining the context in which they are used. Thus, disambiguating the intended meaning of a term can greatly improve the efficacy of a clustering algorithm. One approach to achieve this end has been done through the creation of an ontology - Wordnet, which can act as a look-up as to the intended meaning of a term. Wordnet however, is a static source and does not keep pace with the changing nature of language. The aim of this paper is to show that while Wordnet can be affective, however it is static in nature and thus does not capture some contemporary usage of terms. Particularly when the dataset is taken from online conversation forums, who would not be structured in a standard document format. Our proposed solution involves using Reddit as a contemporary source which moves with new trends in word usage. To better illustrate this point we cluster comments found in online threads such as Reddit and compare the efficacy of different representations of these document sets. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Classification; Data Mining; Document Clustering; Graph Theory; Word Sense Disambiguation; Wordnet; Classification (of Information); Cluster Analysis; Data Mining; Graph Theory; Information Retrieval Systems; Knowledge Engineering; Knowledge Management; Natural Language Processing Systems; Ontology; Clustering Documents; Clustering Techniques; Document Clustering; Document Representation; Standard Documents; Static Sources; Word Sense Disambiguation; Wordnet; Clustering Algorithms},
	keywords = {Classification (of information); Cluster analysis; Data mining; Graph theory; Information retrieval systems; Knowledge engineering; Knowledge management; Natural language processing systems; Ontology; Clustering documents; Clustering techniques; Document Clustering; Document Representation; Standard documents; Static sources; Word Sense Disambiguation; Wordnet; Clustering algorithms},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2019,
	title = {9th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management, IC3K 2017},
	year = {2019},
	journal = {Communications in Computer and Information Science},
	volume = {976},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064002633&partnerID=40&md5=65e7de46e18f47abf994d9eb29ccd9ed},
	abstract = {The proceedings contain 19 papers. The special focus in this conference is on Knowledge Discovery, Knowledge Engineering and Knowledge Management. The topics include: Associative representation and processing of databases using DASNG and AVB+trees for efficient data access; geCoLan: A constraint language for reasoning about ecological networks in the semantic web; the linked data wiki: Leveraging organizational knowledge bases with linked open data; social and community related themes in ontology evaluation: Findings from an interview study; Empowering IT organizations through a confluence of knowledge for value integration into the IT services firm’s business model; How do japanese smes generate digital business value from SMACIT technologies with knowledge creation?; Common information systems maturity validation resilience readiness levels (ResRLs); prediction and trading of dow jones from twitter: A boosting text mining method with relevant tweets identification; behavioural biometric continuous user authentication using multivariate keystroke streams in the spectral domain; constructing language models from online forms to aid better document representation for more effective clustering; identification of relevant hashtags for planned events using learning to rank; investigation of passage based ranking models to improve document retrieval; robust single-document summarizations and a semantic measurement of quality; adaptive cluster based discovery of high utility itemsets; knowledge based system for composing sentences to summarize documents; preface; transfer learning in sentiment classification with deep neural networks. © 2019 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Li2018568,
	author = {Li, Junxian and Wang, Wei and Wang, Jingjing},
	title = {Querying Linked Data Based on Hierarchical Multi-Hop Ranking Model},
	year = {2018},
	journal = {Journal of Shanghai Jiaotong University (Science)},
	volume = {23},
	number = {4},
	pages = {568 - 576},
	doi = {10.1007/s12204-018-1976-z},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051454048&doi=10.1007%2Fs12204-018-1976-z&partnerID=40&md5=795eedf627416d9b692f3a66751d046f},
	abstract = {How to query Linked Data effectively is a challenge due to its heterogeneous datasets. There are three types of heterogeneities, i.e., different structures representing entities, different predicates with the same meaning and different literal formats used in objects. Approaches based on ontology mapping or Information Retrieval (IR) cannot deal with all types of heterogeneities. Facing these limitations, we propose a hierarchical multi-hop language model (HMPM). It discriminates among three types of predicates, descriptive predicates, out-associated predicates and in-associated predicates, and generates multi-hop models for them respectively. All predicates’ similarities between the query and entity are organized into a hierarchy, with predicate types on the first level and predicates of this type on the second level. All candidates are ranked in ascending order. We evaluated HMPM in three datasets, DBpedia, LinkedMDB and Yago. The results of experiments show that the effectiveness and generality of HMPM outperform the existing approaches. © 2018 Elsevier B.V., All rights reserved.},
	author_keywords = {A; Hierarchical Multi-hop Ranking Model (hmpm); Language Model; Linked Data; Tp 391; Computational Linguistics; Data Handling; Different Structure; Heterogeneous Datasets; Language Model; Linked- Mdb; Ontology Mapping; Ranking Model; Second Level; Tp 391; Linked Data},
	keywords = {Computational linguistics; Data handling; Different structure; Heterogeneous datasets; Language model; Linked- mdb; Ontology mapping; Ranking model; Second level; TP 391; Linked data},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{Ayu2018718,
	author = {Ayu, Media A. and Mantoro, Teddy and Asian, Jelita},
	title = {Quality translation enhancement using sequence knowledge and pruning in statistical machine translation},
	year = {2018},
	journal = {Telkomnika (Telecommunication Computing Electronics and Control)},
	volume = {16},
	number = {2},
	pages = {718 - 727},
	doi = {10.12928/TELKOMNIKA.v16i2.8687},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044393717&doi=10.12928%2FTELKOMNIKA.v16i2.8687&partnerID=40&md5=cbebbc38e4e864b99b3fcab928906fb4},
	abstract = {Machine translation has two important parts, a learning process which followed by a translation process. Unfortunately, most of the translation process requires complex operations and in-depth knowledge of the languages in order to give a good quality translation. This study proposes a better approach, which does not require in-depth knowledge of the linguistic properties of the languages, but it produces a good quality translation. This study evaluated 28 different parameters in IRSTLM language modeling, which resulting 270 millions experiments, and proposes a sequence evaluation mechanism based on a maximum evaluation of each parameter in producing a good quality translation based on NIST and BLEU. The parallel corpus and statistical machine learning for English and Bahasa Indonesia were used in this study. The pruning process, user interface, and the personalization of translation have a very important role in implementing of this machine translation. The result is quite promising. It shows that pruning process increases of the translation process time. The particular sequence knowledge/value parameter in translation process has a better performance than the other method using in-depth linguistic knowledge approaches. All these processes, including the process of parsing from a stand-alone mode to an online mode, are also discussed in detail. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Hierarchical Ontology; Irstlm; Ontology Matching; Parallel Corpus; Personalization Translation; Pruning; Statistical Machine Translation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 2; All Open Access; Green Accepted Open Access; Green Final Open Access; Green Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Balachandran2018164,
	author = {Balachandran, Vidhisha and Rajagopal, Dheeraj and Catherine, Rose and Cohen, William W.},
	title = {Learning to Define Terms in the Software Domain},
	year = {2018},
	pages = {164 - 172},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122037010&partnerID=40&md5=810a3d2c63c1b2da606d268ca0598af9},
	abstract = {One way to test a person’s knowledge of a domain is to ask them to define domain-specific terms. Here, we investigate the task of automatically generating definitions of technical terms by reading text from the technical domain. Specifically, we learn definitions of software entities from a large corpus built from the user forum Stack Overflow. To model definitions, we train a language model and incorporate additional domain-specific information like word co-occurrence, and ontological category information. Our approach improves previous baselines by 2 BLEU points for the definition generation task. Our experiments also show the additional challenges associated with the task and the short-comings of language-model based architectures for definition generation. © 2021 Elsevier B.V., All rights reserved.},
	author_keywords = {Definition Generation; Domain Specific; Domain-specific Information; Language Model; Large Corpora; Learn+; Software Domains; Software Entities; Stack Overflow; Technical Terms; Computational Linguistics},
	keywords = {Definition generation; Domain specific; Domain-specific information; Language model; Large corpora; Learn+; Software domains; Software entities; Stack overflow; Technical terms; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 3}
}

@ARTICLE{2018,
	title = {German Medical Data Sciences: A Learning Healthcare System - Proceedings of the 63rd Annual Meeting of the German Association of Medical Informatics, Biometry and Epidemiology (gmds e.V.), GMDS 2018},
	year = {2018},
	journal = {Studies in Health Technology and Informatics},
	volume = {253},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085755032&partnerID=40&md5=46fa277f91b54b00dfaf7b9cd15abafe},
	abstract = {The proceedings contain 17 papers. The topics discussed include: investigating the capabilities of FHIR search for clinical trial phenotyping; analyzing the readability of health information booklets on cardiovascular diseases; interactive feedback of data quality in clinical research. a case study from an infectious diseases cohort; practical extension of provenance to healthcare data based on the W3C PROV standard; a web service to suggest semantic codes based on the MDM-portal; metadata import from RDF to i2b2; ontology-guided markerless navigation and situational awareness for endoscopic surgery; and protecting record linkage identifiers using a language model for patient names. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Adult; Awareness; Biometry; Cardiovascular Disease; Case Report; Clinical Article; Clinical Research; Cohort Analysis; Communicable Disease; Conference Review; Data Quality; Data Science; Endoscopic Surgery; Fast Healthcare Interoperability Resources; Female; Human; Language; Learning Health System; Male; Medical Informatics; Metadata; Ontology; Phenotype; Reading; Surgery},
	keywords = {adult; awareness; biometry; cardiovascular disease; case report; clinical article; clinical research; cohort analysis; communicable disease; conference review; data quality; data science; endoscopic surgery; Fast Healthcare Interoperability Resources; female; human; language; learning health system; male; medical informatics; metadata; ontology; phenotype; reading; surgery},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@CONFERENCE{Futia2018187,
	author = {Futia, Giuseppe and Vetrò, Antonio and Melandri, Alessio and De Martin, Juan Carlos},
	title = {Training neural language models with SPARQL queries for semi-automatic semantic mapping},
	year = {2018},
	journal = {Procedia Computer Science},
	volume = {137},
	pages = {187 - 198},
	doi = {10.1016/j.procs.2018.09.018},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061962055&doi=10.1016%2Fj.procs.2018.09.018&partnerID=40&md5=ee11dbe67136ccf48cb21572e26e183f},
	abstract = {Knowledge graphs are labeled and directed multi-graphs that encode information in the form of entities and relationships. They are gaining attention in different areas of computer science: from the improvement of search engines to the development of virtual personal assistants. Currently, an open challenge in building large-scale knowledge graphs from structured data available on the Web (HTML tables, CSVs, JSONs) is the semantic integration of heterogeneous data sources. In fact, such diverse and scattered information rarely provide a formal description of metadata that is required to accomplish the integration task. In this paper we propose an approach based on neural networks to reconstruct the semantics of data sources to produce high quality knowledge graphs in terms of semantic accuracy. We developed a neural language model trained on a set of SPARQL queries performed on knowledge graphs. Through this model it is possible to semi-automatically generate a semantic map between the attributes of a data source and a domain ontology. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Knowledge Graph; Neural Language Model; Semantic Mapping; Sparql; Computational Linguistics; Directed Graphs; Mapping; Search Engines; User Interfaces; Formal Description; Heterogeneous Data Sources; Knowledge Graphs; Language Model; Personal Assistants; Semantic Integration; Semantic Mapping; Sparql; Semantics},
	keywords = {Computational linguistics; Directed graphs; Mapping; Search engines; User interfaces; Formal Description; Heterogeneous data sources; Knowledge graphs; Language model; Personal assistants; Semantic integration; Semantic mapping; SPARQL; Semantics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 8; All Open Access; Gold Open Access; Green Final Open Access; Green Open Access}
}

@CONFERENCE{Sauri´2018173,
	author = {Sauri´, Roser and Alderslade, Ashleigh and Shapiro, Richard},
	title = {A universal classification of lexical categories and grammatical distinctions for lexicographic and processing purposes},
	year = {2018},
	journal = {EURALEX Proceedings},
	pages = {173 - 185},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059410878&partnerID=40&md5=fb5f42077f1867b164325540cdf17c18},
	abstract = {We introduce COMO (Compositional Morphosyntactic Ontology), a classification of part-of-speech categories and their associated grammatical features, which aims to be valid across languages of very different typology. The work has been carried out within the context of the Oxford Global Languages programme, which has the goal of developing language knowledge for 100 languages, particularly those under-represented in the digital space. The requirements around this project are: to be able to describe languages of different typeS while respecting their grammatical tradition, and to be able to serve two main use cases that define our typical work, namely, the labelling of linguistic information in lexicographic products, and the provision of support for language processing tools and corpus annotation processes. These requirements determined the conception and design of COMO, created as a reference model within a broader data architecture in order to address issues of syntactic and semantic interoperability. Our proposal builds on top of previous initiatives in the field aiming at the same goals, but incorporates different features in order to accommodate for the requirements in the project. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Interoperability; Language Modelling; Morphosyntactic Information; Multilinguality; Part-of-speech Tagging},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 1}
}

@CONFERENCE{Bradshaw2018194,
	author = {Bradshaw, Stephen and O’Riordan, Colm},
	title = {Evaluating better document representation in clustering with varying complexity},
	year = {2018},
	volume = {1},
	pages = {194 - 202},
	doi = {10.5220/0006930901940202},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059040428&doi=10.5220%2F0006930901940202&partnerID=40&md5=fd64592df593564c7ad496aec990d909},
	abstract = {Micro blogging has become a very popular activity and the posts made by users can be a valuable source of information. Classifying this content accurately can be a challenging task due to the fact that comments are typically short in nature and on their own may lack context. Reddit<sup>a</sup> is a very popular microblogging site whose popularity has seen a huge and consistent increase over the years. In this paper we propose using alternative but related Reddit threads to build language models that can be used to disambiguate intend mean of terms in a post. A related thread is one which is similar in content, often consisting of the same frequently occurring terms or phrases. We posit that threads of a similar nature use similar language and that the identification of related threads can be used as a source to add context to a post, enabling more accurate classification. In this paper, graphs are used to model the frequency and co-occurrence of terms. The terms of a document are mapped to nodes, and the co-occurrence of two terms are recorded as edge weights. To show the robustness of our approach, we compare the performance in using related Reddit threads to the use of an external ontology; Wordnet. We apply a number of evaluation metrics to the clusters created and show that in every instance, the use of alternative threads to improve document representations is better than the use of Wordnet or standard augmented vector models. We apply this approach to increasingly harder environments to test the robustness of our approach. A tougher environment is one where the classifying algorithm has more than two categories to choose from when selecting the appropriate class. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Classification Methods; Clustering; Context Discovery; Mining Text; Semi-structured Data; Knowledge Engineering; Ontology; Text Processing; Classification Methods; Clustering; Co-occurrence; Context Discovery; Document Representation; Evaluation Metrics; Language Model; Semi Structured Data; Knowledge Management},
	keywords = {Knowledge engineering; Ontology; Text processing; Classification methods; Clustering; Co-occurrence; Context discovery; Document Representation; Evaluation metrics; Language model; Semi structured data; Knowledge management},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Li201856,
	author = {Li, Ying and Xi, Meng and Chen, Hui and Yin, Jianwei},
	title = {Service language model: New ecology for service development},
	year = {2018},
	journal = {Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
	volume = {2018-July},
	pages = {56 - 59},
	doi = {10.18293/SEKE2018-214},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056850099&doi=10.18293%2FSEKE2018-214&partnerID=40&md5=5fb08715a2c1469cf8522dc5aa396f43},
	abstract = {With rapid development of the Internet, all walks of life are engaged in the tide of Internet. In the era of "Internet+", traditional industries are widely developed and expand plenty of emerging business, such as online transactions, Internet finance and so on. However, various problems arise at the same time in this revolution. On one hand, business processes become increasingly intricate, and different fields may have difficulty in communication. On the other hand, developers are hard to understand the real demands from users and the rate of code reuse is not high. In order to solve these problems, we propose a middle-end and project manager (PM) oriented service language model, which could help decouple software development and user requirements, improve work efficiency and reduce development costs. © 2019 Elsevier B.V., All rights reserved.},
	author_keywords = {Business Process; Ontology; Service Language Model; Computational Linguistics; Knowledge Engineering; Ontology; Business Process; Development Costs; Language Model; Online Transaction; Project Managers; Service Development; Traditional Industry; User Requirements; Software Design},
	keywords = {Computational linguistics; Knowledge engineering; Ontology; Business Process; Development costs; Language model; Online transaction; Project managers; Service development; Traditional industry; User requirements; Software design},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Bronze Open Access}
}

@ARTICLE{Lubani20181437,
	author = {Lubani, Mohamed and Mohd Noah, Shahrul Azman},
	title = {Building compact entity embeddings using Wikidata},
	year = {2018},
	journal = {International Journal on Advanced Science, Engineering and Information Technology},
	volume = {8},
	number = {4-2},
	pages = {1437 - 1445},
	doi = {10.18517/ijaseit.8.4-2.6831},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055353372&doi=10.18517%2Fijaseit.8.4-2.6831&partnerID=40&md5=93336bb84966a1c0008bd66e6687e32d},
	abstract = {Representing natural language sentences has always been a challenge in statistical language modeling. Atomic discrete representations of words make it difficult to represent semantically related sentences. Other sentence components such as phrases and named-entities should be recognized and given representations as units instead of individual words. Different entity senses should be assigned different representations even though they share identical words. In this paper, we focus on building the vector representations (embedding) of named-entities from their contexts to facilitate the task of ontology population where named-entities need to be recognized and disambiguated in natural language text. Given a list of target named-entities, Wikidata is used to compensate for the lack of a labeled corpus to build the contexts of all target named-entities as well as all their senses. Description text and semantic relations with other named-entities are considered when building the contexts from Wikidata. To avoid noisy and uninformative features in the embedding generated from artificially built contexts, we propose a method to build compact entity representations to sharpen entity embedding by removing irrelevant features and emphasizing the most detailed ones. An extended version of the Continuous Bag-of-Words model (CBOW) is used to build the joint vector representations of words and named-entities using Wikidata contexts. Each entity context is then represented by a subset of elements that maximizes the chances of keeping the most descriptive features about the target entity. The final entity representations are built by compressing the embedding of the chosen subset using a deep stacked auto encoders model. Cosine similarity and t-SNE visualization technique are used to evaluate the final entity vectors. Results show that semantically related entities are clustered near each other in the vector space. Entities that appear in similar contexts are assigned similar compact vector representations based on their contexts. © 2020 Elsevier B.V., All rights reserved.},
	author_keywords = {Entity Embeddings; Entity Vector Representations; Named Entity Disambiguation},
	type = {Article},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0; All Open Access; Hybrid Gold Open Access}
}

@CONFERENCE{Alba20187,
	author = {Alba, Alfredo and Gruhl, Daniel and Ristoski, Petar and Welch, Steve R.},
	title = {Interactive dictionary expansion using neural language models},
	year = {2018},
	journal = {CEUR Workshop Proceedings},
	volume = {2169},
	pages = {7 - 15},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052838549&partnerID=40&md5=4b1228b2a35fd831dc145424a6800190},
	abstract = {Dictionaries and ontologies are foundational elements of systems extracting knowledge from unstructured text. However, as new content arrives keeping dictionaries up-to-date is a crucial operation. In this paper, we propose a human-in-the-loop (HumL) dictionary expansion approach that employs a lightweight neural language model coupled with tight HumL supervision to assist the user in building and maintaining a domain-specific dictionary from an input text corpus. The approach is based on the explore/exploit paradigm to effectively discover new instances (explore) from the text corpus as well as predict new “unseen” terms not currently in the corpus using the accepted dictionary entries (exploit). We evaluate our approach on a real-world scenario in the healthcare domain, in which we construct a dictionary of adverse drug reactions from user blogs as input text corpus. The evaluation shows that using our approach the user can easily extend the input dictionary, where tight human-in-the-loop integration results in a 216% improvement in effectiveness. © 2018 Elsevier B.V., All rights reserved.},
	author_keywords = {Adverse Drug Reactions; Domain Specific; Foundational Elements; Healthcare Domains; Human-in-the-loop; Language Model; Real-world Scenario; Unstructured Texts; Computational Linguistics},
	keywords = {Adverse drug reactions; Domain specific; Foundational elements; Healthcare domains; Human-in-the-loop; Language model; Real-world scenario; Unstructured texts; Computational linguistics},
	type = {Conference paper},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 4}
}

@CONFERENCE{2018,
	title = {CEUR Workshop Proceedings},
	year = {2018},
	journal = {CEUR Workshop Proceedings},
	volume = {2169},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052826406&partnerID=40&md5=22ff58304f67ee745b2d5e6c2401de43},
	abstract = {The proceedings contain 6 papers. The topics discussed include: crowd-sourcing updates of large knowledge graphs; interactive dictionary expansion using neural language models; making better job hiring decisions using 'human in the loop' techniques; use of internal testing data to help determine compensation for crowdsourcing tasks; rank scoring via active learning (RaScAL); and ESO-5W1H framework: ontological model for SITL paradigm. © 2018 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{2018,
	title = {15th International Conference on Extended Semantic Web Conference, ESWC 2018},
	year = {2018},
	journal = {Lecture Notes in Computer Science},
	volume = {10843 LNCS},
	pages = {},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048512134&partnerID=40&md5=bfd8e53705e792f9563a799a48481786},
	abstract = {The proceedings contain 48 papers. The special focus in this conference is on Extended Semantic Web Conference. The topics include: Answering Multiple-Choice Questions in Geographical Gaokao with a Concept Graph; TweetsKB: A Public and Large-Scale RDF Corpus of Annotated Tweets; HDTQ: Managing RDF Datasets in Compressed Space; answers Partitioning and Lazy Joins for Efficient Query Relaxation and Application to Similarity Search; evaluation of Schema.org for Aggregation of Cultural Heritage Metadata; dynamic Planning for Link Discovery; a Dataset for Web-Scale Knowledge Base Population; EventKG: A Multilingual Event-Centric Temporal Knowledge Graph; semantic Concept Discovery over Event Databases; GSP (Geo-Semantic-Parsing): Geoparsing and Geotagging with Machine Learning on Top of Linked Data; smart Papers: Dynamic Publications on the Blockchain; mind the (Language) Gap: Generation of Multilingual Wikipedia Summaries from Wikidata for ArticlePlaceholders; what Does an Ontology Engineering Community Look Like? A Systematic Analysis of the schema.org Community; FERASAT: A Serendipity-Fostering Faceted Browser for Linked Data; classifying Crises-Information Relevancy with Semantics; Efficient Temporal Reasoning on Streams of Events with DOTR; intelligent Clients for Replicated Triple Pattern Fragments; knowledge Guided Attention and Inference for Describing Images Containing Unseen Objects; Benchmarking of a Novel POS Tagging Based Semantic Similarity Approach for Job Description Similarity Computation; a Tri-Partite Neural Document Language Model for Semantic Information Retrieval; LDP-DL: A Language to Define the Design of Linked Data Platforms; multiple Models for Recommending Temporal Aspects of Entities; GDPRtEXT - GDPR as a Linked Data Resource; modeling and summarizing news events using semantic triples; Event-Enhanced Learning for KG Completion. © 2018 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}

@ARTICLE{20181,
	title = {Chinese Intelligent Systems Conference, CISC 2017},
	year = {2018},
	journal = {Lecture Notes in Electrical Engineering},
	volume = {459},
	pages = {1 - 781},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030863915&partnerID=40&md5=acec316e8f9df847d8bb62853b75a62a},
	abstract = {The proceedings contain 144 papers. The special focus in this conference is on Chinese Intelligent Systems. The topics include: Recursive state estimation for discrete-time nonlinear complex networks; adaptive observer design for quasi-one-sided lipschitz nonlinear systems; a feature selection method based on information gain and bp neural network; fault diagnosis of hoist braking system based on improved particle swarm optimization algorithm; attitude control of a quad-rotor based on LADRC; quasi-synchronization of chaotic systems with parameter mismatches via aperiodically intermittent control; interactive touch control method based on image denoising technology; EEG multi-fractal de-trended fluctuation mental stress analysis; the simulation of neural oscillations during propofol anesthesia based on the FPGA platform.; decision making in multi-agent systems based on the evolutionary game with switching probabilities; fault diagnosis of rolling bearing based on wavelet packet and extreme learning machine; event-triggered control for multi-agent system with a smart leader; visual based abnormal target annotation with recurrent neural networks; analysis of the fluid approximation of stochastic process algebra models; a wall interactive system based on infrared electronic pen; the design of new energy-saving DC brake device; data missing process by extended kalman filter with equality constraints; adaptive controller for flexible-joint robot; dynamic modeling and analysis of the micro-spacecraft ejection separation system; neural network adaptive control for hysteresis hammerstein system; microblog query expansion based on ontology expansion and borda count rank; microblogging event search based on LSTM model and microblog search method based on neural network language model. © 2017 Elsevier B.V., All rights reserved.},
	type = {Conference review},
	publication_stage = {Final},
	source = {Scopus},
	note = {Cited by: 0}
}


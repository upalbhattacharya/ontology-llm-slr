@incollection{BUTKA20201,
title = {Chapter 1 - Methodologies for Knowledge Discovery Processes in Context of AstroGeoInformatics},
editor = {Petr Škoda and Fathalrahman Adam},
booktitle = {Knowledge Discovery in Big Data from Astronomy and Earth Observation},
publisher = {Elsevier},
pages = {1-20},
year = {2020},
isbn = {978-0-12-819154-5},
doi = {https://doi.org/10.1016/B978-0-12-819154-5.00010-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191545000102},
author = {Peter Butka and Peter Bednár and Juliana Ivančáková},
keywords = {Knowledge discovery process, Data mining, Methodology, Process modeling, Ontology, AstroGeoInformatics},
abstract = {Successful data science projects usually follow some methodology which can provide the data scientist with basic guidelines on how to challenge the problem and how to work with data, algorithms, or models. This methodology is then a structured way to describe the knowledge discovery process. Without a flexible structure of steps, data science projects can be unsuccessful, or at least it will be hard to achieve a result that can be easily applied and shared. Their better understanding is quite beneficial both to data scientists and to anyone who needs to discuss results or steps of the process. Moreover, in some domains, including those working with data from astronomy and geophysics, steps used in preprocessing and analysis of data are crucial to understanding provided data products. In this chapter, we provide an overview of knowledge discovery processes, selected methodologies, and their standardization and sharing using process languages and ontologies. At the end of the chapter, we also discuss these aspects according to the domain of astro/geo data.}
}
@article{LIETO20231,
title = {DEGARI 2.0: A diversity-seeking, explainable, and affective art recommender for social inclusion},
journal = {Cognitive Systems Research},
volume = {77},
pages = {1-17},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2022.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041722000456},
author = {Antonio Lieto and Gian Luca Pozzato and Manuel Striani and Stefano Zoia and Rossana Damiano},
keywords = {Explainable AI, Diversity-seeking emotional recommendations, Description logics, Commonsense reasoning},
abstract = {We present DEGARI 2.0 (Dynamic Emotion Generator And ReclassIfier): an explainable, affective-based, art recommender relying on the commonsense reasoning framework TCL and exploiting an ontological model formalizing the Plutchik’s theory of emotions. The main novelty of this system relies on the development of diversity-seeking affective recommendations obtained by exploiting the spatial structure of the Plutchik’s ‘wheel of emotion’. In particular, such development allows to classify and to suggest, to museum users, cultural items able to evoke not only the very same emotions of already experienced or preferred objects (e.g. within a museum exhibition), but also novel items sharing different emotional stances. The system’s goal, therefore, is to break the filter bubble effect and open the users’ view towards more inclusive and empathy-based interpretations of cultural content. The system has been tested, in the context of the EU H2020 SPICE project, on the community of deaf people and on the collection of the GAM Museum of Turin. We report the results and the lessons learnt concerning both the acceptability and the perceived explainability of the received diversity-seeking recommendations.}
}
@article{SAMAVI20181,
title = {Publishing privacy logs to facilitate transparency and accountability},
journal = {Journal of Web Semantics},
volume = {50},
pages = {1-20},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300088},
author = {Reza Samavi and Mariano P. Consens},
keywords = {Privacy, Policy, Audit log, Accountability, Linked data, Semantic web, Ontology},
abstract = {Compliance with privacy policies imposes requirements on organizations and their information systems. Maintaining auditable privacy logs is one of the key mechanisms employed to ensure compliance, but the logs and their auditing reports are designed and implemented on an application by application basis. This paper develops a Linked Data model and ontologies to facilitate the sharing of logs that support privacy auditing and information accountability among multiple applications and participants. The L2TAP modular ontologies accommodate a variety of privacy scenarios and policies. SCIP is the key module that synthesizes contextual integrity concepts and enables query based solutions that facilitate privacy auditing. Other L2TAP modules describe logs, participants, and log events, all identified by web accessible URIs and include relevant provenance information to support accountability. A health self-management scenario is used to illustrate how privacy preferences, accountability obligations, and access to personal information can be published and accessed as Linked Data by multiple participants, including the internal and external auditors. We contribute query based algorithmic solutions for two fundamental privacy auditing processes that analyse L2TAP logs: obligation derivation and compliance checking. The query based solutions that we develop require SPARQL implementations with limited RDFS reasoning power, and are therefore widely supported by commercial and open source systems. We also provide experimental validation of the scalability of our query based solution for compliance checking over L2TAP logs.}
}
@article{CUI2025103454,
title = {Beyond the images: Comprehensible unsafe behaviour recognition boosted by joint inference graph with multi-hop reasoning},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103454},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103454},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003477},
author = {Dongdong Cui and Sheng Xu and Shunyi Wang and Kaiqi Zhang},
keywords = {Knowledge graph, Computer vision, Network analysis, Construction scenario semantic molecule, Unsafe behaviour recognition},
abstract = {Computer vision (CV) technology has gained widespread adoption in monitoring unsafe behaviours, and efficiently improved the construction safety performance. However, current computer vision technologies focus primarily on image recognition while neglecting reasoning beyond the image by integrating multi-source knowledge graphs. In complex and dynamic construction environments, monitoring unsafe behaviours often requires deeper reasoning by integrating various external information such as regulatory texts and accident data. Therefore, this study proposed a comprehensible unsafe behaviour recognition framework based on a joint inference graph with multi-hop reasoning. The framework utilizes the combined reasoning of computer vision and an integrated knowledge graph to identify and reason about unsafe behaviours, providing corresponding mitigation measures. Specifically, regulatory texts and accident data were first aligned to build an integrated knowledge graph, which was demonstrated to be more in terms of network scale, information propagation connectivity, and node influence distribution, making it suitable for multi-hop reasoning. Then, the joint inference subgraph of the integrated knowledge graph was compared to the construction scene semantic molecules (CSSMs), derived from CV recognition results using the YOLOv10 model, to recognize unsafe behaviours and simultaneously provide related regulatory requirements, potential consequences, and possible mitigation measures. The proposed approach was tested by self-constructed dataset and the results showed an average accuracy of 94.10 %. Lastly, the feasibility and practicality of the method is verified by the implementation of unsafe behaviour recognition in five work scenarios.}
}
@article{HUANG2025113770,
title = {ES-MRE: Evidence subgraph enhanced reasoning for multimodal relation extraction},
journal = {Knowledge-Based Systems},
volume = {325},
pages = {113770},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113770},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125008160},
author = {Wenti Huang and Jiayi Chen and Junjie Li and Yiyu Mao and Ningyi Mao},
keywords = {Multimodal reasoning, Relation extraction, Knowledge graph},
abstract = {The aim of multimodal relation extraction is to identify the semantic relations between two named entities by analyzing linguistic sequences and associated images. However, existing approaches are frequently limited due to inadequate information, which is often attributable to the brevity and ambiguity of the texts. Moreover, these models are easily affected by irrelevant objects during inter-modal alignment, leading to a problem known as error sensitivity. In this paper, we propose the Evidence Subgraph Enhanced Reasoning for Multimodal Relation Extraction (ES-MRE) framework. Specifically, the MLLM-guided Evidence Subgraph Generation (MESG) module is introduced, which leverages the image understanding capabilities of Multimodal Large Language Models (MLLMs) to extract visual and textual entities from image descriptions and original texts. It retrieves the clue paths of different entities in existing Knowledge Graphs (KGs), generating an Evidence Subgraph to provide structural features of inter-entity factual knowledge for relation reasoning. Additionally, we introduce a Multi-image Hierarchical Fusion (MHF) module that treats the generated image as a back-translation of the text, hierarchically fine-grained aligned text and generated images as well as the original image to mitigate the impact of irrelevant objects. Experimental results using the most popular dataset demonstrate the effectiveness of our approach, and ablation studies further validate the contributions of the MESG and MHF modules.}
}
@article{NAWAZ2020102383,
title = {Extractive Text Summarization Models for Urdu Language},
journal = {Information Processing & Management},
volume = {57},
number = {6},
pages = {102383},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102383},
url = {https://www.sciencedirect.com/science/article/pii/S0306457320308785},
author = {Ali Nawaz and Maheen Bakhtyar and Junaid Baber and Ihsan Ullah and Waheed Noor and Abdul Basit},
keywords = {Natural Language Processing, Sentence Weight Algorithm, Text Summarization, Urdu Language, Weighted Term Frequency},
abstract = {In the recent few years, a lot of advancement has been made in Urdu linguistics. There are many portals and news websites that are generating a huge amount of data every day. However, there is still no publicly available dataset nor any framework available for automatic Urdu extractive summary generation. In an automatic extractive summary generation, the sentences with the highest weights are given importance to be included in the summary. The sentence weight is computed by the sum of the weights of the words in the sentence. There are two famous approaches to compute the weight of the words in the English language: local weights (LW) approach and global weights (GW) approach. The sensitivity of the weights depends on the contents of the text, the one word may have different weights in a different article, known as LW based approach. Whereas, in the case of GW, the weights of the words are computed from the independent dataset, which implies the weights of all words remain the same in different articles. In the proposed framework, LW and GW based approaches are modeled for the Urdu language. The sentence weight method and the weighted term-frequency method are LW based approaches that compute the weights of the sentences by the sum of important words and the sum of frequencies of the important words, respectively. Whereas, vector space model (VSM) is GW based approach, that computes the weight of the words from the independent dataset, and then remain the same for all types of the text; GW is widely used in the English language for various applications such as information retrieval and text classification. The extractive summaries are generated by LW and GW based approaches and evaluated with ground-truth summaries that are obtained by the experts. The VSM is used as a baseline framework for sentence weighting. Experiments show that LW based approaches are better for extractive summary generation. The F-score of the sentence weight method and the weighted term-frequency method are 80% and 76%, respectively. The VSM achieved only 62% accuracy on the same dataset. Both, the datasets with ground-truth, and the code are made publicly available for the researchers.}
}
@article{VISALLI2023104765,
title = {First steps towards FAIRization of product-focused sensory data},
journal = {Food Quality and Preference},
volume = {104},
pages = {104765},
year = {2023},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2022.104765},
url = {https://www.sciencedirect.com/science/article/pii/S0950329322002403},
author = {Michel Visalli and Pascal Schlich and Benjamin Mahieu and Arnaud Thomas and Magalie Weber and Elisabeth Guichard},
keywords = {Ontology, Data-driven research, Meta-analyses, Databases, Typology, Minimum information standards},
abstract = {The emergence of open science makes it necessary to formalize data and knowledge in a way that adheres to the FAIR (Findable, Accessible, Interoperable, Reusable) principles. The objective of this work was to lay the foundations for such a FAIRization of sensory data. After an explanation of the rationale, the main characteristics of sensory evaluation methods were identified, and a data-centric typology of the different types of measures was proposed. This typology enabled us to classify the sensory evaluation methods regardless of whether they were used in “sensory only” or “consumer-centric product-focused” research. To complete this conceptual typology, minimal information standards have been defined to help in the reporting and interpreting of sensory evaluation data. These standards include a data format and metadata related to sensory evaluation measures. Based on the typology and metadata, several aspects related to sensory data aggregation were discussed, and examples were presented to demonstrate the interest of such a grouping of datasets. The outputs of this work were intended to be implemented in the “sensory evaluation measures” branch of TransformON, a food ontology. Thus, this work opens up new perspectives related to the use of ontologies for data-driven research related to food sustainability. Moreover, it invites the sensory science community to adopt a broader perspective regarding sensory measures.}
}
@article{CAIN2025103535,
title = {Caring for technologies, caring for Country},
journal = {Futures},
volume = {166},
pages = {103535},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103535},
url = {https://www.sciencedirect.com/science/article/pii/S0016328724002180},
author = {Anna Cain},
keywords = {Feminist care ethics, Caring for Country, Indigenous Australians, Energy social science, Sustainability transitions, Maintenance, Energy futures},
abstract = {Care is an emerging theoretical tool supporting analysis of socioecological equity impacts as energy systems are transformed to support more sustainable futures. Derived from feminist critiques of rationalist, market-led approaches, energy scholars use care to draw attention to the matters of care that are counted into energy system transitions and the care labour required to realise these transitions. Less attention has been applied to non-Western concepts of care and how they might provide alternative futures through energy. This paper draws on Tronto’s (2013) phases of care framework to investigate how care shapes, flows through and is enabled by renewable energy programs in remote Indigenous communities in Australia. Using data collected through multi-sited project ethnography, this analysis considers how care is defined and built into energy program design and implementation. Interrogating these care logics illustrates the importance of prioritising sociocultural alongside technical forms of care. Understanding energy in this way offers insights into the role of energy in underpinning Indigenous futures, by supporting Indigenous ontological imperatives to exist in and care for Country, as well as insights into what it means to care at scale with energy through sustainability transitions.}
}
@article{XU20193,
title = {Improving Publication Pipeline with Automated Biological Entity Detection and Validation Service},
journal = {Data and Information Management},
volume = {3},
number = {1},
pages = {3-17},
year = {2019},
issn = {2543-9251},
doi = {https://doi.org/10.2478/dim-2019-0003},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122000705},
author = {Weijia Xu and Amit Gupta and Pankaj Jaiswal and Crispin Taylor and Patti Lockhart and Jennifer Regala},
keywords = {entity extraction, digital curation, digital library, machine learning, ontology, text mining, natural language processing},
abstract = {With the increasing amount of digital journal submissions, there is a need to deploy new scalable computational methods to improve information accessibilities. One common task is to identify useful information and named entity from text documents such as journal article submission. However, there are many technical challenges to limit applicability of the general methods and lack of general tools. In this paper, we present domain informational vocabulary extraction (DIVE) project, which aims to enrich digital publications through detection of entity and key informational words and by adding additional annotations. In a first of its kind to our knowledge, our system engages authors of the peer-reviewed articles and the journal publishers by integrating DIVE implementation in the manuscript proofing and publication process. The system implements multiple strategies for biological entity detection, including using regular expression rules, ontology, and a keyword dictionary. These extracted entities are then stored in a database and made accessible through an interactive web application for curation and evaluation by authors. Through the web interface, the authors can make additional annotations and corrections to the current results. The updates can then be used to improve the entity detection in subsequent processed articles in the future. We describe our framework and deployment in details. In a pilot program, we have deployed the first phase of development as a service integrated with the journals Plant Physiology and The Plant cell published by the American Society of Plant Biologists (ASPB). We present usage statistics to date since its production on April 2018. We compare automated recognition results from DIVE with results from author curation and show the service achieved on average 80% recall and 70% precision per article. In contrast, an existing biological entity extraction tool, a biomedical named entity recognizer (ABNER), can only achieve 47% recall and return a much larger candidate set.}
}
@article{GIORDANO2023120499,
title = {Unveiling the inventive process from patents by extracting problems, solutions and advantages with natural language processing},
journal = {Expert Systems with Applications},
volume = {229},
pages = {120499},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120499},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423010011},
author = {Vito Giordano and Giovanni Puccetti and Filippo Chiarello and Tommaso Pavanello and Gualtiero Fantoni},
keywords = {Information Retrieval, Inventive Process, Language Model, Natural Language Processing, Patent Analysis},
abstract = {Patents are the main means for disclosing an invention. These documents encompass many steps of the inventive process starting with the definition of the problem to be solved and ending with the identification of a solution. In this study we focus on three fundamental concepts of the inventive process: (A) technical problems; (B) solutions; and (C) advantageous effects of the invention, which, based on the WIPO guidelines, any patent should include. We propose a system based on Natural Language Processing (NLP) pipeline that uses transformer language models to identify technical problems, solutions and advantageous effects from patents. We use a training dataset composed of 480,000 patents sentences contained in sections manually labelled by inventors or attorneys. Our model reaches a F1 score of 90%. The model is evaluated on a random set of patents to assess its deployability in a real-world scenario. The proposed model can be used as a novel tool for prior art mapping, novel ideas generation and technological evolution identification and can help to disclose valuable information hidden in patent documents.}
}
@article{SCHMIDTKE2021162,
title = {Multi-modal actuation with the activation bit vector machine},
journal = {Cognitive Systems Research},
volume = {66},
pages = {162-175},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720300966},
author = {H.R. Schmidtke},
keywords = {Symbol grounding problem, Vector symbolic architectures, Action verbs, Activation bit vector machine, Context logic},
abstract = {Research towards a new approach to the abstract symbol grounding problem showed that through model counting there is a correspondence between logical/linguistic and coordinate representation in the visuospatial domain. The logical/verbal description of a spatial layout directly gives rise to a coordinate representation that can be drawn, with the drawing reflecting what is described. The main characteristic of this logical property is that it does not need any semantic information or ontology apart from a separation into symbols/words referring to relations and symbols/words referring to objects. Moreover, the complete mechanism can be implemented efficiently on a brain inspired cognitive architecture, the Activation Bit Vector Machine (ABVM), an architecture that belongs to the Vector Symbolic Architectures. However, the natural language fragment captured previously was restricted to simple predication sentences, with the corresponding logical fragment being atomic Context Logic (CLA), and the only actuation modality leveraged was visualization. This article extends the approach on all three aspects: adding a third category of action verbs we move to a fragment of first-order Context Logic (CL1), with modalities requiring a temporal dimension, such as film and music, becoming available. The article presents an ABVM generating sequences of images from texts.}
}
@article{BIBBO2022,
title = {Model-Driven Development of Groupware Systems},
journal = {International Journal of e-Collaboration},
volume = {18},
number = {1},
year = {2022},
issn = {1548-3673},
doi = {https://doi.org/10.4018/IJeC.295151},
url = {https://www.sciencedirect.com/science/article/pii/S1548367322000254},
author = {Luis Mariano Bibbo and Claudia Pons and Roxana Giandini},
keywords = {Awareness, Code Generation, Collaborative Software, Groupware, Meta-Model, Model-Driven Engineering},
abstract = {ABSTRACT
Building collaborative systems with awareness (or groupware) is a very complex task. This article presents the use of the domain-specific language CSSL v2.0—Collaborative Software System Language—built as an extension of UML using the metamodeling mechanism. CSSL provides simplicity, expressiveness, and precision to model the main concepts of collaborative systems, especially collaborative processes, protocols, and awareness. The CSSL concrete syntax is defined via a set of editors through which collaborative systems models are created. According to the MDD methodology, models are independent of the implementation platform and are formally prepared to be transformed. The target of the transformation is a web application that provides a set of basic functions that developers can refine to complete the development of the collaborative system. Finally, evaluation, validation, and verification of the language is performed, determining that the CSSL tools allow developers to solve central aspects of collaborative systems implementation in a simple and reasonable way.}
}
@article{LIU2022115853,
title = {Service planning oriented efficient object search: A knowledge-based framework for home service robot},
journal = {Expert Systems with Applications},
volume = {187},
pages = {115853},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115853},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421012136},
author = {Shaopeng Liu and Guohui Tian and Ying Zhang and Mengyang Zhang and Shuo Liu},
keywords = {Service planning, Object search, Ontology-based knowledge, Knowledge system, Service robot},
abstract = {In the unstructured family environment, robots are expected to provide various services to improve the quality of human life, based on the performance of specific action sequences generated by service planning. This paper focuses on one of the greatest challenges in service planning that is aimed at accomplishing the service tasks by generating appropriate object sequences to guide the robot on searching the corresponding target objects efficiently and reasonably. A well-structured knowledge-based framework of object search is proposed in our approach as well as taking into account the multi-domain knowledge of applying object, scene, and service in design. In order to improve the searching efficiency and reasonability, an ontology-based hierarchical and interrelated knowledge structure is formed to support the implementation of complicated service planning with either single or multiple tasks. The proposed framework is tested by comprehensive experiments, and the performance is evaluated with other mainstream methods in both simulation and real-world environments. The experimental results demonstrate the feasibility and effectiveness of applying this knowledge-based framework to efficient object searching aspect in service planning.}
}
@article{MU2024e35511,
title = {Identification of diagnostic biomarkers of rheumatoid arthritis based on machine learning-assisted comprehensive bioinformatics and its correlation with immune cells},
journal = {Heliyon},
volume = {10},
number = {15},
pages = {e35511},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e35511},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024115423},
author = {Kai-lang Mu and Fei Ran and Le-qiang Peng and Ling-li Zhou and Yu-tong Wu and Ming-hui Shao and Xiang-gui Chen and Chang-mao Guo and Qiu-mei Luo and Tian-jian Wang and Yu-chen Liu and Gang Liu},
keywords = {Rheumatoid arthritis, Biomarkers, Diagnostic genes, Immune cells, Machine learning, Bioinformatics},
abstract = {Background
Rheumatoid arthritis (RA) is a chronic systemic autoimmune disease characterized by inflammatory cell infiltration, which can lead to chronic disability, joint destruction and loss of function. At present, the pathogenesis of RA is still unclear. The purpose of this study is to explore the potential biomarkers and immune molecular mechanisms of rheumatoid arthritis through machine learning-assisted bioinformatics analysis, in order to provide reference for the early diagnosis and treatment of RA disease.
Methods
RA gene chips were screened from the public gene GEO database, and batch correction of different groups of RA gene chips was performed using Strawberry Perl. DEGs were obtained using the limma package of R software, and functional enrichment analysis such as gene ontology (GO), Kyoto Encyclopedia of Genes and Genomes (KEGG), disease ontology (DO), and gene set (GSEA) were performed. Three machine learning methods, least absolute shrinkage and selection operator regression (LASSO), support vector machine recursive feature elimination (SVM-RFE) and random forest tree (Random Forest), were used to identify potential biomarkers of RA. The validation group data set was used to verify and further confirm its expression and diagnostic value. In addition, CIBERSORT algorithm was used to evaluate the infiltration of immune cells in RA and control samples, and the correlation between confirmed RA diagnostic biomarkers and immune cells was analyzed.
Results
Through feature screening, 79 key DEGs were obtained, mainly involving virus response, Parkinson's pathway, dermatitis and cell junction components. A total of 29 hub genes were screened by LASSO regression, 34 hub genes were screened by SVM-RFE, and 39 hub genes were screened by Random Forest. Combined with the three algorithms, a total of 12 hub genes were obtained. Through the expression and diagnostic value verification in the validation group data set, 7 genes that can be used as diagnostic biomarkers for RA were preliminarily confirmed. At the same time, the correlation analysis of immune cells found that γδT cells, CD4+ memory activated T cells, activated dendritic cells and other immune cells were positively correlated with multiple RA diagnostic biomarkers, CD4+ naive T cells, regulatory T cells and other immune cells were negatively correlated with multiple RA diagnostic biomarkers.
Conclusions
The results of novel characteristic gene analysis of RA showed that KYNU, EVI2A, CD52, C1QB, BATF, AIM2 and NDC80 had good diagnostic and clinical value for the diagnosis of RA, and were closely related to immune cells. Therefore, these seven DEGs may become new diagnostic markers and immunotherapy markers for RA.}
}
@incollection{SHANKS202484,
title = {Social Theory in Archaeology},
editor = {Efthymia Nikita and Thilo Rehren},
booktitle = {Encyclopedia of Archaeology (Second Edition) (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {84-93},
year = {2024},
isbn = {978-0-323-91856-5},
doi = {https://doi.org/10.1016/B978-0-323-90799-6.00214-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323907996002147},
author = {Michael Shanks},
keywords = {Archaeological theory, History of archaeology, Pragmatism, Science studies, Social archaeology, Social theory},
abstract = {Social theory in archaeology is conventionally summarized as different schools of thought (paradigms and isms), usually associated with thought leaders in the discipline, that aim to reconstruct past societies. Involved are questions of social ontology (asking—what is society and its components?), and epistemology (asking—how might we come to know societies?). In contrast, this contribution avoids the description of schools of thought and, following a pragmatist line, treats social theory in archaeology as a key component of thoughtful practice; what archaeologists do. The components of practice such as opening brief, orientation, deliverable, frame, and project management are related to social theory. The concepts of social theory are argued to form a tool kit mobilized in archaeological research. A major section outlines some key concepts of social theory, including society, culture, systems, networks, agency, power, identity, personhood, and complexity.}
}
@article{XIAO202589,
title = {MetaFactory: A cloud-based framework to configure and generate dynamic data structures from the STEP-NC knowledge graph},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {89-107},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525000421},
author = {Wenlei Xiao and Tianze Qiu and Jiurong Guo and Gang Zhao},
keywords = {Twin-oriented manufacturing, STEP-NC, Dynamic data structure, Object-oriented database, Knowledge graph},
abstract = {In our previous studies, twin-oriented manufacturing has been identified as a crucial solution to address the manufacturing crisis. Within this context, the notion of “digital twin as a service” necessitates that various twin services share and communicate with each other in a standardized manner. STEP-NC offers a potentially unified model to facilitate data exchange, providing object-oriented and standardized data models for a comprehensive representation of manufacturing resources in the digital realm. However, the complexity of STEP-NC renders it too cumbersome for implementation in diverse cloud-based services or PC-based software. This complexity is a fundamental reason why STEP-NC has struggled to find application in commercial CNC systems despite years of research. To overcome this technical challenge, this paper introduces a novel concept termed “dynamic STEP-NC data structure”, inspired by the dynamic language philosophy of dynamic programming language (such as Python). This approach allows different services and software packages to maintain their own data definitions while still aligning with the original STEP-NC definition. We have developed a framework called MetaFactory that supports the configuration of streamlined data structures and generates the corresponding program code required by various service developers. On this basis, we implemented automatic modeling for a STEP-NC object-oriented database. Using the data trimming and dimensionality reduction methods provided by MetaFactory, several prototype systems for different application scenarios have been developed.}
}
@article{NISTICO20242230,
title = {Innovative tools and methods for digitizing both visible and non-visible attributes of cultural heritage items. Part I: needed tools and strategies},
journal = {Procedia Structural Integrity},
volume = {64},
pages = {2230-2237},
year = {2024},
note = {SMAR 2024 – 7th International Conference on Smart Monitoring, Assessment and Rehabilitation of Civil Structures},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2024.09.357},
url = {https://www.sciencedirect.com/science/article/pii/S2452321624009612},
author = {Nicola Nisticò},
keywords = {digitizattion, digiltalization, digital twins, metaverso},
abstract = {Digitization marks the initial phase of digital transformation, encompassing a spectrum of socio-technical developments that unfold thereafter. These processes are pivotal components within a broader domain that: 1) encompasses physical inspection methodologies, business models geared towards enhancing inspection efficiency, integrity engineering, and decision-making; and 2) furnishes invaluable data to enrich the design, production, and maintenance over the lifespan of cultural heritage artifacts. Vrana and Sing (2020) advocate for designating this domain as Non-Destructive Evaluation (NDE) 4.0, underscoring its characterization as "a suite of cyber-physical technologies." These technologies encompass Digital Twin (DT), Industrial Internet of Things (IIoT), Semantic Interoperability and Ontologies, Industry 4.0 Data Processing (Big Data Analysis), Blockchains, Non-Fungible Tokens (NFTs), and Artificial Intelligence. The concept of Digital Twin, introduced in 2003 at the University of Michigan during the Executive Course on Product Lifecycle Management (PLM), as articulated by Grieves (2014), holds particular significance. Glaessgen and Stargel (2012) subsequently advocated for Digital Twins to support the production of NASA and U.S. Air Force vehicles, envisioning them as enabling "integrated multiphysics, multiscale, probabilistic simulation of an as-built vehicle or system that utilizes the best available physical models, sensor updates, fleet history, etc." Within this framework, a methodology will be elucidated for devising and implementing innovative tools and techniques for digitizing and digitalizing attributes of cultural heritage items.}
}
@article{BARATI2019100533,
title = {Automated Class Correction and Enrichment in the Semantic Web},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100533},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.100533},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300605},
author = {Molood Barati and Quan Bai and Qing Liu},
keywords = {Semantic Web data quality issues, Ontology, Incorrect class assignment, Class enrichment, Ontology enrichment, Information theory},
abstract = {The Semantic Web is an effort to interchange unstructured data over the Web into a structured format that is processable not only by human beings but also computers. The key backbones of Semantic Web are ontologies and annotations that provide semantics for data. Ontologies are usually created before actual data is populated. Subsequently, they can be incomplete and they often do not provide all aspects that are required for specific domains of knowledge. Additionally, Semantic Web-based ontologies usually suffer from a considerable amount of faulty facts which are known as Semantic Web data quality issues. Due to the complexity of relationships, Semantic Web data quality issues are continuously growing. This paper follows two main objectives. Firstly, it concentrates on a specific Semantic Web data quality issue that indicates incorrect assignment between instances and classes in the ontology. Secondly, the paper shows how to discover new classes which are not defined in the ontology and how to place them in the hierarchical structure of the ontology. To make ends meet, an entropy-based approach called ACE (Automated Class Corrector and Enricher) is proposed that not only evaluates the correctness and incorrectness of relationships between instances and classes but also generates new classes to enrich ontologies. The contributions of ACE have been also explained throughout the paper. Initial experiments conducted on a Semantic Web dataset demonstrate the effectiveness of the ACE.}
}
@article{URGO2020377,
title = {Formal modelling of release control policies as a plug-in for performance evaluation of manufacturing systems},
journal = {CIRP Annals},
volume = {69},
number = {1},
pages = {377-380},
year = {2020},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2020.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0007850620300287},
author = {Marcello Urgo and Walter Terkaj},
keywords = {Control policies, Performance evaluation, Ontology models},
abstract = {Control policies significantly affect the performance of manufacturing systems, driving the need to assess their impact during both the design and operational phases. Performance evaluation tools can provide a relevant support, but their full exploitation is hindered by the difficulty of considering the huge variety of control decisions that are interwoven with manufacturing system configurations. Herein, a formal modelling approach is presented to jointly describe a manufacturing system and its release control policies, thus enabling the definition of performance evaluation models in terms of different policies. An application case is provided for the automatic generation of discrete event simulation models to assess the viability of the approach for assembly lines.}
}
@article{LIN20191880,
title = {Knowledge Reasoning for Intelligent Manufacturing Control System},
journal = {Procedia Manufacturing},
volume = {39},
pages = {1880-1888},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.250},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920303140},
author = {Yu-Ju Lin and Zheng-Xian Chen and Chin-Yin Huang},
keywords = {Knowledge-Driven Approach, Ontology, SCADA, Smart Factory Control, SPARQL},
abstract = {Manufacturing Control System (MCS) is regarded to have a capability to deliver a response in the manufacturing system to cope with the unexpected events, e.g., machine failures, order changes, etc. It is the core of smart factory control. However, most of today’s MCSs are still with the Supervisory Control and Data Acquisition (SCADA) architecture, which is in conflict with the flexible and adaptive characteristics. On the other hand, knowledge-driven approach is considered an alternative of flexibility, robustness, and re-configurability. By taking the knowledge-drive initiative, this research develops an intelligent MCS (iMCS). There are two cores in iMCS: Ontology and SPARQL. The ontology classifies and describes the relationships between objects of a particular domain with tree structure, whereas SPARQL plays a role of query/inference for data or metadata. This research applies SPARQL to trigger the ontology of the manufacturing system based on the given events. With such an approach, iMCS can timely respond and control the manufacturing conditions such as a new dispatching due to machine failure. iMCS adapts the manufacturing system to the dynamic situations.}
}
@article{STEPHAN2025290,
title = {Fostering Model Reuse in Model-based Systems Engineering using Knowledge Graphs},
journal = {Procedia CIRP},
volume = {136},
pages = {290-295},
year = {2025},
note = {35th CIRP Design 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.08.051},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125008042},
author = {Roman Stephan and Thomas Schumacher and David Inkermann},
keywords = {knowledge graph, labeled property graph, model reuse, mode-based systems engineering, transformation concept},
abstract = {The increasing spread of MBSE is associated with the issue of reusing the knowledge contained in different system models in different development projects. Existing approaches for the reuse of system models or elements of these can be classified into framework-based, retrieval-based, and pattern-based. While framework- and pattern-based approaches require standardized modelling methods, retrieval-based approaches are based on merging of different system models. This is not supported by current MBSE-tools. In this contribution we investigate how knowledge graphs can be generated from various SysML system models and whether and how new knowledge, e.g., about cause-effect relationships or structural patterns, can be determined.}
}
@article{YANG2025116061,
title = {A systematic review of building energy performance forecasting approaches},
journal = {Renewable and Sustainable Energy Reviews},
volume = {223},
pages = {116061},
year = {2025},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2025.116061},
url = {https://www.sciencedirect.com/science/article/pii/S1364032125007348},
author = {Yizhou Yang and Qiuhua Duan and Forooza Samadi},
keywords = {Building energy performance forecasting (BEPF), Physics-based modeling, Black-box modeling, Hybrid-driven, Thermal properties of materials, Weather impact, Occupant behavior, Real-time adaptability, Artificial intelligence (AI) technique},
abstract = {Building energy performance forecasting (BEPF) is an active area of research with the potential to improve the efficiency of building energy management systems, support global sustainability goals, and mitigate climate change impacts. This systematic review examines three main prediction methods: model-driven, data-driven, and hybrid-driven, each with different principles, basics, advantages, disadvantages, practical applications, challenges, and limitations in addressing the complexities of building energy performance. The review focuses on key influencing factors, including building features, climatic conditions, and occupant behavior, while identifying critical research gaps in current methodologies. Through a bibliometric analysis of 95 relevant publications from 2019 to 2024, this review provides a quantitative overview of research progress and emerging trends. Findings indicate that although BEPF techniques have evolved rapidly, most studies continue to overlook the variability and complexity of occupant behavior, a factor with significantly affects forecast accuracy. To address this, we propose a modular AI-integrated forecasting framework that leverages the strengths of existing approaches, integrates real-time IoT data, and incorporate advanced artificial intelligence techniques, such as generative Artificial Intelligence, reinforcement learning, and Large Language Models (LLMs). A decision-making framework is also introduced to guide method selection based on specific building characteristics, data availability, desired accuracy, and operational goals, offering practical guidance for engineering and policy applications. Additionally, future research should extend beyond individual building dynamics to include a wider range of community-level determinants, such as policy frameworks, economic factors, and social determinants of health considerations (SDOH), aiming for a more comprehensive understanding of building energy consumption patterns. This review not only synthesizes current knowledge but also lays the foundation for future innovations in BEPF. We advocate for moving towards an AI-enhanced, adaptive forecasting model that can integrate different driven methods, capture the variability and unpredictability of occupant behavior, and improve the accuracy and reliability of energy forecasts.}
}
@article{OYELADE2020100395,
title = {A case-based reasoning framework for early detection and diagnosis of novel coronavirus},
journal = {Informatics in Medicine Unlocked},
volume = {20},
pages = {100395},
year = {2020},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2020.100395},
url = {https://www.sciencedirect.com/science/article/pii/S2352914820303683},
author = {Olaide N. Oyelade and Absalom E. Ezugwu},
keywords = {COVID-19, Coronavirus, Case-based reasoning, Ontology, Natural language processing},
abstract = {Coronavirus, also known as COVID-19, has been declared a pandemic by the World Health Organization (WHO). At the time of conducting this study, it had recorded over 11,301,850 confirmed cases while more than 531,806 have died due to it, with these figures rising daily across the globe. The burden of this highly contagious respiratory disease is that it presents itself in both symptomatic and asymptomatic patterns in those already infected, thereby leading to an exponential rise in the number of contractions of the disease and fatalities. It is, therefore, crucial to expedite the process of early detection and diagnosis of the disease across the world. The case-based reasoning (CBR) model is a compelling paradigm that allows for the utilization of case-specific knowledge previously experienced, concrete problem situations or specific patient cases for solving new cases. This study, therefore, aims to leverage the very rich database of cases of COVID-19 to solve new cases. The approach adopted in this study employs the use of an improved CBR model for state-of-the-art reasoning task in the classification of suspected cases of COVID-19. The CBR model leverages on a novel feature selection and the semantic-based mathematical model proposed in this study for case similarity computation. An initial population of the archive was achieved from 71 (67 adults and 4 pediatrics) cases obtained from the Italian Society of Medical and Interventional Radiology (SIRM) repository. Results obtained revealed that the proposed approach in this study successfully classified suspected cases into their categories with an accuracy of 94.54%. The study found that the proposed model can support physicians to easily diagnose suspected cases of COVID-19 based on their medical records without subjecting the specimen to laboratory tests. As a result, there will be a global minimization of contagion rate occasioned by slow testing and in addition, reduced false-positive rates of diagnosed cases as observed in some parts of the globe.}
}
@article{KONG2024114032,
title = {Entity recognition method for airborne products metrological traceability knowledge graph construction},
journal = {Measurement},
volume = {225},
pages = {114032},
year = {2024},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.114032},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123015968},
author = {Shengjie Kong and Xiang Huang and Xiao Zhong and Mingye Yang},
keywords = {Metrology, Airborne product, Entity recognition, Knowledge graph},
abstract = {The airborne system, as one of the complex and extensive subsystems of an aircraft, primarily performs critical flight assurance functions. The quality of its components has a direct impact on the aircraft's safety and reliability. Metrology documents comprehensively document the performance parameters throughout the entire product life cycle. The Metrological Traceability Knowledge Graph (MTKG) for airborne products offers decision support to engineers engaged in metrological tasks, ensuring the continuous high quality of the products. This paper introduces an entity recognition method for airborne product metrological traceability knowledge graph construction. First, the ontology for MTKG is developed. Next, a fine-tuned multi-network model is proposed. Named entities in the field of metrology are recognized through three stages: word vector representation, sentence feature extraction, and optimal label assignment. Meanwhile, active learning methods are incorporated to reduce the expense of data annotation. The proposed model is validated using an actual metrology corpus, and the experimental results demonstrate its superior performance compared to the other four baseline methods. Finally, the MTKG is developed using this approach, offering engineers intelligent applications, including metrological traceability analysis and traceability path reasoning within the process of product metrology. This enhances the metrology capabilities of airborne products and demonstrates the extensive potential of knowledge graphs in metrology.}
}
@article{LULLY2018211,
title = {Enhancing explanations in recommender systems with knowledge graphs},
journal = {Procedia Computer Science},
volume = {137},
pages = {211-222},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316259},
author = {Vincent Lully and Philippe Laublet and Milan Stankovic and Filip Radulovic},
keywords = {Recommender system, explanation, knowledge graph, hierarchical data, category, ontology, DBpedia, natural language, e-tourism},
abstract = {Recommender systems are becoming must-have facilities on e-commerce websites to alleviate information overload and to improve user experience. One important component of such systems is the explanations of the recommendations. Existing explanation approaches have been classified by style and the classes are aligned with the ones for recommendation approaches, such as collaborative-based and content-based. Thanks to the semantically interconnected data, knowledge graphs have been boosting the development of content-based explanation approaches. However, most approaches focus on the exploitation of the structured semantic data to which recommended items are linked (e.g. actor, director, genre for movies). In this paper, we address the under-studied problem of leveraging knowledge graphs to explain the recommendations with items’ unstructured textual description data. We point out 3 shortcomings of the state of the art entity-based explanation approach: absence of entity filtering, lack of intelligibility and poor user-friendliness. Accordingly, 3 novel approaches are proposed to alleviate these shortcomings. The first approach leverages a DBpedia category tree for filtering out incorrect and irrelevant entities. The second approach increases the intelligibility of entities with the classes of an integrated ontology (DBpedia, schema.org and YAGO). The third approach explains the recommendations with the best sentences from the textual descriptions selected by means of the entities. We showcase our approaches within a tourist tour recommendation explanation scenario and present a thorough face-to-face user study with a real commercial dataset containing 1310 tours in 106 countries. We showed the advantages of the proposed explanation approaches on five quality aspects: intelligibility, effectiveness, efficiency, relevance and satisfaction.}
}
@article{LI2021281,
title = {Framework for manufacturing-tasks semantic modelling and manufacturing-resource recommendation for digital twin shop-floor},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {281-292},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520301369},
author = {Xixing Li and Lei Wang and Chuanjun Zhu and Zhengchao Liu},
keywords = {DTS, MT, Semantic modelling, MR, Recommendation},
abstract = {With the widespread application of new digital and information technologies, manufacturing patterns have been reformed in different industries. The digital twin shop-floor (DTS), based on digital twins, has gradually become an advanced data-driven manufacturing model for modern manufacturing companies. The DTS integrates physical and virtual manufacturing processes through simulation and optimisation to achieve real-time mapping of data, thereby aiding managers in making more accurate and timely production decisions. However, the existing scheduling models and algorithms cannot effectively satisfy the accuracy and timeliness requirements of simulation and optimisation in DTS. Therefore, to create effective and rapid manufacturing resource (MR) recommendations for production services, this study established a framework for manufacturing task (MT) semantic modelling and MR dynamic recommendation (MT&MR) for DTS. Our model offers an effective approach to the description and conception of MTs based on ontology, MT semantic indexing and retrieval, and MR recommendation for DTS. Finally, a case analysis demonstrated that the method is effective and feasible.}
}
@article{JOHNSON20254711,
title = {Human interpretable grammar encodes multicellular systems biology models to democratize virtual cell laboratories},
journal = {Cell},
volume = {188},
number = {17},
pages = {4711-4733.e37},
year = {2025},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2025.06.048},
url = {https://www.sciencedirect.com/science/article/pii/S0092867425007500},
author = {Jeanette A.I. Johnson and Daniel R. Bergman and Heber L. Rocha and David L. Zhou and Eric Cramer and Ian C. Mclean and Yoseph W. Dance and Max Booth and Zachary Nicholas and Tamara Lopez-Vidal and Atul Deshpande and Randy Heiland and Elmar Bucher and Fatemeh Shojaeian and Matthew Dunworth and André Forjaz and Michael Getz and Inês Godet and Furkan Kurtoglu and Melissa Lyman and John Metzcar and Jacob T. Mitchell and Andrew Raddatz and Jacobo Solorzano and Aneequa Sundus and Yafei Wang and David G. DeNardo and Andrew J. Ewald and Daniele M. Gilkes and Luciane T. Kagohara and Ashley L. Kiemen and Elizabeth D. Thompson and Denis Wirtz and Laura D. Wood and Pei-Hsun Wu and Neeha Zaidi and Lei Zheng and Jacquelyn W. Zimmerman and Jude M. Phillip and Elizabeth M. Jaffee and Joe W. Gray and Lisa M. Coussens and Young Hwan Chang and Laura M. Heiser and Genevieve L. Stein-O’Brien and Elana J. Fertig and Paul Macklin},
keywords = {cell behavior hypothesis grammar, cell behaviors, multicellular systems, mathematical modeling, agent-based modeling, simulation, immunology, cancer biology, immunotherapy, spatial transcriptomics, cell interactions, tissue dynamics, physics of multicellular biology, multicellular systems biology, mathematical biology, modeling language, multi-omics},
abstract = {Summary
Cells interact as dynamically evolving ecosystems. While recent single-cell and spatial multi-omics technologies quantify individual cell characteristics, predicting their evolution requires mathematical modeling. We propose a conceptual framework—a cell behavior hypothesis grammar—that uses natural language statements (cell rules) to create mathematical models. This enables systematic integration of biological knowledge and multi-omics data to generate in silico models, enabling virtual “thought experiments” that test and expand our understanding of multicellular systems and generate new testable hypotheses. This paper motivates and describes the grammar, offers a reference implementation, and demonstrates its use in developing both de novo mechanistic models and those informed by multi-omics data. We show its potential through examples in cancer and its broader applicability in simulating brain development. This approach bridges biological, clinical, and systems biology research for mathematical modeling at scale, allowing the community to predict emergent multicellular behavior.}
}
@article{LIU2024,
title = {Evaluating Medical Entity Recognition in Health Care: Entity Model Quantitative Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/59782},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001455},
author = {Shengyu Liu and Anran Wang and Xiaolei Xiu and Ming Zhong and Sizhu Wu},
keywords = {natural language processing, NLP, model evaluation, macrofactors, medical named entity recognition models},
abstract = {Background
Named entity recognition (NER) models are essential for extracting structured information from unstructured medical texts by identifying entities such as diseases, treatments, and conditions, enhancing clinical decision-making and research. Innovations in machine learning, particularly those involving Bidirectional Encoder Representations From Transformers (BERT)–based deep learning and large language models, have significantly advanced NER capabilities. However, their performance varies across medical datasets due to the complexity and diversity of medical terminology. Previous studies have often focused on overall performance, neglecting specific challenges in medical contexts and the impact of macrofactors like lexical composition on prediction accuracy. These gaps hinder the development of optimized NER models for medical applications.
Objective
This study aims to meticulously evaluate the performance of various NER models in the context of medical text analysis, focusing on how complex medical terminology affects entity recognition accuracy. Additionally, we explored the influence of macrofactors on model performance, seeking to provide insights for refining NER models and enhancing their reliability for medical applications.
Methods
This study comprehensively evaluated 7 NER models—hidden Markov models, conditional random fields, BERT for Biomedical Text Mining, Big Transformer Models for Efficient Long-Sequence Attention, Decoding-enhanced BERT with Disentangled Attention, Robustly Optimized BERT Pretraining Approach, and Gemma—across 3 medical datasets: Revised Joint Workshop on Natural Language Processing in Biomedicine and its Applications (JNLPBA), BioCreative V CDR, and Anatomical Entity Mention (AnatEM). The evaluation focused on prediction accuracy, resource use (eg, central processing unit and graphics processing unit use), and the impact of fine-tuning hyperparameters. The macrofactors affecting model performance were also screened using the multilevel factor elimination algorithm.
Results
The fine-tuned BERT for Biomedical Text Mining, with balanced resource use, generally achieved the highest prediction accuracy across the Revised JNLPBA and AnatEM datasets, with microaverage (AVG_MICRO) scores of 0.932 and 0.8494, respectively, highlighting its superior proficiency in identifying medical entities. Gemma, fine-tuned using the low-rank adaptation technique, achieved the highest accuracy on the BioCreative V CDR dataset with an AVG_MICRO score of 0.9962 but exhibited variability across the other datasets (AVG_MICRO scores of 0.9088 on the Revised JNLPBA and 0.8029 on AnatEM), indicating a need for further optimization. In addition, our analysis revealed that 2 macrofactors, entity phrase length and the number of entity words in each entity phrase, significantly influenced model performance.
Conclusions
This study highlights the essential role of NER models in medical informatics, emphasizing the imperative for model optimization via precise data targeting and fine-tuning. The insights from this study will notably improve clinical decision-making and facilitate the creation of more sophisticated and effective medical NER models.}
}
@article{DHAYNE2021107236,
title = {EMR2vec: Bridging the gap between patient data and clinical trial},
journal = {Computers & Industrial Engineering},
volume = {156},
pages = {107236},
year = {2021},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2021.107236},
url = {https://www.sciencedirect.com/science/article/pii/S0360835221001406},
author = {Houssein Dhayne and Rima Kilany and Rafiqul Haque and Yehia Taher},
keywords = {EMR, Clinical trial, Medical data integration, Neural network, Semantic web},
abstract = {The human suffering from diseases caused by life-threatening viruses such as SARS, Ebola, and COVID-19 motivated many of us to study and discover the best means to harness the potential of data integration to assist clinical researchers to curb these viruses. Integrating patients data with clinical trials data is enormously promising as it provides a comprehensive knowledge base that accelerates the clinical research response-ability to tackle emerging infectious disease outbreaks. This work introduces EMR2vec, a platform that customises advanced NLP, machine learning and semantic web techniques to link potential patients to suitable clinical trials. Linking these two different but complementary datasets allows clinicians and researchers to compare patients to clinical research opportunities or to automatically select patients for personalized clinical care. The platform derives a ’bag of medical terms’ (BoMT) from eligibility criteria by normalizing extracted entities through SNOMED-CT ontology. With the usage of BoMT, an ontological reasoning method is proposed to represent EMR and clinical trials in a vector space model. The platform presents a matching process that reduces vector dimensionality using a neural network, then applies orthogonality projection to measure the similarity between vectors. Finally, the proposed EMR2vec platform is evaluated with an extendable prototype based on Big data tools.}
}
@article{COWLEY2024101624,
title = {Other orientation: uncovering the roots of praxis},
journal = {Language Sciences},
volume = {103},
pages = {101624},
year = {2024},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101624},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000135},
author = {Stephen J. Cowley},
keywords = {Dialogism, Distributed language, Languaging, Ecolinguistics, Generativism, Intersubjectivity},
abstract = {In honouring Per Linell's achievements, I pursue how dialogue was traced back to praxis. Hence, I begin with how, countering generative theory as overblown, Linell found a hard middle way and, thus, adopted a modest realism. In early work, he traced phonology to what can be heard and, later, diagnosed exclusive emphasis on things or rules as written language bias. Since much depends on how we speak, verbalizing derives, in part, from the influence of others. In modelling speech performance, he therefore turns to a duality of planning and execution. Activity can be orienting to others and/or their doings and sayings. The pattern recurs in initiative-response analysis which effectively tracks isomorphisms in the push and pull of dialogue (initiative and response). Given samenesses, forms, ways of acting, and uses of wordings, we sustain the sociodialogical consciousness of practical and linguistic knowhow. Praxis prompts people to act, transcend situations, use dialogue, construct practical theories and, slowly, change their languaging. In scaling down, I argue that the future prospects of Linell's work lie in rethinking the interdisciplinary area that is concerned with languages, human practices and, above all, their effects.}
}
@article{GUDWIN2018155,
title = {An Overview of the Multipurpose Enhanced Cognitive Architecture (MECA)},
journal = {Procedia Computer Science},
volume = {123},
pages = {155-160},
year = {2018},
note = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918300267},
author = {Ricardo Gudwin and André Paraense and Suelen {de Paula} and Eduardo Fróes and Wandemberg Gibaut and Elisa Castro and Vera Figueiredo and Klaus Raizer},
keywords = {Cognitive Architecture, Dual-process Theory, Dynamic Subsumption, CST},
abstract = {In this paper, we present an overview of MECA, the Multipurpose Enhanced Cognitive Architecture, a cognitive architecture developed by our research group and implemented in the Java language. MECA was designed based on many ideas coming from Dual Process Theory, Dynamic Subsumption, Conceptual Spaces and Grounded Cognition, and constructed using CST, a toolkit for the construction of cognitive architectures in Java, also developed by our group. Basically MECA promotes an hybridism of SOAR, used to implement rule-based processing and space-state exploration in System 2 modules, with a Dynamic Subsumption Motivational System performing the role of System 1, using a representational system based on conceptual spaces and grounded cognition. We provide an overview of the many MECA sub-systems and an ontology of the concepts being represented within the architecture.}
}
@article{LIANG2025206,
title = {TCMHTI: a Transformer-based herb-target interaction prediction model for Qingfu Juanbi Decoction in rheumatoid arthritis},
journal = {Digital Chinese Medicine},
volume = {8},
number = {2},
pages = {206-218},
year = {2025},
issn = {2589-3777},
doi = {https://doi.org/10.1016/j.dcmed.2025.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S2589377725000692},
author = {Zhenzhong Liang and Changsong Ding},
keywords = {Transformer, Qingfu Juanbi Decoction, Rheumatoid arthritis, Deep learning, Network pharmacology},
abstract = {Objective
To predict the potential targets of Qingfu Juanbi Decoction (青附蠲痹汤, QFJBD) in treating rheumatoid arthritis (RA) using an improved Transformer model and investigate the network pharmacological mechanisms underlying QFJBD’s therapeutic effects on RA.
Methods
First, a traditional Chinese medicine herb-target interaction (TCMHTI) model was constructed to predict herb-target interactions based on Transformer improvement. The performance of the TCMHTI model was evaluated against baseline models using three metrics: area under the receiver operating characteristic curve (AUC), precision-recall curve (PRC), and accuracy. Subsequently, a protein-protein interaction (PPI) network was built based on the predicted targets, with core targets identified as the top nine nodes ranked by degree values. Gene Ontology (GO) functional and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analyses were performed using the targets predicted by TCMHTI and the targets identified through network pharmacology method for comparison. Then, the results were compared. Finally, the core targets predicted by TCMHTI were validated through molecular docking and literature review.
Results
The TCMHTI model achieved an AUC of 0.883, PRC of 0.849, and accuracy of 0.818, predicting 49 potential targets for QFJBD in RA treatment. Nine core targets were identified: tumor necrosis factor (TNF)-α, interleukin (IL)-1β, IL-6, IL-10, IL-17A, cluster of differentiation 40 (CD40), cytotoxic T-lymphocyte-associated protein 4 (CTLA4), IL-4, and signal transducer and activator of transcription 3 (STAT3). The enrichment analysis demonstrated that the TCMHTI model predicted 49 targets and enriched more pathways directly associated with RA, whereas classical network pharmacology identified 64 targets but enriched pathways showing weaker relevance to RA. Molecular docking demonstrated that the active molecules in QFJBD exhibit favorable binding energy with RA targets, while literature research further revealed that QFJBD can treat RA through 9 core targets.
Conclusion
The TCMHTI model demonstrated greater accuracy than traditional network pharmacology methods, suggesting QFJBD exerts therapeutic effects on RA by regulating targets like TNF-α, IL-1β, and IL-6, as well as multiple signaling pathways. This study provides a novel framework for bridging traditional herbal knowledge with precision medicine, offering actionable insights for developing targeted TCM therapies against diseases.}
}
@article{DU2020107379,
title = {DeepAdd: Protein function prediction from k-mer embedding and additional features},
journal = {Computational Biology and Chemistry},
volume = {89},
pages = {107379},
year = {2020},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2020.107379},
url = {https://www.sciencedirect.com/science/article/pii/S1476927119309181},
author = {Zhihua Du and Yufeng He and Jianqiang Li and Vladimir N. Uversky},
keywords = {Protein function prediction, Convolution neural network, Natural language process, Protein-protein interaction network, Sequence similarity profile},
abstract = {With the application of new high throughput sequencing technology, a large number of protein sequences is becoming available. Determination of the functional characteristics of these proteins by experiments is an expensive endeavor that requires a lot of time. Furthermore, at the organismal level, such kind of experimental functional analyses can be conducted only for a very few selected model organisms. Computational function prediction methods can be used to fill this gap. The functions of proteins are classified by Gene Ontology (GO), which contains more than 40,000 classifications in three domains, Molecular Function (MF), Biological Process (BP), and Cellular Component (CC). Additionally, since proteins have many functions, function prediction represents a multi-label and multi-class problem. We developed a new method to predict protein function from sequence. To this end, natural language model was used to generate word embedding of sequence and learn features from it by deep learning, and additional features to locate every protein. Our method uses the dependencies between GO classes as background information to construct a deep learning model. We evaluate our method using the standards established by the Computational Assessment of Function Annotation (CAFA) and have noticeable improvement over several algorithms, such as FFPred, DeepGO, GoFDR and other methods compared on the CAFA3 datasets.}
}
@article{BOETTGER2022103313,
title = {What ‘translating science’ can learn from ‘translating languages’},
journal = {Drug Discovery Today},
volume = {27},
number = {10},
pages = {103313},
year = {2022},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2022.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S1359644622002847},
author = {Michael K. Boettger},
keywords = {Drug discovery, Translation, Backtranslation, Translational medicine, Language, Functional equivalence},
abstract = {One of the most important steps in drug discovery is the translation of preclinical data to humans. However, the term ‘translation’ has numerous connotations and, often, different stakeholders literally speak different languages. Learning from many years of experience and new concepts in language translation could increase the success rate in translating biomedical research. Beyond being bilingual, this includes applying the concept of functional equivalence, the main characteristic of a good translation. Given that function is defined by the source language text, starting with the patient has advantages over the classical bench-to-bedside approach. Good translators need transfer competence, including knowledge of the limitations of translation. As with languages, computer-assisted translation(-al research) could support increasing functional equivalence and, thus, translation success.}
}
@article{QIU2025101326,
title = {BioLLM: A standardized framework for integrating and benchmarking single-cell foundation models},
journal = {Patterns},
volume = {6},
number = {8},
pages = {101326},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101326},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925001746},
author = {Ping Qiu and Qianqian Chen and Hua Qin and Shuangsang Fang and Yilin Zhang and Yanlin Zhang and Tianyi Xia and Lei Cao and Yong Zhang and Xiaodong Fang and Yuxiang Li and Luni Hu},
keywords = {single-cell foundation models, unified framework, model benchmarking, zero shot and fine-tuning},
abstract = {Summary
The application and evaluation of single-cell foundation models (scFMs) present significant challenges due to heterogeneous architectures and coding standards. To address this, we introduce BioLLM (biological large language model), a unified framework for integrating and applying scFMs to single-cell RNA sequencing analysis. BioLLM provides a unified interface that integrates diverse scFMs, eliminating architectural and coding inconsistencies to enable streamlined model access. With standardized APIs and comprehensive documentation, BioLLM supports streamlined model switching and consistent benchmarking. Our comprehensive evaluation of scFMs revealed distinct strengths and limitations, highlighting scGPT’s robust performance across all tasks, including zero shot and fine-tuning. Geneformer and scFoundation demonstrated strong capabilities in gene-level tasks, benefiting from effective pretraining strategies. In contrast, scBERT lagged behind, likely due to its smaller model size and limited training data. Ultimately, BioLLM aims to empower the scientific community to leverage the full potential of foundational models, advancing our understanding of complex biological systems through enhanced single-cell analysis.}
}
@article{PENG2024,
title = {Use of Metadata-Driven Approaches for Data Harmonization in the Medical Domain: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/52967},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000176},
author = {Yuan Peng and Franziska Bathelt and Richard Gebler and Robert Gött and Andreas Heidenreich and Elisa Henke and Dennis Kadioglu and Stephan Lorenz and Abishaa Vengadeswaran and Martin Sedlmayr},
keywords = {ETL, ELT, Extract-Load-Transform, Extract-Transform-Load, interoperability, metadata-driven, medical domain, data harmonization},
abstract = {Background
Multisite clinical studies are increasingly using real-world data to gain real-world evidence. However, due to the heterogeneity of source data, it is difficult to analyze such data in a unified way across clinics. Therefore, the implementation of Extract-Transform-Load (ETL) or Extract-Load-Transform (ELT) processes for harmonizing local health data is necessary, in order to guarantee the data quality for research. However, the development of such processes is time-consuming and unsustainable. A promising way to ease this is the generalization of ETL/ELT processes.
Objective
In this work, we investigate existing possibilities for the development of generic ETL/ELT processes. Particularly, we focus on approaches with low development complexity by using descriptive metadata and structural metadata.
Methods
We conducted a literature review following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. We used 4 publication databases (ie, PubMed, IEEE Explore, Web of Science, and Biomed Center) to search for relevant publications from 2012 to 2022. The PRISMA flow was then visualized using an R-based tool (Evidence Synthesis Hackathon). All relevant contents of the publications were extracted into a spreadsheet for further analysis and visualization.
Results
Regarding the PRISMA guidelines, we included 33 publications in this literature review. All included publications were categorized into 7 different focus groups (ie, medicine, data warehouse, big data, industry, geoinformatics, archaeology, and military). Based on the extracted data, ontology-based and rule-based approaches were the 2 most used approaches in different thematic categories. Different approaches and tools were chosen to achieve different purposes within the use cases.
Conclusions
Our literature review shows that using metadata-driven (MDD) approaches to develop an ETL/ELT process can serve different purposes in different thematic categories. The results show that it is promising to implement an ETL/ELT process by applying MDD approach to automate the data transformation from Fast Healthcare Interoperability Resources to Observational Medical Outcomes Partnership Common Data Model. However, the determining of an appropriate MDD approach and tool to implement such an ETL/ELT process remains a challenge. This is due to the lack of comprehensive insight into the characterizations of the MDD approaches presented in this study. Therefore, our next step is to evaluate the MDD approaches presented in this study and to determine the most appropriate MDD approaches and the way to integrate them into the ETL/ELT process. This could verify the ability of using MDD approaches to generalize the ETL process for harmonizing medical data.}
}
@incollection{PRANCKEVICIENE2019469,
title = {Gene Prioritization Using Semantic Similarity},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {469-478},
year = {2019},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00346-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027003468},
author = {Erinija Pranckevičienė},
keywords = {Annotation, Controlled vocabulary, Gene ontology, Gene prioritization, Information content, Knowledge engineering, Medical genetics, Next generation sequencing, Ontology, Phenotype, Semantic similarity},
abstract = {Molecular biologists and medical geneticists have to analyze large lists of genes and genomic variants to select the best candidates linked to the phenotype of interest. This task is very challenging therefore computational algorithms are used to prioritize candidate genes. Genes are prioritized based on evidence in favour of their relationship to the phenotype. A technique among other used in this process is measuring semantic similarity (SS) between functional annotations of genes. For SS to be applicable the functional annotations (knowledge about the domain) have to be in a form of hierarchically organized ontology. In this article we outline a computational process of estimating semantic similarity of phenotypic descriptions and gene annotations in Human Phenotype Ontology (HPO) and Gene Ontology. An example of a possible process of differential diagnosis applying semantic similarity to compare HPO phenotypes is presented. A semantic content of Gene Ontology annotations is highlighted. An example of comparison of meanings of individual GO annotations is presented and extended to a comparison of functional similarity of gene products.}
}
@article{SAVARD2020103722,
title = {Considering cultural variables in the instructional design process: A knowledge-based advisor system},
journal = {Computers & Education},
volume = {145},
pages = {103722},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2019.103722},
url = {https://www.sciencedirect.com/science/article/pii/S0360131519302751},
author = {Isabelle Savard and Jacqueline Bourdeau and Gilbert Paquette},
keywords = {Instructional design, Cultural variables, Knowledge-based advisor system, Ontology, Design-based research},
abstract = {This article presents research works in which a cultural adaptation method and a knowledge-based advisor to help instructional designers in considering cultural variables during the instructional design process have been developed. To do so, a conceptual model of Culture was elaborated, cultural variables were identified and knowledge regarding these variables was modeled via an ontology that served to create the “Cultural Diversity” knowledge base integrating knowledge regarding five cultures. The advisor tool uses this knowledge to advise instructional designers on how to adapt a pedagogical scenario to a culture other than their own or for learners with a culture that is different from the one for which a pedagogical scenario was originally designed. The methodology used is Design-Based Research (DBR) and contains five iterations.}
}
@article{ARROTTA2023101780,
title = {Probabilistic knowledge infusion through symbolic features for context-aware activity recognition},
journal = {Pervasive and Mobile Computing},
volume = {91},
pages = {101780},
year = {2023},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2023.101780},
url = {https://www.sciencedirect.com/science/article/pii/S157411922300038X},
author = {Luca Arrotta and Gabriele Civitarese and Claudio Bettini},
keywords = {Human activity recognition, Neuro-symbolic, Context-awareness},
abstract = {In the general machine learning domain, solutions based on the integration of deep learning models with knowledge-based approaches are emerging. Indeed, such hybrid systems have the advantage of improving the recognition rate and the model’s interpretability. At the same time, they require a significantly reduced amount of labeled data to reliably train the model. However, these techniques have been poorly explored in the sensor-based Human Activity Recognition (HAR) domain. The common-sense knowledge about activity execution can potentially improve purely data-driven approaches. While a few knowledge infusion approaches have been proposed for HAR, they rely on rigid logic formalisms that do not take into account uncertainty. In this paper, we propose P-NIMBUS, a novel knowledge infusion approach for sensor-based HAR that relies on probabilistic reasoning. A probabilistic ontology is in charge of computing symbolic features that are combined with the features automatically extracted by a CNN model from raw sensor data and high-level context data. In particular, the symbolic features encode probabilistic common-sense knowledge about the activities consistent with the user’s surrounding context. These features are infused within the model before the classification layer. We experimentally evaluated P-NIMBUS on a HAR dataset of mobile devices sensor data that includes 14 different activities performed by 25 users. Our results show that P-NIMBUS outperforms state-of-the-art neuro-symbolic approaches, with the advantage of requiring a limited amount of training data to reach satisfying recognition rates (i.e., more than 80% of F1-score with only 20% of labeled data).}
}
@article{DEEPAK2020737,
title = {A Semantic Approach for Entity Linking by Diverse Knowledge Integration incorporating Role-Based Chunking},
journal = {Procedia Computer Science},
volume = {167},
pages = {737-746},
year = {2020},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.339},
url = {https://www.sciencedirect.com/science/article/pii/S187705092030805X},
author = {Gerard Deepak and Naresh Kumar D and A Santhanavijayan},
keywords = {Chunking, Folksonomies, Role-based Ontology, Semantic Similarity, Knowledge Graph},
abstract = {Web-data has seen an exponential rise in the past few years. With the increase in the data on the web, the process of associating entities with required knowledge becomes extremely difficult. Linking entities not only becomes a tedious task but also requires the right association of knowledge with the right techniques. With the development of the Semantic Web in recent times, semantic strategies are required to represent, reason and link entities. In this paper, an entity linking approach that rightly associates personalities has been proposed. The proposed algorithm encompasses role-based chunking along with a fragmented parse tree generation. The proposed strategy performs Entity Linking by JSON fragmented parse tree generation and recommends the entities based on the semantic score generated by computing the concept similarity. The knowledge is supplied by a role-based Ontology modeled for various famous personalities. An accuracy of 89.77% is achieved for role-based entity linking which is much better and reliable than the existing strategies, especially when a large number of trials were conducted for the Indian Context.}
}
@article{BERVEN2020103321,
title = {A knowledge-graph platform for newsrooms},
journal = {Computers in Industry},
volume = {123},
pages = {103321},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103321},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305558},
author = {Arne Berven and Ole A. Christensen and Sindre Moldeklev and Andreas L. Opdahl and Kjetil J. Villanger},
keywords = {Computational journalism, Journalistic knowledge platforms, Newsroom systems, Knowledge graphs, Semantic technologies, RDF, OWL, Ontology, Natural-language processing (NLP), Machine learning (ML)},
abstract = {Journalism is challenged by digitalisation and social media, resulting in lower subscription numbers and reduced advertising income. Information and communication techniques (ICT) offer new opportunities. Our research group is collaborating with a software developer of news production tools for the international market to explore how social, open, and other data sources can be leveraged for journalistic purposes. We have developed an architecture and prototype called News Hunter that uses knowledge graphs, natural-language processing (NLP), and machine learning (ML) together to support journalists. Our focus is on combining existing data sources and computation and storage techniques into a flexible architecture for news journalism. The paper presents News Hunter along with plans and possibilities for future work.}
}
@article{DONG2023105383,
title = {Understanding table content for mineral exploration reports using deep learning and natural language processing},
journal = {Ore Geology Reviews},
volume = {156},
pages = {105383},
year = {2023},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2023.105383},
url = {https://www.sciencedirect.com/science/article/pii/S0169136823000987},
author = {Jiahuizi Dong and Qinjun Qiu and Zhong Xie and Kai Ma and Anna Hu and Haitao Wang},
keywords = {Mineral exploration reports, Geological tables, Mask R-CNN, Cell extraction, Table structure parsing, Knowledge graph},
abstract = {The geological reports contain various tables, and can offer mineral element content data and stratum detailed information. Geological tabular information extraction and its semantic fusion with text is of great significance in converting and fusing geological unstructured data into structured knowledge to guide cognitive intelligence analysis in the geoscience domain. While the performance of general tools and existing table structure analysis methods is limited due to the various merged cells and diagonally split table headers. To address this issue, we propose a novel approach based on the improved Mask R-CNN model to identify and parse the forms. The geological table parsing network constructed in this paper consists of two key steps: (1) A cell feature augmentation (CFA) module to learn the contextual features for identifying cells of different sizes. (2) A table parsing method (GTab) to parse the table header cells with split lines. We compare the proposed method with commonly used table parsing methods on our constructed geological table dataset. Our models are easily integrated into a prototype system to provide joint information processing and analysis.}
}
@article{ERONEN2023103250,
title = {Zero-shot cross-lingual transfer language selection using linguistic similarity},
journal = {Information Processing & Management},
volume = {60},
number = {3},
pages = {103250},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103250},
url = {https://www.sciencedirect.com/science/article/pii/S030645732200351X},
author = {Juuso Eronen and Michal Ptaszynski and Fumito Masui},
keywords = {Multilingual natural language processing, Zero-shot learning, Transfer learning, Linguistics, Language similarity},
abstract = {We study the selection of transfer languages for different Natural Language Processing tasks, specifically sentiment analysis, named entity recognition and dependency parsing. In order to select an optimal transfer language, we propose to utilize different linguistic similarity metrics to measure the distance between languages and make the choice of transfer language based on this information instead of relying on intuition. We demonstrate that linguistic similarity correlates with cross-lingual transfer performance for all of the proposed tasks. We also show that there is a statistically significant difference in choosing the optimal language as the transfer source instead of English. This allows us to select a more suitable transfer language which can be used to better leverage knowledge from high-resource languages in order to improve the performance of language applications lacking data. For the study, we used datasets from eight different languages from three language families.}
}
@incollection{DRAGER2021345,
title = {Overview: Standards for Modeling in Systems Medicine},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {345-353},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-816077-0.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128160770000017},
author = {Andreas Dräger and Dagmar Waltemath},
keywords = {CellML, COMBINE, Computational biology, Data formats, MIASE, MIRIAM, Reproducibility, SBGN, SBML, SED-ML, Standardization},
abstract = {Computational analyses of digital information constitute the foundation for modern scientific research. The analysis of computer models can, for instance, guide experimentalists or predict biological functions such as novel drug targets or recovery from external stress. In many cases, such analyses require several bioinformatics tools to interoperate in complex workflows. To this end, the scientific community has been developing clear standards for automatic interpretation and use of computer models within complex computer analysis pipelines for systems biomedicine. The standard development process follows two fundamental design decisions: (i) to organize the information in a modular fashion and (ii) to strictly separate models from methods for their analysis. Following these principles, a complex ecosystem of modeling guidelines, annotation schemes, and representation formats has evolved, for which enthusiastic community members create and maintain rich documentation and necessary software infrastructure. The resulting standards cover model development, storage, the exchange between different analysis programs, the display, automated validity and plausibility checks, publication and review, and many other aspects of computational modeling and simulation. The umbrella organization COMBINE coordinates the evolution of and communication between new and existing standards. In this chapter, we introduce the reader to commonly used data formats, guidelines, and ontologies for the storage and exchange of computational models in systems medicine. We focus on models of biological systems, including their structures, annotation, visualization, and interpretation. Additional chapters in this book offer more detailed information on selected data formats. For the construction of models or the integration of data, further literature needs to be consulted.}
}
@article{MENSHENIN2019235,
title = {Applying the model-based concept framework to capturing the safety aspects of the suborbital human spaceflight missions},
journal = {Journal of Space Safety Engineering},
volume = {6},
number = {4},
pages = {235-247},
year = {2019},
issn = {2468-8967},
doi = {https://doi.org/10.1016/j.jsse.2019.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S2468896719301247},
author = {Yaroslav Menshenin and Edward Crawley},
keywords = {Conceptual design, Model-based system engineering, Suborbital systems, Space tourism, Space safety},
abstract = {Safety issues are one of the core attributes of the projects of the New Space economy such as SpaceShipTwo and New Shepard. While these projects are on their way to shifting the paradigms in space tourism and transportation, the question of tourist safety remains in the cornerstone for a successful operations. One of the important questions we encounter when we start the development of complex systems such as the suborbital transportation and space tourism systems is: what are the concepts available and how can these concepts be represented using strictly defined ontology and model semantics? And finally – how this model-based information would support the system architect in keeping track of safety aspects? In this work we present a model-based concept framework that aims to address this issue. First a concept framework methodology is presented, after which we demonstrate its applicability to suborbital human spaceflight missions–SpaceShipTwo and New Shepard. The analytical conceptual difference between these concepts is demonstrated. The proposed framework includes: the information about the stakeholders and their needs; the solution-neutral environment (the problem statement) in which we formulate the functional intent; the solution-specific environment (solution statement) in which we see the possible solutions; the integrated concept revealed by means of the decomposition of such solution into internal elements and functions; and the concept of operations. Each one of these entries of the concept framework has a counterpart represented in conceptual modeling languages, such as Object-Process Methodology (OPM) or the System Modeling Language (SysML). Such a model-based concept framework encodes the core information required to define a suborbital tourism concept and represent it in a digital environment. We believe this will become a powerful tool to support the makers of architectural decisions that lead to concept and eventually to architecture, and that provides a safe operations for suborbital tourist services.}
}
@article{SIMEONE2019122,
title = {BIM semantic-enrichment for built heritage representation},
journal = {Automation in Construction},
volume = {97},
pages = {122-137},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516303594},
author = {Davide Simeone and Stefano Cursi and Marta Acierno},
keywords = {BIM, Knowledge management, Built Heritage, Semantic-enrichment, Information ontologies},
abstract = {In the built heritage context, BIM has shown difficulties in representing and managing the large and complex knowledge related to non-geometrical aspects of the heritage. Within this scope, this paper focuses on a domain-specific semantic-enrichment of BIM methodology, aimed at fulfilling semantic representation requirements of built heritage through Semantic Web technologies. To develop this semantic-enriched BIM approach, this research relies on the integration of a BIM environment with a knowledge base created through information ontologies. The result is knowledge base system - and a prototypal platform - that enhances semantic representation capabilities of BIM application to architectural heritage processes. It solves the issue of knowledge formalization in cultural heritage informative models, favouring a deeper comprehension and interpretation of all the building aspects. Its open structure allows future research to customize, scale and adapt the knowledge base different typologies of artefacts and heritage activities.}
}
@article{ALAM2023120964,
title = {Automated clinical knowledge graph generation framework for evidence based medicine},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120964},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120964},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423014665},
author = {Fakhare Alam and Hamed Babaei Giglou and Khalid Mahmood Malik},
keywords = {Knowledge Graph, Healthcare Knowledge Graph, Deep Learning, Ontology, COVID-19, Cerebral Aneurysm, PICO, Evidence Based Medicine, Contextualization},
abstract = {To practice the evidence-based medicine, clinicians are interested to find the most suitable research for the clinical decision making. The use of knowledge graphs (KGs) in evidence-based clinical decision support systems is becoming increasingly popular. However, existing KG construction frameworks are not fully automated and contextualized, thus unable to adapt to new domains and incorporate constantly changing information into their knowledge base, resulting in loss of relevance over time. Furthermore, existing KGs construction frameworks don't generate KG that provide relevant information within an acceptable response time for evidence-based practitioners because the organization of constructed subgraphs is neither topic-specific nor evidence-based PICO (Participants/Problem P, Intervention-I, Comparison C, Outcome O) query-friendly. By employing concept extraction, semantic enrichment, optimized clustering, and state of art Recurrent Neural Networks (RNNs) with BioBERT based encoded representation to categorize PICO elements and predict relationships between concepts using huge corpus of publicly available literature on COVID-19 and cerebral aneurysm, this paper proposes a topic specific, PICO enabled, and fully automated framework to curate information and create KG of different clinical domains. The evaluation shows that the proposed framework achieves significant improvement over baseline models and has 93 %, and 82 % accuracy on aneurysm and COVID data set respectively for PICO classification. Also, the relationship extraction module has an accuracy of 96 % with precision and recall being 92 %, and 90 % respectively.}
}
@article{CHEONG2019183,
title = {Translating JSON Schema logics into OWL axioms for unified data validation on a digital manufacturing platform},
journal = {Procedia Manufacturing},
volume = {28},
pages = {183-188},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313726},
author = {Hyunmin Cheong},
keywords = {JSON, JSON Schema, OWL, Ontology, Mapping, Axiom translation, Data validation},
abstract = {JSON (JavaScript Object Notation) is a prevalent data format used in cloud-based platforms that support composable digital manufacturing workflows. The current work presents a method to translate the logics found in JSON Schema into OWL axioms, in order to facilitate ontology-based unified data validation with JSON data. The specific contributions of this paper include the demonstration of using a formal ontology for the logic translation and data validation, a technique for disambiguating implicit relations found in JSON Schema as explicit OWL properties, and mapping JSON Schema validation keywords to equivalent OWL expressions.}
}
@article{CANANAU2025104816,
title = {Critical thinking in preparation for student teachers’ professional practice: A case study of critical thinking conceptions in policy documents framing teaching placement at a Swedish university},
journal = {Teaching and Teacher Education},
volume = {153},
pages = {104816},
year = {2025},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2024.104816},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X24003494},
author = {Iulian Cananau and Silvia Edling and Björn Haglund},
keywords = {Critical thinking, Teacher education, Placement, Teacher profession, Concept, Policy documents},
abstract = {This paper explores the conceptions of critical thinking in national and local policy documents for teaching placement, using the case of teacher education programs at a Swedish university. The concept under scrutiny is based on three contemporary theoretical models of critical thinking in education: critical thinking movement, critical pedagogy, and “criticality” movement. In Sweden, the teacher profession is framed with a broader socio-ethical scope than the focus on individual cognitive skills of the critical thinking movement. Critical reflection and self-reflection, two conceptions identified with the criticality ideal of education for critical being, prevail in the analyzed documents.}
}
@incollection{PRANCKEVICIENE2019898,
title = {Gene Prioritization Using Semantic Similarity},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {898-906},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20405-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338204056},
author = {Erinija Pranckevičienė},
keywords = {Annotation, Controlled vocabulary, Gene ontology, Gene prioritization, Information content, Knowledge engineering, Medical genetics, Next generation sequencing, Ontology, Phenotype, Semantic similarity},
abstract = {Molecular biologists and medical geneticists have to analyze large lists of genes and genomic variants to select the best candidates linked to the phenotype of interest. This task is very challenging therefore computational algorithms are used to prioritize candidate genes. Genes are prioritized based on evidence in favour of their relationship to the phenotype. A technique among other used in this process is measuring semantic similarity (SS) between functional annotations of genes. For SS to be applicable the functional annotations (knowledge about the domain) have to be in a form of hierarchically organized ontology. In this article we outline a computational process of estimating semantic similarity of phenotypic descriptions and gene annotations in Human Phenotype Ontology (HPO) and Gene Ontology. An example of a possible process of differential diagnosis applying semantic similarity to compare HPO phenotypes is presented. A semantic content of Gene Ontology annotations is highlighted. An example of comparison of meanings of individual GO annotations is presented and extended to a comparison of functional similarity of gene products.}
}
@article{ESPOSITO2018124,
title = {Interoperable, dynamic and privacy-preserving access control for cloud data storage when integrating heterogeneous organizations},
journal = {Journal of Network and Computer Applications},
volume = {108},
pages = {124-136},
year = {2018},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2018.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1084804518300316},
author = {Christian Esposito},
keywords = {Access control, Cloud data storage, Ontology, Ontology matching, Trust management, Privacy-preserving authentication, Pseudonym management},
abstract = {Cloud computing is extensively used as an integration means in varies application domains, spanning from the healthcare to the manufacturing, aiming at achieving an easy-to-access and elastic data storage and exchange among heterogeneous and geographically sparse organizations. This cloud-based integration poses crucial security issues related to the data protection from unauthorized access to the outsourced data, which calls for a proper access control solution. However, the heterogeneity among the organizations exacerbates this problem, demanding an interoperable authorization scheme, where multiple access control models must co-exist. The current literature is rich of academic solutions and standards to have an interoperable exchange of security policies and definition of authorization rules, but lacks an effective support to let different access control models to fully coexist. Moreover, the possibility of stealing authentication credentials and authorization claims paves the way to conducting masquerading attacks that cannot be treated by traditional static authorization solutions, but more dynamic approaches are needed. Last but not least, the continuous interaction of users with the cloud over the time has the vulnerability of exposing personal information to malicious adversaries and to let them trace the user activities. In this work, we propose to solve these three issues by having an ontology-based access control solution, to encompass trust within the authorization process and to use pseudonyms to preserve the user privacy.}
}
@article{AHANI2021519,
title = {Evaluating medical travelers’ satisfaction through online review analysis},
journal = {Journal of Hospitality and Tourism Management},
volume = {48},
pages = {519-537},
year = {2021},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2021.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1447677021001261},
author = {Ali Ahani and Mehrbakhsh Nilashi and Waleed Abdu Zogaan and Sarminah Samad and Nojood O. Aljehane and Ashwaq Alhargan and Saidatulakmal Mohd and Hossein Ahmadi and Louis Sanzogni},
keywords = {Medical tourism, Online reviews, Big social data, Semantic filtering, Ontology, Traveler satisfaction},
abstract = {Medical tourism is increasing quickly since it contributes to both the health and tourism sectors. The use of big social data has been effective in the development of medical tourism as a huge amount of data is produced and shared by travelers about the services through different social media platforms. Indeed, communicative information and knowledge can be mined from a large amount of information provided by travelers about medical tourism services. It is important to analyze such data to understand the customers' satisfaction level and their demands. Although several studies have been conducted to find the factors influencing customer satisfaction in medical tourism, there is a lack of studies about big social data and online behavioral analysis of medical travelers. In addition, the analysis of customers' online reviews is fairly unexplored by machine learning techniques in the context of medical tourism. Hence, this research aims to fill this gap and develop a new method to reveal travelers' choice preferences and satisfaction with medical tourism services through the analysis of the online review. Text mining and ontology approaches are used in the proposed method. The method can mine data from medical tourism websites, discover the satisfaction dimensions, and reveal the satisfaction level of medical tourists through textual reviews. We rely on the demographic information of medical tourists and ontological semantic filtering approaches to better detect the travelers’ preferences in medical tourism websites. The proposed method is evaluated through the numerical and textual reviews obtained from medical tourism websites. The results of data analysis showed that the proposed method is effective for big data analysis in the medical tourism context and may help medical tourism organizers to improve their medical tourism services to obtain a high level of medical travelers' satisfaction.}
}
@article{YU2023105318,
title = {Digital Twin-enabled and Knowledge-driven decision support for tunnel electromechanical equipment maintenance},
journal = {Tunnelling and Underground Space Technology},
volume = {140},
pages = {105318},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2023.105318},
url = {https://www.sciencedirect.com/science/article/pii/S0886779823003383},
author = {Gang Yu and Dinghao Lin and Yi Wang and Min Hu and Vijayan Sugumaran and Junjie Chen},
keywords = {Digital twin, Proactive maintenance, Knowledge ontology, Rule extraction and updating, Combinatory inference, Semantic Web technology},
abstract = {Urban tunnel infrastructure plays critical roles in sustaining the wellbeing of a society. The operation of tunnels relies on a diverse range of tunnel electromechanical equipment (TEE), such as ventilation, drainage, and the lighting system. However, effectively and proactively maintaining TEE to prevent unforeseen failures using limited resources remains an unresolved challenge. The utilization of digital twin technology, which combines Building Information Modeling (BIM), Internet of Things (IoT) and Semantic Web technologies, offers a knowledge-rich environment for the development of improved maintenance strategies. This study proposes a digital twin-enabled and knowledge-driven decision support method for proactive TEE maintenance. Initially, a digital twin conceptual framework for proactive TEE maintenance is presented, which integrates a knowledge-driven approach to support decision making. Subsequently, a controlled vocabulary-based method is developed to extract and update maintenance knowledge. Finally, a novel combinatory reasoner, including a rule selection algorithm and an inference algorithm, is devised to automatically generate maintenance schemes. Based on the proposed method, a decision support tool was developed and applied to Wenyi Road Tunnel in Hangzhou, China. The results demonstrated the effectiveness of the method, which can continuously update maintenance knowledge and assist fault detection and TEE state prediction with the combinatory reasoner. Moreover, the inference efficiency achieved by our method surpasses that of traditional approaches by approximately 162 ms.}
}
@article{SILVA2019100877,
title = {Using controlled vocabularies in anatomical terminology: A case study with Strumigenys (Hymenoptera: Formicidae)},
journal = {Arthropod Structure & Development},
volume = {52},
pages = {100877},
year = {2019},
issn = {1467-8039},
doi = {https://doi.org/10.1016/j.asd.2019.100877},
url = {https://www.sciencedirect.com/science/article/pii/S1467803919300234},
author = {Thiago S.R. Silva and Rodrigo M. Feitosa},
keywords = {Morphology, Ants, Ontology, Semantic annotation, Terminology},
abstract = {Morphological studies of insects can help us to understand the concomitant or sequential functionality of complex structures and may be used to hypothetize distinct levels of phylogenetic relationship among groups. Traditional morphological works, generally, have encompassed a set of elements, including descriptions of structures and their respective conditions, literature references and images, all combined in a single document. Fast forward to the digital era, it is now possible to release this information simultaneously but also independently as data sets linked to the original publication in an external environment. In order to link data from various fields of knowledge, disseminating morphological information in an open environment, it is important to use tools that enhance interoperability. For example, semantic annotations facilitate the dissemination and retrieval of phenotypic data in digital environments. The integration of semantic (i.e. web-based) components with anatomic treatments can be used to generate a traditional description in natural language along with a set of semantic annotations. The ant genus Strumigenys currently comprises about 840 described species distributed worldwide. In the Neotropical region, almost 200 species are currently known, but it is possible that much of the species' diversity there remains unexplored and undescribed. The morphological diversity in the genus is high, reflecting an extreme generic reclassification that occurred in the late 20th and early 21st centuries. Here we define the anatomical concepts in this highly diverse group of ants using semantic annotations to enrich the anatomical ontologies available online, focussing on the definition of terms through subjacent conceptualization.}
}
@article{GAO2025114437,
title = {TWDT: Training-Free Word-Level Controllable Diffusion Model for Text Generation},
journal = {Knowledge-Based Systems},
pages = {114437},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114437},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125014765},
author = {Nan Gao and Yangjie Lu and Peng Chen and Guodao Sun and Ronghua Liang and Yilong Zhang},
keywords = {Controlled text generation, diffusion model, word-level controllable diffusion,},
abstract = {Existing controlled text generation (CTG) methods typically require the training of additional components, whereas diffusion models have already achieved fine control in image generation by adjusting latent feature information during the inference process. However, existing diffusion models still face issues such as “attribute leakage” and “overgeneration” when applied to text generation, leading to generated texts lacking precise control. To address these problems, we propose a training-free word-level controllable diffusion language network (TWDT). This network achieves fine-grained control of text generation by adjusting latent space features during the inference process. Specifically, TWDT introduces an Alignment and Word Evaluation (AWE) module, which ensures accurate mapping of the text to a predefined set of feature words through syntactic segmentation and multi-level semantic alignment. At the same time, a similarity threshold filtering mechanism is applied to inject Gaussian noise into low-consistency nodes, ensuring semantic consistency and stability during generation. To evaluate the rigor and accuracy of the model, we have developed a high-quality multi-disease dental diagnostic dataset, all of which are annotated by experienced dental experts, serving as the benchmark for model evaluation. Experimental results show that TWDT outperforms existing diffusion models in terms of generation accuracy and rigor.}
}
@article{KANG2022,
title = {Semantic Network Model for Sign Language Comprehension},
journal = {International Journal of Cognitive Informatics and Natural Intelligence},
volume = {16},
number = {1},
year = {2022},
issn = {1557-3958},
doi = {https://doi.org/10.4018/IJCINI.309991},
url = {https://www.sciencedirect.com/science/article/pii/S1557395822000136},
author = {Xinchen Kang and Dengfeng Yao and Minghu Jiang and Yunlong Huang and Fanshu Li},
keywords = {Attention, Cognitive Processing, Comprehension, Decision-Tree, Game Theory, Linguistics, Perception, Semantic Network, Sign Language},
abstract = {ABSTRACT
In this study, the authors propose a computational cognitive model for sign language (SL) perception and comprehension with detailed algorithmic descriptions based on cognitive functionalities in human language processing. The semantic network model (SNM) that represents semantic relations between concepts is used as a form of knowledge representation. The proposed model is applied in the comprehension of sign language for classifier predicates. The spreading activation search method is initiated by labeling a set of source nodes (e.g., concepts in the semantic network) with weights or “activation” and then iteratively propagating or “spreading” that activation out to other nodes linked to the source nodes. The results demonstrate that the proposed search method improves the performance of sign language comprehension in the SNM.}
}
@article{MAHBOUBI2024104004,
title = {Evolving techniques in cyber threat hunting: A systematic review},
journal = {Journal of Network and Computer Applications},
volume = {232},
pages = {104004},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2024.104004},
url = {https://www.sciencedirect.com/science/article/pii/S1084804524001814},
author = {Arash Mahboubi and Khanh Luong and Hamed Aboutorab and Hang Thanh Bui and Geoff Jarrad and Mohammed Bahutair and Seyit Camtepe and Ganna Pogrebna and Ejaz Ahmed and Bazara Barry and Hannah Gately},
keywords = {Threat hunting, Hypothesis, Machine learning, OpenAI voice engine, Cyber threat intelligence, Generative AI},
abstract = {In the rapidly changing cybersecurity landscape, threat hunting has become a critical proactive defense against sophisticated cyber threats. While traditional security measures are essential, their reactive nature often falls short in countering malicious actors’ increasingly advanced tactics. This paper explores the crucial role of threat hunting, a systematic, analyst-driven process aimed at uncovering hidden threats lurking within an organization’s digital infrastructure before they escalate into major incidents. Despite its importance, the cybersecurity community grapples with several challenges, including the lack of standardized methodologies, the need for specialized expertise, and the integration of cutting-edge technologies like artificial intelligence (AI) for predictive threat identification. To tackle these challenges, this survey paper offers a comprehensive overview of current threat hunting practices, emphasizing the integration of AI-driven models for proactive threat prediction. Our research explores critical questions regarding the effectiveness of various threat hunting processes and the incorporation of advanced techniques such as augmented methodologies and machine learning. Our approach involves a systematic review of existing practices, including frameworks from industry leaders like IBM and CrowdStrike. We also explore resources for intelligence ontologies and automation tools. The background section clarifies the distinction between threat hunting and anomaly detection, emphasizing systematic processes crucial for effective threat hunting. We formulate hypotheses based on hidden states and observations, examine the interplay between anomaly detection and threat hunting, and introduce iterative detection methodologies and playbooks for enhanced threat detection. Our review encompasses supervised and unsupervised machine learning approaches, reasoning techniques, graph-based and rule-based methods, as well as other innovative strategies. We identify key challenges in the field, including the scarcity of labeled data, imbalanced datasets, the need for integrating multiple data sources, the rapid evolution of adversarial techniques, and the limited availability of human expertise and data intelligence. The discussion highlights the transformative impact of artificial intelligence on both threat hunting and cybercrime, reinforcing the importance of robust hypothesis development. This paper contributes a detailed analysis of the current state and future directions of threat hunting, offering actionable insights for researchers and practitioners to enhance threat detection and mitigation strategies in the ever-evolving cybersecurity landscape.}
}
@incollection{PREISIG2020571,
title = {Promo - a Multi-disciplinary Process Modelling Suite},
editor = {Sauro Pierucci and Flavio Manenti and Giulia Luisa Bozzano and Davide Manca},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {48},
pages = {571-576},
year = {2020},
booktitle = {30th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-823377-1.50096-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128233771500963},
author = {Heinz A. Preisig},
keywords = {simulation, ontology, computational engineering},
abstract = {Multi-disciplinary, multi-scale simulations are in demand as computing becomes increasingly available and the different disciplines’ problem solutions become mature and broadly applicable. Material modelling has taken a lead in the subject and ProMo is one of the possible solutions to the still very intrinsic problem of supporting the process of sketching a process as a topology to an integrated solution solving multi-faceted problems that require bits and pieces from various knowledge domains. ProMo coupled with the open platform of MoDeNa provide an integrated solutions.}
}
@article{ROSSOLATOS2020100484,
title = {A brand storytelling approach to Covid-19's terrorealization: Cartographing the narrative space of a global pandemic},
journal = {Journal of Destination Marketing & Management},
volume = {18},
pages = {100484},
year = {2020},
issn = {2212-571X},
doi = {https://doi.org/10.1016/j.jdmm.2020.100484},
url = {https://www.sciencedirect.com/science/article/pii/S2212571X20301062},
author = {George Rossolatos},
keywords = {Covid-19, Place branding, Brand storytelling, Narratology, Metaphorical modeling, Semiotics},
abstract = {This paper offers a brand storytelling or narratological account of the Covid-19 pandemic's emergence phase. By adopting a fictional ontological standpoint, the virus' deploying media-storyworld is identified with a process of narrative spacing. Subsequently, the brand's personality is analyzed as a narrative place brand. The advanced narrative model aims to outline the main episodes that make up the virus' brand personality as process and structural components (actors, settings, actions, and relationships). A series of deep or ontological metaphors are identified as the core DNA of this place brand by applying metaphorical modeling to the tropic articulation of Covid-19's narrative. The virus is fundamentally identified with terror as a menacing force that wipes out existing regimes of signification due to its uncertain motives, origins, and operational mode. In this context, familiar urban spaces, cultural practices, and intersubjective communications are redefined, repurposed, and reprogrammed. This process is called terrorealization, as the desertification and metaphorical sublation of all prior territorial significations. This study contributes to the narrative sub-stream of place branding by approaching a globally relevant socio-cultural phenomenon from a brand storytelling perspective.}
}
@article{MUPPAVARAPU2021101923,
title = {Knowledge extraction using semantic similarity of concepts from Web of Things knowledge bases},
journal = {Data & Knowledge Engineering},
volume = {135},
pages = {101923},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101923},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000501},
author = {Vamsee Muppavarapu and Gowtham Ramesh and Amelie Gyrard and Mahda Noura},
keywords = {Interoperability, Internet of Things, Semantic Web of Things, Popular concepts, Smart building, Smart home},
abstract = {The Internet of Things (IoT) is one of the rapidly growing technologies with the aim of establishing communication among objects, people, and processes. This rapidly growing technology faces a lot of challenges that hinder its wider adoption, specifically in developing applications that involve heterogeneous domains. Currently, developing such interoperable applications require substantial efforts by the developers to hard code the requirements to ensure the correctness of transferring knowledge. The efforts can be significantly reduced by developing an interoperable platform that ensures seamless communication between heterogeneous IoT devices. W3C Web of Things (WoT) is a significant step towards enabling interoperability between IoT devices by integrating the existing Web ecosystem with “Things”. WoT provides a unified interface over a suitable network protocol facilitating interactions between different IoT protocols. WoT Thing Descriptions (TD) enrich interoperability providing both human and machine readable metadata about a Thing. However, the WoT still falls short in providing semantic interoperability due to insufficient standard vocabularies which can describe different IoT application domains. In this paper, we propose a semantic similarity-based approach to automatically identify and extract the most common concepts from sixteen popular ontologies belonging to smart home and smart building domains. The proposed method helps the developers and researchers to develop a domain ontology with reduced effort. The extracted concepts are evaluated by the domain experts and are found to be sufficient in describing the smart home and smart building domains.}
}
@article{BOURGAIS2021507,
title = {Detecting Situations with Stream Reasoning on Health Data Obtained with IoT},
journal = {Procedia Computer Science},
volume = {192},
pages = {507-516},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015398},
author = {Mathieu Bourgais and Franco Giustozzi and Laurent Vercouter},
keywords = {IoT, Stream Reasoning, Health Data},
abstract = {The development of Internet of Things (IoT) creates large amount of data usable by decision making systems in various domains. In particular, in the field of health monitoring, it enables to follow the medical state of a patient at home in real-time. A challenge is to interpret these data with a high-level representation model in order to have a better understanding of the medical state of a patient. We propose in this article to use Stream Reasoning associated to an ontological representation of the medical context of a patient to understand her situation. This permits to combine in real time static knowledge stored in an ontology and dynamic information provided by smart sensors. To facilitate this process, we introduce constraints and situations concepts to ease the translation of expert knowledge into logical queries. We provide in this paper an experimental analysis of real body temperature data to illustrate how situations may be detected.}
}
@article{YUAN2024102616,
title = {Toward dynamic rehabilitation management: A novel smart product-service system development approach based on fine-tuned large vision model and Fuzzy-Dematel},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102616},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102616},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002647},
author = {Wenyu Yuan and Hua Zhao and Xiongjie Yang and Ting Han and Danni Chang},
keywords = {Smart PSS, Fine-tuned large model, Personalized service, Fuzzy DEMATEL, Rehabilitation management},
abstract = {Nowadays, transformative technologies such as artificial intelligence, big data, and cloud computing are significantly influencing and reshaping the daily lives of individuals. Guided by the overarching concept of digital transformation, data-driven Smart Product-Service Systems (SPSS) have emerged, prompting scholars to investigate development approaches tailored to diverse data sources. However, the current approaches employed in the construction of SPSS exhibit limited capability in processing vast amounts of user-generated unstructured data. The relationship between big data intelligence and personalized services remains undisclosed. Moreover, the current focus of SPSS orientation predominantly addresses end consumers or manufacturers, with inadequate attention given to dynamic collaborative models that involve multiple stakeholders. These gaps are particularly conspicuous in complex industries such as rehabilitation management. To tackle these challenges, this study introduces a novel SPSS development approach that integrates a large vision model and the fuzzy-DEMATEL method. Specifically, a data-driven predictive assessment module was proposed, which constructs a medical image dataset and trains a rehabilitation predictive assessment model based on the transformer architecture. Secondly, personalized intervention services were generated, involving the representation of system elements, configuration, and optimization of service parameters. The fuzzy-DEMATEL method is mainly used for the initialization of service parameters. Then, interactive feedback is integrated into rehabilitation exercises for achieving continuous rehabilitation evaluation and service improvement. To validate the proposed approach, a FPRM-SPSS case was implemented, and it shows that the predictive assessment model achieved a high level of accuracy when applied to the clinical dataset constructed in this study, and the system was evaluated with high scores in user satisfaction.}
}
@article{SHANG2024111829,
title = {A Span-based Multivariate Information-aware Embedding Network for joint relational triplet extraction of threat intelligence},
journal = {Knowledge-Based Systems},
volume = {295},
pages = {111829},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111829},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124004635},
author = {Wenli Shang and Bowen Wang and Pengcheng Zhu and Lei Ding and Shuang Wang},
keywords = {Threat intelligence, Knowledge graph, Extraction of relational triplets, Graph convolutional network, Aware embedding},
abstract = {The extraction of relational triplets in threat intelligence is a critical aspect of constructing a knowledge graph. However, the field encounters challenges like high semantic similarity among entities, limited relevance of entities, and a heavy reliance on experts, leading to low extraction efficiency. Currently, there is a lack of research on extracting threat intelligence relational triplets, which necessitates the development of an efficient extraction model. This study proposes a Span-based Multivariate Information-aware Embedding Network (SMIEN) for the joint extraction of threat intelligence relation triplets. SMIEN introduces aware embedding modules to capture fine-grained features of multivariate information, including semantic, temporal order, dependency, spans, span pairs, entity labels, and relational labels, and to enhance their interaction. Designed a Type-Aware Graph Convolutional Network (TA-GCN) to enhance the representation of key information between less relevant tokens in threat intelligence text sentences within the Dependency-Aware Embedding layer. The Entity-Aware Embedding module is designed to enhance fine-grained interaction between span information and other multivariate information, while the Relationship-Aware Embedding module is designed to determine correlations between span pairs and enhance interaction with relationship labels. Experimental results on the HACKER, RE-DNRTI, and RE-IVTI datasets demonstrate micro F1 scores of 44.73%, 81.74%, and 44.15%, respectively, highlighting the method’s effectiveness in extracting cyber threat intelligence and Internet of Vehicle (IoV) threat intelligence information. In addition, we have constructed a threat intelligence ontology (CVTIO) on which we have built a preliminary threat intelligence knowledge graph (CVTIKG) with 23,636 relational triplets. The CVTIKG can be found at https://github.com/wangxtz/CVTIKG.}
}
@article{DESSI2021253,
title = {Generating knowledge graphs by employing Natural Language Processing and Machine Learning techniques within the scholarly domain},
journal = {Future Generation Computer Systems},
volume = {116},
pages = {253-264},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2033003X},
author = {Danilo Dessì and Francesco Osborne and Diego {Reforgiato Recupero} and Davide Buscaldi and Enrico Motta},
abstract = {The continuous growth of scientific literature brings innovations and, at the same time, raises new challenges. One of them is related to the fact that its analysis has become difficult due to the high volume of published papers for which manual effort for annotations and management is required. Novel technological infrastructures are needed to help researchers, research policy makers, and companies to time-efficiently browse, analyse, and forecast scientific research. Knowledge graphs i.e., large networks of entities and relationships, have proved to be effective solution in this space. Scientific knowledge graphs focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. However, the current generation of knowledge graphs lacks of an explicit representation of the knowledge presented in the research papers. As such, in this paper, we present a new architecture that takes advantage of Natural Language Processing and Machine Learning methods for extracting entities and relationships from research publications and integrates them in a large-scale knowledge graph. Within this research work, we (i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, (ii) describe an approach for integrating entities and relationships generated by these tools, (iii) show the advantage of such an hybrid system over alternative approaches, and (vi) as a chosen use case, we generated a scientific knowledge graph including 109,105 triples, extracted from 26,827 abstracts of papers within the Semantic Web domain. As our approach is general and can be applied to any domain, we expect that it can facilitate the management, analysis, dissemination, and processing of scientific knowledge.}
}
@incollection{GAUTAM20233519,
title = {A Cloud-based Collaborative Interactive Platform for Education and Research in Dynamic Process Modelling},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {3519-3524},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50562-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044315274050562X},
author = {Vinay Gautam and Alberto Rodríguez-Fernández and Heinz A. Preisig},
keywords = {Process modelling, computer-aided modelling, Simphony-remote, Jupyterhub},
abstract = {Process modelling is used in many disciplines for various purposes like simulation, process design, optimisation and control. Although there is an increasing demand to build large, complex process models involving multiple disciplines, a systematic approach to developing such models is largely missing from engineering education and research. The ontology-based methodology and ProcessModeller (ProMo) tool suite developed over the years (Preisig, 2021, 2020; Elve and Preisig, 2018) help users to build multi-disciplinary and multi-scale models systematically. This paper presents a cloud-based platform ProMo-Remote that uses Simphony-Remote, a free and open-source web service. In this platform, the graphical user interfaces of the ProMo tool suite are accessible using a regular web browser, freeing the user from installing the desktop tool locally. Users can save, download/upload modelling data and use it in computational workflows utilising cloud-based interactive computing tools like Jupyter notebooks. The user can also collaborate with others by screen sharing in real time. The platform can easily be configured and deployed with additional applications to facilitate further education, training and research on the systematic development of complex process models.}
}
@article{ANTONELLI2019706,
title = {Tiphys: An Open Networked Platform for Higher Education on Industry 4.0},
journal = {Procedia CIRP},
volume = {79},
pages = {706-711},
year = {2019},
note = {12th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 18-20 July 2018, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.02.128},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119302495},
author = {Dario Antonelli and Doriana M. D’Addona and Antonio Maffei and Vladimir Modrak and Goran Putnik and Dorota Stadnicka and Chrysostomos Stylios},
keywords = {Industry 4.0, Social network-based education, Constructive alignment, Ontology, Knowledge base, Virtual reality},
abstract = {Objective of Tiphys project is building an Open Networked Platform for the learning of Industry 4.0 themes. The project will create a Virtual Reality (VR) platform, where users will be able to design and create a VR based environment for training and simulating industrial processes but they will be able to study and select among a set of models in order to standardize the learning and physical processes as a virtual representation of the real industrial world and the required interactions so that to acquire learning and training capabilities. The models will be structured in a modular approach to promote the integration in the existing mechanisms as well as for future necessary adaptations. The students will be able to co-create their learning track and the learning contents by collaborative working in a dynamic environment. The paper presents the development and validation of the learning model, built on CONALI learning ontology. The concepts of the ontology will be detailed and the platform functions will be demonstrated on selected use cases.}
}
@article{HUANG2025130856,
title = {A survey of data augmentation in named entity recognition},
journal = {Neurocomputing},
volume = {651},
pages = {130856},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130856},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225015280},
author = {Yi Huang and Yuhan Gao and Chengjuan Ren},
keywords = {Data augmentation, Named entity recognition, Linguistic features, Natural language processing},
abstract = {Data augmentation (DA), initially prominent in Computer Vision (CV), has been successfully adapted to Natural Language Processing (NLP), proving effective in mitigating data scarcity problems in the context of few-shot settings or scenarios where deep learning techniques may underperform. Moreover, the primary goal of DA is to expand and diversify training datasets by different methods, enabling models to generate more diverse and high-quality sythetic data for training the NER models. This survey explored DA techniques in the context of Named Entity Recognition (NER), including linguistic features and four categories of data augmentation methods. Furthermore, we reviewed commonly used datasets in DA tasks, discussed some potential practical applications, and examined key challenges and future directions in DA for NER. These findings serve as a valuable reference for learners and offer insights for researchers. As an essential and cost-effective approach, DA alleviates data scarcity and overfitting in the NER models by facilitating the integration of diverse augmentation methods.}
}
@article{BARBER2023100202,
title = {Transformative agents of change and investigative neurotechnologies: A qualitative study of psychedelic technology identities},
journal = {SSM - Qualitative Research in Health},
volume = {3},
pages = {100202},
year = {2023},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2022.100202},
url = {https://www.sciencedirect.com/science/article/pii/S2667321522001640},
author = {Michaela Barber and John Gardner and Adrian Carter},
keywords = {Psychedelics, Innovation, Qualitative methodology, Mental health treatment},
abstract = {Clinical and basic science research with psychedelics is a nascent but rapidly growing field. Historically, psychedelics research has faced setbacks due to overenthusiasm, methodological challenges, and conflicting worldviews. This can in part be attributed to the dual heritages of spiritualism and science that ground western psychedelic use. Through semi-structured interviews with Australian researchers, we explore how issues of ontological conflict are playing out in contemporary research, using the analytical frame of promissory technology identities. We illustrate two identities currently at play that are invoked and negotiated by researchers: psychedelics as transformative agents of change and as investigational neurotechnologies. We argue that these identities represent differing ontological heritages in psychedelics research and may affect the design, interpretation and translation of clinical and neuroscientific psychedelics research. These identities exist in compromise, though not without tensions that may rupture or resolve as the field matures, such as priorisation of clinical research versus basic or preclinical research, Researchers appear sensitive to a need to pragmatically deploy both identities in order to engage the diverse stakeholders required for research and translation. This paper both provides a case study of promissory technology identities, and demonstraties the concept's usefulness as a framework for understanding the social factors that affect the production, communication and reception of evidence in the development of novel technologies.}
}
@article{EBERLE2022102502,
title = {Anxiety geopolitics: Hybrid warfare, civilisational geopolitics, and the Janus-faced politics of anxiety},
journal = {Political Geography},
volume = {92},
pages = {102502},
year = {2022},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2021.102502},
url = {https://www.sciencedirect.com/science/article/pii/S0962629821001621},
author = {Jakub Eberle and Jan Daniel},
keywords = {Anxiety, Hybrid warfare, Critical geopolitics, Ontological security, Lacan},
abstract = {Working at the intersection of political geography and international relations, this article does two things. First, it theorises the relationship between geopolitics and anxiety. Second, it uses this conceptual lens to analyse and critique the discourse of ‘hybrid warfare’. The conceptual part draws on Lacanian political theory and contributes to critical geopolitics, ontological security studies, and the literature on politics of anxiety. It is built around the notion of anxiety geopolitics, which denotes a discourse that promises to deal with social anxiety by providing geopolitical fixes to it, yet also ultimately fails in doing so. We then move to argue that ‘hybrid warfare’ is a prime case of such discourse. Using examples from the Czech Republic, we show how the discourse of ‘hybrid warfare’ successfully connects different sorts of anxieties together and creates a sense of ontological security by linking them to familiar East/West civilisational geopolitics that points to Russia as the ultimate culprit. Yet, at the same time, the discourse simultaneously subverts itself by portraying ‘hybrid threats’ as too insidious, invisible and constantly shifting to be ever possibly durably resolved. We conclude that this makes ‘hybrid warfare’ self-defeating, normatively problematic, and strategically impractical.}
}
@article{EMBLEMSVAG2025105716,
title = {A strategic- and tactical model for managing complex risks in social systems},
journal = {International Journal of Disaster Risk Reduction},
volume = {128},
pages = {105716},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2025.105716},
url = {https://www.sciencedirect.com/science/article/pii/S2212420925005400},
author = {Marianne Synnes Emblemsvåg and Jan Emblemsvåg},
keywords = {Complex systems, COVID-19, Financial crisis, OODA loop, Risk management, Risk communication},
abstract = {Complex risks are unique as they emerge from potentially undetectable sources into problematic situations often with high- and lasting impact, such as the COVID-19 pandemic. Their emergent nature, starting with weak signals, renders the traditional risk management approach of risk identification less effective. Assigning probability estimates can also be difficult because some complex risks, such as pandemics, are certain to emerge. To address these challenges, the literature is reviewed to understand complexity, complex risks, risk management and risk communication and more. One key insight is that the maneuverability inherent in the Observe-Orient-Decide and Act (OODA) model used in maneuver warfare is beneficial for managing complex risks. Another important insight is that management and communication cannot be treated as separated and sequential processes. Hence, to manage complex risks, a model based on the OODA model is developed with more explicit focus on risk communication through active leadership. The financial crisis and the COVID-19 pandemic are used throughout the paper to illustrate key points. Operationalization of the model is future work.}
}
@article{MEDEIROS20243014,
title = {Towards Public Health-Risk Detection and Analysis through Textual Data Mining},
journal = {Procedia Computer Science},
volume = {246},
pages = {3014-3023},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.370},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024025},
author = {Gabriel H.A. Medeiros and Lina F. Soualmia and Cecilia Zanni-Merk},
keywords = {Knowledge Graphs, COVID-19, Spatiotemporal reasoning, Event-Based Surveillance, Biomedical data},
abstract = {The coronavirus disease (COVID-19) spread rampantly around the world at the beginning of 2020 before the governments of each country could prevent it by making decisions based on medical data analysis. With proper formalization, the terabytes of new textual data available online every day could have been used for the early description and detection of cases of this virus. Since then, the number of Event-Based Surveillance (EBS) applications has increased exponentially. These applications aim to mine channels of unstructured data to detect signs of possible public health events. However, one problem with such systems is the need for expert intervention to define which event will be captured, which relevant terms should be used in the search, and to analyze the events to modify the search procedure constantly. Another problem is that many of these applications do not consider both spatial and temporal characteristics. Addressing such limitations, this article presents a novel approach. We propose the biomedical domain specialization of the Core Propagation Phenomenon Ontology (PropaPhen) to capture spatiotemporal characteristics of the propagation of health-related phenomena. We also propose the Description-Detection-Framework (DDF), which leverages PropaPhen, UMLS, and OpenStreetMaps to detect new medical events automatically. Finally, we demonstrate a use case with experiments on extracts from online newspapers about COVID-19. The results show that DDF can be useful for detecting clusters of suspicious cases of possible emerging health-related phenomena.}
}
@article{ZHU2023e18600,
title = {Syndecan-4 is the key proteoglycan involved in mediating sepsis-associated lung injury},
journal = {Heliyon},
volume = {9},
number = {8},
pages = {e18600},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e18600},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023058085},
author = {Zhipeng Zhu and Xiaoyan Ling and Hongmei Zhou and Junran Xie},
keywords = {Endothelial cells, Bioinformatic analysis, Syndecans, Differentially expressed genes, Acute lung injury, Lipopolysaccharide},
abstract = {Vascular endothelial cell dysfunction involving syndecan (SDC) proteoglycans contributes to acute sepsis-associated lung injury (ALI), but the exact SDC isoform involved is unclear. We aimed to clarify which SDCs are involved in ALI. A relevant gene expression dataset (GSE5883) was analysed for differentially expressed genes (DEGs) between lipopolysaccharide (LPS)-treated and control lung endothelial cells and for SDC isoform expression. Bioinformatic analyses to predict DEG function were conducted using R language, Gene Ontology, and the Kyoto Encyclopedia of Genes and Genomes. SDC2 and SDC4 expression profiles were examined under inflammatory conditions in human lung vascular endothelial cell and mouse sepsis-associated ALI models. Transcription factors regulating SDC2/4 were predicted to indirectly assess SDC involvement in septic inflammation. Of the DEGs, 224 and 102 genes were up- and downregulated, respectively. Functional analysis indicated that DEGs were involved in modulating receptor ligand and signalling receptor activator activities, cytokine receptor binding, responses to LPS and molecules of bacterial origin, regulation of cell adhesion, tumour necrosis factor signalling, and other functions. DEGs were also enriched for cytoplasmic ribonucleoprotein granules, transcription regulator complexes, and membrane raft cellular components. SDC4 gene expression was 4.5-fold higher in the LPS group than in the control group, while SDC2 levels were similar in both groups. SDC4 mRNA and protein expression was markedly upregulated in response to inflammatory injury, and SDC4 downregulation severely exacerbated inflammatory responses in both in vivo and in vitro models. Overall, our data demonstrate that SDC4, rather than SDC2, is involved in LPS-induced sepsis-associated ALI.}
}
@article{HE2021101345,
title = {A decision-making model for knowledge collaboration and reuse through scientific workflow},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101345},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101345},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000987},
author = {Longlong He and Wei Guo and Pingyu Jiang},
keywords = {Manufacturing knowledge, Knowledge collaboration and reuse, Decision workflow},
abstract = {Past, present and future, to realize the aim of product CTQS (i.e., lower cost, faster time to market, higher quality and better service) with manufacturing intelligence, few manufacturers have no longer engaged in product related production decision support problem (P-DSP). However, P-DSP solving (P-DSPS) is a multi-criteria decision-making problem, which is context sensitive in solution objects-attributes and chaos in the decision process of manufacturing knowledge collaboration and reuse. To alleviate these limitations, this paper presents a novel triple deep workflow model for P-DSPS. Driven by a wicked task query, the proposed workflow of P-DSPS (WP-DSPS) has the function to retrieve similarity-based alternatives from domain knowledge driven solution flow (KSF) and to evaluate with expert knowledge collaboration from knowledge driven decision flow (KDF) based on utility theory under the task event driven control flow (ECF) strategy and operation logic. In the view of alternative adaption, a domain knowledge ontology-based degree of similarity (DoS) determines the P-DSPS alternatives width, a utility function-based degree of decision (DoD) determines alternatives quality, and a belief-based knowledge fusion technique is used to synthesize decision conflicts with a consensus degree (CD). To support the proposed models, a workflow-based system prototype is proposed and validated in two case studies.}
}
@article{CAHYO2025100354,
title = {A Novel Named Entity Recognition approach of Indonesian fake news using part of speech and BERT model on presidential election},
journal = {International Journal of Information Management Data Insights},
volume = {5},
number = {2},
pages = {100354},
year = {2025},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2025.100354},
url = {https://www.sciencedirect.com/science/article/pii/S2667096825000369},
author = {Puji Winar Cahyo and Ulfi Saidata Aesyi and Widodo Agus Setianto and Tatang Sulaiman},
keywords = {Natural language processing, Named Entity Recognition, Part of speech, BERT, Election},
abstract = {Fake news often spreads rapidly and can mislead readers, which makes it important to approach such information with caution. In text-based information, content extraction can be used to determine the meaning and intent of the message. Therefore, this research aims to develop a novel approach for entity detection in Indonesian-language fake news texts by applying BiLSTM-CRF, BiGRU, and BERT models. The novelty of this study lies in the integration of Part-of-Speech (PoS) tagging before processing words for entity detection. Words tagged as Noun (NN) and Proper Noun (NNP) are transformed into entity labels such as ORG for organizations, PER for people, and LOC for locations. Meanwhile, words labeled as Verb (VB) are converted into the ACT entity to represent actions. Evaluations were conducted by integrating PoS tagging with entity detection using the BiLSTM-CRF model, which achieved an F1-Score of 81.26%. The BiGRU-based model achieved an F1-Score of 79.46%, while the BERT-based model achieved the highest F1-Score of 87.38%. These results demonstrate that the BERT model, when combined with PoS tagging, provides the best performance and can effectively be used to detect entities in fake news. The entity detection process was further applied to identify fake news during the 2024 Indonesian presidential and vice-presidential election period. By counting the number of mentions of each candidate and their running mate labeled as PER entities, it has result the Prabowo Subianto–Gibran Rakabuming Raka pair appeared in 49 fake news articles. This was followed by the Ganjar Pranowo–Mahfud MD pair with 14 fake news articles, and the Anies Baswedan–Muhaimin Iskandar pair with 13 articles. All identified data have been filtered to retain only unique entries.}
}
@incollection{WOODWARD2020375,
title = {Poststructuralism/Poststructuralist Geographies},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {375-385},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10686-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955106869},
author = {Keith Woodward and Deborah P. Dixon and John {Paul Jones}},
keywords = {Actor–network theory, Decentered, Deconstruction, Discourse, Epistemology, Feminism, Marxism, Ontology, Poststructuralism, Social construction, Structuralism},
abstract = {Poststructuralism brought to the field of geography in the late 1980s and 1990s a critique that unsettled both the epistemological (i.e., theories on how we know the world) and ontological (theories on what that world consists of and how it works) moorings of the then dominant theoretical frameworks: spatial science, critical realism and Marxism, and humanism. Its originality lies in its rigorous interrogation of core concepts—such as objectivity and subjectivity, center and margin, materialism and idealism, truth and fiction—that underpin much of modern-day academia. By claiming that any ontology is always already an outcome of epistemology, of our socially constructed ways of knowing, poststructuralists asked that we reflect not only on how we know but also on how elements of ontology—such as space, place, nature, culture, individual, and society—become framed in thought in the first instance. Criticisms that poststructuralists have been concerned only with discourse and representation, as opposed to the “real” material conditions within which these meanings were considered to be embedded, had a profound impact on geographic debate during the 1990s. In responding, poststructuralists have interrogated more closely the ontological ramifications of the work of Derrida and Foucault and have also explored the work of Deleuze and Latour.}
}
@article{JIA2023101915,
title = {From simple digital twin to complex digital twin part II: Multi-scenario applications of digital twin shop floor},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101915},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101915},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000435},
author = {Wenjie Jia and Wei Wang and Zhenzu Zhang},
keywords = {Digital twin, Shop floor, Intelligent manufacturing, Deep learning},
abstract = {The shop floor has always been an important application object for the digital twin. It is well known that production, process, and product are the core business of the shop floor. Therefore, the digital twin shop floor covers multi-dimensional information and multi-scale application scenarios. In this paper, the digital twin shop floor is constructed according to the modeling method of the complex digital twin proposed in Part I. The digital twin shop floor is firstly divided into several simple digital twins that focus on scenarios of different scales. Two simple application scenarios are constructed, including tool wear prediction and spindle temperature prediction. Main functions in different application scenarios, such as data acquisition, data processing, and data visualization, are implemented and encapsulated as components to construct simple digital twins. Secondly, ontology models, knowledge graphs, and message queues are used to assemble these simple digital twins into the complex digital twin shop floor. And two complex application scenarios are constructed, including machining geometry simulation considering spindle temperature and production scheduling considering tool wear. The implementation of the complex digital twin shop floor demonstrates the feasibility of the proposed modeling method.}
}
@article{LOPEZBELLO2019100181,
title = {From medical records to research papers: A literature analysis pipeline for supporting medical genomic diagnosis processes},
journal = {Informatics in Medicine Unlocked},
volume = {15},
pages = {100181},
year = {2019},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2019.100181},
url = {https://www.sciencedirect.com/science/article/pii/S2352914819300309},
author = {Fernando {López Bello} and Hugo Naya and Víctor Raggio and Aiala Rosá},
keywords = {Controlled vocabulary, Natural language processing, Genomics, Automated pattern recognition, Publications, Medical records},
abstract = {In this paper, we introduce a framework for processing genetics and genomics literature, based on ontologies and lexical resources from the biomedical domain. The main objective is to support the diagnosis process that is done by medical geneticists who extract knowledge from published works. We constructed a pipeline that gathers several genetics- and genomics-related resources and applies natural language processing techniques, which include named entity recognition and relation extraction. Working on a corpus created from PubMed abstracts, we built a knowledge database that can be used for processing medical records written in Spanish. Given a medical record from Uruguayan healthcare patients, we show how we can map it to the database and perform graph queries for relevant knowledge paths. The framework is not an end user application, but an extensible processing structure to be leveraged by external applications, enabling software developers to streamline incorporation of the extracted knowledge.}
}
@article{HOSSAIN202025,
title = {Knowledge-driven machine learning based framework for early-stage disease risk prediction in edge environment},
journal = {Journal of Parallel and Distributed Computing},
volume = {146},
pages = {25-34},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2020.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0743731520303324},
author = {M. Anwar Hossain and Rahatara Ferdousi and Mohammed F. Alhamid},
keywords = {Machine learning, Epidemiology, Self-screening, Healthcare, Disease likelihood},
abstract = {Early-stage disease risk prediction can be beneficial to improve the health of the mass and can reduce the economic burden of late treatment. Machine learning has played a pivotal role in predictive systems, which requires achieving a specific degree of accuracy for healthcare systems. Most recently researchers have found the necessity of bridging between epidemiology and machine learning classifications toward health risk prediction. This work proposes an epidemiology knowledge-driven unique model that follows the principle of association rule-based ontology to select features and classification techniques. The goal of this approach is to generalize a framework for future robust systems to predict the likelihood of diseases, which can be executed in the edge computing environment. The framework introduces epidemiological library and structured attribute set along with the library of precaution to derive the disease risk-prediction process. To investigate the adoption of the epidemiology knowledge-driven model, we considered a real dataset of early-stage likelihood prediction of diabetes and carried out a set of experiments for highlighting the significance of several epidemiological factors. The classification aspect of the framework is further compared with widely accepted approaches for machine learning based healthcare, which shows the novelty of the proposed model.}
}
@article{BUSBOOM2024100602,
title = {Automated generation of OPC UA information models — A review and outlook},
journal = {Journal of Industrial Information Integration},
volume = {39},
pages = {100602},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100602},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000463},
author = {Axel Busboom},
keywords = {Asset Administration Shell (AAS), Industry 4.0, Information modeling, OPC UA},
abstract = {OPC Unified Architecture (OPC UA) is widely considered a key enabler of “Industry 4.0” and one of the most promising standardized platforms for industrial communications from sensor to cloud. One of its key features is a powerful framework for information modeling that allows to compose semantic models and enables self-describing information provisioning. However, building OPC UA information models can be a tedious task, requiring deep understanding of both the OPC UA meta-model and the application domain to be modeled. Therefore, a wide range of methods for automatically generating OPC UA information models has been described in the literature, either from relational databases, from application-domain specific models, tools, or languages, or by aggregating multiple component-level models into a single, system-level information model. This paper reviews the state-of-the-art in tools and methods for automated generation of OPC UA information models. It is argued that enriching the tool landscape and interoperability, in particular with industrial engineering tools, will be a prerequisite for unleashing the full potential of OPC UA.}
}
@article{KAUR2024114342,
title = {Neurostrategy: A scientometric analysis of marriage between neuroscience and strategic management},
journal = {Journal of Business Research},
volume = {170},
pages = {114342},
year = {2024},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2023.114342},
url = {https://www.sciencedirect.com/science/article/pii/S0148296323007014},
author = {Vaneet Kaur},
keywords = {Dynamic capabilities, Managerial ambidexterity, Managerial attention, Neuroscience, Neurostrategy, Strategic management},
abstract = {This study represents one of the earliest attempts at providing a complete scientometric mapping and a systematic review of the nascent field of neurostrategy. Machine-based algorithms and text-mining have been used to – (a) clarify the dominant concepts at the junction of neuroscience and strategic management; (b) identify the ontological and epistemological foundations of neurostrategy; (c) explain how the scholarly discourse around neurostrategy has evolved; (d) reveal the trends that are gaining traction within neurostrategy research; and (e) develop propositions at the confluence of managerial capabilities, knowledge management, dynamic capabilities, and neurostrategy. The study unveils how neurostrategy represents a quintet of disciplines and lays bare the hypes and hopes surrounding neurostrategy. The study explains how the road that leads to competitive success passes through the development of neuronally intelligent strategies that not only resolve the battle between the organization and its people, but also the one within an organizational decision maker.}
}
@article{HODOROG2022104026,
title = {Machine learning and Natural Language Processing of social media data for event detection in smart cities},
journal = {Sustainable Cities and Society},
volume = {85},
pages = {104026},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104026},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722003468},
author = {Andrei Hodorog and Ioan Petri and Yacine Rezgui},
keywords = {Social media, Smart cities, Event detection, Natural Language Processing, Citizen satisfaction, Machine learning},
abstract = {Social media data analysis in a smart city context can represent an efficacious instrument to inform decision making. The manuscript strives to leverage the power of Natural Language Processing (NLP) techniques applied to Twitter messages using supervised learning to achieve real-time automated event detection in smart cities. A semantic-based taxonomy of risks is devised to discover and analyse associated events from data streams, with a view to: (i) read and process, in real-time, published texts (ii) classify each text into one representative real-world category (iii) assign a citizen satisfaction value to each event. To select the language processing models striking the best balance between accuracy and processing speed, we conducted a pre-emptive evaluation, comparing several baseline language models formerly employed by researchers for event classification. A heuristic analysis of several smart cities and community initiatives was conducted, with a view to define real-world scenarios as basis for determining correlations between two or more co-occurring event types and their associated levels of citizen satisfaction, while further considering environmental factors. Based on Multiple Regression Analysis (MRA), we established the relationships between scenario variables, obtaining a variance of 60%–90% between the dependent and independent variables. The selected combination of supervised NLP techniques leverages an accuracy of 88.5%. We found that all regression models had at least one variable below the 0.05 threshold of the f−test, therefore at least one statistically significant independent variable. These findings ultimately illustrate how citizens, taking the role of active social sensors, can yield vital data that authorities can use to make educated decisions and sustainably construct smarter cities.}
}
@article{PEREZESCOBAR202423,
title = {Minimal logical teleology in artifacts and biology connects the two domains and frames mechanisms via epistemic circularity},
journal = {Studies in History and Philosophy of Science},
volume = {104},
pages = {23-37},
year = {2024},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2024.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0039368124000104},
author = {José Antonio Pérez-Escobar},
keywords = {Minimal logical teleology, Analogies, Scientific explanation, Epistemic circularity, Scientific modelling, Cognitive neuroscience},
abstract = {The understanding of artifacts and biological phenomena has often influenced each other. This work argues that at the core of these epistemic bridges there are shared teleological notions and explanations manifested in analogies between artifacts and biological phenomena. To this end, I first propose a focus on the logical structure of minimal teleological explanations, which renders said epistemic bridges more evident than an ontological or metaphysical approach to teleology, and which can be used to describe scientific practices in different areas by virtue of formal generality and minimalism (section 2). Second, I show how this approach highlights some epistemic features shared by the understanding of artifacts and biological phenomena, like a specific kind of epistemic circularity, and how functional analogies between artifacts and biological phenomena translate such epistemic circularity from one domain to the other (section 3). Third, I conduct a case study on the scientific practice around the brain's “compass”, showing how the understanding of artifacts influences purpose ascription and measurement, and frames mechanisms in biology, especially in areas where purpose ascription is most difficult, like cognitive neuroscience (sections 4 and 5).}
}
@article{MORTENSEN2025100368,
title = {The FAIR AOP roadmap for 2025: Advancing findability, accessibility, interoperability, and re-usability of adverse outcome pathways},
journal = {Computational Toxicology},
volume = {35},
pages = {100368},
year = {2025},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2025.100368},
url = {https://www.sciencedirect.com/science/article/pii/S2468111325000283},
author = {Holly M. Mortensen and Maciej Gromelski and Ginnie Hench and Marvin Martens and Clemens Wittwehr and Saurav Kumar and Vikas Kumar and Karine Audouze and Vassilis Virvilis and Penny Nymark and Michelle Angrish and Iseult Lynch and Stephen Edwards and Barbara Magagna and Marcin W. Wojewodzic},
keywords = {AOP, Annotation, Artificial intelligence (AI), Biomedical, Computational toxicology, Data domain, Data integration, Database, Environmental health science, FAIR, Gene, Protein, Proteomic, Mechanistic, Natural language processing, Next generation risk assessment, New Approach Methods (NAMs), OMICs, Pathway, Toxicology},
abstract = {Adverse Outcome Pathways (AOPs) describe the mechanistic interactions of biological entities with a stressor (chemical, nanomaterial, radiation, virus, etc.) that produce an adverse response. How these interactions and associations are catalogued contributes to our ability to understand mechanistic effects and apply this knowledge to New Approach Methods (NAMs) that have the potential to reduce animal testing in chemical, biological, and material safety assessments. Making AOP data align with FAIR (Findable, Accessible, Interoperable, and Reusable) metadata standards relies on technical tools that implement and process AOP data and related metadata, and the establishment of coordinated and consensus computational bioinformatic methods. Herein current efforts in addressing the FAIRification of AOP mechanistic data and metadata, as well as the international, collaborative efforts to document, and improve the (re)-use and reliability of AOP information will be described. These coordinated efforts contribute to the establishment of a directive for the processing and storing of standardized AOP mechanistic data in the AOP-Wiki repository, and application of these data to next generation risk assessment.}
}
@article{BROWN20202410,
title = {Capturing Multicellular System Designs Using Synthetic Biology Open Language (SBOL)},
journal = {ACS Synthetic Biology},
volume = {9},
number = {9},
pages = {2410-2417},
year = {2020},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.0c00176},
url = {https://www.sciencedirect.com/science/article/pii/S2161506320000704},
author = {Bradley Brown and Bryan Bartley and Jacob Beal and Jasmine E. Bird and Ángel Goñi-Moreno and James Alastair McLaughlin and Göksel Mısırlı and Nicholas Roehner and David James Skelton and Chueh Loo Poh and Irina Dana Ofiteru and Katherine James and Anil Wipat},
keywords = {multicellular systems, microbial communities, Synthetic Biology Open Language (SBOL), data standards},
abstract = {Synthetic biology aims to develop novel biological systems and increase their reproducibility using engineering principles such as standardization and modularization. It is important that these systems can be represented and shared in a standard way to ensure they can be easily understood, reproduced, and utilized by other researchers. The Synthetic Biology Open Language (SBOL) is a data standard for sharing biological designs and information about their implementation and characterization. Previously, this standard has only been used to represent designs in systems where the same design is implemented in every cell; however, there is also much interest in multicellular systems, in which designs involve a mixture of different types of cells with differing genotype and phenotype. Here, we show how the SBOL standard can be used to represent multicellular systems, and, hence, how researchers can better share designs with the community and reliably document intended system functionality.
}
}
@incollection{GOLEBIEWSKI2025321,
title = {Data Formats for Systems Biology, Systems Medicine and Computational Modeling},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {321-333},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00164-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001640},
author = {Martin Golebiewski and Gerhard Mayer},
keywords = {Combine, Data standards, FAIR data, Interoperability, Metadata, Modeling, Systems biology, Systems medicine, Virtual human twin},
abstract = {In systems biology and systems medicine data of many different types and obtained from manifold methods are used to set up computational models for simulations and predictions. These data need to be interoperable to integrate them into the models or use them for model validation. To achieve this, standards are crucial for structuring, describing, and associating models and data, as well as their respective parts, graphical visualization, and information about applied experimental or computational methods. Such standards also assist with describing how constituent parts interact together, or are linked, and how they are embedded in their environmental and experimental context.}
}
@article{ELSAPPAGH2021680,
title = {Alzheimer’s disease progression detection model based on an early fusion of cost-effective multimodal data},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {680-699},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20329824},
author = {Shaker El-Sappagh and Hager Saleh and Radhya Sahal and Tamer Abuhmed and S.M. Riazul Islam and Farman Ali and Eslam Amer},
keywords = {Alzheimer disease, Machine learning, Multimodal data analysis, Disease progression detection},
abstract = {Alzheimer’s disease (AD) is a severe neurodegenerative disease. The identification of patients at high risk of conversion from mild cognitive impairment to AD via earlier close monitoring, targeted investigations, and appropriate management is crucial. Recently, several machine learning (ML) algorithms have been used for AD progression detection. Most of these studies only utilized neuroimaging data from baseline visits. However, AD is a complex chronic disease, and usually, a medical expert will analyze the patient’s whole history when making a progression diagnosis. Furthermore, neuroimaging data are always either limited or not available, especially in developing countries, due to their cost. In this paper, we compare the performance of five widely used ML algorithms, namely, the support vector machine, random forest, k-nearest neighbor, logistic regression, and decision tree to predict AD progression with a prediction horizon of 2.5 years. We use 1029 subjects from the Alzheimer’s disease neuroimaging initiative (ADNI) database. In contrast to previous literature, our models are optimized using a collection of cost-effective time-series features including patient’s comorbidities, cognitive scores, medication history, and demographics. Medication and comorbidity text data are semantically prepared. Drug terms are collected and cleaned before encoding using the therapeutic chemical classification (ATC) ontology, and then semantically aggregated to the appropriate level of granularity using ATC to ensure a less sparse dataset. Our experiments assert that the early fusion of comorbidity and medication features with other features reveals significant predictive power with all models. The random forest model achieves the most accurate performance compared to other models. This study is the first of its kind to investigate the role of such multimodal time-series data on AD prediction.}
}
@article{ZHANG2018153,
title = {Representing place locales using scene elements},
journal = {Computers, Environment and Urban Systems},
volume = {71},
pages = {153-164},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0198971517303903},
author = {Fan Zhang and Ding Zhang and Yu Liu and Hui Lin},
keywords = {Scene ontology, Quantitative representation, Street-level images, Deep learning, Place formalization},
abstract = {Locale is one of the basic elements of place, referring to the physical settings and visual appearance of a place. Understanding and representing a locale is of great importance in terms of human perception and human activity. However, taking a quantitative measurement of the visual appearance of urban environment has proven to be challenging because visual information is inherently ambiguous and semantically impoverished. To mitigate this issue, this paper employs street-level images as the proxy for urban physical appearance, utilizes the recently developed image semantic segmentation techniques to parse an urban scene into scene elements, and proposes a framework for locale representation using scene elements. The framework is composed of two major components: street scene ontology and street visual descriptor, which are aimed at street scene qualitative understanding and quantitative representation respectively. A case study is developed to demonstrate the application and advantage of the street scene ontology and street visual descriptor. A series of quantitative analyses demonstrates the ability and great potential of the framework for investigating the connections between place and other socioeconomic factors.}
}
@incollection{HOVENGA2022169,
title = {Chapter 8 - Health data standards’ limitations},
editor = {Evelyn Hovenga and Heather Grain},
booktitle = {Roadmap to Successful Digital Health Ecosystems},
publisher = {Academic Press},
pages = {169-207},
year = {2022},
isbn = {978-0-12-823413-6},
doi = {https://doi.org/10.1016/B978-0-12-823413-6.00015-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012823413600015X},
author = {Evelyn Hovenga and Heather Grain},
keywords = {Ontology, Terminology, Data set, Electronic data processing, Big data, Data management, Linguistics},
abstract = {Data represent foundational assets of any healthcare delivery system. Clinical data form the basis of electronic communications from point of data collection to storage and archiving. Computers cannot handle ambivalence, hence the need for the widespread adoption of technical and terminology standards. Many domain ontologies and terminologies, developed to suit a variety of different purposes well before this digital era, are reviewed and examined to determine their usability within a digital ecosystem. The ontological data modelling approach was found to result in the highest degrees of expressivity and formalism available today. Resulting artefacts linked to standard value sets were found to be most comprehensive with their ability to best represent data for a lifetime support, patient safety, and electronic communication. Many issues and limitations, such as variations regarding design principles used, overlaps, and shortcomings, are identified and discussed. There is a need for a major globally led transformation.}
}
@article{HUGUET2024100721,
title = {Effects of gene dosage on cognitive ability: A function-based association study across brain and non-brain processes},
journal = {Cell Genomics},
volume = {4},
number = {12},
pages = {100721},
year = {2024},
issn = {2666-979X},
doi = {https://doi.org/10.1016/j.xgen.2024.100721},
url = {https://www.sciencedirect.com/science/article/pii/S2666979X24003501},
author = {Guillaume Huguet and Thomas Renne and Cécile Poulain and Alma Dubuc and Kuldeep Kumar and Sayeh Kazem and Worrawat Engchuan and Omar Shanta and Elise Douard and Catherine Proulx and Martineau Jean-Louis and Zohra Saci and Josephine Mollon and Laura M. Schultz and Emma E.M. Knowles and Simon R. Cox and David Porteous and Gail Davies and Paul Redmond and Sarah E. Harris and Gunter Schumann and Guillaume Dumas and Aurélie Labbe and Zdenka Pausova and Tomas Paus and Stephen W. Scherer and Jonathan Sebat and Laura Almasy and David C. Glahn and Sébastien Jacquemont},
keywords = {copy-number variants, gene dosage, cognitive ability, CNV-GWAS, burden association, genetic constraint, transcriptomic, Gene Ontology},
abstract = {Summary
Copy-number variants (CNVs) that increase the risk for neurodevelopmental disorders also affect cognitive ability. However, such CNVs remain challenging to study due to their scarcity, limiting our understanding of gene-dosage-sensitive biological processes linked to cognitive ability. We performed a genome-wide association study (GWAS) in 258,292 individuals, which identified—for the first time—a duplication at 2q12.3 associated with higher cognitive performance. We developed a functional-burden analysis, which tested the association between cognition and CNVs disrupting 6,502 gene sets biologically defined across tissues, cell types, and ontologies. Among those, 864 gene sets were associated with cognition, and effect sizes of deletion and duplication were negatively correlated. The latter suggested that functions across all biological processes were sensitive to either deletions (e.g., subcortical regions, postsynaptic) or duplications (e.g., cerebral cortex, presynaptic). Associations between non-brain tissues and cognition were driven partly by constrained genes, which may shed light on medical comorbidities in neurodevelopmental disorders.}
}
@article{DAURIA2023200161,
title = {Improving graph embeddings via entity linking: A case study on Italian clinical notes},
journal = {Intelligent Systems with Applications},
volume = {17},
pages = {200161},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200161},
url = {https://www.sciencedirect.com/science/article/pii/S2667305322000989},
author = {Daniela D'Auria and Vincenzo Moscato and Marco Postiglione and Giuseppe Romito and Giancarlo Sperlí},
keywords = {Entity linking, Graph embedding, Link prediction, Health analytics, Healthcare},
abstract = {The ever-increasing availability of Electronic Health Records (EHRs) is the key enabling factor of precision medicine, which aims to provide therapies and diagnoses based not only on medical literature, but also on clinical experience and individual information of patients (e.g. genomics, lifestyle, health history). The unstructured nature of EHRs has posed several challenges on their effective analysis, and heterogeneous graphs are the most suitable solution to handle the heterogeneity of information contained in EHRs. However, while EHRs are an extremely valuable data source, information from current medical literature has yet to be considered in clinical decision support systems. In this work, we build an heterogeneous graph from Italian EHRs provided by the Hospital of Naples Federico II, and we define a methodological workflow allowing us to predict the presence of a link between patients and diagnosed diseases. We empirically demonstrate that linking concepts to biomedical ontologies (e.g. UMLS, DBpedia) — which allow us to extract entities and relationships from medical literature — is significantly beneficial to our link-prediction workflow in terms of Area Under the ROC curve (AUC) and Mean Reciprocal Rank (MRR).}
}
@article{GUVEN2022116592,
title = {Natural language based analysis of SQuAD: An analytical approach for BERT},
journal = {Expert Systems with Applications},
volume = {195},
pages = {116592},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116592},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422000884},
author = {Zekeriya Anil Guven and Murat Osman Unalir},
keywords = {Natural language processing, BERT, Text analysis, Question answering, SQuAD},
abstract = {In recent years, deep learning models have been used in the implementation of question answering systems. In this study, the performance of the question answering system was evaluated from the perspective of natural language processing using SQuAD, which was developed to measure the performance of deep learning language models. In line with the evaluations, in order to increase the performance, 3 natural language based methods, namely RNP, that can be used with pre-trained BERT language models have been proposed and they have increased the performance of the question answering system in which the pre-trained BERT models are used by 1.1% to 2.4%. As a result of the application of RNP methods with sentence selection, an increase in accuracy between 6.6% and 8.76% was achieved in answer detection. Since these methods don’t require any training process, it has been shown that they can be used in question answering systems to increase the performance of any deep learning model.}
}
@article{HU2024109484,
title = {LVF: A language and vision fusion framework for tomato diseases segmentation},
journal = {Computers and Electronics in Agriculture},
volume = {227},
pages = {109484},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109484},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924008755},
author = {Yang Hu and Jiale Zhu and Guoxiong Zhou and Mingfang He and Mingjie Lv and Junhui Wang and Aibin Chen and Jinsheng Deng and Yichu Jiang},
keywords = {Pre-segmentation, Probabilistic differential fusion network, Reinforcement feature network, Threshold filtering network, Multi-scale cross-nesting network},
abstract = {With the development of deep learning technology, the control of tomato diseases has emerged as a crucial aspect of intelligent agricultural management. While current research on tomato disease segmentation has made considerable strides, challenges persist due to the susceptibility of tomato leaf diseases to strong light reflections and shadow gradients in sunlight. Additionally, the complex backgrounds found in agricultural fields often lead to model confusion, resulting in inaccurate segmentation. Traditional methods for tomato disease segmentation rely on single-modal image-based models, which struggle when dealing with the nuanced features and limited scope of tomato leaf diseases. To address these issues, our study introduces the LVF framework, a dual-modal approach combining image and text information for pre-segmentation of tomato diseases. We began by creating a new dataset labeled with both images and text, specifically focusing on diseased tomato leaves with guidance from agricultural experts. For image processing, we developed a probabilistic differential fusion network to mitigate interference caused by high-frequency noise, leveraging color and grayscale images. Furthermore, our reinforcement feature network and threshold filtering network enhance useful information while filtering out negative information from the fused images. In text processing, we proposed a multi-scale cross-nesting network to integrate semantic information about diseases across different scales and types. By nesting Bert-processed word vectors with fused image vectors, our model gains a deeper understanding of semantic information, thereby improving its ability to segment crop diseases accurately. Our experiments, conducted on self-constructed tomato datasets as well as public datasets for tomatoes and maize, demonstrated the efficacy and robustness of our approach in leaf disease segmentation. The LVF framework offers a valuable tool to enhance the accuracy of crop disease segmentation, especially in complex agricultural environments.}
}
@article{LUNARDI201852,
title = {IoT-based human action prediction and support},
journal = {Internet of Things},
volume = {3-4},
pages = {52-68},
year = {2018},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2018.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S2542660518300647},
author = {Gabriel Machado Lunardi and Fadi Al Machot and Vladimir A. Shekhovtsov and Vinícius Maran and Guilherme Medeiros Machado and Alencar Machado and Heinrich C. Mayr and José Palazzo M. {de Oliveira}},
keywords = {Probabilistic ontologies, Uncertainty reasoning, Ambient assistance, Smart home, Context-awareness},
abstract = {It is an important topic in Active and Assisted Living (AAL) research and development to support elderly people suffering from memory impairment in their daily activities. A promising approach to such support is providing memory aids based on knowledge of how the person to be supported usually (i.e., in an unimpaired condition) copes with her/his daily activities. Such knowledge may be captured by IoT solutions,appropriately structured and stored in a knowledge base, and exploited when the need of support is detected. Determining the best help for a given situation implies decision-making, since the actions– flow (behavior) of an activity usually involves probabilistic branches: An automated system needs to decide which of the possible next actions is best suited for the user in a given situation. Problems of this nature involve uncertainty levels that have to be dealt with. Many approaches to this problem exploit statistical data only, thus ignoring important semantic data as, for instance, are provided by Ontologies. However, ontologies do not support reasoning over uncertainty natively. In this paper, we present a probabilistic semantic model that represent information from IoT sources and enables reasoning over uncertainty without losing semantic information. This model is implemented as an extension of the Human Behavior Monitoring and Support (HBMS) approach that provides a conceptual ”human cognitive model” for representing the user–s behavior and its context in her/his living environment. The performance of this approach was evaluated using real data collected from a smart home prototype equipped with installable sensors and IoT devices. The experiments provided promising results which we will discuss regarding limits and challenges to overcome.}
}
@article{LIN2025102383,
title = {A systematic review of artificial intelligence applications and methodological advances in patent analysis},
journal = {World Patent Information},
volume = {82},
pages = {102383},
year = {2025},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2025.102383},
url = {https://www.sciencedirect.com/science/article/pii/S017221902500050X},
author = {Tzu-Yu Lin and Li-Chieh Chou},
keywords = {Artificial intelligence, Patent analysis, Bibliometric analysis, Topic modeling, Systematic review},
abstract = {Structured Abstract
Purpose
This study aims to systematically synthesize the practical applications of artificial intelligence (AI) in patent analysis by constructing a comprehensive matrix that aligns distinct AI techniques with their corresponding analytical tasks. The “AI Technique and Analytical Task” matrix provides a structured framework for understanding how various AI approaches are deployed across different functional objectives within the patent analysis domain.
Design/methodology/approach
This study integrates bibliometric analysis, BERT-based topic modeling, and literature review to explore AI applications in patent analysis. Data were retrieved from the Web of Science Core Collection using a dual-focus search strategy targeting AI techniques and patent analysis tasks. A clear distinction was made to exclude studies analyzing AI trends using patent data, retaining only those applying AI methods to patent analytics. With these strategies, 718 relevant publications were selected as the basis for analysis.
Findings
The results reveal exponential growth in AI-powered patent analysis research since the mid-2010s, with Technological Forecasting and Social Change (TFSC), Scientometrics, and World Patent Information (WPI) identified as the leading publication platforms. Geographical analysis shows that China and South Korea have rapidly increased their research output and institutional engagement, while the U.S. maintains a foundational yet less recent presence. With topic modeling technique, this study identified eleven major thematic clusters, spanning tasks such as emerging knowledge discovery, technology forecasting, and opportunity identification. These were integrated into “AI Technique and Analytical Task” matrix, which systematically maps the relationships between AI methods (such as pretrained language models, convolutional neural networks, semantic analysis, and topic modeling) and their practical implementations. Among these, patent classification and nature language processing (NLP) emerged as the most impactful applications, underscoring AI's vital role in enabling scalable, data-driven approaches to managing complex patent information.
Originality
This study presents a novel integration of multi-layered literature retrieval strategies, bibliometric analysis, BERT-based topic modeling, and an AI technique-to-analytical task matrix to construct a systematic and structured knowledge framework. This integrative approach not only delineates the interdisciplinary evolution of AI applications in patent analysis but also provides strategic guidance for future research, particularly in advancing empirical validation, informing policy applications, and promoting global inclusivity in this emerging field.}
}
@article{BAILEY2024100647,
title = {Knowing versus doing: Children's social conceptions of and behaviors toward virtual reality agents},
journal = {International Journal of Child-Computer Interaction},
volume = {40},
pages = {100647},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100647},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000151},
author = {Jakki O. Bailey and J. Isabella Schloss},
keywords = {Virtual reality, Children, Characters, Interpersonal distance, Ontological understanding, Embodied agents},
abstract = {Virtual reality (VR) can blur fantasy and reality for children by replacing their physical world with artificial stimuli. This immersive technology often includes intelligent and interactive embodied agents. In this within-participant study, we investigated 5- to 9-year-old children's (N = 25) social conceptions of and behaviors toward embodied agents in VR that represented different probabilities of existence in their daily lives (i.e., a probable child, an improbable giraffe, and an impossible Muppet). Participants rated the child and the giraffe agents significantly higher as social living beings than they rated the Muppet agent. When tasked with walking up to each embodied agent, significantly more children chose to approach the giraffe agent first rather than the child and Muppet agents. However, children stood significantly closer to the child agent, and significantly more children spontaneously reached out to try to touch the Muppet agent. Finally, children expressed strong emotions (amazement, excitement, happiness, fear, worry) toward all three embodied agents, with the giraffe evoking the most positive and the Muppet the most negative emotions. These results show that types of embodied agents in VR significantly impact children's conscious and unconscious social conceptions and behaviors differently, with implications for future interventions.}
}
@article{JIANG2025114439,
title = {Dynamic model selection in enterprise forecasting systems using sequence modeling},
journal = {Decision Support Systems},
volume = {193},
pages = {114439},
year = {2025},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2025.114439},
url = {https://www.sciencedirect.com/science/article/pii/S0167923625000405},
author = {Jinhang Jiang and Kiran Kumar Bandeli and Karthik Srinivasan},
keywords = {Enterprise forecasting, Deep learning, Model flip, Dynamic model selection, Large-scale forecasting},
abstract = {Enterprise forecasting systems often involve modeling a large scale of heterogeneous time series using a pool of candidate algorithms, such as in the case of simultaneous sales forecasts of thousands of stock-keeping units. In such cases, it can be advantageous to automatically monitor and replace algorithms for each time series. We introduce TimeSpeaks, a framework that adapts sequence modeling in natural language processing to the problem of dynamic model selection in enterprise forecasting. We instantiate our framework using sequential (BiLSTM) and transformer-based (TimeXer) deep learning models to learn the temporal dependencies between candidate algorithms. We compare the performance of our framework with state-of-the-art forecasting models using two public benchmarking datasets. We further demonstrate its practical application on two retail case studies, while comparing them to alternative model selection scenarios. TimeSpeaks has superior predictive performance and scalability across different scenarios and datasets. Its ability to adapt to evolving data patterns and its minimal reliance on exogenous information make TimeSpeaks a suitable framework for large-scale enterprise forecasting applications.}
}
@article{QIU2025100691,
title = {Text semantics to controllable design: A residential layout generation method based on stable diffusion model},
journal = {Developments in the Built Environment},
volume = {23},
pages = {100691},
year = {2025},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2025.100691},
url = {https://www.sciencedirect.com/science/article/pii/S2666165925000912},
author = {Zijin Qiu and Jiepeng Liu and Yi Xia and Hongtuo Qi and Pengkun Liu},
keywords = {Residential layout generation, Multimodal generative design, Natural language processing, Stable diffusion model, Knowledge graph},
abstract = {Controlling flexibility and design effectiveness is challenging in artificial intelligence-based residential layout design. Compared to the previous rule-based or graph-based generation methods, this paper proposes a controllable multimodal approach based on the Stable Diffusion model for generating residential layouts. This method incorporates natural language as design constraints and introduces ControlNet, which enables the generation of controllable layouts through two distinct pathways. A knowledge graph and natural language mapping scheme is proposed to provide an interpretable representation of the design knowledge encapsulated in the knowledge graph. The comprehensibility of natural language and the diversity of input options enable professionals and non-professionals to directly express design requirements, thereby providing a flexible and controllable design method. Finally, comparative and ablation experiments verify the controllability and diversity of the two proposed design paths under multimodal constraints.}
}
@article{ROULAND2025112219,
title = {A model-driven formal methods approach to software architectural security vulnerabilities specification and verification},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112219},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112219},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002632},
author = {Quentin Rouland and Brahim Hamid and Jason Jaskolka},
keywords = {Engineering secure systems, Model-driven approach, Software architecture, Vulnerability, Formal methods, Metamodel},
abstract = {Detecting and addressing security vulnerabilities in software designs is crucial for ensuring the reliable and safe operation of systems. Existing approaches for vulnerability specification lack the necessary flexibility for practical use. To tackle this issue, we propose an integrated model-driven approach for vulnerability detection and treatment during software architecture design. The approach involves specifying vulnerabilities as properties of a modeled system in a technology-independent language, expressing conditions for vulnerability detection using a language supported by automated tools, and recommending security requirements to mitigate detected vulnerabilities. Formalized vulnerabilities and security requirements are presented as model libraries to facilitate reuse. Our methodology employs first-order and modal logic as a technology-independent formalism, with Alloy as the tool-supported language for modeling and software development. We have developed a Model-Driven Engineering (MDE) tool to implement this approach. To validate our work, we apply it to representative vulnerabilities based on the Common Weakness Enumeration (CWE) classifications within the context of secure component-based software architecture development.}
}
@article{ZHAO2025562,
title = {Intelligent Agent-based Analysis of Collaborative Problem Solving Skills in Sessions},
journal = {Procedia Computer Science},
volume = {266},
pages = {562-569},
year = {2025},
note = {The 12th International Conference on Information Technology and Quantitative Management (ITQM 2025)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.08.071},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925023774},
author = {Yilin Zhao and Juanqiong Gou and Fangcong Zhang and Mengxin Zhang and Xiaodan Yu},
keywords = {Collaborative Problem Solving Skills, Session, Human-to-Human Collaboraion, Large Language Model, Intelligent Agent},
abstract = {Collaborative Problem Solving Skills (CPS Skills) as the core literacy of talents in the 21st century, the analysis and assessment of it is an important issue in the field of education. With the advancement of digitalization and intelligence in education, the application of artificial intelligence technology in education tends to be the norm, which also brings more opportunities and challenges to the analysis of CPS skills. At present, CPS skills assessment is mainly conducted based on computers with high training costs. The superior creation ability, reasoning ability and interpretability of AI agents can precisely solve the limitations of the current computer-based analysis and assessment of students’ abilities. In this paper, we propose an analysis method of CPS skills based on students’ conversation data in complex problem situations, and design an AI agent framework based on the method to automate the analysis and assessment of CPS skills, to provide scientific and effective evaluation tools and improvement suggestions for the educational practice in the intelligent era.}
}
@article{OZCINAR2025101039,
title = {Exploring technological evolution of AIED field using topic modelling and link prediction analysis based on patent data},
journal = {Sustainable Futures},
volume = {10},
pages = {101039},
year = {2025},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2025.101039},
url = {https://www.sciencedirect.com/science/article/pii/S2666188825006033},
author = {Hüseyin Özçınar and Aylin {Sabancı Bayramoğlu}},
keywords = {Artificial intelligence, Education, Patent data, Link prediction, Topic modelling},
abstract = {This study aims to analyze the artificial intelligence in education (AIED) field using patent data to identify the key actors, main themes, and their evolution over time. The study also aims to predict potential innovative technological development areas that may emerge in the AIED field in the future. Descriptive statistics were used to investigate the current actors in the field and the changes in their contributions. Technological developments in the field were attempted to be revealed using topic modelling and IPC code pair trend analysis methods. To obtain predictions about the future of the field, the IPC co-occurrence network was analyzed using link prediction methods. Research results indicate that educational robots, characterized by personalized interaction and social engagement capabilities, are among the most frequently patented technologies in the AIED field. Similarly, innovative foreign language teaching materials, notably speech recognition and interactive learning tools, also emerge as prominent areas of technological innovation. Additionally, artificial intelligence (AI) technologies have been increasingly integrated into educational management, particularly in areas such as admission processes, performance tracking, and resource management. These AI-driven management innovations enable institutions to operate more sustainably and efficiently by optimizing resource allocation and reducing administrative burdens. The use of AI is also expected to find application in various innovative forms in foreign language instruction.}
}
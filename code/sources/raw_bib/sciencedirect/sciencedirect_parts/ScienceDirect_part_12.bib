@article{ZHANG2025103781,
title = {A knowledge injection method for supporting automated compliance checking of shield tunnel designs},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103781},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103781},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625006743},
author = {Yuxian Zhang and Xuhua Ren and Stefan Fuchs and Robert Amor},
keywords = {Shield tunnel, Compliance checking, Ontology, Deep learning, Knowledge injection},
abstract = {Shield tunnels have become a widely adopted structure in infrastructure construction, and the compliance checking process is critical to ensuring both structural safety and functional performance. Traditional manual drawing verification is labor-intensive and inefficient. Currently, automated compliance checking (ACC) is based on the development of a conversion process that translates code text into a computer-readable format. However, these conversion processes are typically based on the building design code text alone and overlook significant domain knowledge. Therefore, this study targets the gap in automated model checking in the shield tunneling field and proposes a method that integrates domain knowledge and deep learning (DL) models to achieve computerisation of code texts to facilitate ACC. An ontology encompassing knowledge related to shield tunnel design is developed, enabling the management and utilization of structured domain knowledge. The pre-trained DL model is employed as the translation strategy, and a knowledge injection method based on Induced-SP is proposed to enhance model performance by incorporating ontology-based knowledge. The proposed method is evaluated using the Chinese shield tunnel design code text. The results demonstrate that the DL model enhanced by the concise knowledge injection method achieved an 8.60% improvement in the F1-score compared to the original model in the test set. The DL model integrated with domain knowledge, as proposed in this study, establishes a novel paradigm for code text translation.}
}
@article{CHEN2018177,
title = {An ontology-based spatial data harmonisation for urban analytics},
journal = {Computers, Environment and Urban Systems},
volume = {72},
pages = {177-190},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518300632},
author = {Yiqun Chen and Soheil Sabri and Abbas Rajabifard and Muyiwa Elijah Agunbiade},
keywords = {Ontology, Spatial data integration, Semantic enrichment, Urban analytics},
abstract = {Data heterogeneity is one of the most challenging problems in urban data analytics. When obtained from various providers or custodians, datasets for the same domain themes may dramatically differ in formats due to many reasons such as historical legacies, changing definitions or standards across jurisdictions etc. It hinders urban analysts and researchers from understanding and using these data and makes results comparison and interpretation obscure. Ontology, usually created by domain experts, offers a comprehensive representation of knowledge including concepts, relations and properties in a domain. It defines the real world in abstract and offers a universal and stable schema for data harmonisation. This paper proposes a fast, extensible solution for eliminating data heterogeneity by using ontology. Starting from conceptualising domain knowledge to domain ontology, we discuss a two-level mapping mechanism which bonds the nexus between data and ontology using mapping rules. A semantic translation engine is also introduced to automate the data harmonisation process. A real case - urban density indicators computation - also demonstrates the usability of the proposed framework and the results show strong potentials for applying this method to broader urban analytics application scenarios.}
}
@article{LOMAS2023101035,
title = {The person as an extended field: Querying the ontological binaries and dominant “container” metaphor at the core of psychology},
journal = {New Ideas in Psychology},
volume = {70},
pages = {101035},
year = {2023},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2023.101035},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X23000284},
author = {Tim Lomas},
keywords = {Ontology, Self, Consciousness, Mind, Body},
abstract = {The Western-centricity of psychology means it has inherited some of the key ontological categories and distinctions at the heart of Western cultures. This paper identifies four such distinctions that have been particularly influential in psychology: mind-body; subjective-objective; self-other; and inner-outer. Together, these have created a pervasive view that the mind – and the person more broadly – is metaphorically like a “container.” However, this paper proposes that a better conceptualization, or at least a complementary one, may be a “field,” whereby people's being extends outwards, beyond the apparent boundary of their skin, into the world. Such perspectives have been especially prominent in other cultures and traditions (like Buddhism), but have pedigree in the West too. The paper thus draws on various cultural sources, and numerous disciplines both within psychology and beyond, to make its case. It is hoped the discussion may help psychology reflect on and re-evaluate the ontological assumptions at its core, and to engage with field-based perspectives that may be provide a useful alternative or complement to the standard container metaphor.}
}
@article{KHAN2018826,
title = {Semantic Web and ontology-based applications for digital libraries},
journal = {The Electronic Library},
volume = {36},
number = {5},
pages = {826-841},
year = {2018},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-08-2017-0168},
url = {https://www.sciencedirect.com/science/article/pii/S0264047318000334},
author = {Shakeel Ahmad Khan and Rubina Bhatti},
keywords = {Pakistan, Digital libraries, Ontology, Semantic Web},
abstract = {Purpose
The purpose of this paper is to explore useful Semantic Web technologies and ontology-based applications for digital libraries. It also investigates the perceptions of university librarians and academicians in Pakistan about Semantic Web technologies and their use in digital libraries.
Design/methodology/approach
An exploratory research design based on Delphi research strategy was conducted to answer the research questions. Interviews were conducted with a purposive sample of 50 key informants including university librarians and academicians to explore their perceptions about Semantic Web technologies and their use in digital libraries. Thematic analysis of interview data was conducted to obtain results.
Findings
The results of this paper showed that DuraCloud, Semantic information mashup, OntoEdit and resource description framework (RDF) are the various Semantic Web applications which are useful for digital libraries to develop semantic relationships among digital contents and increase their accessibility in the web environment. Findings revealed that Semantic Web provides precise results and meets user information needs in an effective way. Results also showed that next-generation digital libraries use context-awareness technology, intelligent agent software and detecting sensors to analyze user information needs and provide dynamic information services. This paper recommended that librarians should embrace the use of emerging web technologies in libraries and offer library services through the medium of the web.
Practical implications
This paper envisaged the future of digital library services and Semantic Web applications that can be used to re-structure metadata of digital library. This paper has practical implications for librarians to consider the useful applications of Semantic Web for digital library and enhance the interoperability of metadata among heterogeneous information systems. Practically, results obtained from this paper are highly useful for library schools and LIS teachers to up-date their curriculum by incorporating new contents related to web languages and Semantic Web applications for digital libraries.
Originality/value
This paper identifies various Semantic Web applications which are useful for developing Semantic Digital Libraries.}
}
@article{ANGSUCHOTMETEE20201140,
title = {MSSN-Onto: An ontology-based approach for flexible event processing in Multimedia Sensor Networks},
journal = {Future Generation Computer Systems},
volume = {108},
pages = {1140-1158},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.01.044},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18301419},
author = {Chinnapong Angsuchotmetee and Richard Chbeir and Yudith Cardinale},
keywords = {Multimedia sensors, Ontology, Event processing, Semantic interoperability, Reasoning},
abstract = {Multimedia Sensor Networks (MSNs) have gained much attention in recent years from the emerging trends of Internet of Things (IoT). They can be found in different scenarios in our everyday life (e.g., smart homes, smart buildings). Sensors in MSNs can have different capacities, produce multiple kinds of outputs, and have different output encoding formats. Thus, detecting complex events, which requires the aggregation of several sensor readings, can be difficult due to the lack of a generic model that can describe: (i) sensor networks infrastructure, (ii) individual sensor specificities, as well as (iii) multimedia data, while allowing the alignment with the application domain knowledge. In this study, we propose Multimedia Semantic Sensor Network Ontology (MSSN-Onto) to ensure MSNs modeling and provide both syntactic and semantic data interoperability for defining and detecting events in various domains. To show the readiness of MSSN-Onto, we used it as the core ontology of a dedicated framework (briefly defined here). We also adopted MSSN-Onto in HIT2GAP European Project. A prototype has been implemented to conduct a set of tests. Experimental results show that MSSN-Onto can be used to: (i) effectively model MSNs and multimedia data; (ii) define complex events; and (iii) allow to build an efficient event querying engine for MSNs.}
}
@article{SYAMILI2018119,
title = {Developing an ontology for Greek mythology},
journal = {The Electronic Library},
volume = {36},
number = {1},
pages = {119-132},
year = {2018},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-02-2017-0030},
url = {https://www.sciencedirect.com/science/article/pii/S026404731800067X},
author = {C. Syamili and R.V. Rekha},
keywords = {Ontology, Greek mythology},
abstract = {Purpose
The purpose of this study is to illustrate the development of ontology for the heroes of the ancient Greek mythology and religion. At present, a number of ontologies exist in different domains. However, ontologies of epics and myths are comparatively very few. To be more specific, nobody has developed such ontology for Greek mythology. This paper describes the attempts at developing ontology for Greek mythology to fill this gap.
Design/methodology/approach
This paper follows a combination of different methodologies, which is assumed to be a more effective way of developing ontology for mythology. It has adopted motivating scenario concept from Gruninger and Fox, developing cycle from Methontology and the analytico–synthetic approach from yet another methodology for ontology, and hence, it is a combination of three existing approaches.
Findings
A merged methodology has been adopted for this paper. The developed ontology was evaluated and made to meet with the information needs of its users. On the basis of the study, it was found that Greek mythology ontology could answer 62 per cent of the questions after first evaluation, i.e. 76 out of the 123 questions. The unanswered questions were analyzed in detail for further development of the ontology. The missing concepts were fed into the ontology; the ontology obtained after this stage was an exhaustive one.
Practical implications
This ontology will grow with time and can be used in semantic applications or e-learning modules related to the domain of Greek mythology.
Originality/value
This work is the first attempt to build ontology for Greek mythology. The approach is unique in that it has attempted to trace out the individual characteristics as well as the relationship between the characters described in the work.}
}
@article{JONQUET2018126,
title = {AgroPortal: A vocabulary and ontology repository for agronomy},
journal = {Computers and Electronics in Agriculture},
volume = {144},
pages = {126-143},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2017.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168169916309541},
author = {Clément Jonquet and Anne Toulet and Elizabeth Arnaud and Sophie Aubin and Esther {Dzalé Yeumo} and Vincent Emonet and John Graybeal and Marie-Angélique Laporte and Mark A. Musen and Valeria Pesce and Pierre Larmande},
keywords = {Ontologies, Controlled vocabularies, Knowledge organization systems or artifacts, Ontology repository, Metadata, Mapping, Recommendation, Semantic annotation, Agronomy, Food, Plant sciences, Biodiversity},
abstract = {Many vocabularies and ontologies are produced to represent and annotate agronomic data. However, those ontologies are spread out, in different formats, of different size, with different structures and from overlapping domains. Therefore, there is need for a common platform to receive and host them, align them, and enabling their use in agro-informatics applications. By reusing the National Center for Biomedical Ontologies (NCBO) BioPortal technology, we have designed AgroPortal, an ontology repository for the agronomy domain. The AgroPortal project re-uses the biomedical domain’s semantic tools and insights to serve agronomy, but also food, plant, and biodiversity sciences. We offer a portal that features ontology hosting, search, versioning, visualization, comment, and recommendation; enables semantic annotation; stores and exploits ontology alignments; and enables interoperation with the semantic web. The AgroPortal specifically satisfies requirements of the agronomy community in terms of ontology formats (e.g., SKOS vocabularies and trait dictionaries) and supported features (offering detailed metadata and advanced annotation capabilities). In this paper, we present our platform’s content and features, including the additions to the original technology, as well as preliminary outputs of five driving agronomic use cases that participated in the design and orientation of the project to anchor it in the community. By building on the experience and existing technology acquired from the biomedical domain, we can present in AgroPortal a robust and feature-rich repository of great value for the agronomic domain.}
}
@article{DEEPAK201814,
title = {Personalized and Enhanced Hybridized Semantic Algorithm for web image retrieval incorporating ontology classification, strategic query expansion, and content-based analysis},
journal = {Computers & Electrical Engineering},
volume = {72},
pages = {14-25},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2018.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617318189},
author = {Gerard Deepak and J Sheeba Priyadarshini},
keywords = {Homonyms, Image retrieval, Ontologies, Recommendation systems, SVM, Web image mining},
abstract = {Most of the existing web search systems are query-centric and are not user-centric. Mining images from the Web is a challenging task as it requires choosing the right methodology. A strategy that recommends images for homonyms and contextually similar terms have been proposed. The proposed system facilitates ontology modeling for homonyms and contextually related synonymous terms using description logics semantics and semantic similarity computation. An Enhanced Hybrid Semantic Algorithm that computes the semantic similarity and establishes dynamic OntoPath for easing the web image recommendation has been proposed. The proposed system classifies the ontologies using SVM and a Homonym LookUp directory. The methodology focuses on generating unique classes of images as an initial recommendation set. Based on the user click, strategic expansion of OntoPath takes place. Personalization is achieved by content-based analysis of the user click-through data. An overall accuracy of 95.87% is achieved by the proposed system.}
}
@article{GREENBAUM2019103981,
title = {Improving documentation of presenting problems in the emergency department using a domain-specific ontology and machine learning-driven user interfaces},
journal = {International Journal of Medical Informatics},
volume = {132},
pages = {103981},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.103981},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619307427},
author = {Nathaniel R. Greenbaum and Yacine Jernite and Yoni Halpern and Shelley Calder and Larry A. Nathanson and David A. Sontag and Steven Horng},
keywords = {User-computer interface, Artificial intelligence, Machine learning, Contextual autocomplete, Ontology},
abstract = {Objectives
To determine the effect of a domain-specific ontology and machine learning-driven user interfaces on the efficiency and quality of documentation of presenting problems (chief complaints) in the emergency department (ED).
Methods
As part of a quality improvement project, we simultaneously implemented three interventions: a domain-specific ontology, contextual autocomplete, and top five suggestions. Contextual autocomplete is a user interface that ranks concepts by their predicted probability which helps nurses enter data about a patient’s presenting problems. Nurses were also given a list of top five suggestions to choose from. These presenting problems were represented using a consensus ontology mapped to SNOMED CT. Predicted probabilities were calculated using a previously derived model based on triage vital signs and a brief free text note. We evaluated the percentage and quality of structured data captured using a mixed methods retrospective before-and-after study design.
Results
A total of 279,231 consecutive patient encounters were analyzed. Structured data capture improved from 26.2% to 97.2% (p < 0.0001). During the post-implementation period, presenting problems were more complete (3.35 vs 3.66; p = 0.0004) and higher in overall quality (3.38 vs. 3.72; p = 0.0002), but showed no difference in precision (3.59 vs. 3.74; p = 0.1). Our system reduced the mean number of keystrokes required to document a presenting problem from 11.6 to 0.6 (p < 0.0001), a 95% improvement.
Discussion
We demonstrated a technique that captures structured data on nearly all patients. We estimate that our system reduces the number of man-hours required annually to type presenting problems at our institution from 92.5 h to 4.8 h.
Conclusion
Implementation of a domain-specific ontology and machine learning-driven user interfaces resulted in improved structured data capture, ontology usage compliance, and data quality.}
}
@article{INGRAM2019100300,
title = {Searching for meaning: Co-constructing ontologies with stakeholders for smarter search engines in agriculture},
journal = {NJAS - Wageningen Journal of Life Sciences},
volume = {90-91},
pages = {100300},
year = {2019},
issn = {1573-5214},
doi = {https://doi.org/10.1016/j.njas.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1573521418302161},
author = {Julie Ingram and Pete Gaskell},
keywords = {Ontology, Stakeholder, Knowledge domain, Search engine, Digital tools, Farmers},
abstract = {A key challenge in agriculture, as in other disciplines, is taking a large body of research-based knowledge and making it meaningful to the user-audience. Computer aided search engines potentially can offer widespread access to large repositories with relevant reports and publications, however the usefulness of such systems for the practitioners who are dealing with multi-faceted and context-related issues is often limited. Building search engines with user-centered ontologies offer a means of resolving this as it provides a vocabulary common to different stakeholders and can optimise the interaction between practitioner users and the expert system. The paper critically reflects on the methodology used to construct a user-centered ontology in the development of a search engine designed to help agricultural practitioners (farmers and advisers) find useful research outputs. This involved the iterative participation of domain experts, adviser practitioners and stakeholder communities in ten diverse case studies across Europe. Specifically it analyses the design, validation and evaluation phases of the ontology development drawing on qualitative data (reports, observations, interviews) from four case studies and asks: How effective is the process of co-constructing an ontology with experts, practitioners and other stakeholders in enabling the search for useful and meaningful knowledge? In doing this, it contributes to a deeper theoretical understanding of shared concepts and meanings in the context of digital communications in the agricultural arena by adapting Carlile’s (2004) framework of syntactic, semantic and pragmatic capacities.}
}
@article{POLENGHI202155,
title = {Multi-attribute Ontology-based Criticality Analysis of manufacturing assets for maintenance strategies planning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {55-60},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.192},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321009630},
author = {A. Polenghi and I. Roda and M. Macchi and A. Pozzetti},
keywords = {criticality analysis, maintenance, maintenance strategy, ontology, OWL, SWRL},
abstract = {Planning maintenance strategies in advance with respect to the installation and running of manufacturing assets positively affects operational expenditure during their usage. However, the early stages of the asset lifecycle are poor of operational data. Thus, domain knowledge of experts, related to the asset, the process and production requirements, is the primary source to determine which maintenance strategy better fits in a specific context. Hence, ontology-based systems represent a relevant help in this direction. In this work, given the importance of the criticality analysis (CA) for maintenance planning, the CA is analyzed from an ontological perspective to automatically associate a maintenance strategy to the asset under analysis. Moreover, to unveil the power of CA, its multi-attribute nature is considered, including not only availability as guiding criterion, but also quality and energy. The developed ontology-based CA allows to (i) semantically align all involved experts, and (ii) potentiate the analysis through reasoning capabilities. Finally, preliminary results from an industrial case in a food company are shown.}
}
@article{JIANG2024100723,
title = {Generating the assembly instructions of helicopter subassemblies using the hierarchical pruning strategy and large language model},
journal = {Journal of Industrial Information Integration},
volume = {42},
pages = {100723},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100723},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001663},
author = {Mingjie Jiang and Yu Guo and Shaohua Huang and Jun Pu},
keywords = {Assembly instruction generation, Knowledge graph, Subgraph matching, Large language model, Helicopter subassembly assembly},
abstract = {Assembly instructions are process documents in detail describing the operation steps, materials, tools, fixtures, and assembly sequences in assembly procedures. Due to assembly instructions including numerous contents, and the content being easy for workers to understand, process designers need to spend lots of time thinking and authoring assembly instructions to ensure that workers can complete the assembly task according to the assembly instructions. Focusing on the difficulties of the variety of assembly instructions and the process factors implicit in the standard languages of assembly instructions, a method of assembly instruction generation for helicopter subassemblies is proposed. First, a data representation model of multi-source heterogeneous knowledge and information based on knowledge graphs is designed and established. Then, a hierarchical pruning VF3 algorithm is presented to reuse assembly instructions according to hybrid similarity. Finally, a process factor revision model based on RoBERTa-BiLSTM-CRF is proposed to generate revised assembly instructions. Helicopter subassemblies, which contain 11,240 assembly procedures, are used to evaluate the performance of the method for generating assembly instructions. The proposed method greatly reduces the time cost of assembly instruction authoring and promotes the intelligent development of assembly process design.}
}
@incollection{MAHDAVI2023447,
title = {14 - Ontologically streamlined data for building design and operation support},
editor = {Peter Droege},
booktitle = {Intelligent Environments (Second Edition)},
publisher = {North-Holland},
edition = {Second Edition},
pages = {447-474},
year = {2023},
isbn = {978-0-12-820247-0},
doi = {https://doi.org/10.1016/B978-0-12-820247-0.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202470000035},
author = {Ardeshir Mahdavi and Dawid Wolosiuk},
keywords = {Building design, Building information, Building operation, Building performance, Ontology, Modeling},
abstract = {Evidence-based methods for building design and operation support require multiple streams of data. The heterogeneous nature of this data and the multiplicity of deployed software formats represent major obstacles toward the efficient use of such data in the building delivery and management processes, including building performance specification and assessment. To address the related challenges, versatile data ontologies are needed. To this end, a recently introduced building performance data ontology can identify, categorize, and capture the complexities of building related performance data and its attributes. The related data ontologization approach involves i) preprocessing, ii) categorical identification, and iii) supplementation of the relevant attributes, and iv) encoding in a proper file format. This chapter describes such an ontologization process as applied to large real-world building-related datasets. Specifically, building-related data is first processed in terms of fidelity and quality to be subsequently ontologized and delivered to a number of building performance assessment applications.}
}
@article{HWANG2025112879,
title = {AI agent-based indoor environmental informatics: Concept, methodology, and case study},
journal = {Building and Environment},
volume = {277},
pages = {112879},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2025.112879},
url = {https://www.sciencedirect.com/science/article/pii/S0360132325003610},
author = {Jaemin Hwang and Sungmin Yoon},
keywords = {LLM, Indoor environments, Thermal environment, PMV, Brick schema, AI agent, Ontology},
abstract = {Analyzing and improving indoor environments requires continuous intervention from human experts, which is challenging in practice. To address this limitation, an ontology-based AI agent system can be employed. This study proposes a conceptual model of AI agent-based indoor environmental informatics (IEI), develops an indoor environmental ontology by extending Brick schema, and demonstrates its application in real indoor environment. AI agent-based IEI is an approach that builds an integrated information system to capture relationships among indoor environments, occupants, and building systems so that utilizes AI agent for continuous indoor environment management. The AI agent leverages indoor environmental ontology, intrusive data, and indoor environmental toolkit to perform holistic analysis of indoor thermal environments and provides strategies for enhancing thermal comfort during the operational phase. The proposed concept was implemented in the Dynamo environment and applied to an office space. For the collection of real indoor environmental data, intrusive measurement was conducted over five days, and an indoor environmental ontology for the target space was developed. Indoor environmental toolkit used for the system included spatial coordinate extractor (SCE) for extracting spatial element coordinates, ontology file generator (OFG) for creating ontology files, and predicted mean vote (PMV) model for calculating PMV. The AI agent identified a PMV variation of 0.77, a discomfort rate of 28.2 %, and the disparity between physical sensor data and occupants’ subjective thermal comfort. Furthermore, the AI agent suggested practical strategies for improvement, including determining window status based on outdoor temperature, adjusting air conditioner operation, and modifying occupant seating arrangements.}
}
@article{JOHANSEN2025103305,
title = {Knowledge graph exploitation to enhance the usability of risk assessment in construction safety planning},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103305},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103305},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625001983},
author = {K.W. Johansen and C. Schultz and J. Teizer},
keywords = {Construction safety, hazards, and risk assessment, Construction safety design and planning, Cross-domain knowledge exploitation, Digital twin for construction safety, Knowledge graph representation, Ontology for construction safety},
abstract = {Construction projects and their dynamic and yet hazardous work environments face significant challenges. Despite advancements, many proposed solutions for information extraction and utilization remain impractical due to complexity and lack of interoperability. Information is often siloed in proprietary formats, making it difficult to integrate. This issue is evident in the construction safety domain, where advanced risk analysis tools provide detailed insights to hazards but can be overwhelming. Similar challenges exist in cost estimation, schedule evaluation, progress monitoring, and quality compliance checking. Decision-making in construction scheduling struggles to assess how changes impact site safety due to insufficient information and knowledge extraction capabilities, especially when it comes to cross-domain knowledge extraction. This study aims to make safety information accessible to safety and planning professionals. By leveraging Digital Twins, automated safety analysis, and knowledge representation, we enable decision-makers to gain deeper insights into their domain and understand the interplay between project planning and safety. We propose a framework for knowledge extraction, an ontology for capturing knowledge, and query building blocks to transform natural language questions into actionable queries. These methods are tested in a case study, revealing valuable insights into the cross-domain impact of decisions.}
}
@article{KHOUDJA2022,
title = {Deep Embedding Learning With Auto-Encoder for Large-Scale Ontology Matching},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.297042},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000515},
author = {Meriem Ali Khoudja and Messaouda Fareh and Hafida Bouarfa},
keywords = {Auto Encoder, Large-Scale Ontology Matching, Ontology Alignment, Semantic Concepts’ Embeddings, Semantic Web, Unsupervised Deep Learning},
abstract = {ABSTRACT
Ontology matching is an efficient method to establish interoperability among heterogeneous ontologies. Large-scale ontology matching still remains a big challenge for its time and large memory space consumption. The actual solution to this problem is ontology partitioning, which is also challenging. This paper presents DeepOM, an ontology matching system, to deal with this large-scale heterogeneity problem without partitioning using deep learning techniques. It consists of creating semantic embeddings for concepts of input ontologies using a reference ontology and uses them to train an auto-encoder in order to learn more accurate and less dimensional representations for concepts. The experimental results of its evaluation on large ontologies and its comparison with different ontology matching systems which have participated to the same test challenge are very encouraging with a precision score of 0.99. They demonstrate the higher efficiency of the proposed system to increase the performance of the large-scale ontology matching task.}
}
@article{MASMOUDI20181865,
title = {An ontology-based monitoring system for multi-source environmental observations},
journal = {Procedia Computer Science},
volume = {126},
pages = {1865-1874},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.076},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918313541},
author = {Maroua Masmoudi and Sana Ben {Abdallah Ben Lamine} and Hajer Baazaoui Zghal and Mohamed Hedi Karray and Bernard Archimede},
keywords = {Ontology, modularity, semantic heterogeneity, interoperability, environmental monitoring},
abstract = {Multi-source observed data are generally characterized by their syntactic, structural and semantic heterogeneities. A key challenge is the semantic interoperability of these data. In this context, we propose an ontology-based system that supports environmental monitoring. Our contributions could be resumed around 1) the construction of an ontology which allows to represent the knowledge and reuse it in a real-world way, 2) the guarantee of the semantic interoperability of ontological modules since the proposed ontology is based on the upper level ontology Basic Formal Ontology (BFO) 3) the modularity of the proposed ontology in order to facilitate its reuse and evolution. The proposed ontology has been implemented and evaluated using quality metrics. We also present a real use case study that demonstrates how the proposed ontology allows implicit knowledge generation.}
}
@article{LEAO2019100499,
title = {Extending WordNet with UFO foundational ontology},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100499},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300162},
author = {Felipe Leão and Kate Revoredo and Fernanda Baião},
keywords = {WordNet, Semantic types, Unified foundational ontology, Ontology learning},
abstract = {WordNet is a large lexical database used by an uncountable number of applications for computational linguistics. Many proposals have attempted to better describe it in a semantic perspective, especially addressing synonymy, taxonomy and mereology properties, which led to very good results in domain-specific applications. A philosophical shift on this semantic description could, however, improve the scope of these results across different domains. In this direction, this work extends WordNet’s semantic knowledge by addressing philosophical meta-properties. Specifically, we apply the notion of Semantic Types to propose mapping rules between the noun synsets of Wordnet and the top-level constructs of a foundational ontology. For this task we have chosen the Unified Foundational Ontology (UFO), which explicitly exposes philosophical meta-properties of concepts in its structure, leading to a well-founded semantically-enriched version of Wordnet. The proposed rules were validated through an experiment over approximately 5,200 sample mappings, obtaining an average accuracy of 93.5% Furthermore, to show its applicability, the proposal was applied to the task of automatically learning a well-founded domain ontology.}
}
@article{FRANCEMENSAH2019100929,
title = {A shared ontology for integrated highway planning},
journal = {Advanced Engineering Informatics},
volume = {41},
pages = {100929},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100929},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618306773},
author = {Jojo France-Mensah and William J. O'Brien},
keywords = {Ontology, Semantic Web, Formalized knowledge, Infrastructure management, Integrated planning},
abstract = {Many highway agencies have several functional groups responsible for planning for safety, maintenance and rehabilitation (M&R), mobility, and other functions. The functional nature of State Highway Agencies (SHAs) can result in a siloed approach to planning. Such efforts are further challenged by functional groups utilizing legacy systems which lack interoperability. In practice, this leads to redundant planning efforts and potential spatial-temporal conflicts in the projects proposed by the different groups over a planning period. There is a need for an integrated approach to planning supported by information systems. However, the existing literature on formalized knowledge representation fails to adequately account for the level of information needed for cross-functional planning of projects scheduled for the same network. Hence, this study presents an ontology for integrating information to support the cross-functional and spatial-temporal planning of highway projects. The Integrated Highway Planning Ontology (IHP-Onto) is a shared representation of knowledge about pavement assets, M&R planning, and inter-project coordination. Sources of the knowledge acquired included expert interviews, a review of nation-wide studies, and previously published ontologies. The implementation phase included a case study demonstration of the ontology by answering relevant competency questions via SPARQL queries. Based on the data-driven evaluation of the ontology, the precision and recall rates obtained were 97% and 92% respectively. Based on the results of the evaluation approaches, IHP-Onto was demonstrated as being sufficient to represent domain knowledge capable of supporting integrated highway planning.}
}
@article{GUTIERREZ2019381,
title = {Developing an ontology schema for enriching and linking digital media assets},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {381-397},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.06.023},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18323859},
author = {Yoan Gutiérrez and David Tomás and Isabel Moreno},
keywords = {Semantic representation, Ontology, Digital media asset, Entertainment industry},
abstract = {The abundance of digital media information coming from different sources, completely redefines approaches to media content production management and distribution for all contexts (i.e. technical, business and operational). Such content includes descriptive information (i.e. metadata) about an asset (e.g. a movie, song or game), as well as playable media (e.g. audio or video files). Metadata is organised following a variety of inconsistent structures and formats that are supplied by various content providers. Some challenges have been addressed in terms of standardising and enriching media assets metadata from a semantic perspective. Well known examples include Europeana and DBpedia. Nevertheless, due to the ongoing variability and evolution of digital contents, constant support and creation of new semantic representations are necessary. This article presents an ontology schema covering the requirements of users (content providers and content consumers) involved in the overall life cycle of a digital media asset, which has been designed and developed for a real scenario. The construction of this schema has been documented and evaluated following a methodology supported by quantitative and qualitative metrics. As part of the tangible results, the following outcomes were produced: (i) an RDF/XML schema available via Zenodo and GitHub; (ii) competence questions used for validation are published at GitHub; (iii) an exemplary ontology repository; and (iv) CRUD (Create, Read, Update and Delete) technologies for managing semantic repositories based on such schema. These results form an active part of the framework of a European project and other ongoing research initiatives.}
}
@article{CHEN2021104332,
title = {PCLiON: An Ontology for Data Standardization and Sharing of Prostate Cancer Associated Lifestyles},
journal = {International Journal of Medical Informatics},
volume = {145},
pages = {104332},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2020.104332},
url = {https://www.sciencedirect.com/science/article/pii/S1386505620306079},
author = {Yalan Chen and Chunjiang Yu and Xingyun Liu and Ting Xi and Guangfei Xu and Yan Sun and Fei Zhu and Bairong Shen},
keywords = {Prostate Cancer, Lifestyle, Risk Factor, Ontology, Data Standardization},
abstract = {Background
Researches on Lifestyle medicine (LM) have emerged in recent years to garner wide attention. Prostate cancer (PCa) could be prevented and treated by positive lifestyles, but the association between lifestyles and PCa is always personalized.
Objectives
In order to solve the heterogeneity and diversity of different data types related to PCa, establish a standardized lifestyle ontology, promote the exchange and sharing of disease lifestyle knowledge, and support text mining and knowledge discovery.
Methods
The overall construction of PCLiON was created in accordance with the principles and methodology of ontology construction. Following the principles of evidence-based medicine, we screened and integrated the lifestyles and their related attributes. Protégé was used to construct and validate the semantic framework. All annotations in PCLiON were based on SNOMED CT, NCI Thesaurus, the Cochrane Library and FooDB, etc. HTML5 and ASP.NET was used to develop the independent Web page platform and corresponding intelligent terminal application. The PCLiON also uploaded to the National Center for Biomedical Ontology BioPortal.
Results
PCLiON integrates 397 lifestyles and lifestyle-related factors associated with PCa, and is the first of its kind for a specific disease. It contains 320 attribute annotations and 11 object attributes. The logical relationship and completeness meet the ontology requirements. Qualitative analysis was carried out for 329 terms in PCLiON, including factors which are protective, risk or associated but functional unclear, etc. PCLiON is publicly available both at http://pcaontology.net/PCaLifeStyleDefault.aspx and https://bioportal.bioontology.org/ontologies/PCALION.
Conclusions
Through the bilingual online platforms, complex lifestyle research data can be transformed into standardized, reliable and responsive knowledge, which can promote the shared-decision making (SDM) on lifestyle intervention and assist patients in lifestyle self-management toward the goal of PCa targeted prevention.}
}
@incollection{OLIVA20191,
title = {Chapter 1 - Ontology-Based Process for Unstructured Medical Report Mapping},
editor = {Nilanjan Dey and Surekha Borra and Amira S. Ashour and Fuqian Shi},
booktitle = {Machine Learning in Bio-Signal Analysis and Diagnostic Imaging},
publisher = {Academic Press},
pages = {1-18},
year = {2019},
isbn = {978-0-12-816086-2},
doi = {https://doi.org/10.1016/B978-0-12-816086-2.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128160862000011},
author = {Jefferson Tales Oliva and Huei Diana Lee and Newton Spolaôr and Claudio Saddy Rodrigues Coy and João José Fagundes and Maria {de Lourdes Setsuko Ayrizono} and Feng Chung Wu},
keywords = {Text mining, Natural language processing, Data mining, Ontologies, Medical reports},
abstract = {Hospitals and clinics store an increasing amount of clinical data, such as medical reports. These reports often describe, in natural language, findings from bio-signals, images, and videos collected during a medical procedure. Data mining can explore reports data to find patterns useful to assist experts’ decision making processes and medical procedure development. However, the content of medical reports is rarely organized into an appropriate format. To tackle this issue, we developed the ontology-based Medical Report Mapping Process to represent the content of unstructured reports into a database format. This chapter applied the ontology-based process to map 3654 unstructured upper gastrointestinal endoscopy reports written in Brazilian Portuguese. As a result, a satisfactory mapping performance was achieved. By comparing this result with previous ones in smaller and simpler sets of reports, this chapter suggests that the ontology-based process performs well in sets with different sizes.}
}
@article{DONALDS2019403,
title = {Toward a cybercrime classification ontology: A knowledge-based approach},
journal = {Computers in Human Behavior},
volume = {92},
pages = {403-418},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.11.039},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218305740},
author = {Charlette Donalds and Kweku-Muata Osei-Bryson},
keywords = {Cybercrime, Ontology, Classification, Knowledge-based approach, Design science},
abstract = {In recent years there has been an increase in cybercrimes and its negative impacts on the lives of individuals, organizations, and governments. It has been argued that a better understanding of cybercrime is a necessary condition to develop appropriate legal and policy responses to cybercrime. While a universally agreed-upon classification scheme would facilitate the development of such understanding and also collaborations, current classification schemes are insufficient, fragmented and often incompatible since each focuses on different perspectives (e.g., role of the computer, attack, attacker's or defender's viewpoint), or uses varying terminologies to refer to the same thing, making consistent cybercrime classifications improbable. In this paper we present and illustrate a new cybercrime ontology that incorporates multiple perspectives and offers a more holistic viewpoint for cybercrime classification than prior works. It should therefore prove to be a more useful tool for cybercrime stakeholders.}
}
@article{HOCKER2020671,
title = {Participatory design for ontologies: a case study of an open science ontology for qualitative coding schemas},
journal = {Aslib Journal of Information Management},
volume = {72},
number = {4},
pages = {671-685},
year = {2020},
issn = {2050-3806},
doi = {https://doi.org/10.1108/AJIM-11-2019-0320},
url = {https://www.sciencedirect.com/science/article/pii/S2050380620000344},
author = {Julian Hocker and Christoph Schindler and Marc Rittberger},
keywords = {Ontology engineering, Participatory design, Digital humanities, Semantic web, Open science, Qualitative research, Coding schemas},
abstract = {Purpose
The open science movement calls for transparent and retraceable research processes. While infrastructures to support these practices in qualitative research are lacking, the design needs to consider different approaches and workflows. The paper bases on the definition of ontologies as shared conceptualizations of knowledge (Borst, 1999). The authors argue that participatory design is a good way to create these shared conceptualizations by giving domain experts and future users a voice in the design process via interviews, workshops and observations.
Design/methodology/approach
This paper presents a novel approach for creating ontologies in the field of open science using participatory design. As a case study the creation of an ontology for qualitative coding schemas is presented. Coding schemas are an important result of qualitative research, and reuse can yield great potential for open science making qualitative research more transparent, enhance sharing of coding schemas and teaching of qualitative methods. The participatory design process consisted of three parts: a requirement analysis using interviews and an observation, a design phase accompanied by interviews and an evaluation phase based on user tests as well as interviews.
Findings
The research showed several positive outcomes due to participatory design: higher commitment of users, mutual learning, high quality feedback and better quality of the ontology. However, there are two obstacles in this approach: First, contradictive answers by the interviewees, which needs to be balanced; second, this approach takes more time due to interview planning and analysis.
Practical implications
The implication of the paper is in the long run to decentralize the design of open science infrastructures and to involve parties affected on several levels.
Originality/value
In ontology design, several methods exist by using user-centered design or participatory design doing workshops. In this paper, the authors outline the potentials for participatory design using mainly interviews in creating an ontology for open science. The authors focus on close contact to researchers in order to build the ontology upon the expert's knowledge.}
}
@article{FATFOUTA2021103344,
title = {An ontology-based knowledge management approach supporting simulation-aided design for car crash simulation in the development phase},
journal = {Computers in Industry},
volume = {125},
pages = {103344},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103344},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305789},
author = {Naouress Fatfouta and Julie {Stal le-Cardinal}},
keywords = {Knowledge management, Ontology, Knowledge model, Knowledge retrieval, Collaboration, Engineering design, Crash simulation},
abstract = {In the automotive industry, the design process is both costly and time-consuming. This research focuses on improving the design process by mainly reducing time while producing more robust vehicles. Vehicle development is based on simulation; thus, the design process is referred to as simulation-aided design. Engineering design is highly collaborative and knowledge intensive. Therefore, knowledge management plays a crucial role in today's global economy and is essential for the competitiveness of companies. However, current research on engineering knowledge management focuses on either the codification or the personalisation approaches of knowledge management. Thus, this paper addresses an integrated and collaborative approach. This paper aims to develop an ontology-based knowledge management approach to support simulation-aided design, specifically car crash simulation. The knowledge management support system is designed to ensure the capture and retrieval of engineering knowledge and to enable collaboration between different stakeholders. An evaluation of the models and technologies used is also undertaken, based on use case scenarios.}
}
@article{STEINMETZ2018169,
title = {Using Ontology and Standard Middleware for integrating IoT based in the Industry 4.0⁎⁎This work was conducted during a scholarship supported by the International Cooperation Program PROBRAL CAPES/DAAD at the University of Muenster. Financed by CAPES Brazilian Federal Agency for Support and Evaluation of Graduate Education within the Ministry of Education of Brazil.},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {10},
pages = {169-174},
year = {2018},
note = {3rd IFAC Conference on Embedded Systems, Computational Intelligence and Telematics in Control CESCIT 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.06.256},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318305767},
author = {Charles Steinmetz and Achim Rettberg and Fabíola Gonçalves C. Ribeiro and Greyce Schroeder and Michel S. Soares and Carlos E. Pereira},
keywords = {Internet of things, semantic models, ontology, industry 4.0, integration},
abstract = {Adoption of semantic technologies in the context of automation systems is growing. Semantic technologies can contribute and intensify the machine-to-machine communication, and it has been allowing each time more the collaboration between human and machines. In this sense, semantic technologies have been contributing to adoption of Cyber Physical Systems in the Industry 4.0. In this context, there are plenty of data being generated that has to be integrated between the system components. However, it is important to provide a manner to integrate these objects in an easy and understandable way for the users. This paper presents how it is possible to map industrial elements into a semantic model in order to support services and, also, to allow the communication of these elements with the physical/real system through an IoT middleware. For this proposal, an ontology has been developed as well as an extension of a consolidate IoT middleware to support using these models. The proposed ideas are being evaluated with some industrial case studies and some of the preliminary results are described in the paper.}
}
@article{BLONDET201826,
title = {An ontology for numerical design of experiments processes},
journal = {Computers in Industry},
volume = {94},
pages = {26-40},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517300970},
author = {Gaëtan Blondet and Julien Le Duigou and Nassim Boudaoud and Benoît Eynard},
keywords = {Ontology, Design of experiments, Simulation data management},
abstract = {Numerical Designs of Experiments (NDoE) are used in a product development process to optimize the product. A NDoE may combine a costly numerical model and numerous experiments. The NDoE process consequently becomes very expensive. However, some methods and algorithms were developed to shorten the NDoE process, as sensitivity analysis, surrogate modelling and adaptive DoE. Because of their complexity, advanced expert knowledge or a long preparation step is required to optimally choose and configure all of these methods, in order to run the most efficient NDoE process. To answer this issue, a knowledge management approach is proposed in this paper. It capitalizes and reuses knowledge about NDoE process. This solution is proposed because of the lack in term of models and standardized processes for this specific NDoE application. An ontology was developed to manage, share and reuse knowledge and enable queries for information retrieval in a database. The database lists every NDoE processes executed. Then, the knowledge is analysed by a decision-support system to help designers to choose the best configuration.}
}
@article{CHEN2019361,
title = {Understanding Granular Aspects of Ontology for Blockchain Databases},
journal = {Procedia Computer Science},
volume = {162},
pages = {361-367},
year = {2019},
note = {7th International Conference on Information Technology and Quantitative Management (ITQM 2019): Information technology and quantitative management based on Artificial Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.296},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919320083},
author = {Zhengxin Chen},
keywords = {Blockchain databases, Granular aspects, Ontology},
abstract = {Blockchain technology first appeared a decade ago and is gaining momentum in recent years. The role of ontology to blockchain technology has drawn much attention from researchers. In this paper, we explore ontology in blockchain technology from a unique perspective: Since granular computing can be applied to ontology, it would be a good idea to explore granular aspects of ontology in blockchain technology. Continuing our previous examinations on granular aspects on databases, in this paper, we study granular aspects of ontology for blockchain databases. We provide our own observations, and analyse implications of recent research work related the nature of blockchain technology. As shown in our discussion, this kind of exploration not only helps a better understanding on the nature of blockchain technology, but could also advance the study of granular computing itself.}
}
@article{CHIANG2022,
title = {Using an Ontology-Based Neural Network and DEA to Discover Deficiencies of Hotel Services},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.306748},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000205},
author = {Tzu-An Chiang and Z. H. Che and Yi-Ling Huang and Chang-You Tsai},
keywords = {Back Propagation Neural Network, Hotel Service, Online Customer Review, Ontology, Performance Evaluation},
abstract = {ABSTRACT
Companies can gain critical real-time insights into customer requirements and service evaluation by mining social media. To acquire the service performance and improve the service deficiencies for hotels, this research proposes a benchmark-based performance evaluation model for hotel service to enable hotel managers to assess the service performance. In the case of non-benchmark service hotels, the identification and improvement model for non-benchmark criteria can recognize and analyze the required quantities of performance improvements for non-benchmark criteria. For understanding the causes of service deficiencies, this research mines the online posts and creates a hierarchical ontology of service deficiencies for hotels. A hierarchical ontology-based neural network is proposed to automatically identify the causes of service deficiencies. This study employs an online forum as a case to achieve the identification accuracy of causes of service deficiencies of 92.68%. The analytical result can demonstrate the significant effectiveness and practical value of the proposed methodology.}
}
@article{ZHENG2025106301,
title = {Semantic digital twin framework for monitoring construction workflows},
journal = {Automation in Construction},
volume = {176},
pages = {106301},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106301},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525003413},
author = {Yuan Zheng and Alaa {Al Barazi} and Olli Seppänen and Hisham Abou-Ibrahim and Christopher Görsch},
keywords = {Ontology, Digital twin (DT), Data modeling, Simulation, Construction workflow},
abstract = {As construction workflows become increasingly dynamic, there is a growing need for Digital Twins (DTs) to support integrated, real-time workflow monitoring. However, establishing DTs in construction remains challenging due to fragmented data sources and the lack of systematic semantic integration methods. This paper investigates how semantic web ontologies can be systematically applied to establish a semantic DT for monitoring construction workflows. Accordingly, a DT framework (DiCon-DT) is proposed, utilizing an ontology network to model and integrate diverse data into a semantic DT data lake, and further enabling simulation and contextual interpretation. Validated through a furniture installation case study, the framework successfully enabled semantic data integration and supported predictive and cognitive tasks for construction monitoring. Future research should focus on extending the ontology network, automating semantic data mapping, and validating the framework at larger complex project scales.}
}
@article{ROCHA2018373,
title = {DKDOnto: An Ontology to Support Software Development with Distributed Teams},
journal = {Procedia Computer Science},
volume = {126},
pages = {373-382},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.271},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831247X},
author = {Rodrigo Rocha and Arthur Araújo and Diogo Cordeiro and Assuero Ximenes and Jean Teixeira and Gabriel Silva and Daliton da Silva and Diogo Espinhara and Renan Fernandes and João Ambrosio and Marcos Duarte and Ryan Azevedo},
keywords = {Distributed Software Development, Ontology, Software Engineering},
abstract = {The Distributed Software Development has become an option for software companies to expand their perspective and work with dispersed teams, exploiting the advantages brought by this approach. However, this way of developing software enables new challenges to arise, such as the inexistence of a formal, normalized model of a project’s data and artifacts accessible to all the individuals involved, which makes it harder for them to communicate, understand each other and what is specified on the project’s artifacts. This paper proposes a knowledge base called DKDOnto, a domain-specific ontology for distributed development, aiming to help projects with a common vocabulary, allowing to assist better the distributed software development process.}
}
@incollection{DRAGONI20217,
title = {Chapter 2 - Convology: an ontology for conversational agents in digital health},
editor = {Sarika Jain and Vishal Jain and Valentina Emilia Balas},
booktitle = {Web Semantics},
publisher = {Academic Press},
pages = {7-21},
year = {2021},
isbn = {978-0-12-822468-7},
doi = {https://doi.org/10.1016/B978-0-12-822468-7.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128224687000043},
author = {Mauro Dragoni and Giuseppe Rizzo and Matteo A. Senese},
keywords = {Science, publication, complicated},
abstract = {Conversational agents are a modality for making the human–computer interaction paradigm more friendly from the user perspective. Conversational agents rely on natural language understanding capabilities for classifying the intents that users want to communicate through open natural language text. Recently, conversational agents are equipped with background knowledge for improving the overall effectiveness, efficiency, and reliability of systems concerning the acquisition of information from the dialog management perspective. However, while the literature discussed some introductory strategies, there are no evidence that such knowledge-equipped conversational agents have been used in practice. Within the digital health domain, the use of conversational agents ranges from assisting patients during the self-management of chronic diseases to supporting physicians during daily activities. In this chapter, we propose an ontology, namely Convology, aiming to describe conversational scenarios with the scope of providing a tool that, once deployed into a real-world application, allows to ease the management and understanding of the entire dialog workflow between users, physicians, and systems. We integrated Convology into a living laboratory concerning the adoption of conversational agents for supporting the self-management of patients affected by asthma. Observer results demonstrated the feasibility of investigating this research direction.}
}
@article{ZHENG2021104,
title = {A knowledge graph method for hazardous chemical management: Ontology design and entity identification},
journal = {Neurocomputing},
volume = {430},
pages = {104-111},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.10.095},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220317082},
author = {Xue Zheng and Bing Wang and Yunmeng Zhao and Shuai Mao and Yang Tang},
keywords = {Knowledge graph, Ontology, Hazardous chemicals management, Named entity recognition},
abstract = {Hazardous chemicals are widely used in the production activities of the chemical industry. The risk management of hazardous chemicals is critical to the safety of life and property. Hence, the effective risk management of hazardous chemicals has always been important to the chemical industry. Since a large quantity of knowledge and information of hazardous chemicals is stored in isolated databases, it is challenging to manage hazardous chemicals in an information-rich manner. Herein, we prompt a knowledge graph to overcome the information gap between decentralized databases, which would improve the hazardous chemical management. In the implementation of the knowledge graph, we design an ontology schema of hazardous chemicals management. To facilitate enterprises to master the knowledge in the full lifecycle of hazardous chemicals, including production, transportation, storage, etc., we jointly use data from companies and open data from the public domain of hazardous chemicals to construct the knowledge graph. The named entity recognition task is one of the key tasks in the implementation of the knowledge graph, which is of great significance for extracting entity information from unstructured data, namely the hazardous chemical accidents records. To extract useful information from multi-source data, we adopt the pre-trained BERT-CRF model to conduct named entity recognition for incidents records. The model achieves good results, exhibiting the effectiveness in the task of named entity recognition in the chemical industry.}
}
@article{ILIADIS20191021,
title = {The Tower of Babel problem: making data make sense with Basic Formal Ontology},
journal = {Online Information Review},
volume = {43},
number = {6},
pages = {1021-1045},
year = {2019},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-07-2018-0210},
url = {https://www.sciencedirect.com/science/article/pii/S1468452719000325},
author = {Andrew Iliadis},
keywords = {Data ethics, Applied computational ontology, Semantic technology, Social ontology, Tower of Babel problem},
abstract = {Purpose
Applied computational ontologies (ACOs) are increasingly used in data science domains to produce semantic enhancement and interoperability among divergent data. The purpose of this paper is to propose and implement a methodology for researching the sociotechnical dimensions of data-driven ontology work, and to show how applied ontologies are communicatively constituted with ethical implications.
Design/methodology/approach
The underlying idea is to use a data assemblage approach for studying ACOs and the methods they use to add semantic complexity to digital data. The author uses a mixed methods approach, providing an analysis of the widely used Basic Formal Ontology (BFO) through digital methods and visualizations, and presents historical research alongside unstructured interview data with leading experts in BFO development.
Findings
The author found that ACOs are products of communal deliberation and decision making across institutions. While ACOs are beneficial for facilitating semantic data interoperability, ACOs may produce unintended effects when semantically enhancing data about social entities and relations. ACOs can have potentially negative consequences for data subjects. Further critical work is needed for understanding how ACOs are applied in contexts like the semantic web, digital platforms, and topic domains. ACOs do not merely reflect social reality through data but are active actors in the social shaping of data.
Originality/value
The paper presents a new approach for studying ACOs, the social impact of ACO work, and describes methods that may be used to produce further applied ontology studies.}
}
@article{GAWICH2019341,
title = {Ontology Maintenance System for Rheumatoid Disease},
journal = {Procedia Computer Science},
volume = {154},
pages = {341-346},
year = {2019},
note = {Proceedings of the 9th International Conference of Information and Communication Technology [ICICT-2019] Nanning, Guangxi, China January 11-13, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.06.049},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930818X},
author = {Mariam Gawich and Marco Alfonse and Mostafa Aref and Abdel-Badeeh M. Salem},
keywords = {Ontology engineering, Ontology maintenance, Ontology evolution, Ontology pruning, Medical Ontology},
abstract = {The constructed medical ontologies need to be updated in order to reflect the changes occurred on the medicine such as the clinical findings, treatments and their side effects. Various researchers defined the ontology maintenance as the process of updating the ontology or the evolution of the ontology. Other researches consider the ontology maintenance as a composed process that involves the ontology evolution and pruning. This paper presents a Rheumatoid ontology maintenance system that incorporates both of the ontology evolution and the ontology pruning. The evolution is executed in a way that ensures the ontology consistency and its relevance to the domain of interest.}
}
@incollection{PUJAN20201963,
title = {Modelling Ontologies for Biorefinery Processes - A Case Study},
editor = {Sauro Pierucci and Flavio Manenti and Giulia Luisa Bozzano and Davide Manca},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {48},
pages = {1963-1968},
year = {2020},
booktitle = {30th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-823377-1.50283-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128233771502834},
author = {Robert Pujan and Roy Nitzsche and Jakob Köchermann and Heinz A. Preisig},
keywords = {processmodelling, ontology, topology, biorefinery},
abstract = {This study introduces the systematic modelling approach of “visual modelling” to biorefinery processes by applying it to a concept from the demonstration project KomBiChemPRO. The process consists of a hydrothermal conversion of hemicellulose-derived oligo sugars into monomeric C5 sugars, combined with purification steps of adsorption and nanofiltration. The experimental studies determine on model effluents hydrothermal conversion kinetics, adsorption isotherms and kinetics, as well as separation efficiencies of the nanofiltration. The paper shows an efficient graphical representation of the process topology, which is combined with an equation ontology.}
}
@article{FLAHARTY20241819,
title = {Evaluating large language models on medical, lay-language, and self-reported descriptions of genetic conditions},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {9},
pages = {1819-1833},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2024.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0002929724002556},
author = {Kendall A. Flaharty and Ping Hu and Suzanna Ledgister Hanchard and Molly E. Ripper and Dat Duong and Rebekah L. Waikel and Benjamin D. Solomon},
keywords = {large language model, deep learning, in-context prompting, artificial intelligence, machine learning, medical genetics, medical genomics},
abstract = {Summary
Large language models (LLMs) are generating interest in medical settings. For example, LLMs can respond coherently to medical queries by providing plausible differential diagnoses based on clinical notes. However, there are many questions to explore, such as evaluating differences between open- and closed-source LLMs as well as LLM performance on queries from both medical and non-medical users. In this study, we assessed multiple LLMs, including Llama-2-chat, Vicuna, Medllama2, Bard/Gemini, Claude, ChatGPT3.5, and ChatGPT-4, as well as non-LLM approaches (Google search and Phenomizer) regarding their ability to identify genetic conditions from textbook-like clinician questions and their corresponding layperson translations related to 63 genetic conditions. For open-source LLMs, larger models were more accurate than smaller LLMs: 7b, 13b, and larger than 33b parameter models obtained accuracy ranges from 21%–49%, 41%–51%, and 54%–68%, respectively. Closed-source LLMs outperformed open-source LLMs, with ChatGPT-4 performing best (89%–90%). Three of 11 LLMs and Google search had significant performance gaps between clinician and layperson prompts. We also evaluated how in-context prompting and keyword removal affected open-source LLM performance. Models were provided with 2 types of in-context prompts: list-type prompts, which improved LLM performance, and definition-type prompts, which did not. We further analyzed removal of rare terms from descriptions, which decreased accuracy for 5 of 7 evaluated LLMs. Finally, we observed much lower performance with real individuals’ descriptions; LLMs answered these questions with a maximum 21% accuracy.}
}
@incollection{JIN201845,
title = {Chapter 4 - Ontology-Oriented Interactive Environment Modeling},
editor = {Zhi Jin},
booktitle = {Environment Modeling-Based Requirements Engineering for Software Intensive Systems},
publisher = {Morgan Kaufmann},
address = {Oxford},
pages = {45-67},
year = {2018},
isbn = {978-0-12-801954-2},
doi = {https://doi.org/10.1016/B978-0-12-801954-2.00004-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019542000042},
author = {Zhi Jin},
keywords = {Attribute, Domain modeling, Environment entity modeling, Environment ontology},
abstract = {This chapter mainly introduces the representation and construction of the environment ontology. For environment ontology, the main extension to the normal ontology structure is about the state machine–based behavior representation of causal entities. The chapter is devoted to presenting techniques for building domain environment ontologies, i.e., application domain-dependent ontologies, for the purpose of specifying environment modeling system capabilities.}
}
@incollection{SERNA2022847,
title = {Application of an ontology-based decision support system for the design of emulsion-based cosmetic products},
editor = {Ludovic Montastruc and Stephane Negny},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {51},
pages = {847-852},
year = {2022},
booktitle = {32nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-95879-0.50142-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323958790501429},
author = {Juliana Serna and Jose L. Rivera-Gil and Alex Gabriel and Javier A. Arrieta-Escobar and Vincent Boly and Véronique Falk and Paulo C. Narváez-Rincón},
keywords = {Product design, Cosmetic emulsions, Ontology},
abstract = {The decision-making process for the design of formulated products faces different challenges because of its intrinsic complexity. On the one hand, it is not sequential, but iterative due to the fragmented and heterogeneous nature of available information. On the other hand, there is not a unique design workflow because it changes from company to company according to its context and specific requirements. The lack of structure of knowledge for product formulation requires developing a robust knowledge representation to show coherently and explicitly concepts, models, and data. Furthermore, this representation must allow design teams to use it flexibly and to adapt it to specific design contexts. In view of the above, this work proposes an ontology for formulated products with emphasis on cosmetic emulsions. This ontology integrates concepts from emulsion science, cosmetic formulation, expert knowledge, and design heuristics in a systematic and accessible way. It was done based on the recent work of our research group in Chemical Product Design. This document shows an overview of the ontology and one of its possible applications: verification of the formulation of a skin care cream. As a conclusion, it was found that the ontology enables the access to precise information according to design requirements. It is a versatile and useful information tool for the design of emulsion-based products.}
}
@article{ZHOU2021101239,
title = {Semantic information alignment of BIMs to computer-interpretable regulations using ontologies and deep learning},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101239},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101239},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620302081},
author = {Peng Zhou and Nora El-Gohary},
keywords = {Building information modeling, Semantic information alignment, Information extraction, Ontology, Deep learning, Automated compliance checking},
abstract = {A semantic information alignment method is proposed to align the representations used in building information models (BIMs) to the representations used in energy regulations. Compared to existing alignment efforts, which are either manual or semi-automated, the proposed method aims to automate the alignment process for supporting fully automated energy compliance checking. A first-level simple alignment method is proposed to align single design information instances to single regulatory concepts, in which (1) domain knowledge is used for interpreting the meaning of concepts to recognize candidate instances, and (2) deep learning is used for capturing the semantics behind the words to measure semantic similarity and select the matches. A final complex alignment method is proposed to recognize the instance groups belonging to a regulatory requirement, in which (1) supervised and unsupervised searching algorithms are used to identify the instance pairs, and (2) network modeling is used to group and link the instance pairs to the requirement. The proposed method showed 93.4% recall and 94.7% precision on the testing data.}
}
@incollection{GALITSKY2022365,
title = {Chapter 11 - Building medical ontologies relying on communicative discourse trees},
editor = {Boris Galitsky and Saveli Goldberg},
booktitle = {Artificial Intelligence for Healthcare Applications and Management},
publisher = {Academic Press},
pages = {365-414},
year = {2022},
isbn = {978-0-12-824521-7},
doi = {https://doi.org/10.1016/B978-0-12-824521-7.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128245217000016},
author = {Boris Galitsky and Dmitry Ilvovsky},
keywords = {Discourse analysis, Ontology construction, Extracting ontology entries from text, Satellites and nuclei of rhetorical relations, Ontology and search relevance},
abstract = {In this chapter, we explore the role of discourse analysis in ontology construction. Extracting candidate phrases to form ontology entries from text, it is important to pay attention to which discourse units these phrases occur in. It turns out that not all discourse units are equal in terms of their contribution to forming ontology entries; satellites are usually good, and nuclei are not. We survey text mining (TM) and ontology information extraction (IE) techniques in the medical domain and select the ones where advanced linguistic analysis including discourse processing is leveraged the most to produce a robust and efficient ontology. We evaluate the consistency of the resultant ontology and its role in assuring high search relevance.}
}
@article{CUBRIC2020109,
title = {Design and evaluation of an ontology-based tool for generating multiple-choice questions},
journal = {Interactive Technology and Smart Education},
volume = {17},
number = {2},
pages = {109-131},
year = {2020},
issn = {1741-5659},
doi = {https://doi.org/10.1108/ITSE-05-2019-0023},
url = {https://www.sciencedirect.com/science/article/pii/S174156592000012X},
author = {Marija Cubric and Milorad Tosic},
keywords = {Computer-assisted assessment, Design-science research, Multiple-choice question, Ontologies, Automatic question generation},
abstract = {Purpose
The recent rise in online knowledge repositories and use of formalism for structuring knowledge, such as ontologies, has provided necessary conditions for the emergence of tools for generating knowledge assessment. These tools can be used in a context of interactive computer-assisted assessment (CAA) to provide a cost-effective solution for prompt feedback and increased learner’s engagement. The purpose of this paper is to describe and evaluate a tool developed by the authors, which generates test questions from an arbitrary domain ontology, based on sound pedagogical principles encapsulated in Bloom’s taxonomy.
Design/methodology/approach
This paper uses design science as a framework for presenting the research. A total of 5,230 questions were generated from 90 different ontologies and 81 randomly selected questions were evaluated by 8 CAA experts. Data were analysed using descriptive statistics and Kruskal–Wallis test for non-parametric analysis of variance.
Findings
In total, 69 per cent of generated questions were found to be useable for tests and 33 per cent to be of medium to high difficulty. Significant differences in quality of generated questions were found across different ontologies, strategies for generating distractors and Bloom’s question levels: the questions testing application of knowledge and the questions using semantic strategies were perceived to be of the highest quality.
Originality/value
The paper extends the current work in the area of automated test generation in three important directions: it introduces an open-source, web-based tool available to other researchers for experimentation purposes; it recommends practical guidelines for development of similar tools; and it proposes a set of criteria and standard format for future evaluation of similar systems.}
}
@article{MA20181,
title = {Ontology- and freeware-based platform for rapid development of BIM applications with reasoning support},
journal = {Automation in Construction},
volume = {90},
pages = {1-8},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517305885},
author = {Zhiliang Ma and Zhe Liu},
keywords = {BIM (Building Information Modeling), Reasoning, Rapid development platform, Ontology, Freeware},
abstract = {In the Architecture, Engineering and Construction (AEC) area, a prominent tendency is to use Building Information Modeling (BIM) data to perform analyses and calculations based on specified rules in regulations or standards so that BIM applications with reasoning support (BIM-R applications) are necessary. The current method is to develop BIM-R applications separately and represent the rules by coding or using proprietary formats, which has the problems of cost and efficiency. To solve these problems, a new method is proposed: to use a platform to rapidly develop BIM-R applications (BIM-R platform) based on ontology and freeware components. Thus, a BIM-R platform must be developed. This study establishes: 1) the functional requirements of the BIM-R platform, 2) the mechanism to transform BIM data into ontology data, and 3) the architecture of the BIM-R platform. This study also selects freeware components to develop the BIM-R platform. A BIM-R platform is implemented accordingly and applied to develop a prototype BIM-R application for the as-bid cost estimation of buildings for illustration. It is concluded that the proposed platform can help reduce the cost and improve the efficiency for the development of BIM-R applications and can be used by both researchers and developers.}
}
@article{TANG2018847,
title = {Exchanging knowledge for test-based diagnosis using OWL Ontologies and SWRL Rules},
journal = {Procedia Computer Science},
volume = {131},
pages = {847-854},
year = {2018},
note = {Recent Advancement in Information and Communication Technology:},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.279},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918306598},
author = {Xilang Tang and Mingqing Xiao and Bin Hu and Dongqing Pan},
keywords = {diagnosis, test, knowledge, OWL ontology, SWRL rules},
abstract = {To solve the difficulty of exchanging knowledge of complex engineering systems for diagnosis, which resulting in huge work and long cycle for developing test-based diagnosis equipment, this paper proposes a novel method of exchanging knowledge of systems under diagnosis using OWL ontologies and SWRL rules. The knowledge model of systems for diagnosis is decomposed into structure model and function model, and this paper proposes a general procedure to represent the two models with OWL and SWRL. For that OWL and SWRL support reasoning, so that a generic reasoning engine for test-based diagnosis can be developed and the knowledge can be used directly by the reasoning engine. Therefore, the software of ADE can be generic and portable, so that the development process of diagnosis equipment can be accelerated.}
}
@article{ERIKSSON2025102404,
title = {Turning Conceptual Modeling Institutional – The prescriptive role of conceptual models in transforming institutional reality},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102404},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102404},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24001289},
author = {Owen Eriksson and Paul Johannesson and Maria Bergholtz and Pär Ågerfalk},
keywords = {Institution, Digital infrastructure, Institutional language, Ontological reversal, Institutional turn, Digital agency, Digital transformation},
abstract = {It has traditionally been assumed that information systems describe physical reality. However, this assumption is becoming obsolete as digital infrastructures are increasingly part of real-world experiences. Digital infrastructures (ubiquitous and scalable information systems) no longer merely map physical reality representations onto digital objects but increasingly assume an active role in creating, shaping, and governing physical reality. We currently witness an “ontological reversal”, where conceptual models and digital infrastructures change physical reality. Still, the fundamental assumption remains that physical reality is the only real world. However, to fully embrace the implications of the ontological reversal, conceptual modeling needs an “institutional turn” that abandons the idea that physical reality always takes priority. Institutional reality, which includes, for example, institutional entities such as organizations, contracts, and payment transactions, is not simply part of physical reality detached from digital infrastructures. Digital infrastructures are part of institutional reality. Accordingly, the research question we address is: What are the fundamental constructs in the design of digital infrastructures that constitute and transform institutional reality? In answering this question, we develop a foundation for conceptual modeling, which we illustrate by modeling the institution of open banking and its associated digital infrastructure. In the article, we identify digital institutional entities, digital agents regulated by software, and digital institutional actions as critical constructs for modeling digital infrastructures in institutional contexts. In so doing, we show how conceptual modeling can improve our understanding of the digital transformation of institutional reality and the prescriptive role of conceptual modeling. We also generate theoretical insights about the need for legitimacy and liability that advance the study and practice of digital infrastructure design and its consequences.}
}
@article{GUPTA2021101260,
title = {Feature-based ontological framework for semantic interoperability in product development},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101260},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101260},
url = {https://www.sciencedirect.com/science/article/pii/S147403462100015X},
author = {Ravi Kumar Gupta and Balan Gurumoorthy},
keywords = {Product information exchange, Semantic interoperability, Shape feature taxonomy, Feature semantics, Product informatics, Computer-aided design, Product lifecycle management, Product development},
abstract = {An essential requirement in integrating tasks in product development is to have a seamless exchange of product information through the entire product lifecycle. A key challenge in the integration is the exchange of shape semantics in terms of understandable labels and representations. A unified taxonomy is proposed to represent, classify, and extract shape features. This taxonomy is built using the Domain-Independent Form Feature (DIFF) model as the representation of features. All the shape features in a product model are classified under three main classes, namely, volumetric features, deformation features and free-form surface features. Shape feature ontology is developed using the unified taxonomy, which brings the shape features under a single reasoning framework. One-to-many reasoning framework is presented for mapping semantically equivalent information (label and representation) of the feature to be exchanged to target applications, and the reconstruction of the shape model automatically in that target application. An algorithm has been developed to extract the semantics of shape features and construct the model in the target application. The algorithm developed has been tested for shape models taken from literature and test cases are selected based on variations of topology and geometry. Results of exchanging product information are presented and discussed. Finally, the limitations of the proposed method for exchanging product information are explained.}
}
@article{FARAZI2020106813,
title = {Linking reaction mechanisms and quantum chemistry: An ontological approach},
journal = {Computers & Chemical Engineering},
volume = {137},
pages = {106813},
year = {2020},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2020.106813},
url = {https://www.sciencedirect.com/science/article/pii/S0098135419310038},
author = {Feroz Farazi and Nenad B. Krdzavac and Jethro Akroyd and Sebastian Mosbach and Angiras Menon and Daniel Nurkowski and Markus Kraft},
keywords = {Chemical kinetic reaction mechanism, Quantum chemistry, Linked data, Chemical ontology, Thermodynamic data, Knowledge-graph, J-Park Simulator},
abstract = {In this paper, a linked-data framework for connecting species in chemical kinetic reaction mechanisms with quantum calculations is presented. A mechanism can be constructed from thermodynamic, reaction rate, and transport data that has been obtained either experimentally, computationally, or by a combination of both. This process in practice requires multiple sources of data, which raises, inter alia, species naming and data inconsistency issues. A linked data-centric knowledge-graph approach is taken in this work to address these challenges. In order to implement this approach, two existing ontologies, namely OntoKin, for representing chemical kinetic reaction mechanisms, and OntoCompChem, for representing quantum chemistry calculations, are extended. In addition, a new ontology, which we call OntoSpecies, is developed for uniquely representing chemical species. The framework also includes agents to populate and link knowledge-bases created through the instantiation of these ontologies. In addition, the developed knowledge-graph and agents naturally form a part of the J-Park Simulator (JPS) – an Industry 4.0 platform which combines linked data and an eco-system of autonomous agents for cross-domain applications. The functionality of the framework is demonstrated via a use-case based on a hydrogen combustion mechanism.}
}
@article{KARDINATA2019826,
title = {Integration of Crowdsourcing into Ontology Relation Extraction},
journal = {Procedia Computer Science},
volume = {161},
pages = {826-833},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.189},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319003},
author = {Eunike Andriani Kardinata and Nur Aini Rakhmawati},
keywords = {crowdsourcing, integration, online incremental, ontology learning, relation extraction},
abstract = {Ontology learning is a continuous process that is always being researched and developed. A learning method for one domain may not be applicable to another because of the different characteristics of the data involved. Researchers have been developing various methodologies to build the highest quality of ontology efficiently. As identified in the previous works, one problem which could not be solved my machine alone is the extra-logical errors. These errors can only be identified by human judges and are usually related to the domain of the ontology. In this research, we aim to catalogue available methods, specifically for relation extraction, and the online incremental algorithms which will allow integration of crowdsourcing into ontology learning—to handle said challenge. We also briefly discussed an existing ontology editor called OntoCop, which may be used as a reference for further research. Henceforth, we propose a framework based on our review to improve the current relation extraction method.}
}
@article{CUENCA2020100550,
title = {DABGEO: A reusable and usable global energy ontology for the energy domain},
journal = {Journal of Web Semantics},
volume = {61-62},
pages = {100550},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100550},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300020},
author = {Javier Cuenca and Felix Larrinaga and Edward Curry},
keywords = {Ontology, Energy domain, Ontology reusability, Ontology usability},
abstract = {The heterogeneity of energy ontologies hinders the interoperability between ontology-based energy management applications to perform a large-scale energy management. Thus, there is the need for a global ontology that provides common vocabularies to represent the energy subdomains. A global energy ontology must provide a balance of reusability–usability to moderate the effort required to reuse it in different applications. This paper presents DABGEO: a reusable and usable global ontology for the energy domain that provides a common representation of energy domains represented by existing energy ontologies. DABGEO can be reused by ontology engineers to develop ontologies for specific energy management applications. In contrast to previous global energy ontologies, it follows a layered structure to provide a balance of reusability–usability. In this work, we provide an overview of the structure of DABGEO and we explain how to reuse it in a particular application case. In addition, the paper includes an evaluation of DABGEO to demonstrate that it provides a balance of reusability–usability.}
}
@article{CHI2019180,
title = {Developing base domain ontology from a reference collection to aid information retrieval},
journal = {Automation in Construction},
volume = {100},
pages = {180-189},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517310713},
author = {Nai-Wen Chi and Yu-Huei Jin and Shang-Hsien Hsieh},
keywords = {Ontology, Information retrieval, Earthquake engineering},
abstract = {Information Retrieval (IR) is a common technique used to manage a growing technical document collection. Owing to the complexity of technical documents, the direct application of IR often leads to unsatisfactory results. Therefore, many semantic approaches, such as ontology, are applied to enhance IR performance. However, ontology development is often a labor-intensive process and the availability of ontologies significantly influences the applicability of IR. Consequently, many efforts are dedicated to the automation of the ontology development process for reducing the human labors. In addition, reference collections, which are developed as the golden standards to evaluate IR performance in many IR research, can be regarded as an available resource for ontology development. To ease domain ontology development for supporting IR, this research proposes a semi-automated approach to develop a base domain ontology from a reference collection. This research also validates the base ontology on an Earthquake Engineering reference collection, called the NCREE (National Center for Research on Earthquake Engineering) collection. The results reveal that the human workload of the proposed approach is affordable. Furthermore, the base domain ontology can help achieve a satisfactory IR performance.}
}
@article{WALOSZEK2020733,
title = {Improving the Performance of Ontological Querying by using a Contextual Approach},
journal = {Procedia Computer Science},
volume = {176},
pages = {733-742},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.062},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319578},
author = {Wojciech Waloszek and Aleksander Waloszek},
keywords = {knowledge bases, contexts, Description Logics, reasoning},
abstract = {In the paper we present the results of experiment we performed to determine whether a contextual approach may be used to increase the performance of querying a knowledge base. For the experiments we have used a unique setting where we put much effort in developing a contextual and a non-contextual ontology which are as much close counterparts as possible. To achieve this we created a contextual version of a non-contextual ontology and reformulated the set of competency questions to reflect the contextual structure of the newly created knowledge base. The results of the experiment strongly suggest that using contexts might be advantageous for improving performance, and also show the further ways of development of the approach.}
}
@article{DASILVASERAPIAOLEAL2019100100,
title = {An ontology for interoperability assessment: A systemic approach},
journal = {Journal of Industrial Information Integration},
volume = {16},
pages = {100100},
year = {2019},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X19300433},
author = {Gabriel {da Silva Serapião Leal} and Wided Guédria and Hervé Panetto},
keywords = {Enterprise interoperability, Interoperability assessment, Ontology, System engineering},
abstract = {Enterprise Interoperability is a requirement for ensuring an effective collaboration within a network of enterprises. Therefore, interoperability should be continuously assessed and improved for avoiding collaboration issues. To do so, an interoperability assessment can be performed by the concerned enterprises. Such an assessment provides an overview of the enterprise systems’ strengths and weaknesses regarding interoperability. A plethora of assessment approaches are proposed in the literature. The majority of them focus on one single aspect of interoperability. In general, to have a holistic view of the assessed systems, i.e. consider different aspects, enterprises have to apply different approaches. However, the application of multiple approaches may cause redundancy and confusion when assessing the same system using different metrics and viewpoints. Therefore, this article is to propose an ontology for interoperability assessment. The main objective of such an ontology is to provide a sound description of all relevant concepts and relationships regarding an interoperability assessment. Inference rules are also provided for reasoning on interoperability problems. A case study based on a real enterprise in presented to evaluate the proposed ontology.}
}
@article{TEBES2020106298,
title = {Analyzing and documenting the systematic review results of software testing ontologies},
journal = {Information and Software Technology},
volume = {123},
pages = {106298},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106298},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300495},
author = {Guido Tebes and Denis Peppino and Pablo Becker and Gerardo Matturro and Martin Solari and Luis Olsina},
keywords = {Software testing ontology, Systematic literature review, Systematic literature review process, Secondary study, Analysis, Testing strategy},
abstract = {Context
Software testing is a complex area since it has a large number of specific methods, processes and strategies, involving a lot of domain concepts. Therefore, it would be valuable to have a conceptualized software testing ontology that explicitly and unambiguously defines the concepts. Consequently, it is important to find out the available evidence in the literature on primary studies for software testing ontologies. In particular, we are looking for research that has a rich ontological coverage that includes Non-Functional Requirements (NFRs) and Functional Requirements (FRs) concepts in conjunction with static and dynamic testing concepts, which can be used in method and process specifications for a family of testing strategies.
Objective
The main goal for this secondary study is to identify, evaluate and synthesize the available primary studies on conceptualized software testing ontologies.
Method
To conduct this study, we use the Systematic Literature Review (SLR) approach, which follows our enhanced SLR process. We set three research questions. Additionally, to quantitatively evaluate the quality of the selected conceptualized ontologies, we designed a NFRs tree and its associated metrics and indicators.
Results
We obtained 12 primary studies documenting conceptualized testing ontologies by using three different retrieval methods. In general, we noted that most of them have a lack of NFRs and static testing terminological coverage. Finally, we observe that none of them is directly linked with FRs and NFRs conceptual components.
Conclusion
A general benefit of having the suitable software testing ontology is to minimize the current heterogeneity, ambiguity and incompleteness problems in terms, properties and relationships. We have confirmed that exists heterogeneity, ambiguity, and incompleteness for concepts dealing with testing artifacts, roles, activities, and methods. Moreover, we did not find the suitable ontology for our aim since none of the conceptualized ontologies are directly linked with NFRs and FRs components.}
}
@article{CONG2025101700,
title = {Demystifying large language models in second language development research},
journal = {Computer Speech & Language},
volume = {89},
pages = {101700},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101700},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000834},
author = {Yan Cong},
keywords = {Large language models, Natural language processing, Automatic essay scoring, L2 writing development, L2 interlanguage, Bilingualism},
abstract = {Evaluating students' textual response is a common and critical task in language research and education practice. However, manual assessment can be tedious and may lack consistency, posing challenges for both scientific discovery and frontline teaching. Leveraging state-of-the-art large language models (LLMs), we aim to define and operationalize LLM-Surprisal, a numeric representation of the interplay between lexical diversity and syntactic complexity, and to empirically and theoretically demonstrate its relevance for automatic writing assessment and Chinese L2 (second language) learners’ English writing development. We developed an LLM-based natural language processing pipeline that can automatically compute text Surprisal scores. By comparing Surprisal metrics with the widely used classic indices in L2 studies, we extended the usage of computational metrics in Chinese learners’ L2 English writing. Our analyses suggested that LLM-Surprisals can distinguish L2 from L1 (first language) writing, index L2 development stages, and predict scores provided by human professionals. This indicated that the Surprisal dimension may manifest itself as critical aspects in L2 development. The relative advantages and disadvantages of these approaches were discussed in depth. We concluded that LLMs are promising tools that can enhance L2 research. Our showcase paves the way for more nuanced approaches to computationally assessing and understanding L2 development. Our pipelines and findings will inspire language teachers, learners, and researchers to operationalize LLMs in an innovative and accessible manner.}
}
@article{STEVENS2019100469,
title = {Measuring expert performance at manually classifying domain entities under upper ontology classes},
journal = {Journal of Web Semantics},
volume = {57},
pages = {100469},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S157082681830043X},
author = {Robert Stevens and Phillip Lord and James Malone and Nicolas Matentzoglu},
keywords = {OWL, Ontologies, Upper ontologies, Ontology engineering, Empirical study},
abstract = {Background:
Classifying entities in domain ontologies under upper ontology classes is a recommended task in ontology engineering to facilitate semantic interoperability and modelling consistency. Integrating upper ontologies this way is difficult and, despite emerging automated methods, remains a largely manual task.
Problem:
Little is known about how well experts perform at upper ontology integration. To develop methodological and tool support, we first need to understand how well experts do this task. We designed a study to measure the performance of human experts at manually classifying classes in a general knowledge domain ontology with entities in the Basic Formal Ontology (BFO), an upper ontology used widely in the biomedical domain.
Method:
We recruited 8 BFO experts and asked them to classify 46 commonly known entities from the domain of travel with BFO entities. The tasks were delivered as part of a web survey.
Results:
We find that, even for a well understood general knowledge domain such as travel, the results of the manual classification tasks are highly inconsistent: the mean agreement of the participants with the classification decisions of an expert panel was only 51%, and the inter-rater agreement using Fleiss’ Kappa was merely moderate (0.52). We further follow up on the conjecture that the degree of classification consistency is correlated with the frequency the respective BFO classes are used in practice and find that this is only true to a moderate degree (0.52, Pearson).
Conclusions:
We conclude that manually classifying domain entities under upper ontology classes is indeed very difficult to do correctly. Given the importance of the task and the high degree of inconsistent classifications we encountered, we further conclude that it is necessary to improve the methodological framework surrounding the manual integration of domain and upper ontologies.}
}
@incollection{GREENBERG2024265,
title = {13 - A schema for harms-sensitive reasoning, and an approach to populate its ontology by human annotation},
editor = {Prithviraj Dasgupta and James Llinas and Tony Gillespie and Scott Fouse and William Lawless and Ranjeev Mittu and Donald Sofge},
booktitle = {Putting AI in the Critical Loop},
publisher = {Academic Press},
pages = {265-278},
year = {2024},
isbn = {978-0-443-15988-6},
doi = {https://doi.org/10.1016/B978-0-443-15988-6.00006-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159886000066},
author = {Ariel M. Greenberg},
keywords = {Machine ethics, Nonmaleficence, Harm ontology, Visual affordances, Knowledge graphs},
abstract = {For intelligent systems to be permitted to act autonomously amidst and on behalf of humans, they must be sensitive to harms: what constitutes harms, how harms come to be, and how harms may be mitigated or avoided. The ability to identify harms is necessary for intelligent systems to adhere to the principle of nonmaleficence, i.e., the obligation to “do no harm.” Adherence to the principle of nonmaleficence is essential to the ethical use of artificial agency as well as to the production of ethical behavior in artificial agents. Our research team is working to implement nonmaleficence in artificial ethical agents. Specifically, we are drafting a means to train machines to identify potential harms to humans, for those machines to reason about previously unencountered dangers in a way legible to and governable by ethical considerations. We accomplish this by:1Designing a schema that organizes symbolic knowledge about harms,2Identifying corpora containing various parts of this knowledge, along with lexical and visual techniques to provide input during operation, and by3Constructing a task for human annotators to tag the elements of these corpora to populate an ontology in the form of a knowledge graph over which to perform inferences. We do this first for physical harms before examining harms of other sorts, including those psychological and financial in nature, which in practice must be balanced with one another. For physical harms, instances of physical injury from the National Electronic Injury Surveillance System (NEISS, https://www.cpsc.gov/Research--Statistics/NEISS-Injury-Data) would be crowdsourced, presented to microtask workers (mechanical Turkers, mturk.com) to provide the relevant attributes of environment, vector, host, and agent. These attributes, which may be recognized in a particular scene by computer vision, activate the knowledge graph relating insult to vulnerability, enabling inference of dangerousness between an object and an individual.}
}
@article{BOOKER2025123907,
title = {Utilising Natural Language Processing to Identify Brain Tumor Patients for Clinical Trials: Development and Initial Evaluation},
journal = {World Neurosurgery},
volume = {197},
pages = {123907},
year = {2025},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2025.123907},
url = {https://www.sciencedirect.com/science/article/pii/S1878875025002633},
author = {James Booker and Jack Penn and Kawsar Noor and Richard J.B. Dobson and Naomi Fersht and Jonathan P. Funnell and Ciaran S. Hill and Danyal Z. Khan and Nicola Newall and Tom Searle and Siddharth Sinha and Lewis Thorne and Simon C. Williams and Michael Kosmin and Hani J. Marcus},
keywords = {Brain tumors, Clinical trials, Machine learning, Natural language processing, Neuro-oncology, Recruitment},
abstract = {Background
Identifying patients eligible for clinical trials through eligibility screening is time and resource-intensive. Natural Language Processing (NLP) models may enhance clinical trial screening by extracting data from Electronic Health Records (EHRs).
Objective
We aimed to determine whether an NLP model can extract brain tumor diagnoses from outpatient clinic letters and link this with ongoing clinical trials.
Methods
This retrospective cohort study reviewed outpatient neuro-oncology clinic letters, to detect brain tumor diagnoses. We used an NLP model to perform a Named Entity Recognition + Linking algorithm that identified medical concepts in free text and linked them to a Systematized Nomenclature of Medicine Clinical Terms ontology, which we used to search a clinical trials database. Human annotators reviewed the accuracy of the concepts extracted and the relevance of recommended clinical trials. Search results were shown on a notification dashboard accessible by clinicians and patients on the EHR. We report the model's performance using precision, recall, and F1 scores.
Results
The model recognized 399 concepts across 196 letters with macro-precision = 0.994, macro-recall = 0.964, and macro-F1 = 0.977. Linking the model results with a clinical trials database identified 1417 ongoing clinical trials; of these, 755 were highly relevant to the individual patient, who met the eligibility criteria for trial recruitment.
Conclusions
NLP can be used effectively to extract brain tumor diagnoses from free-text EHR records with minimal additional training. The extracted concepts can then be linked to ongoing clinical trials. While further analysis is required to assess the impact on clinical outcomes, these findings suggest a potential application for integrating NLP algorithms into clinical care.}
}
@article{SAYEB20211114,
title = {Managing COVID-19 Crisis using C3HIS Ontology},
journal = {Procedia Computer Science},
volume = {181},
pages = {1114-1121},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.308},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921003574},
author = {Yemna Sayeb and Marwa Jebri and Henda Ben Ghezala},
keywords = {Health care service management, Covid-19 ontology, actor-competencies},
abstract = {The paper aims to present the C3HIS Ontology project, a web based solution for Covid-19 Crisis Health Care Information System. In the health care services, employee skills are a major resource and an essential part of everyday practice and a requirement for all health professions. We aim to prove how using individual profiles based on competencies can make a difference between life and death in times. As the performance assessment is driven by actors competencies we have to put human actors in the core of quality processes of health care services management in COVID-19 crisis.}
}
@article{LUKOVIC2019247,
title = {An ontology-based module of the information system ScolioMedIS for 3D digital diagnosis of adolescent scoliosis},
journal = {Computer Methods and Programs in Biomedicine},
volume = {178},
pages = {247-263},
year = {2019},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2019.06.027},
url = {https://www.sciencedirect.com/science/article/pii/S0169260719300665},
author = {Vanja Luković and Saša Ćuković and Danijela Milošević and Goran Devedžić},
keywords = {Ontology-based information system, Adolescent idiopathic scoliosis, Protégé-OWL API, Lenke classification},
abstract = {Background and objective
Conventional information systems are built on top of a relational database. The main weakness of these systems is impossibility to define stable data schema ahead when the knowledge of the system is evolving and dynamic. The widely accepted alternatives to relational databases are ontologies that can be used for designing information systems. Many research papers describe various methods for improving reliability and precision in generating the type of the Lenke classification based on the image processing techniques or a computer program, but all of them require radiograph images. The main objective of this paper is to demonstrate the development of an ontology-based module of the information system ScolioMedIS for adolescent idiopathic scoliosis (AIS) diagnosis and monitoring, which uses optical 3D methods to determine the Lenke classification of AIS and to avoid harmful effects of traditional radiation diagnosis.
Methods
For creating an ontology-based module of the ScolioMedIS we used the following steps: specification, conceptualization, formalization and implementation. In the specification and conceptualization phase we performed data collection and analysis to define domain, concepts and relationships for ontology design. In the formalization and implementation stage we developed the OBR-Scolio ontology and the ontology-based module of the ScolioMedIS. The module employs the Protégé-OWL API, as a collection of Java interfaces for the OBR-Scolio ontology, which enables the creating, deleting, and editing of the basic elements of the OBR-Scolio ontology, as well as the querying of the ontology.
Results
The ontology-based module of ScolioMedIS is tested on the datasets of 20 female and 15 male patients with AIS between the ages of 11 and 18, to categorize spinal curvatures and to automatically generate statistical indicators about the frequency of the basic spinal curvatures, degree of progression or regression of deformity and statistical indicators about curvature characteristics according to the Lenke classification system and Lenke scoliosis types. Results are then compared with analysis of the Lenke classification of 315 observed patients, performed using traditional radiation techniques.
Conclusions
This part of the system allows continuous monitoring of the progression/regression of spinal curvatures for each registered patient, which may provide a better management of scoliosis (diagnosis and treatment).}
}
@article{MORENTEMOLINERA2020105657,
title = {A dynamic group decision making process for high number of alternatives using hesitant Fuzzy Ontologies and sentiment analysis},
journal = {Knowledge-Based Systems},
volume = {195},
pages = {105657},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.105657},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120301052},
author = {J.A. Morente-Molinera and F.J. Cabrerizo and J. Mezei and C. Carlsson and E. Herrera-Viedma},
keywords = {Group decision making, Fuzzy ontologies, Sentiment analysis, Computing with words, Type-2 hesitant fuzzy sets},
abstract = {The high spread of Internet and social networks have completely changed the way that Group Decision Making methods are designed, developed and implemented. Experts now operate in environments where a large amount of information is available and new ideas and participants can appear at any time; this results in a dynamically changing decision environment. In this paper, a novel group decision making method for dynamic contexts with a high number of decision alternatives is presented. As the main component of the proposal, a perceptual computing scheme is used in order to extract information from the experts. In the process, sentiment analysis is used when analyzing the debate texts in order to obtain information for selecting the best alternatives on each round. Moreover, interval type-2 hesitant Fuzzy Ontologies are used in order to store the information related to alternatives. By combining interval type-2 and hesitant fuzzy sets, imprecise information can be represented in a comfortable and intuitive way within the ontology.}
}
@article{KHATTAK2021455,
title = {Enhanced concept-level sentiment analysis system with expanded ontological relations for efficient classification of user reviews},
journal = {Egyptian Informatics Journal},
volume = {22},
number = {4},
pages = {455-471},
year = {2021},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2021.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110866521000219},
author = {Asad Khattak and Muhammad Zubair Asghar and Zain Ishaq and Waqas Haider Bangyal and Ibrahim A Hameed},
keywords = {Machine learning techniques, Support vector machine, Formal concept analysis (FCA), Concept lattice, Ontological relations},
abstract = {Background/introduction
Concept-level sentiment analysis deals with the extraction and classification of concepts and features from user reviews expressed online about products and other entities like political leaders, government policies, and others. The prior studies on concept-level sentiment analysis have used a limited set of linguistic rules for extracting concepts and their associated features. Furthermore, the ontological relations used in the early works for performing concept-level sentiment analysis need enhancement in terms of the extended set of features concepts and ontological relations.
Methods
This work aims at addressing the aforementioned issues and tries to bridge the literature gap by proposing an extended set of linguistic rules for concept-feature pair extraction along with enhanced set ontological relations. Additionally, a supervised a machine learning technique is implemented for performing concept-level sentiment analysis.
Results and conclusions
Experimental results depict the effectiveness of the proposed system in terms of improved efficiency (P: 88%, R: 88%, F-score: 88%, and A: 87.5%).}
}
@article{LI2020106436,
title = {Repairing mappings across biomedical ontologies by probabilistic reasoning and belief revision},
journal = {Knowledge-Based Systems},
volume = {209},
pages = {106436},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106436},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120305657},
author = {Weizhuo Li and Songmao Zhang},
keywords = {Mapping validation and repair, Biomedical ontologies, Probabilistic description logics, Belief revision},
abstract = {The ontology matching approaches identify correspondences among entities across ontologies, and the quality of ontology mappings is crucial for supporting knowledge sharing and reuse on the Semantic Web. In the annual Ontology Evaluation Alignment Initiative (OAEI) competitions, matching large and complex, real-world biomedical ontologies is one of the most challenging endeavors. As matching methods are basically heuristic, wrong mappings often exist in the generated alignments. The general framework of mapping validation collects candidate wrong mappings based on unsatisfiable concepts and adopts the removal strategy to gain the coherence of alignments w.r.t. source ontologies. Although it ensures logical coherence, such repairing does not necessarily guarantee the quality of mappings obtained, i.e., the disposed mappings can be positive and the retained ones can be wrong in terms of the domain knowledge intended in the ontologies per se. This can be demonstrated by the existence of incoherences when the UMLS Metathesaurus® has been used as the basis of reference alignments for OAEI biomedical ontology matching tasks. To address this problem, we propose a novel approach for repairing biomedical ontology mappings by probabilistic reasoning and belief revision techniques, featuring a combination of removal strategy and revision strategy. More concretely, mappings are transformed into probabilistic description logics (PDL) conditional constraints and their weights into probability intervals based on our designed rules. Then, the incoherence checking of mappings is reduced to solving a linear program with the constraints under the PDL semantics. For identified incoherent mappings, instead of simply discarding, we revise them by relaxing their probability intervals until the probabilistic coherence is reached. The evaluation on repairing OAEI biomedical alignments shows that our approach can be effective in retaining correct mappings and removing wrong ones. Moreover, we show that repair systems following the general framework of mapping validation have improved their performance when equipped with our revision module at the repair stage. Furthermore, feeding structural matchers with repaired alignments as seeds shows that the mappings generated by our approach lead to the best structural matching result compared with other repair systems. Being non-aggressive, our approach is suitable for applications like ontology-supported medical information retrieval, semantic annotation and indexing of medical articles, and matchmaking and ranking objects among multiple ontologies.}
}
@article{ZEMMOUCHIGHOMARI2018453,
title = {Ontology assessment based on linked data principles},
journal = {International Journal of Web Information Systems},
volume = {14},
number = {4},
pages = {453-479},
year = {2018},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-01-2018-0003},
url = {https://www.sciencedirect.com/science/article/pii/S1744008418000137},
author = {Leila Zemmouchi-Ghomari and Kaouther Mezaache and Mounia Oumessad},
keywords = {Metadata and ontologies, Linked data principles, Ontology evaluation, Web of data, Semantic web technologies, Web technologies},
abstract = {Purpose
The purpose of this paper is to evaluate ontologies with respect to the linked data principles. This paper presents a concrete interpretation of the four linked data principles applied to ontologies, along with an implementation that automatically detects violations of these principles and fixes them (semi-automatically). The implementation is applied to a number of state-of-the-art ontologies.
Design/methodology/approach
Based on a precise and detailed interpretation of the linked data principles in the context of ontologies (to become as reusable as possible), the authors propose a set of algorithms to assess ontologies according to the four linked data principles along with means to implement them using a Java/Jena framework. All ontology elements are extracted and examined taking into account particular cases, such as blank nodes and literals. The authors also provide propositions to fix some of the detected anomalies.
Findings
The experimental results are consistent with the proven quality of popular ontologies of the linked data cloud because these ontologies obtained good scores from the linked data validator tool.
Originality/value
The proposed approach and its implementation takes into account the assessment of the four linked data principles and propose means to correct the detected anomalies in the assessed data sets, whereas most LD validator tools focus on the evaluation of principle 2 (URI dereferenceability) and principle 3 (RDF validation); additionally, they do not tackle the issue of fixing detected errors.}
}
@article{LI2022102098,
title = {Ontology-based knowledge representation and semantic topic modeling for intelligent trademark legal precedent research},
journal = {World Patent Information},
volume = {68},
pages = {102098},
year = {2022},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2022.102098},
url = {https://www.sciencedirect.com/science/article/pii/S0172219022000059},
author = {Gi-Kuen J. Li and Charles V. Trappey and Amy J.C. Trappey and Annie A.S. Li},
keywords = {Legal research, Ontology-based knowledge system, Latent Dirichlet allocation (LDA), Semantic topic modeling},
abstract = {An intelligent methodology and its prototype system are developed for automatically discovering legal precedents using semantic analysis. The concept of the trademark legal precedent recommendation was originated from our TE2020 conference paper. The approach is to identify matching cases related to given seed case with respect to their legal case brief attributes using advanced text mining techniques. In the paper, dynamic topic modeling is further developed to analyze the dataset over three time-sequential cohorts to identify trademark law topics varied over time. Further, the prototype system was demonstrated and verified using real trademark case analysis with satisfactory results.}
}
@article{HUANG2020103189,
title = {Smart manufacturing and DVSM based on an Ontological approach},
journal = {Computers in Industry},
volume = {117},
pages = {103189},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103189},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519305275},
author = {Zhuoyu Huang and Casey Jowers and Ali Dehghan-Manshadi and Matthew S. Dargusch},
keywords = {Neo4j, Semantic ontology, CHAMP, CPS, DVSM},
abstract = {Smart manufacturing is characterized as transparent shop floor production, rapid and intelligent responses to dynamic changes, and a utilization of high-performance inter-cooperation networks. Smart manufacturing and a global appetite for personalized products have transitioned industry from mass production into the age of mass customization. Increased autonomy is slowly changing customer expectations as well, enabling customers to modify a product design not only during an order, but sometimes even long after placing an order. In this context, this paper fills a gap by presenting a data-centric infrastructure to enable interaction with a “global, virtual data space,” which overcomes the problems with traditional direct access methods such as interoperability and compatibility. Using a Cyber-Physical System (CPS), resource monitoring on the shopfloor as well as multiple parities beyond the enterprise boundary will be interconnected through this data-centric infrastructure. A semantic knowledge management system, which encompasses product lifecycle knowledge and manufacturing process ontology, is developed as the data schema in the data-centric infrastructure. In comparison to relational databases which are effective at handling paper forms and tabular structure, the flexible schema of graph databases enable these to handle dynamic and uncertain variables. These capabilities are deemed critical for a platform supporting real-time information exchange between customer, manufacturer and collaborators. One advantage of such a system allowing for real-time information exchange is that it enables last minute order changes by the customer, allowing for product design changes even after production has started on the order. The other advantage is that it allows manufacturing managers to monitor the productivity of customer-directed, dynamic manufacturing processes by utilizing Dynamic Value Stream Mapping (DVSM) methods.}
}
@article{YACO2018230,
title = {Informatics for cultural heritage instruction: an ontological framework},
journal = {Journal of Documentation},
volume = {75},
number = {2},
pages = {230-246},
year = {2018},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-02-2018-0035},
url = {https://www.sciencedirect.com/science/article/pii/S0022041818000498},
author = {Sonia Yaco and Arkalgud Ramaprasad},
keywords = {Higher education, Digital libraries, Information-seeking behaviour, Ontologies, Curriculum, Information literacy, Digital humanities, Academic libraries – relations with faculty, Galleries, libraries, archives and museums, Primary source instruction},
abstract = {Purpose
The purpose of this paper is to suggest a framework that creates a common language to enhance the connection between the domains of cultural heritage (CH) artifacts and instruction.
Design/methodology/approach
The CH and instruction domains are logically deconstructed into dimensions of functions, semiotics, CH, teaching/instructional materials, agents and outcomes. The elements within those dimensions can be concatenated to create natural-English sentences that describe aspects of the problem domain.
Findings
The framework is valid using traditional social sciences content, semantic, practical and systemic validity constructs.
Research limitations/implications
The framework can be used to map current research literature to discover areas of heavy, light and no research.
Originality/value
The framework provides a new way for CH and education stakeholders to describe and visualize the problem domain, which could allow for significant enhancements of each. Better understanding the problem domain would serve to enhance instruction informed from collections and vice versa. The educational process would have more depth due to better access to primary sources. Increased use of collections would reveal more ways through which they could be used in instruction. The framework can help visualize the past and present of the domain, and envisage its future.}
}
@article{LI2022101699,
title = {Aligning social concerns with information system security: A fundamental ontology for social engineering},
journal = {Information Systems},
volume = {104},
pages = {101699},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101699},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920301460},
author = {Tong Li and Xiaowei Wang and Yeming Ni},
keywords = {Social engineering, Ontology, Information system security, Psychology, Attacks},
abstract = {Along with the rapid development of socio-technical systems, people are playing an increasingly important role in information system and have actually become an essential system component. However, unlike technology-based attacks that have been investigated for decades, social engineering attacks have not been efficiently addressed. In particular, due to the interdisciplinary nature of social engineering, there is a lack of consensus on its definition, hindering the further development of this research field. In this paper, we propose a comprehensive and fundamental ontology of social engineering based on a systematic review of existing social engineering taxonomies and ontologies in order to provide a theoretical foundation for social engineering analysis. The essential contributions of this paper include: (1) propose a comprehensive ontology of social engineering and precisely specify ontological definitions of its essential concepts based on Situation Calculus; (2) enumerate and summarize a set of social engineering techniques and present their fine-grained classification based on the proposed ontology; (3) incorporate psychology and sociology knowledge into social engineering analysis, encapsulating such knowledge in terms of a formalized ontology. We have evaluated our ontology based on a set of real social engineering attacks, the results of which show the usefulness of our proposal.}
}
@article{NORTON202014,
title = {Loop quantum ontology: Spacetime and spin-networks},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {71},
pages = {14-25},
year = {2020},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2020.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1355219819302382},
author = {Joshua Norton},
keywords = {Loop quantum gravity, Spin-networks, Spacetime, Substantivalism, Emergence},
abstract = {It is standardly claimed in loop quantum gravity (LQG) that spacetime both disappears, fundamentally, and emerges from spin-networks in the low energy regime. In this paper, I critically explore these claims and develop a variety of substantival and relational interpretations of LQG for which these claims are false. According to most of the interpretations I consider, including the “received interpretation”, it is in fact false that spacetime emerges from spin-networks. In the process of supporting these claims, I also explain why spacetime is thought to be missing from the theory's fundamental ontology and demonstrate how this conclusion depends on our interpretation of the theory. In fact, I will argue that for a variety of interpretations spacetime survives quantization just as the electromagnetic field survives quantization. The upshot of the following analysis is a much needed clarification of the ontology of LQG and how it relates, or fails to relate, to the spacetime of general relativity.}
}
@article{HUANG2019197,
title = {Data-driven ontology generation and evolution towards intelligent service in manufacturing systems},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {197-207},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.075},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18317904},
author = {Chengxi Huang and Hongming Cai and Lida Xu and Boyi Xu and Yizhi Gu and Lihong Jiang},
keywords = {Data integration, Intelligent manufacturing, Services computing, Ontology evolution, Semantic disposing},
abstract = {To support intelligent manufacturing, providing a unified production data view by integrating distributed data collected by different enterprise information systems is critical. Because various information systems are often heterogeneous, ontology is widely adopted to present a global reference view for data integration. However, construction and maintenance of these ontologies is difficult because of the heterogeneity and dynamism of these large-scale data. In this paper, with the objective of intelligent manufacturing application implementation, we propose a comprehensive ontology generation and evolution method that automatically abstracts ontology from raw production data and dynamically adjusts the ontology in accordance with changes in the manufacturing data environment. The proposed method comprises four phases: data extraction, ontology construction, ontology connection, and ontology evolution. In the first phase, data from different sources are mapped to data entities to form a unified data structure. In the second phase, an initial ontology is generated via instance-driven ontology construction. In the third phase, to support intelligent manufacturing, the initial ontologies are organised in terms of the dimensions of the various business elements, such as stuff, machine, product, process, and scenarios. In the fourth phase, rules regarding ontology restrictions are formulated to realise ontology evolution that respond to changes in the manufacturing environment. To verify the efficacy of the proposed method, a prototype was implemented with real data from a manufacturing factory, in which the constructed ontology was used as the metadata of product data in intelligent manufacturing.}
}
@article{MURTAZINA2019628,
title = {An Ontology-based Approach to Support for Requirements Traceability in Agile Development},
journal = {Procedia Computer Science},
volume = {150},
pages = {628-635},
year = {2019},
note = {Proceedings of the 13th International Symposium “Intelligent Systems 2018” (INTELS’18), 22-24 October, 2018, St. Petersburg, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919303898},
author = {M.Sh. Murtazina and T.V. Avdeenko},
keywords = {ontology, requirements traceability, agile development},
abstract = {The paper proposes an ontology-based approach to support for requirements traceability in Agile development. The task of supporting the requirements traceability in a software development project is considered as a part of requirements engineering process. A brief overview of the benefits of requirements traceability usage is given. The expediency of using the ontology-based approach to support for requirements engineering, requirements traceability in particular, is shown through the analysis of scientific publications. OWL ontology is chosen to support the requirements engineering process. The ontology is implemented in the Protégé environment. The developed ontology takes into account particular qualities of working with the requirements in Agile projects, accumulates knowledge about the requirements types and requirements artefacts, enables tracing the relations between them.}
}
@article{NADAL20193,
title = {An integration-oriented ontology to govern evolution in Big Data ecosystems},
journal = {Information Systems},
volume = {79},
pages = {3-19},
year = {2019},
note = {Special issue on DOLAP 2017: Design, Optimization, Languages and Analytical Processing of Big Data},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917304660},
author = {Sergi Nadal and Oscar Romero and Alberto Abelló and Panos Vassiliadis and Stijn Vansummeren},
keywords = {Data integration, Evolution, Semantic web},
abstract = {Big Data architectures allow to flexibly store and process heterogeneous data, from multiple sources, in their original format. The structure of those data, commonly supplied by means of REST APIs, is continuously evolving. Thus data analysts need to adapt their analytical processes after each API release. This gets more challenging when performing an integrated or historical analysis. To cope with such complexity, in this paper, we present the Big Data Integration ontology, the core construct to govern the data integration process under schema evolution by systematically annotating it with information regarding the schema of the sources. We present a query rewriting algorithm that, using the annotated ontology, converts queries posed over the ontology to queries over the sources. To cope with syntactic evolution in the sources, we present an algorithm that semi-automatically adapts the ontology upon new releases. This guarantees ontology-mediated queries to correctly retrieve data from the most recent schema version as well as correctness in historical queries. A functional and performance evaluation on real-world APIs is performed to validate our approach.}
}
@article{JANKOVIC202056,
title = {Space debris ontology for ADR capture methods selection},
journal = {Acta Astronautica},
volume = {173},
pages = {56-68},
year = {2020},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2020.03.047},
url = {https://www.sciencedirect.com/science/article/pii/S0094576520301752},
author = {Marko Jankovic and Mehmed Yüksel and Mohammad Mohammadzadeh Babr and Francesca Letizia and Vitali Braun},
keywords = {Space debris, Ontology, Active debris removal, Breakup hazard, Protégé, Python},
abstract = {Studies have concluded that active debris removal (ADR) of the existing in-orbit mass is necessary. However, the quest for an optimal solution does not have a unique answer and the available data often lacks coherence. To improve this situation, modern knowledge representation techniques, that have been shaping the World Wide Web, medicine and pharmacy, should be employed. Prior efforts in the domain of space debris have only focused onto the space situational awareness, neglecting ADR. To bridge this gap we present a domain-ontology of intact derelict objects, i. e. payloads and rocket bodies, for ADR capture methods selection. The ontology is defined on a minimal set of physical, dynamical and statistical parameters of a target object. The practicality and validity of the ontology are demonstrated by applying it onto a database of 30 representative objects, built by combining structured and unstructured data from publicly available sources. The analysis of results proves the ontology capable of inferring the most suited ADR capture methods for considered objects. Furthermore, it confirms its ability to handle the input data from different sources transparently, minimizing user input. The developed ontology provides an initial step towards a more comprehensive knowledge representation framework meant to improve data management and knowledge discovery in the domain of space debris. Furthermore, it provides a tool that should make the initial planning of future ADR missions simpler yet more systematic.}
}
@article{LEI2021103781,
title = {Formalized control logic fault definition with ontological reasoning for air handling units},
journal = {Automation in Construction},
volume = {129},
pages = {103781},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103781},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521002326},
author = {Xuechen Lei and Yan Chen and Mario Bergés and Burcu Akinci},
keywords = {HVAC control, Fault detection and diagnosis, Direct digital control, Building information modeling, HVAC schema},
abstract = {Control logic programs determine the behavior of Heating, Ventilation, and Air Conditioning (HVAC) systems under different operating conditions. Faults in control logic account for more than 15% of all HVAC system problems, causing energy waste and occupancy discomfort. The first step towards systematically detecting and diagnosing control logic faults is to have an unambiguous control logic fault definition so that the existence of control logic faults in an HVAC system can be discovered from its operational data. In order to have high level of fault detection accuracy, the control logic fault definition needs to be customized for different HVAC systems as they have different components and control sequences. In this paper, we propose an object-oriented classification approach to systematically define customized control logic faults in terms of control logic input/output variable expressions. Focusing on air handling unit (AHU) systems, and their general operation objectives of energy efficiency and occupancy comfort, we elaborated four control goals and developed corresponding reasoning mechanisms to derive fault definitions. We also developed an HVAC component and control information ontology to be used in these reasoning mechanisms by extending existing HVAC information models. The prototype of the developed approach was tested with 27 common AHUs specified by the American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE), and the results show that using the developed approach, it is possible to define a customized set of control logic faults applicable to each specific AHU with an overall precision of 94.2% and recall of 83.0%. This demonstrates the generality of our proposed approach in providing customized control logic fault definitions for different types of AHUs.}
}
@article{HAN2021633,
title = {APTMalInsight: Identify and cognize APT malware based on system call information and ontology knowledge framework},
journal = {Information Sciences},
volume = {546},
pages = {633-664},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.08.095},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520308628},
author = {Weijie Han and Jingfeng Xue and Yong Wang and Fuquan Zhang and Xianwei Gao},
keywords = {APT attack, APT malware, System call information, Ontology},
abstract = {APT attacks have posed serious threats to the security of cyberspace nowadays which are usually tailored for specific targets. Identification and understanding of APT attacks remains a key issue for society. Attackers often utilize malware as the weapons to launch cyber-attacks. For this reason, detecting APT malware and gaining an insight of its malicious behaviors can strengthen the power to understand and counteract APT attacks. Based on the above motivation, this paper proposes a novel APT malware detection and cognition framework named APTMalInsight aiming at identifying and cognizing APT malware by leveraging system call information and ontology knowledge. We systematically study APT malware and extracts dynamic system call information to describe its behavioral characteristics. With respect to the established feature vectors, the APT malware can be detected and clustered into their belonging families accurately. Furthermore, a horizontal comparison between APT malware and the traditional malware is conducted from the perspective of behavior types, to understand the behavioral characteristics of APT malware in depth. On the above basis, the ontology model is introduced to construct the APT malware knowledge framework to represent its typical malicious behaviors, thereby implementing the systematic cognition of APT malware and providing contextual understanding of APT attacks. The evaluation results based on real APT malware samples demonstrate that the detection and clustering accuracy can reach up to 99.28% and 98.85% respectively. In addition, APTMalInsight supplies an effective cognition framework for APT malware and enhances the capability to understand APT attacks.}
}
@article{ZANGENEH2020101164,
title = {Ontology-based knowledge representation for industrial megaprojects analytics using linked data and the semantic web},
journal = {Advanced Engineering Informatics},
volume = {46},
pages = {101164},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101164},
url = {https://www.sciencedirect.com/science/article/pii/S147403462030135X},
author = {Pouya Zangeneh and Brenda McCabe},
keywords = {Industrial megaprojects, Knowledge representation, Project analytics, Risk analysis, Ontology, Semantic web},
abstract = {The fourth industrial revolution has affected most industries, including construction and those within the delivery chain of megaprojects. These major paradigm shifts, however, did not considerably improve the track record in predicting project outcomes and estimating required resources. One reason is the lack of unified data definitions and expandable knowledge representation across project lifecycle to represent megaprojects for analytics. This paper proposes and evaluates a unified ontology for project knowledge representation that facilitates data collection, processing, and utilization for industrial megaprojects through their lifecycle. The proposed Uniform Project Ontology, or UPonto, provides a data infrastructure for project analytics by enabling logical deductions and inferences, and flexible expansion and partitioning of the data utilizing linked data and the semantic web. The ontology facilitates cost normalization processes, temporal queries, and graph queries using SPARQL, while defining universal semantics for a wide range of project risk factors and characteristics based on comprehensive research of the empirical project risk and success literature augmented by practical considerations gained through expert consultations. UPonto forms the basis for a project knowledge graph to utilize unstructured data; it as well provides semantic definitions for smart IoT agents to consume project risk data and knowledge.}
}
@article{CLARDY2020653,
title = {The ontological foundation for studying the future},
journal = {Foresight},
volume = {22},
number = {56},
pages = {653-670},
year = {2020},
issn = {1463-6689},
doi = {https://doi.org/10.1108/FS-02-2020-0016},
url = {https://www.sciencedirect.com/science/article/pii/S146366892000019X},
author = {Alan Clardy},
keywords = {Ontology, Forecasting, Futures studies, Foresight fundamentals, Social systems prediction},
abstract = {Purpose
The purpose of this paper is to develop an ontological foundation for future studies, based in part on integrating some prior albeit incomplete work in this area.
Design/methodology/approach
This manuscript is based on a literature review, as well as on conceptual and theoretical enhancements from this subject field.
Findings
As the future does not exist (it is always something yet to come), the ontological foundations for studying the future must be based on the current reality of the physical, biological and social-psychological worlds of experience and ideas. From this basis, 10 postulates are provided that are based on that current reality and are applied to studying the future. Thus, by characterizing the current reality and how it is understood by people, meaningful statements about the future are possible.
Practical implications
For each ontological postulate, one or more implications for the study of the future are provided as guidelines for practice.
Originality/value
This manuscript integrates and builds on prior offerings about ontological concerns into a comprehensive framework that legitimates and focuses the practices of studying the future.}
}
@article{KESTEL2019292,
title = {Ontology-based approach for the provision of simulation knowledge acquired by Data and Text Mining processes},
journal = {Advanced Engineering Informatics},
volume = {39},
pages = {292-305},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618304270},
author = {Philipp Kestel and Patricia Kügler and Christoph Zirngibl and Benjamin Schleich and Sandro Wartzack},
keywords = {Knowledge-based engineering, Simulation, Finite Element Analysis, Ontology-based knowledge representation, Text Mining, Data Mining},
abstract = {Numerical simulation techniques such as Finite Element Analyses are essential in today's engineering design practices. However, comprehensive knowledge is required for the setup of reliable simulations to verify strength and further product properties. Due to limited capacities, design-accompanying simulations are performed too rarely by experienced simulation engineers. Therefore, product models are not sufficiently verified or the simulations lead to wrong design decisions, if they are applied by less experienced users. This results in belated redesigns of already detailed product models and to highly cost- and time-intensive iterations in product development. Thus, in order to support less experienced simulation users in setting up reliable Finite Element Analyses, a novel ontology-based approach is presented. The knowledge management tools developed on the basis of this approach allow an automated acquisition and target-oriented provision of necessary simulation knowledge. This knowledge is acquired from existing simulation models and text-based documentations from previous product developments by Text and Data Mining. By offering support to less experienced simulation users, the presented approach may finally lead to a more efficient and extensive application of reliable FEA in product development.}
}
@article{BAUDRIT2022,
title = {Decision Support Tool for the Agri-Food Sector Using Data Annotated by Ontology and Bayesian Network:},
journal = {International Journal of Agricultural and Environmental Information Systems},
volume = {13},
number = {1},
year = {2022},
issn = {1947-3192},
doi = {https://doi.org/10.4018/IJAEIS.309136},
url = {https://www.sciencedirect.com/science/article/pii/S1947319222000041},
author = {Cédric Baudrit and Patrice Buche and Nadine Leconte and Christophe Fernandez and Maëllis Belna and Geneviève Gésan-Guiziou},
keywords = {Bayesian Network, Data Integration, INRAE, Knowledge Base, Knowledge Integration, Milk Microfiltration, Ontology, Reliability, Uncertainty},
abstract = {ABSTRACT
The scientific literature is a valuable source of information for developing predictive models to design decision support systems. However, scientific data are heterogeneously structured expressed using different vocabularies. This study developed a generic workflow that combines ontology, databases, and computer calculation tools based on the theory of belief functions and Bayesian networks. The ontology paradigm is used to help integrate data from heterogeneous sources. Bayesian network is estimated using the integrated data taking into account their reliability. The proposed method is unique in the sense that it proposes an annotation and reasoning tool dedicated to systematic analysis of the literature, which takes into account expert knowledge of the domain at several levels: ontology definition, reliability criteria, and dependence relations between variables in the BN. The workflow is assessed successfully by applying it to a complex food engineering process: skimmed milk microfiltration. It represents an original contribution to the state of the art in this application domain.}
}
@article{MOTTA2021101333,
title = {Relational ontologies and performance: Identifying humans and nonhuman animals in the rock art from north-eastern Kimberley, Australia},
journal = {Journal of Anthropological Archaeology},
volume = {63},
pages = {101333},
year = {2021},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2021.101333},
url = {https://www.sciencedirect.com/science/article/pii/S0278416521000660},
author = {Ana Paula Motta and Peter M. Veth and  {Balanggarra Aboriginal Corporation}},
keywords = {Relational ontologies, Human/animal relations, Performance, Indigenous ontologies, Rock art, Figurative art, Australia},
abstract = {Even though the study of animal depictions in early art is one of the most researched topics in rock art, interpretations have often been anthropocentric. Rather than seeing how human and animal populations co-exist and become with, rock art explanations of animals often linger around economic appreciations that prioritize their value for human beings. This view has been extensively influenced by a Cartesian philosophy that has at its core an idea of human exceptionalism and domination over other species. Here, we are concerned with deconstructing the ontological footing of humans and animals in the early rock art from the Kimberley, Australia, from a relational and performative point of view. Methods used in rock art to identify figurative motifs are deeply entangled with Western conceptualizations of what it means to be human/animal, marginalising Indigenous ontologies. Our main objective is to advance an epistemological approach that will allow us to identify and understand the modes of representation used by artists in the study area. We do so through the application of an iconographic analysis that incorporates performative relationships between motifs. By considering performance, we are able to engage with non-essentialists ways of being and focus instead on Indigenous ontologies.}
}
@article{BILETSKIY201881,
title = {Building a business domain meta-ontology for information pre-processing},
journal = {Information Processing Letters},
volume = {138},
pages = {81-88},
year = {2018},
issn = {0020-0190},
doi = {https://doi.org/10.1016/j.ipl.2018.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0020019018301340},
author = {Yevgen Biletskiy and J. Anthony Brown and Girish R. Ranganathan and Ebrahim Bagheri and Ismail Akbari},
keywords = {Software engineering, Ontology, Information extraction},
abstract = {Business analysts, along with other business domain software application users, have created a vast amount of business documents, which often do not have any business domain ontologies in the background. This situation leads to misinterpretation of such documents, when being processed by machines, that results in inhibiting the productiveness of computer-assisted analytical work and effectiveness of business solutions due to lack of effective semantics; therefore, business analysts (especially, if rotating) can use well-designed business domain ontologies as a backbone for their official applications. The process of extracting and capturing domain ontologies from these voluminous documents requires extensive involvement of domain experts and application of methods of ontology learning that is substantially labor intensive; therefore, some intermediate solutions which would assist in capturing business domain ontologies must be developed. The present paper proposes a solution in this direction which involves building a meta-ontology as a rapid approach in conceptualizing a business domain from huge amounts of source documents. This meta-ontology can be populated by ontological concepts, attributes and relations from business documents, and then refined in order to form better business domain ontology either through automatic ontology learning methods or some other traditional ontology building approaches.}
}
@incollection{BUSSEMAKER20181565,
title = {Ontology Modelling for Lignocellulosic Biomass: Composition and Conversion},
editor = {Anton Friedl and Jiří J. Klemeš and Stefan Radl and Petar S. Varbanov and Thomas Wallek},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {43},
pages = {1565-1570},
year = {2018},
booktitle = {28th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64235-6.50273-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642356502734},
author = {Madeleine Bussemaker and Nikolaos Trokanas and Linsey Koo and Franjo Cecelja},
keywords = {lignocellulose, ontology, biomass, biorefining},
abstract = {This paper presents an expansion of an already developed ontology BiOnto (Trokanas, Bussemaker, Velliou, Tokos, & Cecelja, 2015) and processing technology eSymbiosis ontology (Raafat, Trokanas, Cecelja, & Bimi, 2013) towards valorisation of lignocellulosic biomass. The ontology provides a reference model interpretable by humans and computers by further classifying and characterizing lignocellulosic biomass (LCB) in several ways, such as: lignin, hemicellulose and cellulose content, C5 and C6 composition, elemental composition, and heat value. Similarly, LCB processing technologies are classified and characterised based on the input of LCB components, with related conversion rates of specific components. The combination of these classifications can elucidate additional information to assist in decision making for the ontology user. For example, the theoretical conversion rates of C5 and C6 polymeric sugars to ethanol are 0.5987 and 0.5679, then by employing the inference capabilities of the knowledge model, the user can gain insights into theoretical ethanol yields for various biomass types based on their C5 and C6 polymeric composition. This can also be applied to theoretical and actual yields of technologies modelled within the ontology, providing a useful reference tool for biorefinery development.}
}
@article{ABIOUI2018426,
title = {Towards a Novel and Generic Approach for OWL Ontology Weighting},
journal = {Procedia Computer Science},
volume = {127},
pages = {426-435},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.140},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918301522},
author = {Hasna Abioui and Ali Idarrou and Ali Bouzit and Driss Mammass},
keywords = {Ontologies, Ontology Weighting, Taxonomic Structure, Semantic Relationships},
abstract = {Semantic search is qualified – by web-related enterprises as well as, academic research – as a key technology, ensuring important improvements in terms of shared data understanding, while it leads to refined and targeted interpretations. Accordingly, ontologies are the focal asset for a well-functioning semantic search approach, since their ability to share, represent and reuse explicit and semantic domain specification. Nowadays, a multitude of ontologies containing up to hundreds of thousands of concepts are proposed. Thus, our challenge as researchers exceeds conceptualizing or creating ontologies to being able to choose the fitting and suitable one, taking into account specific criteria. This paper comes within the same context as it presents a novel approach for weighting OWL ontologies, in order to choose the most appropriate one from a set of proposed ontologies. Our approach takes into account not only the taxonomic structure, but also the semantic aspect of the ontology. Furthermore, semantic relationships and specific concepts are the favored since they reflect the semantic richness of the ontology.}
}
@article{PRABHU2019511,
title = {Improved scalability in mining using ontology record linkage algorithm},
journal = {Computers & Electrical Engineering},
volume = {74},
pages = {511-519},
year = {2019},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2018.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617309308},
author = {T. Prabhu and C. Suresh {Gnana Dhas}},
keywords = {Record linkage, Data mining, Angle based neighborhood, Ontology, Conventional method},
abstract = {Record linkage offers wide role in record identification and relevant datasets matching. The conventional researchers use probabilistic approach to identify reliable and unique datasets. Record linkage with probabilistic approach exploits data, which are common to an individual record pair. Classical methods have equality based record linkage in common fields. Therefore, errors associated with record linkage reduce the scalability. In this paper, a similarity between individual values of record pairs is improved using ontology-based semantic similarity model. Semantic similarity between the records is tested successfully using angle based neighborhood graph. To validate the proposed approach, a conventional record linkage algorithm is compared with angle based neighborhood ontology record linkage technique, which achieves improved accuracy and scalability. Finally, the accuracy of identifying similar semantic matches is more scalable in proposed technique than conventional methods.}
}
@article{KILINTZIS2019103179,
title = {Supporting integrated care with a flexible data management framework built upon Linked Data, HL7 FHIR and ontologies},
journal = {Journal of Biomedical Informatics},
volume = {94},
pages = {103179},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103179},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419300978},
author = {Vassilis Kilintzis and Ioanna Chouvarda and Nikolaos Beredimas and Pantelis Natsiavas and Nicos Maglaveras},
keywords = {Linked Data, Ontology, Telemonitoring, HL7 FHIR, OWL, Web services, Integrated care},
abstract = {In this paper we present the methodology and decisions behind an implementation of a telehealth data management framework, aiming to support integrated care services for chronic and multimorbid patients. The framework leverages an OWL ontology, built upon HL7 FHIR resources, to provide storage and representation of semantically enriched EHR data following Linked Data principles. This is presented along with the realization of the persistent storage solution and communication web services that allow the management of EHR data, ensuring the validity and integrity of the exchanged patient data as self-describing ontology instances. The framework concentrates on flexibility and reusability, which is addressed by regarding the aforementioned ontology as a single point of change. This solution has been implemented in the scope of the EU project WELCOME for managing data in a telemonitoring system for patients with COPD and co-morbidities and was also successfully deployed for the INLIFE EU project with minimal effort. The results of the two applications suggest it can be adopted and properly adapted in a series of integrated care scenarios with minimal effort.}
}
@article{WYSOCKA2024104724,
title = {Large Language Models, scientific knowledge and factuality: A framework to streamline human expert evaluation},
journal = {Journal of Biomedical Informatics},
volume = {158},
pages = {104724},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104724},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001424},
author = {Magdalena Wysocka and Oskar Wysocki and Maxime Delmas and Vincent Mutel and André Freitas},
keywords = {Factual knowledge, Large language models, Antibiotic discovery, Retrieval-augmented generation},
abstract = {Objective:
The paper introduces a framework for the evaluation of the encoding of factual scientific knowledge, designed to streamline the manual evaluation process typically conducted by domain experts. Inferring over and extracting information from Large Language Models (LLMs) trained on a large corpus of scientific literature can potentially define a step change in biomedical discovery, reducing the barriers for accessing and integrating existing medical evidence. This work explores the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery.
Methods:
The framework involves three evaluation steps, each assessing different aspects sequentially: fluency, prompt alignment, semantic coherence, factual knowledge, and specificity of the generated responses. By splitting these tasks between non-experts and experts, the framework reduces the effort required from the latter. The work provides a systematic assessment on the ability of eleven state-of-the-art LLMs, including ChatGPT, GPT-4 and Llama 2, in two prompting-based tasks: chemical compound definition generation and chemical compound–fungus relation determination.
Results:
Although recent models have improved in fluency, factual accuracy is still low and models are biased towards over-represented entities. The ability of LLMs to serve as biomedical knowledge bases is questioned, and the need for additional systematic evaluation frameworks is highlighted.
Conclusion:
While LLMs are currently not fit for purpose to be used as biomedical factual knowledge bases in a zero-shot setting, there is a promising emerging property in the direction of factuality as the models become domain specialised, scale up in size and level of human feedback.}
}
@article{KIOURTIS2019104002,
title = {Aggregating the syntactic and semantic similarity of healthcare data towards their transformation to HL7 FHIR through ontology matching},
journal = {International Journal of Medical Informatics},
volume = {132},
pages = {104002},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.104002},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618309377},
author = {Athanasios Kiourtis and Sokratis Nifakos and Argyro Mavrogiorgou and Dimosthenis Kyriazis},
keywords = {Healthcare 4.0, Ontology matching, Syntactic similarity, Semantic similarity, HL7 FHIR},
abstract = {Background and objective
Healthcare systems deal with multiple challenges in releasing information from data silos, finding it almost impossible to be implemented, maintained and upgraded, with difficulties ranging in the technical, security and human interaction fields. Currently, the increasing availability of health data is demanding data-driven approaches, bringing the opportunities to automate healthcare related tasks, providing better disease detection, more accurate prognosis, faster clinical research advance and better fit for patient management. In order to share data with as many stakeholders as possible, interoperability is the only sustainable way for letting systems to talk with one another and getting the complete image of a patient. Thus, it becomes clear that an efficient solution in the data exchange incompatibility is of extreme importance. Consequently, interoperability can develop a communication framework between non-communicable systems, which can be achieved through transforming healthcare data into ontologies. However, the multidimensionality of healthcare domain and the way that is conceptualized, results in the creation of different ontologies with contradicting or overlapping parts. Thus, an effective solution to this problem is the development of methods for finding matches among the various components of ontologies in healthcare, in order to facilitate semantic interoperability.
Methods
The proposed mechanism promises healthcare interoperability through the transformation of healthcare data into the corresponding HL7 FHIR structure. In more detail, it aims at building ontologies of healthcare data, which are later stored into a triplestore. Afterwards, for each constructed ontology the syntactic and semantic similarities with the various HL7 FHIR Resources ontologies are calculated, based on their Levenshtein distance and their semantic fingerprints accordingly. Henceforth, after the aggregation of these results, the matching to the HL7 FHIR Resources takes place, translating the healthcare data into a widely adopted medical standard.
Results
Through the derived results it can be seen that there exist cases that an ontology has been matched to a specific HL7 FHIR Resource due to its syntactic similarity, whereas the same ontology has been matched to a different HL7 FHIR Resource due to its semantic similarity. Nevertheless, the developed mechanism performed well since its matching results had exact match with the manual ontology matching results, which are considered as a reference value of high quality and accuracy. Moreover, in order to furtherly investigate the quality of the developed mechanism, it was also evaluated through its comparison with the Alignment API, as well as the non-dominated sorting genetic algorithm (NSGA-III) which provide ontology alignment. In both cases, the results of all the different implementations were almost identical, proving the developed mechanism’s high efficiency, whereas through the comparison with the NSGA-III algorithm, it was observed that the developed mechanism needs additional improvements, through a potential adoption of the NSGA-III technique.
Conclusions
The developed mechanism creates new opportunities in conquering the field of healthcare interoperability. However, according to the mechanism’s evaluation results, it is almost impossible to create syntactic or semantic patterns for understanding the nature of a healthcare dataset. Hence, additional work should be performed in evaluating the developed mechanism, and updating it with respect to the results that will derive from its comparison with similar ontology matching mechanisms and data of multiple nature.}
}
@article{MOHAMMADI2020100592,
title = {Evaluating and comparing ontology alignment systems: An MCDM approach},
journal = {Journal of Web Semantics},
volume = {64},
pages = {100592},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100592},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300330},
author = {Majid Mohammadi and Jafar Rezaei},
keywords = {Ontology alignment, Ranking, Evaluation, MCDM, Bayesian BWM},
abstract = {Ontology alignment is vital in Semantic Web technologies with numerous applications in diverse disciplines. Due to diversity and abundance of ontology alignment systems, a proper evaluation can portray the evolution of ontology alignment and depicts the efficiency of a system for a particular domain. Evaluation can help system designers recognize the strength and shortcomings of their systems, and aid application developers to select a proper alignment system. This article presents a new evaluation and comparison methodology based on multiple performance metrics that accommodates experts’ preferences via a multi-criteria decision-making (MCDM) method, i.e., Bayesian best–worst method (BWM). First, the importance of a performance metric for a specific task/application is determined according to experts’ preferences. The alignment systems are then evaluated based on proposed expert-based collective performance (ECP) that takes into account multiple metrics as well as their calibrated importance. For comparison, the alignment systems are ranked based on a probabilistic scheme, where it includes the extent to which one alignment system is preferred over another. The proposed methodology is applied to six tracks from ontology alignment evaluation initiative (OAEI), where the importance of performance metrics are calibrated by designing a survey and eliciting the preferences of ontology alignment experts. Accordingly, the participating alignment systems in the OAEI 2018 are evaluated and ranked. While the proposed methodology is applied to six OAEI tracks to demonstrate its applicability, it can also be applied to any benchmark or application of ontology alignment.}
}
@article{ALAKA2023,
title = {Models and Approaches for Comprehension of Dysarthric Speech Using Natural Language Processing: Systematic Review},
journal = {JMIR Rehabilitation and Assistive Technologies},
volume = {10},
year = {2023},
issn = {2369-2529},
doi = {https://doi.org/10.2196/44489},
url = {https://www.sciencedirect.com/science/article/pii/S2369252923000376},
author = {Benard Alaka and Bernard Shibwabo},
keywords = {dysarthria, speech comprehension, speech contextualization, meaning extraction, ontology extraction, familiarity, topic knowledge},
abstract = {Background
Speech intelligibility and speech comprehension for dysarthric speech has attracted much attention recently. Dysarthria is characterized by irregularities in the speed, strength, pitch, breath control, range, steadiness, and accuracy of muscle movements required for articulatory aspects of speech production.
Objective
This study examined the contributions made by other studies involved in dysarthric speech comprehension. We focused on the modes of meaning extraction used in generalizing speaker-listener underpinnings in light of semantic ontology extraction as a desired technique, applied method types, speech representations used, and databases sourced from.
Methods
This study involved a systematic literature review using 7 electronic databases: Cochrane Database of Systematic Reviews, Web of Science Core Collection, Scopus, PubMed, ACM, IEEE Xplore, and Google Scholar. The main eligibility criterion was the extraction of meaning from dysarthric speech using natural language processing or understanding approaches to improve on dysarthric speech comprehension. In total, out of 834 search results, 30 studies that matched the eligibility requirements were acquired following screening by 2 independent reviewers, with a lack of consensus being resolved through joint discussion or consultation with a third party. In order to evaluate the studies’ methodological quality, the risk of bias assessment was based on the Cochrane risk-of-bias tool version 2 (RoB2) with 23 of the studies (77%) registering low risk of bias and 7 studies (33%) raising some concern over the risk of bias. The overall quality assessment of the study was done using TRIPOD (Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis).
Results
Following a review of 30 primary studies, this study revealed that the reviewed studies focused on natural language understanding or clinical approaches, with an increase in proposed solutions from 2020 onwards. Most studies relied on speaker-dependent speech features, while others used speech patterns, semantic knowledge, or hybrid approaches. The prevalent use of vector representation aligned with natural language understanding models, while Mel-frequency cepstral coefficient representation and no representation approaches were applied in neural networks. Hybrid representation studies aimed to reconstruct dysarthric speech or improve comprehension. Comprehensive databases, like TORGO and UA-Speech, were commonly used in combination with other curated databases, while primary data was preferred for specific or unique research objectives.
Conclusions
We found significant gaps in dysarthric speech comprehension characterized by the lack of inclusion of important listener or speech-independent features in the speech representations, mode of extraction, and data sources used. Further research is therefore proposed regarding the formulation of models that accommodate listener and speech-independent features through semantic ontologies that will be useful in the inclusion of key features of listener and speech-independent features for meaning extraction of dysarthric speech.}
}
@article{JAROSLAW20182238,
title = {An Attempt to Knowledge Conceptualization of Methods and Tools Supporting Ontology Evaluation Process},
journal = {Procedia Computer Science},
volume = {126},
pages = {2238-2247},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.225},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918311992},
author = {Wątróbski Jarosław},
keywords = {ontology, ontology evaluation process, methods, tools supporting ontology evaluation process},
abstract = {In the recent years, a growing popularity of knowledge management and knowledge conceptualization in the form of ontology is observed. However, one of the most important and up-to-date challenges infolds the quality evaluation of knowledge conceptualization. The prospective mistakes and omissions in ontologies have an influence on the exploitation of the full potential of exchanging data. For this reason, the process of ontology evaluation has a great impact on the higher degree of reuse and a better cooperation over the boundaries of applications and domains. However, this process is quite complex and it requires the specified knowledge about available methods and tools supporting the ontology evaluation process. This paper discusses existing evaluation metrics supporting the ontology evaluation process. Based on this, the author’s proposal of ontology-based approach for evaluation metrics is provided, offering a useful guidance to knowledge handling, conceptualization and representation, and using knowledge engineering.}
}
@article{CHEN2025104213,
title = {AECR: Automatic attack technique intelligence extraction based on fine-tuned large language model},
journal = {Computers & Security},
volume = {150},
pages = {104213},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104213},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005194},
author = {Minghao Chen and Kaijie Zhu and Bin Lu and Ding Li and Qingjun Yuan and Yuefei Zhu},
keywords = {Cyber threat intelligence (CTI), Attack technique extraction, Prompt engineering, Large language model (LLM), Advanced persistent threat (APT)},
abstract = {Cyber Threat Intelligence (CTI) reports contain resourceful intelligence on cyber-attack campaigns, which provides great help for security analysts to infer attack trends and enhance their defenses. However, due to the diversity of report content and writing styles, current intelligence extraction is mostly based on time-consuming manual efforts. Moreover, existing automatic methods generally neglect the importance of background knowledge and produce inexact extraction results. These problems prevent the effective utilization and sharing of intelligence from CTI reports. In this paper, we primarily focus on the automatic extraction of attack technique (AT) intelligence, which reveals patterns of attack behaviors and hardly changes over time. We propose a novel automatic AT extraction pipeline for CTI reports (AECR). AECR explores the feasibility of extracting AT intelligence based on a fined-tuned large language model (LLM). Particularly, we endow the selected LLM with enhanced domain-specific knowledge to improve its comprehension of AT-relevant content and alleviate the hallucination problem. Experimental results demonstrate that AECR outperforms state-of-the-art methods by a wide margin with a reasonable time cost. Specifically, we improve the accuracy, precision, recall, and F1-score by 108%, 37.2%, 22.4%, and 67.5% respectively. To the best of our knowledge, AECR is the first to perform AT extraction based on fine-tuned LLM.}
}
@article{MARAN2018152,
title = {Domain content querying using ontology-based context-awareness in information systems},
journal = {Data & Knowledge Engineering},
volume = {115},
pages = {152-173},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17301428},
author = {Vinícius Maran and Alencar Machado and Guilherme Medeiros Machado and Iara Augustin and José Palazzo M. {de Oliveira}},
keywords = {Ontology, Context-awareness, Information systems, Ubiquitous computing},
abstract = {Ubiquitous computing technologies have been applied in several areas. However, it still presents a number of challenges, both for the full implementation of technologies and for the integration with existing information systems. One of the main mismatches evidenced by recent works is how context-awareness, a widely used capability in ubiquitous computing and actual information systems with relational databases may be integrated to allow ubiquitous and traditional systems to query relational data sources without the necessity to modify the schema of the database. This paper presents an integration model relating context and domain information allowing relational data to be retrieved in context without the necessity to change the originally used relational queries. A set of linking rules and algorithms are formalized in a model and this model is implemented in a prototype. The evaluation of the model is performed by applying it in a case study in a Massive Open Online Course (MOOC) platform. The evaluation of the model by the application of it in a case study in a MOOC platform demonstrated the possibility to use an ontology frequently used in ubiquitous middleware as an extra filtering layer for information systems without the necessity to recreate queries or make a re-engineering in the relational database schema. The results of the queries after the application of the model showed an average decrease of 21% in returned tuples, which was evaluated as a significant reduce in tuple results.}
}
@incollection{KUMBURU2023333,
title = {Chapter 20 - Ontology-based knowledge management framework in business organizations and water users networks in Tanzania},
editor = {Saeid Eslamian and Faezeh Eslamian},
booktitle = {Handbook of Hydroinformatics},
publisher = {Elsevier},
pages = {333-348},
year = {2023},
isbn = {978-0-12-821285-1},
doi = {https://doi.org/10.1016/B978-0-12-821285-1.00014-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128212851000142},
author = {Neema Penance Kumburu},
keywords = {Ontology, Knowledge management, Water user networks, Business organizations, Tanzania},
abstract = {This chapter portrays that ontology is the formal depiction of ideas and their shared relations. Ontology of knowledge management is considered effective in enhancing performance of organizations water users’ networks inclusive as well as its competitiveness due to its potential to enhance knowledge acquisition, knowledge representation, knowledge organization, knowledge manning, and retrieval. In this chapter, a framework of ontology of knowledge management that can be used in business organization and water users’ networks in Tanzania is proposed and presented. The chapter also shows how knowledge acquisition, knowledge representation, knowledge organization, knowledge manning, and retrieval can enhance performance of business organization and water users’ networks without forgetting to show how technology, culture, structure, and people can facilitate attainment of organizations competitive advantage. The chapter further shows that most business organizations and water users’ networks in Tanzania lack framework of ontology-based knowledge management that creates challenges to the enterprise due to ambiguity and unstructured nature of knowledge management in business. It is therefore recommended that in order to manage knowledge effectively, management should invest substantial capital in establishing structure and systems of knowledge management, educate employees on the various aspects of knowledge management ontology as well as fostering knowledge sharing culture based on trust.}
}
@article{STADNICKI2020753,
title = {Towards a Modern Ontology Development Environment},
journal = {Procedia Computer Science},
volume = {176},
pages = {753-762},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.070},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319657},
author = {Adrian Stadnicki and Filip {Filip Pietroń} and Patryk Burek},
keywords = {Knowledge-Based Systems, Knowledge Representation, Management, Ontology Engineering, Semantic Web},
abstract = {Ontologies provide engineers and developers with an unambiguous, verifiable, and expandable knowledge base related to a certain domain. Every project that requires control over consistent knowledge, which is especially relatable when using artificial intelligence with datasets increasing in size every second, would reap benefits from adding ontologies to the equation. It is a powerful asset enabling the development of a project with integrity between platforms or teams. Unfortunately, the cost of entry for a developer into the ontology engineering area is high, as it has been proven over the last decades that developing an ontology is a complex, collaborative task, which requires the support of an adequate methodology as well as software tools. The current paper’s objective is twofold. First, it provides a survey on the methodology and software tools used for the creation of the ontology, its maintenance and collaboration. The paper investigates how the tools evolved over the years and what trends have emerged. Second, as the result of the analysis conducted, we show that current solutions have deficiencies and a technological debt; therefore, we present our plan to build a modern tool that uses state-of-the-art technology.}
}
@article{DELUSIGNAN2020,
title = {COVID-19 Surveillance in a Primary Care Sentinel Network: In-Pandemic Development of an Application Ontology},
journal = {JMIR Public Health and Surveillance},
volume = {6},
number = {4},
year = {2020},
issn = {2369-2960},
doi = {https://doi.org/10.2196/21434},
url = {https://www.sciencedirect.com/science/article/pii/S2369296020001611},
author = {Simon {de Lusignan} and Harshana Liyanage and Dylan McGagh and Bhautesh Dinesh Jani and Jorgen Bauwens and Rachel Byford and Dai Evans and Tom Fahey and Trisha Greenhalgh and Nicholas Jones and Frances S Mair and Cecilia Okusi and Vaishnavi Parimalanathan and Jill P Pell and Julian Sherlock and Oscar Tamburis and Manasa Tripathy and Filipa Ferreira and John Williams and F D Richard Hobbs},
keywords = {COVID-19, medical informatics, sentinel surveillance},
abstract = {Background
Creating an ontology for COVID-19 surveillance should help ensure transparency and consistency. Ontologies formalize conceptualizations at either the domain or application level. Application ontologies cross domains and are specified through testable use cases. Our use case was an extension of the role of the Oxford Royal College of General Practitioners (RCGP) Research and Surveillance Centre (RSC) to monitor the current pandemic and become an in-pandemic research platform.
Objective
This study aimed to develop an application ontology for COVID-19 that can be deployed across the various use-case domains of the RCGP RSC research and surveillance activities.
Methods
We described our domain-specific use case. The actor was the RCGP RSC sentinel network, the system was the course of the COVID-19 pandemic, and the outcomes were the spread and effect of mitigation measures. We used our established 3-step method to develop the ontology, separating ontological concept development from code mapping and data extract validation. We developed a coding system–independent COVID-19 case identification algorithm. As there were no gold-standard pandemic surveillance ontologies, we conducted a rapid Delphi consensus exercise through the International Medical Informatics Association Primary Health Care Informatics working group and extended networks.
Results
Our use-case domains included primary care, public health, virology, clinical research, and clinical informatics. Our ontology supported (1) case identification, microbiological sampling, and health outcomes at an individual practice and at the national level; (2) feedback through a dashboard; (3) a national observatory; (4) regular updates for Public Health England; and (5) transformation of a sentinel network into a trial platform. We have identified a total of 19,115 people with a definite COVID-19 status, 5226 probable cases, and 74,293 people with possible COVID-19, within the RCGP RSC network (N=5,370,225).
Conclusions
The underpinning structure of our ontological approach has coped with multiple clinical coding challenges. At a time when there is uncertainty about international comparisons, clarity about the basis on which case definitions and outcomes are made from routine data is essential.}
}
@article{SANDKUHL20191609,
title = {Facilitating Digital Transformation by Multi-Aspect Ontologies: Approach and Application Steps},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1609-1614},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.430},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319314119},
author = {Kurt Sandkuhl and Nikolay Shilov and Alexander Smirnov},
keywords = {Digital transformation, digitalization, ontology, multi-aspect ontology},
abstract = {Today, companies feel a need to invest in digital transformation due to customer demand and market pressure, but at the same time also experience many challenges in planning and implementing digital transformation processes. The aim of the paper is to facilitate this process in the following areas: identification of factors on digital transformation projects to be observed, multi-aspect digital transformation ontology formalizing the elements and interrelationships of these factors with an illustrative example and application steps.}
}
@article{LEE2021114681,
title = {Use of a domain-specific ontology to support automated document categorization at the concept level: Method development and evaluation},
journal = {Expert Systems with Applications},
volume = {174},
pages = {114681},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114681},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421001226},
author = {Yen-Hsien Lee and Paul Jen-Hwa Hu and Wan-Jung Tsao and Liang Li},
keywords = {Automated document categorization, Ontology-based text categorization, Knowledge management, Domain-specific ontology, -nearest neighbors},
abstract = {Voluminous, conveniently accessible textual documents, created and disseminated by modern information technology, makes automated document organization increasingly important for both individuals and organizations. Many existing techniques rely on document content analysis that classifies new, unlabeled documents by examining the similarity based on the overlap between their important features and the representative features of each document category. However, the performance of feature-based techniques can be significantly hindered by word mismatch and ambiguity problems. As a remedy, this study takes a concept-based approach and propose a text categorization method that incorporates a domain-specific ontology to support automated document categorization more effectively. The proposed method classifies documents according to their respective range of relevant concepts. We empirically evaluate our method versus several prevalent benchmarks that include feature-based k-nearest neighbors (kNN) and semantic-based techniques. The results show the proposed method more effective than the benchmark techniques; it achieves better performances when using a complete concept hierarchy without considering the hierarchical relationships among concepts. The proposed method illustrates how to incorporate a domain-specific ontology to improve document classification. Our method is computationally efficient because it produces a concept space of relatively few dimensionalities and does not require semantic space reconstruction as new documents arrive. Moreover, the relationships and patterns for classifying documents, generated by our method, are explicit and comprehensible.}
}
@article{SALGUERO20181,
title = {Ontology-based feature generation to improve accuracy of activity recognition in smart environments},
journal = {Computers & Electrical Engineering},
volume = {68},
pages = {1-13},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2018.03.048},
url = {https://www.sciencedirect.com/science/article/pii/S0045790617315483},
author = {A.G. Salguero and M. Espinilla},
keywords = {Activity recognition, Smart environments, Ambient assisted living (AAL), Activities of Daily Living (ADL), Ontology, Data-Driven approaches, Knowledge-Driven approaches},
abstract = {In recent years, many techniques have been proposed for automatic recognition of Activities of Daily Living from smart home sensor data. However, classifiers usually use features created ad hoc. In this work, the use of ontologies is proposed for the fully automatic generation of these features. The process consists of converting the original dataset into an ontology and then combine all the concepts and relations in that ontology to obtain relevant class expressions. The high formalization of ontologies allows us to reduce the search space by discarding many meaningless expressions, such as contradictory or unsatisfiable expressions. The relevant class expressions are then used as features by the classifiers to build the classification model. To validate our proposal, we have used as reference the results obtained by four different classification algorithms that use the most commonly used features.}
}
@article{KONYS20182208,
title = {Towards Knowledge Handling in Ontology-Based Information Extraction Systems},
journal = {Procedia Computer Science},
volume = {126},
pages = {2208-2218},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.228},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312031},
author = {Agnieszka Konys},
keywords = {Ontology-Based Information Extraction, Information Extraction, ontology, knowledge management},
abstract = {Ontologies proved to be efficient and powerful tools for gathering and sharing knowledge, providing explicit specifications of conceptualizations. However, the proper ontology construction processes and their updating of various data sources, require a huge effort and well-adjusted mechanisms to their extraction, positioning and sharing. In this context, the application of a proper Ontology-Based Information Extraction (OBIE) system may help in solving these problems. The paper is a successful attempt to purvey state-of-the-art of selected OBIE systems, followed by the process of taxonomy construction and aftermath knowledge systematization of particular OBIE approaches.}
}
@article{ALZOGHBY2018206,
title = {Ontological Optimization for Latent Semantic Indexing of Arabic Corpus},
journal = {Procedia Computer Science},
volume = {142},
pages = {206-213},
year = {2018},
note = {Arabic Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.477},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918321811},
author = {Aya M. Al-Zoghby and Khaled Shaalan},
keywords = {LSI, Universal WordNet Ontology, Vector Models, Indexing, Weighting approaches, Dimensionality Reduction, Arabic Text},
abstract = {The dimensionality reduction is a critical problem in the information retrieval process. The higher dimensions directly affect the search performance in terms of Recall and Precision. The dimensionality reduction enabling the search to be semantically based instead of lexically based as the dimensions are defined in terms of the semantic concepts instead of traditional terms or keywords. Latent Semantic Indexing (LSI) is a mathematical extension of the classical Vector Space Model (VSM). LSI is used to discover the latent semantic in the search space by extracting concepts from the original terms in the space. LSI is based on the Singular Value Decomposition (SVD) to reduce the dimension of the term space into a lower dimensional LSI space. In this paper, we propose a methodology for extra optimal LSI dimension reduction via two reduction levels. The first reduction level is based on an ontological conceptualization process. The Universal Wordnet ontology (UWN) is used to develop an ontological based concept space instead of the term space. As a second reduction level, the SVD is applied to the extracted concept space for getting an optimal LSI conceptualization. The experimental results of this research indicate an improvement in the search results in terms of both Precision and Recall as the proposed methodology addresses the Synonymy and Polysemy problems effectively.}
}
@article{MICHELET2024301683,
title = {ChatGPT, Llama, can you write my report? An experiment on assisted digital forensics reports written using (local) large language models},
journal = {Forensic Science International: Digital Investigation},
volume = {48},
pages = {301683},
year = {2024},
note = {DFRWS EU 2024 - Selected Papers from the 11th Annual Digital Forensics Research Conference Europe},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2023.301683},
url = {https://www.sciencedirect.com/science/article/pii/S2666281723002020},
author = {Gaëtan Michelet and Frank Breitinger},
keywords = {Digital forensics investigation, Local large language models, ChatGPT, Report automation, Assisted report writing},
abstract = {Generative AIs, especially Large Language Models (LLMs) such as ChatGPT or Llama, have advanced significantly, positioning them as valuable tools for digital forensics. While initial studies have explored the potential of ChatGPT in the context of investigations, the question of to what extent LLMs can assist the forensic report writing process remains unresolved. To answer the question, this article first examines forensic reports with the goal of generalization (e.g., finding the ‘average structure’ of a report). We then evaluate the strengths and limitations of LLMs for generating the different parts of the forensic report using a case study. This work thus provides valuable insights into the automation of report writing, a critical facet of digital forensics investigations. We conclude that combined with thorough proofreading and corrections, LLMs may assist practitioners during the report writing process but at this point cannot replace them.}
}
@article{KHAN2020112925,
title = {A framework for suspect face retrieval using linguistic descriptions},
journal = {Expert Systems with Applications},
volume = {141},
pages = {112925},
year = {2020},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.112925},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419306438},
author = {Mohd. Aamir Khan and Anand Singh Jalal},
keywords = {Linguistic face retrieval, Face recognition, Linguistic description.},
abstract = {Facial sketch plays a vital role in suspect’s identification and apprehension by law enforcement agencies. The sketch artist develops sketches based on the memory of the eye witness. However, sketch artists are less in number and limited to availability. It is also observed that as time passes, the onlooker even forgets many of the critical attributes, which proves costly in time-sensitive investigations. State-of-the-art sketch-photo retrieval methods have used the sketch to retrieve the suspect’s photograph and ignore the importance of time sensitivity. In this paper, we have proposed a linguistic description based approach for suspect’s photo retrieval. In the proposed approach, the facial attributes and their descriptions are extracted from the input linguistic information using parts of speech and attributes ontology. Attributes are ranked according to their importance in the identification of a suspect. An ensemble classifier which aggregates the predictions of multiple classifiers is used to retrieve a suspect’s image. Experiments are performed on standard datasets. The performance of the proposed approach is evaluated by comparing it with the existing methods of linguistic sketch-based retrieval as well as with sketch to photo retrieval method. Experimental results illustrate that the proposed linguistic-base method using ensemble classification attains auspicious performance as compared to state-of-the-art methods.}
}
@article{MEYER2023,
title = {Aligning Standards Communities for Omics Biodiversity Data: Sustainable Darwin Core-MIxS Interoperability},
journal = {Biodiversity Data Journal},
volume = {11},
year = {2023},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.11.e112420},
url = {https://www.sciencedirect.com/science/article/pii/S131428362300057X},
author = {Raïssa Meyer and Ward Appeltans and William D. Duncan and Mariya Dimitrova and Yi-Ming Gan and Thomas {Stjernegaard Jeppesen} and Christopher Mungall and Deborah L Paul and Pieter Provoost and Tim Robertson and Lynn Schriml and Saara Suominen and Ramona Walls and Maxime Sweetlove and Visotheary Ung and Anton {Van de Putte} and Elycia Wallis and John Wieczorek and Pier Luigi Buttigieg},
keywords = {microbiome, eDNA, biodiversity, information standards, omics, metadata, harmonization, FAIR, MIxS, Darwin Core},
abstract = {The standardization of data, encompassing both primary and contextual information (metadata), plays a pivotal role in facilitating data (re-)use, integration, and knowledge generation. However, the biodiversity and omics communities, converging on omics biodiversity data, have historically developed and adopted their own distinct standards, hindering effective (meta)data integration and collaboration.
In response to this challenge, the Task Group (TG) for Sustainable DwC-MIxS Interoperability was established. Convening experts from the Biodiversity Information Standards (TDWG) and the Genomic Standards Consortium (GSC) alongside external stakeholders, the TG aimed to promote sustainable interoperability between the Minimum Information about any (x) Sequence (MIxS) and Darwin Core (DwC) specifications.
To achieve this goal, the TG utilized the Simple Standard for Sharing Ontology Mappings (SSSOM) to create a comprehensive mapping of DwC keys to MIxS keys. This mapping, combined with the development of the MIxS-DwC extension, enables the incorporation of MIxS core terms into DwC-compliant metadata records, facilitating seamless data exchange between MIxS and DwC user communities.
Through the implementation of this translation layer, data produced in either MIxS- or DwC-compliant formats can now be efficiently brokered, breaking down silos and fostering closer collaboration between the biodiversity and omics communities. To ensure its sustainability and lasting impact, TDWG and GSC have both signed a Memorandum of Understanding (MoU) on creating a continuous model to synchronize their standards. These achievements mark a significant step forward in enhancing data sharing and utilization across domains, thereby unlocking new opportunities for scientific discovery and advancement.}
}
@article{TEVDORADZE202588,
title = {The processing of fictional characters: A synthesis of Culpeper's and Pettersson's models},
journal = {Journal of Pragmatics},
volume = {235},
pages = {88-98},
year = {2025},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2024.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S037821662400211X},
author = {Nino Tevdoradze},
keywords = {Fictional characterisation, An interdisciplinary synthesis of two models of fictional characterisation, The cluster conception, The cognitive stylistic model of characterisation, Pragmatic and cognitive explanations, Sherwood Anderson's },
abstract = {In the last few decades, fictional characterisation has become an issue of an exceptionally big interest in cognitive science. Among many other important works in this area of research, Jonathan Culpeper's cognitive stylistic model is a particularly worthy contribution, which provides pragmatic and cognitive explanations for various insights regarding fictional character processing. This paper proposes that there is a way of further elucidating the picture of characterisation by bringing into Culpeper's model Anders Pettersson's perspective on characterisation, emerging from his cluster conception. The explanation of how this perspective can be added to Culpeper's work will be provided in terms of similarities and differences between their models. My chief aim is to provide an interdisciplinary synthesis of the two models and apply the ideas coming from two different research directions to Sherwood Anderson's story "Mother" from the Winesburg, Ohio cycle.}
}
@article{MAZZOTTI2021101529,
title = {Landscape of biomedical informatics standards and terminologies for clinical sleep medicine research: A systematic review},
journal = {Sleep Medicine Reviews},
volume = {60},
pages = {101529},
year = {2021},
issn = {1087-0792},
doi = {https://doi.org/10.1016/j.smrv.2021.101529},
url = {https://www.sciencedirect.com/science/article/pii/S1087079221001143},
author = {Diego R. Mazzotti},
keywords = {Sleep, Standards, Terminologies, Harmonization, Clinical research informatics},
abstract = {Summary
A systematic literature review was conducted to understand the current landscape of standards and terminologies used in clinical sleep medicine. Literature search on PubMed, EMBASE, Medline and Web of Science was performed in March 2021 using terms related to sleep, terminologies, standards, harmonization, semantics, ontology, and electronic health records (EHR). Systematic review was carried out according to PRISMA. Among 128 included studies, 35 were eligible for review. Articles were broadly classified into six topics: standard terminology efforts, reporting standards, databases and resources, data integration efforts, EHR abstraction and standards for automated sleep scoring. This review highlights the progress and challenges related to establishing computable terminologies in sleep medicine, and identifies gaps, limitations and research opportunities related to data integration that could improve adoption of clinical research informatics in this field. There is a need for the systematic adoption of standardized terminologies in all areas of sleep medicine. Existing data aggregation resources could be leveraged to support the development of an integrated infrastructure and subsequent deployment in EHR systems within sleep centers. Ultimately, the adoption of standardized practices for documenting sleep disorders and related traits facilitates data sharing, thus accelerating discovery and clinical translation of informatics approaches applied to sleep medicine.}
}
@article{ABEL2025105378,
title = {“Assembly Theory” in life-origin models: A critical review},
journal = {BioSystems},
volume = {247},
pages = {105378},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2024.105378},
url = {https://www.sciencedirect.com/science/article/pii/S0303264724002636},
author = {David Lynn Abel},
keywords = {Protolife, Protocells, Abiogenesis, Life origin, Origin of life, Protometabolism, Protocellular metabolomics, ProtoBioCybernetics, Molecular evolution, Chemical evolution, Pre-Darwinian evolution},
abstract = {Any homeostatic protometabolism would have required orchestration of disparate biochemical pathways into integrated circuits. Extraordinarily specific molecular assemblies were also required at the right time and place. Assembly Theory conflated with its cousins—Complexity Theory, Chaos theory, Quantum Mechanics, Irreversible Nonequilibrium Thermodynamics and Molecular Evolution theory— collectively have great naturalistic appeal in hopes of their providing the needed exquisite steering and controls. They collectively offer the best hope of circumventing the need for active selection required to formally orchestrate bona fide formal organization (as opposed to the mere self-ordering of chaos theory) (Abel and Trevors, 2006b). This paper focuses specifically on AT's contribution to naturalistic life-origin models.}
}
@article{ZENG2025100020,
title = {Research on Human-Machine Collaborative Innovation Methods in Design Education Inspired by the Learning Mechanism of Generative Adversarial Networks},
journal = {Design and Artificial Intelligence},
pages = {100020},
year = {2025},
issn = {3050-7413},
doi = {https://doi.org/10.1016/j.daai.2025.100020},
url = {https://www.sciencedirect.com/science/article/pii/S3050741325000205},
author = {Shaoting Zeng and Suihong Lan and Zhengwen Xu and Shanshan Hu and Mengnan Li and Siyu Liu and Tingting Chen},
keywords = {Generative Adversarial Network, Human-Computer Collaborative Design Education, Learning Mechanism Innovation, Knowledge Acquisition and Elicitation, Interior Design Studio Model Innovation},
abstract = {This study explores dual innovations in human-computer collaborative design education methods and learning mechanisms, based on Generative Adversarial Network (GAN) technology and its underlying learning principles, taking the freshman interior design studio as the research platform. By analyzing the coursework submissions from 50 students, 1,250 interior design floor plans were collected, along with 1,250 corresponding annotated images marking work and rest areas, creating a dataset of 1,250 paired inputs for training the Pix2PixGAN model. Subsequently, 52 master-level design floor plans were input into the trained Pix2PixGAN model, allowing the GAN to generate annotations for the distribution of work and rest areas, based on patterns learned from the student data, onto the master-level designs. Students were then tasked with comparing the GAN-generated annotations with the actual annotations in the master designs, analyzing the differences between student and master patterns. Through this process of bottom-up design practice and top-down comparative analysis, students gained insights into the design patterns and expertise of the masters. This research not only integrates Pix2PixGAN technology but also adheres to the learning mechanisms of GANs, achieving dual innovation from both a micro-level technological application and a macro-level learning framework grounded in GAN principles.}
}
@article{LI2024,
title = {A Patient Similarity Network (CHDmap) to Predict Outcomes After Congenital Heart Surgery: Development and Validation Study},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/49138},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424000073},
author = {Haomin Li and Mengying Zhou and Yuhan Sun and Jian Yang and Xian Zeng and Yunxiang Qiu and Yuanyuan Xia and Zhijie Zheng and Jin Yu and Yuqing Feng and Zhuo Shi and Ting Huang and Linhua Tan and Ru Lin and Jianhua Li and Xiangming Fan and Jingjing Ye and Huilong Duan and Shanshan Shi and Qiang Shu},
keywords = {medicine-based evidence, general prediction model, patient similarity, congenital heart disease, echocardiography, postoperative complication, similarity network, heart, cardiology, NLP, natural language processing, predict, predictive, prediction, complications, complication, surgery, surgical, postoperative},
abstract = {Background
Although evidence-based medicine proposes personalized care that considers the best evidence, it still fails to address personal treatment in many real clinical scenarios where the complexity of the situation makes none of the available evidence applicable. “Medicine-based evidence” (MBE), in which big data and machine learning techniques are embraced to derive treatment responses from appropriately matched patients in real-world clinical practice, was proposed. However, many challenges remain in translating this conceptual framework into practice.
Objective
This study aimed to technically translate the MBE conceptual framework into practice and evaluate its performance in providing general decision support services for outcomes after congenital heart disease (CHD) surgery.
Methods
Data from 4774 CHD surgeries were collected. A total of 66 indicators and all diagnoses were extracted from each echocardiographic report using natural language processing technology. Combined with some basic clinical and surgical information, the distances between each patient were measured by a series of calculation formulas. Inspired by structure-mapping theory, the fusion of distances between different dimensions can be modulated by clinical experts. In addition to supporting direct analogical reasoning, a machine learning model can be constructed based on similar patients to provide personalized prediction. A user-operable patient similarity network (PSN) of CHD called CHDmap was proposed and developed to provide general decision support services based on the MBE approach.
Results
Using 256 CHD cases, CHDmap was evaluated on 2 different types of postoperative prognostic prediction tasks: a binary classification task to predict postoperative complications and a multiple classification task to predict mechanical ventilation duration. A simple poll of the k-most similar patients provided by the PSN can achieve better prediction results than the average performance of 3 clinicians. Constructing logistic regression models for prediction using similar patients obtained from the PSN can further improve the performance of the 2 tasks (best area under the receiver operating characteristic curve=0.810 and 0.926, respectively). With the support of CHDmap, clinicians substantially improved their predictive capabilities.
Conclusions
Without individual optimization, CHDmap demonstrates competitive performance compared to clinical experts. In addition, CHDmap has the advantage of enabling clinicians to use their superior cognitive abilities in conjunction with it to make decisions that are sometimes even superior to those made using artificial intelligence models. The MBE approach can be embraced in clinical practice, and its full potential can be realized.}
}
@article{ZHANG2024e27603,
title = {Mechanisms of action of Shizhenqing granules for eczema treatment: Network pharmacology analysis and experimental validation},
journal = {Heliyon},
volume = {10},
number = {6},
pages = {e27603},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e27603},
url = {https://www.sciencedirect.com/science/article/pii/S240584402403634X},
author = {Hairong Zhang and Zhenbo Li and Yike Sun and Wenna Li and Xiao Sun and Yapeng Zhang and Leilei Liu and Shuran Ma},
keywords = {Shizhenqing granules, Eczema, Network pharmacology, Molecular docking, Inflammatory response},
abstract = {Background
Jiuwan decoction has been used to treat chronic eczema since the Qing Dynasty. According to clinical experience, Shizhenqing granules (SZQG), derived from the Jiuwan decoction, exert beneficial clinical effects on acute eczema and reduce recurrence. Therefore, we elucidated the underlying mechanisms of SZQG through network pharmacology, molecular docking, and experimental validation.
Methods
The main chemical components of SZQG were identified by ultra-high-performance liquid chromatography-tandem mass spectrometry (UHPLC-MS/MS). And the targets of SZQG against eczema were screened out through online databases. Then, the regulatory network map of the “herbal compound–potential target” and the target protein-protein interaction (PPI) network was constructed. The Gene Ontology (GO) analysis and the Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analyses were conducted using by R language. Additionally, the interaction between the active compounds and the targets was verified by molecular docking technology. Finally, an experiment in vivo was used to verify the effect and mechanism of SZQG on eczema.
Results
Using UHPLC-MS/MS, 158 main chemical compounds of SZQG were identified, and 72 compounds were selected according to the criteria for further analysis. All 237 potential targets of SZQG in eczema were explored using multiple online databases. The network with 14 core targets was screened out, including STAT3, RELA, TNF, JUN, MAPK3, IL-6, PIK3CA, STAT1, MAPK14, MAPK1, IL-4, NFKBIA, IL1B, and MYC. KEGG analyses indicated that the therapeutic effects of SZQG on eczema were predominantly associated with cytokine-cytokine receptor interaction, TNF, MAPK, NF-κB, toll-like receptor, T cell receptor, and Th1 and Th2 cell differentiation signaling pathways. Furthermore, the good affinity between the core compounds and core targets was verified by molecular docking technology, particularly for RELA and MAPK. Animal experiments revealed that SZQG downregulated MAPK14, RELA, T-bet, and GATA3 mRNA expression, reduced immunoglobulin E (IgE) and interleukin-4 (IL-4) serum concentrations, and improved eczema-like lesions in model rats.
Conclusion
This study identified potential targets and signaling pathways of SZQG in the treatment of eczema, whereby RELA and MAPK14 may constitute the main therapeutic targets of SZQG in cytokine regulation and reduction of inflammatory responses.}
}
@article{HARUNA2025103578,
title = {AddManBERT: A combinatorial triples extraction and classification task for establishing a knowledge graph to facilitate design for additive manufacturing},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103578},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103578},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004719},
author = {Auwal Haruna and Khandaker Noman and Yongbo Li and Xin Wang and Md Junayed Hasan and Ahmad Bala Alhassan},
keywords = {Knowledge graph, BERT model, Textual Data, Additive manufacturing, Triples extraction and classification},
abstract = {In recent years, triple extraction and classification have received attention in the context of Additive Manufacturing (AM). However, the lack of a formalized process to extract and classify triple from textual data poses challenges for the effective embedding learning techniques in utilizing AM’s product innovation and manufacturing capabilities. Hence, the AM field’s manual cognitive process hinders the broader adoption of Design for AM (DFAM) in manufacturing. Aiming to solve these challenging problems, this research proposes a Natural Language Processing (NLP) and Knowledge Graph (KG) methodology for triple extraction and classification from textual data to provide an embedding learning approach. Initially, multi-source textual data for triple extraction and classification is developed. Then, AM Bidirectional Encoder Representation from the Transformers (AddManBERT) is used for triple extraction and classification. The AddManBERT utilizes dependency parsing to determine the semantic relations between the entities for triple extraction and classification. Consequently, the AddManBERT transformed each extracted piece of knowledge from the textual data into a 768-dimensional vector structure by analyzing the projected probability of the output within the center word based on the token embedding surrounding the input. The triples extracted and classified are then saved in the Neo4j database and displayed as graph nodes. An experiment and an application case study verify the proposed method’s efficacy. The experiment results indicate that the proposed method outperforms the traditional centralized approaches in responsiveness, classification accuracy, and prediction efficiency.}
}
@article{MOHAN2024103152,
title = {De-risking, re-balancing and recentralising: Intra-state relations in Chinese-backed transport infrastructure projects in Europe},
journal = {Political Geography},
volume = {113},
pages = {103152},
year = {2024},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2024.103152},
url = {https://www.sciencedirect.com/science/article/pii/S096262982400101X},
author = {Giles Mohan and Filippo Boni and Samuel Rogers and Florian Schaefer and Yue Wang},
keywords = {China, Europe, Central-local, The state, Infrastructure, Investment, Polymorphy},
abstract = {China's Belt and Road Initiative is the most visible manifestation of the country's wider internationalisation efforts in which infrastructure connectivity projects are central. Existing spatialised narratives of these projects have usefully focused on long-standing geopolitical binaries and bilateral state relations, as well as newer spatial ontologies of corridors, zones and networks. Yet they tend to underplay central-local state relations in the countries receiving Chinese infrastructure investment and so this paper examines these intra-state dynamics through three case studies of Chinese-backed transport projects in Germany, Italy and Hungary. Using Jessop, Brenner and Jones' ‘TPSN’ approach we argue that the promise of these infrastructure projects was virtuous insertion of places into global production networks, but in practice we see the central state over-riding local political actors. In Germany and Italy this is in the name of ‘de-risking’ Chinese investments whereby the re-centralisation of state power is a response to a perceived ‘China threat’. In Hungary, the centralised regime uses major infrastructure for legitimatory purposes and uses the growing connectivity to China as an Eastwards balance to its strained relations with Western Europe. We conclude by arguing that greater attentiveness to spatiality and power are needed in future studies of ‘de-risking’.}
}
@article{J2018426,
title = {Sentiment Classification of Tweets with Non-Language Features},
journal = {Procedia Computer Science},
volume = {143},
pages = {426-433},
year = {2018},
note = {8th International Conference on Advances in Computing & Communications (ICACC-2018)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.414},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918321112},
author = {Akilandeswari J and Jothi G},
keywords = {Sentiment Analysis, Opinion Mining, Microblogging Services, Scoring Model, Classification, Social Media Analysis},
abstract = {In recent years, mining social media sites like Twitter, Facebook have become a hot research focus. Twitter is one of the most fashionable microblogging services that permit users to express their views on e-commerce websites, sports, politics, modern technologies, movies, spirituality and so on. Sentiment can be considered as a natural expression of viewer’s perception. It is extremely difficult to identify the sentiment/opinion about a specific product or event by collecting and compiling microblog tasks manually. It is only fair to develop a system which collects, compile and analyse microblog tasks to arrive on insights that helps to take an action against an event. The system can monitor and evaluate in real time online views, to demonstrate how the whole world is reacting to a concept/ideology/event. Developing such a system which assigns polarity to a tweet is a hard task. In this paper we propose a scoring methodology to find the sentiment polarity of the Twitter messages. Emotions, shortened words and non-language features are integrated to increase the significance of the score computed for assigning the polarity for the tweets. The experimental results show that the proposed method enhances the accuracy of the assignment of polarity to tweets.}
}
@article{HAO2025112579,
title = {FLL2 modulates Arabidopsis development and stress tolerance via polyadenylation and CPSF73 interaction},
journal = {iScience},
volume = {28},
number = {6},
pages = {112579},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.112579},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225008405},
author = {Saiqi Hao and Danhui Zhao and Congting Ye and Haidong Qu and Xiangming Zhou and Qingshun Q. Li},
keywords = {Natural sciences, Plant Biology, Plant physiology, Plant development},
abstract = {Summary
The coiled-coil protein FLL2 is known to promote liquid–liquid phase separation in the polyadenylation complex, however, the role of FLL2 in polyadenylation remains unclear. Here, we investigated the impact of a developmental mutant sof78 in FLL2 gene of Arabidopsis leading to many effects on primary root, flowering time and developmental alterations, and tolerance to oxidative stress. To explore the relationship between FLL2 and CPSF73, we employed a boron-containing benzoxaborole compound AN3661, a known inhibitor of the poly(A) factor CPSF73. Our results indicate when treated with AN3661, the phenotypic responses of the sof78 mutant were similar to that control line C2, suggesting both CPSF73 and FLL2 co-modulated the growth and adaptation. Using poly(A) tag sequencing (PAT-seq) to compile poly(A) sites at the transcriptome level, we discovered 2.2%–12.8% of significant differentially expressed alternative polyadenylation (APA) events were changed in sof78, upon AN3661 or oxidative treatment. Under such conditions, transcripts in sof78 exhibited a preference of less U and more G in their poly(A) signals, along with decreased the usage of canonical poly(A) sites in 3′UTRs but increased the usage of non-canonical poly(A) sites in other genic regions. Gene ontology analyses demonstrate that these APA genes are enriched in abiotic stress, osmotic stress, auxin signaling, and ethylene signaling pathways, which were identified and linked to respective phenotypes. These results provide functional linkages between FLL2 and CPSF73 mediated by APA in critical genes for plant development and environmental responses.}
}
@article{FARGHALY20211036,
title = {Digital information technologies for prevention through design (PtD): a literature review and directions for future research},
journal = {Construction Innovation: Information, Process, Management},
volume = {22},
number = {4},
pages = {1036-1058},
year = {2021},
issn = {1471-4175},
doi = {https://doi.org/10.1108/CI-02-2021-0027},
url = {https://www.sciencedirect.com/science/article/pii/S1471417521000580},
author = {Karim Farghaly and William Collinge and Mojgan Hadi Mosleh and Patrick Manu and Clara Man Cheung},
keywords = {BIM, Construction safety, Digital design, Design for safety, Prevention through design, Safety management, Design and construction, Information systems/management, Health and safety, IT building design’s construction, Virtual reality and visualization},
abstract = {Purpose
With the rapid development of digital information and modelling software applications for construction, questions have arisen about their impact on construction safety. Meanwhile, recognition that designers can help reduce risks involved in construction, operation and maintenance via a prevention through design (PtD) approach (also known as design for safety) highlights the significance of digital technologies and tools to PtD. Thus, this paper aims to provide a systematic review of a wide range of digital technologies for enhancing PtD.
Design/methodology/approach
A five-stage systematic literature review with coding and synthesis of findings is presented. The review covers journal articles published between 2000 and 2020 related to the applications of various digital technologies, such as building information modelling (BIM), 4D, databases, ontologies, serious games, virtual reality and augmented reality, for addressing safety issues during the design phase in construction.
Findings
Analysis of the articles yielded a categorisation of the digital applications for PtD into four main areas: knowledge-based systems; automatic rule checking; hazard visualization; and safety training for designers. The review also highlighted designers’ limited knowledge towards construction safety and the possibility to address this by using gaming environments for educating designers on safety management and using artificial intelligence for predicting hazards and risks during design stage in a BIM environment. Additionally, the review proposes other directions for future research to enhance the use of digital technologies for PtD.
Originality/value
This paper contextualises current digital technology applications for construction health and safety and enables future directions of research in the field to be identified and mapped out.}
}
@article{GUOQIANREN2019100828,
title = {Developing an information exchange scheme concerning value for money assessment in Public-Private Partnerships},
journal = {Journal of Building Engineering},
volume = {25},
pages = {100828},
year = {2019},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2019.100828},
url = {https://www.sciencedirect.com/science/article/pii/S2352710219305698},
author = { {Guoqian Ren} and Haijiang Li and Rui Ding and Jisong Zhang and Calin Boje and Weishuai Zhang},
keywords = {Public-private partnership (PPP), Value for money, Building information modelling (BIM), Information exchange},
abstract = {Value for money (VFM) as the procurement justification assessment approach in Public-Private Partnerships (PPP) essentially justify the overall project feasibility in both qualitative and quantitative ways. However, the performance measurement in VFM still lacks automation and information exchange schema. Concerning the project management domain within the Architectural–Engineering–Construction/Facility Management (AEC/FM) sectors, Building Information Modelling (BIM) is emerging as a potential solution for owners to automate the process of value for money assessment. Based on the published performance structure in PPP procurement, This paper develops a novel schema to formalize the information exchange needed to support data extraction and performance measurement. The schema was achieved by referencing the standardised Information Delivery Manual (IDM), which specifies the required information exchanges. This proposed method facilitates the automatic VFM assessment thus benefit the PPP procurement. Software development and case study model are presented towards the validation of the schema and methods employed.}
}
@article{BUCHALCEVOVA2019103,
title = {Using ArchiMate to model ISO/IEC 29110 standard for very small entities},
journal = {Computer Standards & Interfaces},
volume = {65},
pages = {103-121},
year = {2019},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0920548918303386},
author = {Alena Buchalcevova},
keywords = {, ArchiMate, Mapping, Modeling, Basic Profile},
abstract = {Paper concentrates on the ISO/IEC 29110 standard aimed at improving systems and software engineering life cycle processes in very small entities, which has been recently developed and started to be implemented worldwide. With the purpose to facilitate development and maintenance of this standard and its implementation in very small entities utilization of ArchiMate is proposed. The ArchiMate is a widely accepted open standard for modeling Enterprise Architecture supported by a variety of modeling tools, which was also successfully used beyond Enterprise Architecture domain. The contribution of this paper lies in (1) the development of the ISO/IEC 29110 Basic Profile Metamodel; (2) the definition of the mapping between the ISO/IEC 29110 Basic Profile Metamodel and ArchiMate; (3) its evaluation using the Bunge-Wand-Weber (BWW) model; (4) the application of the mapping resulting in the development of the ISO/IEC 29110 Basic Profile ArchiMate Model being implemented in a modeling tool that is freely available and can be used by VSEs, assessors and standard developers.}
}
@article{SHIN2023102922,
title = {Standards, dissemination, and best practices in systems biology},
journal = {Current Opinion in Biotechnology},
volume = {81},
pages = {102922},
year = {2023},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2023.102922},
url = {https://www.sciencedirect.com/science/article/pii/S0958166923000320},
author = {Janis Shin and Veronica Porubsky and James Carothers and Herbert M Sauro},
abstract = {The reproducibility of scientific research is crucial to the success of the scientific method. Here, we review the current best practices when publishing mechanistic models in systems biology. We recommend, where possible, to use software engineering strategies such as testing, verification, validation, documentation, versioning, iterative development, and continuous integration. In addition, adhering to the Findable, Accessible, Interoperable, and Reusable modeling principles allows other scientists to collaborate and build off of each other’s work. Existing standards such as Systems Biology Markup Language, CellML, or Simulation Experiment Description Markup Language can greatly improve the likelihood that a published model is reproducible, especially if such models are deposited in well-established model repositories. Where models are published in executable programming languages, the source code and their data should be published as open-source in public code repositories together with any documentation and testing code. For complex models, we recommend container-based solutions where any software dependencies and the run-time context can be easily replicated.}
}
@article{MA2024100393,
title = {Energy-efficient façade design of residential buildings: A critical review},
journal = {Developments in the Built Environment},
volume = {18},
pages = {100393},
year = {2024},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2024.100393},
url = {https://www.sciencedirect.com/science/article/pii/S2666165924000747},
author = {Wei Ma and Xiangyu Wang and Wenchi Shou and Jun Wang},
keywords = {Façade for residential buildings, Energy-efficient façade design, Design research, Domain knowledge, Research guide},
abstract = {Appropriate design of residential façades can save the buildings’ energy consumption on indoor comforts significantly. Research on energy-efficient façade design of residential buildings (EFDRB) has attracted increasing academic interest during the past decade. Although a variety of studies have been conducted diversely, no review has organised the relevant knowledge structurally, leaving the domain research dispersed. This research conducts a literature review of the current EFDRB study based on 62 highly relevant publications from 2011 to -2021 in the field. A descriptive analysis, thematic analysis, and thematic discussion are conducted to reveal the research status quo, outline, contents, and features of the field comprehensively. Correspondingly, an ontological discussion with an explicative domain knowledge map is proposed, followed by insights into future directions. The findings will assist researchers in understanding the field, determining appropriate methods and design strategies, and positioning their work effectively.}
}
@article{MOGHADDASI201950,
title = {“See Figure 1”: Visual moves in discrete mathematics research articles},
journal = {English for Specific Purposes},
volume = {56},
pages = {50-67},
year = {2019},
issn = {0889-4906},
doi = {https://doi.org/10.1016/j.esp.2019.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0889490618301315},
author = {Shahin Moghaddasi and Heather A.B. Graves and Roger Graves and Xavier Gutierrez},
keywords = {Visual communication, Discrete mathematics research articles, Genre analysis, Academic writing, Visual/verbal relationship, Visual rhetoric},
abstract = {This research is motivated by John Swales' genre analysis of academic texts and rhetorical studies of visuals in scientific texts. Swales' approach still generates insights into the rhetorical structure of research genres across disciplines, yet few studies explore their nonverbal aspects. Rhetorical studies of visuals examine images and ignore links with surrounding texts and may overlook ways that the nonverbal contributes to the discipline's intellectual project. ESP genre research has begun multimodal analysis of academic genres but its focus on conference presentations has left a gap in the study of verbal-visual links in other academic genres. This paper addresses this gap in multimodal genre scholarship. After analyzing a corpus of 30 RAs from discrete mathematics to identify the functions of visuals and possible visual–move associations, we found that visuals in two-thirds of the corpus perform three functions: ontological, argumentative, and epistemological. Our data also indicates three multimodal rhetorical moves initiated primarily by visuals. Implications of our findings are that visual moves go beyond textual considerations, they can disrupt the RA's chronological structure, and novice writers in the field would benefit from understanding the crucial associations between the visual representations, disciplinary knowledge, and the rhetorical structure of RAs in this field.}
}
@article{LETICHE2019362,
title = {Is There a Designer?},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {5},
number = {4},
pages = {362-367},
year = {2019},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2019.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2405872619300796},
author = {Hugo Letiche},
keywords = {Object-oriented-ontology, critique of corrolationism, Graham Harman},
abstract = {This is a commentary on Michael Lissack’s two-part article “Understanding is a Design Problem: Cognizing from a Designerly Thinking Perspective.” Michael Lissack and I co-authored Coherence in the Midst of Complexity. In this article, readers will see how our thought has since diverged. While Lissack has increasingly focused on a cognitive interpretation of basic concepts or categories of the mind, I have pursued object-oriented-ontology (O-O-O) and rejected the mind-based thesis. Lissack has chosen for human cognition and the brain as his baseline, while I claim that objects inherently withdraw from perception, and that knowing is ontologically partial and incomplete. The “realness” of objects, whether the objects are mental, cultural, physical, imaginary or whatever, is limitedly accessible. I criticize “corrolationism,” or the assertion that all there is, is mind (or cognition and awareness). I insist that objects, in their enormous variety and complexity, require to be acknowledged and not reduced to epiphenomena dominated by thought; i.e. it is not mind all the way down.}
}
@article{GUERREIRO202024,
title = {Conceptualizing on dynamically stable business processes operation: a literature review on existing concepts},
journal = {Business Process Management Journal},
volume = {27},
number = {1},
pages = {24-54},
year = {2020},
issn = {1463-7154},
doi = {https://doi.org/10.1108/BPMJ-02-2020-0072},
url = {https://www.sciencedirect.com/science/article/pii/S1463715420000308},
author = {Sérgio Guerreiro},
keywords = {Action, Business process, Controllability, Instance, Model, Observability, Operation, Ontology},
abstract = {Purpose
The purposes of this paper are: (1) to identify what types of business process operation controllers are discussed in literature and how can they be classified in order to establish the available body of knowledge in the literature, and then, (2) to identify which concepts are relevant for business process operation control and how are these concepts related in order to offer a reference model for assessing how well are control layers enforced in the dynamic stable nature of an enterprise' business processes operation.
Design/methodology/approach
One cycle of the circular framework for literature review as proposed by Vom Brocke et al. (2009) is followed. Five stages are comprised: (1) definition of review scope (Section 1 and 2), (2) conceptualization of topic (Section 3), (3) literature search (Section 4), (4) literature analysis and synthesis (Section 5), and (5) definition of a research agenda (Section 6 that also concludes the paper). Vom Brocke, J., Simons, A., Niehaves, B., Niehaves, B., Reimer, K., Plattfaut, R., and Cleven, A. (2009), “Reconstructing the giant: on the importance of rigour in documenting the literature search process”, ECIS 2009 Proceedings, Vol. 161.
Findings
Results indicate that (1) many studies exist in the literature, but no integrated knowledge is proposed, hindering the advance of knowledge in this field, (2) a knowledge gap exists between the implemented solutions and the conceptualization needed to generalize the solution to other contexts. Also, the ontology proposed provides a reference model for assessing the maturity of the business process control operation.
Research limitations/implications
The contents contained in the paper needs to be further deepened to include the concepts of “business process management” and “business process mining”, as well as a semantic equivalence study between concepts can integrate better this conceptual framework and identify similarities. Then, the relationship between industries and dynamically stable business processes operation concepts have not yet been fully investigated. Thirdly, the atypical curve of interest that business processes operational control has been receiving in literature is not fully understood.
Practical implications
Some example applications that could benefit from this ontology are (1) security policy for business processes fine grained access control; (2) business processes enforced with decentralized policies, e.g. blockchain; (3) business process compliance and change; or (4) intelligent enterprise decision-making process, e.g. using AI trained neural network to support the human decision to choose if a control actuation is positive or negative instead of relying only on human-based decisions.
Social implications
We understand that business process operation is a dynamically stable system, where steady motion is achieved with the continuous imposition of actor's actions. Therefore, all the work that contribute to the development of knowledge regarding the actor's actions in their execution environment offer the ability to optimize, and/or reengineering, business processes delivering more social value or better social conditions.
Originality/value
In the best of our knowledge this work is unique in the sense that integrates a set of concepts that is rarely, or never, combined. Table 3 corroborates this result.}
}
@article{KESHAN2022413,
title = {Semiautomated process for generating knowledge graphs for marginalized community doctoral-recipients},
journal = {International Journal of Web Information Systems},
volume = {18},
number = {56},
pages = {413-431},
year = {2022},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-02-2022-0046},
url = {https://www.sciencedirect.com/science/article/pii/S1744008422000088},
author = {Neha Keshan and Kathleen Fontaine and James A. Hendler},
keywords = {Semiautomation process, Knowledge graphs, Institute demographics, Graduate mobility, NSF doctoral recipients survey data},
abstract = {Purpose
This paper aims to describe the “InDO: Institute Demographic Ontology” and demonstrates the InDO-based semiautomated process for both generating and extending a knowledge graph to provide a comprehensive resource for marginalized US graduate students. The knowledge graph currently consists of instances related to the semistructured National Science Foundation Survey of Earned Doctorates (NSF SED) 2019 analysis report data tables. These tables contain summary statistics of an institute’s doctoral recipients based on a variety of demographics. Incorporating institute Wikidata links ultimately produces a table of unique, clearly readable data.
Design/methodology/approach
The authors use a customized semantic extract transform and loader (SETLr) script to ingest data from 2019 US doctoral-granting institute tables and preprocessed NSF SED Tables 1, 3, 4 and 9. The generated InDO knowledge graph is evaluated using two methods. First, the authors compare competency questions’ sparql results from both the semiautomatically and manually generated graphs. Second, the authors expand the questions to provide a better picture of an institute’s doctoral-recipient demographics within study fields.
Findings
With some preprocessing and restructuring of the NSF SED highly interlinked tables into a more parsable format, one can build the required knowledge graph using a semiautomated process.
Originality/value
The InDO knowledge graph allows the integration of US doctoral-granting institutes demographic data based on NSF SED data tables and presentation in machine-readable form using a new semiautomated methodology.}
}
@article{JU2020109317,
title = {Materials knowledge reasoning with production based system},
journal = {Computational Materials Science},
volume = {173},
pages = {109317},
year = {2020},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2019.109317},
url = {https://www.sciencedirect.com/science/article/pii/S0927025619306160},
author = {Mengdi Ju and Quan Qian},
keywords = {Materials knowledge, Materials ontology, Production based reasoning},
abstract = {Due to high-throughput experiments and computations, data-generating is relatively easier than before. Currently, materials digital data gradually become one of the key factors of data-driven based novel materials design. Therefore, how to extract useful knowledge from the massive data is a hot topic in materials informatics. In this paper, ontology is used to build a knowledge base, and then production rules and an inference machine are used to derive new knowledge. To realize from the database to the knowledge base of circulation, and then extract the knowledge, further enrich the knowledge base. In experiments, we use stainless steel as an example. And production rules are used to represent the domain knowledge according to the influence of stainless steel metallographic structure and chemical composition on corrosion resistance. And the experimental results show that when adding different rules to the inference engine for reasoning, the knowledge number increases from 202 to 264, 326, 388, 450, 512, 518 and 642 respectively. Moreover, after knowledge reasoning, materials corrosion resistances are added to the knowledge base, including resistance to atmospheric corrosion, acid resistance, stress corrosion crack resistance, resistance to hole corrosion and clearance corrosion, low-temperature strength and toughness and room temperature strength. Therefore, using production-based reasoning, the metallographic structure, corrosion resistance, and materials composition are semantically correlated that can hence enrich the knowledge from the existing data.}
}
@article{RHAYEM2020100206,
title = {Semantic Web Technologies for the Internet of Things: Systematic Literature Review},
journal = {Internet of Things},
volume = {11},
pages = {100206},
year = {2020},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2020.100206},
url = {https://www.sciencedirect.com/science/article/pii/S2542660520300421},
author = {Ahlem Rhayem and Mohamed Ben Ahmed Mhiri and Faiez Gargouri},
keywords = {Internet of Things, Web of Things, Semantic Web Technologies, Ontology, Interoperability, Standards},
abstract = {Nowadays, the use of the Internet of Things (IoT) in diverse applications becomes very popular. Accordingly, a proliferation of objects with remote sensing, actuation, analysis, and sharing capabilities will be interconnected on top of heterogeneous communication networks. Their deployment contexts are continuously changed, which imply a change in their descriptions and characteristics. In addition, they are a fundamental source of a huge quantity of gathered data with different encoding formats. Accordingly, this data is badly expressed, understood and exploited by other systems and devices. From this regard, several challenges associated with standardization, interoperability, discovery, security, and description of IoT resources and their corresponding data have emerged. In this context, Semantic Web Technologies (SWT) seem a suitable and an efficient solution to relieve these challenges. Therefore, a Systematic Literature Review (SLR) methodology is performed to investiagte and analyze a set of the most recent and relevant approaches that deal with SWT in the IoT domain. These approaches are discussed and evaluated based on seven different research questions. Finally, future insights and research opportunities are suggested.}
}
@article{GASTONBRETON2025115350,
title = {Mapping consumer well-being: Contextualization, conceptualization, and classification using topic modeling},
journal = {Journal of Business Research},
volume = {194},
pages = {115350},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2025.115350},
url = {https://www.sciencedirect.com/science/article/pii/S0148296325001730},
author = {Charlotte Gaston-Breton and M. Cristina {De Stefano}},
keywords = {Consumer happiness, Consumer well-being, Content analysis, Systematic review, Topic modeling},
abstract = {A thorough understanding of consumer well-being is crucial for achieving both business success and societal progress. However, the huge body of literature in this domain is causing significant conceptual fragmentation, hindering a clear and unified comprehension. A domain-based systematic review is therefore essential yet challenging, requiring advanced techniques to enhance human analytical capabilities. This study applies topic modeling techniques complemented by metatheoretical content analyses to systematically map 659 academic articles, published in 90 business and management journals in the past six decades. Graphical insights are provided to clearly illustrate how the consumer well-being literature can be organized into 16 topics, along with discussions on the differences and relationships between various approaches and definitions within these topics. An integrative framework is further developed to classify consumer well-being across three levels of analysis (i.e., micro, meso, and macro) and four categories of well-being (i.e., well-thinking, well feeling, well-flourishing, and well-doing).}
}
@article{KARAKUS2025318,
title = {Mechanisms of developmental neurotoxicity of Dechlorane Plus, a recently identified persistent organic pollutant: An in silico study},
journal = {NeuroToxicology},
volume = {108},
pages = {318-327},
year = {2025},
issn = {0161-813X},
doi = {https://doi.org/10.1016/j.neuro.2025.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0161813X25000488},
author = {Fuat Karakuş and Zübeyde Tanrıverdi and Burak Kuzu},
keywords = {Dechlorane Plus, Developmental neurotoxicity, GABRA1, GABRB3},
abstract = {Dechlorane Plus (DP), a polychlorinated flame retardant, has recently been recognized as a persistent organic pollutant. In this study, the molecular mechanisms and targets associated with DP-induced developmental neurotoxicity (DNT) in humans were investigated through network toxicology, multi-level bioinformatics approaches, and molecular docking. Through comprehensive database analysis, 32 potential targets associated with DP-induced DNT were identified. Gene Ontology terms enrichment analysis revealed significant enrichment in pathways related to the nervous system processes, GABA-A receptor complex, and various binding and channel activities. KEGG pathway enrichment analysis indicated that DP-induced DNT is mediated through complex interactions involving neuroactive ligand-receptor interaction pathways. Further analysis using GeneMANIA, STRING, Cytoscape tools, and MCODE identified 11 hub targets, including GABRA1, GABRB1, GABRB3, and GABRG2 as key targets. Molecular docking revealed that DP binds to the GABRB3-GABRA1-GABRG2 protein complex to a degree comparable to the control bicuculline, a potent and selective antagonist of the GABA-A receptor. These findings suggest that DP may have antagonistic effects on the GABA-A receptor, potentially increasing neuronal excitability. This study offers valuable insights into the molecular mechanisms underlying DP-induced DNT and provides data for in vitro or in vivo studies.}
}
@article{DIAZOCHOA2022102359,
title = {Graph neural network modelling as a potentially effective method for predicting and analyzing procedures based on patients' diagnoses},
journal = {Artificial Intelligence in Medicine},
volume = {131},
pages = {102359},
year = {2022},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2022.102359},
url = {https://www.sciencedirect.com/science/article/pii/S0933365722001245},
author = {Juan G. {Diaz Ochoa} and Faizan E Mustafa},
keywords = {Graph neural networks, Recommender systems, Diagnoses, Medical procedures},
abstract = {Background
Currently, the healthcare sector strives to improve the quality of patient care management and to enhance/increase its economic performance/efficiency (e.g., cost-effectiveness) by healthcare providers. The data stored in electronic health records (EHRs) offer the potential to uncover relevant patterns relating to diseases and therapies, which in turn could help identify empirical medical guidelines to reflect best practices in a healthcare system. Based on this pattern of identification model, it is thus possible to implement recommender systems with the notion that a higher volume of procedures is often associated with better high-quality models.
Methods
Although there are several different applications that uses machine learning methods to identify such patterns, such identification is still a challenge, due in part because these methods often ignore the basic structure of the population, or even considering the similarity of diagnoses and patient typology. To this end, we have developed a method based on graph-data representation aimed to cluster ‘similar’ patients. Using such a model, patients will be linked when there is a same and/or similar patterns are being observed amongst them, a concept that will enable the construction of a network-like structure which is called a patient graph.11In several fields in science the concept of network is preferred over the concept of graph, which is commonly employed in informatics. In this article we will continue using the concept of graph due to the fact that the implemented methodology relies on methods coming from informatics but we will point out to the concept of network when we explicitly require a more visual definition that considers nodes interlinked with edges. This structure can be then analyzed by Graph Neural Networks (GNN) to identify relevant labels, and in this case the appropriate medical procedures that will be recommended.
Results
We were able to construct a patient graph structure based on the patient's basic information like age and gender as well as the diagnosis and the trained GNNs models to identify the corresponding patient's therapies using a synthetic patient database. We have even compared our GNN models against different baseline models (using the SCIKIT-learn library of python) and also against the performance of these different model-methods. We have found that the GNNs models are superior, with an average improvement of the f1 score of 6.48 % in respect to the baseline models. In addition, the GNNs models are useful in performing additional clustering analysis which allow a distinctive identification of specific therapeutic/treatment clusters relating to a particular combination of diagnoses.
Conclusions
We found that the GNNs models offer a promising lead to model the distribution of diagnoses in patient population, and is thus a better model in identifying patients with similar phenotype based on the combination of morbidities and/or comorbidities. Nevertheless, network/graph building is still challenging and prone to biases as it is highly dependent on how the ICD distribution affects the patient network embedding space. This graph setup not only requires a high quality of the underlying diagnostic ecosystem, but it also requires a good understanding on how patients at hand are identified by disease respectively. For this reason, additional work is still needed to better improve patient embedding in graph structures for future investigations and the applications of this service-based technology. Therefore, there has not been any interventional study yet.}
}
@article{HO20253945,
title = {Bio-Inspired Algorithms in NLP Techniques: Challenges, Limitations and Its Applications},
journal = {Computers, Materials and Continua},
volume = {83},
number = {3},
pages = {3945-3973},
year = {2025},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2025.063099},
url = {https://www.sciencedirect.com/science/article/pii/S1546221825004461},
author = {Huu-Tuong Ho and Thi-Thuy-Hoai Nguyen and Duong Nguyen Minh Huy and Luong Vuong Nguyen},
keywords = {Natural language processing, bio-inspired, genetic algorithms, ant colony optimization, particle swarm optimization},
abstract = {Natural Language Processing (NLP) has become essential in text classification, sentiment analysis, machine translation, and speech recognition applications. As these tasks become complex, traditional machine learning and deep learning models encounter challenges with optimization, parameter tuning, and handling large-scale, high-dimensional data. Bio-inspired algorithms, which mimic natural processes, offer robust optimization capabilities that can enhance NLP performance by improving feature selection, optimizing model parameters, and integrating adaptive learning mechanisms. This review explores the state-of-the-art applications of bio-inspired algorithms—such as Genetic Algorithms (GA), Particle Swarm Optimization (PSO), and Ant Colony Optimization (ACO)—across core NLP tasks. We analyze their comparative advantages, discuss their integration with neural network models, and address computational and scalability limitations. Through a synthesis of existing research, this paper highlights the unique strengths and current challenges of bio-inspired approaches in NLP, offering insights into hybrid models and lightweight, resource-efficient adaptations for real-time processing. Finally, we outline future research directions that emphasize the development of scalable, effective bio-inspired methods adaptable to evolving data environments.}
}
@article{XU2025200,
title = {Exploring the effect and mechanism of Shaoyao Gancao Decoction in the treatment of pain in Parkinson's disease using network pharmacology and molecular docking},
journal = {IBRO Neuroscience Reports},
volume = {18},
pages = {200-210},
year = {2025},
issn = {2667-2421},
doi = {https://doi.org/10.1016/j.ibneur.2025.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S2667242125000132},
author = {Zhaohui Xu and Qing Zhao},
keywords = {Parkinson's Disease, Pain, , Network pharmacology, Molecular docking},
abstract = {This study explores the potential effects and mechanisms of SGD in treating pain in PD based on network pharmacology and molecular docking technology.The chemical components in the aqueous extract from SGD were identified using GC-MS analysis. A prediction network describing the relationship between SGD and pain in PD was established based on information collected from multiple databases. Using Gene Ontology (GO) enrichment and Kyoto Encyclopedia of Genes and Geomes (KEGG) pathway enrichment of key target genes in the DAVID6.8 database to obtain treatment target genes. To further investigate the molecular interactions, AutoDock Vina were employed to perform molecular docking and visualize the resulting outcomes. There were 206 targets obtained from the 105 active ingredients in Paeoniae Radix Alba and Radix Rhizoma Glycyrrhizae, and 5110 disease targets related to pain in PD were identified. GO enrichment analysis indicates that its Biologica Process (BP) involve response to lipopolysaccharide, response to metal ion. Cellular Component (CC) analysis suggests its primary impact on various membrane structural components. Molecular Function (MF) enrichment results primarily include ubiquitin-like protein ligase binding. KEGG pathway enrichment mainly encompasses MAPK, AGE-RAGE, IL-17, TNF, and Toll-like receptor signaling pathways. According to the results of molecular docking, the binding activity between core components and targets was marvelous (affinity < −5.0 kcal/mol). SGD has more advantages in the regulation of various types of pain in PD through multiple targets, which is worthy of further study.}
}
@article{MORZINGER2018595,
title = {A large-scale framework for storage, access and analysis of time series data in the manufacturing domain},
journal = {Procedia CIRP},
volume = {67},
pages = {595-600},
year = {2018},
note = {11th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 19-21 July 2017, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.12.267},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117312131},
author = {Benjamin Mörzinger and Thomas Weiler and Thomas Trautner and Iman Ayatollahi and Bernhard Angerer and Burkhard Kittl},
keywords = {Linked data, Semantic web, Manufacturing},
abstract = {Time series data from machining process monitoring promises to be a rich resource for optimization applications. Limited data access, however restricts the number of potential applications significantly. Semantic technologies such as ontology based data access could help overcoming those restrictions and therefore pave the way for a wider use of state of the art data analysis applications. Semantic web technologies are not yet widely applied in the manufacturing domain which partly has to do with the fact that in the past no relevant use cases where presented in this area. Therefore, in this paper, semantic technologies and their potential applications are illustrated using an existing research database.}
}
@article{GAO2024723,
title = {Artificial Intelligence in manufacturing: State of the art, perspectives, and future directions},
journal = {CIRP Annals},
volume = {73},
number = {2},
pages = {723-749},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.04.101},
url = {https://www.sciencedirect.com/science/article/pii/S000785062400115X},
author = {Robert X. Gao and Jörg Krüger and Marion Merklein and Hans-Christian Möhring and József Váncza},
keywords = {Artificial intelligence, Smart manufacturing, Machine learning},
abstract = {Inspired by the natural intelligence of humans and bio-evolution, Artificial Intelligence (AI) has seen accelerated growth since the beginning of the 21st century. Successful AI applications have been broadly reported, with Industry 4.0 providing a thematic platform for AI-related research and development in manufacturing. This paper highlights applications of AI in manufacturing, ranging from production system design and planning to process modeling, optimization, quality assurance, maintenance, automated assembly and disassembly. In addition, the paper presents an overview of representative manufacturing problems and matching AI solutions, and a perspective of future research to leverage AI towards the realization of smart manufacturing.}
}
@article{KHALED2019435,
title = {Recommendations-based on semantic analysis of social networks in learning environments},
journal = {Computers in Human Behavior},
volume = {101},
pages = {435-449},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.08.051},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218304266},
author = {Abdelaziz Khaled and Samir Ouchani and Chemseddine Chohra},
keywords = {Web semantics, Web 2.0, Social network analysis, Social learning networks, User's behavior, Community detection},
abstract = {The expansion of social utilization within web applications provides a set of social media that allows users to contribute freely and interact with each other. Consequently, E-learning systems have benefited greatly from the concepts of the social web and the emerging technologies of the web semantic, where students and teachers are living and interacting mainly in the world of Web 2.0 and social networks. Especially, the Web semantic provides content that is understandable by machines and Web 2.0, which help to develop collaborative services. Further it can be integrated in a single social structure semantics that could be a good tool to improve the quality of learning. Our goal is to show that these technologies can be adopted to improve the e-Learning user experience and to provide a full automatic learning platform called iLearn. We present a learning environment formulated as a social network, in order to carry out an automatic semantic reasoning including the interactions between users as well as their relationships with the provided learning resources. We merge the analysis of social networks with the web semantics to go beyond the analysis of social graphs by integrating a treatment semantics of the knowledge that they contain and designed by the ontology formalism. iLearndevelops two ontologies, the first helps to understand the feeling of users versus resources and recommendations, whereas the second categorizes the different resources. In particular, we present an interactive method of detecting communities to provide students belonging to the same learning community the best learning strategies, the strong collaborators candidates, and the relevant resources that meet well within their needs.}
}
@incollection{FORTE2025,
title = {Posthumanist Applied Linguistics},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00577-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041005779},
author = {Magali Forte and Kelleen Toohey},
keywords = {Posthumanism, Indigenous perspectives, Relationality, Assemblage, Intra-action, Entanglement, Critical applied linguistics, Anthropocene, Language education, Post-qualitative inquiry, Affect},
abstract = {This entry contextualizes the emergence of posthumanist perspectives in relation to applied linguistics and explains how they have challenged traditional Western humanist thinking in the field. Rejecting human exceptionalism, posthumanism emphasizes the interconnectedness of humans with more-than-human entities. Many of these ideas resonate with long-standing worldviews at the heart of various and diverse Indigenous perspectives. Posthumanism eschews dualistic worldviews and, drawing on concepts such as entanglement, intra-action and assemblage, urges redefinitions of language in applied linguistics research and practice. The convergence of posthumanist theories and Indigenous perspectives offers new and important avenues for research and theory in applied linguistics, compelling us to recognize the political contexts and the social justice imperatives of the field.}
}
@article{FIX201816,
title = {Achieving goals and making meanings: Toward a unified model of recreational experience},
journal = {Journal of Outdoor Recreation and Tourism},
volume = {23},
pages = {16-25},
year = {2018},
issn = {2213-0780},
doi = {https://doi.org/10.1016/j.jort.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2213078018300252},
author = {Peter J. Fix and Jeffrey J. Brooks and Andrew M. Harrington},
keywords = {Expectancy-valence theory, Hermeneutics, Meaning construction, Modified analytic induction, Motivation, Ontology, Research paradigms},
abstract = {Understanding recreational experiences is a longstanding research tradition and key to effective management. Given the complexities of human experience, many approaches have been applied to study recreational experience. Two such approaches are the experiential approach (based in a positivistic paradigm) and emergent experience (based in an interpretive paradigm). While viewed as being complementary, researchers have not offered guidance for incorporating the approaches into a common model of recreational experience. This study utilized longitudinal, qualitative data to examine aspects of recreational experience posited by these two approaches. Results provided a framework for synthesizing across the two approaches. Respondents had clear pre-activity expectations, and most respondents realized their expected outcomes. This supports the experiential approach. Of the 48 activity narratives, 27 experienced something unexpected, and 45 described process-oriented, intrinsic motivation, suggesting evidence of emergent and unique characteristics specific to an individual's realization of recreational experience. This supports the application of the emergent experience approach to understand how individuals create meaning from recreational engagements. The paper proposes a model for integrating results of the two approaches. While not advocating for any specific approach, the findings can serve as an example of building a holistic model of the outdoor recreation experience. The purpose of the model is to allow for a more complete understanding of how individuals create recreation experiences, more complete documentation of the benefits of outdoor recreation for both researchers and managers, and better synthesis across studies.
Management implications
Information regarding the recreational experience can assist in implementing informed management decisions. This paper presents commonly applied approaches and discusses their differences and the benefits when combining them. The paper gives insights into different approaches focusing on desired experiences, emergent experiences, satisfaction, or long-term benefits and the related management questions. These help managers to select the most suitable approach for their respective challenges.}
}
@article{PERISIC2023e16836,
title = {The foundation for future education, teaching, training, learning, and performing infrastructure - The open interoperability conceptual framework approach},
journal = {Heliyon},
volume = {9},
number = {6},
pages = {e16836},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e16836},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023040434},
author = {Ana Perisic and Ines Perisic and Marko Lazic and Branko Perisic},
keywords = {Advanced distributed learning, Competence-based education, Competence-based learning, Education teaching and learning infrastructure, Conceptual frameworks, Software framework interoperability},
abstract = {The proliferation of Information and Communication Technologies (ICT) in Education, Teaching, Training, Learning, and the operational application (Performing) of acquired knowledge, skills, and competencies in contemporary social environments has directly influenced the transformation of early Networked Learning (NL) concepts into a Global Learning Infrastructure. Creating cooperative/collaborative stakeholders networks composed of learning subjects, objects, and competencies consumers, exposes the significant potential for gaining overall social progress. The main challenging obstacles of such globalization are: embedding semantics into the competence credentials carriers, trusted dissemination of verifiable competence tokens, the heterogeneous ontologies mapping, and sustainable service delivery infrastructure. The mainstream motivation of our research is the specification and development of a conceptual framework that fosters the interoperability of different stakeholders, whether individual or institutional, to declare, share and maintain the representative collections of information resources related to the particular Education, Learning, Teaching, Training, and Performing (Research, Development, Production, and Service) endeavors. In this article, we have specified an open, heterogeneous, interoperable conceptual framework capable of orchestrating past, current, and future paradigms to foster building the foundations for comparative analysis and evaluation of traditional and nontraditional competency-building processes, joined with students' portfolio creation, dissemination, and management. It is a starting specification that would serve for the future: open, heterogeneous, cooperative/collaborative, service-oriented software framework specification and development.}
}
@article{RINALDI2021114320,
title = {A semantic approach for document classification using deep neural networks and multimedia knowledge graph},
journal = {Expert Systems with Applications},
volume = {169},
pages = {114320},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114320},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420310149},
author = {Antonio M. Rinaldi and Cristiano Russo and Cristian Tommasino},
keywords = {Multimedia topic detection, Document classification, Semantic analysis, Ontologies, Big data, Deep neural networks, Knowledge graph},
abstract = {The amount of available multimedia data in different formats and from different sources increases everyday. From an information retrieval point of view, this high volume and heterogeneity of data involves several issues to be addressed related to information overload and lacks of well structured information. Even if modern information retrieval systems offer to the user manifold search options, it is still hard to find systems with optimal performances in the document seeking process starting from a given topic. In recent years, several frameworks have been proposed and developed to support this task based on different models and techniques. In this paper we propose a semantic approach to document classification using both textual and visual topic detection techniques based on deep neural networks and multimedia knowledge graph. A semantic multimedia knowledge base has been exploited and several experimental results show the effectiveness of our proposed approach.}
}
@article{DRURY2019487,
title = {A survey of semantic web technology for agriculture},
journal = {Information Processing in Agriculture},
volume = {6},
number = {4},
pages = {487-501},
year = {2019},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214317318302580},
author = {Brett Drury and Robson Fernandes and Maria-Fernanda Moura and Alneu {de Andrade Lopes}},
keywords = {Knowledge, Representation, Semantic web technology, Ontologies, Linked data, Survey},
abstract = {Semantic web technologies have become a popular technique to apply meaning to unstructured data. They have been infrequently applied to problems within the agricultural domain when compared to complementary domains. Despite this lack of application, agriculture has a large number of semantic resources that have been developed by large NGOs such as the Food and Agriculture Organization (FAO). This survey is intended to motivate further research in the application of semantic web technologies for agricultural problems, by making available a self contained reference that provides: a comprehensive review of preexisting semantic resources and their construction methods, data interchange standards, as well as a survey of the current applications of semantic web technologies.}
}
@article{WU2025102181,
title = {Identification of immune escape-related prognostic genes and immune infiltration analysis in hepatocellular carcinoma based on bioinformatics},
journal = {Biochemistry and Biophysics Reports},
volume = {43},
pages = {102181},
year = {2025},
issn = {2405-5808},
doi = {https://doi.org/10.1016/j.bbrep.2025.102181},
url = {https://www.sciencedirect.com/science/article/pii/S2405580825002687},
author = {Xue-song Wu and Dong Wei and Ya Zhu and Song-ling Zhao and Li-xin Liu and Fang-ming Tian and Xin Liu and Zhi-tian Shi},
keywords = {Hepatocellular carcinoma (HCC), Immune escape, Immune infiltration analysis, CEP55, GPAA1, PIGU},
abstract = {Background
Immune escape is a critical barrier to effective cancer immunotherapy for cancers such as hepatocellular carcinoma (HCC). The aim of this study was to identify prognostic genes associated with immune escape and to analyse immune infiltration in HCC.
Methods
The TCGA-LIHC cohort gene expression matrix and TCGA cohort were downloaded from the UCSC Xena and TCGA databases, respectively, for differential expression analysis, as well as for clinical data and survival information. Additionally, gene expression matrices from HCC tumor tissue samples were downloaded from the ICGC database to validate prognostic models. Subsequently, enrichment analysis utilizing the Kyoto Encyclopedia of Genes and Genomes (KEGG) and Gene Ontology (GO) were conducted. Risk modeling was subsequently performed, followed by univariate and multivariate Cox regression analyses, as well as LASSO regression analysis. Overall survival (OS) curves, receiver operating characteristic (ROC) curves, and nomograms were also generated. Finally, immune infiltration analysis was performed by single-sample genomic enrichment analysis (ssGSEA) and GeneMANIA to predict the functions and pathways of associated with prognostic genes.
Results
A total of 4489 differential expression genes were obtained, including 3259 up-regulated, and 1230 down-regulated. Among them, 2123 GO biological functions and 334 KEGG results were enriched. Subsequently, eight differential genes related to immune escape became candidate genes. Finally, we constructed a risk model using three genes, CEP55, GPAA1 and PIGU, and demonstrated better results. The results of immune infiltration showed that the prognostic genes affected the patient's condition through these immune cells. Subsequently, we performed drug sensitivity analysis and finally discovered that CEP55 and PIGU were positively associated with five drugs in the high-risk group. And these three key prognostic genes have high expression levels in HCC tumor tissues.
Conclusion
Our study found that three prognostic genes: CEP55, GPAA1 and PIGU have good prognostic value for HCC patients, and are the pivotal prognostic biomarkers.}
}
@article{ESCOBAR2019132,
title = {Habitability and design: Radical interdependence and the re-earthing of cities},
journal = {Geoforum},
volume = {101},
pages = {132-140},
year = {2019},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2019.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0016718519300545},
author = {Arturo Escobar}
}
@article{ZHU20243327,
title = {Knowledge-based in silico fragmentation and annotation of mass spectra for natural products with MassKG},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {3327-3341},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024002897},
author = {Bingjie Zhu and Zhenhao Li and Zehua Jin and Yi Zhong and Tianhang Lv and Zhiwei Ge and Haoran Li and Tianhao Wang and Yugang Lin and Huihui Liu and Tianyi Ma and Shufang Wang and Jie Liao and Xiaohui Fan},
keywords = {Mass spectrometry, Natural products, In silico fragmentation, MS annotation, Deep learning},
abstract = {Liquid chromatography coupled with tandem mass spectrometry (LC-MS/MS) is a potent analytical technique utilized for identifying natural products from complex sources. However, due to the structural diversity, annotating LC-MS/MS data of natural products efficiently remains challenging, hindering the discovery process of novel active structures. Here, we introduce MassKG, an algorithm that combines a knowledge-based fragmentation strategy and a deep learning-based molecule generation model to aid in rapid dereplication and the discovery of novel NP structures. Specifically, MassKG has compiled 407,720 known NP structures and, based on this, generated 266,353 new structures using chemical language models for the discovery of potential novel compounds. Furthermore, MassKG demonstrates exceptional performance in spectra annotation compared to state-of-the-art algorithms. To enhance usability, MassKG has been implemented as a web server for annotating tandem mass spectral data (MS/MS, MS2) with a user-friendly interface, automatic reporting, and fragment tree visualization. Lastly, the interpretive capability of MassKG is comprehensively validated through composition analysis and MS annotation of Panax notoginseng, Ginkgo biloba, Codonopsis pilosula, and Astragalus membranaceus. MassKG is now accessible at https://xomics.com.cn/masskg.}
}
@article{NIELSEN2019118,
title = {Reductions in ecology and thermodynamics. On the problems arising when shifting the concept of exergy to other hierarchical levels and domains},
journal = {Ecological Indicators},
volume = {100},
pages = {118-134},
year = {2019},
note = {Sven Erik Jørgensen - Memorial Issue},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2018.04.062},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X18303157},
author = {Søren Nors Nielsen},
keywords = {Reduction, Thermodynamics, Ontology, Hierarchy, Biological systems},
abstract = {The introduction of thermodynamical views to interpret phenomenological behavior of biological systems such as organisms, populations, ecosystems or even the bio- and anthroposphere is not an easy exercise and not carried out without obstacles. The resistance against such an endeavor can be met from all scientific disciplines ranging from hardcore physics at one end to ecological fundamentalists at the other. Thus the area represents a field of tension. The reasons for this may be many, but at the center one will recognize complication that are closely linked – directly or indirectly – to the issues around philosophical reductions undertaken in the process. Such steps are necessary to take in an attempt to formulate a new holistic biological theory that tends to formulate a new metaphysics, namely that of making possible interpretations of all kinds of systems within the biosphere in terms of a thermodynamic framework. Common to all attempts have been the establishing and definition of boundaries – real, physical as well as constructed and virtual – together with a generalized application of isomorphic mathematical expressions used different sets of ontological entities that vary with levels of the biological hierarchy. Prebiotic systems represent the closest we may get to a situation where classical thermodynamic descriptions may be used but still only to a certain extent. All levels having a fixed boundary from monocellular organisms or conglomerates of such systems and upwards in the hierarchy – i.e. higher organisms and levels throughout the biological hierarchy represent systems under conditions so far away from equilibrium that they – under the given energetic, material and intrinsic constraints tend to stabilize at dynamic states or sequential semi-stable repetitive patterns. This means that we may not talk about classical thermodynamics any longer. Terms like entropy, free energy, exergy and power may be used as convenient metaphors, but it must be stated that it as a consequence is necessary to (re-)define concepts and reformulate equations based on adequate ontological units. Although they may perform with a similar phenomenology as the classical variables, it should be kept in mind that they are no longer the same, – each domain will need its own definitions.}
}
@article{MONTALI2024102363,
title = {Relating behaviour of data-aware process models},
journal = {Data & Knowledge Engineering},
volume = {154},
pages = {102363},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102363},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000879},
author = {Marco Montali and Sarah Winkler},
keywords = {Data-aware processes, Data petri nets, Behaviour comparison, Process equivalence, Configuration equivalence, Language equivalence},
abstract = {Data Petri nets (DPNs) have gained traction as a model for data-aware processes, thanks to their ability to balance simplicity with expressiveness, and because they can be automatically discovered from event logs. While model checking techniques for DPNs have been studied, more complex analysis tasks that are highly relevant for BPM are beyond methods known in the literature. We focus here on equivalence and inclusion of process behaviour with respect to language and configuration spaces, optionally taking data into account. Such comparisons are important in the context of key process mining tasks, namely process repair and discovery, and related to conformance checking. To solve these tasks, we propose approaches for bounded DPNs based on constraint graphs, which are faithful abstractions of the reachable state space. Though the considered verification tasks are undecidable in general, we show that our method is a decision procedure DPNs that admit a finite history set. This property guarantees that constraint graphs are finite and computable, and was shown to hold for large classes of DPNs that are mined automatically, and DPNs presented in the literature. The new techniques are implemented in the tool ada, and an evaluation proving feasibility is provided.}
}
@article{MOSCICKA2020315,
title = {Description of old maps in the Europeana Data Model},
journal = {Journal of Cultural Heritage},
volume = {45},
pages = {315-326},
year = {2020},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2020.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1296207420303861},
author = {Albina Mościcka and Agnieszka Zwirowicz-Rutkowska},
keywords = {ISO 19115-1 standard, Metadata, Europeana Data Model, Archival maps, Unified Modeling Language},
abstract = {This paper presents research on archival map descriptions and aims to provide effective access to cartographic resources collected in digital libraries and archives. In the study, metadata profiling of archival maps is proposed, which is based on the ISO 19115-1 standard and contains map-specific information, which should be saved as metadata to provide the proper characteristics of the old maps. To develop metadata profiles for archival maps, some ISO metadata elements have been deleted, while others that are important for the maps have been added or changed. Controlled vocabularies with pre-defined values are also proposed. Metadata profiles for old maps are presented as Unified Modeling Language (UML) class diagrams. Moreover, the implementation of the proposed metadata profiles in the Europeana Data Model (EDM) is presented. EDM elements, such as features and properties, which enable the recording of map-specific data, are indicated. New metadata elements, proposed by the authors, have also been added.}
}
@article{MASENYA2022,
title = {Decolonization of Indigenous Knowledge Systems in South Africa:},
journal = {International Journal of Knowledge Management},
volume = {18},
number = {1},
year = {2022},
issn = {1548-0666},
doi = {https://doi.org/10.4018/IJKM.310005},
url = {https://www.sciencedirect.com/science/article/pii/S1548066622000066},
author = {Tlou Maggie Masenya},
keywords = {Colonization, Decolonization, Indigenous Knowledge Owners, Indigenous Knowledge System, Indigenous Research, Knowledge Sharing},
abstract = {ABSTRACT
This article analyses the protection of indigenous knowledge in South Africa, exploring if and how indigenous knowledge is aligned with existing policy and protocol frameworks as enacted by the government. Indigenous knowledge is mainly preserved in the memories of elders and shared through oral communication and traditional practices. The question arises: How can knowledge generated in indigenous knowledge systems research be recovered and protected to benefit indigenous knowledge owners and accessible for future generations? The study utilised literature review to critically analyse the policy, protocols, and strategies relating to the protection and preservation of indigenous knowledge systems. Decolonial theory and knowledge ontology and modelling framework were also used as underpinning theories to guide the study. Recommendations suggest the need for decolonizing indigenous knowledge systems through collaborative approach with indigenous knowledge holders and their communities.}
}
@article{LIU2023102099,
title = {Modeling and validating temporal rules with semantic Petri net for digital twins},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102099},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102099},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002276},
author = {Han Liu and Xiaoyu Song and Ge Gao and Hehua Zhang and Yu-Shen Liu and Ming Gu},
keywords = {Digital twin, Building information model (BIM), Petri net, Semantic web, SPARQL},
abstract = {The semantic web has become an important resource of domain knowledge in the construction industry, and there is a need to model the temporal states and check the transition rules of digital twins in the semantic web. Related studies have added timestamps to describe an RDF graph that varies in time, but digital twin applications require a formal representation of the temporal states and transition rules other than simple timestamps. Related studies also focused on the interaction between temporal and semantic models, but there are still challenges in the two-way sharing of knowledge in the runtime of the temporal model. In this paper, the Semantic Petri Net (SPN) is proposed as a method to represent the temporal states and rules in RDF and SPARQL so that a runnable temporal model can be implemented on a common SPARQL engine, and the state change rules can be checked with access to the rich domain knowledge provided by the semantic web. An application case is presented for modeling and simulating the constraints in the process of a construction project.}
}
@article{LIANG2024102645,
title = {A knowledge graph-based approach to modeling & representation for machining process design intent},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102645},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102645},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002933},
author = {Jiachen Liang and Shusheng Zhang and Yajun Zhang and Rui Huang and Changhong Xu and Zhen Wang and Hang Zhang},
keywords = {Process design intent, Design cognition, Process design, Knowledge graph},
abstract = {Numerical control machining process design, which connects product design and modern manufacturing, is the extension of the design process in the process field. The process design intent, derived from in-depth part analysis, embodies the cognitive and strategic thinking of technologists in response to the specific design requirements during the process design. It encapsulates interpretable experiential knowledge and provides valuable insights for subsequent process design stages. However, the process design intent is the tacit experiential knowledge of technologists that implicitly exists in the process design, lacking a reasonable organizational framework and effective method to effectively represent and capture it. To fill this gap, a process design intent model for representing and capturing useful design intent knowledge for future reuse is developed and evaluated in this study. Firstly, this paper analyzes the existence of process design intent in process design and gives a brief description of the working scheme of the process design intent model. Then, the key modules of the model are introduced in detail and a knowledge graph of process design intent is built based on the model structure. Moreover, the process design intent annotator is developed to effectively capture process design intent. Finally, the example for capturing and representing process design intent is shown and the model is evaluated and discussed by the example.}
}
@article{RICHARZ2023122139,
title = {Graph-based research field analysis by the use of natural language processing: An overview of German energy research},
journal = {Technological Forecasting and Social Change},
volume = {186},
pages = {122139},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2022.122139},
url = {https://www.sciencedirect.com/science/article/pii/S0040162522006606},
author = {Jan Richarz and Stephan Wegewitz and Sarah Henn and Dirk Müller},
keywords = {Text mining, Keyword extraction, Network analysis, Research trends, Research funding},
abstract = {To stop climate change caused by the anthropogenic greenhouse effect, the amount of research project funding to develop and demonstrate solutions for carbon emission reduction has increased. Focused topics and procedures of research projects are always presented in text-based descriptions. Natural language processing offers the possibility to process and analyze data like this. In this work, we used natural language processing to extract keywords from descriptions of energy research projects with the algorithms TextRank and TF-IDF which has not been done for this kind of data basis before. A survey-based validation showed TF-IDF to be better suited for our data basis. Extracted keywords were used to conduct a keyword network analysis and calculate static and dynamic indices of the words concerning their recent importance and development over the past two decades. Further insights are shown by allocating the research projects’ amounts of funding to the extracted keywords. We found energy research to be more focused on individual components in the past. The last years were characterized by projects on energy systems and the interaction of renewable energy technologies and their integration into existing infrastructure. Finally, the results were compared with governmental research programs and we could analyze a comprehensive agreement.}
}
@article{JAMES2023127,
title = {Artificial Intelligence in the Genetic Diagnosis of Rare Disease},
journal = {Clinics in Laboratory Medicine},
volume = {43},
number = {1},
pages = {127-143},
year = {2023},
note = {Artificial Intelligence in the Clinical Laboratory: Current Practice and Emerging Opportunities},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2022.09.023},
url = {https://www.sciencedirect.com/science/article/pii/S027227122200083X},
author = {Kiely N. James and Sujal Phadke and Terence C. Wong and Shimul Chowdhury},
keywords = {Genomics, Precision medicine, Natural language processing, Artificial intelligence}
}
@article{FORTUNATI2022107426,
title = {Is Alexa female, male, or neutral? A cross-national and cross-gender comparison of perceptions of Alexa's gender and status as a communicator},
journal = {Computers in Human Behavior},
volume = {137},
pages = {107426},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2022.107426},
url = {https://www.sciencedirect.com/science/article/pii/S0747563222002485},
author = {Leopoldina Fortunati and Autumn Edwards and Chad Edwards and Anna Maria Manganelli and Federico {de Luca}},
keywords = {Gender, Alexa, Voice-based assistants, Cross-national comparison, Human-machine communication, Status, Pronoun},
abstract = {The gendering of machine agents risks further complicating the social framework in ways that reverberate to humans' relationships and identities. This paper explores how users perceive the gender and status of Amazon's Alexa. We argue that voice-based assistants, particularly those with feminine names and voices, may contribute to reinforcing a retrieval ideology of the feminine as the place of social subordination and contempt. We conducted an online survey of women and men in the US (n = 322) and Italy (n = 333). Most (80%) identified Alexa as “female.” However, there was a lack of concordance between the gender respondents ascribed to Alexa and their spontaneous use of pronouns in writing. In terms of status, over half of the sample perceived Alexa as an inferior communicator (for Italian respondents, inferiority was associated with perceiving Alexa as female), while over one-third rated Alexa as equal or superior to humans, evidencing the change happening in the ontological order. The few respondents who noticed gender differences in people's interactions with Alexa perceived women to be more courteous, serious and accommodating in their use. Respondent gender and culture comparisons are presented, implications of the findings are discussed, and future research is proposed to reduce harmful impact.}
}
@article{DALZOCHIO2020103298,
title = {Machine learning and reasoning for predictive maintenance in Industry 4.0: Current status and challenges},
journal = {Computers in Industry},
volume = {123},
pages = {103298},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2020.103298},
url = {https://www.sciencedirect.com/science/article/pii/S0166361520305327},
author = {Jovani Dalzochio and Rafael Kunst and Edison Pignaton and Alecio Binotto and Srijnan Sanyal and Jose Favilla and Jorge Barbosa},
keywords = {Industry 4.0, Internet of Things, Artificial intelligence, Systematic literature review, Predictive maintenance, Ontology},
abstract = {In recent years, the fourth industrial revolution has attracted attention worldwide. Several concepts were born in conjunction with this new revolution, such as predictive maintenance. This study aims to investigate academic advances in failure prediction. The prediction of failures takes into account concepts as a predictive maintenance decision support system and a design support system. We focus on frameworks that use machine learning and reasoning for predictive maintenance in Industry 4.0. More specifically, we consider the challenges in the application of machine learning techniques and ontologies in the context of predictive maintenance. We conduct a systematic review of the literature (SLR) to analyze academic articles that were published online from 2015 until the beginning of June 2020. The screening process resulted in a final population of 38 studies of a total of 562 analyzed. We removed papers not directly related to predictive maintenance, machine learning, as well as researches classified as surveys or reviews. We discuss the proposals and results of these papers, considering three research questions. This article contributes to the field of predictive maintenance to highlight the challenges faced in the area, both for implementation and use-case. We conclude by pointing out that predictive maintenance is a hot topic in the context of Industry 4.0 but with several challenges to be better investigated in the area of machine learning and the application of reasoning.}
}
@article{KRABINA2023100771,
title = {Building a Knowledge Graph for the History of Vienna with Semantic MediaWiki},
journal = {Journal of Web Semantics},
volume = {76},
pages = {100771},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100771},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000555},
author = {Bernhard Krabina},
keywords = {Semantic MediaWiki, Knowledge graphs, Vienna History Wiki, Semantic wikis, Linked data, Open government, Cultural heritage, Digital curation},
abstract = {While research on semantic wikis is declining, Semantic MediaWiki (SMW) can still play an important role in the emerging field of knowledge graph curation. The Vienna History Wiki, a large knowledge base curated by the city government in collaboration with other institutions and the general public, provides an ideal use case for demonstrating strengths and weaknesses of SMW as well as discussing the challenges of co-curation in a cultural heritage setting. This paper describes processes like collaborative editing, interlinking unique identifiers on the web, sharing data with Wikidata, making use of Schema.org, and other ontologies. It presents insights from a user survey, access statistics, and a knowledge graph analysis. This work contributes to the scarce research in wiki usage outside of the Wikipedia ecosystem as well as to the field of community-based knowledge graph curation. The availability of a now significantly improved RDF representation indicates future directions for research and practice.}
}
@article{DUSARTZDEVIGNEULLES2025105751,
title = {Improving care pathways through evidence-based modeling strategies: a scoping review},
journal = {Public Health},
volume = {244},
pages = {105751},
year = {2025},
issn = {0033-3506},
doi = {https://doi.org/10.1016/j.puhe.2025.105751},
url = {https://www.sciencedirect.com/science/article/pii/S0033350625001970},
author = {Benjamin {du Sartz de Vigneulles} and Romain Lan and Gérard Mick and Claude Dussart and Florence Carrouel},
keywords = {Modeling framework, Quality of care, Care improvement, Care management, Process optimization},
abstract = {Objectives
Noncommunicable and communicable diseases represent significant public health problems, heavily straining healthcare systems. The care pathway (CP) concept has emerged as a promising framework to improve care coordination and delivery, but its complexity often hinders implementation. Modeling, with its various methodologies, represents a valuable approach to address this challenge. Systematizing these methodologies is essential for enhancing CPs. This scoping review aims to describe and analyze CP modeling methodologies.
Study design
Scoping review.
Methods
Following PRISMA-ScR guidelines, searches were performed in PubMed, Web of science and Embase. Inclusion criteria were: (i) publications in English; (ii) human studies, (iii) published between January 1, 2019 and April 3, 2024 and (iv) use of modeling to analyze CPs. For each publication included, data were extracted and categorized based on modeling goals, methods used, functions of the techniques and their respective strengths and limitations.
Results
Analysis of the 41 included articles revealed that the main goals of CP modeling were quality of care (46.3 %), continuous improvement (31.7 %), and process optimization (22.0 %). The methods used for modeling were qualitative (41.5 %), quantitative (34.1 %), or mixed (24.4 %). Technical goals were description (48.8 %), decision support (36.6 %), and prediction (14.6 %). Qualitative methods (68.5 %) were common in studies focused on quality of care. Only 11 articles shared similar methodologies across at least two studies. Key weaknesses of CP modeling were data availability and implementation acceptance.
Conclusions
This scoping review identified key categories and commonly used methodologies in CP modeling, offering a framework to help researchers and healthcare professionals improve CP design and implementation, leading to better patient outcomes and more efficient healthcare systems.}
}
@incollection{SGORBISSA2022165,
title = {Chapter 8 - From guidelines to culturally competent artificial intelligence},
editor = {Irena Papadopoulos and Christina Koulouglioti and Chris Papadopoulos and Antonio Sgorbissa},
booktitle = {Transcultural Artificial Intelligence and Robotics in Health and Social Care},
publisher = {Academic Press},
pages = {165-189},
year = {2022},
isbn = {978-0-323-90407-0},
doi = {https://doi.org/10.1016/B978-0-323-90407-0.00010-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323904070000106},
author = {Antonio Sgorbissa},
keywords = {Bayesian networks, Computer ontologies, Conversational systems, Knowledge representation, Probabilistic reasoning},
abstract = {This chapter introduces cultural knowledge representation: how to encode the knowledge required to interact appropriately with people who belong to different cultures in a computer program. First, a review is provided of the most popular solutions adopted by roboticists to encode knowledge in a formalism that robots can automatically process. Second, a discussion follows on how guidelines provided by experts in transcultural nursing can be translated into a computer ontology and how artificial intelligence programs can automatically process cultural knowledge to produce an engaging verbal interaction with people. Third, we show how tools for probabilistic reasoning may be used to learn more about a person during the interaction to cope with individual differences and avoid stereotyped representations of people's cultures.}
}
@article{CHRISTY2021101405,
title = {Deciphering the molecular interplay between pelvic inflammatory disease (PID) and ovarian cancer (OC)—A network biology approach},
journal = {Gene Reports},
volume = {25},
pages = {101405},
year = {2021},
issn = {2452-0144},
doi = {https://doi.org/10.1016/j.genrep.2021.101405},
url = {https://www.sciencedirect.com/science/article/pii/S2452014421003897},
author = {Jemmy Christy and  Harini and Swetha Vasudevan and Priyadharshini Lingesan and Daniel Alex Anand},
keywords = {Pelvic inflammatory disease, Ovarian cancer, Endometriosis, Inflammation, Molecular target, Network biology},
abstract = {Endometriosis is a condition that is associated with viable endometrial tissue external to the uterus that may lead on to Endometriosis-Associated Ovarian Carcinoma (EAOC). The primary risk factor is chronic pelvic inflammation involving malignant transformation of epithelial cells or the transformation of premalignant lesions in the fallopian tubes leading to ovarian cancer. Differential gene expression analysis of EAOC and pelvic inflammatory Disease (PID) gene expression profiles revealed that 134 overlapping up-regulated genes. Protein-protein interaction (PPI) networks, gene ontology and pathway-based functional enrichment analysis revealed pathways of the hub genes namely CDK1, CDC20, BRCA1, FEN1, RAD51, BUB1, AURKA, CDC28A and their ontology (GO). Up-regulated genes were enriched with GO terms Progesterone-mediated oocyte maturation, mitotic cell cycle, p53 signalling pathway, Double Strand Break Repair, etc. The identified DEGs and hub genes may hold promise of deciphering the underlying molecular mechanism of tumour progression in endometriosis-associated ovarian cancer and associated tissues.}
}
@article{SZALKAI201850,
title = {Near perfect protein multi-label classification with deep neural networks},
journal = {Methods},
volume = {132},
pages = {50-56},
year = {2018},
note = {Comparison and Visualization Methods for High-Dimensional Biological Data},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2017.06.034},
url = {https://www.sciencedirect.com/science/article/pii/S104620231730035X},
author = {Balázs Szalkai and Vince Grolmusz},
abstract = {Biological sequences can be considered as data items of high-, non-fixed dimensions, corresponding to the length of those sequences. The comparison and the classification of biological sequences in their relations to large databases are important areas of research today. Artificial neural networks (ANNs) have gained a well-deserved popularity among machine learning tools upon their recent successful applications in image- and sound processing and classification problems. ANNs have also been applied for predicting the family or function of a protein, knowing its residue sequence. Here we present two new ANNs with multi-label classification ability, showing impressive accuracy when classifying protein sequences into 698 UniProt families (AUC=99.99%) and 983 Gene Ontology classes (AUC=99.45%).}
}
@article{WANG20237,
title = {ChatGPT for design, manufacturing, and education},
journal = {Procedia CIRP},
volume = {119},
pages = {7-14},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004262},
author = {Xingzhi Wang and Nabil Anwer and Yun Dai and Ang Liu},
keywords = {Artificial Intelligence, Engineering Design, Product Development, Smart Manufacturing, ChatGPT, AI-generated content, Education},
abstract = {The manufacturing industry involves innumerable complex tasks that require significant knowledge and experience to execute. With the rapid development of artificial intelligence, particularly with the emergence of powerful large language models such as ChatGPT, new opportunities have risen to provide knowledge through conversation. With its seemingly endless knowledge base and highly organized response style, ChatGPT is expected to revolutionize every aspect of the industry. However, the extent of ChatGPT's capabilities and how they could contribute to the industry's future revolution remains unclear. In light of this, this paper performed a systematic testing of ChatGPT to uncover its advantages and limitations. Based on the testing results, the authors provided some prospects and critical research questions of ChatGPT from a manufacturing perspective. Furthermore, the authors recommended a technology development roadmap to successfully integrate ChatGPT into the manufacturing industry.}
}
@article{KARAKUS2025107447,
title = {An in silico analysis of dicofol-induced neurotoxicity mechanisms in humans},
journal = {Neurotoxicology and Teratology},
volume = {109},
pages = {107447},
year = {2025},
issn = {0892-0362},
doi = {https://doi.org/10.1016/j.ntt.2025.107447},
url = {https://www.sciencedirect.com/science/article/pii/S0892036225000248},
author = {Fuat Karakuş and Burak Kuzu},
keywords = {Dicofol, Neurotoxicity, Na/K-ATPase, ATP1A3},
abstract = {Dicofol (DCF) is an organochlorine pesticide that has recently been recognized as a persistent organic pollutant. This study begins by investigating the neurotoxicity of DCF and its metabolites through in silico tools. It subsequently explores the molecular mechanisms and key targets associated with DCF-induced neurotoxicity in humans by employing network toxicology, multi-level bioinformatics approaches, and molecular docking analyses. The prediction results indicate that both DCF and its metabolites can traverse the blood-brain barrier, penetrating the central nervous system, and inducing neurotoxicity. A thorough analysis has identified 56 potential targets linked to DCF-induced neurotoxicity. Gene Ontology enrichment analysis revealed significant associations with pathways related to sodium ion transmembrane transport, sodium/potassium-exchanging ATPase complexes, and P-type calcium transporter activity. Pathway enrichment analysis suggests that DCF-induced neurotoxicity arises from disruptions in ion transport via P-type ATPases. Further examination of gene-gene and protein-protein interactions, along with centrality analysis, identified 11 hub targets, including ATP1A1, ATP1A2, ATP1A3, ATP1A4, ATP1B1, ATP1B2, and MAPK1, as key players. Notably, six of these targets are subunits of the Na+/K+-ATPase, a P-type ATPase. Molecular docking results demonstrated that DCF binds more effectively to the ATP1A3-ATP1B1 protein complex than to its natural ligand, ATP. These findings suggest that DCF may inhibit Na+/K+-ATPase through ATP1A3, resulting in an imbalance of sodium and potassium gradients and ultimately leading to neurotoxicity.}
}
@article{VANHAMME2018258,
title = {Managing distributed trust relationships for multi-modal authentication},
journal = {Journal of Information Security and Applications},
volume = {40},
pages = {258-270},
year = {2018},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2018.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214212617304180},
author = {Tim {Van hamme} and Davy Preuveneers and Wouter Joosen},
keywords = {Authentication, Behavioral biometrics, Reputation, Trust model, Ontology},
abstract = {Multi-modal active authentication schemes fuse decisions of multiple behavioral biometrics (behaviometrics) to reduce identity verification errors. The challenge that we address in this work is the security risk caused by these decision fusion schemes making invalid assumptions, such as a fixed probability of (in)correct recognition and a temporal congruence of behaviometrics. To mitigate this risk, this paper presents a formal trust model that drives the behaviometric selection and composition. Our trust model adopts a hybrid approach combining policy and reputation based knowledge representation techniques. Our model and framework (1) externalizes trust knowledge from the authentication logic to achieve loosely coupled trust management, and (2) formalizes this knowledge in description logic to reason upon and broker complex distributed trust relationships to make risk-adaptive decisions for multi-modal authentication. The evaluation of our proof-of-concept illustrates an acceptable performance overhead while lifting the burden of manual trust and behaviometric management for multi-modal authentication.}
}
@article{HEPACH2022139,
title = {Ephemeral climates: Plato's geographic myths and the phenomenological nature of climate and its changes},
journal = {Journal of Historical Geography},
volume = {78},
pages = {139-148},
year = {2022},
issn = {0305-7488},
doi = {https://doi.org/10.1016/j.jhg.2022.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0305748822000263},
author = {Maximilian Gregor Hepach},
keywords = {Ancient Greek philosophy, Phenomenology, Myth, Climate, Climate change},
abstract = {Historical and cultural approaches to climate generally consider climate to be a stabilising concept between weather and culture. Different historical and cultural concepts of climate signify different ways of learning to live with the weather. However, anthropogenic climate change evidences the limit of this approach: instead of stabilising, climates ephemeralise together with the ways we have come to adapt to them. Changing climates require a concept of climate that captures how climates are experienced both as stable and ephemeral. To create such a concept, I engage in an exercise of counterfactual etymology, reconstructing the concept of climate that might have emerged from the Ancient Greek term hora as opposed to klima. Central to my re-creation of phenomenological climate are Plato's myths, through which I highlight the methodological kinship between myth and phenomenology. Drawing on a later dialogue, Philebus, I provide an ontological account of climates as both stable and ephemeral. I conclude by situating my approach to climate and its changes in recent work on the relationship between weather and climate, arguing for the necessity of phenomenological climate in order to make sense of what changes with climate change. My turn to Ancient Greek philosophy and its application to the phenomenology of climate and its changes sounds out a novel approach to research in historical geography.}
}
@article{ZEROUAL201882,
title = {Data science in light of natural language processing: An overview},
journal = {Procedia Computer Science},
volume = {127},
pages = {82-91},
year = {2018},
note = {PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2017},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918301121},
author = {Imad Zeroual and Abdelhak Lakhouaja},
keywords = {Data science, Natural language processing, Data driven approches, Corpora, Machine learning},
abstract = {The focus of data scientists is essentially divided into three areas: collecting data, analyzing data, and inferring information from data. Each one of these tasks requires special personnel, takes time, and costs money. Yet, the next and the fastidious step is how to turn data into products. Therefore, this field grabs the attention of many research groups in academia as well as industry. In the last decades, data-driven approaches came into existence and gained more popularity because they require much less human effort. Natural Language Processing (NLP) is strongly among the fields influenced by data. The growth of data is behind the performance improvement of most NLP applications such as machine translation and automatic speech recognition. Consequently, many NLP applications are frequently moving from rule-based systems and knowledge-based methods to data-driven approaches. However, collected data that are based on undefined design criteria or on technically unsuitable forms will be useless. Also, they will be neglected if the size is not enough to perform the required analysis and to infer the accurate information. The chief purpose of this overview is to shed some lights on the vital role of data in various fields and give a better understanding of data in light of NLP. Expressly, it describes what happen to data during its life-cycle: building, processing, analyzing, and exploring phases.}
}
@article{FARALLI2018180,
title = {CrumbTrail: An efficient methodology to reduce multiple inheritance in knowledge graphs},
journal = {Knowledge-Based Systems},
volume = {151},
pages = {180-197},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.03.030},
url = {https://www.sciencedirect.com/science/article/pii/S095070511830162X},
author = {Stefano Faralli and Irene Finocchi and Simone Paolo Ponzetto and Paola Velardi},
keywords = {Semantic networks, Ontologies, Knowledge graph pruning},
abstract = {In this paper we present CrumbTrail, an algorithm to clean large and dense knowledge graphs. CrumbTrail removes cycles, out-of-domain nodes and non-essential nodes, i.e., those that can be safely removed without breaking the knowledge graph’s connectivity. It achieves this through a bottom-up topological pruning on the basis of a set of input concepts that, for instance, a user can select in order to identify a domain of interest. Our technique can be applied to both noisy hypernymy graphs – typically generated by ontology learning algorithms as intermediate representations – as well as crowdsourced resources like Wikipedia, in order to obtain clean, domain-focused concept hierarchies. CrumbTrail overcomes the time and space complexity limitations of current state-of-art algorithms. In addition, we show in a variety of experiments that it also outperforms them in tasks such as pruning automatically acquired taxonomy graphs, and domain adaptation of the Wikipedia category graph.}
}
@article{PANG2025102852,
title = {Towards cognition-augmented human-centric assembly: A visual computation perspective},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {91},
pages = {102852},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102852},
url = {https://www.sciencedirect.com/science/article/pii/S073658452400139X},
author = {Jiazhen Pang and Pai Zheng and Junming Fan and Tianyuan Liu},
keywords = {Cognitive assistance, Human-centric assembly, Computer vision, Metaverse, Cloud service, Large language model, Brain computer interface},
abstract = {Human-centric assembly is emerging as a promising paradigm for achieving mass personalization in the context of Industry 5.0, as it fully capitalizes on the advantages of human flexibility with robot assistance. However, in small-batch and highly customized assembly tasks, frequently changes in production procedures pose significant cognition challenges. To address this, leveraging computer vision technology to enhance human cognition becomes a feasible solution. Therefore, this review aims to explore the cognitive characteristics of human beings and classify existing computer vision technologies in a manner that discusses the future development of cognition-augmented human-centric assembly. The concept of cognition-augmented assembly is first proposed based on the brain's functional structure - the frontal, parietal, temporal, and occipital lobes. Corresponding to these brain regions, cognitive issues in spatiality, memory, knowledge, and decision-making are summarized. Recent studies conducted between 2014 and 2023 on visual computation of assembly are categorized into four groups: position registration, multi-layer recognition, contextual perception, and mixed-reality fusion, all aimed at addressing these cognitive challenges. The applications and limitations of current computer vision technology are discussed. Furthermore, considering the rapidly evolving technologies such as the metaverse, cloud services, large language models, and brain-computer interfaces, future trends on computer vision are prospected to augment human cognition corresponding to the cognitive issues.}
}
@article{DECKERS2022111415,
title = {Systematic literature review of domain-oriented specification techniques},
journal = {Journal of Systems and Software},
volume = {192},
pages = {111415},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111415},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001261},
author = {Robert Deckers and Patricia Lago},
keywords = {Domain-specific language, Domain model, Systematic literature review, Method comparison, Specification method, Modeling language},
abstract = {Context:
The popularity of domain-specific languages and model driven development has made the tacit use of domain knowledge in system development more tangible. Our vision is a development process where a (software) system specification is based on multiple domain models, and where the specification method is built from cognitive concepts, presumably derived from natural language.
Goal:
To realize this vision, we evaluate and reflect upon the existing literature in domain-oriented specification techniques.
Method:
We designed and conducted a systematic literature review on domain-oriented specification techniques.
Results:
We identified 53 primary studies, populated the classification framework for each study, and summarized our findings per classification aspect. We found many approaches for creating domain models or domain-specific languages. Observations include: (i) most methods are defined incompletely; (ii) none offers methodical support for the use of domain models or domain-specific languages to create other specifications; (iii) there are specification techniques to integrate models in general, but no study offers methodical support for multiple domain models.
Conclusion:
The results indicate which topics need further research and which can instead be reused to realize our vision on system development. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{YAMIN2021102450,
title = {Serious games as a tool to model attack and defense scenarios for cyber-security exercises},
journal = {Computers & Security},
volume = {110},
pages = {102450},
year = {2021},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2021.102450},
url = {https://www.sciencedirect.com/science/article/pii/S0167404821002741},
author = {Muhammad Mudassar Yamin and Basel Katt and Mariusz Nowostawski},
keywords = {Cyber range, Cyber-security, Exercises, Scenarios, Attack, Defense},
abstract = {Technology is evolving rapidly; this poses a problem for security specialists and average citizens as their technological skill sets are quickly made obsolete. This makes the knowledge and understanding of cyber-security in a technologically evolving world difficult. Global IT infrastructure and individuals’ privacy are constantly under threat. One way to tackle this problem is by providing continuous training and self-learning platforms. Cyber-security exercises can provide a necessary platform for training people’s cyber-security skills. However, conducting cyber-security exercises with new and unique scenarios requires comprehensive planning and commitment to the preparation time and resources. In this work, we propose a serious game for the development of cyber-security exercise scenarios. The game provides a platform to model simulated cyber-security exercise scenarios, transforming them into an emulated cyber-security exercise environment using domain-specific language (DSL) and infrastructure orchestration. In this game, players can play as cyber attackers or defenders in a multiplayer environment to make operational cyber-security decisions in real-time. The decisions are evaluated for the development of operational cyber-attack and defense strategies.}
}
@article{LISOWSKA2023104276,
title = {SATO (IDEAS expAnded wiTh BCIO): Workflow for designers of patient-centered mobile health behaviour change intervention applications},
journal = {Journal of Biomedical Informatics},
volume = {138},
pages = {104276},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104276},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422002817},
author = {Aneta Lisowska and Szymon Wilk and Mor Peleg},
keywords = {Digital behaviour change intervention (DBCI), Mobile health, Personalisation, Application design, Wellbeing, Cancer},
abstract = {Designing effective theory-driven digital behaviour change interventions (DBCI) is a challenging task. To ease the design process, and assist with knowledge sharing and evaluation of the DBCI, we propose the SATO (IDEAS expAnded wiTh BCIO) design workflow based on the IDEAS (Integrate, Design, Assess, and Share) framework and aligned with the Behaviour Change Intervention Ontology (BCIO). BCIO is a structural representation of the knowledge in behaviour change domain supporting evaluation of behaviour change interventions (BCIs) but it is not straightforward to utilise it during DBCI design. IDEAS (Integrate, Design, Assess, and Share) framework guides multi-disciplinary teams through the mobile health (mHealth) application development life-cycle but it is not aligned with BCIO entities. SATO couples BCIO entities with workflow steps and extends IDEAS Integrate stage with consideration of customisation and personalisation. We provide a checklist of the activities that should be performed during intervention planning with concrete examples and a tutorial accompanied with case studies from the Cancer Better Life Experience (CAPABLE) European project. In the process of creating this workflow, we found the necessity to extend the BCIO to support the scenarios of multiple clinical goals in the same application. To ensure the SATO steps are easy to follow for the incomers to the field, we performed a preliminary evaluation of the workflow with two knowledge engineers, working on novel mHealth app design tasks.}
}
@article{KIM2022103806,
title = {Four urban health paradigms: The search for coherence},
journal = {Cities},
volume = {128},
pages = {103806},
year = {2022},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2022.103806},
url = {https://www.sciencedirect.com/science/article/pii/S0264275122002451},
author = {Jinhee Kim and Evelyne {de Leeuw} and Ben Harris-Roxas and Peter Sainsbury},
keywords = {Paradigms, Urban health, Healthy cities, Healthy urban planning, Health social movements, Medical-industrial complex},
abstract = {Scholars, practitioners and policymakers view urban health based on their foundational ontologies, or paradigms, which provide a framework of norms that specifies the policy goals or research questions, the preferred policy instruments or research methodologies, and defines the nature of the urban health issue. This paper identifies four paradigms in current research and practice that address the links between the urban built environment and human health: the ‘medical-industrial city’, ‘urban health science’, ‘healthy built environment’ and ‘health social movement’ paradigms. We argue that scholars, practitioners and policymakers must recognise their diverse and sometimes contradictory views in order to create an opportunity for coherence in understanding knowledge generated from different paradigms.}
}
@article{LIU2025103502,
title = {Digital twin-based assembly process framework utilizing STEP and knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103502},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103502},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003957},
author = {Yazui Liu and Haodong Shen and Gang Zhao and Xiaoxiao Du and Xishuang Jing},
keywords = {Digital twin-based assembly process, Assembly information model, Standard for the Exchange of Product model data (STEP), Knowledge graph},
abstract = {Effective information organization and data analysis are foundational for achieving assembly process digital twin. However, the development of the assembly digital twin technology faces significant challenges due to “information silos” resulting from ineffective data exchange among multi-source heterogeneous data involved in assembly processes. This study proposes a Digital Twin-based Assembly Process Framework (DT-APF) designed to systematically organize and manage multi-stage manufacturing data. Through object instantiation, the framework establishes a Digital Twin-based Assembly Process Information Model (DT-APIM) that enables standardized transformation of heterogeneous data. By converting the instance of the DT-APIM into a graph-based structure, the framework generates a Digital Twin-based Assembly Process Knowledge Graph (DT-APKG), achieving deep integration of multi-source data. Furthermore, the integration of Autoregressive Structured Prediction (ASP) algorithm constructs a knowledge reasoning engine, establishing a comprehensive data organization and knowledge reasoning system that spans the entire assembly lifecycle. Experimental validation was conducted using an aero-engine casing assembly case. Results demonstrate that the ASP-based reasoning mechanism validates the effectiveness in identifying assembly performance correlations, while the DT-APF framework improves data integration efficiency by 31.6% compared with traditional knowledge graph construction approaches. This research provides a systematic solution for overcoming information barriers in complex product assembly and enhances the implementation of DT technology in industrial applications.}
}
@article{PYKETT2023115619,
title = {Urban precarity and youth mental health: An interpretive scoping review of emerging approaches},
journal = {Social Science & Medicine},
volume = {320},
pages = {115619},
year = {2023},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2022.115619},
url = {https://www.sciencedirect.com/science/article/pii/S027795362200925X},
author = {Jessica Pykett and Niyah Campbell and Sarah-Jane Fenton and Elizabeth Gagen and Anna Lavis and Karen Newbigging and Verity Parkin and Jessy Williams},
keywords = {Cities, Inequalities, Interdisciplinarity, Mental health, Scoping review, Social model, Social theory, Youth},
abstract = {Circumstances of living are key to shaping emotional and affective experiences, long term health, wellbeing and opportunities. In an era characterised by rapid urbanisation across the majority of the world, there is increasing interest in the interaction between mental health and urban environments, but insufficient attention is paid to how mental health is situated in space and time. Socio-economic inequalities are prevalent in many urban environments globally, making conditions of living highly precarious for some social groups including young people. There remains a large volume of unmet mental health service needs, and young people are impacted by uncertain economic futures. The purpose of this scoping review is to develop an interdisciplinary and globally-informed understanding of the urban conditions which affect youth mental health across a range of scales, and to identify protective factors which can promote better youth mental health. We seek to broaden the scope of urban mental health research beyond the physical features of urban environments to develop an interpretive framework based on perspectives shared by young people. We illustrate how concepts from social theory can be used as an integrative framework to emphasise both young people's lived experiences and the wider cultural and political dynamics of urban mental health.}
}
@article{FARHADI2021108568,
title = {Friendship selection and management in social internet of things: A systematic review},
journal = {Computer Networks},
volume = {201},
pages = {108568},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.108568},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621004813},
author = {Babak Farhadi and Amir {Masoud Rahmani} and Parvaneh Asghari and Mehdi Hosseinzadeh},
keywords = {Friendship selection, Relationship management, Network navigability, Social internet of things, Systematic literature review, Smart objects},
abstract = {The Social Internet of Things (SIoT) paradigm integrates the Internet of Things (IoT) and human social networks. SIoT network plays an essential role in establishing social friendships between smart devices. Any physical object deployed on the SIoT environment will access the required service utilizing its social friendships. Discovering desirable and efficient services among SIoT smart devices is possible only if we create, select, and manage social friendships intelligently and reliably. However, despite the significant importance of the thrust area of social friendship selection and management in the SIoT network, there is no systematic and comprehensive research study about exclusively analyzing and reviewing all the key strategies of this research field. Therefore, this survey paper investigates the state-of-the-art SIoT friendship selection and management strategies based on the Systematic Literature Review (SLR) methodology. It achieves this target by looking at the journal and conference papers published on social friendship selection and Relationship Management (RM) scope between 2013 and December 2020. We propose a novel technical taxonomy to categorize the state-of-the-art studies into the five categories in which social relationship selection and management play a significant role: structure-based, community-based, ontology-based, recommendation-based, and other strategies. In each category, the relevant research studies are presented and described. Also, a summary of the benefits and drawbacks of the reviewed studies is provided, and a comprehensive comparison of studies based on some evaluation metrics is made. Finally, we outline new emerging challenges and discuss and identify future research directions and open issues.}
}
@article{PINO2022105341,
title = {Processes in DNA damage response from a whole-cell multi-omics perspective},
journal = {iScience},
volume = {25},
number = {11},
pages = {105341},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.105341},
url = {https://www.sciencedirect.com/science/article/pii/S2589004222016133},
author = {James C. Pino and Alexander L.R. Lubbock and Leonard A. Harris and Danielle B. Gutierrez and Melissa A. Farrow and Nicole Muszynski and Tina Tsui and Stacy D. Sherrod and Jeremy L. Norris and John A. McLean and Richard M. Caprioli and John P. Wikswo and Carlos F. Lopez},
keywords = {Bioinformatics, Complex system biology, Systems biology, Data processing in systems biology},
abstract = {Summary
Technological advances have made it feasible to collect multi-condition multi-omic time courses of cellular response to perturbation, but the complexity of these datasets impedes discovery due to challenges in data management, analysis, visualization, and interpretation. Here, we report a whole-cell mechanistic analysis of HL-60 cellular response to bendamustine. We integrate both enrichment and network analysis to show the progression of DNA damage and programmed cell death over time in molecular, pathway, and process-level detail using an interactive analysis framework for multi-omics data. Our framework, Mechanism of Action Generator Involving Network analysis (MAGINE), automates network construction and enrichment analysis across multiple samples and platforms, which can be integrated into our annotated gene-set network to combine the strengths of networks and ontology-driven analysis. Taken together, our work demonstrates how multi-omics integration can be used to explore signaling processes at various resolutions and demonstrates multi-pathway involvement beyond the canonical bendamustine mechanism.}
}
@incollection{ALKHALAF2025180,
title = {Transformer-Based Biomedical Text Extraction},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {180-189},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00036-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000361},
author = {Ruba {Al Khalaf} and Anna Bernasconi},
keywords = {BERT, Biomedical NLP tasks, Deep learning, GPT, Information retrieval, Natural language processing, Transformer-based models},
abstract = {Unlike formal languages, natural languages are unstructured and more complex. Understanding and generating meaningful text are the goals of natural language processing (NLP). Deep learning recently had a significant impact on this field. Innovative and effective transformer-based models have achieved state-of-the-art results on a wide range of NLP tasks, including those working on specialized clinical and biomedical text. The most widely-adopted models (BERT and GPT) are here described along with their domain-specific versions and applications in the biomedical domain.}
}
@article{XU2025122434,
title = {Fine-grained entity typing based on hyperbolic representation and label-context interaction},
journal = {Information Sciences},
volume = {719},
pages = {122434},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2025.122434},
url = {https://www.sciencedirect.com/science/article/pii/S0020025525005663},
author = {Mingying Xu and Wenxuan Zhang and Jie Liu and Weiping Ding and Lei Shi and Kaiyang Zhong},
keywords = {Fine-grained entity typing, Hyperbolic representation, Graph convolutional network, Label-context interaction},
abstract = {Fine-grained entity typing (FET) is a crucial task in natural language processing (NLP), which aims to assign detailed type labels to entities based on context. Accurate entity typing is essential for many downstream applications, such as knowledge graph construction, information retrieval, and question answering. However, existing FET methods face significant challenges in capturing the hierarchical structure of entity types and effectively leveraging contextual information. Many prior approaches either rely on label co-occurrence statistics, which may introduce noise, or utilize hyperbolic space, which performs well for ultra-fine entities but struggles with coarse-grained entity types. Furthermore, the lack of effective label-context interaction limits the model's ability to filter out irrelevant type labels, leading to suboptimal entity typing performance. To address these issues, we propose a novel FET framework that integrates hyperbolic representation and label-context interaction. First, we map the hierarchical structure of entity labels into hyperbolic space, allowing for a more effective representation of type relationships. A graph convolutional network (GCN) is then employed to model label dependencies while filtering out noisy co-occurrence information. Additionally, we introduce a label-context interaction module using attention mechanism to refine type selection by modeling semantic correlations between context and labels. This mechanism dynamically enhances the relevance of selected type labels while mitigating noise. Experiments on multiple public datasets demonstrate the effectiveness of combining hyperbolic representation with label-context interaction for FET.}
}
@article{ABDULLAH20253052,
title = {Enhanced Information Retrieval Using Hybrid p-Norm Extended Boolean Models with BERT},
journal = {Procedia Computer Science},
volume = {258},
pages = {3052-3061},
year = {2025},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.563},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925016679},
author = {A. Sheik Abdullah and S Geetha and Yeshwanth Govindarajan and A G {Vishal Pranav} and A Aashish Vinod},
keywords = {Information Retrieval, BERT, Extended Boolean Model, p-Norm, TF-IDF, Document Retrieval},
abstract = {This paper presents an Enhanced Extended Boolean Retrieval (EBR) framework that integrates Bidirectional Encoder Representations from Transformers (BERT) to address the shortcomings of traditional retrieval models, such as Term Frequency-Inverse Document Frequency (TF-IDF), which struggle with nuanced term relationships. The approach involves fine-tuning BERT and optimizing p-Norm values to enhance retrieval accuracy and query processing. Experimental results demonstrate that the Fine-Tuned BERT with EBR achieves 92% accuracy and an Area Under the Curve (AUC) of 0.92, outperforming the TF-IDF model’s 72% accuracy and traditional p-Norm models, which achieve around 83%. This hybrid framework ofers a balanced trade-of between precision and recall, proving effective for large-scale document retrieval. The study underscores the model’s scalability and robustness, highlighting its potential to enhance Information Retrieval (IR) systems for handling complex queries across various datasets.}
}
@article{SYAUQI2025100158,
title = {Advances in machine transliteration methods, limitations, challenges, applications and future directions},
journal = {Natural Language Processing Journal},
volume = {11},
pages = {100158},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100158},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000342},
author = {A’la Syauqi and Aji Prasetya Wibawa},
keywords = {Systematic literature review, Machine transliteration, Transliteration methods, Natural language processing, Script conversion, Low-resource languages, Cross-linguistic communication},
abstract = {Machine transliteration is critical in natural language processing (NLP), facilitating script conversion while preserving phonetic integrity across diverse languages. Using the PRISMA framework, this review analyzes 73 selected studies on machine transliteration, covering both methodological advancements and its role in NLP applications. Among these, 37 studies focus on transliteration methods (rule-based, statistical, machine learning, hybrid, and semantic), while 32 studies explore their application in NLP tasks such as machine translation, sentiment analysis, and text normalization. Rule-based methods provide structured frameworks but face challenges in adapting to linguistic variability. Statistical techniques demonstrate robustness yet depend heavily on the availability of parallel corpora. Machine learning models leverage neural architectures to achieve high accuracy but are constrained by data scarcity for low-resource languages. Hybrid approaches integrate multiple methodologies, while semantic knowledge-based models enhance accuracy by incorporating linguistic features. The review highlights transliteration’s role in NLP applications such as machine translation, sentiment analysis, and text normalization, which are critical for improving multilingual language accessibility. Findings show that machine learning-based approaches dominate transliteration research (32 of 73 studies), followed by rule-based and hybrid methods. These approaches contribute to improving multilingual accessibility and NLP performance. This study provides actionable insights for researchers and practitioners by synthesizing advancements and identifying challenges. These insights enable the development more efficient and inclusive transliteration systems, ultimately supporting linguistic diversity and advancing multilingual NLP technologies. The review identifies gaps in addressing underrepresented languages like Javanese, where complex character sets, orthographic rules, and scriptio continua remain underexplored.}
}
@article{VINARTI2019821,
title = {Knowledge Representation for Infectious Disease Risk Prediction System: A Literature Review},
journal = {Procedia Computer Science},
volume = {161},
pages = {821-825},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.188},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931899X},
author = {Retno Aulia Vinarti},
keywords = {Knowledge Representation, Infectious Diseases, Prediction},
abstract = {This article contains a literature review to seek knowledge representation for infectious disease risk prediction system. The knowledge representation should be able to encode knowledge related to infectious diseases and usable by experts. 188 articles are collected through several constraints. From these articles, 14 knowledge representations are gathered. Ontology comes out as the most used knowledge representation related to disease, followed by fuzzy and rules. This leads to the next step of the research, on how to encode knowledge using these representations.}
}
@article{KOO20224191,
title = {Resource identifier interoperability among heterogeneous IoT platforms},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {7},
pages = {4191-4208},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822001525},
author = {Jahoon Koo and Young-Gab Kim},
keywords = {Internet of Things, Interoperability, IoT platform, Resource identifier, Smart city},
abstract = {Many standards, projects, and platforms are being developed as the Internet of Things (IoT) is adopted in a wide range of fields. However, because each IoT platform is based on a different resource identifier (ID), it is difficult to identify each device and use the service among heterogeneous IoT platforms. To solve this problem, we propose an interoperability framework that includes an IoT resource name system (RNS) based on analysis of the resource IDs (i.e., device ID and resource request formats) of five selected IoT platforms: oneM2M, Oliot, Watson IoT, IoTivity, and FIWARE. The IoT RNS converts a specific resource path into a resource request format for each platform. The converted resource path is shared among IoT RNSs for each platform, and users can request services from other platforms using converted resource paths. We also present an example of interoperability scenario among heterogeneous IoT platforms using the proposed IoT RNS in a smart city. The scenario includes each stage, such as resource registration and deletion, sharing mapping tables, converting resource addresses, and service requests. Furthermore, to prove the aims of the proposed approach, we implemented the resource interoperability scenario between oneM2M and FIWARE. In the experiments, resources can interwork in the two platforms through resource path conversion. Based on the results, we performed a qualitative evaluation of the IoT RNS with the current studies. In conclusion, our proposal overcomes the issues of building an existing integrated platform or specific central ontology and duplicating resources inside the platform. In addition, we separate the functions of the root and local IoT RNSs to solve communication traffic and memory capability issues.}
}
@article{LAM2025109947,
title = {Local interpretation of deep learning models for Aspect-Based Sentiment Analysis},
journal = {Engineering Applications of Artificial Intelligence},
volume = {143},
pages = {109947},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109947},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624021067},
author = {Stefan Lam and Yin Liu and Max Broers and Jasper {van der Vos} and Flavius Frasincar and David Boekestijn and Finn {van der Knaap}},
keywords = {Machine learning, Model-agnostic interpretation models, Textual data, Sentiment analysis},
abstract = {Currently, deep learning models are commonly used for Aspect-Based Sentiment Analysis (ABSA). These deep learning models are often seen as black boxes, meaning that they are inherently difficult to interpret. To improve deep learning models, it is crucial to understand their inner workings. We aim to interpret black box models by implementing model-agnostic local interpretation methods. Inspired by Local Interpretable Model-agnostic Explanations (LIME) and Local Rule-based Explanations (LORE) and combined with a Similarity-based Sampling (SS) method, we propose SS-LIME and SS-LORE, and use Anchor to explain two state-of-the-art ABSA deep learning models. The deep learning models build upon the Left-Center-Right separated neural network with Rotatory attention (LCR-Rot) model, extended by iterating multiple times over the rotatory attention mechanism (LCR-Rot-hop) and hierarchical attention and context-dependent word embeddings (LCR-Rot-hop++). We evaluate the proposed models in terms of fidelity, hit rate, and user interpretability using the SemEval 2016 dataset consisting of restaurant reviews for ternary sentiment classification. Results show that the LCR-Rot-hop and LCR-Rot-hop++ models are best explained by SS-LIME and SS-LORE, respectively. Furthermore, we conclude that the LCR-Rot-hop++ model can be better interpreted than the LCR-Rot-hop model.}
}
@article{BAO2024102638,
title = {Hierarchical construction and application of machining domain knowledge graph based on as-fabricated information model},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102638},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102638},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002866},
author = {Qiangwei Bao and Pai Zheng and Sheng Dai},
keywords = {Machining, Domain knowledge graph, As-fabricated information model},
abstract = {The increasing complexity and customization of mechanical products have put forward higher requirements on the quality and process iteration speed of machining parts. The effective organization and utilization of the vast knowledge found in past production processes has become a critical concern in refining improvements to the subsequent machining process. To overcome this challenge, this paper proposes a framework for constructing a machining domain knowledge graph based on as-fabricated information model combining the inherent characteristics and development needs of machining process. Firstly, the state of machining part between procedures is described with as-fabricated information model concerning geometric and non-geometric information, after which the corresponding subgraph plotting method is presented with the definition of nodes, relationships and properties. Secondly, the overall route in building machining domain knowledge graph is analyzed, in which the semantic extraction mechanism of unstructured texts is proposed. Moreover, a case study is presented, including the instantiation of as-fabricated information model and the construction of knowledge graph with further approach of quality prediction based on a set of manufacturing process documentations of the example machining parts. The case demonstrates that the machining domain knowledge graph construction framework proposed in this paper is capable of recording the multi-source heterogeneous data generated during the machining process and providing data support for subsequent analysis and optimization.}
}
@article{GARCIADELACERDA201884,
title = {Enactive management: A nurturing technology enabling fresh decision making to cope with conflict situations},
journal = {Futures},
volume = {103},
pages = {84-93},
year = {2018},
note = {Futures of Society: The Interactions Revolution},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0016328717304603},
author = {Osvaldo {García De la Cerda} and Patrick Humphreys and María Soledad {Saavedra Ulloa}},
keywords = {Enactive management, Conflict management, Bodyhood, Interactions, CLEHES, Enactive laboratory},
abstract = {The focus of this paper is observation, self-observation, and enactive management of organizational conflict situations whereby a community, an organization, or a human being has the possibility of recognizing their resources and generating changes in their practices if they so desire, and making fresh decisions, in the sense that different ontological dimensions are involved. We show how considering Body11The word Body is used instead of the Spanish “Cuerpo” in the English version, but we keep the Spanish initial letters.- Language- Emotions- History- Eros- Silence can configure a nurturing technology call CLEHES. This tool has been applied for diverse people, groups, communities, and organizations that need and wish to develop their own skills to inquire conflict practice resolutions, in order to learn as a human decision support system. Conflict situations are understood as interactions, a breakdown in-between CLEHES from the individual or social standpoints. This tool allows observing the boundaries of conflict situations and building an observer system with the ability to manage, solve, or attenuate the situation, enabling fresh decision-making attending to the context in which the organization moves. This learning process happens in a constructed place called an Enactive Laboratory where strategies are developed to cope with the domains and context in the perceived individual and human activities systems. We present a case study focusing on a Learning Family Mediators System.}
}
@article{SHEET2024200079,
title = {Recognition of cancer mediating genes using MLP-SDAE model},
journal = {Systems and Soft Computing},
volume = {6},
pages = {200079},
year = {2024},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2024.200079},
url = {https://www.sciencedirect.com/science/article/pii/S2772941924000085},
author = {Sougata Sheet and Ranjan Ghosh and Anupam Ghosh},
keywords = {Stacked denoising auto-encoder, Deep learning, MLP, -value, -test},
abstract = {This article introduces a predictive deep learning model called MLP-SDAE, which combines Multilayer Perceptron (MLP) and Stacked Denoising Auto-encoder (SDAE) techniques. Our model, MLP-SDAE is trained using Stacked Denoising Auto-Encoder for feature selection, and backpropagation is employed within the MLP structure. We have incorporated dropout to enhance the model’s performance and prevent overfitting. The primary objective of the MLP-SDAE model is to identify associations among genes that have undergone significant alterations from a normal to a diseased state based on their expression behaviors. This concept allows us to predict disease-mediating genes and their altered associations. The methodology involves calculating gene-based correlation coefficients and selecting a subset of genes based on this analysis. We have demonstrated the effectiveness of our methods using four gene expression datasets related to human leukemia, lung, colon, and breast cancer. As a result, we have identified several potentially important genes, such as CACLA, HBA, IGFBP3, EFGR, TFN, TP53, LI6, and TMTC1, which may play a crucial role in developing these cancers. Furthermore, we conducted a comprehensive comparative study with other deep learning techniques, including Recurrent Neural Network (RNN), Deep Belief Network (DBN), Deep Boltzmann Machine (DBM), Auto-encoder (AE), and Denoising Auto-encoder (DAE). Our results have been validated through biochemical pathway analysis, t-tests, F-score, Gene Ontology (GO) identification, and the NCBI database. These validations demonstrate that our proposed MLP-SDAE model outperforms existing methods.}
}
@article{BUSCHHAUS2025102443,
title = {Overcoming the hurdle of legal expertise: A reusable model for smartwatch privacy policies},
journal = {Data & Knowledge Engineering},
volume = {159},
pages = {102443},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102443},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000382},
author = {Constantin Buschhaus and Arvid Butting and Judith Michael and Verena Nitsch and Sebastian Pütz and Bernhard Rumpe and Carolin Stellmacher and Sabine Theis},
keywords = {Conceptual model, Smartwatches, Wearables, Privacy, Privacy policies, Sovereignty, Model-driven software engineering},
abstract = {Regulations for privacy protection aim to protect individuals from the unauthorized storage, processing, and transfer of their personal data but oftentimes fail in providing helpful support for understanding these regulations. To better communicate privacy policies for smartwatches, we need an in-depth understanding of their concepts and provide better ways to enable developers to integrate them when engineering systems. Up to now, no conceptual model exists covering privacy statements from different smartwatch manufacturers that is reusable for developers. This paper introduces such a conceptual model for privacy policies of smartwatches and shows its use in a model-driven software engineering approach to create a platform for data visualization of wearable privacy policies from different smartwatch manufacturers. We have analyzed the privacy policies of various manufacturers and extracted the relevant concepts. Moreover, we have checked the model with lawyers for its correctness, instantiated it with concrete data, and used it in a model-driven software engineering approach to create a platform for data visualization. This reusable privacy policy model can enable developers to easily represent privacy policies in their systems. This provides a foundation for more structured and understandable privacy policies which, in the long run, can increase the data sovereignty of application users.}
}
@article{AGRAWAL2025252,
title = {Characterizing the pan-cancer role of exosomal miRNAs in metastasis across cancers},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {252-264},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024004483},
author = {Piyush Agrawal and Gulden Olgun and Arashdeep Singh and Vishaka Gopalan and Sridhar Hannenhalli},
keywords = {Exosomal miRNA, Cancer metastasis, Pre-metastatic Niche, Survival Analysis, Machine Learning},
abstract = {Exosomal microRNAs (exomiRs) play a critical role in intercellular communication, especially in cancer, where they regulate key cellular processes like proliferation, angiogenesis, and metastasis, highlighting their significance as potential diagnostic and therapeutic targets. Here, we aimed to characterize the role of exomiRs, derived from seven cancer types (four cell lines and three tumors), in influencing the pre-metastatic niche (PMN). In each cancer type we extracted high confidence exomiRs (LogFC >= 2 in exosomes relative to control), their experimentally validated targets, and the enriched pathways among those targets. We then selected the top100 high-confidence targets based on their frequency of appearance in the enriched pathways. We observed significantly higher GC content in exomiRs relative to genomic background. Gene Ontology analysis revealed both general cancer processes, such as wound healing and epithelial cell proliferation, as well as cancer-specific processes, such as “angiogenesis” in the kidney and “ossification” in the lung. ExomiR targets were enriched for cancer-specific tumor suppressor genes and downregulated in PMN formed in lungs compared to normal. Motif analysis showed high inter-cancer similarity among motifs enriched in exomiRs. Our analysis recapitulated exomiRs associated with M2 macrophage differentiation and chemoresistance, such as miR-21 and miR-222–3p, regulating signaling pathways like PTEN/PI3/Akt, NF-kB, etc. Additionally, Cox regression analysis in TCGA indicated that exomiR targets are significantly associated with better overall survival of patients. Lastly, support vector machine model using exomiR targets gene expression classified responders and non-responders to therapy with an AUROC ranging from 0.72 to 0.96, higher than previously reported gene signatures.}
}
@article{HAN2025,
title = {Modeling the vertebrate regulatory sequence landscape by UUATAC-seq and deep learning},
journal = {Cell},
year = {2025},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2025.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0092867425006865},
author = {Xiaoping Han and Hanyu Wu and Xueyi Wang and Daiyuan Liu and Yuting Fu and Lei Yang and Renying Wang and Peijing Zhang and Jingjing Wang and Lifeng Ma and Jizhong Mao and Lina Zhou and Siqi Wang and Xinlian Zhang and Mengmeng Jiang and Xinru Wang and Guoxia Wen and Danmei Jia and Guoji Guo},
keywords = {snATAC-seq, chromatin accessibility landscape, cCRE, deep learning, UUATAC-seq, NvwaCE, genomics, regulatory sequence, mutation effect, genome editing},
abstract = {Summary
The regulatory sequences of vertebrate genomes remain incompletely understood. To address this, we developed an ultra-throughput, ultra-sensitive single-nucleus assay for transposase-accessible chromatin using sequencing (UUATAC-seq) protocol that enables the construction of chromatin accessibility landscapes for one species in a 1-day experiment. Using UUATAC-seq, we mapped candidate cis-regulatory elements (cCREs) across five representative vertebrate species. Our analysis revealed that genome size differences across species influence the number but not the size of cCREs. We introduced Nvwa cis-regulatory element (NvwaCE), a mega-task deep-learning model designed to interpret cis-regulatory grammar and predict cCRE landscapes directly from genomic sequences with high precision. NvwaCE demonstrated that regulatory grammar is more conserved than nucleotide sequences and that this grammar organizes cCREs into distinct functional modules. Moreover, NvwaCE accurately predicted the effects of synthetic mutations on lineage-specific cCRE function, aligning with causal quantitative trait loci (QTLs) and genome editing results. Together, our study provides a valuable resource for decoding the vertebrate regulatory language.}
}
@article{KIM201841,
title = {Semantic weldability prediction with RSW quality dataset and knowledge construction},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {41-53},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617301428},
author = {Kyoung-Yun Kim and Fahim Ahmed},
keywords = {Semantic weldability prediction, Resistance spot welding, Welding quality, Weldability knowledge construction, Decision tree algorithm, Semantic rules},
abstract = {This paper presents a semantic Resistance Spot Welding (RSW) weldability prediction framework. The framework constructs a shareable weldability knowledge database based on the regression rules from inconsistent RSW quality datasets. This research aims to effectively predict the weldability of RSW process for existing or new weldment design. A real welding test dataset collected from an automotive OEM is used to extract decision rules using a decision tree algorithm, Classification and Regression Trees (CART). The extracted decision rules are converted systematically into SWRL rules for capturing the semantics and to increase the shareability of the constructed knowledge. The experiments show that the RSW ontology, along with SWRL rules that contains weldability rules constructed from the datasets, successfully predicts the weldability (nugget width) values for RSW cases. The predicted nugget width values are found to be in-close proximity of the actual values. This paper shows that semantic prediction framework construes an intelligent way for constructing accurate and transparent predictive models for RSW weldability verification.}
}
@article{HUANG2022,
title = {Diagnosis of a Single-Nucleotide Variant in Whole-Exome Sequencing Data for Patients With Inherited Diseases: Machine Learning Study Using Artificial Intelligence Variant Prioritization},
journal = {JMIR Bioinformatics and Biotechnology},
volume = {3},
number = {1},
year = {2022},
issn = {2563-3570},
doi = {https://doi.org/10.2196/37701},
url = {https://www.sciencedirect.com/science/article/pii/S2563357022000204},
author = {Yu-Shan Huang and Ching Hsu and Yu-Chang Chune and I-Cheng Liao and Hsin Wang and Yi-Lin Lin and Wuh-Liang Hwu and Ni-Chung Lee and Feipei Lai},
keywords = {next-generation sequencing, genetic variation analysis, machine learning, artificial intelligence, whole-exome sequencing},
abstract = {Background
In recent years, thanks to the rapid development of next-generation sequencing (NGS) technology, an entire human genome can be sequenced in a short period. As a result, NGS technology is now being widely introduced into clinical diagnosis practice, especially for diagnosis of hereditary disorders. Although the exome data of single-nucleotide variant (SNV) can be generated using these approaches, processing the DNA sequence data of a patient requires multiple tools and complex bioinformatics pipelines.
Objective
This study aims to assist physicians to automatically interpret the genetic variation information generated by NGS in a short period. To determine the true causal variants of a patient with genetic disease, currently, physicians often need to view numerous features on every variant manually and search for literature in different databases to understand the effect of genetic variation.
Methods
We constructed a machine learning model for predicting disease-causing variants in exome data. We collected sequencing data from whole-exome sequencing (WES) and gene panel as training set, and then integrated variant annotations from multiple genetic databases for model training. The model built ranked SNVs and output the most possible disease-causing candidates. For model testing, we collected WES data from 108 patients with rare genetic disorders in National Taiwan University Hospital. We applied sequencing data and phenotypic information automatically extracted by a keyword extraction tool from patient’s electronic medical records into our machine learning model.
Results
We succeeded in locating 92.5% (124/134) of the causative variant in the top 10 ranking list among an average of 741 candidate variants per person after filtering. AI Variant Prioritizer was able to assign the target gene to the top rank for around 61.1% (66/108) of the patients, followed by Variant Prioritizer, which assigned it for 44.4% (48/108) of the patients. The cumulative rank result revealed that our AI Variant Prioritizer has the highest accuracy at ranks 1, 5, 10, and 20. It also shows that AI Variant Prioritizer presents better performance than other tools. After adopting the Human Phenotype Ontology (HPO) terms by looking up the databases, the top 10 ranking list can be increased to 93.5% (101/108).
Conclusions
We successfully applied sequencing data from WES and free-text phenotypic information of patient’s disease automatically extracted by the keyword extraction tool for model training and testing. By interpreting our model, we identified which features of variants are important. Besides, we achieved a satisfactory result on finding the target variant in our testing data set. After adopting the HPO terms by looking up the databases, the top 10 ranking list can be increased to 93.5% (101/108). The performance of the model is similar to that of manual analysis, and it has been used to help National Taiwan University Hospital with a genetic diagnosis.}
}
@article{LOPEZOTAL2025106382,
title = {SeqIA: A Python framework for extracting drought impacts from news archives},
journal = {Environmental Modelling & Software},
volume = {187},
pages = {106382},
year = {2025},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2025.106382},
url = {https://www.sciencedirect.com/science/article/pii/S1364815225000660},
author = {Miguel López-Otal and Fernando Domínguez-Castro and Borja Latorre and Javier Vela-Tambo and Jorge Gracia},
keywords = {Drought, Impacts, Newspaper, Spain, Classification, Python},
abstract = {Drought is a hazard that causes great economic, ecological, and human loss. With an ever-growing risk of climate change, their frequency and magnitude are expected to increase. While there are many indices and metrics available for the analysis of droughts, assessing their impacts represents one of the best ways to understand their magnitude and extent. However, there are no systematic records outlining these impacts. To help in their ongoing creation, we present a software framework that leverages raw newspaper articles, identifies any drought-related ones, and automatically classifies them according to a set of socioeconomic impacts. The information is provided to the user in a structured format, including geographical coordinates and their date of reporting. Our approach employs state-of-the-art Transformer-based Natural Language Processing (NLP) techniques, which achieve great accuracy. We currently support newspaper articles in the Spanish language within Spain, but our framework can be expanded to other countries and languages.}
}
@article{LELONG2019,
title = {Building a Semantic Health Data Warehouse in the Context of Clinical Trials: Development and Usability Study},
journal = {JMIR Medical Informatics},
volume = {7},
number = {4},
year = {2019},
issn = {2291-9694},
doi = {https://doi.org/10.2196/13917},
url = {https://www.sciencedirect.com/science/article/pii/S229196941900111X},
author = {Romain Lelong and Lina F Soualmia and Julien Grosjean and Mehdi Taalba and Stéfan J Darmoni},
keywords = {data warehousing, search engine, semantics, clinical trial, patient selection},
abstract = {Background
The huge amount of clinical, administrative, and demographic data recorded and maintained by hospitals can be consistently aggregated into health data warehouses with a uniform data model. In 2017, Rouen University Hospital (RUH) initiated the design of a semantic health data warehouse enabling both semantic description and retrieval of health information.
Objective
This study aimed to present a proof of concept of this semantic health data warehouse, based on the data of 250,000 patients from RUH, and to assess its ability to assist health professionals in prescreening eligible patients in a clinical trials context.
Methods
The semantic health data warehouse relies on 3 distinct semantic layers: (1) a terminology and ontology portal, (2) a semantic annotator, and (3) a semantic search engine and NoSQL (not only structured query language) layer to enhance data access performances. The system adopts an entity-centered vision that provides generic search capabilities able to express data requirements in terms of the whole set of interconnected conceptual entities that compose health information.
Results
We assessed the ability of the system to assist the search for 95 inclusion and exclusion criteria originating from 5 randomly chosen clinical trials from RUH. The system succeeded in fully automating 39% (29/74) of the criteria and was efficiently used as a prescreening tool for 73% (54/74) of them. Furthermore, the targeted sources of information and the search engine–related or data-related limitations that could explain the results for each criterion were also observed.
Conclusions
The entity-centered vision contrasts with the usual patient-centered vision adopted by existing systems. It enables more genericity in the information retrieval process. It also allows to fully exploit the semantic description of health information. Despite their semantic annotation, searching within clinical narratives remained the major challenge of the system. A finer annotation of the clinical texts and the addition of specific functionalities would significantly improve the results. The semantic aspect of the system combined with its generic entity-centered vision enables the processing of a large range of clinical questions. However, an important part of health information remains in clinical narratives, and we are currently investigating novel approaches (deep learning) to enhance the semantic annotation of those unstructured data.}
}
@article{LIU2025109772,
title = {Federated digital twins platform for smart city logistics: A knowledge-driven approach},
journal = {International Journal of Production Economics},
pages = {109772},
year = {2025},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2025.109772},
url = {https://www.sciencedirect.com/science/article/pii/S0925527325002579},
author = {Yu Liu and Shenle Pan and Eric Ballot},
keywords = {Smart city logistics, Digital twins, Asset management, Semantics and ontology, Constrained K-Means clustering, Multi-echelon and multi-modal logistics networks},
abstract = {Urban logistics faces increasing pressure from rising population densities, escalating delivery demands, and constrained urban resources. Traditional logistics systems struggle to adapt to real-time urban dynamics, leading to inefficiencies, congestion, and environmental concerns. A key challenge lies in mobilizing underutilized assets, such as off-hour freight parking, and adopting multimodal solutions to navigate diverse and increasingly strict regulations, thereby enhancing both sustainability and operational efficiency. However, effective management and utilization of these assets require real-time visibility, cross-stakeholder collaboration, and intelligent decision-making. This study proposes a federated digital twin platform to enhance logistics operations efficiency by integrating asset management and knowledge-driven operations management, relying on real-time asset visibility and delivery knowledge, such as destination characteristics and preferred logistics modalities. Unlike traditional logistics planning, which relies on static assumptions, our approach adapts to urban constraints by continuously querying real-time asset information and integrating logistics-related knowledge into operations management. To assess the effectiveness of this approach, an optimization-based simulation framework with decision-making tools is developed. The study evaluates multi-echelon logistics networks, incorporating micro-hubs, dynamic transshipment points, and multimodal logistics options, including on-foot porters, E-cargo bikes, and Road Autonomous Delivery Robots (RADRs). Findings demonstrate that integrating federated digital twins with knowledge-driven approaches, such as destination-based clustering and modality selection, reduces costs by over 50 % and emissions by more than 30 %. This study underscores the transformative potential of digital twins in enabling real-time, knowledge-driven operations management, and fostering more sustainable and efficient urban logistics systems.}
}
@article{ZHANG2024104613,
title = {iCORPP: Interleaved commonsense reasoning and probabilistic planning on robots},
journal = {Robotics and Autonomous Systems},
volume = {174},
pages = {104613},
year = {2024},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2023.104613},
url = {https://www.sciencedirect.com/science/article/pii/S092188902300252X},
author = {Shiqi Zhang and Piyush Khandelwal and Peter Stone},
keywords = {Integrated Reasoning and Planning, Commonsense reasoning, Planning under uncertainty, Autonomous Robots, Markov Decision Processes, POMDPs},
abstract = {Robot sequential decision-making in the real world is a challenge because it requires the robots to simultaneously reason about the current world state and dynamics, while planning actions to accomplish complex tasks. On the one hand, declarative languages and reasoning algorithms support representing and reasoning with commonsense knowledge. But these algorithms are not good at planning actions toward maximizing cumulative reward over a long, unspecified horizon. On the other hand, probabilistic planning frameworks, such as Markov decision processes (MDPs) and partially observable MDPs (POMDPs), support planning to achieve long-term goals under uncertainty. But they are ill-equipped to represent or reason about knowledge that is not directly related to actions. In this article, we present an algorithm, called iCORPP, to simultaneously estimate the current world state, reason about world dynamics, and construct task-oriented controllers. In this process, robot decision-making problems are decomposed into two interdependent (smaller) subproblems that focus on reasoning to “understand the world” and planning to “achieve the goal” respectively. The developed algorithm has been implemented and evaluated both in simulation and on real robots using everyday service tasks, such as indoor navigation, and dialog management. Results show significant improvements in scalability, efficiency, and adaptiveness, compared to competitive baselines including handcrafted action policies.}
}
@article{SULTAN2021102979,
title = {Assembling and diversifying social contexts of recovery},
journal = {International Journal of Drug Policy},
volume = {87},
pages = {102979},
year = {2021},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2020.102979},
url = {https://www.sciencedirect.com/science/article/pii/S0955395920303170},
author = {Aysel Sultan and Cameron Duff},
keywords = {Drug use, Relational ontology, Recovery, Social context, Youth},
abstract = {Recovery from drug use is receiving increased attention in critical drug studies. Researchers point out the importance of scrutinizing the term and its meanings anew in order to better understand drug use treatment policies and their effects on the individuals they target. Informed by relational ontological thinking, this article analyses a series of empirical accounts of recovery experiences, and offers a critical assessment of the social contexts of recovery. Qualitative data collected in Azerbaijan and Germany provide distinctive reports of the differentiated experiences of youth as they make and re-make sense of their recovery within specific recovery contexts. Discussions reveal how recovery advances in relations between human and nonhuman actors including spaces, bodies, affects, and practices. On the basis of this analysis, we argue that recovery may be framed as an emergent and dynamic context that becomes with and from drug use.}
}
@article{PEREZPAREDES2024100141,
title = {Epistemologies of corpus linguistics across disciplines},
journal = {Research Methods in Applied Linguistics},
volume = {3},
number = {3},
pages = {100141},
year = {2024},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2024.100141},
url = {https://www.sciencedirect.com/science/article/pii/S2772766124000478},
author = {Pascual Pérez-Paredes and Niall Curry},
keywords = {Corpus linguistics, Corpus linguistics for education, Corpus linguistics for philosophy, Corpus linguistics for sociology, Corpus methods, Epistemology},
abstract = {Despite the growing use of corpus linguistics across an ever-growing range of disciplines such as sociology, sports studies, journalism, media discourse or education, there is a dearth of research that examines the epistemological foundations of corpus methods in these disciplines. This paper builds on well-established conceptualisations about research methodology and the role of methods in the wider literature. Drawing on existing discussions about the use of research methods in objectivist and subjectivist conceptualisations of social reality, we seek to bring to the fore the underlying methodological tensions found in the use of corpus linguistics in the application of corpus methods in research that lies outside the interest of major linguistic disciplines. Through this process, we explore how the notions of natural language use and data elicitation are interpreted by current research in order to advance our understanding of how experts from different research camps engage with and epistemologically localise corpus linguistics.}
}
@article{LI2023100397,
title = {A Multi-View Filter for Relation-Free Knowledge Graph Completion},
journal = {Big Data Research},
volume = {33},
pages = {100397},
year = {2023},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2023.100397},
url = {https://www.sciencedirect.com/science/article/pii/S2214579623000308},
author = {Juan Li and Wen Zhang and Hongtao Yu},
keywords = {Knowledge graph, Knowledge graph embedding, Relation-free knowledge graph completion, Graph neural networks},
abstract = {As knowledge graphs are often incomplete, knowledge graph completion methods have been widely proposed to infer missing facts by predicting the missing element of a triple given the other two elements. However, the assumption that the two elements have to be correlated is strong. Thus in this paper, we investigate relation-free knowledge graph completion to predict relation-tail(r-t) pairs given a head entity. Considering the large scale of candidate relation-tail pairs, previous work proposed to filter r-t pairs before ranking them relying on entity types, which fails when entity types are missing or insufficient. To tackle the limitation, we propose a relation-free knowledge graph completion method that can cope with knowledge graphs without additional ontological information, such as entity types. Specifically, we propose a multi-view filter, including two intra-view modules and an inter-view module, to filter r-t pairs. For the intra-view modules, we construct head-relation and tail-relation graphs based on triples. Two graph neural networks are respectively trained on these two graphs to capture the correlations between the head entities and the relations, as well as the tail entities and the relations. The inter-view module is learned to bridge the embeddings of entities that appeared in the two graphs. In terms of ranking, existing knowledge graph embedding models are applied to score and rank the filtered candidate r-t pairs. Experimental results show the efficiency of our method in preserving higher-quality candidate r-t pairs for knowledge graphs and resulting in better relation-free knowledge graph completion.}
}
@article{JIN2024102681,
title = {The paradigm logic of blockchain governance},
journal = {Technology in Society},
volume = {78},
pages = {102681},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102681},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X2400229X},
author = {Sheng Jin},
keywords = {Blockchain, Governance paradigm, Non-rational, Judicature-based governance},
abstract = {Blockchain technology is mainly composed of public-key cryptography, hash algorithms and consensus mechanisms. Blockchain solves the two trust problems inherent in human interaction: the Byzantine Generals Problem and the Prisoner's Dilemma, which allows blockchain to replace the function of public power as a centre of trust to a certain extent, and also enables it to become a kind of decentralised large-scale collaborative infrastructure for human beings. Blockchain itself has a strong escape from public power, and the blockchain order is also non-rational. Its emergence has impacted the traditional constructivist-rationalism governance paradigm. Current research exploring blockchain governance issues at the level of governance paradigms is still limited. This study mainly adopts a normative analysis methodology, firstly outlining the decentralised technology model of blockchain and the economic ecosystem model built on top of this technology model. Secondly, this study explores the ontology of blockchain technology and its economic model on a philosophical level as well as the philosophical roots of the limitations of the traditional governance paradigm when confronted with blockchain governance. On this basis, this study further proposes ideas for shifting the blockchain governance paradigm. Specifically, it includes that a judicature-based governance paradigm should be established while the executive power should shift to focus on the issue of equality on the blockchain, and in the context of weakly centralised legislation, soft law governance should be allowed to play more of a role. Considering the popularity and development of blockchain technology, this study provides insights into the theoretical level of blockchain governance.}
}
@article{ROMANO2024124292,
title = {An NLP-based approach to assessing a company’s maturity level in the digital era},
journal = {Expert Systems with Applications},
volume = {252},
pages = {124292},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124292},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424011588},
author = {Simon Pietro Romano and Giancarlo Sperlì and Andrea Vignali},
keywords = {Natural language processing, Embedding representation, Maturity model, Digital transformation},
abstract = {Conducting a maturity assessment allows companies to measure their readiness in implementing novel technologies. However, this task is challenging due to the multidimensional, complex, unpredictable, and non-linear nature of innovation. In this paper, we introduce an innovative approach to maturity assessment that enables both intra- and inter-company analysis. Our approach evaluates a company’s absolute maturity score concerning a specific technology or area. By leveraging a Natural Language Processing pipeline applied to a semi-structured questionnaire we extract popular concepts from the answers and present them to a human expert for analysis. The expert can refine the analysis by adding or removing concepts as needed. Subsequently, we compute a similarity metric for each answer to determine a company’s maturity in specific concepts. The output of our analysis is presented through human-readable plots, offering clear insights into the internal maturity level of the company and allowing for a comparison with competitors across the chosen concepts. To demonstrate the capabilities of our method, we provide a running example showcasing both quantitative and qualitative results of the analysis. Our approach demonstrates efficiency, with preprocessing completed in 1.967±0.758 s, and information extraction in 0.074±0.017 s on average, excluding human intervention time, and requiring low hardware resources.}
}
@article{DELAMO2022107954,
title = {Hybrid recommendations and dynamic authoring for AR knowledge capture and re-use in diagnosis applications},
journal = {Knowledge-Based Systems},
volume = {239},
pages = {107954},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107954},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121010868},
author = {Iñigo Fernández {del Amo} and John Ahmet Erkoyuncu and Maryam Farsi and Dedy Ariansyah},
keywords = {Augmented Reality, Failure diagnosis, Authoring systems, Knowledge capture, Ontology-based reporting},
abstract = {In Industry 4.0, integrated data management is an important challenge due to heterogeneity and the lack of structure of numerous existing data sources. A relevant research gap involves human knowledge integration, especially in maintenance operations. Augmented Reality (AR) can bridge this gap, but it requires improved augmented content to enable effective and efficient knowledge capture. This paper proposes dynamic authoring and hybrid recommender methods for accurate AR-based reporting. These methods aim to provide maintainers with augmented data input formats and recommended datasets for enhancing the efficiency and effectiveness of their reporting tasks. The proposed contributions have been validated through experiments and surveys in two failure diagnosis reporting scenarios. Experimental results indicated that the proposed reporting solution can reduce reporting errors by 50% and reporting time by 20% compared to alternative recommender and AR tools. Besides, survey results suggested that testers perceived the proposed reporting solution as more effective and satisfactory for reporting tasks than alternative tools. Thus, proving that the proposed methods can improve the effectiveness and efficiency of diagnosis reporting applications. Finally, this paper proposes future works towards a framework for automatic adaptive authoring in AR knowledge transfer and capture applications for human knowledge integration in the context of Industry 4.0.}
}
@article{ZONG2023100506,
title = {Solving math word problems concerning systems of equations with GPT models},
journal = {Machine Learning with Applications},
volume = {14},
pages = {100506},
year = {2023},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2023.100506},
url = {https://www.sciencedirect.com/science/article/pii/S2666827023000592},
author = {Mingyu Zong and Bhaskar Krishnamachari},
keywords = {AI, Math education, Language model},
abstract = {Researchers have been interested in developing AI tools to help students learn various mathematical subjects. One challenging set of tasks for school students is learning to solve math word problems. We explore how recent advances in natural language processing, specifically the rise of powerful transformer based models, can be applied to help math learners with such problems. Concretely, we evaluate the use of GPT-3, GPT-3.5, and GPT-4, all transformer models with billions of parameters recently released by OpenAI, for three related challenges pertaining to math word problems corresponding to systems of two linear equations. The three challenges are classifying word problems, extracting equations from word problems, and generating word problems. For the first challenge, we define a set of problem classes and find that GPT models generally result in classifying word problems with an overall accuracy around 70%. There is one class that all models struggle about, namely the “item and property” class, which significantly lowered the value. For the second challenge, our findings align with researchers’ expectation: newer models are better at extracting equations from word problems. The highest accuracy we get from fine-tuning GPT-3 with 1000 examples (78%) is surpassed by GPT-4 given only 20 examples (79%). For the third challenge, we again find that GPT-4 outperforms the other two models. It is able to generate problems with accuracy ranging from 76.7% to 100%, depending on the problem type.}
}
@article{CHEN2024102614,
title = {Automated fire risk assessment and mitigation in building blueprints using computer vision and deep generative models},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102614},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102614},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002623},
author = {Dayou Chen and Long Chen and Yu Zhang and Shan Lin and Mao Ye and Simon Sølvsten},
keywords = {Fire risk management, Automated compliance checking, Deep generative models, Blueprint analysis, Floorplan generation},
abstract = {Building fire risks pose significant threats to individual safety and bear substantial economic consequences. Consequently, developing effective automated fire risk assessment and mitigation solutions has become increasingly crucial to mitigate fire risks and reduce losses. Previous automated fire risk assessment approaches have predominantly relied on structured building design information, such as Industry Foundation Classes (IFC)-based Building Information Modelling (BIM) models, which limited their applicability in scenarios without such data. Additionally, there is a notable absence of a comprehensive approach in existing research for effectively mitigating fire risks identified during the assessment process. This study aims to bridge these gaps by proposing an innovative approach for assessing and mitigating fire risks using raw building blueprints. This approach incorporates advanced computer vision techniques to process both paper-based and digital blueprints. It then employs a knowledge-based algorithm for evaluating fire safety and regulatory compliance within these blueprints. A key innovation is the development of a deep generative model that redesigns unqualified blueprint designs to meet safety standards. This research contributes to the field by providing a more capable, accessible, and flexible approach for automated building safety, introducing Artificial Intelligence (AI)-enabled solutions for risk mitigation. It offers a versatile option applicable to various building types, significantly enhancing fire safety and compliance, especially for buildings without extensive BIM data. This study addresses the limitations of current methodologies and lays the groundwork for future advancements in automated fire risk assessment and mitigation.}
}
@article{LI202447,
title = {Intelligent question answering system for traditional Chinese medicine based on BSG deep learning model: taking prescription and Chinese materia medica as examples},
journal = {Digital Chinese Medicine},
volume = {7},
number = {1},
pages = {47-55},
year = {2024},
issn = {2589-3777},
doi = {https://doi.org/10.1016/j.dcmed.2024.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S2589377724000247},
author = {Ran Li and Gao Ren and Junfeng Yan and Beiji Zou and Qingping Liu},
keywords = {Traditional Chinese medicine (TCM), Deep learning, Knowledge graph, Intelligent question answering system, BERT+Slot-Gated (BSG) model},
abstract = {Objective
To construct a traditional Chinese medicine (TCM) knowledge base using knowledge graph based on deep learning methods, and to explore the application of joint models in intelligent question answering systems for TCM.
Methods
Textbooks Prescriptions of Chinese Materia Medica and Chinese Materia Medica were applied to construct a comprehensive knowledge graph serving as the foundation for the intelligent question answering system. In the study, a BERT+Slot-Gated (BSG) deep learning model was applied for the identification of TCM entities and question intentions presented by users in their questions. Answers retrieved from the knowledge graph based on the identified entities and intentions were then returned to the user. The Flask framework and BSG model were utilized to develop the intelligent question answering system of TCM.
Results
A TCM knowledge map encompassing 3 149 entities and 6 891 relational triples based on the prescriptions and Chinese materia medica was drawn. In the question answering test assisted by a question corpus, the F1 value for recognizing entities when answering 20 types of TCM questions was 0.996 9, and the accuracy rate for identifying intentions was 99.75%. This indicates that the system is both feasible and practical. Users can interact with the system through the WeChat Official Account platform.
Conclusion
The BSG model proposed in this paper achieved good results in experiments by increasing the vector dimension, indicating the effectiveness of the joint model method and providing new research ideas for the implementation of intelligent question answering systems in TCM.}
}
@article{KHUDE2025100529,
title = {AI-driven clinical decision support systems: Revolutionizing medication selection and personalized drug therapy},
journal = {Advances in Integrative Medicine},
volume = {12},
number = {4},
pages = {100529},
year = {2025},
issn = {2212-9588},
doi = {https://doi.org/10.1016/j.aimed.2025.100529},
url = {https://www.sciencedirect.com/science/article/pii/S2212958825000886},
author = {Hrishikesh Khude and Pravin Shende},
keywords = {Artificial Intelligence, Personalized medicine, Cost-effective prescribing, Healthcare},
abstract = {Artificial Intelligence (AI) analyzes complex medical data records using Machine learning (ML), Natural Language Processing (NLP), and Deep Learning (DL) algorithms. These algorithms assist physicians in the optimization of therapeutic decisions that allow for the integration and interpretation of individual biological data, including genomics, proteomics, and transcriptomics. By identifying complex patterns in these data records, AI-driven systems facilitate the development of personalized treatment strategies that align with individual patient profiles. Furthermore, AI enhances pharmacovigilance by predicting potential drug interactions and conducting in-silico toxicity risk assessments through advanced molecular composition analysis. Moreover, AI accelerates the drug discovery process by screening and identifying novel drugs, thereby facilitating the development of targeted treatment. AI empowers physicians to prescribe medications, perform real-time formulary checks, and recommend therapeutic equivalent, economically viable alternatives to patient-specific factors. AI-driven clinical decision support systems (CDSS) further assist physicians in improving drug compliance and optimizing population health strategies by identifying pharmacologically cost-effective therapies. Additionally, AI enhances real-time clinical decision-making by improving diagnostic precision, refining therapeutic choices, and patient outcomes. The evolution of AI technologies offers immense potential for seamless integration into healthcare systems despite challenges such as data bias, limited model interpretability, and regulatory complexities. This integration revolutionizes personalized medicines, accelerates the drug discovery process, and improves the safety, efficacy, and cost-effectiveness of drug therapy. In summary, AI plays a significant role in modern medicine, promoting data-based clinical decisions and enhancing the overall quality and efficiency of healthcare delivery.}
}
@article{VENTURA2025617,
title = {Modelling circular-driven Digital Twins},
journal = {Procedia Computer Science},
volume = {256},
pages = {617-623},
year = {2025},
note = {CENTERIS - International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.02.159},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925005162},
author = {André Ventura and Cristóvão Sousa and Carla Pereira and Nelson Duarte and Mário Martins and Bruno Silva},
keywords = {Circularity, Digital Product Passport, Asset Administration Shell, Digital Twin},
abstract = {In the current era of digital transformation, adopting circular business models that blend circularity principles with advanced digital technologies, is fundamental for sustainable industrial practices. This paper suggests a semantic model for a Digital Twin based on an Asset Administration Shell. It also explores the Digital Product Passport topic since this will be the final goal for the Digital Twin. The Digital Product Passport serves as a complete digital record of the product life cycle to improve traceability and circularity. The Asset Administration Shell provides a standardized digital representation of assets, facilitating interoperability and fluid data exchange. By taking advantage of a Digital Twin, industries can optimize performance and predict product needs. Moreover, it enriches the Digital Product Passport with updated and accurate data, facilitating traceability and efficient product management. The application of semantic models ensures a consistent interpretation of data across all platforms, increasing the reliability of digital interactions and interoperability. This article explains the potential of these technologies to promote a circular economy, focusing in the particular case of the Digital Product Passport.}
}
@article{NEGRI2019201,
title = {FMU-supported simulation for CPS Digital Twin},
journal = {Procedia Manufacturing},
volume = {28},
pages = {201-206},
year = {2019},
note = {7th International conference on Changeable, Agile, Reconfigurable and Virtual Production (CARV2018)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.12.033},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918313763},
author = {Elisa Negri and Luca Fumagalli and Chiara Cimino and Marco Macchi},
keywords = {Digital Twin, Cyber Physical Systems, FMU, simulation, Ontology, Industry 4.0},
abstract = {Manufacturing companies are experiencing the fourth industrial revolution characterised by the introduction of new technologies into production equipment, such as the Cyber Physical Systems and the Digital Twin simulations. Companies are then challenged by the digitization of products and production systems information, which leads to new potentials for digital continuity – i.e. information available and continuously updated for the decision makers – along the lifecycles. A semantic data model, that structures and stores physical and operational data from the field, can support the digital continuity to be used in production system simulations in a Digital Twin paradigm. This work proposes to model specific aspects and behaviours of the production system separately from the core simulation, in order to flexibly decide whether to activate the replica of the specific behaviours only when needed. The modules interact with the main simulation run through standard interfaces, allowing an easy reusability of the single modules also in different simulation environments.}
}
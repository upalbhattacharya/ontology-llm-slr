@article{PARVEEN2024102278,
title = {Fuzzy-Ontology based knowledge driven disease risk level prediction with optimization assisted ensemble classifier},
journal = {Data & Knowledge Engineering},
volume = {151},
pages = {102278},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102278},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000028},
author = {Huma Parveen and Syed Wajahat Abbas Rizvi and Raja Sarath Kumar Boddu},
keywords = {Fuzzy ontology, Improved stemming, Improved fuzzy, Bi-GRU, DHUEAO algorithm},
abstract = {Modern medicinal analysis is a complex procedure, requiring precise patient data, scientific knowledge obtained over numerous years and a theoretical understanding of related medical literature. To improve the accuracy and to reduce the time for diagnosis, clinical decision support systems (DSS) were introduced, which incorporate data mining schemes for enhancing the disease diagnosing accuracy. This work proposes a new disease-predicting model that involves 3 stages. Initially, “improved stemming and tokenization” are carried out in the pre-processing stage. Then, the “Fuzzy ontology, improved mutual information (MI), and correlation features” are extracted. Then, prediction is carried out via ensemble classifiers that include “improved Fuzzy logic, Long Short Term Memory (LSTM), Deep Convolution Neural Network (DCNN), and Bidirectional Gated Recurrent Unit (Bi-GRU)”.The outcomes from improved fuzzy logic, LSTM, and DCNN are further classified via Bi-GRU which offers the results. Specifically, Bi-GRU weights are optimally tuned using Deer Hunting Update Explored Arithmetic Optimization (DHUEAO). Finally, the efficiency of the proposed work is determined concerning a variety of metrics.}
}
@article{BENADDI2022429,
title = {Ontology Model for Public Services in Morocco Based on 5W1H Approach: PSOM-eGovMa},
journal = {Procedia Computer Science},
volume = {198},
pages = {429-434},
year = {2022},
note = {12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.265},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921025047},
author = {Hanane Benaddi and Naziha Laaz and Elyoussfi El Kettani and Yaâcoub Hannad},
keywords = {e-Government, Public Services, Ontology Modeling, Semantic Web, Domain ontologies, Interoperability},
abstract = {Morocco has launched several e-government programs to develop efficient public e-services which meet the needs of users. However, while digitizing public services processes for Moroccan administrations, the designers of information systems record the entered data in databases, without being too interested in the meaning of the entries made by citizens nor on the benefit of exchanged information between public administrations. This causes inconsistency and lack of interoperability between public administration systems. Moreover, the systems require information collection and shared knowledge based on a common vocabulary of public services in Morocco. For this purpose, this paper presents an ontology model based on the 5w1h Method for defining the public services domain. This PSOM ontology is based on the referential defined by the Moroccan Ministry of Administration Reform and Public service. Indeed, we have collected the different concepts related to this field and all possible relationships between them in order to ensure interoperability between all government entities.}
}
@article{OUF2022,
title = {Ontology-Based Knowledge Modelling for Food Supply Chain Data Representation},
journal = {International Journal of e-Collaboration},
volume = {18},
number = {1},
year = {2022},
issn = {1548-3673},
doi = {https://doi.org/10.4018/IJeC.299009},
url = {https://www.sciencedirect.com/science/article/pii/S1548367322000527},
author = {Shimaa Ouf},
keywords = {Food, Ontology, Outbreaks, Reasoning, Semantic Web, Supply Chain Management},
abstract = {ABSTRACT
There has been a lack of common data management and encoding standards, as well as an inability to connect products to the many stages of their transformation process. This paper proposes a food supply chain ontology to help in the management of food traceability at various phases of the supply chain, from the extraction of raw materials to the delivery of goods to customers. Traceability has the potential to be a game-changer in the industry since it is inextricably linked to product safety and quality. Integrating the semantic web into the food supply chain enhances planning, forecasting, productivity, and competitiveness. Most of the research papers concentrate on conveying knowledge about one food type while neglecting the other food categories. Furthermore, these papers did not represent knowledge about all stages of the food supply chain. This paper proposes and implements a food supply chain ontology. During the management of the entire food supply chain stages, the suggested ontology describes all types of food products.}
}
@article{MAVROMATIDIS2025105404,
title = {Constructal thermodynamics and its semantic ontology in autopoietic, digital, and computational architectural and urban space open systems},
journal = {BioSystems},
volume = {249},
pages = {105404},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2025.105404},
url = {https://www.sciencedirect.com/science/article/pii/S0303264725000140},
author = {Lazaros Mavromatidis},
keywords = {Complexity, constructal law, Exergy, Energy, Subject, Predicate, Gouy-Stodola theorem, Entropy, Heterotopia},
abstract = {This paper explores the intersections of constructal thermodynamics, and its semantic ontology within the context of autopoietic, digital and computational design in protocell inspired numerical architectural and urban narratives that are examined here as open systems. Constructal law is the thermodynamic theory based on the analysis of fluxes across the boundaries of an open system. Protocells, as dynamic, autopoietic and adaptive open finite size systems, serve in this paper as a compelling metaphor and design model for responsive and sustainable manmade architectural and urban environments. The ability of protocells to harness energy, minimize entropy, and adapt to environmental changes mirrors the principles of constructal thermodynamics, which govern the flow and distribution of resources in complex self-organizing information open systems in nature. By applying these principles to digital architecture, this study investigates how relational dynamics between spaces, materials, and functions can create adaptive designs that “go with the flow” of ecological and cultural systems. The research demonstrates using the Gouy-Stodola theorem as a variational principle, how protocell-inspired processes facilitate exergy-efficient designs, minimizing waste while maximizing resilience and flexibility. The present paper argues -through an applied case study- for a paradigm where protocell digital architecture serves not only as an ecological and material model but mainly as a spatial narrative driver, blending constructal and digital tools with cultural mythos. Finally, this paper exploring simultaneously the semantic complexity and ontology of such systems, in turn, connects these constructal driven digital designs to broader poly-narratives, embedding cultural, symbolic, philosophical and functional predicates into architectural forms.}
}
@article{KIM2021,
title = {A Care Knowledge Management System Based on an Ontological Model of Caring for People With Dementia: Knowledge Representation and Development Study},
journal = {Journal of Medical Internet Research},
volume = {23},
number = {6},
year = {2021},
issn = {1438-8871},
doi = {https://doi.org/10.2196/25968},
url = {https://www.sciencedirect.com/science/article/pii/S1438887121005884},
author = {Gyungha Kim and Hwawoo Jeon and Sung Kee Park and Yong Suk Choi and Yoonseob Lim},
keywords = {caregiver, caregiver for person with dementia, knowledge model, ontology, knowledge management, semantic reasoning},
abstract = {Background
Caregivers of people with dementia find it extremely difficult to choose the best care method because of complex environments and the variable symptoms of dementia. To alleviate this care burden, interventions have been proposed that use computer- or web-based applications. For example, an automatic diagnosis of the condition can improve the well-being of both the person with dementia and the caregiver. Other interventions support the individual with dementia in living independently.
Objective
The aim of this study was to develop an ontology-based care knowledge management system for people with dementia that will provide caregivers with a care guide suited to the environment and to the individual patient’s symptoms. This should also enable knowledge sharing among caregivers.
Methods
To build the care knowledge model, we reviewed existing ontologies that contain concepts and knowledge descriptions relating to the care of those with dementia, and we considered dementia care manuals. The basic concepts of the care ontology were confirmed by experts in Korea. To infer the different care methods required for the individual dementia patient, the reasoning rules as defined in Semantic Web Rule Languages and Prolog were utilized. The accuracy of the care knowledge in the ontological model and the usability of the proposed system were evaluated by using the Pellet reasoner and OntOlogy Pitfall Scanner!, and a survey and interviews were conducted with caregivers working in care centers in Korea.
Results
The care knowledge model contains six top-level concepts: care knowledge, task, assessment, person, environment, and medical knowledge. Based on this ontological model of dementia care, caregivers at a dementia care facility in Korea were able to access the care knowledge easily through a graphical user interface. The evaluation by the care experts showed that the system contained accurate care knowledge and a level of assessment comparable to normal assessment tools.
Conclusions
In this study, we developed a care knowledge system that can provide caregivers with care guides suited to individuals with dementia. We anticipate that the system could reduce the workload of caregivers.}
}
@article{PANZARELLA2023100059,
title = {Using ontologies for life science text-based resource organization},
journal = {Artificial Intelligence in the Life Sciences},
volume = {3},
pages = {100059},
year = {2023},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2023.100059},
url = {https://www.sciencedirect.com/science/article/pii/S266731852300003X},
author = {Giulia Panzarella and Pierangelo Veltri and Stefano Alcaro},
keywords = {Information overload, Ontology, Semantic web, Life science terms},
abstract = {Ontologies are used to support access to a multitude of databases that cover domains relevant information. Heterogeneity and different semantics can be accessed by using structured texts and descriptions in a hierarchical concept definition. We are interested in Life Sciences (LS) related ontologies including components taken from molecular biology, bioinformatics, physics, chemistry, medicine and other related areas. An Ontology comprises: (i) term connections, (ii) the identification of core concepts, (iii) data management, (iv) knowledge classification and integration to collect key information. An ontology may be very useful in navigating through LS terms. This paper explores some available biomedical ontologies and frameworks. It describes the most common ontology development environments (ODE): Protégé, Topbraid Composer, Ontostudio, Fluent Editor, VocBench, Swoop and Obo-edit, to create ontologies from textual scientific resources for LS plans. It also compares ontology methodologies in terms of Usability, Scalability, Stability, Integration, Documentation and Originality.}
}
@article{PEREIRA2023405,
title = {The Adoption of 4Step-Rule-Set Method for Ontological Design: Application in a Real Industrial Project},
journal = {Procedia Computer Science},
volume = {219},
pages = {405-415},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923003150},
author = {Tiago F. Pereira and Francisco Morais and Carlos E. Salgado and Ana Lima and António Silva and Manuel Pereira and João Oliveira and Ricardo J. Machado},
keywords = {Ontology Building, Development Method, Agile Method, Semantic Interoperability, Graph Database},
abstract = {Ontology building can greatly influence the development cycle of an information system and enhance interoperability among its constituent elements. Throughout the projects we have been developing we have detected, by studying the current literature, a need to develop an agile method to conceive and mapping ontologies, which allows a quick and effective response to R&D projects. Designing a method for building an ontology, which is integrated and aligned with a systematic development approach, represents a crucial challenge in new approaches to system design and exploitation. Extant proposed methods for building an ontology, especially following agile approaches, have achieved interesting results but lack integration and alignment with a wider-view development framework. Thus, we have defined the first version of a semantic model allowing the alignment with the previously defined information model. Following the best practices for ontology building and based on our previous work on software system development, we now propose a method for designing an ontology, the 4SRS Method for Ontological Design based on the V-Model 4SRS, aligning it with a proven development method. We further demonstrate this approach by applying the proposed method in a real case, to develop an ontology for a choen restricted scope within the domain problem.}
}
@article{VARVARIGOU2023527,
title = {Ontology-based Solution for Handling Safety and Cybersecurity Interdependency in NFV Safety Architecture},
journal = {Procedia Computer Science},
volume = {220},
pages = {527-534},
year = {2023},
note = {The 14th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) and The 6th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.03.067},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923006026},
author = {Dionysia Varvarigou and David Espes and Giacomo Bersano},
keywords = {Safety ontology, NFV safety architecture, Safety, cybersecurity interdependency},
abstract = {For safety-critical systems based on Network Function Virtualization (NFV) technology both safety and cybersecurity need to be taken into account. In case these systems face an anomaly (either intentional or not), they can have an impact on humans and environment. Thus, it is understood that safety and cybersecurity affect each other and so they need to be considered as interdependent. For this reason, an ontology-based solution for safety is needed, through which it is possible to handle the safety and cybersecurity interdependency since ontologies describe the whole knowledge of a domain. Thus, we propose a new safety ontology for NFV framework which is able to cover reliability, availability, maintainability, and integrity-related breakdown types. This is because these properties interact and influence safety according to ENISA. Our ontology allows us to have a uniformized representation of the potential anomalies that a system and its elements can face. Based on this representation, it is also possible to have a decision-making process to avoid potential conflicts between safety and cybersecurity, in order to best handle their interdependency. According to the results of our implementation, it is shown that our ontology handles the safety and cybersecurity interdependency, and has little impact on decision-making time, which makes it an effective methodology for NFV framework.}
}
@article{CERIANI2023100787,
title = {Semantic integration of audio content providers through the Audio Commons Ontology},
journal = {Journal of Web Semantics},
volume = {77},
pages = {100787},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100787},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000161},
author = {Miguel Ceriani and Fabio Viola and Saša Rudan and Francesco Antoniazzi and Mathieu Barthet and György Fazekas},
keywords = {Ontology, Audio content, Web API integration},
abstract = {A broad variety of audio content is available online through an increasing number of repositories and platforms. Resources such as music tracks, recorded sounds or instrument samples may be accessed by users for tasks ranging from customised music listening and exploration, to music making and sound design using existing sounds and samples. However, each online repository offers its own API and represents information through its own data model, making it difficult for applications to exploit the plurality of online audio and music content on the web. A crucial step toward integrating audio repositories in a flexible manner is a shared basis for modelling the data therein. This paper describes and extends the Audio Commons Ontology, a common data model designed to integrate existing repositories in the audio media domain. The ontology is designed with the involvement of users through surveys and requirements analyses, and evaluated in-use, by demonstrating how it supports the integration of four relevant repositories with heterogeneous APIs and data models. While this work proves the concept in the audio domain, our proposed methodology may transfer across a broad range of media integration tasks.}
}
@article{CLEMENTE2022118171,
title = {A proposal for an adaptive Recommender System based on competences and ontologies},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118171},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118171},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422013392},
author = {Julia Clemente and Héctor Yago and Javier {de Pedro-Carracedo} and Javier Bueno},
keywords = {Recommender system, , Ontology network, Methodological development, Student modeling},
abstract = {Context:
Competences represent an interesting pedagogical support in many processes like diagnosis or recommendation. From these, it is possible to infer information about the progress of the student to provide help targeted both, trainers who must make adaptive tutoring decisions for each learner, and students to detect and correct their learning weaknesses. For the correct development of any of these tasks, it is important to have a suitable student model that allows the representation of the most significant information possible about the student. Additionally, it would be very advantageous for this modeling to incorporate mechanisms from which it would be possible to infer more information about the student’s state of knowledge.
Objective:
To facilitate this goal, in this paper a new approach to develop an adaptive competence-based recommender system is proposed.
Method:
We present a methodological development guide as well as a set of ontological and non-ontological resources to develop and adapt the prototype of the proposed recommender system.
Results:
A modular flexible ontology network previously built for this purpose has been extended, which is responsible for recording the instructional design and student information. Furthermore, we describe a case study based on a first aid learning experience to assess the prototype with the proposed methodology.
Conclusions:
We highlight the relevance of flexibility and adaptability in learning modeling and recommendation processes. In order to promote improvement in the personalized learning of students, we present a Recommender System prototype taking advantages of ontologies, with a methodological guide, a broad taxonomy of recommendation criteria and the nature of competences. Future lines of research lines, including a more comprehensive evaluation of the system, will allow us to demonstrate in depth its adaptability according to the characteristics of the student, flexibility and extensibility for its integration in various environments and domains.}
}
@article{GHEDINI20236370,
title = {An ontology to integrate process-based approach in ZDM strategies in a Digital Twin framework},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {6370-6375},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.825},
url = {https://www.sciencedirect.com/science/article/pii/S240589632301203X},
author = {Lorenzo Ghedini and Adalberto Polenghi and Marco Macchi},
keywords = {Zero Defect Manufacturing, Ontology, Digital Twin, Quality Monitoring, Process-oriented approach, Product-oriented approach},
abstract = {The current context of Industry 4.0 is characterized by challenges that were not present in the past like greater variability, higher customization and greater complexity. To address these challenges and the increasing need from companies to focus on sustainability-related issues is necessary to adopt a quality improvement method. In this paper, the method considered is Zero Defect Manufacturing (ZDM) a “tool” which shows considerable potential, but needs some auxiliary technologies to operate. In this regard, the model proposed is an ontology based on a pre-existent ontology. This new ontology is capable of applying Detect and Repair strategies to go with the creation of a Digital Twin of the product. The realized ontology can be used to support decision-making in an industrial context: in fact, it provides to any operator in the production process, an indication about the quality of the product, also advising some corrective actions if needed, like the repair or the disassembly of the product and the subsequent recycling of the good quality components. To obtain this outcome, an analysis of the literature was performed to determine the gaps present in the literature, then an ontology editor allowed the creation of the ontology and, finally, the ontology was validated in the context of Industry 4.0 Laboratory at the Politecnico di Milano. In this environment, the proposed solution was populated with the data coming from the servers, determining the quality of the product as a function of the state of product components and the condition of one of the assets installed in the production line.}
}
@article{KESHAVARZI2023107520,
title = {An ontology-driven framework for knowledge representation of digital extortion attacks},
journal = {Computers in Human Behavior},
volume = {139},
pages = {107520},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2022.107520},
url = {https://www.sciencedirect.com/science/article/pii/S0747563222003405},
author = {Masoudeh Keshavarzi and Hamid Reza Ghaffary},
keywords = {Ransomware, Cyber-ontology, Conceptual modeling, Knowledge base, Knowledge graph, Philosophy of computer science},
abstract = {With the COVID-19 pandemic and the growing influence of the Internet in critical sectors of industry and society, cyberattacks have not only not declined, but have risen sharply. In the meantime, ransomware is at the forefront of the most devastating threats that have launched the lucrative illegal business. Due to the proliferation and variety of ransomware forays, there is a need for a new theory of categories. The intricacy and multiplicity of components involved in digital extortions entails the construction of a knowledge representation system that is able to organize large volumes of information from heterogeneous sources in a formal structured format and infer new knowledge from it. This paper suggests and develops a dedicated ontology of digital blackmails, called Rantology, with a particular focus on ransomware assaults. The logic coded in this ontology allows to assess the maliciousness of programs based on various factors, including called API functions and their behaviors. The proposed framework can be used to facilitate interoperability between cybersecurity experts and knowledge-based systems, and identify sensitive points for surveillance. The evaluation results based on several criteria confirm the adequacy of the suggested ontology in terms of clarity, modularity, consistency, coverage and inheritance richness.}
}
@article{MANZINI2022117446,
title = {Mapping layperson medical terminology into the Human Phenotype Ontology using neural machine translation models},
journal = {Expert Systems with Applications},
volume = {204},
pages = {117446},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.117446},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422007813},
author = {Enrico Manzini and Jon Garrido-Aguirre and Jordi Fonollosa and Alexandre Perera-Lluna},
keywords = {Machine translation, Word embedding, Deep learning, Medical informatics, Deep phenotyping, Human Phenotype Ontology},
abstract = {In the medical domain there exists a terminological gap between patients and caregivers and the healthcare professionals. This gap may hinder the success of the communication between healthcare consumers and professionals in the field, with negative emotional and clinical consequences. In this work, we build a machine learning-based tool for the automatic translation between the terminology used by laypeople and that of the Human Phenotype Ontology (HPO). HPO is a structured vocabulary of phenotypic abnormalities found in human disease. Our method uses a vector space to represent an HPO-specific embedding as the output space for a neural network model trained on vector representations of layperson versions and other textual descriptors of medical terms. We explored different output embeddings coupled to different neural network architectures for the machine translation stage. We compute a similarity measure to evaluate the ability of the model to assign an HPO term to a layperson input. The best-performing models resulted with a similarity higher than 0.7 for more than 80% of the terms, with a median between 0.98 and 1. The translator model is made available in a web application at this link: https://hpotranslator.b2slab.upc.edu.}
}
@article{LIU2025,
title = {Intelligent Fault Diagnosis for CNC Through the Integration of Large Language Models and Domain Knowledge Graphs},
journal = {Engineering},
year = {2025},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2025.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2095809925001948},
author = {Yuhan Liu and Yuan Zhou and Yufei Liu and Zhen Xu and Yixin He},
keywords = {Large language model, Domain knowledge graph, Knowledge graph-based retrieval augmented generation, Learning mechanism, Decision support system},
abstract = {As large language models (LLMs) continue to demonstrate their potential in handling complex tasks, their value in knowledge-intensive industrial scenarios is becoming increasingly evident. Fault diagnosis, a critical domain in the industrial sector, has long faced the dual challenges of managing vast amounts of experiential knowledge and improving human–machine collaboration efficiency. Traditional fault diagnosis systems, which are primarily based on expert systems, suffer from three major limitations: ① ineffective organization of fault diagnosis knowledge, ② lack of adaptability between static knowledge frameworks and dynamic engineering environments, and ③ difficulties in integrating expert knowledge with real-time data streams. These systemic shortcomings restrict the ability of conventional approaches to handle uncertainty. In this study, we proposed an intelligent computer numerical control (CNC) fault diagnosis system, integrating LLMs with knowledge graph (KG). First, we constructed a comprehensive KG that consolidated multi-source data for structured representation. Second, we designed a retrieval-augmented generation (RAG) framework leveraging the KG to support multi-turn interactive fault diagnosis while incorporating real-time engineering data into the decision-making process. Finally, we introduced a learning mechanism to facilitate dynamic knowledge updates. The experimental results demonstrated that our system significantly improved fault diagnosis accuracy, outperforming engineers with two years of professional experience on our constructed benchmark datasets. By integrating LLMs and KG, our framework surpassed the limitations of traditional expert systems rooted in symbolic reasoning, offering a novel approach to addressing the cognitive paradox of unstructured knowledge modeling and dynamic environment adaptation in industrial settings.}
}
@article{FAN2024103646,
title = {CuPe-KG: Cultural perspective–based knowledge graph construction of tourism resources via pretrained language models},
journal = {Information Processing & Management},
volume = {61},
number = {3},
pages = {103646},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103646},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000062},
author = {Zhanling Fan and Chongcheng Chen},
keywords = {Knowledge graph, Pretrained language models, Cultural tourism, Cultural type, ChatGPT, Travel intelligence},
abstract = {Tourism knowledge graphs lack cultural content, limiting their usefulness for cultural tourists.This paper presents the development of a cultural perspective-based knowledge graph (CuPe-KG). We evaluated fine-tuning ERNIE 3.0 (FT-ERNIE) and ChatGPT for cultural type recognition to strengthen the relationship between tourism resources and cultures. Our investigation used an annotated cultural tourism resource dataset containing 2,745 items across 16 cultural types. The results showed accuracy scores for FT-ERNIE and ChatGPT of 0.81 and 0.12, respectively, with FT-ERNIE achieving a micro-F1 score of 0.93, a 26 percentage point lead over ChatGPT's score of 0.67. These underscore FT-ERNIE's superior performance (the shortcoming is the need to annotate data) while highlighting ChatGPT's limitations because of insufficient Chinese training data and lower identification accuracy in professional knowledge. A novel ontology was designed to facilitate the construction of CuPe-KG, including elements such as cultural types, historical figures, events, and intangible cultural heritage. CuPe-KG effectively addresses cultural tourism visitors’ information retrieval needs.}
}
@incollection{RODRIGUEZFERNANDEZ20241129,
title = {Ontology-driven automation of process modelling and simulation},
editor = {Flavio Manenti and Gintaras V. Reklaitis},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {53},
pages = {1129-1134},
year = {2024},
booktitle = {34th European Symposium on Computer Aided Process Engineering / 15th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-28824-1.50189-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443288241501897},
author = {Alberto Rodriguez-Fernandez and Vinay Kumar Gautam and Heinz A. Preisig},
keywords = {Ontology, simulation, computational engineering, automatic code generation},
abstract = {NTNU's Process Modelling suite, ProMo, adopts a rigorous ontology-driven modelling paradigm that aligns with the foundational tenets of physical, chemical, and biological systems. This approach serves as the basis for the systematic development of models. Creating a coherent variable/expression list, manifested as a multi-bipartite graph, forms the bedrock for assembling fundamental building blocks. These building blocks, in turn, expedite the construction of complex models, ensuring coherence across the modelling lifecycle. This work showcases our method for preparing model data to be integrated into the code generation process.}
}
@article{SOHR2022371,
title = {Decision Modeling for an ISA-95 based Production Ontology},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {371-376},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.421},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322017074},
author = {Annelie Sohr and Franz Georg Listl and Katharina Ecker and Jan Fischer and Jan Christoph Wehrstedt and Michael Weyrich},
keywords = {Decision Support, Digital Twin, Simulation, Shop Floor Management, Production Planning, Scheduling, Domain Model, Semantic Model},
abstract = {Decision support systems in production and manufacturing help to achieve economic and ecological objectives and increase flexibility and resilience. Semantic models as knowledge base for these systems enable and empower analytical components such as machine learning, simulation, and optimization algorithms. This paper discusses some of the relevant aspects of such semantic models. Existing solutions and standards for common data models in manufacturing are explored, and based on that, alternatives and decisions for managing manufacturing operations are modeled in an ontology. Specifically, a general ontology for handling decisions in any type of factory is introduced.}
}
@article{MONACO2023137614,
title = {A non-functional requirements-based ontology for supporting the development of industrial energy management systems},
journal = {Journal of Cleaner Production},
volume = {414},
pages = {137614},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.137614},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623017729},
author = {Roberto Monaco and Xiufeng Liu and Teresa Murino and Xu Cheng and Per Sieverts Nielsen},
keywords = {Non-functional requirements, Ontology, Framework, Energy management, Software development},
abstract = {Non-functional requirements (NFRs) are essential in the development of industrial energy management systems (IEMS). This paper presents a comprehensive ontology model for NFRs in IEMS, derived from an extensive survey of NFRs in both the software industry and the energy domain. The proposed ontology encompasses four critical factors influencing the quality attributes of IEMS: technologies, stakeholders, markets, and regulations. Implemented using the OWL 2 DL standard, the ontology model aims to provide a clear and consistent understanding of NFRs and their relationship to the energy domain, as well as identify which factors have a significant impact on the environmental performance of IEMS. The ontology is evaluated through various methods, such as technical validation, user evaluations, its applications in RDF data management, and application-based evaluation, including software architecture, knowledge base data model design, and regulatory framework design. By understanding the link between software quality requirements and the characterizing factors of the energy domain, as provided by our ontology model, this information can be used to inform life cycle assessments and quantify potential reductions in energy consumption, emissions, waste generation, and other environmental impacts associated with implementing cleaner and more sustainable solutions. The results demonstrate that the proposed ontology effectively supports IEMS development and serves as a foundation for developing reusable and adaptable software systems across different industrial domains.}
}
@article{FERRARI2025107697,
title = {Formal requirements engineering and large language models: A two-way roadmap},
journal = {Information and Software Technology},
volume = {181},
pages = {107697},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2025.107697},
url = {https://www.sciencedirect.com/science/article/pii/S0950584925000369},
author = {Alessio Ferrari and Paola Spoletini},
keywords = {Requirements engineering, Formal methods, Large language models, LLMs, Natural language processing, NLP, NLP4RE, Prompt engineering, Prompt requirements engineering},
abstract = {Context:
Large Language Models (LLMs) have made remarkable advancements in emulating human linguistic capabilities, showing potential also in executing various requirements engineering (RE) tasks. However, despite their generally good performance, the adoption of LLM-generated solutions and artefacts prompts concerns about their correctness, fairness, and trustworthiness.
Objective:
This paper aims to address the concerns associated with the use of LLMs in RE activities. Specifically, it seeks to develop a roadmap that leverages formal methods (FMs) to provide guarantees of correctness, fairness, and trustworthiness when LLMs are utilised in RE. Symmetrically, it aims to explore how LLMs can be employed to make FMs more accessible.
Methods:
We use two sets of examples to show the current limits of FMs when used in software development and of LLMs when used for RE tasks. The highlighted limitations are addressed by proposing two roadmaps grounded in the current literature and technologies.
Results:
The proposed examples show the potential and limits of FMs in supporting software development and of LLMs when used for RE tasks. The initial investigation into how these limitations can be overcome has been concretised in two detailed roadmaps for the RE and, more largely, the software engineering community.
Conclusion:
The proposed roadmaps offer a promising approach to address the concerns of correctness, fairness, and trustworthiness associated with the use of LLMs in RE tasks through the use of FMs and to enhance the accessibility of FMs by utilising LLMs.}
}
@article{ZHOU2023104649,
title = {BIM and ontology-based knowledge management for dam safety monitoring},
journal = {Automation in Construction},
volume = {145},
pages = {104649},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104649},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522005192},
author = {Yuhang Zhou and Tengfei Bao and Xiaosong Shu and Yueyang Li and Yangtao Li},
keywords = {Ontology, BIM, Reasoning, Information extraction, Relational database, Dam, Dam safety monitoring system},
abstract = {Dam Safety Monitoring Systems (DSMSs) are crucial in evaluating the operational state of dams. However, heterogeneous monitoring data distributed from multiple sources during the dam operation lacks a unified data integration method and prohibits knowledge extraction and intelligent analysis, which currently poses a labor-intensive task. To address this issue, a solution relying on Building Information Modeling (BIM) and domain ontologies is proposed. Specifically, a DSMS domain ontology (OntoDSMS) is developed by comprehensively collecting domain knowledge and extracting context information from the dam information model. Furthermore, a rule-based reasoner and SPARQL queries are implemented. The proposed approach facilitates the effective integration of dam safety monitoring information while reducing retrieval time effectively compared with traditional databases. A case is illustrated to demonstrate the feasibility and practicality of the proposed approach.}
}
@article{SHANNON202186,
title = {Comparative study using inverse ontology cogency and alternatives for concept recognition in the annotated National Library of Medicine database},
journal = {Neural Networks},
volume = {139},
pages = {86-104},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000265},
author = {George J. Shannon and Naga Rayapati and Steven M. Corns and Donald C. Wunsch},
keywords = {Cogent confabulation, Natural language processing, Concept recognition, Language model},
abstract = {This paper introduces inverse ontology cogency, a concept recognition process and distance function that is biologically-inspired and competitive with alternative methods. The paper introduces inverse ontology cogency as a new alternative method. It is a novel distance measure used in selecting the optimum mapping between ontology-specified concepts and phrases in free-form text. We also apply a multi-layer perceptron and text processing method for named entity recognition as an alternative to recurrent neural network methods. Automated named entity recognition, or concept recognition, is a common task in natural language processing. Similarities between confabulation theory and existing language models are discussed. This paper provides comparisons to MetaMap from the National Library of Medicine (NLM), a popular tool used in medicine to map free-form text to concepts in a medical ontology. The NLM provides a manually annotated database from the medical literature with concepts labeled, a unique, valuable source of ground truth, permitting comparison with MetaMap performance. Comparisons for different feature set combinations are made to demonstrate the effectiveness of inverse ontology cogency for entity recognition. Results indicate that using both inverse ontology cogency and corpora cogency improved concept recognition precision 20% over the best published MetaMap results. This demonstrates a new, effective approach for identifying medical concepts in text. This is the first time cogency has been explicitly invoked for reasoning with ontologies, and the first time it has been used on medical literature where high-quality ground truth is available for quality assessment.}
}
@article{NGUYEN2023100788,
title = {Pattern-based detection, extraction and analysis of code lists in ontologies and vocabularies},
journal = {Journal of Web Semantics},
volume = {77},
pages = {100788},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100788},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000173},
author = {Viet Bach Nguyen and Vojtěch Svátek},
keywords = {Code list, Knowledge base, Knowledge graph, Knowledge representation, Ontology, RDF, Semantic Web},
abstract = {While the early phase of the Semantic Web put emphasis on conceptual modeling through ontology classes, and the recent years saw the rise of loosely structured, instance-level knowledge graphs (used even for modeling concepts), in this paper, we focus on a third kind of concept modeling: via code lists, primarily those embedded in ontologies and vocabularies. We attempt to characterize the candidate structures for code lists based on our observations in OWL ontologies. Our main contribution is then an approach implemented as a series of SPARQL queries and a lightweight web application that can be used to browse and detect potential code lists in ontologies and vocabularies, in order to extract and enhance them, and to store them in a stand-alone knowledge base. The application allows inspecting query results coming from the Linked Open Vocabularies catalog dataset. In addition, we describe a complementary bottom-up analysis of potential code lists. We also provide in this paper a demonstration of the dominant nature of embedded codes from the aspect of ontological universals and their alternatives for modeling code lists.}
}
@article{TAYUR2022100616,
title = {Multi-ontology mapping generative adversarial network in internet of things for ontology alignment},
journal = {Internet of Things},
volume = {20},
pages = {100616},
year = {2022},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2022.100616},
url = {https://www.sciencedirect.com/science/article/pii/S2542660522000981},
author = {Varun M Tayur and R Suchithra},
keywords = {Generative adversarial network, Ontology alignment, IoT and OntoGenerator and OntoLSTM},
abstract = {On the Semantic web, ontologies are thought to be the remedy to data heterogeneity, and correlating ontologies is a highly effective technique. Although the use of representation learning approaches to a variety of applications has showed significant promise, they have had little effect on the issue of ontology matching and classification. In order to establish alignments between two ontologies, this research presents the Multi-Ontology Mapping Generative Adversarial Network in Internet of Things (MOMGANI). For the instance of ontology mapping, we suggest using a two-system representation learning network consisting of a Generator and Discriminator. The Generator applies a probabilistic softmax classifier to the different Name, Label, Comments, Properties, Instance descriptions, concept characteristics, and the neighbourhood concepts for each of the ontology's properties. In order to support the assertions that the Generator has generated, the Discriminator network employs a novel Bidirectional Long Short-Term Memory (Bi-LSTM network) with an Ontology Attention mechanism enhanced by the concept's descriptions. As a result, both systems are in a feedback mechanism where they can learn from one another. The system will produce a set of triples that list all the associated concepts from various ontologies as its final product. Domain experts will review these triples outside of the band to ensure that only true concepts and triples are chosen for the alignment. In comparison to using the ontologies separately, the aligned ontology enables extended querying and inference across related ontologies and domains. Considering metrics like recall, precision, and F-measure, the experimental evaluation was performed utilizing the datasets for classes alignment, property alignment, and instances alignment. The proposed architecture provides a recall, precision, and F-measure of 0.92, 0.99, and 0.83 respectively which reveals that this model outperforms the traditional methods.}
}
@article{KARIM2025100004,
title = {Large language models and their applications in roadway safety and mobility enhancement: A comprehensive review},
journal = {Artificial Intelligence for Transportation},
volume = {1},
pages = {100004},
year = {2025},
issn = {3050-8606},
doi = {https://doi.org/10.1016/j.ait.2025.100004},
url = {https://www.sciencedirect.com/science/article/pii/S3050860625000043},
author = {Muhammad Monjurul Karim and Yan Shi and Shucheng Zhang and Bingzhang Wang and Mehrdad Nasri and Yinhai Wang},
keywords = {LLMs in transportation, Road safety, Mobility, Intelligent transportation systems, Multimodal large language model, AI in transportation},
abstract = {Roadway safety and mobility remain critical challenges for modern transportation systems, demanding innovative analytical frameworks capable of addressing complex, dynamic, and heterogeneous environments. While traditional engineering methods have made progress, the complexity and dynamism of real-world traffic necessitate more advanced analytical frameworks. Large Language Models (LLMs), with their unprecedented capabilities in natural language understanding, knowledge integration, and reasoning, represent a promising paradigm shift. This paper comprehensively reviews the application and customization of LLMs for enhancing roadway safety and mobility. A key focus is how LLMs are adapted—via architectural, training, prompting, and multimodal strategies—to bridge the “modality gap” with transportation’s unique spatio-temporal and physical data. The review systematically analyzes diverse LLM applications in mobility (e.g., traffic flow prediction, signal control, simulation) and safety (e.g., crash analysis, driver behavior assessment, rule formalization). Enabling technologies such as V2X integration, domain-specific foundation models, explainability frameworks, and edge computing are also examined. Despite significant potential, challenges persist regarding inherent LLM limitations (hallucinations, reasoning deficits), data governance (privacy, bias), deployment complexities (sim-to-real, latency), and rigorous safety assurance. Promising future research directions are highlighted, including advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI collaboration, continuous learning, and the development of efficient, verifiable systems. This review provides a structured roadmap of current capabilities, limitations, and opportunities, underscoring LLMs’ transformative potential while emphasizing the need for responsible innovation to realize safer, more intelligent transportation systems.}
}
@article{CALVOCIDONCHA2023,
title = {An Ontology-Based Approach to Improving Medication Appropriateness in Older Patients: Algorithm Development and Validation Study},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/45850},
url = {https://www.sciencedirect.com/science/article/pii/S2291969423000364},
author = {Elena Calvo-Cidoncha and Julián Verdinelli and Javier González-Bueno and Alfonso López-Soto and Concepción {Camacho Hernando} and Xavier Pastor-Duran and Carles Codina-Jané and Raimundo Lozano-Rubí},
keywords = {biological ontologies, decision support systems, inappropriate prescribing, elderly, medication regimen complexity, anticholinergic drug burden, trigger tool, clinical, ontologies, pharmacy, medication, decision support, pharmaceutic, pharmacology, chronic condition, chronic disease, domain, adverse event, ontology-based, alert},
abstract = {Background
Inappropriate medication in older patients with multimorbidity results in a greater risk of adverse drug events. Clinical decision support systems (CDSSs) are intended to improve medication appropriateness. One approach to improving CDSSs is to use ontologies instead of relational databases. Previously, we developed OntoPharma—an ontology-based CDSS for reducing medication prescribing errors.
Objective
The primary aim was to model a domain for improving medication appropriateness in older patients (chronic patient domain). The secondary aim was to implement the version of OntoPharma containing the chronic patient domain in a hospital setting.
Methods
A 4-step process was proposed. The first step was defining the domain scope. The chronic patient domain focused on improving medication appropriateness in older patients. A group of experts selected the following three use cases: medication regimen complexity, anticholinergic and sedative drug burden, and the presence of triggers for identifying possible adverse events. The second step was domain model representation. The implementation was conducted by medical informatics specialists and clinical pharmacists using Protégé-OWL (Stanford Center for Biomedical Informatics Research). The third step was OntoPharma-driven alert module adaptation. We reused the existing framework based on SPARQL to query ontologies. The fourth step was implementing the version of OntoPharma containing the chronic patient domain in a hospital setting. Alerts generated from July to September 2022 were analyzed.
Results
We proposed 6 new classes and 5 new properties, introducing the necessary changes in the ontologies previously created. An alert is shown if the Medication Regimen Complexity Index is ≥40, if the Drug Burden Index is ≥1, or if there is a trigger based on an abnormal laboratory value. A total of 364 alerts were generated for 107 patients; 154 (42.3%) alerts were accepted.
Conclusions
We proposed an ontology-based approach to provide support for improving medication appropriateness in older patients with multimorbidity in a scalable, sustainable, and reusable way. The chronic patient domain was built based on our previous research, reusing the existing framework. OntoPharma has been implemented in clinical practice and generates alerts, considering the following use cases: medication regimen complexity, anticholinergic and sedative drug burden, and the presence of triggers for identifying possible adverse events.}
}
@article{CASCALLARFUENTES2022108612,
title = {Automatic generation of textual descriptions in data-to-text systems using a fuzzy temporal ontology: Application in air quality index data series},
journal = {Applied Soft Computing},
volume = {119},
pages = {108612},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108612},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622001120},
author = {Andrea Cascallar-Fuentes and Javier Gallego-Fernández and Alejandro Ramos-Soto and Anthony Saunders-Estévez and Alberto Bugarín-Diz},
keywords = {Fuzzy linguistic terms, Linguistic descriptions of data, Data to text systems, Natural language generation},
abstract = {In this paper we present a model based on computational intelligence and natural language generation for the automatic generation of textual summaries from numerical data series, aiming to provide insights which help users to understand the relevant information hidden in the data. Our model includes a fuzzy temporal ontology with temporal references which addresses the problem of managing imprecise temporal knowledge, which is relevant in data series. We fully describe a real use case of application in the environmental information systems field, providing linguistic descriptions about the air quality index (AQI), which is a very well-known indicator provided by all meteorological agencies worldwide. We consider two different data sources of real AQI data provided by the official Galician (NW Spain) Meteorology Agency: (i) AQI distribution in the stations of the meteorological observation network and (ii) time series which describe the state and evolution of the AQI in each meteorological station. Both application models were evaluated following the current standards and good practices of manual human expert evaluation of the Natural Language Generation field. Assessment results by two experts meteorologists were very satisfactory, which empirically confirm that the proposed textual descriptions fit this type of data and service both in content and layout.}
}
@article{KRONK2021104601,
title = {An ontology-based review of transgender literature: Revealing a history of medicalization and pathologization},
journal = {International Journal of Medical Informatics},
volume = {156},
pages = {104601},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2021.104601},
url = {https://www.sciencedirect.com/science/article/pii/S1386505621002276},
author = {Clair A. Kronk and Judith W. Dexheimer},
keywords = {Transgender Persons, Transsexualism, Biological Ontologies, Bibliography, Natural Language Processing},
abstract = {Objectives
To evaluate the linguistic changes of transgender-related resources prior to 1999 to create a comprehensive dataset of resources using an ontology-derived search system, laying a framework for ontology-based reviews to be used in informatics.
Methods
We analyzed 77 bibliographies and 11 databases for transgender resources published prior to 31 December 1999. We used 858 variants of the term “transgender” to identify resources. Individual sources were tagged by subject matter and major conceptual terminology usage. We evaluated the accuracy of a Gender, Sex, and Sexual Orientation (GSSO) ontology-based mechanism on tagging relevant literature searches.
Results
We identified 3,058 sources in 19 languages. Primary subjects covered included surgery, psychology, psychiatry, endocrinology, and sexology. The GSSO-based tagging mechanism correctly tagged 97.7% of MEDLINE resources as transgender-related.
Discussion
The GSSO-based tagging mechanism was more effective than keyword-specific elucidations of terminologically complex literature and was just as effective at manual identification of subjects discussed within resources. Diverse language relating to transgender persons can be identified using the GSSO, which can also be used for structured literature review based on subject matter thus improving research in the area.}
}
@article{SEIPEL2018102,
title = {Domain-specific languages in Prolog for declarative expert knowledge in rules and ontologies},
journal = {Computer Languages, Systems & Structures},
volume = {51},
pages = {102-117},
year = {2018},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2017.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1477842416301804},
author = {Dietmar Seipel and Falco Nogatz and Salvador Abreu},
keywords = {Domain-specific language, Expert knowledge, Declarative rule, , Deductive database},
abstract = {Declarative if–then rules have proven very useful in many applications of expert systems. They can be managed in deductive databases and evaluated using the well-known forward-chaining approach. For domain-experts, however, the syntax of rules becomes complicated quickly, and already many different knowledge representation formalisms exist. Expert knowledge is often acquired in story form using interviews. In this paper, we discuss its representation by defining domain-specific languages (Dsls) for declarative expert rules. They can be embedded in Prolog systems in internal Dsls using term expansion and as external Dsls using definite clause grammars and quasi-quotations – for more sophisticated syntaxes. Based on the declarative rules and the integration with the Prolog-based deductive database system DDbase, multiple rules acquired in practical case studies can be combined, compared, graphically analysed by domain-experts, and evaluated, resulting in an extensible system for expert knowledge. As a result, the actual modeling Dsl becomes executable; the declarative forward-chaining evaluation of deductive databases can be understood by the domain experts. Our Dsl for rules can be further improved by integrating ontologies and rule annotations.}
}
@article{GRIFFO2021101454,
title = {Service contract modeling in Enterprise Architecture: An ontology-based approach},
journal = {Information Systems},
volume = {101},
pages = {101454},
year = {2021},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.101454},
url = {https://www.sciencedirect.com/science/article/pii/S030643791930506X},
author = {Cristine Griffo and João Paulo A. Almeida and Giancarlo Guizzardi and Julio Cesar Nardi},
keywords = {Legal contracts, Service modeling, Enterprise architecture, Service contract ontology, ArchiMate},
abstract = {Service contracts bind parties legally, regulating their behavior in the scope of a (business) service relationship. Given that there are legal consequences attached to service contracts, understanding the elements of a contract is key to managing services in an enterprise. After all, provisions in a service contract and in legislation establish obligations and rights for service providers and customers that must be respected in service delivery. The importance of service contracts to service provisioning in an enterprise has motivated us to investigate their representation in enterprise models. We have observed that approaches fall into two extremes of a spectrum. Some approaches, such as ArchiMate, offer an opaque “contract” construct, not revealing the rights and obligations in the scope of the governed service relationship. Other approaches, under the umbrella term “contract languages”, are devoted exactly to the formal representation of the contents of contracts. Despite the applications of contract languages, they operate at a level of detail that does not match that of enterprise architecture models. In this paper, we explore and bridge the gap between these two extremes. We address the representation of service contract elements with a systematic approach: we first propose a well-founded service contract ontology, and then extend the ArchiMate language to reflect the elements of the service contract ontology. The applicability of the proposed extension is assessed in the representation of a real-world cloud service contract.}
}
@article{RODLER2022108987,
title = {One step at a time: An efficient approach to query-based ontology debugging},
journal = {Knowledge-Based Systems},
volume = {251},
pages = {108987},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108987},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004786},
author = {Patrick Rodler},
keywords = {Ontology debugging, Query-based ontology debugging, Interactive debugging, Fault localization, Sequential diagnosis, Expert questions, Ontology quality assurance, Ontology repair, Test-driven debugging, Singleton query, Model-based diagnosis, Semantic Web, User interaction, Effort minimization, Performance optimization, Entailment, Test cases, Query computation, Query selection, One-step lookahead, Debugging cost criteria, Query quality assessment, Expert types, Heuristics, Knowledge-base debugging, OntoDebug, Protégé},
abstract = {When ontologies reach a certain size and complexity, faults such as inconsistencies, unsatisfiable classes or wrong entailments are hardly avoidable. Locating the incorrect axioms that cause these faults is a hard and time-consuming task. Addressing this issue, several techniques for a semi-automatic fault localization in ontologies have been proposed and extensively studied. One class of these approaches involve a human expert who provides answers to system-generated queries about the intended (correct) ontology in order to reduce the possible fault locations. To suggest as informative questions as possible, existing methods draw on various algorithmic optimizations as well as heuristics. However, these computations are often based on certain assumptions about the interacting expert. In this work, we demonstrate that these assumptions might not always be adequate and discuss consequences of their violations. In particular, we characterize a range of expert types with different query answering behavior and show that existing approaches are far from achieving optimal efficiency for all of them. In addition, we find that the cost metric adopted by state-of-the-art techniques might not always be realistic and that a change of metric has a decisive impact on the best choice of query answering strategy. As a remedy, we suggest a new – and simpler – type of expert question that leads to a stable fault localization performance for all analyzed expert types and effort metrics, and has numerous further advantages over existing techniques. Moreover, we present an algorithm which computes and optimizes this new query type in worst-case polynomial time and which is fully compatible with existing concepts (e.g., query selection heuristics) and infrastructure (e.g., debugging user interfaces) in the field. Comprehensive experiments on faulty real-world ontologies attest that the new querying method is substantially and statistically significantly superior to existing techniques both in terms of the number of necessary expert interactions and in terms of the query computation time. We find that relying on the new querying method can save an interacting expert more than 80% of their work, and can reduce the expert’s waiting time for the next query by more than three orders of magnitude. Beside these findings, we demonstrate that the efficiency of existing query-based tools can be significantly boosted by suggesting an appropriate query answering strategy to an expert; we also make recommendations in this regard. Further, we suggest optimal configurations of a debugger for situations where the new type of query is used. Remarkably, the proposed approach is not only applicable to ontologies, but to any monotonic knowledge representation language, and can even be adopted to solve general model-based diagnosis problems expressible using Reiter’s theory.}
}
@article{TSANEVA2025104145,
title = {Knowledge graph validation by integrating LLMs and human-in-the-loop},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104145},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104145},
url = {https://www.sciencedirect.com/science/article/pii/S030645732500086X},
author = {Stefani Tsaneva and Danilo Dessì and Francesco Osborne and Marta Sabou},
keywords = {Knowledge graph validation, Large language models, Hybrid human-AI workflows},
abstract = {Ensuring the quality of knowledge graphs (KGs) is crucial for the success of the intelligent applications they support. Recent advances in large language models (LLMs) have demonstrated human-level performance across various tasks, raising the question of their potential for KG validation. In this work, we explore the role of LLMs in human-centric KG validation workflows, examining different collaboration strategies between LLMs and domain experts. We propose and evaluate nine distinct approaches, ranging from fully automated validation to hybrid methods that combine expert oversight with AI assistance. These workflows are tested within a real-world KG construction pipeline used to generate the Computer Science Knowledge Graph (CS-KG), a large-scale resource designed to support scientometric tasks such as trend forecasting and hypothesis generation. CS-KG comprises 41 million statements represented as 350 million triples within the Computer Science domain. Our findings show that integrating LLMs into the CS-KG verification process enhances precision by 12%, improving alignment with expert-level validation. However, this comes at the cost of recall, resulting in a 5% decrease in the overall F1 score. In contrast, a hybrid approach which involves both human-in-the-loop and LLM modules, yields the best overall results, improving F1 score by 5% with minimal human involvement.}
}
@article{DAVARPANAH2025111736,
title = {Semantic knowledge and data modeling of environmental justice},
journal = {Engineering Applications of Artificial Intelligence},
volume = {159},
pages = {111736},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111736},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625017385},
author = {Armita Davarpanah and Hassan A. Babaie and Fatemeh Shafiei and Na'Taki Osborne Jelks},
keywords = {Environmental justice, Ontology, Knowledge graph, Semantic web rule language, Proctor creek watershed},
abstract = {Most environmental justice information and data are unstructured, posing substantial challenges for effective risk management, disaster response, and comprehensive environmental justice assessments through software tools. To overcome these barriers, we created the Environmental Justice Ontology, a structured framework that organizes essential environmental justice concepts and their complex relationships, enabling machines to interpret environmental justice knowledge. The ontology applies the Semantic Web Rule Language to model EJScreen measures, including % Unemployment and % People of Color. Leveraging the widely adopted Basic Formal Ontology and Common Core Ontologies, Environmental Justice Ontology organizes environmental justice resources—including reports, charts, and datasets—to enable seamless data exchange across various domains. We applied the ontology to structure environmental justice data gathered from the Proctor Creek Watershed in Atlanta, Georgia, using the U.S. Environmental Protection Agency's EJScreen tool. Applying Neo4J AuraDB and natural language processing techniques in Python, we developed two knowledge graphs to support the visualization, querying, and analysis of this environmental justice data. Together, the ontology and knowledge graphs create a structured framework for uncovering complex interactions and correlations between natural and social systems, fostering a comprehensive understanding of the interconnections between environmental, socioeconomic, demographic, and other influencing factors. This framework empowers users and software to assist policymakers in designing sustainable solutions for overburdened communities facing environmental, health, and climate-related challenges. The ontology encompasses a broad range of classes—including disparities, burdens, discrimination, bias, and the challenges faced by disadvantaged communities—that extends beyond the capabilities of EJScreen, enabling seamless integration of future environmental justice data and knowledge.}
}
@article{YANG2023326,
title = {An ontology-based shop-floor digital twin configuration approach},
journal = {Procedia CIRP},
volume = {120},
pages = {326-331},
year = {2023},
note = {56th CIRP International Conference on Manufacturing Systems 2023},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.08.058},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123007291},
author = {Xiaolang Yang and Xuemei Liu and Heng Zhang and Ling Fu and Yanbin Yu},
keywords = {Digital twin, Shop-floor digital twin, Ontology, Resource allocation},
abstract = {Digital Twin (DT) technology has been considered as the one of key enabling technologies of intelligent manufacturing. DT is regarded as the digital representation of physical entities to mirror their state and behavior in the virtual space. The development of modeling and simulation technology makes the construction of DT professional and systematic. A lot of modeling methods and virtual models for DT construction are accumulated. However, the value of existing scattered models is not fully considered. These models could be used as virtual resources for the DT construction to save time and costs. Therefore, this paper proposes an ontology-based approach to support the configuration of Shop-floor Digital Twin (SDT). The ontologies are defined based on the analysis of the multi-domain heterogeneous models and knowledge. The top-level ontology is represented as the demand-oriented integration view of SDT. The shop-floor manufacturing resources and activities could be represented by the object-oriented virtual resources based on the resource-level ontology and are integrated into a SDT scheme with the domain specific meta-modeling method, which contributes to the rapid configuration of SDT. Finally, the feasibility of this approach is verified by the case of the Fischer Learning Factory 4.0 (FLF4.0).}
}
@article{LIU20251242,
title = {A survey of cognitive digital twin and the potential use of LLMs},
journal = {Manufacturing Letters},
volume = {44},
pages = {1242-1253},
year = {2025},
note = {53rd SME North American Manufacturing Research Conference (NAMRC 53)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2025.06.144},
url = {https://www.sciencedirect.com/science/article/pii/S2213846325001762},
author = {Yangyang Liu and Tang Ji and Xiangyu Guo and Xun Xu and Jan Polzer},
keywords = {Cognitive Digital Twin (CDT), Large Language Models (LLMs), Industry 4.0, Smart manufacturing, Artificial intelligence},
abstract = {Digital Twin (DT) technology is pivotal for Industry 4.0 and intelligent manufacturing, enabling real-time monitoring and high-fidelity digital representations of physical entities. However, traditional DTs face challenges in autonomous decision-making, environmental adaptability, reasoning abilities, and functionality when applied to dynamic, complex scenarios. Cognitive Digital Twin (CDT) have emerged to address these challenges, offering enhanced cognitive capabilities over conventional DT. Concurrently, advancements in Large Language Models (LLMs) present new opportunities to augment CDT through advanced semantic understanding and contextual analysis. This paper systematically reviews CDT development, elucidating their core cognitive mechanisms and assessing the impact of LLMs on their future evolution. By analyzing existing literature, we outline the developmental context, core concepts, key characteristics, and framework construction methods of CDT. We identify significant research gaps, notably the lack of systematic articulation of CDT’s cognitive mechanisms and the underexplored potential of integrating LLMs to enhance their cognitive functions. Our contributions are threefold: (1) providing the research progress of CDT, (2) exploring the application potential of LLMs in enhancing CDT’s cognitive capabilities, and (3) identifying key challenges in applying LLMs within manufacturing environments. Future research should focus on developing comprehensive integration frameworks for LLMs and CDT and improving the reliability and security of CDT systems. By advancing CDT technology and integrating LLM capabilities, this study hopes to support the realization of Industry 4.0.}
}
@article{RUEHLE20251534,
title = {Natural language processing for automated workflow and knowledge graph generation in self-driving labs††Electronic supplementary information (ESI) available: Links to further resources, detailed description of dataset creation steps, and further figures and tables. See DOI: https://doi.org/10.1039/d5dd00063g},
journal = {Digital Discovery},
volume = {4},
number = {6},
pages = {1534-1543},
year = {2025},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d5dd00063g},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X25000786},
author = {Bastian Ruehle},
abstract = {Natural language processing with the help of large language models such as ChatGPT has become ubiquitous in many software applications and allows users to interact even with complex hardware or software in an intuitive way. The recent concepts of Self-Driving Labs and Material Acceleration Platforms stand to benefit greatly from making them more accessible to a broader scientific community through enhanced user-friendliness or even completely automated ways of generating experimental workflows that can be run on the complex hardware of the platform from user input or previously published procedures. Here, two new datasets with over 1.5 million experimental procedures and their (semi)automatic annotations as action graphs, i.e., structured output, were created and used for training two different transformer-based large language models. These models strike a balance between performance, generality, and fitness for purpose and can be hosted and run on standard consumer-grade hardware. Furthermore, the generation of node graphs from these action graphs as a user-friendly and intuitive way of visualizing and modifying synthesis workflows that can be run on the hardware of a Self-Driving Lab or Material Acceleration Platform is explored. Lastly, it is discussed how knowledge graphs – following an ontology imposed by the underlying node setup and software architecture – can be generated from the node graphs. All resources, including the datasets, the fully trained large language models, the node editor, and scripts for querying and visualizing the knowledge graphs are made publicly available.}
}
@article{OLIVARESALARCOS2022103627,
title = {OCRA – An ontology for collaborative robotics and adaptation},
journal = {Computers in Industry},
volume = {138},
pages = {103627},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103627},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000227},
author = {Alberto Olivares-Alarcos and Sergi Foix and Stefano Borgo and  {Guillem Alenyà,}},
keywords = {Ontology, Ontological analysis, Industrial collaborative robots, Human-robot collaboration, Adaptation, Reliability},
abstract = {Industrial collaborative robots will be used in unstructured scenarios and a large variety of tasks in the near future. These robots shall collaborate with humans, who will add uncertainty and safety constraints to the execution of industrial robotic tasks. Hence, trustworthy collaborative robots must be able to reason about their collaboration’s requirements (e.g., safety), as well as the adaptation of their plans due to unexpected situations. A common approach to reasoning is to represent the knowledge of interest using logic-based formalisms, such as ontologies. However, there is not an established ontology defining notions such as collaboration or adaptation yet. In this article, we propose an Ontology for Collaborative Robotics and Adaptation (OCRA), which is built around two main notions: collaboration, and plan adaptation. OCRA ensures a reliable human-robot collaboration, since robots can formalize, and reason about their plan adaptations and collaborations in unstructured collaborative robotic scenarios. Furthermore, our ontology enhances the reusability of the domain’s terminology, allowing robots to represent their knowledge about different collaborative and adaptive situations. We validate our formal model, first, by demonstrating that a robot may answer a set of competency questions using OCRA. Second, by studying the formalization’s performance in limit cases that include instances with incongruent and incomplete axioms. For both validations, the example use case consists in a human and a robot collaborating on the filling of a tray.}
}
@article{MA2023101851,
title = {A proposed ontology to support the hardware design of building inspection robot systems},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101851},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101851},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622003093},
author = {Leyuan Ma and Timo Hartmann},
keywords = {Building Inspection Robot, Robot Hardware Design, Robot Hardware Requirement, Ontology, Knowledge Representation},
abstract = {Due to the growing number of architecturally complex buildings built in recent decades, mobile building inspection robot systems are required to operate in increasingly complex environments. This leads to higher requirements for their hardware design. The existing literature on the design of building inspection robots has implicitly mentioned the impact of building environments and building defects on defining robot hardware design requirements. However, the explicit representation of what information is required to define a specific hardware requirement is stimissing. To fill this gap, this paper presents an ontology that provides an overview of the building and inspection domain objects that affect the determination of robot hardware design requirements (RoboDesign). It also explores the relationship between specific robot hardware requirements and features of complex buildings and their defects. The RoboDesign ontology integrates two main domain ontology models including a Robot System Model and a Building and Defect Model. A content evaluation and an automated consistency checking are conducted for the internal evaluation of the ontology. Additionally, the proposed ontology is implemented in two wall-climbing inspection robot design cases to check if the investigation of the robot’s application environment is comprehensive. The validation results also demonstrate that the use of the proposed ontological model allows to efficiently retrieve information required to determine a particular hardware requirement.}
}
@article{ZOU20191229,
title = {Facilitating Consistency of Business Model and Technical Models in Product-Service-Systems Development: An Ontology Approach},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1229-1235},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.366},
url = {https://www.sciencedirect.com/science/article/pii/S240589631931345X},
author = {M. Zou and M.R. Basirati and H. Bauer and N. Kattner and G. Reinhart and U. Lindemann and M. Böhm and H. Krcmar and B. Vogel-Heuser},
keywords = {product-service systems (PSS), model-based engineering, ontology, aligning business, technical models, model consistency},
abstract = {Due to the fast growing and ever changing innovation cycles, industries are changing their strategies from a product-centric to service-centric approach, leading to the emergence of product-service systems (PSS) which integrate services into physical technical systems. In the model-based development of PSS, various models are employed by different stakeholders to represent their views on the system. However, there is a high diversity of these models in both forms and contents. Among others, business models and technical models come in different levels of abstraction, and thus, are hard to align with each other. In this study, we propose an approach to support PSS development by relating business models to technical models using ontology. Web ontology language (OWL) is employed to describe PSS knowledge, the Query Language SPARQL and Semantic Web Rule Language (SWRL) are used for consistency checking and reasoning potential inconsistencies.}
}
@article{RICO2023103856,
title = {Context-aware representation of digital twins’ data: The ontology network role},
journal = {Computers in Industry},
volume = {146},
pages = {103856},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103856},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000064},
author = {Mariela Rico and María Laura Taverna and María Rosa Galli and María Laura Caliusco},
keywords = {Digital Twins, Context-aware representation, Ontology network},
abstract = {A Digital Factory is made up of various Digital Twins (DTs) that interact with each other to improve decision-making. With this aim, DTs need to share accurate and high-quality data that guarantees their integration at syntactic, semantic, and semiotic levels. Effective decision-making requires consideration of the context features to which each DT belongs. In order to facilitate data exchange, this paper proposes the use of contextualized ontologies, obtained from an ontology network in which context features are made explicit.}
}
@article{GU2024102822,
title = {Automatic quantitative stroke severity assessment based on Chinese clinical named entity recognition with domain-adaptive pre-trained large language model},
journal = {Artificial Intelligence in Medicine},
volume = {150},
pages = {102822},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102822},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724000642},
author = {Zhanzhong Gu and Xiangjian He and Ping Yu and Wenjing Jia and Xiguang Yang and Gang Peng and Penghui Hu and Shiyan Chen and Hongjie Chen and Yiguang Lin},
keywords = {Automatic stroke severity assessment, Chinese electronic health records, Clinical named entity recognition, Domain-adaptive pre-training, Large language model},
abstract = {Background:
Stroke is a prevalent disease with a significant global impact. Effective assessment of stroke severity is vital for an accurate diagnosis, appropriate treatment, and optimal clinical outcomes. The National Institutes of Health Stroke Scale (NIHSS) is a widely used scale for quantitatively assessing stroke severity. However, the current manual scoring of NIHSS is labor-intensive, time-consuming, and sometimes unreliable. Applying artificial intelligence (AI) techniques to automate the quantitative assessment of stroke on vast amounts of electronic health records (EHRs) has attracted much interest.
Objective:
This study aims to develop an automatic, quantitative stroke severity assessment framework through automating the entire NIHSS scoring process on Chinese clinical EHRs.
Methods:
Our approach consists of two major parts: Chinese clinical named entity recognition (CNER) with a domain-adaptive pre-trained large language model (LLM) and automated NIHSS scoring. To build a high-performing CNER model, we first construct a stroke-specific, densely annotated dataset “Chinese Stroke Clinical Records” (CSCR) from EHRs provided by our partner hospital, based on a stroke ontology that defines semantically related entities for stroke assessment. We then pre-train a Chinese clinical LLM coined “CliRoberta” through domain-adaptive transfer learning and construct a deep learning-based CNER model that can accurately extract entities directly from Chinese EHRs. Finally, an automated, end-to-end NIHSS scoring pipeline is proposed by mapping the extracted entities to relevant NIHSS items and values, to quantitatively assess the stroke severity.
Results:
Results obtained on a benchmark dataset CCKS2019 and our newly created CSCR dataset demonstrate the superior performance of our domain-adaptive pre-trained LLM and the CNER model, compared with the existing benchmark LLMs and CNER models. The high F1 score of 0.990 ensures the reliability of our model in accurately extracting the entities for the subsequent automatic NIHSS scoring. Subsequently, our automated, end-to-end NIHSS scoring approach achieved excellent inter-rater agreement (0.823) and intraclass consistency (0.986) with the ground truth and significantly reduced the processing time from minutes to a few seconds.
Conclusion:
Our proposed automatic and quantitative framework for assessing stroke severity demonstrates exceptional performance and reliability through directly scoring the NIHSS from diagnostic notes in Chinese clinical EHRs. Moreover, this study also contributes a new clinical dataset, a pre-trained clinical LLM, and an effective deep learning-based CNER model. The deployment of these advanced algorithms can improve the accuracy and efficiency of clinical assessment, and help improve the quality, affordability and productivity of healthcare services.}
}
@article{TANG2023101426,
title = {Construction and application of an ontology-based domain-specific knowledge graph for petroleum exploration and development},
journal = {Geoscience Frontiers},
volume = {14},
number = {5},
pages = {101426},
year = {2023},
issn = {1674-9871},
doi = {https://doi.org/10.1016/j.gsf.2022.101426},
url = {https://www.sciencedirect.com/science/article/pii/S1674987122000792},
author = {Xianming Tang and Zhiqiang Feng and Yitian Xiao and Ming Wang and Tianrui Ye and Yujie Zhou and Jin Meng and Baosen Zhang and Dongwei Zhang},
keywords = {Knowledge Graph, Petroleum exploration and development, Natural language processing, Ontology},
abstract = {The massive amount and multi-sourced, multi-structured data in the upstream petroleum industry impose great challenge on data integration and smart application. Knowledge graph, as an emerging technology, can potentially provide a way to tackle the challenges associated with oil and gas big data. This paper proposes an engineering-based method that can improve upon traditional natural language processing to construct the domain knowledge graph based on a petroleum exploration and development ontology. The exploration and development knowledge graph is constructed by assembling Sinopec’s multi-sourced heterogeneous database, and millions of nodes. The two applications based on the constructed knowledge graph are developed and validated for effectiveness and advantages in providing better knowledge services for the oil and gas industry.}
}
@article{DEGIORGIS2025104127,
title = {Neurosymbolic graph enrichment for Grounded World Models},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104127},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104127},
url = {https://www.sciencedirect.com/science/article/pii/S030645732500069X},
author = {Stefano {De Giorgis} and Aldo Gangemi and Alessandro Russo},
keywords = {Neurosymbolic AI, Knowledge representation, Knowledge extraction, Large language models, Graph KAG, Hybrid reasoning},
abstract = {The development of artificial intelligence systems capable of understanding and reasoning about complex real-world scenarios is a significant challenge. In this work we present a novel approach to enhance and exploit LLM reactive capability to address complex problems and interpret deeply contextual real-world meaning. We introduce a method and a tool for creating a multimodal, knowledge-augmented formal representation of meaning that combines the strengths of large language models with structured semantic representations. Our method begins with an image input, utilizing state-of-the-art large language models to generate a natural language description. This description is then transformed into an Meaning Representation (AMR) graph, which is formalized and enriched with logical design patterns, and layered semantics derived from linguistic and factual knowledge bases. The resulting graph is then fed back into the LLM to be extended with implicit knowledge activated by complex heuristic learning, including semantic implicatures, moral values, embodied cognition, and metaphorical representations. By bridging the gap between unstructured language models and formal semantic structures, our method opens new avenues for tackling intricate problems in natural language understanding and reasoning.}
}
@article{LAMICHHANE2025100420,
title = {Context-aware decision making in autonomous vehicles: Integrating social behavior modeling with large language models},
journal = {Array},
volume = {27},
pages = {100420},
year = {2025},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2025.100420},
url = {https://www.sciencedirect.com/science/article/pii/S2590005625000475},
author = {Badri Raj Lamichhane and Aueaphum Aueawatthanaphisut and Gun Srijuntongsiri and Teerayut Horanont},
keywords = {Autonomous vehicles, Social context, Scene understanding, Large language model, Reinforcement learning},
abstract = {Integrating context-aware decision-making in autonomous vehicles (AVs) is critical for advancing operational efficiency, safety, and user experience. However, existing frameworks struggle to incorporate social context into real-time navigation, relying on deterministic algorithms or reinforcement learning models that overlook implicit social norms and face challenges in translating LLM-derived reasoning into safety-compliant control policies. This paper investigates the application of social behavior modeling fused with large language models (LLMs) to establish a comprehensive framework for context-aware understanding and decision-making processes by AVs. Through understanding of the scene and the deployment of LLMs, this framework enables AVs to interpret and respond to complex social interactions and contextual cues, enhancing adaptability in dynamic environments. We propose concepts and approaches to foster context-aware and socially responsible decision-making processes, including test cases for validation to some level. The results demonstrate substantial improvements in decision accuracy adopting virtual simulation, providing a foundation for addressing complex ethical dilemmas and real-time decision-making challenges that AVs encounter in diverse and dynamic settings.}
}
@article{XU2025105642,
title = {Large language model applications in disaster management: An interdisciplinary review},
journal = {International Journal of Disaster Risk Reduction},
volume = {127},
pages = {105642},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2025.105642},
url = {https://www.sciencedirect.com/science/article/pii/S2212420925004662},
author = {Fengyi Xu and Jun Ma and Nan Li and Jack C.P. Cheng},
keywords = {Disaster management, Large language models, Information processing, Multi-modal data fusion, Emergency response system},
abstract = {Disasters increasingly challenge urban resilience, demanding advanced computational approaches for effective information management and response coordination. This interdisciplinary review systematically assesses Large Language Model (LLM) applications in disaster management, analyzing 70 LLM-focused studies within the broader landscape of AI-driven disaster management. Our analysis establishes a phase-based framework spanning detection, tracking, analysis, and action, and reveals three critical gaps in current disaster management solutions: limited advancement beyond disaster response to include preparedness, recovery, and mitigation phases; insufficient integration across diverse stakeholder groups and available resources; and inadequate transformation of situation awareness data into actionable insights. Leveraging cross-modal semantic reasoning, knowledge graph-constrained entity extraction, and advanced code generation, LLMs are well positioned to overcome information ambiguity and verification challenges often encountered in rapidly evolving disaster contexts. These capabilities also enable automation in disaster investigation and communication, effectively orchestrating diverse analytical tools and resources. To harness these advantages and promote further progress, we introduce the “3M” framework for intelligent disaster information management: multi-modal data fusion for integrated assessment, multi-source information validation for robust truth-finding, and multi-agent collaboration in physical–virtual disaster systems. This framework provides a systematic foundation for advancing next-generation LLM-driven disaster management research and practice in increasingly complex contexts}
}
@article{BOUDIA2022,
title = {Formalization of Ontology Conceptualizations Using Model Transformation},
journal = {International Journal of Information System Modeling and Design},
volume = {13},
number = {1},
year = {2022},
issn = {1947-8186},
doi = {https://doi.org/10.4018/IJISMD.305229},
url = {https://www.sciencedirect.com/science/article/pii/S1947818622000424},
author = {Malika Boudia and Mustapha Bourahla},
keywords = {Conceptual Model, Description Logic, MDA (Model-Driven Architecture), Model Transformation, Ontology, OWL (Web Ontology Language), Semantic Web, TGGs (Triple Graph Grammars)},
abstract = {ABSTRACT
Conceptual models are built with concepts and relationships between them to reach a unified view of domain problems. There are many kinds of conceptual models developed in different modeling languages, such as class diagrams and entity-relationship models. In this paper, the authors have developed a specific meta-model following the Ecore standard to define conceptual models. These domain-specific conceptual models can be automatically formalized as domain ontologies using model transformation with the technique of triple graph grammars into ontology formal descriptions in accordance with the defined Ecore meta-model of the language OWL (web ontology language). For ontology deployment, its OWL code may be generated from OWL models using model-to-code transformation guided by Xpand templates. A performance evaluation is realized using a benchmark from the university domain with very large conceptual models. Through the experiments, they validate the performance and we prove the exactness and the scalability of the automatic transformation process of conceptual models.}
}
@article{JIANG2022101449,
title = {Multi-ontology fusion and rule development to facilitate automated code compliance checking using BIM and rule-based reasoning},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101449},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101449},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621002019},
author = {Liu Jiang and Jianyong Shi and Chaoyu Wang},
keywords = {Automated compliance checking, Ontology, Rule-based reasoning, Building information modeling, Chinese building codes},
abstract = {Code compliance checking plays a critical role that identifies substandard designs according to regulatory documents and promises the accuracy of the designs before construction. However, the traditional code compliance checking process relies heavily on human work. To help the users better understand the checking process, this study proposes a gray-box checking technique and a BIM-based (Building Information Modeling)automated code compliance checking methodology that leverages ontology. The proposed approach contains a code ontology, a designed model ontology, a merged ontology, a code compliance checking ontology, a set of mapping rules, and a set of checking rules. During the checking process, the ontologies provide knowledge bases, and the rules provide necessary logic. A five-step roadmap is proposed for code ontology development for domain experts. For the time being, pre-processing is applied to create the designed model ontology to achieve time saving. Next, an ontology mapping procedure between the code and the designed model ontology is executed to obtain the merged ontology. In the ontology mapping procedure, the mapping rules aim to mitigate the semantic ambiguity between design information and regulatory information and enrich building information's semantics. Subsequently, rule-based reasoning is applied based on the checking rules and the merged ontology for checking reports generation. Finally, according to Chinese building codes, an automated code compliance checking platform is implemented for real construction projects to validate the proposed methodology.}
}
@article{LI2022119484,
title = {A novel Ontology-guided Attribute Partitioning ensemble learning model for early prediction of cognitive deficits using quantitative Structural MRI in very preterm infants},
journal = {NeuroImage},
volume = {260},
pages = {119484},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119484},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922006000},
author = {Zhiyuan Li and Hailong Li and Adebayo Braimah and Jonathan R. Dillman and Nehal A. Parikh and Lili He},
keywords = {Machine learning, Ensemble learning, Ontology, Structural MRI, Brain image, Neuroimaging, Early prediction, Preterm infants},
abstract = {Structural magnetic resonance imaging studies have shown that brain anatomical abnormalities are associated with cognitive deficits in preterm infants. Brain maturation and geometric features can be used with machine learning models for predicting later neurodevelopmental deficits. However, traditional machine learning models would suffer from a large feature-to-instance ratio (i.e., a large number of features but a small number of instances/samples). Ensemble learning is a paradigm that strategically generates and integrates a library of machine learning classifiers and has been successfully used on a wide variety of predictive modeling problems to boost model performance. Attribute (i.e., feature) bagging method is the most commonly used feature partitioning scheme, which randomly and repeatedly draws feature subsets from the entire feature set. Although attribute bagging method can effectively reduce feature dimensionality to handle the large feature-to-instance ratio, it lacks consideration of domain knowledge and latent relationship among features. In this study, we proposed a novel Ontology-guided Attribute Partitioning (OAP) method to better draw feature subsets by considering the domain-specific relationship among features. With the better-partitioned feature subsets, we developed an ensemble learning framework, which is referred to as OAP-Ensemble Learning (OAP-EL). We applied the OAP-EL to predict cognitive deficits at 2 years of age using quantitative brain maturation and geometric features obtained at term equivalent age in very preterm infants. We demonstrated that the proposed OAP-EL approach significantly outperformed the peer ensemble learning and traditional machine learning approaches.}
}
@article{SHI2023116213,
title = {Research on a methodology for intelligent seismic performance evaluation and optimization design of buildings based on IFC and ontology},
journal = {Engineering Structures},
volume = {288},
pages = {116213},
year = {2023},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2023.116213},
url = {https://www.sciencedirect.com/science/article/pii/S0141029623006272},
author = {Jianyong Shi and Zeyu Pan and Liu Jiang and Peizhi Chen and Chao An and Nazhaer Mulatibieke},
keywords = {Seismic performance assessment, Seismic loss prediction, FEMA P-58, Ontology, IFC},
abstract = {Seismic loss prediction and performance assessment for buildings have already been hot spots for resilience after earthquakes with the emergence of the concepts of digital twins. However, because a large amount of multisource heterogeneous data and a real-time accurate prediction model are required to implement the process, the popularization and implementation of these technologies are hampered. Based on performance-based seismic design (PBSD) theory, this study introduces BIM and ontology and other emerging technologies to carry out methodology research on intelligent earthquake damage prediction and seismic performance evaluation for individual buildings. First, based on FEMA P-58 (Seismic Performance Assessment of Buildings), this paper uses BIM technology to integrate and transmit component-level engineering information. Ontology method is used to express the assessment content and logic, and a multidimensional semantic ontology model based on IFC is proposed to form the linked data foundation to organize, store, associate and interact the multisource heterogeneous data required for assessment in a unified way to realize the semi-automated seismic performance assessment for individual RC frame building. On this basis, the seismic optimization design under the guidance of the “investment - benefit” criterion is regarded as the problem of seeking the balance between the initial construction cost and the expectation of seismic loss. The multi-objective genetic algorithm (NSGA-II) is used to realize the optimization iteration at the component level. According to the results of the case study, the new method in this paper significantly improves the quality of seismic performance assessment and seismic design optimization.}
}
@article{CIMA2023103976,
title = {The notion of Abstraction in Ontology-based Data Management},
journal = {Artificial Intelligence},
volume = {323},
pages = {103976},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103976},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223001224},
author = {Gianluca Cima and Antonella Poggi and Maurizio Lenzerini},
keywords = {Abstraction, Ontology-based Data Management, Computational complexity},
abstract = {We study a novel reasoning task in Ontology-based Data Management (OBDM), called Abstraction, which aims at associating formal semantic descriptions to data services. In OBDM a domain ontology is used to provide a semantic layer mapped to the data sources of an organization. The basic idea of the work presented in this paper is to explain the semantics of a data service in terms of a query over the ontology. We illustrate a formal framework for this problem, based on three different notions of abstraction, called sound, complete, and perfect, respectively. We present a thorough complexity analysis of two computational problems, namely verification (checking whether a query is an abstraction of a given data service), and computation (computing an abstraction of a given data service).}
}
@article{VARGA202343,
title = {Infrastructure and city ontologies},
journal = {Proceedings of the Institution of Civil Engineers - Smart Infrastructure and Construction},
volume = {176},
number = {2},
pages = {43-52},
year = {2023},
issn = {2397-8759},
doi = {https://doi.org/10.1680/jsmic.22.00005},
url = {https://www.sciencedirect.com/science/article/pii/S2397875923000078},
author = {Liz Varga and Lauren McMillan and Stephen Hallett and Tom Russell and Luke Smith and Ian Truckell and Andrey Postnikov and Sunil Rodger and Noel Vizcaino and Bethan Perkins and Brian Matthews and Nik Lomax},
keywords = {artificial intelligence, city-scale infrastructure operations, city-scale simulations & data analytics, critical infrastructure, data, data analytics for infrastructure, digital twin, information technology, infrastructure planning, modelling, UN SDG 9: Industry, innovation and infrastructure, urban infrastructure development},
abstract = {The creation and use of ontologies has become increasingly relevant for complex systems in recent years. This is because of the growing number of use of cases that rely on real-world integration of disparate systems, the need for semantic congruence across boundaries and the expectations of users for conceptual clarity within evolving domains or systems of interest. These needs are evident in most spheres of research involving complex systems, but they are particularly apparent in infrastructure and cities where traditionally siloed and sectoral approaches have dominated, undermining the potential for integration to solve societal challenges such as net zero, resilience to climate change, equity and affordability. This paper reports on findings of a literature review on infrastructure and city ontologies and puts forward some hypotheses inferred from the literature findings. The hypotheses are discussed with reference to the literature and provide avenues for further research on (a) belief systems that underpin non-top-level ontologies and the potential for interference from them, (b) the need for a small number of top-level ontologies and translation mechanisms between them and (c) clarity on the role of standards and information systems in the adaptability and quality of data sets using ontologies. A gap is also identified in the extent that ontologies can support more complex automated coupling and data transformation when dealing with different scales.}
}
@article{BENJDIRA2025107723,
title = {Prompting Robotic Modalities (PRM): A structured architecture for centralizing language models in complex systems},
journal = {Future Generation Computer Systems},
volume = {166},
pages = {107723},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2025.107723},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X25000184},
author = {Bilel Benjdira and Anis Koubaa and Anas M. Ali},
keywords = {Expert systems architectures, Robotics, Languages models in robotics, Prompting robotic modalities, Large language models, LLMs, Vision language models, VLMs, Robotic operating system, ROS, ROS2, Robotic prompt engineering, Visual prompt, LLM prompt},
abstract = {Despite significant advancements in robotics and AI, existing systems often struggle to integrate diverse modalities (e.g., image, sound, actuator data) into a unified framework, resulting in fragmented architectures that limit adaptability, scalability, and explainability. To address these gaps, this paper introduces Prompting Robotic Modalities (PRM), a novel architecture that centralizes language models for controlling and managing complex systems through natural language. In PRM, each system modality (e.g., image, sound, actuator) is handled independently by a Modality Language Model (MLM), while a central Task Modality, powered by a Large Language Model (LLM), orchestrates complex tasks using information from the MLMs. Each MLM is trained on datasets that pair modality-specific data with rich textual descriptions, enabling intuitive, language-based interaction. We validate PRM with two main contributions: (1) ROSGPT_Vision, a new open-source ROS 2 package (available at https://github.com/bilel-bj/ROSGPT_Vision) for visual modality tasks, achieving up to 66% classification accuracy in driver-focus monitoring—surpassing other tested models in its category; and (2) CarMate, a driver-distraction detection application that significantly reduces development time and cost by allowing rapid adaptation to new monitoring tasks via simple prompt adjustments. In addition, we develop a Navigation Language Model (NLM) that converts free-form human language orders into detailed ROS commands, underscoring PRM’s modality-agnostic adaptability. Experimental results demonstrate that PRM simplifies system development, outperforms baseline vision-language approaches in specialized tasks (e.g., driver monitoring), reduces complexity through prompt engineering rather than extensive coding, and enhances explainability via natural-language-based diagnostics. Hence, PRM lays a promising foundation for next-generation complex and robotic systems by integrating advanced language model capabilities at their core, making them more adaptable to new environments, cost-effective, and user-friendly.}
}
@article{XU2025103244,
title = {A large language model-enabled machining process knowledge graph construction method for intelligent process planning},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103244},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103244},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625001375},
author = {Qingfeng Xu and Fei Qiu and Guanghui Zhou and Chao Zhang and Kai Ding and Fengtian Chang and Fengyi Lu and Yongrui Yu and Dongxu Ma and Jiancong Liu},
keywords = {Large language models, Knowledge graphs, Machining process, Intelligent process planning},
abstract = {As a pivotal step in translating design into production, process planning significantly influences product quality, cost, production efficiency, and market competitiveness. The process knowledge base, a fundamental element of process planning, determines the intelligence level of product manufacturing. Methods that construct process knowledge bases using Knowledge Graphs (KGs) have increasingly become critical technologies for supporting intelligent process planning. However, traditional deep learning-based named entity recognition methods for constructing KGs require extensive manual effort in domain-specific data annotation, resulting in inefficiencies, prolonged construction cycles, and high costs. To address these challenges, this paper introduces a Large Language Model-enabled method for constructing Machining Process KGs (LLM-MPKG). Initially, Large Language Models (LLMs) are employed to pre-annotate machining process text datasets. A verifier is then developed to assess and filter the pre-annotated datasets, with domain experts re-annotating deficient data to create a high-quality annotated machining process dataset. Subsequently, using this dataset and a fine-tuned LLM, a machining process knowledge extraction model, MPKE-GPT, is constructed. MPKE-GPT is then applied to extract knowledge from process planning case data for 50 parts within an enterprise, leading to the creation of the MPKG. A prototype system was also developed to support intelligent process planning. Compared to traditional deep learning methods, the proposed method reduces construction time by 48.58%, lowers costs by 46.44%, and enhances performance by 1.96%.}
}
@article{LAWSON2021100028,
title = {The Data Use Ontology to streamline responsible access to human biomedical datasets},
journal = {Cell Genomics},
volume = {1},
number = {2},
pages = {100028},
year = {2021},
issn = {2666-979X},
doi = {https://doi.org/10.1016/j.xgen.2021.100028},
url = {https://www.sciencedirect.com/science/article/pii/S2666979X21000355},
author = {Jonathan Lawson and Moran N. Cabili and Giselle Kerry and Tiffany Boughtwood and Adrian Thorogood and Pinar Alper and Sarion R. Bowers and Rebecca R. Boyles and Anthony J. Brookes and Matthew Brush and Tony Burdett and Hayley Clissold and Stacey Donnelly and Stephanie O.M. Dyke and Mallory A. Freeberg and Melissa A. Haendel and Chihiro Hata and Petr Holub and Francis Jeanson and Aina Jene and Minae Kawashima and Shuichi Kawashima and Melissa Konopko and Irene Kyomugisha and Haoyuan Li and Mikael Linden and Laura Lyman Rodriguez and Mizuki Morita and Nicola Mulder and Jean Muller and Satoshi Nagaie and Jamal Nasir and Soichi Ogishima and Vivian {Ota Wang} and Laura D. Paglione and Ravi N. Pandya and Helen Parkinson and Anthony A. Philippakis and Fabian Prasser and Jordi Rambla and Kathy Reinold and Gregory A. Rushton and Andrea Saltzman and Gary Saunders and Heidi J. Sofia and John D. Spalding and Morris A. Swertz and Ilia Tulchinsky and Esther J. {van Enckevort} and Susheel Varma and Craig Voisin and Natsuko Yamamoto and Chisato Yamasaki and Lyndon Zass and Jaime M. {Guidry Auvil} and Tommi H. Nyrönen and Mélanie Courtot},
keywords = {data access, consent, FAIR, ontology, GA4GH, standard, controlled access, data restrictions, secondary data use, automated data access},
abstract = {Summary
Human biomedical datasets that are critical for research and clinical studies to benefit human health also often contain sensitive or potentially identifying information of individual participants. Thus, care must be taken when they are processed and made available to comply with ethical and regulatory frameworks and informed consent data conditions. To enable and streamline data access for these biomedical datasets, the Global Alliance for Genomics and Health (GA4GH) Data Use and Researcher Identities (DURI) work stream developed and approved the Data Use Ontology (DUO) standard. DUO is a hierarchical vocabulary of human and machine-readable data use terms that consistently and unambiguously represents a dataset’s allowable data uses. DUO has been implemented by major international stakeholders such as the Broad and Sanger Institutes and is currently used in annotation of over 200,000 datasets worldwide. Using DUO in data management and access facilitates researchers’ discovery and access of relevant datasets. DUO annotations increase the FAIRness of datasets and support data linkages using common data use profiles when integrating the data for secondary analyses. DUO is implemented in the Web Ontology Language (OWL) and, to increase community awareness and engagement, hosted in an open, centralized GitHub repository. DUO, together with the GA4GH Passport standard, offers a new, efficient, and streamlined data authorization and access framework that has enabled increased sharing of biomedical datasets worldwide.}
}
@article{ALASWADI2023103140,
title = {Enhancing relevant concepts extraction for ontology learning using domain time relevance},
journal = {Information Processing & Management},
volume = {60},
number = {1},
pages = {103140},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103140},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322002412},
author = {Fatima N. AL-Aswadi and Huah Yong Chan and Keng Hoon Gan and Wafa’ Za'al Alma'aitah},
keywords = {Concepts extraction, Relevance measurement, Concept evaluation, Ontology learning, Sustainability distribution},
abstract = {Concepts extraction is the backbone task for an ontology construction system. Identifying the relevant concepts is considered one of the main challenges of automatic ontology construction (called Ontology Learning (OL)). This research introduces a novel relevance metric that estimates the sustainability distribution of concepts to identify the modern and relevant concepts, called Domain Time Relevance (DTR). Also, this research proposes a Developed Concepts extraction Model based on the proposed DTR, namely DTR-DCEM. This proposed model uses the proposed Concepts extraction stopwords (CE-stopwords) list to avoid noise data. This proposed model, DTR-DCEM, aims to extract the relevant concepts from scientific publications. The experiments are conducted in two different datasets: DL2019 and ACLRelAcS. The experimental results show that the proposed model DTR-DCEM outperforms the state-of-the-art models. It got between (3.32∼51.07)% better performance than comparative models for the DL2019 dataset and between (3.29∼24.51)% better performance for the ACLRelAcS dataset. Moreover, the statistical significance of the proposed model DTR-DCEM has been proved using paired t-test.}
}
@article{KIM2022,
title = {Developing a Dietary Lifestyle Ontology to Improve the Interoperability of Dietary Data: Proof-of-Concept Study},
journal = {JMIR Formative Research},
volume = {6},
number = {4},
year = {2022},
issn = {2561-326X},
doi = {https://doi.org/10.2196/34962},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X2200453X},
author = {Hyeoneui Kim and Jinsun Jung and Jisung Choi},
keywords = {dietary lifestyle data, person-generated health data, ontology, common data element, data interoperability, data standardization, dietary, health informatics},
abstract = {Background
Dietary habits offer crucial information on one's health and form a considerable part of the patient-generated health data. Dietary data are collected through various channels and formats; thus, interoperability is a significant challenge to reusing this type of data. The vast scope of dietary concepts and the colloquial expression style add difficulty to standardizing the data. The interoperability issues of dietary data can be addressed through Common Data Elements with metadata annotation to some extent. However, making culture-specific dietary habits and questionnaire-based dietary assessment data interoperable still requires substantial efforts.
Objective
The main goal of this study was to address the interoperability challenge of questionnaire-based dietary data from different cultural backgrounds by combining ontological curation and metadata annotation of dietary concepts. Specifically, this study aimed to develop a Dietary Lifestyle Ontology (DILON) and demonstrate the improved interoperability of questionnaire-based dietary data by annotating its main semantics with DILON.
Methods
By analyzing 1158 dietary assessment data elements (367 in Korean and 791 in English), 515 dietary concepts were extracted and used to construct DILON. To demonstrate the utility of DILON in addressing the interoperability challenges of questionnaire-based multicultural dietary data, we developed 10 competency questions that asked to identify data elements sharing the same dietary topics and assessment properties. We instantiated 68 data elements on dietary habits selected from Korean and English questionnaires and annotated them with DILON to answer the competency questions. We translated the competency questions into Semantic Query-Enhanced Web Rule Language and reviewed the query results for accuracy.
Results
DILON was built with 262 concept classes and validated with ontology validation tools. A small overlap (72 concepts) in the concepts extracted from the questionnaires in 2 languages indicates that we need to pay closer attention to representing culture-specific dietary concepts. The Semantic Query-Enhanced Web Rule Language queries reflecting the 10 competency questions yielded correct results.
Conclusions
Ensuring the interoperability of dietary lifestyle data is a demanding task due to its vast scope and variations in expression. This study demonstrated that we could improve the interoperability of dietary data generated in different cultural contexts and expressed in various styles by annotating their core semantics with DILON.}
}
@article{WANG2022641,
title = {An ontology-based product usage context modeling method for smart customization},
journal = {Procedia CIRP},
volume = {109},
pages = {641-646},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.307},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007569},
author = {Xingzhi Wang and Ang Liu and Sami Kara},
keywords = {Design ontology, Product usage context, Smart customization},
abstract = {Product usage context (PUC) identification is an effective approach to approximate the complex driver behind heterogeneous customer preference. With the sweeping trend of data-driven smart customization, a large volume of product usage data has allowed designers to understand contextual customer needs (CNs) and enable them to offer highly customized products and services in time. However, as the PUC ontology is not clearly defined, most of the existing PUC models are incomplete, ambiguous and imprecise. Inappropriate use of those models will result in failing to extract knowledge from data. In this paper, an ontology-based context modeling method is proposed, with the aim to help designers understand PUC in a comprehensive manner. A case study of robot vacuum cleaner (RVC) is used as an illustrative example. It is concluded that the proposed method can enable designers quickly establish a well-defined PUC model to support smart customization.}
}
@article{LIU2025104654,
title = {Extracting individual trajectories from text by fusing large language models with diverse knowledge},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {141},
pages = {104654},
year = {2025},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2025.104654},
url = {https://www.sciencedirect.com/science/article/pii/S1569843225003012},
author = {Le Liu and Tao Pei and Zidong Fang and Xiaorui Yan and Chenglong Zheng and Xi Wang and Ci Song and Wenfei Luan and Jie Chen},
keywords = {Large language model, Human mobility trajectory, Geographical information retrieval, Prompt learning, GeoAI},
abstract = {Individual trajectories offer insights into human mobility, with data either passively recorded, such as GPS, or actively recorded, such as natural language text. While the former provides detailed movement data, it lacks important context such as personal experiences, which can be obtained from the latter. Extracting trajectories from text can enhance travel experience optimization, historical analysis, and pandemic management. However, existing trajectory extraction methods rely on rule-based frameworks that fail to capture contextual semantics, resulting in limited generalizability and loss of trajectory semantics. While general-purpose large language models (LLMs) demonstrate potential for contextual reasoning capabilities, their deficient domain-specific knowledge pertinent to trajectory patterns hinders efficient and precise trajectory extraction. To address these limitations, we propose T2TrajLLM, a novel framework that fuses LLMs with domain knowledge through three components: (1) a lightweight trajectory model for structured guidance, (2) a text-to-trajectory transformation model enabling multi-step reasoning, and (3) labelled text-trajectory samples for learning domain-adaptive constraint rules. Central to T2TrajLLM is a prompt method that dynamically fuses these components with LLMs while avoids rigid rule dependency. Evaluated across three heterogeneous datasets, T2TrajLLM achieves ∼8 % higher accuracy than existing methods, demonstrating strong transferability across datasets and extensibility to diverse application requirements. Overall, T2TrajLLM effectively extracts trajectories from diverse textual sources, providing robust support for the analysis and understanding of individual mobility.}
}
@article{QIAN2023159,
title = {Quantitative scenario construction of typical disasters driven by ontology data},
journal = {Journal of Safety Science and Resilience},
volume = {4},
number = {2},
pages = {159-166},
year = {2023},
issn = {2666-4496},
doi = {https://doi.org/10.1016/j.jnlssr.2022.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666449623000026},
author = {Jing Qian and Yi Liu},
keywords = {Semantic analysis, Disaster, Scenario construction, EOC model, Quantitative},
abstract = {This study introduces a quantitative scenario-building method for analyzing emergency scenarios based on ontological methods and the EOC (element-object-consequence) model. The ontological structure of disasters concisely describes the knowledge, concepts, attributes, and relationships of the disaster scenario. It reduces the granularity of the data from the document to the data level. Disaster ontologies comprise a set of basic knowledge of a given domain, which is reusable, relatively fixed, and applicable in different areas at different periods. The EOC model is based on the ontology of a disaster and adopts a multiclass structure for the development of a complete process scenario and the adaptation of a disaster scenario by combining objects, elements, environments, and consequences.}
}
@article{LI2022100113,
title = {A semantic ontology for representing and quantifying energy flexibility of buildings},
journal = {Advances in Applied Energy},
volume = {8},
pages = {100113},
year = {2022},
issn = {2666-7924},
doi = {https://doi.org/10.1016/j.adapen.2022.100113},
url = {https://www.sciencedirect.com/science/article/pii/S2666792422000312},
author = {Han Li and Tianzhen Hong},
keywords = {Building energy flexibility, Semantic interoperability, Ontology, Grid-interactive efficient buildings, Demand side management},
abstract = {Energy flexibility of buildings can be an essential resource for a sustainable and reliable power grid with the growing variable renewable energy shares and the trend to electrify and decarbonize buildings. Traditional demand-side management technologies, advanced building controls, and emerging distributed energy resources (including electric vehicle, energy storage, and on-site power generation) enable the transition of the building stock to grid-interactive efficient buildings (GEBs) that operate efficiently to meet service needs and are responsive to grid pricing or carbon signals to achieve energy and carbon neutrality. Although energy flexibility has received growing attention from industry and the research community, there remains a lack of common ground for energy flexibility terminologies, characterization, and quantification methods. This paper presents a semantic ontology—EFOnt (Energy Flexibility Ontology)—that extends existing terminologies, ontologies, and schemas for building energy flexibility applications. EFOnt aims to serve as a standardized tool for knowledge co-development and streamlining energy flexibility related applications. We demonstrate potential use cases of EFOnt via two examples: (1) energy flexibility analytics with measured data from a residential smart thermostat dataset and a commercial building, and (2) modeling and simulation to evaluate energy flexibility of buildings. The compatibility of EFOnt with existing ontologies and the outlook of EFOnt's role in the building energy data tool ecosystem are discussed.}
}
@article{ELZ2024104017,
title = {The IDMP Ontology – A Catalyst to Unleash the Potential of AI and Accelerate Data-Driven Decisions with Industry-Wide Standards},
journal = {Drug Discovery Today},
volume = {29},
number = {6},
pages = {104017},
year = {2024},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2024.104017},
url = {https://www.sciencedirect.com/science/article/pii/S1359644624001429},
author = {Sheila R. Elz and Gerd R. Kleemann and Torsten Osthus and Martin Petracchi and Raphael Sergent}
}
@article{ELLWEIN202291,
title = {Ambiguity Tolerant Commissioning Ontology: From an upper ontology to a domain specific implementation},
journal = {Procedia CIRP},
volume = {112},
pages = {91-96},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.042},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122011969},
author = {Carsten Ellwein and Marc Hekeler and Oliver Riedel},
keywords = {Industry 4.0, Cloud manufacturing, Data model, Ontology},
abstract = {There are no standardized interfaces or descriptions for the outsourcing of production orders, e.g. through cloud manufacturing. For this reason, an upper ontology was developed in a preliminary work; associated models describe the entire process and parameters from the request for quotation over its return to the conclusion of the contract. Products can either be described by their final state or by the initial product in conjunction with the applied processes. In this paper, the process from this upper ontology to a concrete implementation is further elaborated and a middleware is introduced which offers a machine-readable interface to edit the ontology as well as various validation options are addressed.}
}
@article{XU2023101461,
title = {A comprehensive construction of the domain ontology for stratigraphy},
journal = {Geoscience Frontiers},
volume = {14},
number = {5},
pages = {101461},
year = {2023},
issn = {1674-9871},
doi = {https://doi.org/10.1016/j.gsf.2022.101461},
url = {https://www.sciencedirect.com/science/article/pii/S1674987122001141},
author = {Huiqing Xu and Yingying Zhao and Hao Huang and Shaochun Dong and Yukun Shi and Chunju Huang and Huaichun Wu and Zhiqi Qian and Qiang Fang and Huaguo Wen and Zhongtang Su and Shuang Dai and Ronghua Wang and Chao Li and Chao Sun and Junxuan Fan},
keywords = {Domain ontology, Stratigraphy, Biostratigraphic unit, Biostratigraphic horizon, Fossil},
abstract = {Stratigraphic knowledge, the cornerstone of geoscience, needs to be represented by the Knowledge Graph based upon ontology, in order to apply the state-of-the-art big-data techniques. This study aims to comprehensively construct the ontologies for the stratigraphic domain. This has been achieved by a federated, crowd intelligence-based collaboration among domain experts of major stratigraphic subdisciplines. The initial step is to enumerate key terms from authoritative references and incorporate them into the Geoscience Professional Knowledge Graphs (GPKGs) of Deep-time Digital Earth Project. During this process, semantic heterogeneities were meticulously addressed by professional judgement aided by an automatic detection of Homonyms at the GPKGs platform. Afterwards, these terms were further differentiated as either classes or properties and arranged in a hierarchical framework in a top-down process. Consequently, seven ontologies are constructed for major stratigraphic branches, i.e., Lithostratigraphy, Biostratigraphy, Chronostratigraphy, Chemostratigraphy, Magnetostratigraphy, Cyclostratigraphy and Sequence Stratigraphy. The ontology of Biostratigraphy, among them, is elaborated here, as no biostratigraphic ontology has been attempted before to our knowledge. The constructed biostratigraphic ontology comprises following major root classes: Fossil, Biostratigraphic unit, Biostratigraphic horizon. Altogether, they contribute to the eventual dating and correlating of strata in another root class: Biostratigraphic correlation. In summary, the achievements of this study are probably heretofore the most comprehensive ontologies for the stratigraphic domain. Moreover, a proto model of semantic search engine was conceived to discuss potential application of our work for better querying stratigraphic references, utilizing the semantic liaison of the classes in the constructed ontologies.}
}
@article{SANCHEZZAS2023462,
title = {Ontology-based approach to real-time risk management and cyber-situational awareness},
journal = {Future Generation Computer Systems},
volume = {141},
pages = {462-472},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22004058},
author = {Carmen Sánchez-Zas and Víctor A. Villagrá and Mario Vega-Barbas and Xavier Larriva-Novo and José Ignacio Moreno and Julio Berrocal},
keywords = {Ontology, Cybersecurity, SPARQL Inference Notation, Anomaly, Cyber threat intelligence, Risk management},
abstract = {The requirement of continuous risk assessment and management is attracting growing attention because of the need of keeping risk under control. Over the years, companies are dealing with a growing number of malicious actions coming from heterogeneous sources, so risk management must be dynamic in real-time to define action strategies and validate the effectiveness of the safeguards in place. This exposure makes it imperative to use sensor-based systems to detect anomalies or to have an updated catalog of vulnerabilities to understand the situation in which the system finds itself and its level of risk. Such a wealth of heterogeneous information has led to the use of ontologies to organize data, as they allow the extraction of new concepts and behaviors, for instance, measuring the risk level of a system or generating metrics for decision support systems. This paper presents an ontology to describe different types of anomalies, merged with previously developed models for Cyber-Threat Intelligence, becoming a proposal to define real-time risk management in a converged secure environment with physical and logical elements, using these ontologies and SPARQL Rules to infer knowledge and calculate dynamically the risk level of the system.}
}
@incollection{BICKHARD2025465,
title = {Naturalistic ontological psychology},
editor = {Mark H. Bickhard},
booktitle = {The Whole Person},
publisher = {Academic Press},
pages = {465-466},
year = {2025},
isbn = {978-0-443-33050-6},
doi = {https://doi.org/10.1016/B978-0-443-33050-6.00012-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443330506000124},
author = {Mark H. Bickhard},
keywords = {naturalism, normativity, ontological psychology, process metaphysics, substance metaphysics, emergence, self-consistency},
abstract = {Persons are part of the natural world — naturalistic ontological psychology is coherent, reflexively consistent, and has strong warrant. One consequence is that issues of human meaning are not situated against a meaningless world, but are within the natural world. This follows from minds and persons being naturally emergent, which is enabled by a model of emergence more generally, which requires a process metaphysics, which is itself strongly supported by philosophical arguments and scientific results. This chapter offers a brief overview of these arguments.}
}
@article{MARCHESIN2025104281,
title = {Large Language Models and Data Quality for Knowledge Graphs},
journal = {Information Processing & Management},
volume = {62},
number = {6},
pages = {104281},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104281},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325002225},
author = {Stefano Marchesin and Gianmaria Silvello and Omar Alonso},
keywords = {Data quality, Knowledge graph, Large language model},
abstract = {Knowledge Graphs (KGs) have become essential for applications such as virtual assistants, web search, reasoning, and information access and management. Prominent examples include Wikidata, DBpedia, YAGO, and NELL, which large companies widely use for structuring and integrating data. Constructing KGs involves various AI-driven processes, including data integration, entity recognition, relation extraction, and active learning. However, automated methods often lead to sparsity and inaccuracies, making rigorous KG quality evaluation crucial for improving construction methodologies and ensuring reliable downstream applications. Despite its importance, large-scale KG quality assessment remains an underexplored research area. The rise of Large Language Models (LLMs) introduces both opportunities and challenges for KG construction and evaluation. LLMs can enhance contextual understanding and reasoning in KG systems but also pose risks, such as introducing misinformation or “hallucinations” that could degrade KG integrity. Effectively integrating LLMs into KG workflows requires robust quality control mechanisms to manage errors and ensure trustworthiness. This special issue explores the intersection of KGs and LLMs, emphasizing human–machine collaboration for KG construction and evaluation. We present contributions on LLM-assisted KG generation, large-scale KG quality assessment, and quality control mechanisms for mitigating LLM-induced errors. Topics covered include KG construction methodologies, LLM deployment in KG systems, scalable KG evaluation, human-in-the-loop approaches, domain-specific applications, and industrial KG maintenance. By advancing research in these areas, this issue fosters innovation at the convergence of KGs and LLMs.}
}
@article{SWEIDAN2023101720,
title = {Fuzzy ontology-based approach for liver fibrosis diagnosis},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {8},
pages = {101720},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101720},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823002744},
author = {Sara Sweidan and Nuha Zamzami and Sahar F. Sabbeh},
keywords = {Liver fibrosis, Fuzzy ontology, Rule-based system, Semantics reasoning, Fuzzy reasoning},
abstract = {The domain of the digestive system is prone to severe chronic disease in the form of liver cirrhosis, which is currently a leading cause of mortality. This article presents a new intelligent system for predicting the severity of liver fibrosis in patients with chronic viral hepatitis C. The proposed system is based on the inference capabilities of fuzzy ontology and operates on semantic rule-based techniques. A fuzzy decision tree technique was employed to generate the ontology rule base using a dataset of real fibrosis cases from the Mansoura University Hospital, Egypt. These rules were then encoded into a set of fuzzy semantic rules using the fuzzy description logic format. To evaluate the system’s effectiveness, the proposed ontology was then tested on 47 chronic HCV cases, with an attempt made to see if this correctly diagnosed the patients’ conditions. The performance of the proposed system was compared with that of the now-standard Mamdani fuzzy inference system; while the latter achieved an accuracy of 95.7/%, the proposed fuzzy ontology-based system demonstrated higher performance, with 97.8% accuracy. Furthermore, the proposed system also supports semantic interoperability between clinical decision support systems and electronic health record ecosystems. The positive impacts of this system on the correct prediction of liver fibrosis severity thus suggest that it has the potential to assist medical professionals in diagnosing and treating this dangerous disease.}
}
@article{BAO2023168093,
title = {The dcGO Domain-Centric Ontology Database in 2023: New Website and Extended Annotations for Protein Structural Domains},
journal = {Journal of Molecular Biology},
volume = {435},
number = {14},
pages = {168093},
year = {2023},
note = {Computation Resources for Molecular Biology},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2023.168093},
url = {https://www.sciencedirect.com/science/article/pii/S0022283623001559},
author = {Chaohui Bao and Chang Lu and James Lin and Julian Gough and Hai Fang},
keywords = {protein structural domains, ontologies, annotations, enrichment analysis, computational resources},
abstract = {Protein structural domains have been less studied than full-length proteins in terms of ontology annotations. The dcGO database has filled this gap by providing mappings from protein domains to ontologies. The dcGO update in 2023 extends annotations for protein domains of multiple definitions (SCOP, Pfam, and InterPro) with commonly used ontologies that are categorised into functions, phenotypes, diseases, drugs, pathways, regulators, and hallmarks. This update adds new dimensions to the utility of both ontology and protein domain resources. A newly designed website at http://www.protdomainonto.pro/dcGO offers a more centralised and user-friendly way to access the dcGO database, with enhanced faceted search returning term- and domain-specific information pages. Users can navigate both ontology terms and annotated domains through improved ontology hierarchy browsing. A newly added facility enables domain-based ontology enrichment analysis.}
}
@article{AMINU2022200125,
title = {MaCOnto: A robust maize crop ontology based on soils, fertilizers and irrigation knowledge},
journal = {Intelligent Systems with Applications},
volume = {16},
pages = {200125},
year = {2022},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2022.200125},
url = {https://www.sciencedirect.com/science/article/pii/S266730532200062X},
author = {Enesi Femi Aminu and Ishaq Oyebisi Oyefolahan and Muhammad Bashir Abdullahi and Muhammadu Tajudeen Salaudeen},
keywords = {Maconto, Ontology evolution, Competency question, Maize's soils knowledge, Maize's fertilizer knowledge, Maize's irrigation knowledge},
abstract = {The demand for relevant information in a timely manner portrays the significance of knowledge management in all areas of lives; for instance, agriculture. To this end, soils, fertilizers and irrigation as agronomic concepts are essential knowledge inputs for any crops, such as maize. Conversely, there is always difficulty in timely retrieval of these relevant information owing to the unstructured nature of data in repositories, and complexity of concepts mismatch. Sequel to this development, ontology, a semantic data modeling technique is promising as it has been recently employed to deal with these challenges across different domains. However, the robustness of ontology, in terms of semantic expressivity of hidden knowledge, and autonomous growth of ontology leave some gaps to contend with. In view of this development, this research aims to design a robust OWL Rule based ontology for maize crop domain by considering primarily soils, fertilizers and irrigation agronomic concepts capable to evolve autonomously. The proposed ontology herein christened MaCOnto, is developed using the adapted six steps ontology-engineering principle. Over 1,430 entities are encoded in OWL; eighty Competency Questions (CQs) validated by domain experts are modeled in FOL, and implemented as rules via SWRL. Thus, the ontology is queried by SQWRL. Besides, the novel algorithmic design for the ontology to autonomously evolve is implemented in Java environment by employing WordNet. The results obtained from structural based evaluation show an outstanding performance across the eight metrics. Similarly, the results of the competency-based evaluation are also promising. Therefore, the proposed MaCOnto is a robust application based ontology capable to infer and responds to user's query based on its contextual information.}
}
@article{JORDAN2025101459,
title = {Enhancing interoperability in digital calibration data exchange: A case for ontology development},
journal = {Measurement: Sensors},
volume = {38},
pages = {101459},
year = {2025},
note = {Proceedings of the XXIV IMEKO World Congress},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101459},
url = {https://www.sciencedirect.com/science/article/pii/S2665917424004355},
author = {Moritz Jordan and Shanna Schönhals and Sören Auer},
keywords = {Digital Calibration Certificate (DCC), Ontology, Semantic web, Digital System of Units (DSI)},
abstract = {This paper presents the creation of an ontology for Digital Calibration Certificates (DCCs), enhancing interoperability by harmonizing terminology. Developed with semantic technologies, it streamlines communication, data exchange, and collaboration in calibration. The ontology also lays groundwork for middleware software, advancing interoperability and efficiency in digital calibration processes.}
}
@article{FRID2023,
title = {An Ontology-Based Approach for Consolidating Patient Data Standardized With European Norm/International Organization for Standardization 13606 (EN/ISO 13606) Into Joint Observational Medical Outcomes Partnership (OMOP) Repositories: Description of a Methodology},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/44547},
url = {https://www.sciencedirect.com/science/article/pii/S2291969423000042},
author = {Santiago Frid and Xavier {Pastor Duran} and Guillem {Bracons Cucó} and Miguel Pedrera-Jiménez and Pablo Serrano-Balazote and Adolfo {Muñoz Carrero} and Raimundo Lozano-Rubí},
keywords = {health information interoperability, health research, health information standards, dual model, secondary use of health data, Observational Medical Outcomes Partnership Common Data Model, European Norm/International Organization for Standardization 13606, health records, ontologies, clinical data},
abstract = {Background
To discover new knowledge from data, they must be correct and in a consistent format. OntoCR, a clinical repository developed at Hospital Clínic de Barcelona, uses ontologies to represent clinical knowledge and map locally defined variables to health information standards and common data models.
Objective
The aim of the study is to design and implement a scalable methodology based on the dual-model paradigm and the use of ontologies to consolidate clinical data from different organizations in a standardized repository for research purposes without loss of meaning.
Methods
First, the relevant clinical variables are defined, and the corresponding European Norm/International Organization for Standardization (EN/ISO) 13606 archetypes are created. Data sources are then identified, and an extract, transform, and load process is carried out. Once the final data set is obtained, the data are transformed to create EN/ISO 13606–normalized electronic health record (EHR) extracts. Afterward, ontologies that represent archetyped concepts and map them to EN/ISO 13606 and Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) standards are created and uploaded to OntoCR. Data stored in the extracts are inserted into its corresponding place in the ontology, thus obtaining instantiated patient data in the ontology-based repository. Finally, data can be extracted via SPARQL queries as OMOP CDM–compliant tables.
Results
Using this methodology, EN/ISO 13606–standardized archetypes that allow for the reuse of clinical information were created, and the knowledge representation of our clinical repository by modeling and mapping ontologies was extended. Furthermore, EN/ISO 13606–compliant EHR extracts of patients (6803), episodes (13,938), diagnosis (190,878), administered medication (222,225), cumulative drug dose (222,225), prescribed medication (351,247), movements between units (47,817), clinical observations (6,736,745), laboratory observations (3,392,873), limitation of life-sustaining treatment (1,298), and procedures (19,861) were created. Since the creation of the application that inserts data from extracts into the ontologies is not yet finished, the queries were tested and the methodology was validated by importing data from a random subset of patients into the ontologies using a locally developed Protégé plugin (“OntoLoad”). In total, 10 OMOP CDM–compliant tables (“Condition_occurrence,” 864 records; “Death,” 110; “Device_exposure,” 56; “Drug_exposure,” 5609; “Measurement,” 2091; “Observation,” 195; “Observation_period,” 897; “Person,” 922; “Visit_detail,” 772; and “Visit_occurrence,” 971) were successfully created and populated.
Conclusions
This study proposes a methodology for standardizing clinical data, thus allowing its reuse without any changes in the meaning of the modeled concepts. Although this paper focuses on health research, our methodology suggests that the data be initially standardized per EN/ISO 13606 to obtain EHR extracts with a high level of granularity that can be used for any purpose. Ontologies constitute a valuable approach for knowledge representation and standardization of health information in a standard-agnostic manner. With the proposed methodology, institutions can go from local raw data to standardized, semantically interoperable EN/ISO 13606 and OMOP repositories.}
}
@article{JIANG2020103581,
title = {Multi-Ontology Refined Embeddings (MORE): A hybrid multi-ontology and corpus-based semantic representation model for biomedical concepts},
journal = {Journal of Biomedical Informatics},
volume = {111},
pages = {103581},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103581},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420302094},
author = {Steven Jiang and Weiyi Wu and Naofumi Tomita and Craig Ganoe and Saeed Hassanpour},
keywords = {Biomedical Semantic Representation, Semantic Similarity, Distributional Semantics, Biomedical Ontology},
abstract = {Objective
Currently, a major limitation for natural language processing (NLP) analyses in clinical applications is that concepts are not effectively referenced in various forms across different texts. This paper introduces Multi-Ontology Refined Embeddings (MORE), a novel hybrid framework that incorporates domain knowledge from multiple ontologies into a distributional semantic model, learned from a corpus of clinical text.
Materials and Methods
We use the RadCore and MIMIC-III free-text datasets for the corpus-based component of MORE. For the ontology-based part, we use the Medical Subject Headings (MeSH) ontology and three state-of-the-art ontology-based similarity measures. In our approach, we propose a new learning objective, modified from the sigmoid cross-entropy objective function.
Results and Discussion
We used two established datasets of semantic similarities among biomedical concept pairs to evaluate the quality of the generated word embeddings. On the first dataset with 29 concept pairs, with similarity scores established by physicians and medical coders, MORE’s similarity scores have the highest combined correlation (0.633), which is 5.0% higher than that of the baseline model, and 12.4% higher than that of the best ontology-based similarity measure. On the second dataset with 449 concept pairs, MORE’s similarity scores have a correlation of 0.481, based on the average of four medical residents’ similarity ratings, and that outperforms the skip-gram model by 8.1%, and the best ontology measure by 6.9%. Furthermore, MORE outperforms three pre-trained transformer-based word embedding models (i.e., BERT, ClinicalBERT, and BioBERT) on both datasets.
Conclusion
MORE incorporates knowledge from several biomedical ontologies into an existing corpus-based distributional semantics model, improving both the accuracy of the learned word embeddings and the extensibility of the model to a broader range of biomedical concepts. MORE allows for more accurate clustering of concepts across a wide range of applications, such as analyzing patient health records to identify subjects with similar pathologies, or integrating heterogeneous clinical data to improve interoperability between hospitals.}
}
@article{KHALEGHI2022103199,
title = {Context-Aware Ontology-based Security Measurement Model},
journal = {Journal of Information Security and Applications},
volume = {67},
pages = {103199},
year = {2022},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2022.103199},
url = {https://www.sciencedirect.com/science/article/pii/S2214212622000795},
author = {Mahmoud Khaleghi and Mohammad Reza Aref and Mehdi Rasti},
keywords = {Security measurement, Security assessment, Context modelling, Security ontology, Context-aware, National network, Large-scale network},
abstract = {Security measurement models (SMMs) and quantitative security metrics (QSMs) are crucial pillars of systematic security measurement. How to design the enhanced SMMs and effective QSMs has been seriously considered in recent years. However, to the best of our knowledge, a desirable SMM has not yet been provided to measure the security effectiveness of a national-level network (NLN) due to its specific attributes. NLN has three main attributes, including plurality and diversity of network components, continuous changes, and simultaneous functionalities. These attributes cause three major challenges to designing a desirable SMM for NLN, including complexity, dynamic measurement, and multidimensionality. Hence, a desirable SMM for NLN should fulfill five desirability criteria to overcome the challenges, including simplicity, dynamics, comprehensiveness, scalability, and simultaneous overall and granular measurement. Considering the comparison of SMMs, such a desirable model should exclusively be a context-aware ontology-based SMM (CAO-SMM). In this paper, we propose a three layers CAO-SMM in which a comprehensive set of contextual dynamic QSMs are embedded. Our proposed SMM measures the security effectiveness component of network security situation relying on three indices: (1) deterrence against threats; (2) resiliency versus attacks; (3) survivability to impacts. First, an ontology-based SMM is designed. Then, the context-awareness feature is embedded to turn it into a CAO-SMM. Eventually, the desirability of our proposed CAO-SMM and its embedded QSMs are evaluated. CAO-SMM desirability along with the comprehensive coverage and distribution of its embedded QSMs enable us to precisely measure the security effectiveness across the whole network and its contextual components, including the network functionalities.}
}
@article{FU2025106638,
title = {GeoMinLM: A Large Language Model in Geology and Mineral Survey in Yunnan Province},
journal = {Ore Geology Reviews},
volume = {182},
pages = {106638},
year = {2025},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2025.106638},
url = {https://www.sciencedirect.com/science/article/pii/S0169136825001982},
author = {Yu Fu and Mingguo Wang and Chengbin Wang and Shuaixian Dong and Jianguo Chen and Jiyuan Wang and Hongping Yu and Jing Huang and Liheng Chang and Bo Wang},
keywords = {Geology and Mineral Exploration, Large Language Model, Intelligent Service, Knowledge Graph, Knowledge Embedding},
abstract = {In recent years, the development of artificial intelligence and big data technologies has led to the advancement of tools and solutions for transforming the geological and mineral survey paradigm, which requires a large amount of geological knowledge in a complex and arduous working environment. The large language model (LLM) has a significant advantage in answering generative intelligent questions. However, LLMs for general fields have limitations in answering professional questions in a vertical domain like geology. To overcome this challenge, we proposed and developed GeoMinLM, an LLM for geological and mineral exploration scenarios in Yunnan Province, and explored its applications in intelligent Q&A. Leveraging a proprietary dataset of 5.16 million words in geology and mineral exploration, we trained GeoMinLM based on Baichuan-2, achieving superior performance through fine-tuning and hyperparameter optimization. By integrating expert knowledge via a knowledge graph, we significantly reduced hallucinations and enhanced professionalism. This study proves that GeoMinLM is helpful for accurate information retrieval and knowledge dissemination, thereby supporting the intelligent advancement of geological and mineral fields.}
}
@article{KAMRAN2023100797,
title = {SemanticHadith: An ontology-driven knowledge graph for the hadith corpus},
journal = {Journal of Web Semantics},
volume = {78},
pages = {100797},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100797},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000264},
author = {Amna Binte Kamran and Bushra Abro and Amna Basharat},
keywords = {Knowledge graph, Linked data, Semantic web, Hadith, Quran, Ontology},
abstract = {Hadith is an essential and much-celebrated resource for the Islamic domain. It is one of the two primary sources of Islamic legislation. The hadith corpus is quite large, consisting of the collection of sayings, actions and silent approval of the Prophet Muhammad. Minimal efforts have been made to date, towards unified semantic modelling, and knowledge representation of the hadith structure for enhanced interlinking and knowledge discovery. This paper presents the design, development and publishing of the hadith corpus as a knowledge graph. First, we design the SemanticHadith ontology to describe and relate core structural concepts from the hadith. We then publish the six prominent hadith collections as an RDF-Based hadith knowledge graph, which is an effort towards making the available hadith both human and machine-readable. This is the first step in the annotation and linking process of the hadith corpus aimed at enabling semantic search capabilities to support scholars, students, and researchers in the creation, evolution, and consultation of a digital representation of Islamic knowledge. The SemanticHadith knowledge graph is freely accessible at http://www.semantichadith.com.}
}
@article{RABOUDI2022104007,
title = {The BMS-LM ontology for biomedical data reporting throughout the lifecycle of a research study: From data model to ontology},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {104007},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104007},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000235},
author = {Amel Raboudi and Marianne Allanic and Daniel Balvay and Pierre-Yves Hervé and Thomas Viel and Thulaciga Yoganathan and Anais Certain and Jacques Hilbey and Jean Charlet and Alexandre Durupt and Philippe Boutinaud and Benoît Eynard and Bertrand Tavitian},
keywords = {Provenance, Local terminologies, Data sharing, Research Data Management, Data annotation, Heterogeneous data},
abstract = {Biomedical research data reuse and sharing is essential for fostering research progress. To this aim, data producers need to master data management and reporting through standard and rich metadata, as encouraged by open data initiatives such as the FAIR (Findable, Accessible, Interoperable, Reusable) guidelines. This helps data re-users to understand and reuse the shared data with confidence. Therefore, dedicated frameworks are required. The provenance reporting throughout a biomedical study lifecycle has been proposed as a way to increase confidence in data while reusing it. The Biomedical Study - Lifecycle Management (BMS-LM) data model has implemented provenance and lifecycle traceability for several multimodal-imaging techniques but this is not enough for data understanding while reusing it. Actually, in the large scope of biomedical research, a multitude of metadata sources, also called Knowledge Organization Systems (KOSs), are available for data annotation. In addition, data producers uses local terminologies or KOSs, containing vernacular terms for data reporting. The result is a set of heterogeneous KOSs (local and published) with different formats and levels of granularity. To manage the inherent heterogeneity, semantic interoperability is encouraged by the Research Data Management (RDM) community. Ontologies, and more specifically top ontologies such as BFO and DOLCE, make explicit the metadata semantics and enhance semantic interoperability. Based on the BMS-LM data model and the BFO top ontology, the BioMedical Study - Lifecycle Management (BMS-LM) core ontology is proposed together with an associated framework for semantic interoperability between heterogeneous KOSs. It is made of four ontological levels: top/core/domain/local and aims to build bridges between local and published KOSs. In this paper, the conversion of the BMS-LM data model to a core ontology is detailed. The implementation of its semantic interoperability in a specific domain context is explained and illustrated with examples from small animal preclinical research.}
}
@article{LIANG2025883,
title = {A survey of large language model-augmented knowledge graphs for advanced complex product design},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {883-901},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.04.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525001050},
author = {Xinxin Liang and Zuoxu Wang and Jihong Liu},
keywords = {Knowledge Graph, Large Language Model, Complex Product Design, Intelligent Manufacturing},
abstract = {In the Human-AI collaboration rapid development era, the design and development of knowledge-intensive complex products should enable the design process with the help of advanced AI technology, and enhance the reasoning and application of design domain knowledge. Extracting and reusing domain knowledge would greatly facilitate the success of complex product design. Knowledge graphs (KGs), a powerful knowledge representation and storage technology, have been widely deployed in advanced complex product design because of their advantages in mining and applying large-scale, complex, and specialized domain knowledge. But merely KG and its related reasoning approaches still cannot fully support the ill-defined product design tasks. In the future complex product design, Human-AI collaboration will become a mainstream prevention trend. Large language models (LLMs) have outstanding performance in natural language understanding and generation, showing promising potential to collaborate with KGs in complex product design and development. Till 2024/03/04, only a few studies have systematically reviewed the current status of LLM and KG applications in the engineering field, not to mention a further detailed review in the complex product design field, leaving many issues not covered or fully examined. To fill this gap, 100 articles published in the last 4 years (i.e., 2021–2024) were screened and surveyed. This study provides a statistical analysis of the screened research articles, mainstream techniques of LLM & KG, and LLM & KG applications were analyze. To understand how KG and LLM could support complex product design, a framework of LLMs-augmented KG in advanced complex product design was proposed, which contains data layer, KG & LLM collaboration layer, enhanced design capability layer, and design task layer. Furthermore, we also discussed the challenges and future research directions of the LLM-KG-collaborated complex product design paradigm. As an exploratory review paper, it provides insightful ideas for implementing more specialized domain KGs in product design field.}
}
@article{YANG2023103186,
title = {Listen carefully to experts when you classify data: A generic data classification ontology encoded from regulations},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103186},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103186},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322002874},
author = {Min Yang and Xingshu Chen and Liuyan Tan and Xiao Lan and Yonggang Luo},
keywords = {Data classification, Ontology, Regulations, Machine learning, NLP},
abstract = {Along with the proliferation of big data technology, organizations are involved in an overwhelming data ocean, the huge volume of data makes them at a loss in the face of frequent data breaches due to their failure of efficient data security management. Data classification has become a hot topic as a cornerstone of data protection especially in China in recent years, by categorizing information types and distinguishing protective measures at different classification levels. Both the text and tables of the promulgated data classification-related regulations (for simplicity, laws, regulations, policies, and standards are collectively referred to as “regulations”) contain a wealth of valuable information which can guide the work of data classification. To best assist data practitioners, in this paper, we automatically “grasp” expert experience on how to classify data from the analysis of such regulations. We design a framework, GENONTO, that automatically extracts data classification practices (DCPs), such as information types and their corresponding sensitive levels to construct an information type lexicon as well as to encode a generic ontology on top of 38 real-world regulations promulgated in China. GENONTO employs machine learning techniques and natural language processing (NLP) to parse unstructured text and tables. To our knowledge, GENONTO is the first work that explores critical information like the category and the sensitivity of information types from regulations, and organizes them in a structured form of ontology, characterizing the subsumptive relations between different information types. Our research helps provide a well-defined integrated view across regulations and bridges the gap between what experts say and how data practitioners do.}
}
@article{LI2025114215,
title = {Reviewing clinical knowledge in medical large language models: Training and beyond},
journal = {Knowledge-Based Systems},
volume = {328},
pages = {114215},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114215},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125012560},
author = {Qiyuan Li and Haijiang Liu and Caicai Guo and Chao Gao and Deyu Chen and Meng Wang and Feng Gao and Frank {van Harmelen} and Jinguang Gu},
keywords = {Large language models, Clinical knowledge, Medical academic, Medical practice},
abstract = {The large-scale development of large language models (LLMs) in medical contexts, such as diagnostic assistance and treatment recommendations, necessitates that these models possess accurate medical knowledge and deliver traceable decision-making processes. Clinical knowledge, encompassing the insights gained from research on the causes, prognosis, diagnosis, and treatment of diseases, has been extensively examined within real-world medical practices. Recently, there has been a notable increase in research efforts aimed at integrating this type of knowledge into LLMs, encompassing not only traditional text and multimodal data integration but also technologies such as knowledge graphs (KGs) and retrieval-augmented generation (RAG). In this paper, we review the various initiatives to embed clinical knowledge into training-based, KG-supported, and RAG-assisted LLMs. We begin by gathering reliable knowledge sources from the medical domain, including databases and datasets. Next, we evaluate implementations for integrating clinical knowledge through specialized datasets and collaborations with external knowledge sources such as KGs and relevant documentation. Furthermore, we discuss the applications of the developed medical LLMs in the industrial sector to assess the disparity between models developed in academic settings and those in industry. We conclude the survey by presenting evaluation systems applicable to relevant tasks and identifying potential challenges facing this field. In this review, we do not aim for completeness, since any ostensibly “complete” review would soon be outdated. Our goal is to illustrate diversity by selecting representative and accessible items from current research and industry practices, reflecting real-world situations rather than claiming completeness. Thus, we emphasize showcasing diverse approaches.}
}
@article{SAMBANDAM2023102123,
title = {Deep attention based optimized Bi-LSTM for improving geospatial data ontology},
journal = {Data & Knowledge Engineering},
volume = {144},
pages = {102123},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102123},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X22001148},
author = {Palaniappan Sambandam and D. Yuvaraj and P. Padmakumari and Subbiah Swaminathan},
keywords = {Ontology, Geospatial data, Deep attention based bidirectional search and rescue LSTM},
abstract = {Recently, the geospatial semantic information of remote sensing (RS) has attracted attention due to its various applications. This paper introduces a model for ontology based geospatial data integration using novel deep learning techniques. Here, we use a semantic web technology to establish the spatial ontology of risk knowledge with deep learning (DL), namely deep attention based bidirectional search and rescue LSTM for analysis. This approach takes into consideration of the study which presents the technique driven by the spatial ontology which minimizes the cost of modelling. The classification results from DL model enhances the performance of the ontology module. In this paper, ontological reasoning and DL model are jointly used for increase the module efficiency. The implementation of the proposed scheme is implemented on MATLAB 2020a. The performance of the implemented scheme is compared against the existing models like U-Net, Semantic referee and collaboratively boosting framework (CBF). The Overall accuracy (OA) of the system is found to be 0.923 on UCM dataset. Thus, the developed spatial ontologies provide the semantic foundation to achieve a semantic knowledge of geospatial data understandings.}
}
@article{OCKER2022103571,
title = {A framework for merging ontologies in the context of smart factories},
journal = {Computers in Industry},
volume = {135},
pages = {103571},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103571},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001780},
author = {Felix Ocker and Birgit Vogel-Heuser and Christiaan J.J. Paredis},
keywords = {Ontology merging, Knowledge representation, Smart factories},
abstract = {Current trends, such as individualization, increasing complexity, and specialization, require digitization in engineering and production. However, digitization by itself often leads to so-called data silos, which cannot be leveraged effectively when designing and operating smart factories due to the heterogeneity of the information available. This paper presents a framework for (semi-)automatically merging the highly reusable terminological components of production ontologies in an a posteriori way. The framework combines translations, domain-specific vocabularies, and inconsistency checks with syntactic, terminological, and structural analyses to integrate knowledge representations formalized in the Web Ontology Language. Integrating heterogeneous knowledge representations in the production domain can improve support systems for engineers, increase awareness of interdependencies, and enable unambiguous communication in smart factories.}
}
@article{PUCHIANU20202635,
title = {Conceptual and Ontological Modeling of In-Vehicle Life-logging Software Systems},
journal = {Procedia Computer Science},
volume = {176},
pages = {2635-2644},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.304},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920322158},
author = {Crenguta M. Puchianu and Elena Bautu},
keywords = {In-vehicle system, Conceptual modeling, Ontological modeling, Software},
abstract = {In-vehicle software systems that contain interconnected lifelogging components such as in-vehicle infotainment systems, smartphones, On Board Diagnostics (OBD) devices or wearable devices, operate with very large volumes of data. To manage the complexity and big volume of data, it is necessary to analyze the data and build models. Our goal, in this paper, is to build suitable models for in-vehicle lifelogging data, in the context of vehicle information standardization. To this aim, we build conceptual and ontological models for the data that is transmitted between the lifelogging components of an in-vehicle system, by applying conceptualization and abstraction mechanisms. Conceptual models can be built considering three viewpoints: functional, architectural, and behavioral. In this paper, we constructed a facet of the architectural view that contains the Unified Modeling Language (UML) classes and the relationships between them. Next, we designed and implemented the class diagram and generated each type of message in JavaScript Object Notation (JSON) format. In order to model the data and their semantics, we built an application ontology in Web Ontology Language (OWL). The consistency of this ontology was verified in Protégé. The ontology was further generated in the JavaScript Object Notation for Linked Data (JSON-LD) format, which provides us with a formal format of the messages transmitted between the components of the in-vehicle system. The resulting ontology is used in the persistence layer of the in-vehicle system we are developing.}
}
@article{CHANDRA2022101821,
title = {Semantic sensor network ontology based decision support system for forest fire management},
journal = {Ecological Informatics},
volume = {72},
pages = {101821},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101821},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002710},
author = {Ritesh Chandra and Sonali Agarwal and Navjot Singh},
keywords = {Semantic sensor networks ontology, Decision support system, Semantic web rule language, SPARQL, RDF},
abstract = {The forests are significant assets for every country. When it gets destroyed, it may negatively impact the environment, and forest fire is one of the primary causes. Fire weather indices are widely used to measure fire danger and issue bushfire warnings. It can also be used to predict the demand for emergency management resources. Sensor networks have grown in popularity in data collection and processing capabilities for various applications in industries such as medical, environmental monitoring, home automation, etc. Semantic sensor networks can collect various climatic circumstances like wind speed, temperature, and relative humidity. However, estimating fire weather indices is challenging due to the various issues involved in processing the data streams generated by the sensors. Hence, the importance of forest fire detection has increased day by day. The underlying Semantic Sensor Network (SSN) ontologies are built to allow developers to create rules for calculating fire weather indices and the converted dataset into Resource Description Framework (RDF). This research describes the various steps in developing rules for calculating fire weather indices. Besides, this work presents a Web-based mapping interface to help users visualize the changes in fire weather indices over time. With the help of the inference rule, it designed a decision support system using the SSN ontology and query on it through SPARQL. The proposed fire management system acts according to the situation and supports reasoning and the general semantics of the open world, followed by all the ontologies.}
}
@article{CARDILLO2024109048,
title = {PN-OWL: A two-stage algorithm to learn fuzzy concept inclusions from OWL 2 ontologies},
journal = {Fuzzy Sets and Systems},
volume = {490},
pages = {109048},
year = {2024},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2024.109048},
url = {https://www.sciencedirect.com/science/article/pii/S0165011424001945},
author = {Franco Alberto Cardillo and Franca Debole and Umberto Straccia},
keywords = {OWL 2 ontologies, Machine learning, Fuzzy logic, Concept/class inclusion rules},
abstract = {Given a target class T of an OWL 2 ontology, positive (and possibly negative) examples of T, we address the problem of learning, viz. inducing, from the examples, fuzzy class inclusion rules that aim to describe conditions for being an individual classified as an instance of the class T. To do so, we present PN-OWL which is a two-stage learning algorithm consisting of a P-stage and an N-stage. In the P-stage, the algorithm learns fuzzy class inclusion rules (the P-rules). These rules aim to cover as many positive examples as possible, increasing recall, without compromising too much precision. In the N-stage, the algorithm learns fuzzy class inclusion rules (the N-rules), that try to rule out as many false positives, covered by the rules learnt at the P-stage, as possible. Roughly, the P-rules tell why an individual should be classified as an instance of T, while the N-rules tell why it should not. PN-OWL then aggregates the P-rules and the N-rules by combining them via an aggregation function to allow for a final decision on whether an individual is an instance of T or not. We also illustrate the effectiveness of PN-OWL through extensive experimentation.}
}
@article{BOU2024101569,
title = {Two evaluations on Ontology-style relation annotations},
journal = {Computer Speech & Language},
volume = {84},
pages = {101569},
year = {2024},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2023.101569},
url = {https://www.sciencedirect.com/science/article/pii/S0885230823000888},
author = {Savong Bou and Makoto Miwa and Yutaka Sasaki},
keywords = {Ontology-style relation annotation, Relation extraction, Named entity recognition, Corpus construction},
abstract = {In this paper, we propose an Ontology-Style Relation (OSR) annotation approach. In conventional Relation Extraction (RE) datasets, relations are annotated as a link between two entity mentions. In contrast, in our OSR annotation, a relation is annotated as a relation mention (i.e., not a link but a node) and rdfs:domain and rdfs:range links are annotated from the relation mention to its argument entity mentions. This approach has the following benefits: (1) the relation annotations can be easily converted to Resource Description Framework (RDF) triples to populate an Ontology, (2) some part of conventional RE tasks can be tackled as Named Entity Recognition (NER) tasks, and the relation classes are limited to several RDF properties, and (3) OSR annotations can be used for clear documentations of Ontology contents. We conducted two kinds of evaluation to investigate effects of OSR annotation. We converted (1) an in-house corpus of Japanese Rules of the Road (RoR) in conventional annotations into the OSR annotations and built a novel OSR-RoR corpus and (2) SemEval-2010 Task 8 dataset into the OSR annotations (called OSR-SemEval corpus). We compared the NER and RE performance using neural NER/RE tool DyGIE++ on the conventional and OSR annotations. The experimental results show that the OSR annotations make the RE task easier while introducing slight complexity into the NER task.}
}
@article{PIGNOT2024100500,
title = {Affect and relational agency: How a negative ontology can broaden our understanding of IS research},
journal = {Information and Organization},
volume = {34},
number = {1},
pages = {100500},
year = {2024},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2023.100500},
url = {https://www.sciencedirect.com/science/article/pii/S1471772723000544},
author = {Edouard Pignot and Mark Thompson},
keywords = {Negative ontology, Affect, Lack, Materiality, Agency, Epistemology},
abstract = {The sociomaterial lens within IS research holds that agency should not be considered as a property solely of humans, or of technology, but instead arises from an emergent interaction between the two. This, emergent, account of agency deepens our understanding of unfolding IS practice, but its largely cognitive orientation remains naïve towards affectively-sensed motivations that also form part of this interaction. By implication, a sociomaterial perspective lacking an affective dimension offers an incomplete conceptualisation of information systems. In response, an affectively-informed negative ontology encourages IS researchers to extend their focus beyond the visible, to encompass how actors' receptiveness towards material objects (discourses, technologies) is shaped by deep, affectively-derived motivations of which they are not focally aware, but which nonetheless acquire agency in contributing to a sociomaterial outcome. A central argument, and illustrative empirical vignette, demonstrate how the concepts of sociomateriality, affect, and negative ontology combine to offer researchers an enhanced understanding of relational agency. A discussion follows, exploring some initial ontological, epistemological and methodological implications of an affectively-informed negative ontology for IS research.}
}
@article{DANG2025106399,
title = {Semantic-driven parametric 3D geographic scene modeling: Integrating knowledge graphs and large language models},
journal = {Environmental Modelling & Software},
volume = {188},
pages = {106399},
year = {2025},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2025.106399},
url = {https://www.sciencedirect.com/science/article/pii/S1364815225000830},
author = {Pei Dang and Jun Zhu and Chao Dang and Heng Zhang},
keywords = {3D geographic scene modeling, Large language model, Knowledge graph, Chain of thought},
abstract = {Parametric geographic scene modeling serves as the primary method for achieving large-scale rapid spatial visualization. However, balancing modeling efficiency and specificity of geographic entities poses significant challenges due to the complexity and diversity of real-world geographic environments. This study proposes a novel 3D geographic scene modeling approach that integrates knowledge graphs and large language models (LLMs). The method leverages the extensive pre-trained knowledge and inference capabilities of LLMs to autonomously infer and enhance semantic information of unknown geographic entities. Through progressive knowledge graphs, it transforms the semantic information of geographic entities into modeling parameters, ultimately achieving more intelligent 3D geographic scene modeling. Our approach addresses current limitations in parametric modeling by offering a flexible and adaptive solution capable of efficiently handling diverse geographic entities. Through case studies and comparative analyses, we examine the inference results and modeling effects under various prompt ratios, validating the effectiveness and advantages of this method.}
}
@article{MORIOKA2025762,
title = {Automatic construction of asset knowledge graph with large language model},
journal = {Procedia CIRP},
volume = {135},
pages = {762-767},
year = {2025},
note = {32nd CIRP Conference on Life Cycle Engineering (LCE2025)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.01.097},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125004469},
author = {Tomoaki Morioka and Toshiaki Kono and Takehisa Nishida},
keywords = {Maintenance, Knowledge graph construction, Large language model, Reliability centered maintenance},
abstract = {Lifecycle engineering is a critical concept for fostering environmentally sustainable practices within the manufacturing sector. An essential component of lifecycle management for achieving sustainability is reliability-centered maintenance, which enhances various key performance indicators (KPIs), including machine availability and environmental impact. Effective and reliable maintenance necessitates expert knowledge of the equipment. For instance, determining which components and failure modes should be addressed through condition-based maintenance requires insights derived from failure mode and effect analysis (FMEA). However, constructing expert knowledge is labor-intensive, and ensuring its quality presents significant challenges. This study proposes a method for the automated construction of expert knowledge related to maintenance, along with a corresponding tool designed to reduce construction costs and enhance knowledge quality. The proposed method leverages a large language model (LLM) to automatically generate asset knowledge graphs based on FMEA. By combining general knowledge about equipment derived from the pre-trained LLM with specialized information extracted from technical documents, the tool creates knowledge structures such as component trees and failure modes. Subject matter experts can then iteratively refine and validate this knowledge. To evaluate the proposed approach, we assessed the accuracy and coverage of the knowledge generated by the tool in two case studies involving specific types of equipment. The results indicated that the LLM-generated output contained 4.98 times more items than those manually created, with precision ranging from 0.490 to 0.662 and recall ranging from 0.481 to 0.810.}
}
@article{SUTER2022104041,
title = {Modeling multiple space views for schematic building design using space ontologies and layout transformation operations},
journal = {Automation in Construction},
volume = {134},
pages = {104041},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.104041},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521004921},
author = {Georg Suter},
keywords = {Building information modeling, View definition, Model transformation, Semantic enrichment},
abstract = {Modeling multiple views of spaces involves mapping or transformation between multiple models. Automated model transformation is challenging as semantic and spatial criteria need to be considered. This paper proposes a novel method and data processing pipeline to define space views and semi-automatically transform room-based building data created in BIM authoring systems into multi-view space models. The method is based on space ontologies and their integration with space layout transformation operations. It is used to define a set of functional views that are relevant to schematic building design. An existing space modeling system is extended with the method and data processing pipeline. Results from a validation study show that the method can cover specific semantic and spatial aspects of space views. Both are relevant for consistent model transformation and accurate analysis. Results further show that it is feasible to fully automate data processing steps, except for space classification, which is semi-automated.}
}
@article{LUTHER2023250,
title = {Detection of Disturbances in a Monitoring System on ITS and Usage of Ontologies Approaches: A Critical Review and Challenges in Developing Countries.},
journal = {Procedia Computer Science},
volume = {224},
pages = {250-257},
year = {2023},
note = {18th International Conference on Future Networks and Communications / 20th International Conference on Mobile Systems and Pervasive Computing / 13th International Conference on Sustainable Energy Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.09.034},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923010815},
author = {Mfenjou Martin Luther and Moskolai Ngossaha Justin and Kaladzavi Guidedi and Igor Tchapi and Abba Ari Ado Adamou and  Kolyang and Amro Naijjar},
keywords = {Detection, Disturbances, Ontology, Intelligent Transportation System, Monitoring System, Inter-urban Network},
abstract = {The demographic explosion that the world is experiencing today is impacting the demand on road networks. The monitoring of activities on these road networks has evolved thanks to the paradigms provided by intelligent transport system. In developing countries, the lack of sophisticated infrastructures lead to accidents and loss of life. Thus, some work proposes solutions that aim to deploy control points in a way that maximises network coverage while minimising financial expenditures. Despite this, there are still problems of detecting disturbances in this context. The aim of this paper is to do a literature review on the management of disturbances on intelligent transport systems and their use in the context of developing countries. We also review the use of ontologies in disturbance detections. This shows that the perception of disturbances changes from one model to another. No model proposes an approach that guarantees a good detection of disturbances in the context of monitoring road networks in developing countries context. Therefore, taking ontologies into account for this detection could guarantee good results.}
}
@article{DRAGOS2022593,
title = {Ontology Adaptation for Opinion Mining in French Corpora},
journal = {Procedia Computer Science},
volume = {207},
pages = {593-603},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009954},
author = {Valentina Dragos and Adrien Legros},
keywords = {Opinion mining, Social data analysis, Appraisal theory},
abstract = {This paper presents the development of an ontology for opinion mining in French corpora. Since ontology construction from scratch is expensive and time consuming, the model is built by adapting an existing ontology modeling appraisal categories in English. The construction method consists of two main steps: first, the ontology of appraisals is translated in French by using a concept-to-concept approach; then different adaptation strategies are implemented to improve the result of this translation. Adaptation strategies are based on text mining, weakly supervised methods and the use of WordNet to refine the model. The goal of the adaptation phase is to cope with limitations of concept-to-concept translation, to integrate concepts and relations from external corpora and resources, and to build a model that captures various features of opinions. The ontology was designed and build to incorporate linguistic and extra linguistic information on the description of opinions in French. The model was validated by using both expert insights to validate the relevance of concepts and relations and formal criteria to describe the ontology qualities for practical use.}
}
@article{DOU201819,
title = {Knowledge graph based on domain ontology and natural language processing technology for Chinese intangible cultural heritage},
journal = {Journal of Visual Languages & Computing},
volume = {48},
pages = {19-28},
year = {2018},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X18300041},
author = {Jinhua Dou and Jingyan Qin and Zanxia Jin and Zhuang Li},
keywords = {Intangible cultural heritage, The 24 solar terms, Domain ontology, Knowledge graph, Natural language processing, Deep learning},
abstract = {Intangible cultural heritage (ICH) is a precious historical and cultural resource of a country. Protection and inheritance of ICH is important to the sustainable development of national culture. There are many different intangible cultural heritage items in China. With the development of information technology, ICH database resources were built by government departments or public cultural services institutions, but most databases were widely dispersed. Certain traditional database systems are disadvantageous to storage, management and analysis of massive data. At the same time, a large quantity of data has been produced, accompanied by digital intangible cultural heritage development. The public is unable to grasp key knowledge quickly because of the massive and fragmented nature of the data. To solve these problems, we proposed the intangible cultural heritage knowledge graph to assist knowledge management and provide a service to the public. ICH domain ontology was defined with the help of intangible cultural heritage experts and knowledge engineers to regulate the concept, attribute and relationship of ICH knowledge. In this study, massive ICH data were obtained, and domain knowledge was extracted from ICH text data using the Natural Language Processing (NLP) technology. A knowledge base based on domain ontology and instances for Chinese intangible cultural heritage was constructed, and the knowledge graph was developed. The pattern and characteristics behind the intangible cultural heritage were presented based on the ICH knowledge graph. The knowledge graph for ICH could foster support for organization, management and protection of the intangible cultural heritage knowledge. The public can also obtain the ICH knowledge quickly and discover the linked knowledge. The knowledge graph is helpful for the protection and inheritance of intangible cultural heritage.}
}
@article{CICCONETO2022105005,
title = {GeoReservoir: An ontology for deep-marine depositional system geometry description},
journal = {Computers & Geosciences},
volume = {159},
pages = {105005},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.105005},
url = {https://www.sciencedirect.com/science/article/pii/S0098300421002880},
author = {Fernando Cicconeto and Lucas Valadares Vieira and Mara Abel and Renata dos Santos Alvarenga and Joel Luis Carbonera and Luan Fonseca Garcia},
keywords = {Ontology, Deep-marine depositional system, Turbidite, Artificial intelligence},
abstract = {An ontology is a logical theory that accounts for a domain vocabulary’s intended meaning, allowing us to develop a computational artifact that explicitly and formally represents the community’s conceptualization related to a vocabulary. This paper presents the GeoReservoir ontology — the result of the ontological analysis of the terminology adopted by geologists in sedimentological studies about deep-marine depositional systems, which is one of the most relevant types of oil and gas reservoirs around the world. Despite the variety of studies describing the patterns of productive reservoirs, this domain demanded new approaches in conceptual modeling methodologies because the available terminology presents issues such as ambiguity and contamination of different interpretations in the terms’ definitions. Previous work has dealt with these issues in geology, resulting in the GeoCore, an ontology that explicitly defines the most generic entities in geology. However, the domain in focus still demanded a specialized ontology containing the particular terms found in reports about deep-marine deposits. GeoReservoir is an extension of GeoCore, which, in its turn, extends the Basic Formal Ontology (BFO), a foundational ontology for scientific domains. GeoReservoir takes advantage of both BFO and GeoCore’s foundations, allowing us to focus our effort on defining the domain-specific entities. A team of professional reservoir geologists supported the knowledge acquisition process. We developed the ontology in iterative steps from an initial prototype to a complete artifact containing the taxonomy of entities and the relations between the entities, being increasingly refined by the experts. We further present a case study demonstrating how one could describe a depositional system in GeoReservoir terms. Moreover, we validated the ontology against defined competency questions (CQs). This work’s final result is offering a sound, consistent and unambiguous terminology to support the integration of data and knowledge about deep-marine depositional system geometry and lithology.}
}
@article{MAHDAVI2023106804,
title = {Toward a theory-driven ontological framework for the representation of inhabitants in building performance computing},
journal = {Journal of Building Engineering},
volume = {73},
pages = {106804},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106804},
url = {https://www.sciencedirect.com/science/article/pii/S235271022300983X},
author = {Ardeshir Mahdavi and Dawid Wolosiuk and Christiane Berger},
keywords = {Built environment, Performance simulation, Inhabitants, Perception and behavior, Ontology},
abstract = {Building performance computing focuses primarily on physical processes in buildings. As such, early practices in building simulation adopted a reductionist approach to the representation of buildings' inhabitants. More recently, efforts have been undertaken to enhance the representational realism of inhabitants in building modeling. However, progress in this area requires a robust ontological foundation, which in turn requires a theoretical understanding of the relevant domain. Based on the appraisal of past efforts, this paper identifies a gap between behavioral theories and occupant representations in building models. Consequently, a high-level occupant behavior theory is introduced and its relevance for ontological developments is illustrated through a derivative representational scheme (“otto”: occupants theory-tailored ontology). The established link between behavioral theory and the derivative data schema is suggested to provide the necessary conditions for the development of a comprehensive ontological framework toward representation of inhabitants' presence and behavior in computational building models.}
}
@article{ALKARMOUTY2025126111,
title = {Harnessing large language models for structured extraction of cytochrome P450–substance interactions from biomedical texts},
journal = {International Journal of Pharmaceutics},
volume = {684},
pages = {126111},
year = {2025},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2025.126111},
url = {https://www.sciencedirect.com/science/article/pii/S0378517325009482},
author = {Mariam Alkarmouty and Junya Ooka and Fumiyoshi Yamashita},
keywords = {Text mining, Large language models, ChatGPT, Information extraction, Cytochrome P450},
abstract = {Building on our previous work in biomedical text mining, we revisit the extraction of cytochrome P450 (CYP) and substance interactions using recent advances in large language models (LLMs). We present a scalable, high-accuracy framework that leverages the ChatGPT O3-mini model, employing prompt engineering with structured output formatting, embedded definitions, and selected few-shot examples, combined with batch processing without relying on dictionaries or domain-specific ontologies. Our system achieves strong performance, with recall and precision of 0.963 and 0.987 across all CYP targets, and 0.923 and 0.993 for CYP3A4 specifically. This represents a substantial improvement over our earlier rule-based method. The resulting large-scale analysis not only reflects existing knowledge but also enables a more systematic and comprehensive integration of CYP isoform–substance interaction data, addressing the limitations of previous fragmented efforts. While previous studies have attempted to catalog these interactions, the scale, precision, and automation demonstrated here represent a significant step forward. These findings underscore the potential of LLM-driven pipelines to accelerate biomedical text mining and to support research in drug metabolism and related fields.}
}
@article{HAO2023100798,
title = {Ontology alignment with semantic and structural embeddings},
journal = {Journal of Web Semantics},
volume = {78},
pages = {100798},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100798},
url = {https://www.sciencedirect.com/science/article/pii/S1570826823000276},
author = {Zhigang Hao and Wolfgang Mayer and Jingbo Xia and Guoliang Li and Li Qin and Zaiwen Feng},
keywords = {Ontology alignment, Semantic embedding, Structural embedding, Representation learning},
abstract = {Ontology alignment is essential for data integration and interoperability across multiple applications across diverse disciplines. In recent decades, significant advancements have been made in the development of advanced methods and systems for ontology alignment. Empirical results have suggested that ontological semantics can be effectively employed to enhance the alignment process. Besides, structural information is crucial for ontology alignment as it reflects the relations among adjacent concepts in the ontology. Previous works are mainly based on external lexicon and predefined rules based on ontological structure. Recently, deep learning has imposed positive impacts on ontology alignment and obtained substantial improvement. This paper proposes a new method based on ontology embedding incorporating the semantic and structural features. It utilizes the distance between the embedding of two ontological concepts to be aligned as the criterion for alignment. The proposed method is used to align two widely used food ontologies and three Chinese food classification ontologies. The experimental results show that our method enhances the performance compared to several state-of-the-art alignment systems, demonstrating the importance of learning semantic representation and structural representation. Furthermore, the proposed method is evaluated on several different tracks of the Ontology Alignment Evaluation Initiative (OAEI), and experimental results show that our method outperforms other baselines in effectiveness. The data and code can be obtained from: https://github.com/haozhigang1111/Ontology-Alignment.git.}
}
@article{MOULOUEL2023468,
title = {Ontology-based hybrid commonsense reasoning framework for handling context abnormalities in uncertain and partially observable environments},
journal = {Information Sciences},
volume = {631},
pages = {468-486},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.02.078},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523002748},
author = {Koussaila Moulouel and Abdelghani Chibani and Yacine Amirat},
keywords = {Deep learning, CNN-LSTM model, Transformer model, Probabilistic commonsense reasoning, Probabilistic planning, Ontology, Event calculus, Answer set programming, POMDP},
abstract = {Ambient intelligence (AmI) systems aim to provide users with context-aware assistance services intended to improve the quality of their lives in terms of autonomy, safety, and well-being. Taking the uncertainty and partial observability of these environments into account is of major importance for context recognition and, more specifically, to detect and solve context abnormalities such as those related to the user's behavior or those related to context attribute prediction. In this paper, an ontology-based framework integrating machine learning and probabilistic planning within commonsense reasoning is proposed to recognize the user's context and abnormalities associated with it. The reasoning is performed using event calculus in answer set programming (ECASP); ECASP allows for abductive and temporal reasoning, which results in an eXplainable AI (XAI) approach. A context ontology is proposed to axiomatize the reasoning and introduce the notion of probabilistic fluents into the EC formalism in order to perform probabilistic reasoning. The reasoning incorporates probabilistic planning based on a partially observable Markov decision process (POMDP) to solve knowledge incompleteness. To evaluate the proposed framework, real-life scenarios, based on the Orange4Home and SIMADL public datasets are implemented and discussed.}
}
@article{BABAEI2024102380,
title = {Enabling digital transformation of dynamic location-inventory-routing optimization in natural gas-to-product and energy networks via a domain-adaptable ontological agent-based framework},
journal = {Advanced Engineering Informatics},
volume = {60},
pages = {102380},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102380},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624000284},
author = {F. Babaei and R. Bozorgmehry Boozarjomehry and Z. Kheirkhah Ravandi and M.R. Pishvaie},
keywords = {Multi-agent optimization framework, Natural gas systems, Location-inventory-routing, Ontological knowledge management},
abstract = {Information and Communication Technology (ICT) and Artificial Intelligence (AI) can support the transition from traditional practices toward smart supply chain (SC) coordination schemes. Such a direction allows for integrating organizational levels of natural gas transportation enterprises, comprising Location, Inventory, and Routing Problem (LIRP) decisions. Specific works have investigated the LIRP of natural gas-to-product and energy networks. However, due to the absence of a systematic ICT-assisted mindset, no study has been oriented from the perspective of the dynamic inventory management and geographic data-driven features of land resources. To handle these challenges, this study introduces a vertically-coordinated heuristic optimization framework, in the form of an autonomous multi-agent system, for the LIRP of gas transmission infrastructures as major carrier systems to supply feedstock and energy demands. Various extendable behavior classes are programmed into the framework to support automated agent specializations regarding procedural tasks of planning, transient inventory coordination, routing, and spatial data management in the specific LIRP settings. Moreover, the study develops an ontology-driven knowledge system, formally conceptualizing the intended application-level knowledge according to a general domain semantic model. Leveraging the knowledge system, the agents determine the necessary steps and suitable ready-made tools to solve the problem collaboratively. Several industrial applications showcase the capabilities of the proposed framework. The results manifest that the devised ontology automates model construction and the entire solution process while supporting agent interoperation and scaling the platform applications to emergent domains. Additionally, transient demand fluctuations, dynamic inventory levels, and topological considerations heavily influence long-term planning. Compared to existing practices, the ontology-driven multi-agent framework expounds a more realistic view of the trade-off between strategic investments and dynamic operational welfare in the gas system. Such an automated and practical methodology, in turn, realizes AI-oriented cross-functional integrations in the enhanced product and energy sector.}
}
@article{WEBER2023529,
title = {Methodology for agile and iterative ontology development for toolmaking},
journal = {Procedia CIRP},
volume = {120},
pages = {529-534},
year = {2023},
note = {56th CIRP International Conference on Manufacturing Systems 2023},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123007643},
author = {Sebastian Weber and Tammo Dannen and Lars Stauder and Sebastian Barth and Thomas Bergs},
keywords = {Semantic technology, ontology, toolmaking, digital twin, knowledge retrieval methodology},
abstract = {Complex manufacturing processes and the absence of repeat effects characterize the toolmaking industry. German-speaking toolmaking companies are increasingly faced with the challenge of having to reach the limits of what is technically feasible and are confronted with an erosion of know-how, induced by demographic change. This applies in particular to know-how-intensive areas such as design, CAM-programming and work preparation. There is currently no comprehensive system support in these areas and the knowledge required for planning activities is often only available in the form of implicit technical and empirical knowledge. However, the use of heterogeneous manufacturing technologies requires a profound understanding of technology along the entire value chain. As a result of the very high semantic expressiveness of ontologies, they enable the representation of the most complex data models with logical relationships that go beyond hierarchical subdivision of content. This paper presents a novel agile methodology for the development of domain-specific ontologies in the environment of toolmaking. The methodology makes it possible to integrate the implicitly existing technical and experiential knowledge of employees at an early stage in the modelling process. In particular, the developed methodology extends conventional methods by the identified deficits in terms of knowledge acquisition, iteration and agility, as well as a separate consideration of the life cycle along the ontology of the use phase, taking into account agile methods from requirements engineering. Compliance with various guidelines and requirements is mandatory, such as the formulation of competence questions and the use of a standardised specification document. The iterative approach also ensures the needs-based integration of the characteristics of toolmaking. Furthermore, the methodology enables an early integration of IT structure and user interface for the needs-based design and use of the domain-specific ontology. The validation is based on an example in the mechanical production of a toolmaking company.}
}
@article{RODRIGUEZREVELLO2023120239,
title = {KNIT: Ontology reusability through knowledge graph exploration},
journal = {Expert Systems with Applications},
volume = {228},
pages = {120239},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120239},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423007418},
author = {Jorge Rodríguez-Revello and Cristóbal Barba-González and Maciej Rybinski and Ismael Navas-Delgado},
keywords = {Life sciences, Ontology, Ontology learning, Knowledge graphs},
abstract = {Ontologies have become a standard for knowledge representation across several domains. In Life Sciences, numerous ontologies have been introduced to represent human knowledge, often providing overlapping or conflicting perspectives. These ontologies are usually published as OWL or OBO, and are often registered in open repositories, e.g., BioPortal. However, the task of finding the concepts (classes and their properties) defined in the existing ontologies and the relationships between these concepts across different ontologies – for example, for developing a new ontology aligned with the existing ones – requires a great deal of manual effort in searching through the public repositories for candidate ontologies and their entities. In this work, we develop a new tool, KNIT, to automatically explore open repositories to help users fetch the previously designed concepts using keywords. User-specified keywords are then used to retrieve matching names of classes or properties. KNIT then creates a draft knowledge graph populated with the concepts and relationships retrieved from the existing ontologies. Furthermore, following the process of ontology learning, our tool refines this first draft of an ontology. We present three BioPortal-specific use cases for our tool. These use cases outline the development of new knowledge graphs and ontologies in the sub-domains of biology: genes and diseases, virome and drugs.}
}
@article{WU2021477,
title = {Unified modeling of a tractor performance prototype based on ontology},
journal = {Transactions of The Canadian Society for Mechanical Engineering},
volume = {46},
number = {2},
pages = {477-489},
year = {2021},
issn = {0315-8977},
doi = {https://doi.org/10.1139/tcsme-2021-0124},
url = {https://www.sciencedirect.com/science/article/pii/S0315897721000161},
author = {Yiwei Wu and Zhili Zhou and Xianghai Yan},
keywords = {tractor, performance prototype, unified modeling, ontology, metamodel, tracteur, prototype de performance, modélisation unifiée, ontologie, métamodèle},
abstract = {A tractor is a type of agricultural machinery with complex structure and harsh operating conditions. It is evolving toward a large-scale, multifunctional, and intelligent system. Digital prototype technology is an effective approach for experts in multidisciplinary fields to collaborate in the development of new tractor products. Tractor performance prototype design is an important part of realizing digital tractor design. In the modeling process, the performance prototype models designed by experts have a problem with inconsistent expressions, making tractor digital design difficult to implement. This study aims to investigate the unified modeling of a tractor performance prototype. The design process of the tractor performance prototype was analyzed according to the characteristics of new tractor product development. Combined with the ontology modeling method, the construction process of the tractor performance prototype ontology was designed. Based on ontology metamodel theory, a multidisciplinary unified modeling method for a tractor performance prototype is proposed, and an ontology metamodel architecture was constructed. Using a wheeled tractor as an example, a performance prototype ontology was designed. Subsequently, an ontology model was created and verified in Protégé. The results indicate that the model can be used for the digital design of new tractor product development. An ontology model database was established, which realized the sharing and management of ontology model data, and the effectiveness of the method was verified.
Le tracteur est une machine agricole à la structure complexe et à l’environnement d’application difficile. Il évolue pour devenir une machine à grande échelle, multifonctionnelle et intelligente. La technologie du prototype numérique est une approche efficace permettant aux experts de domaines pluridisciplinaires de collaborer au développement de nouveaux produits de tracteurs. La conception du prototype de performance du tracteur est un élément important pour la réalisation de la conception numérique du tracteur. Dans son processus de modélisation, les prototypes de performance conçus par des experts ont un problème d’expression incohérente, ce qui rend la conception numérique des tracteurs difficile à mettre en œuvre. Cet article vise à étudier la modélisation unifiée du prototype de performance du tracteur. Selon les caractéristiques du développement du nouveau produit tracteur, le processus de conception du prototype de performance du tracteur est analysé. Combiné avec la méthode de modélisation de l’ontologie, le processus de construction de l’ontologie du prototype de performance du tracteur a été conçu. Sur la base de la théorie du métamodèle ontologique, une méthode de modélisation unifiée multidisciplinaire du prototype de performance du tracteur est proposée, et l’architecture du métamodèle ontologique est construite. En prenant le tracteur à roues comme exemple, son ontologie prototype de performance a été conçue. Le modèle d’ontologie a ensuite été créé et vérifié dans Protégé. Les résultats montrent que le modèle peut être utilisé pour la conception numérique du développement de nouveaux produits de tracteurs. La base de données du modèle ontologique a été établie, ce qui a permis de réaliser le partage et la gestion des données du modèle ontologique et de vérifier l’efficacité de la méthode. [Traduit par la Rédaction]}
}
@article{WISNIEWSKI2019100534,
title = {Analysis of Ontology Competency Questions and their formalizations in SPARQL-OWL},
journal = {Journal of Web Semantics},
volume = {59},
pages = {100534},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.100534},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300617},
author = {Dawid Wiśniewski and Jedrzej Potoniec and Agnieszka Ławrynowicz and C. Maria Keet},
keywords = {Ontology Authoring, Competency Questions, SPARQL-OWL},
abstract = {Competency Questions (CQs) are natural language questions outlining and constraining the scope of knowledge represented in an ontology. Despite that CQs are a part of several ontology engineering methodologies, the actual publication of CQs for the available ontologies is very limited and even scarcer is the publication of their respective formalizations in terms of, e.g., SPARQL queries. This paper aims to contribute to addressing the myriad of engineering hurdles to using CQs in ontology development. A prerequisite to this is to understand the relation between CQs and the queries over the ontology. We use a new dataset of 234 competency questions and their SPARQL-OWL queries for several ontologies in different domains developed by different groups, and analysed the CQs in two principal ways. The first stage focused on a linguistic analysis of the natural language text itself, i.e., a lexico-syntactic analysis without any presuppositions of ontology elements, and a subsequent step of semantic analysis in order to find patterns. This increased diversity of CQ sources resulted in a 4-5-fold increase of hitherto published patterns, to 106 distinct CQ patterns, which have a limited subset of few patterns shared across the CQ sets from the different ontologies. Next, we analysed the relation between the found CQ patterns and their respective SPARQL-OWL patterns, which revealed that one CQ pattern may be realized by more than one SPARQL-OWL query pattern, and vice versa. These insights may contribute to establishing common practices, templates, automation, and user tools that will support CQ formulation, formalization, execution, and general management.}
}
@article{ALI201927,
title = {Transportation sentiment analysis using word embedding and ontology-based topic modeling},
journal = {Knowledge-Based Systems},
volume = {174},
pages = {27-42},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.02.033},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119300942},
author = {Farman Ali and Daehan Kwak and Pervez Khan and Shaker El-Sappagh and Amjad Ali and Sana Ullah and Kye Hyun Kim and Kyung-Sup Kwak},
keywords = {Social network analysis, Sentiment analysis, Topic modeling, Mobility users, Word embedding},
abstract = {Social networks play a key role in providing a new approach to collecting information regarding mobility and transportation services. To study this information, sentiment analysis can make decent observations to support intelligent transportation systems (ITSs) in examining traffic control and management systems. However, sentiment analysis faces technical challenges: extracting meaningful information from social network platforms, and the transformation of extracted data into valuable information. In addition, accurate topic modeling and document representation are other challenging tasks in sentiment analysis. We propose an ontology and latent Dirichlet allocation (OLDA)-based topic modeling and word embedding approach for sentiment classification. The proposed system retrieves transportation content from social networks, removes irrelevant content to extract meaningful information, and generates topics and features from extracted data using OLDA. It also represents documents using word embedding techniques, and then employs lexicon-based approaches to enhance the accuracy of the word embedding model. The proposed ontology and the intelligent model are developed using Web Ontology Language and Java, respectively. Machine learning classifiers are used to evaluate the proposed word embedding system. The method achieves accuracy of 93%, which shows that the proposed approach is effective for sentiment classification.}
}
@article{PERNISCH2022100715,
title = {Visualising the effects of ontology changes and studying their understanding with ChImp},
journal = {Journal of Web Semantics},
volume = {74},
pages = {100715},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100715},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000117},
author = {Romana Pernisch and Daniele Dell’Aglio and Mirko Serbak and Rafael S. Gonçalves and Abraham Bernstein},
keywords = {Ontology editing, Materialisation, User study, Ontology evolution impact},
abstract = {Due to the Semantic Web’s decentralised nature, ontology engineers rarely know all applications that leverage their ontology. Consequently, they are unaware of the full extent of possible consequences that changes might cause to the ontology. Our goal is to lessen the gap between ontology engineers and users by investigating ontology engineers’ understanding of ontology changes’ impact at editing time. Hence, this paper introduces the Protégé plugin ChImp which we use to reach our goal. We elicited requirements for ChImp through a questionnaire with ontology engineers. We then developed ChImp according to these requirements and it displays all changes of a given session and provides selected information on said changes and their effects. For each change, it computes a number of metrics on both the ontology and its materialisation. It displays those metrics on both the originally loaded ontology at the beginning of the editing session and the current state to help ontology engineers understand the impact of their changes. We investigated the informativeness of materialisation impact measures, the meaning of severe impact, and also the usefulness of ChImp in an online user study with 36 ontology engineers. We asked the participants to solve two ontology engineering tasks – with and without ChImp (assigned in random order) – and answer in-depth questions about the applied changes as well as the materialisation impact measures. We found that ChImp increased the participants’ understanding of change effects and that they felt better informed. Answers also suggest that the proposed measures were useful and informative. We also learned that the participants consider different outcomes of changes severe, but most would define severity based on the amount of changes to the materialisation compared to its size. The participants also acknowledged the importance of quantifying the impact of changes and that the study will affect their approach of editing ontologies.}
}
@article{AYDIN2020105589,
title = {Ontology-based data acquisition model development for agricultural open data platforms and implementation of OWL2MVC tool},
journal = {Computers and Electronics in Agriculture},
volume = {175},
pages = {105589},
year = {2020},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2020.105589},
url = {https://www.sciencedirect.com/science/article/pii/S0168169920304658},
author = {Sahin Aydin and Mehmet N. Aydin},
keywords = {Agricultural ontology-based forms generation, Ontology-based data acquisition forms, Ontology-based data acquisition model, Ontology-based web forms, Open data platforms},
abstract = {In the open data world, it is difficult to collect data in compliance with a certain data model that is of interest to different types of stakeholders within a domain like agriculture. Ontologies that provide broad vocabularies and metadata with respect to a given domain can be used to create various data models. We consider that while creating data acquisition forms to gather data related to an agricultural product, which is hazelnut in this study, from stakeholders of the relevant domain, the traits can be modeled as attributes of the data models. We propose a generic ontology-based data acquisition model to create data acquisition forms based on model-view-controller (MVC) design pattern, to publish and make use of on the agricultural open data platforms. We develop a tool called OWL2MVC that integrates the Hazelnut Ontology, which illustrates the effectiveness of the proposed model for generating data acquisition forms. Because model creation is implemented in compliance with the selection of ontology classes, stakeholders; in other words, the users of OWL2MVC Tool could generate data acquisition forms quickly and independently. OWL2MVC Tool was evaluated in terms of usability by fifty-three respondents implementing the case-study scenario. Among others the findings show that the tool has satisfactory usability score overall and is promising to provide stakeholders with required support for agricultural open data platforms.}
}
@incollection{DAVID2024251,
title = {Chapter Thirteen - Automatic programming (source code generator) based on an ontological model},
editor = {Preetha Evangeline David and P. Anandhakumar},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {132},
pages = {251-272},
year = {2024},
booktitle = {Applying Computational Intelligence for Social Good},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2023.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0065245823000748},
author = {Preetha Evangeline David and S. Malathi and P. Anandhakumar},
keywords = {Deep learning, Language model, Source code, Software engineering, Natural language processing, Ontological modeling},
abstract = {Source AI is an AI-powered tool that can generate code in any programming language from any human language description. It can also simplify, find errors and fix them and debug your code. Automatic code generation capabilities continue to evolve within programming languages, IDEs and tools that work at compile time. This coding technique has proliferated because it can reduce mundane programming grunt work, and developers have found that it improves turnaround times and accuracy. Auto generated code usually becomes a hindrance for developers who want to tweak it later on Teams should plan to restrict these tools to only certain parts of the SDLC, such as where they can act as facilitators in smaller, less complex situations.}
}
@article{MEYER2020136263,
title = {Enhancing life cycle chemical exposure assessment through ontology modeling},
journal = {Science of The Total Environment},
volume = {712},
pages = {136263},
year = {2020},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2019.136263},
url = {https://www.sciencedirect.com/science/article/pii/S004896971936259X},
author = {David E. Meyer and Sidney C. Bailin and Daniel Vallero and Peter P. Egeghy and Shi V. Liu and Elaine A. {Cohen Hubal}},
keywords = {Human exposure modeling, Life cycle assessment, Ontology modeling, Data accessibility, Consumer products, -Dichlorobenzene},
abstract = {In its 2014 report, A Framework Guide for the Selection of Chemical Alternatives, the National Academy of Sciences placed increased emphasis on comparative exposure assessment throughout the life cycle (i.e., from manufacturing to end-of-life) of a chemical. The inclusion of the full life cycle greatly increases the data demands for exposure assessments, including both the quantity and type of data. High throughput tools for exposure estimation add to this challenge by requiring rapid accessibility to data. In this work, ontology modeling was used to bridge the domains of exposure modeling and life cycle inventory modeling to facilitate data sharing and integration. The exposure ontology, ExO, is extended to describe human exposure to consumer products, while an inventory modeling ontology, LciO, is formulated to support automated data mining. The core ontology pieces are connected using a bridging ontology and discussed through a theoretical example to demonstrate how data from LCA can be leveraged to support rapid exposure modeling within a life cycle context.}
}
@article{ROVETTO2020451,
title = {Orbital debris ontology, terminology, and knowledge modeling},
journal = {Journal of Space Safety Engineering},
volume = {7},
number = {3},
pages = {451-458},
year = {2020},
note = {Space Debris: The State of Art},
issn = {2468-8967},
doi = {https://doi.org/10.1016/j.jsse.2020.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2468896720300720},
author = {Robert J. Rovetto and T.S. Kelso and Daniel A. O'Neil},
keywords = {Orbital debris, space debris, ontology, knowledge graph, semantic technology, artificial intelligence, information fusion, space object, knowledge representation and reasoning, knowledge model, semantic model, terminology, taxonomy, classification, metadata, linked data, orbit data, two-line element set, knowledge engineering, space situational awareness, space system ontology},
abstract = {ABSTRACT
The looming threat orbital debris poses to assets in orbit demands solutions. As the orbital population grows, so does the risk of collision, as well as the volume of associated data. It is, however, an opportunity for interdisciplinary innovation and cooperation. This paper focuses on the data management and knowledge modeling aspect of developing solutions for a sustainable and safe orbital space environment. An in-progress work to develop an orbital debris reference ontology [Rovetto, 2015] is summarized in order to discuss knowledge modeling for orbital debris. The formal analysis of this effort can contribute to standards development, as well as terminological and policy challenges. Leveraging the growing volumes of orbital debris and space situational awareness (SSA) data will create a more complete picture of the orbital space environment. Part of the solution will be: consistent and correct data interpretation, sharing orbital debris and SSA data in one form or another, terminology development & harmonization, and knowledge or domain modeling. To facilitate this, [Rovetto, 2015] proposed ontology development for the orbital debris and broader space domain. This paper summarizes concepts from that paper, and subsequently developed concepts [2-9]. See also https://purl.org/space-ontology and https://purl.org/space-ontology/odo. Ontology engineering is an interdisciplinary area of research related to knowledge representation and reasoning in artificial intelligence, model-based systems engineering, semantic technologies and the so-called semantic web. An ontology is effectively a computable and semantically rich terminology that presents a knowledge or domain model for a topic area. Expressions of knowledge, beliefs, or assertions are stored using formally defined terms. This knowledge base is reasoned over to yield answers to database queries, among other things. Ontologies have been developed in knowledge-based projects across various disciplines, and used for such things as search engines, chatbots, and enterprise knowledge graphs. Ontologies aim to support: interoperability, automated reasoning, data sharing and integration, data search and retrieval, and communicating the meaning of data. The Orbital Debris Ontology (ODO) [1], and related ontologies [Rovetto & Kelso 2016] [Rovetto 2016, 2017], were proposed to help achieve this. ODO, for instance, is intended as a domain ontology that can be used across federated databases, offering an explicitly specified set of concepts describing the orbital debris domain. Its meaning-rich taxonomy will provide a sharable semantics for orbital debris data to, in part, consistently communicate the meaning of data to both humans and machines, and tag data elements in space object catalogs to help afford automated reasoning tasks, decision support, knowledge discovery, and information integration. ODO and the SSA ontology (SSAO) [2] is part of the overall Orbital Space Ontology concept, which is conceived as a unifying domain reference ontology. It aims to provide a knowledge representation structure of orbital space, a common semantic model, and develop a sharable terminology. Collectively this will provide common meaning for datasets, a high-level taxonomy or classification for orbital space objects, and a means to characterize space objects. Ongoing efforts have included using visualizations, R, JSON-LD, and contemporary semantic technologies. Potential applications include web-based platforms, web apps, visualizations, modeling and simulations. Community input and participation may yield a more widely understood domain model as well as facilitate terminological standards. For example, conceptual, terminological, and ontological analysis can make helpful contributions to such efforts as the Space Debris Mitigation Requirements by developing more precise, consistent and coherent terms and definitions. Other projects can also use ODO (and its related ontologies) as a common knowledge model, metadata set, taxonomy or vocabulary. Readers interested in supporting development are encouraged to make contact.}
}
@article{DAVID2019349,
title = {Attaining Learning Objectives by Ontological Reasoning using Digital Twins},
journal = {Procedia Manufacturing},
volume = {31},
pages = {349-355},
year = {2019},
note = {Research. Experience. Education. 9th Conference on Learning Factories 2019 (CLF 2019), Braunschweig, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2019.03.055},
url = {https://www.sciencedirect.com/science/article/pii/S2351978919304196},
author = {Joe David and Andrei Lobov and Minna Lanz},
keywords = {Digital Twin, Learning Outcomes, ontology, Reasoning, Web Ontology Language (OWL), Learning, Pedagogy},
abstract = {Learning Factories provide a propitious learning environment for nurturing production related competencies. However, several problems continue to plague their widespread adoption. This study mentions these issues before proposing the use of digital twins as an alternative. The study presents an approach towards modelling such a digital twin and proposes a solution that uses ontologies to develop a formal representation of the domain (a flexible manufacturing system) and the learning that occurs in the environment. A reasoning mechanism is used deduce inferences from the ontology to facilitate automated assessment of the learner. A use-case for the pedagogic digital twin is presented and discussed before proposing future directions for work.}
}
@article{ZHOU201845,
title = {Parallel tractability of ontology materialization: Technique and practice},
journal = {Journal of Web Semantics},
volume = {52-53},
pages = {45-65},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300489},
author = {Zhangquan Zhou and Guilin Qi and Birte Glimm},
keywords = {Ontology, Materialization, Datalog, Parallel tractability,  complexity},
abstract = {Materialization is an important reasoning service for many ontology-based applications, but the rapid growth of semantic data poses the challenge to efficiently perform materialization on large-scale ontologies. Parallel materialization algorithms work well for some ontologies, although the reasoning problem for the used ontology language is not in NC, i.e., the theoretical complexity class for parallel tractability. This motivates us to study the problem of parallel tractability of ontology materialization from a theoretical perspective. We focus on the datalog rewritable ontology languages DL-Lite and Description Horn Logic (DHL) and propose algorithms, called NC algorithms, to identify classes of ontologies for which materialization is tractable in parallel. To verify the practical usability of the above results, we analyze different benchmarks and real-world datasets, including LUBM and the YAGO ontology, and show that for many ontologies expressed in DHL materialization is tractable in parallel. The implementation of our optimized parallel algorithm shows performance improvements over the highly optimized state-of-the-art reasoner RDFox on ontologies for which materialization is tractable in parallel.}
}
@article{MALEC2023104368,
title = {Causal feature selection using a knowledge graph combining structured knowledge from the biomedical literature and ontologies: A use case studying depression as a risk factor for Alzheimer’s disease},
journal = {Journal of Biomedical Informatics},
volume = {142},
pages = {104368},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104368},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000898},
author = {Scott A. Malec and Sanya B. Taneja and Steven M. Albert and C. {Elizabeth Shaaban} and Helmet T. Karim and Arthur S. Levine and Paul Munro and Tiffany J. Callahan and Richard D. Boyce},
keywords = {Knowledge representation, management, or engineering, Knowledge graphs, Causal modeling, Feature selection, Alzheimer’s disease, Depression},
abstract = {Background
Causal feature selection is essential for estimating effects from observational data. Identifying confounders is a crucial step in this process. Traditionally, researchers employ content-matter expertise and literature review to identify confounders. Uncontrolled confounding from unidentified confounders threatens validity, conditioning on intermediate variables (mediators) weakens estimates, and conditioning on common effects (colliders) induces bias. Additionally, without special treatment, erroneous conditioning on variables combining roles introduces bias. However, the vast literature is growing exponentially, making it infeasible to assimilate this knowledge. To address these challenges, we introduce a novel knowledge graph (KG) application enabling causal feature selection by combining computable literature-derived knowledge with biomedical ontologies. We present a use case of our approach specifying a causal model for estimating the total causal effect of depression on the risk of developing Alzheimer’s disease (AD) from observational data.
Methods
We extracted computable knowledge from a literature corpus using three machine reading systems and inferred missing knowledge using logical closure operations. Using a KG framework, we mapped the output to target terminologies and combined it with ontology-grounded resources. We translated epidemiological definitions of confounder, collider, and mediator into queries for searching the KG and summarized the roles played by the identified variables. We compared the results with output from a complementary method and published observational studies and examined a selection of confounding and combined role variables in-depth.
Results
Our search identified 128 confounders, including 58 phenotypes, 47 drugs, 35 genes, 23 collider, and 16 mediator phenotypes. However, only 31 of the 58 confounder phenotypes were found to behave exclusively as confounders, while the remaining 27 phenotypes played other roles. Obstructive sleep apnea emerged as a potential novel confounder for depression and AD. Anemia exemplified a variable playing combined roles.
Conclusion
Our findings suggest combining machine reading and KG could augment human expertise for causal feature selection. However, the complexity of causal feature selection for depression with AD highlights the need for standardized field-specific databases of causal variables. Further work is needed to optimize KG search and transform the output for human consumption.}
}
@article{ALZAMIL2020100469,
title = {An ontological artifact for classifying social media: Text mining analysis for financial data},
journal = {International Journal of Accounting Information Systems},
volume = {38},
pages = {100469},
year = {2020},
note = {2019 UW CISA Symposium},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2020.100469},
url = {https://www.sciencedirect.com/science/article/pii/S1467089520300373},
author = {Zamil Alzamil and Deniz Appelbaum and Robert Nehmer},
keywords = {FIBO, Ontology, Social media, Frames and slots, Municipal bonds},
abstract = {In this paper we utilize a structured natural language processing implementation of the Financial Industry Business Ontology (FIBO) to extract financial information from the unstructured textual data of the social media platform Twitter regarding financial and budget information in the public sector, namely the two public-private agencies of the Port Authority of NY and NJ (PANYNJ), and the NY Metropolitan Transportation Agency (MTA). This research initiative uses the Design Science Research (DSR) perspective to develop an artifact to classify tweets as being either relevant to financial bonds or not. We apply a frame and slot approach from the artificial intelligence and natural language processing literature to operationalize this artifact. FIBO provides standards for defining the facts, terms, and relationships associated with financial concepts. We show that FIBO grammar can be used to mine semantic meaning from unstructured textual data and that it provides a nuanced representation of structured financial data. With this artifact, social media such as Twitter may be accessed for the knowledge that its text contains about financial concepts using the FIBO ontology. This process is anticipated to be of interest to bond issuers, regulators, analysts, investors, and academics. It may also be extended towards other financial domains such as securities, derivatives, commodities, and banking that relate to FIBO ontologies, as well as more generally to develop a structured knowledge representation of unstructured data through the application of an ontology.}
}
@article{CAMIRE2023102342,
title = {Proposing an ontological shift from intervention to intravention in sport and exercise psychology},
journal = {Psychology of Sport and Exercise},
volume = {64},
pages = {102342},
year = {2023},
issn = {1469-0292},
doi = {https://doi.org/10.1016/j.psychsport.2022.102342},
url = {https://www.sciencedirect.com/science/article/pii/S1469029222002102},
author = {Martin Camiré},
keywords = {Diffraction, Entanglement, Relationality, Phenomena, Intra-action},
abstract = {Intervention research entails the measurement of change in a situation or individual after a modification has been imposed. In sport and exercise psychology, interventions have been implemented in a variety of situations (e.g., performance, doping, physical activity, mental health) for a variety of individuals (e.g., athletes, coaches, sedentary people). Despite their widespread use, accruing evidence indicates that interventions in sport and exercise psychology have had less than anticipated success in instigating change. While some scholars have pointed to the need to confront the methodological challenges of intervention implementation, others have called for the creation of viable alternative forms of inquiry that can complement intervention research. The purpose of the present article is to propose an ontological shift from intervention to intravention in sport and exercise psychology. This shift is undertaken through the deployment of Barad’s (2007) agential realist ontology. The paper is divided into six sections. First, the concept of interaction is situated as it forms the basis for how interventions are conducted. Second, the Baradian ontology of agential realism is explained, along with key concepts. Third, intravention is positioned in relation to five guiding principles that delineate the progression for conducting intravention inquiries. Fourth, alternate meanings are proposed for behaviour, change, and knowledge. Fifth, examples are provided as to how intravention inquiries can be deployed in sport and exercise psychology. Sixth, concluding thoughts are offered. As an ontological becoming of interventions, intraventions are situated as open-ended approaches to inquiry that can help researchers derive alternate understandings of existence and can take the psychology of sport and exercise in exciting and affirmative directions.}
}
@article{ADHIKARI2022116281,
title = {Finding most informative common ancestor in cross-ontological semantic similarity assessment: An intrinsic information content-based approach},
journal = {Expert Systems with Applications},
volume = {192},
pages = {116281},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116281},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015888},
author = {Abhijit Adhikari and Biswanath Dutta and Animesh Dutta},
keywords = {Semantic similarity, Information theory, Knowledge based systems, Information retrieval, Ontology},
abstract = {Semantic Similarity (SS) has become a long-standing research domain in artificial intelligence and cognitive science for measuring the strength of the semantic relationship between entities (e.g., words, documents). Several ontology-based SS measures have been proposed in the recent time due to their ability of mimicking the cognitive process of humans. Among them, intrinsic information content (IC) based approaches have shown a significant correlation with human assessment. The design principle of the existing intrinsic IC-based SS measures constrain themselves to be applicable in a single ontology. However, such SS measures can be leveraged within two ontologies with the help of identifying the most informative common ancestors (MICA) across the ontologies. Existing IC-based MICA identification algorithms follow string matching of the labels of the concepts. In this paper, we propose a novel intrinsic IC-based MICA finding algorithm that exploits two domain-ontologies for finding SS without using string matching of the labels. The proposed approach has been evaluated using a widely used benchmark dataset of medical terms. The experimental results show that the proposed IC-based approach can be a stepping stone to a new direction in the process of finding MICA over two ontologies.}
}
@article{AUDRITO2025106171,
title = {Towards semi-automating European legislative harmonisation analysis: A harmonised glossary for LLM-based legal concept detection},
journal = {Computer Law & Security Review},
volume = {58},
pages = {106171},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106171},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000446},
author = {Davide Audrito and Ivan Spada and Rachele Mignone and Emilio Sulis and Luigi Di Caro},
keywords = {Legal harmonisation, Harmonisation glossary, Large language models, Similarity, Legal concept detection, Concept-based similarity},
abstract = {Achieving legislative harmonisation within the European Union (EU) is a multifaceted challenge, hampered by various political, economic, and legal complexities. This article addresses the significant issue of non-compliance by EU member states in transposing EU laws into national frameworks, underscored by numerous infringement procedures. This work introduces a novel methodological framework that combines semantic knowledge modelling and transformer-based language models to address discrepancies in legislative harmonisation. Central to the proposed methodology is the creation of a comprehensive glossary designed to establish correspondences between European legislative concepts and their national counterparts, thus facilitating greater accuracy in legal harmonisation. By deploying Large Language Models (LLMs) for semi-automating concept detection, complemented by legal harmonisation expert’s oversight, this research provides an exhaustive, explainable assessment of legislative approximation within the EU. The findings enrich the academic debate on legal harmonisation offering actionable tools designed to decrease the frequency and gravity of infringement procedures, while promoting a more unified and efficient legal framework across the Union. The complete dataset and resources are available at the following link: GitHub repository.}
}
@article{CIFUENTES2023100068,
title = {Co-producing autonomy? Forest monitoring programs, territorial ontologies, and Indigenous politics in Amazonia},
journal = {Digital Geography and Society},
volume = {5},
pages = {100068},
year = {2023},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2023.100068},
url = {https://www.sciencedirect.com/science/article/pii/S266637832300020X},
author = {Sylvia Cifuentes},
keywords = {Smart earth technologies, Indigenous geographies, Ontological politics, Territorial politics, Climate justice},
abstract = {Forest monitoring programs have become widespread in Amazon Basin countries. Using GPS artifacts, smartphones, drones, and other technologies, international environmental non-government organizations (IENGOs) propose these programs as tools to control and stop deforestation events—and thus of climate change mitigation. These also seem like ideal initiatives for IENGOs to collaborate with Indigenous organizations, responding to calls to include their knowledge in climate governance. I analyze forest/territorial monitoring programs created by the Coordinator of Indigenous Organizations of the Amazon Basin (COICA) and its member organizations in Ecuador and Peru. Scholarship in Science and Technology Studies (STS) and digital geographies has demonstrated how digital environmental technologies and environmental relations and politics can co-produce and shape one another. It has also referred to the historically exploitative relationships that technoscientific projects have enforced towards Indigenous peoples, and the potential of digital tools for emancipatory goals. I argue that forest monitoring programs and technologies co-produce forms of climate and territorial politics in Amazonia. Through forest monitoring programs, Indigenous leaders and organizations imagine and enact territorial defense, or a politics founded on integral territorial ontologies. That is, they see the programs as tools to strengthen their autonomy, to build the capacities of leaders at all scales of political organization, and to support their claims for territorial rights. For them, technologies can make Indigenous cosmovisions or ancestral knowledges visible. However, these programs can also reinforce a politics (of IENGOs) where territories are spaces with strict boundaries and exclusive rights, and which encourages open-access information, thus potentially threatening Indigenous autonomy. Thus, I discuss the intrinsically contradictory impact of monitoring technologies, as the conceptions of territories as lifeworlds, and the embeddedness of ancestral knowledges in them, further exceed their possibilities. Conclusions highlight the importance of attending to Indigenous territorial defense to understand how (new) technologies and society shape each other, and the many implications of climate change responses to justice issues.}
}
@article{VILLAMARGOMEZ2021103763,
title = {Ontology-based knowledge management with verbal interaction for command interpretation and execution by home service robots},
journal = {Robotics and Autonomous Systems},
volume = {140},
pages = {103763},
year = {2021},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2021.103763},
url = {https://www.sciencedirect.com/science/article/pii/S0921889021000488},
author = {L. {Villamar Gómez} and J. Miura},
keywords = {Knowledge management, Ontology-based, Service robots, Human–robot interaction},
abstract = {This paper describes a system for service robots that combines ontological knowledge reasoning and human–robot interaction to interpret natural language commands and successfully perform household chores, such as finding and delivering objects. Knowledge and context reasoning is essential for providing more efficient service robots, given their diverse and continuously changing environments. Moreover, since they are in contact with humans, robots require such skills as interaction and language. Therefore, we developed a system with specific modules to manage robots’ knowledge and reasoning, command analysis, decision-making, and talking interaction. The system relies on inference methods and verbal interaction to understand commands and clarify uncertain information. We tested our system inside a simulated environment where the robot receives commands with missing or unclear information. The system’s performance was compared with the average performance of human subjects who completed the same commands in the simulation.}
}
@article{PENDLETON2021104542,
title = {Development and application of the ocular immune-mediated inflammatory diseases ontology enhanced with synonyms from online patient support forum conversation},
journal = {Computers in Biology and Medicine},
volume = {135},
pages = {104542},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104542},
url = {https://www.sciencedirect.com/science/article/pii/S001048252100336X},
author = {Samantha C. Pendleton and Karin Slater and Andreas Karwath and Rose M. Gilbert and Nicola Davis and Konrad Pesudovs and Xiaoxuan Liu and Alastair K. Denniston and Georgios V. Gkoutos and Tasanee Braithwaite},
keywords = {Uveitis, Ontology, Inflammation, Patient voice, Sentiment},
abstract = {Background
Unstructured text created by patients represents a rich, but relatively inaccessible resource for advancing patient-centred care. This study aimed to develop an ontology for ocular immune-mediated inflammatory diseases (OcIMIDo), as a tool to facilitate data extraction and analysis, illustrating its application to online patient support forum data.
Methods
We developed OcIMIDo using clinical guidelines, domain expertise, and cross-references to classes from other biomedical ontologies. We developed an approach to add patient-preferred synonyms text-mined from oliviasvision.org online forum, using statistical ranking. We validated the approach with split-sampling and comparison to manual extraction. Using OcIMIDo, we then explored the frequency of OcIMIDo classes and synonyms, and their potential association with natural language sentiment expressed in each online forum post.
Findings
OcIMIDo (version 1.2) includes 661 classes, describing anatomy, clinical phenotype, disease activity status, complications, investigations, interventions and functional impacts. It contains 1661 relationships and axioms, 2851 annotations, including 1131 database cross-references, and 187 patient-preferred synonyms. To illustrate OcIMIDo's potential applications, we explored 9031 forum posts, revealing frequent mention of different clinical phenotypes, treatments, and complications. Language sentiment analysis of each post was generally positive (median 0.12, IQR 0.01–0.24). In multivariable logistic regression, the odds of a post expressing negative sentiment were significantly associated with first posts as compared to replies (OR 3.3, 95% CI 2.8 to 3.9, p < 0.001).
Conclusion
We report the development and validation of a new ontology for inflammatory eye diseases, which includes patient-preferred synonyms, and can be used to explore unstructured patient or physician-reported text data, with many potential applications.}
}
@incollection{GERBER2023292,
title = {The ontological imperative in conducting qualitative research in online spaces},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {292-301},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.11027-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305110279},
author = {Hannah R. Gerber},
keywords = {Algorithms, API data, Big data, Digital methods, Internet research, Networked field sites, Online spaces, Ontological imperative, Qualitative research, Software},
abstract = {Increasingly, researchers are turning to the Internet as a networked field site for engaging in rigorous qualitative studies to understand learning, socialization, culture, and meaning making. However, the tools, systems, and services that are used by researchers to study online spaces have specific purposes that do not always align with researchers' purposes. Therefore, these digital tools, systems, and services must be closely examined and interrogated by researchers in order to establish trustworthy epistemological claims. This chapter introduces the ontological imperative framework as a scaffold for researchers to interrogate the digital tools, systems, and services that are part of their research studies and provides an empirical application of the ontological imperative framework within an existing qualitative study. The chapter concludes with practical applications for researchers to consider.}
}
@article{XUE2022103549,
title = {Ontologies representing multidisciplinary decision-making rationales for sustainable infrastructure developments},
journal = {Sustainable Cities and Society},
volume = {77},
pages = {103549},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103549},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721008155},
author = {Bin Xue and Xingbin Chen and Bingsheng Liu and Dong Zhao and Zheng Zhang and Jung In Kim},
keywords = {Multidisciplinary decision-making, Ontology model, Decision rationale, Infrastructure sustainability, Uncertainty mitigation},
abstract = {A multidisciplinary decision-making (MDM) process is required for sustainable infrastructure developments in early design stage of integrated project delivery. Decision makers from multiple disciplines employ heterogeneous decision criteria when evaluating decision alternatives for the project developments. This results in uncertainty in alternative prioritization as the scoring ranges of sustainable performance indexes for those alternatives are indistinguishable. This study has formalized the rationales of decision coordinators for converting scores of sustainability performance indexes assessed by decision makers to the normalized ones based on their preferred decision criteria in a consistent and collective manner. Two ontologies have been formalized, which include 45 classes, 119 properties, 50 sub-properties and their facets. The ontologies are validated using three cases of sustainable infrastructure developments. Validation results indicate that they are formal, comprehensive, and reusable in representing MDM rationales, i.e., the interrelationships among decision makers, decision criteria, decision alternatives, and sustainability performance index. Theoretically, these two ontologies contribute to the Triple Bottom Line theory and social choice theory by specifying heterogeneous decision preferences in evaluating sustainable infrastructure developments. For practical implications, the developed ontologies serve as a computer-readable decision support tool to systematically store and communicate MDM information between multidisciplinary professionals in infrastructure project management.}
}
@article{LIU2025106482,
title = {Automated knowledge graph-based risk assessment for fall-from-height accidents in construction},
journal = {Automation in Construction},
volume = {179},
pages = {106482},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106482},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525005229},
author = {Qiong Liu and Yuexiong Ding and Xiaowei Luo},
keywords = {Fall-from-height, Accident reports, Knowledge graph, Large language models, Risk factor analysis},
abstract = {Fall-from-height (FFH) accidents remain a leading cause of fatalities in the construction industry. To systematically extract and analyze risk factors from unstructured FFH accident reports, this paper employed large language models (LLMs) to enable zero-shot automated fall-from-height knowledge graph (FFHKG) construction. By clustering FFHKG entities to merge semantically similar factors, a weighted complex network is formed, enabling topological analysis for quantitative risk assessment. A case study on 1097 FFH accident reports validates the proposed framework. Results demonstrate that GPT-4o achieves high extraction accuracy, with an F1 score of 0.94 in named entity recognition and a precision of 0.90 in relationship extraction. Key risk factors, such as poor safety management, lack of training, insufficient edge protection, etc., are quantitatively identified across multiple perspectives. Observable unsafe behaviors are also detected, offering insights for behavior-based safety monitoring. The proposed framework provides a data-driven solution for more effective safety management on construction sites.}
}
@article{CHEN2025,
title = {Large Language Model Applications for Health Information Extraction in Oncology: Scoping Review},
journal = {JMIR Cancer},
volume = {11},
year = {2025},
issn = {2369-1999},
doi = {https://doi.org/10.2196/65984},
url = {https://www.sciencedirect.com/science/article/pii/S236919992500045X},
author = {David Chen and Saif Addeen Alnassar and Kate Elizabeth Avison and Ryan S Huang and Srinivas Raman},
keywords = {artificial intelligence, chatbot, data extraction, AI, conversational agent, health information, oncology, scoping review, natural language processing, NLP, large language model, LLM, digital health, health technology, electronic health record},
abstract = {Background
Natural language processing systems for data extraction from unstructured clinical text require expert-driven input for labeled annotations and model training. The natural language processing competency of large language models (LLM) can enable automated data extraction of important patient characteristics from electronic health records, which is useful for accelerating cancer clinical research and informing oncology care.
Objective
This scoping review aims to map the current landscape, including definitions, frameworks, and future directions of LLMs applied to data extraction from clinical text in oncology.
Methods
We queried Ovid MEDLINE for primary, peer-reviewed research studies published since 2000 on June 2, 2024, using oncology- and LLM-related keywords. This scoping review included studies that evaluated the performance of an LLM applied to data extraction from clinical text in oncology contexts. Study attributes and main outcomes were extracted to outline key trends of research in LLM-based data extraction.
Results
The literature search yielded 24 studies for inclusion. The majority of studies assessed original and fine-tuned variants of the BERT LLM (n=18, 75%) followed by the Chat-GPT conversational LLM (n=6, 25%). LLMs for data extraction were commonly applied in pan-cancer clinical settings (n=11, 46%), followed by breast (n=4, 17%), and lung (n=4, 17%) cancer contexts, and were evaluated using multi-institution datasets (n=18, 75%). Comparing the studies published in 2022‐2024 versus 2019‐2021, both the total number of studies (18 vs 6) and the proportion of studies using prompt engineering increased (5/18, 28% vs 0/6, 0%), while the proportion using fine-tuning decreased (8/18, 44.4% vs 6/6, 100%). Advantages of LLMs included positive data extraction performance and reduced manual workload.
Conclusions
LLMs applied to data extraction in oncology can serve as useful automated tools to reduce the administrative burden of reviewing patient health records and increase time for patient-facing care. Recent advances in prompt-engineering and fine-tuning methods, and multimodal data extraction present promising directions for future research. Further studies are needed to evaluate the performance of LLM-enabled data extraction in clinical domains beyond the training dataset and to assess the scope and integration of LLMs into real-world clinical environments.}
}
@article{SERRANORUIZ2022150,
title = {Toward smart manufacturing scheduling from an ontological approach of job-shop uncertainty sources},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {150-155},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.185},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322001860},
author = {Julio C. Serrano-Ruiz and Josefa Mula and Raúl Poler},
keywords = {Industry 4.0, smart manufacturing scheduling, job-shop, scheduling, stochastic, disturbance, disruption, zero-defect manufacturing, digital twin, ontological framework},
abstract = {An integral application of the enabling technologies of Industry 4.0 in the job-shop scheduling problem (JSSP) must contemplate the automation and autonomy of the involved decision-making processes as a goal, which is the main purpose of the smart manufacturing scheduling (SMS) paradigm. In a real production context, uncertainty acts as a barrier that hinders this goal being met and, therefore, any SMS model should integrate uncertainty generators in one way or another. This paper proposes an ontological framework that identifies and structures the entities shaping the joint domain formed by the job-shop scheduling process in its itinerary toward the SMS paradigm, the sources of uncertainty that it faces, and the interrelationship type that link these entities. This ontological framework will serve in future research as a conceptual basis to design new quantitative models that, from a holistic perspective, will address the stochasticity of manufacturing environments and incorporate the management of disturbances into the realtime resolution of automatic and autonomous job-shop scheduling.}
}
@article{ELVE2019383,
title = {From ontology to executable program code},
journal = {Computers & Chemical Engineering},
volume = {122},
pages = {383-394},
year = {2019},
note = {2017 Edition of the European Symposium on Computer Aided Process Engineering (ESCAPE-27)},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2018.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0098135418309311},
author = {Arne Tobias Elve and Heinz A. Preisig},
keywords = {Process modelling, Automatic code generation, Ontology, Graph-based modelling},
abstract = {The implementation of coded mathematical process models is regarded as a cumbersome and challenging task, reasons being that the modeller needs to have expertise both in modelling and computer science. Our ProcessModellerSuite implements a staged approach to modelling starting with the formulation of a context-dependent ontology defining a structure against which the mathematical representation of the principal model components is defined. Process models are then generated by interactively constructing a graph of communicating principle components, which enables the generation of arbitrary complex process models and intermediate storage of customised unit models. This storage of unit models forms the equivalent of the traditional unit-operations libraries, by allowing for insertion of the unit models into other graphs. A task builder combines the information from the graph with the used model components to automatically generate executable program code of the process model, which will be the topic of this paper.}
}
@article{ZHANG2025104220,
title = {AttacKG+: Boosting attack graph construction with Large Language Models},
journal = {Computers & Security},
volume = {150},
pages = {104220},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104220},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005261},
author = {Yongheng Zhang and Tingwen Du and Yunshan Ma and Xiang Wang and Yi Xie and Guozheng Yang and Yuliang Lu and Ee-Chien Chang},
keywords = {Cyber threat intelligence analysis, Attack graph construction, Large Language Models},
abstract = {Attack graph construction seeks to convert textual cyber threat intelligence (CTI) reports into structured representations, portraying the evolutionary traces of cyber attacks. Even though previous research has proposed various methods to construct attack graphs, they generally suffer from limited generalization capability to diverse knowledge types as well as requirement of expertise in model design and tuning. Addressing these limitations, we seek to utilize Large Language Models (LLMs), which have achieved enormous success in a broad range of tasks given exceptional capabilities in both language understanding and zero-shot task fulfillment. Thus, we propose a fully automatic LLM-based framework to construct attack graphs named: AttacKG+. Our framework consists of four consecutive modules: rewriter, parser, identifier, and summarizer, each of which is implemented by instruction prompting and in-context learning empowered by LLMs. Furthermore, we upgrade the existing attack knowledge schema and propose a comprehensive version. We represent a cyber attack as a temporally unfolding event, each temporal step of which encapsulates three layers of representation, including behavior graph, MITRE TTP labels, and state summary. Extensive evaluation demonstrates that: (1) our formulation seamlessly satisfies the information needs in threat event analysis, (2) our construction framework is effective in faithfully and accurately extracting the information defined by AttacKG+. and (3) our attack graph directly benefits downstream security practices such as attack reconstruction. All the code and datasets will be released upon acceptance.}
}
@article{NIU2022104012,
title = {Fusion of sequential visits and medical ontology for mortality prediction},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {104012},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104012},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422000284},
author = {Ke Niu and You Lu and Xueping Peng and Jingni Zeng},
keywords = {Deep learning, Medical ontology, Electronic healthcare records, ICU mortality prediction},
abstract = {The goal of mortality prediction task is to predict the future death risk of patients according to their previous Electronic Healthcare Records (EHR). The main challenge of mortality prediction is how to design an accurate and robust predictive model with sequential, multivariate, sparse and irregular EHR data. In addition, the performance of model may be affected by lack of sufficient information of some patients with rare diseases in EHRs. To address these challenges, we propose a model to fuse Sequential visits and Medical Ontology to predict patients’ death risk. SeMO not only learns reasonable embeddings for medical concepts from sequential and irregular visits, but also exploits medical ontology to improve the prediction performance. With integration of multivariate features, SeMO learns robust representations of medical codes, mitigating data insufficiency and insightful sequential dependencies among patient’s visits. Experimental results on real world datasets prove that the proposed SeMO improves the prediction performance compared with the baseline approaches. Our model achieves an precision of up to 0.975. Compared with RNN, the precision has been improved up to 2.204%.}
}
@article{KIM20242190,
title = {Assessing the utility of large language models for phenotype-driven gene prioritization in the diagnosis of rare genetic disease},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {10},
pages = {2190-2202},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2024.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0002929724002969},
author = {Junyoung Kim and Kai Wang and Chunhua Weng and Cong Liu},
keywords = {large language model, rare disease diagnosis, gene prioritization, precision medicine, artificial intelligence, generative pre-trained transformers, phenotypes},
abstract = {Summary
Phenotype-driven gene prioritization is fundamental to diagnosing rare genetic disorders. While traditional approaches rely on curated knowledge graphs with phenotype-gene relations, recent advancements in large language models (LLMs) promise a streamlined text-to-gene solution. In this study, we evaluated five LLMs, including two generative pre-trained transformers (GPT) series and three Llama2 series, assessing their performance across task completeness, gene prediction accuracy, and adherence to required output structures. We conducted experiments, exploring various combinations of models, prompts, phenotypic input types, and task difficulty levels. Our findings revealed that the best-performed LLM, GPT-4, achieved an average accuracy of 17.0% in identifying diagnosed genes within the top 50 predictions, which still falls behind traditional tools. However, accuracy increased with the model size. Consistent results were observed over time, as shown in the dataset curated after 2023. Advanced techniques such as retrieval-augmented generation (RAG) and few-shot learning did not improve the accuracy. Sophisticated prompts were more likely to enhance task completeness, especially in smaller models. Conversely, complicated prompts tended to decrease output structure compliance rate. LLMs also achieved better-than-random prediction accuracy with free-text input, though performance was slightly lower than with standardized concept input. Bias analysis showed that highly cited genes, such as BRCA1, TP53, and PTEN, are more likely to be predicted. Our study provides valuable insights into integrating LLMs with genomic analysis, contributing to the ongoing discussion on their utilization in clinical workflows.}
}
@article{ZALETELJ2018129,
title = {A foundational ontology for the modelling of manufacturing systems},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {129-141},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S1474034616301264},
author = {Viktor Zaletelj and Rok Vrabič and Elvis Hozdić and Peter Butala},
keywords = {Manufacturing system, Modelling, Collaborative environment, Ontology},
abstract = {Models of distributed manufacturing systems cannot be consistent without a formal ontology. In this paper, the ontology formulation and maintenance are addressed in the scope of a collaborative modelling environment – in which concurrency, consistency, and model life cycle management should be supported. Thus, an extensible foundational ontology for manufacturing – system modelling is proposed in which the formal definitions of the modelling environment itself enable the definition of the manufacturing system’s elements. The presented approach ensures the consistency of ever-changing models. The ontology is integrated into a modelling framework through the concept of description layers that assist in the management of the model description’s complexity. The feasibility of the approaches is illustrated in an industrial case study that models of a manufacturing system for material processing.}
}
@article{KIV2022116520,
title = {Using an ontology for systematic practice adoption in agile methods: Expert system and practitioners-based validation},
journal = {Expert Systems with Applications},
volume = {195},
pages = {116520},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116520},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422000215},
author = {Soreangsey Kiv and Samedi Heng and Yves Wautelet and Stephan Poelmans and Manuel Kolp},
keywords = {Agile practice, Ontology, Knowledge representation, Real case study, Systematic literature review, Survey},
abstract = {As many software development teams have started to adopt agile methods, a vast amount of valuable experiences have been reported on in both academic and industrial knowledge bases. This information has been used through various approaches to guide and help practitioners finding suitable practices for their software development projects. Nevertheless, not many of these approaches could gather the available experiences to make them systematically reusable and help practitioners understanding agile practices in depth. To the best of our knowledge, only one ontology has been created to solve this problem; some limitations related to its quality and usability make it nevertheless unqualified to serve the intended purpose. The aim of this paper is to build an expert system (i.e. an evidence-based tool) to ease agile practices adoption by efficiently and effectively providing information on them. Firstly, we improve the concepts and relationships in the aforementioned ontology and theoretically validate it using a large data-set of agile practices adoption experiences collected through a Systematic Literature Review (SLR). Secondly, we develop a supporting tool having a friendly Graphical User Interface (GUI) allowing to use the ontology as a concrete agile practice knowledge provider. Finally, we empirically validate the enhanced ontology and evaluate the supporting tool using a survey with agile experts. Our supporting tool can help practitioners to decide what practice to adopt, how to adopt it, how to solve practical issues, etc. The ontology and the tool materialize our contribution to the field of systematic agile practices adoption.}
}
@article{BRENAS2019,
title = {Adverse Childhood Experiences Ontology for Mental Health Surveillance, Research, and Evaluation: Advanced Knowledge Representation and Semantic Web Techniques},
journal = {JMIR Mental Health},
volume = {6},
number = {5},
year = {2019},
issn = {2368-7959},
doi = {https://doi.org/10.2196/13498},
url = {https://www.sciencedirect.com/science/article/pii/S2368795919000787},
author = {Jon Hael Brenas and Eun Kyong Shin and Arash Shaban-Nejad},
keywords = {ontologies, mental health surveillance, adverse childhood experiences, semantics, computational psychiatry},
abstract = {Background
Adverse Childhood Experiences (ACEs), a set of negative events and processes that a person might encounter during childhood and adolescence, have been proven to be linked to increased risks of a multitude of negative health outcomes and conditions when children reach adulthood and beyond.
Objective
To better understand the relationship between ACEs and their relevant risk factors with associated health outcomes and to eventually design and implement preventive interventions, access to an integrated coherent dataset is needed. Therefore, we implemented a formal ontology as a resource to allow the mental health community to facilitate data integration and knowledge modeling and to improve ACEs’ surveillance and research.
Methods
We use advanced knowledge representation and semantic Web tools and techniques to implement the ontology. The current implementation of the ontology is expressed in the description logic ALCRIQ(D), a sublogic of Web Ontology Language (OWL 2).
Results
The ACEs Ontology has been implemented and made available to the mental health community and the public via the BioPortal repository. Moreover, multiple use-case scenarios have been introduced to showcase and evaluate the usability of the ontology in action. The ontology was created to be used by major actors in the ACEs community with different applications, from the diagnosis of individuals and predicting potential negative outcomes that they might encounter to the prevention of ACEs in a population and designing interventions and policies.
Conclusions
The ACEs Ontology provides a uniform and reusable semantic network and an integrated knowledge structure for mental health practitioners and researchers to improve ACEs’ surveillance and evaluation.}
}
@article{ALOBAIDI2018117,
title = {Automated ontology generation framework powered by linked biomedical ontologies for disease-drug domain},
journal = {Computer Methods and Programs in Biomedicine},
volume = {165},
pages = {117-128},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0169260717315791},
author = {Mazen Alobaidi and Khalid Mahmood Malik and Maqbool Hussain},
keywords = {Semantic web, Ontology generation, Linked biomedical ontologies},
abstract = {Objective and background: The exponential growth of the unstructured data available in biomedical literature, and Electronic Health Record (EHR), requires powerful novel technologies and architectures to unlock the information hidden in the unstructured data. The success of smart healthcare applications such as clinical decision support systems, disease diagnosis systems, and healthcare management systems depends on knowledge that is understandable by machines to interpret and infer new knowledge from it. In this regard, ontological data models are expected to play a vital role to organize, integrate, and make informative inferences with the knowledge implicit in that unstructured data and represent the resultant knowledge in a form that machines can understand. However, constructing such models is challenging because they demand intensive labor, domain experts, and ontology engineers. Such requirements impose a limit on the scale or scope of ontological data models. We present a framework that will allow mitigating the time-intensity to build ontologies and achieve machine interoperability. Methods: Empowered by linked biomedical ontologies, our proposed novel Automated Ontology Generation Framework consists of five major modules: a) Text Processing using compute on demand approach. b) Medical Semantic Annotation using N-Gram, ontology linking and classification algorithms, c) Relation Extraction using graph method and Syntactic Patterns, d), Semantic Enrichment using RDF mining, e) Domain Inference Engine to build the formal ontology. Results: Quantitative evaluations show 84.78% recall, 53.35% precision, and 67.70% F-measure in terms of disease-drug concepts identification; 85.51% recall, 69.61% precision, and F-measure 76.74% with respect to taxonomic relation extraction; and 77.20% recall, 40.10% precision, and F-measure 52.78% with respect to biomedical non-taxonomic relation extraction. Conclusion: We present an automated ontology generation framework that is empowered by Linked Biomedical Ontologies. This framework integrates various natural language processing, semantic enrichment, syntactic pattern, and graph algorithm based techniques. Moreover, it shows that using Linked Biomedical Ontologies enables a promising solution to the problem of automating the process of disease-drug ontology generation.}
}
@article{EITOBRUN2022485,
title = {Knowledge tools to organise software engineering Data: Development and validation of an ontology based on ECSS standard},
journal = {Advances in Space Research},
volume = {70},
number = {2},
pages = {485-495},
year = {2022},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2022.04.052},
url = {https://www.sciencedirect.com/science/article/pii/S0273117722003295},
author = {Ricardo Eito-Brun and Juan {Miguel Gómez-Berbís} and Antonio {de Amescua Seco}},
keywords = {Software Development, Aeropace Software engineering, ECSS Standards, Ontologies and taxonomies, Data exchange, Data aggregation},
abstract = {This paper describes an OWL-based ontology to manage the data and artifacts created as part of software development projects based on the ECSS-E-ST-40C standard for SW development. ECSS standards are the main reference in aerospace projects. ECSS-E-ST-40C establishes the process, activities, requirements, and work products that must be generated. As part of its requirements, ECSS-E-ST-40C implicitly refers to specific data that must be generated, collected, and managed. The ontology identifies these data and the relationships that are inferred from the requirements in the standard, intending to set up the basis for building advanced information systems where data coming from different projects can be integrated and connected into a coherent set. The definition of this ontology provides the conceptual framework to develop data exchange processes and interfaces between the different tools and information systems used by the different players in the aerospace industry.}
}
@article{KONYS20191614,
title = {Knowledge Repository of Ontology Learning Tools from Text},
journal = {Procedia Computer Science},
volume = {159},
pages = {1614-1628},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.332},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919315339},
author = {Agnieszka Konys},
keywords = {Ontology learning tools, Learning techniques, Ontology learninng from text, Knowledge repository},
abstract = {Ontologies are one of the fundamental elements of the Semantic Web, and they have gained a lot of popularity and recognition because they are viewed as the answer to the need for interoperable semantics in modern information systems. The intermingling of techniques in areas such as natural language processing, information retrieval, machine learning, data mining, and knowledge representation provide a lot of possibilities for development of ontology learning approaches. A rise in focus on the ability to cope with the scale of Web data required for ontology learning forces the potential growth of cross-language research, emphasizing the automatic or semi-automatic generation of the tools dedicated to text mining and information extraction. This paper presents the integration of ontology learning tools from text in the knowledge repository to incorporate the applied techniques and outputs of an ontology learning algorithm into the one complex multifunctional solution. The proposed knowledge repository covers various applicability of existing techniques of learning ontologies from text, and offers competency question-based reasoning mechanism for individuals to specify their profiles of ontology learning tools. The validation stage is also provided in the form of applied reasoning.}
}
@article{XU2018118,
title = {A knowledge base with modularized ontologies for eco-labeling: Application for laundry detergents},
journal = {Computers in Industry},
volume = {98},
pages = {118-133},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517306322},
author = {Da Xu and Mohamed Hedi Karray and Bernard Archimède},
keywords = {Ontology engineering, Ontology modularization, Knowledge base, OWL imports, SWRL, Eco-labeling},
abstract = {Along with the rising concern of environmental performance, eco-labeling is becoming more and more popular. However, the complex process of eco-labeling is demotivating manufacturers and service providers to be certificated. The knowledge contained in eco-labeling criteria documents is not semantically exploitable to computers. Traditional knowledge base in relational data model is not inter-operable, lacks inference support and is difficult to be reused. In our research, we propose a comprehensive knowledge base composed of interconnected OWL (Ontology Web Language) ontologies. This ontology based knowledge base allows reasoning and semantic query. In this paper, a modularization scheme about ontology development is introduced and it has been applied to EU Eco-label (European Union Eco-label) laundry detergent product criteria. This scheme separates entity knowledge and rule knowledge so that the ontology modules can be reused easily in other domains. Reasoning and inference based on SWRL (Semantic Web Rule Language) rules in favor of eco-labeling process is also presented.}
}
@article{GARCIADIAZ2020641,
title = {Ontology-driven aspect-based sentiment analysis classification: An infodemiological case study regarding infectious diseases in Latin America},
journal = {Future Generation Computer Systems},
volume = {112},
pages = {641-657},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2030892X},
author = {José Antonio García-Díaz and Mar Cánovas-García and Rafael Valencia-García},
keywords = {Aspect-based sentiment analysis, Infodemiology, Deep learning, Ontologies},
abstract = {Infodemiology is the process of mining unstructured and textual data so as to provide public health officials and policymakers with valuable information regarding public health. The appearance of this new data source, which was previously unimaginable, has opened up a new way in which to improve public health systems, resulting in better communication policies and better detection systems. However, the unstructured nature of the Internet, along with the complexity of the infectious disease domain, prevents the information extracted from being easily understood. Moreover, when dealing with languages other than English, for which some of the most common Natural Language Processing resources are not available, the correct exploitation of this data becomes even more difficult. We intend to fill these gaps proposing an ontology-driven aspect-based sentiment analysis with which to measure the general public’s opinions as regards infectious diseases when expressed in Spanish by employing a case study of tweets concerning the Zika, Dengue and Chikungunya viruses in Latin America. Our proposal is based on two technologies. We first use ontologies in order to model the infectious disease domain with concepts such as risks, symptoms, transmission methods or drugs, among other concepts. We then measure the relationship between these concepts in order to determine the degree to which one concept influences other concepts. This new information is subsequently applied in order to build an aspect-based sentiment analysis model based on statistical and linguistic features. This is done by applying deep-learning models. Our proposal is available on a web platform, where users can see the sentiment for each concept at a glance and analyse how each concept influences the sentiment of the others.}
}
@article{LI2020,
title = {Ontological Organization and Bioinformatic Analysis of Adverse Drug Reactions From Package Inserts: Development and Usability Study},
journal = {Journal of Medical Internet Research},
volume = {22},
number = {7},
year = {2020},
issn = {1438-8871},
doi = {https://doi.org/10.2196/20443},
url = {https://www.sciencedirect.com/science/article/pii/S1438887120005373},
author = {Xiaoying Li and Xin Lin and Huiling Ren and Jinjing Guo},
keywords = {ontology, adverse drug reactions, package inserts, information retrieval, natural language processing, bioinformatics, drug, adverse events, machine-understandable knowledge, clinical applications},
abstract = {Background
Licensed drugs may cause unexpected adverse reactions in patients, resulting in morbidity, risk of mortality, therapy disruptions, and prolonged hospital stays. Officially approved drug package inserts list the adverse reactions identified from randomized controlled clinical trials with high evidence levels and worldwide postmarketing surveillance. Formal representation of the adverse drug reaction (ADR) enclosed in semistructured package inserts will enable deep recognition of side effects and rational drug use, substantially reduce morbidity, and decrease societal costs.
Objective
This paper aims to present an ontological organization of traceable ADR information extracted from licensed package inserts. In addition, it will provide machine-understandable knowledge for bioinformatics analysis, semantic retrieval, and intelligent clinical applications.
Methods
Based on the essential content of package inserts, a generic ADR ontology model is proposed from two dimensions (and nine subdimensions), covering the ADR information and medication instructions. This is followed by a customized natural language processing method programmed with Python to retrieve the relevant information enclosed in package inserts. After the biocuration and identification of retrieved data from the package insert, an ADR ontology is automatically built for further bioinformatic analysis.
Results
We collected 165 package inserts of quinolone drugs from the National Medical Products Administration and other drug databases in China, and built a specialized ADR ontology containing 2879 classes and 15,711 semantic relations. For each quinolone drug, the reported ADR information and medication instructions have been logically represented and formally organized in an ADR ontology. To demonstrate its usage, the source data were further bioinformatically analyzed. For example, the number of drug-ADR triples and major ADRs associated with each active ingredient were recorded. The 10 ADRs most frequently observed among quinolones were identified and categorized based on the 18 categories defined in the proposal. The occurrence frequency, severity, and ADR mitigation method explicitly stated in package inserts were also analyzed, as well as the top 5 specific populations with contraindications for quinolone drugs.
Conclusions
Ontological representation and organization using officially approved information from drug package inserts enables the identification and bioinformatic analysis of adverse reactions caused by a specific drug with regard to predefined ADR ontology classes and semantic relations. The resulting ontology-based ADR knowledge source classifies drug-specific adverse reactions, and supports a better understanding of ADRs and safer prescription of medications.}
}
@article{LAHIRI2025100683,
title = {Benchmarking transformer embedding models for biomedical terminology standardization},
journal = {Machine Learning with Applications},
volume = {21},
pages = {100683},
year = {2025},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2025.100683},
url = {https://www.sciencedirect.com/science/article/pii/S2666827025000660},
author = {Aditya Lahiri and Sangeeta Shukla and Ben Stear and Taha Mohseni Ahooyi and Katherine Beigel and Elizabeth Margolskee and Deanne Taylor},
keywords = {NIH Clinical Trials Registry, WHO Tumor Classification, Text embedding, Clinical text standardization, Large language models},
abstract = {Biomedical text in public databases often exhibits unstandardized terminology and inconsistencies that impede machine learning applications and hinder data integration across biomedical databases. Leveraging generalized and specialized transformer/large language models (LLMs) offers a potential scalable solution for terminology standardization. We evaluated this opportunity using the National Institutes of Health Clinical Trials Registry (CTR), which contains heterogeneous, free-text records of disease from therapeutic trials. To systematically assess the ability of machine learning methods to assign biomedical terms accurately, we benchmarked 36 approaches using transformer/LLM-based text embeddings, along with traditional text-matching algorithms, against a clinical gold standard: the World Health Organization Classification of Tumours system (WHO System, also known as the WHO Blue Books). For this evaluation, we developed CANTOS (Clinical Trials Automated Nomenclature and Tumor Ontology Standardization), a computational benchmarking framework that extracts tumor names from the CTR and standardizes them using the WHO System and the National Cancer Institute Thesaurus (NCIt). We assessed standardization accuracy using a random sample of 1600 CTR tumor names manually annotated with WHO System terms. LLM/transformer-based embedding methods significantly outperformed text-matching approaches: all-MiniLM-L12-v2+Euclidean distance achieved 67.7% accuracy (WHO-5th edition), while LTE-3+Euclidean distance achieved 69.4% (WHO-all editions). Text-matching methods peaked at 32.6% accuracy. A majority voting approach combining three high-accuracy, low-agreement methods improved accuracy to 71.9% (WHO-5th) and 71.6% (WHO-all). Our findings demonstrate the effectiveness of embedding models in standardizing biomedical terminology and provide a reproducible framework for benchmarking machine learning methods against clinical gold standards using real-world datasets.}
}
@article{WANG2023106356,
title = {A knowledge empowered explainable gene ontology fingerprint approach to improve gene functional explication and prediction},
journal = {iScience},
volume = {26},
number = {4},
pages = {106356},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.106356},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223004339},
author = {Ying Wang and Hui Zong and Fan Yang and Yuantao Tong and Yujia Xie and Zeyu Zhang and Honglian Huang and Rongbin Zheng and Shuangkuai Wang and Danqi Huang and Fanglin Tan and Shiyang Cheng and M. James C. Crabbe and Xiaoyan Zhang},
keywords = {Biological sciences tools, Data processing in systems biology},
abstract = {Summary
Functional explication of genes is of great scientific value. However, conventional methods have challenges for those genes that may affect biological processes but are not annotated in public databases. Here, we developed a novel explainable gene ontology fingerprint (XGOF) method to automatically produce knowledge networks on biomedical literature in a given field which quantitatively characterizes the association between genes and ontologies. XGOF provides systematic knowledge for the potential function of genes and ontologically compares similarities and discrepancies in different disease-XGOFs integrating omics data. More importantly, XGOF can not only help to infer major cellular components in a disease microenvironment but also reveal novel gene panels or functions for in-depth experimental research where few explicit connections to diseases have previously been described in the literature. The reliability of XGOF is validated in four application scenarios, indicating a unique perspective of integrating text and data mining, with the potential to accelerate scientific discovery.}
}
@article{QIN2025107883,
title = {Intelligent Chinese patent medicine (CPM) recommendation framework: Integrating large language models, retrieval-augmented generation, and the largest CPM dataset},
journal = {Pharmacological Research},
volume = {219},
pages = {107883},
year = {2025},
issn = {1043-6618},
doi = {https://doi.org/10.1016/j.phrs.2025.107883},
url = {https://www.sciencedirect.com/science/article/pii/S1043661825003081},
author = {Suyang Qin and Yifan Wang and Tangming Cui and Jinge Ma and Xin Zhou and Xinglin Guo and Chuchu Zhang and Chongyun Zhou and Rongjuan Guo and Haiyan Li},
keywords = {Large language models (LLMs), Multi-LLMs Validation, Chinese patent medicine (CPM) dataset, retrieval-augmented generation, intelligent CPM recommendation framework, guidelines evaluation},
abstract = {Chinese patent medicines (CPMs), a vital component of healthcare systems in China and worldwide, has been increasingly utilized in clinical practice. However, approximately 70 % of CPMs are prescribed by Western medicine physicians who lack expertise in traditional Chinese medicine syndrome differentiation and treatment. Here, in this study, we constructed RAG-CPMF, an intelligent CPM recommendation framework integrating large language models (LLMs), retrieval-augmented generation (RAG), and the largest CPM dataset. Specifically, we first applied the Multi-LLMs Validation method, significantly reducing the manual proofreading workload involved in structured data extraction. Furthermore, based on this approach, we successfully established the largest CPM dataset (https://gitee.com/tcmdoc/cpm) with continuous on-line updates. Afterwards, we combined the CPM dataset and the RAG architecture, along with effective integration of generative capacity from LLMs and external data retrieval; all of which contributed to the RAG-CPMF. Finally, the accuracy of RAG-CPMF was evaluated against clinical guidelines, demonstrating that this framework significantly improved CPM recommendation accuracy compared with general-purpose LLMs.}
}
@article{WEN2025131116,
title = {AcuGPT-Agent: An LLM-powered intelligent system for acupuncture-based infertility treatment},
journal = {Neurocomputing},
volume = {652},
pages = {131116},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131116},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225017886},
author = {Jing Wen and Diandong Liu and Yuxin Xie and Yi Ren and Jiacun Wang and Youbing Xia and Peng Zhu},
keywords = {AcuGPT, AcuGPT-Agent, Large language model, Acupuncture, Artificial intelligence},
abstract = {This paper introduces AcuGPT-Agent, a novel intelligent system powered by a domain-specific large language model (LLM) designed for acupuncture-based infertility treatment. Traditional Chinese Medicine (TCM), particularly acupuncture, presents unique challenges for general-purpose LLM due to its complex theoretical framework and specialized terminology. To bridge this gap, we make three key contributions. First, we develop AcuGPT, the first LLM specifically fine-tuned for the acupuncture domain, utilizing advanced techniques such as LoRA, DoRA, and a Pretrain + Supervised Fine-Tuning (SFT) pipeline. These methods significantly improve the ability of the model to understand and generate medical content related to TCM. Second, we propose AcuGPT-Agent, a modular and extensible system that integrates AcuGPT with standard agent tools, a domain-specific acupuncture knowledge graph for infertility treatment, and a novel LLM-driven Multi-Department Knowledge Base Routing Management Mechanism (MDKBRMM). This mechanism enhances retrieval-augmented generation by effectively managing semantic complexity and routing queries across specialized knowledge bases in medical contexts. Third, we build EvalAcu, a dedicated benchmark dataset for evaluating large language models and intelligent agents within the acupuncture domain. Experimental results demonstrate that AcuGPT exhibits strong domain knowledge and reasoning capabilities, while AcuGPT-Agent further enhances performance in real-world infertility treatment tasks. All code, datasets, and resources are publicly available at: https://github.com/suda7788/AcuGPT-Agent.}
}
@article{ADAMS2023107541,
title = {Surgical procedure prediction using medical ontological information},
journal = {Computer Methods and Programs in Biomedicine},
volume = {235},
pages = {107541},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107541},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723002067},
author = {T. Adams and M. O’Sullivan and C. Walker},
keywords = {Surgical duration prediction, Medical ontology, Machine learning},
abstract = {Background and Objective: Predicting the duration of surgical procedures is an important step in scheduling operating rooms. Many factors have been shown to influence the duration of a procedure, in this research we aim to use medical ontological information to improve the predictions. Methods: This paper presents two methods for incorporating the medical information about a surgical procedure into the prediction of the duration of the procedure. The first method uses the Systematised Nomenclature of Medicine Clinical Terms to relate different procedures to each other. The second uses simple text fragments. The relationships between types of procedures are included in a regression model for the procedure duration. These methods are applied to data from New Zealand healthcare facilities and the accuracy of the estimations of the durations is compared. In addition a simulation of scheduling the procedures in an operating room is performed. Results: It is shown that both of the methods provide an improvement in the prediction of procedure durations. When compared to a traditional categorical encoding, the ontological information provides an improvement in the continuous ranked probability scores of the prediction of procedure durations from 18.4 min to 17.1 min, and from 25.3 to 21.3 min for types of procedures that are not performed very often. Conclusions: Different methods for encoding medical ontological information in surgery procedure duration predictions are presented, and show an improvement over traditional models. The improvement in duration prediction is shown to improve the efficiency of scheduling in a simple simulation.}
}
@article{ZEB202036,
title = {An ontology of condition assessment technologies for sewer networks},
journal = {Infrastructure Asset Management},
volume = {7},
number = {1},
pages = {36-45},
year = {2020},
issn = {2053-0250},
doi = {https://doi.org/10.1680/jinam.18.00034},
url = {https://www.sciencedirect.com/science/article/pii/S2053025019000113},
author = {Jehan Zeb},
keywords = {Building Information Modelling (BIM), knowledge management, maintenance & inspection},
abstract = {Ageing infrastructure, budgetary constraints, economic downturns, regulatory requirements, the high level of service requirements and a declining rate of reinvestment require effective asset management of sewer collection systems. Inspection and condition assessment is one of the core and costly elements of effective asset management. The availability of many sophisticated condition assessment technologies and lack of common understanding of them in the municipal environment require formalising the knowledge related to these technologies using an ontological approach to support the development of applications for effective selection of the most appropriate technology for sewer condition assessment. The purpose of this research work is to develop an ontology of condition assessment technologies for sewer networks Cats_Onto using the Ontology Web Language following a seven-step methodology. The outcome of this research work is a comprehensive knowledge representation in the form of an ontology that will be used to develop applications (beyond the scope of this paper) for the selection of the most appropriate technology for sewer condition assessment. The knowledge representation is verified using automated built-in reasoners in the Protg ontology editor and validated by industry experts.}
}
@article{YAN2025101275,
title = {Adaptive multi-view learning method for enhanced drug repurposing using chemical-induced transcriptional profiles, knowledge graphs, and large language models},
journal = {Journal of Pharmaceutical Analysis},
volume = {15},
number = {6},
pages = {101275},
year = {2025},
issn = {2095-1779},
doi = {https://doi.org/10.1016/j.jpha.2025.101275},
url = {https://www.sciencedirect.com/science/article/pii/S2095177925000929},
author = {Yudong Yan and Yinqi Yang and Zhuohao Tong and Yu Wang and Fan Yang and Zupeng Pan and Chuan Liu and Mingze Bai and Yongfang Xie and Yuefei Li and Kunxian Shu and Yinghong Li},
keywords = {Drug repurposing, Multi-view learning, Chemical-induced transcriptional profile, Knowledge graph, Large language model, Heterogeneous network},
abstract = {Drug repurposing offers a promising alternative to traditional drug development and significantly reduces costs and timelines by identifying new therapeutic uses for existing drugs. However, the current approaches often rely on limited data sources and simplistic hypotheses, which restrict their ability to capture the multi-faceted nature of biological systems. This study introduces adaptive multi-view learning (AMVL), a novel methodology that integrates chemical-induced transcriptional profiles (CTPs), knowledge graph (KG) embeddings, and large language model (LLM) representations, to enhance drug repurposing predictions. AMVL incorporates an innovative similarity matrix expansion strategy and leverages multi-view learning (MVL), matrix factorization, and ensemble optimization techniques to integrate heterogeneous multi-source data. Comprehensive evaluations on benchmark datasets (Fdataset, Cdataset, and Ydataset) and the large-scale iDrug dataset demonstrate that AMVL outperforms state-of-the-art (SOTA) methods, achieving superior accuracy in predicting drug-disease associations across multiple metrics. Literature-based validation further confirmed the model's predictive capabilities, with seven out of the top ten predictions corroborated by post-2011 evidence. To promote transparency and reproducibility, all data and codes used in this study were open-sourced, providing resources for processing CTPs, KG, and LLM-based similarity calculations, along with the complete AMVL algorithm and benchmarking procedures. By unifying diverse data modalities, AMVL offers a robust and scalable solution for accelerating drug discovery, fostering advancements in translational medicine and integrating multi-omics data. We aim to inspire further innovations in multi-source data integration and support the development of more precise and efficient strategies for advancing drug discovery and translational medicine.}
}
@article{SHI2025104330,
title = {Enhancing retrieval-augmented generation for interoperable industrial knowledge representation and inference toward cognitive digital twins},
journal = {Computers in Industry},
volume = {171},
pages = {104330},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104330},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525000958},
author = {Dachuan Shi and Jianzhang Li and Olga Meyer and Thomas Bauernhansl},
keywords = {Large language model, Asset administration shell, Retrieval-augmented generation, Digital twin, Entity matching},
abstract = {The escalating volume and complexity of digital data within the manufacturing sector highlight an urgent need for an efficient knowledge representation and inference solution. Traditional approaches, which often rely on ontologies, knowledge graphs, or digital twins (DTs) for knowledge representation, and rule-based algorithms for inference, are becoming insufficient. The emergence of generative AI, particularly large language models (LLM) and retrieval-augmented generation (RAG), offers a more efficient and intelligent alternative. However, the performance of an RAG system is heavily dependent on the quality of retrieval results, which can be compromised by domain-specific knowledge and retrieval distractors. To address this challenge, we propose to enhance RAG systems tailored for the manufacturing industry in two aspects. First, we utilize the Asset Administration Shell (AAS), which represents the German industrial perspective on cognitive DTs, to create a representation of assets and knowledge in standardized information models. This establishes a robust foundation for the retrieval sources. Second, we propose a contrastive selection loss (CSL) to fine-tune an open-source LLM to refine the retrieval results. Fine-tuned LLMs possess higher efficiency and accuracy on task- and domain-specific datasets, while the CSL further enhances the model's ability to distinguish true positives from similar distractors. The enhanced RAG system is demonstrated in a robotic work cell integration use case and evaluated through a novel evaluation protocol. Additionally, the retrieval effectiveness of the RAG system, specifically the LLM fine-tuned with CSL, is extensively validated through statistical experiments. The results confirm its superior performance over state-of-the-art methods, including GPT-4 with in-context learning prompts and other fine-tuned models.}
}
@article{TANG2020919,
title = {An ontology-improved vector space model for semantic retrieval},
journal = {The Electronic Library},
volume = {38},
number = {56},
pages = {919-942},
year = {2020},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-04-2020-0081},
url = {https://www.sciencedirect.com/science/article/pii/S0264047320000454},
author = {Mingwei Tang and Jiangping Chen and Haihua Chen and Zhenyuan Xu and Yueyao Wang and Mengting Xie and Jiangwei Lin},
keywords = {Ontology, Semantic retrieval, Vector space model},
abstract = {Purpose
The purpose of this paper is to provide an integrated semantic information retrieval (IR) solution based on an ontology-improved vector space model for situations where a digital collection is established or curated. It aims to create a retrieval approach which could return the results by meanings rather than by keywords.
Design/methodology/approach
In this paper, the authors propose a semantic term frequency algorithm to create a semantic vector space model (SeVSM) based on ontology. To support the calculation, a multi-branches tree model is created to represent the ontology and a set of algorithms is developed to operate it. Then, a semantic ontology-based IR system based on the SeVSM model is designed and developed to verify the effectiveness of the proposed model.
Findings
The experimental study using 30 queries from 15 different domains confirms the effectiveness of the SeVSM and the usability of the proposed system. The results demonstrate that the proposed model and system can be a significant exploration to enhance IR in specific domains, such as a digital library and e-commerce.
Originality/value
This research not only creates a semantic retrieval model, but also provides the application approach via designing and developing a semantic retrieval system based on the model. Comparing with most of the current related research, the proposed research studies the whole process of realizing a semantic retrieval.}
}
@article{MAO202176,
title = {Event prediction based on evolutionary event ontology knowledge},
journal = {Future Generation Computer Systems},
volume = {115},
pages = {76-89},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20311778},
author = {Qianren Mao and Xi Li and Hao Peng and Jianxin Li and Dongxiao He and Shu Guo and Min He and Lihong Wang},
keywords = {Event prediction, Evolutionary event, Event knowledge, Event ontology knowledge, Event knowledge graph},
abstract = {The evolution and development of breaking news events usually present regular patterns, leading to the happening of sequential events. Therefore, the analysis of such evolutionary patterns among events and prediction to breaking news events from free text is a valuable capability for decision support systems. Traditional systems tend to focus on contents distribution information but ignore the inherent regularity of evolutionary events. We introduce evolutionary event ontology knowledge (EEOK) structuring the evolutionary patterns in five different event domains, namely Explosion, Conflagration, Geological Hazard, Traffic Accident, Personal Injury. Based on EEOK which provides a representing general-purpose ontology knowledge, we also explore a framework with a pipeline semantic analysis procedure of event extraction, evolutionary event recognition, and event prediction. Since the evolutionary event under each event domain has different evolution patterns, our proposed event prediction model combines the event types to capture the inherent regulation of evolutionary events. Comparative analyses are presented to show the effectiveness of the proposed prediction model compared to other alternative methods.}
}
@article{ZARIBAFZADEH20251306,
title = {Development of a natural language processing algorithm to extract social determinants of health from clinician notes},
journal = {American Journal of Transplantation},
volume = {25},
number = {6},
pages = {1306-1318},
year = {2025},
issn = {1600-6135},
doi = {https://doi.org/10.1016/j.ajt.2025.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S1600613525001029},
author = {Hamed Zaribafzadeh and Jacqueline B. Henson and Norine W. Chan and Ursula Rogers and Wendy Webster and Tyler Schappe and Fan Li and Roland A. Matsouaka and Allan D. Kirk and Ricardo Henao and Lisa M. McElroy},
keywords = {social determinants of health, natural language processing},
abstract = {Disparities in access to the organ transplant waitlist are well-documented, but research into modifiable factors has been limited due to a lack of access to organized prewaitlisting data. This study aimed to develop a natural language processing (NLP) algorithm to extract social determinants of health (SDOH) from free-text notes and quantify the association of SDOH with access to the transplant waitlist. We collected 261 802 clinician notes from 11 111 adults referred for kidney or liver transplants between 2016 and 2022 at the Duke University Health System. An SDOH ontology and a rule-based NLP algorithm were created to extract and organize terms. Education, transportation, and age were the most frequent terms identified. Negative sentiment and refer were the most negatively associated features with listing in both kidney and liver transplant patients. Income and employment for the kidney, and judgment and positive sentiment for liver were the most positively associated features with the listing. This study suggests that the integration of NLP tools into the transplant clinical workflow could help improve collection and organization of SDOH and inform center-level efforts at resource allocation, potentially improving access to the transplant waitlist and posttransplant outcomes.}
}
@article{FORTIN2022234,
title = {Entanglement and indistinguishability in a quantum ontology of properties},
journal = {Studies in History and Philosophy of Science},
volume = {91},
pages = {234-243},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2021.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0039368121002004},
author = {Sebastian Fortin and Olimpia Lombardi},
keywords = {Indistinguishability, Entanglement, Algebraic formalism, Ontology of properties},
abstract = {The aim of the present article is to address the problem of entanglement in the case of indistinguishable particles from a perspective based on the algebraic formalism of quantum mechanics, which is the natural formal counterpart of an ontology of properties, devoid of the ontological category of object. On the basis of this perspective, an algebraic definition of entanglement is adopted, which supplies a unified conception, valid for both the distinguishable and the indistinguishable cases. An additional advantage of this algebraic definition is that it does justice to the relativity of entanglement, a feature that cannot be ignored.}
}
@article{CHIANG2024110918,
title = {Customized GPT model largely increases surgery decision accuracy for pharmaco-resistant epilepsy},
journal = {Journal of Clinical Neuroscience},
volume = {130},
pages = {110918},
year = {2024},
issn = {0967-5868},
doi = {https://doi.org/10.1016/j.jocn.2024.110918},
url = {https://www.sciencedirect.com/science/article/pii/S0967586824004570},
author = {Kuo-Liang Chiang and Yu-Cheng Chou and Hsin Tung and Chin-Yin Huang and Liang-Po Hsieh and Kai-Ping Chang and Shang-Yeong Kwan and Wan-Yu Huang},
keywords = {Semiology, Seizure descriptors, Localization, Generative pre-trained transformer, Large-scale language model},
abstract = {Background
To develop an enhanced epilepsy diagnosis system by integrating an expert-informed ontology with a custom generative pre-trained transformer (GPT), validated by inferring possible seizure lateralization and localization using retrospective textual data from the pre-surgical assessments of patients with pharmaco-resistant epilepsy (PRE).
Methods
We developed an AI system for epilepsy diagnosis using Protégé with OWL/SWRL, integrating a knowledge base with seizure semiology, seizure types EEG descriptors, expert insights, and literature to pinpoint seizure locations. A customized GPT model was then tailored for specific diagnostic needs. Validated through 16 surgical cases, the system’s accuracy in seizure localization and the JSON (JavaScript Object Notation) Epilepsy Matcher’s term matching capabilities were confirmed against a Protégé-based knowledge base.
Results
A total of 117 patients with PRE underwent video-EEG monitoring at a single institution. However, only 16 of these patients received epilepsy surgery. The Protégé system achieved 75 % accuracy in diagnosing epilepsy from 16 cases using semiology, which increased to 87.5 % with EEG data. The Json Epilepsy Matcher further improved accuracy to 87.5 % with symptoms alone and 93.8 % when including EEG data, highlighting the benefits of applying GPT techniques.
Conclusions
This study highlights the efficacy of the JSON Epilepsy Matcher in improving seizure diagnosis accuracy. When combined with EEG data, it achieves a 93.8 % accuracy rate, suggesting a potential improvement in the practicality and generalizability of the original ontology expert system, boosting physicians’ confidence in confirming surgery and potentially sparing many children from prolonged suffering. This innovative approach not only improves diagnostic accuracy but also sets a precedent for future applications of AI in neurology.}
}
@article{YANG2025113503,
title = {A comprehensive survey on integrating large language models with knowledge-based methods},
journal = {Knowledge-Based Systems},
volume = {318},
pages = {113503},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113503},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125005490},
author = {Wenli Yang and Lilian Some and Michael Bain and Byeong Kang},
keywords = {LLMs, Knowledge-based, Knowledge integration, RAG, KG},
abstract = {The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.}
}
@article{TAO2025114011,
title = {Fine-grained Stateful Knowledge Exploration: Effective and efficient graph retrieval with Large Language Models},
journal = {Knowledge-Based Systems},
volume = {326},
pages = {114011},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114011},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125010561},
author = {Dehao Tao and Congqi Wang and Feng Huang and Junhao Chen and Yongfeng Huang and Minghu Jiang},
keywords = {Knowledge base question answering, Large Language Model, Knowledge graph, Fine-grained stateful knowledge exploration},
abstract = {Large Language Models (LLMs) have shown impressive capabilities, yet updating their knowledge remains a significant challenge, often leading to outdated or inaccurate responses. A proposed solution is the integration of external knowledge bases, such as knowledge graphs, with LLMs. Most existing methods use a paradigm that treats the whole question as the objective, with relevant knowledge being incrementally retrieved from the knowledge graph. However, this paradigm often leads to a granularity mismatch between the target question and the retrieved entities and relations. As a result, the information in the question cannot precisely correspond to the retrieved knowledge. This may cause redundant exploration or omission of vital knowledge, thereby leading to enhanced computational consumption and reduced retrieval accuracy. To address the limitations of coarse-grained knowledge exploration, we propose FiSKE, a novel paradigm for Fine-grained Stateful Knowledge Exploration. FiSKE first decomposes questions into fine-grained clues, then employs an adaptive mapping strategy during knowledge exploration process to resolve ambiguity in clue-to-graph mappings. This strategy dynamically infers contextual correspondences while maintaining a stateful record of the mappings. A clue-driven termination mechanism ensures rigorous augmentation—leveraging fully mapped paths for LLMs while reverting to chain-of-thought reasoning when necessary. Our approach balances precision and efficiency. Experiments on multiple datasets revealed that our paradigm surpasses current advanced methods in knowledge retrieval while significantly reducing the average number of LLM invocations. The code for this paper can be found at https://github.com/nnnoidea/stateful-KGQA.}
}
@article{GHORBANI2022108658,
title = {Type-2 fuzzy ontology-based semantic knowledge for indoor air quality assessment},
journal = {Applied Soft Computing},
volume = {121},
pages = {108658},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.108658},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622001338},
author = {Abolfazl Ghorbani and Kamran Zamanifar},
keywords = {Type-2 fuzzy ontology, Semantic interoperability, Semantic reasoning, Internet of things, Air pollution},
abstract = {Semantic web technology plays an increasing role in performing smart home applied programs and it has led to improve semantic interoperability among different systems. However, classical ontologies fail to illustrate ambiguous, incomplete, and uncertain knowledge often available in the real world. On the other hand, the air quality assessment carried out to determine “the degree of pollution” lacks accurately specified boundaries; therefore, the conventional approach based on classic ontology cannot extract real-valued memberships and consequently fails to support ambiguous, incomplete, and uncertain knowledge. Integrating semantic web of things technology (SWOT) and type-2 fuzzy logic improves the capability of semantic reasoning to retrieve query information. Annotation of sensor-generated data and the ability to infer and represent knowledge based on type-2 fuzzy logic are extremely essential when the available data are ambiguous and uncertain. Hence, in this paper, we have provided a framework to build an IoT-based home air quality assessment system by using type-2 fuzzy ontology so that smart home systems can make a decision and control appropriately based on predefined rules by employing the provided semantic reasoning.}
}
@article{YAN2024112684,
title = {Collaborate SLM and LLM with latent answers for event detection},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112684},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112684},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013182},
author = {Youcheng Yan and Jinshuo Liu and Donghong Ji and Jinguang Gu and Ahmed Abubakar Aliyu and Xinyan Wang and Jeff Z. Pan},
keywords = {Large language model, Small language model, Event detection, Latent answers},
abstract = {Event detection (ED) intends to identify events from text and classify them into predefined event types. One of the major issues for ED is the low-resource problem due to inadequate samples. Some studies address the low-resource issue with retrieving knowledge entries directly from knowledge bases while introducing a lot of irrelevant knowledge or failing the lookup. Moreover, recent work has attempted to employ large language models (LLMs, e.g., ChatGPT) that directly access event types in unstructured text under low-resource scenarios. Although LLM-based approaches have obtained promising results, we consider that the full potential of LLMs has not been activated due to insufficient prompt information. Our research proposes a two-stage event detection method that collaborates small language models (SLMs) and LLMs, namely LSLAED. Specifically, we first fine-tune the SLM to generate three types of latent answers: answer-aware examples, structure-aware examples, and corresponding answer candidates. Subsequently, all latent answers will form the prompt and enable the LLM to improve performance through in-context learning. We evaluate the proposed method using precision, recall, and F1-score as evaluation metrics. Experiments on the ACE2005 and ERE-EN datasets have demonstrated that LSLAED achieves significant improvement in both full-shot and few-shot scenarios.}
}
@incollection{OKIBE20233363,
title = {Ontology Modelling for Valorisation of Sugarcane Bagasse},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {3363-3368},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50536-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740505369},
author = {Maureen Chiebonam Okibe and Michael Short and Franjo Cecelja and Madeleine Bussemaker},
keywords = {Sugarcane Bagasse, Biorefining, Knowledge Modelling, Ontology},
abstract = {Sugarcane bagasse (SCB) is an agro-industrial residue extracted during sugar processing. Sugar mills use basic process technologies for bagasse valorisation. Residues are often disposed improperly or burned inefficiently by thermal boilers causing environmental pollution. Biorefining bagasse as substrate for bio-based chemical production is superior to biomass disposal by incineration, burning and landfill. The conversion of bagasse for value-added applications may be economical with less environmental impact on humans and the ecosystem. Computer aided tools such as ontologies for knowledge modelling represent available information for bagasse feedstocks. This paper presents a reference model referred to as the SCB Ontology which is limited to a framework for the modelling of knowledge on the current utilisation of SCB feedstocks from principal sugarcane-cultivating countries. The SCB Ontology identifies opportunities for efficient bagasse valorisation by principal sugarcane producers. Potential application of the SCB Ontology for bagasse valorisation which is a circular bioeconomy initiative was also discussed.}
}
@article{WAGNER2022103927,
title = {Building product ontology: Core ontology for Linked Building Product Data},
journal = {Automation in Construction},
volume = {133},
pages = {103927},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103927},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003782},
author = {Anna Wagner and Wendelin Sprenger and Christoph Maurer and Tilmann E. Kuhn and Uwe Rüppel},
keywords = {Product Data, Linked Data, Semantic Web technologies, Construction industry, Linked Product Data},
abstract = {The digitalisation of the Architecture, Engineering and Construction domain introduced new methods for digital collaboration, i.e. Building Information Modelling (BIM). While this method focuses on building data, the distribution of digital product models is still problematic, complicating uniform product searches and automated product data processing. Existing schemas, such as a subpart of the Industry Foundation Classes or the German VDI 3805, rely on rigid or template-driven schemas, that do not support the description of innovative or multi-functional products or impose a large schema overhead and complexity on manufacturers. Therefore, this article combines flexible and modular product descriptions with Semantic Web technologies and Linked Data. By applying Web-based technologies, the searchability of product data and the applicability of distributed data are expected to be enhanced. More precisely, this article proposes a concept for Linked Building Product Data and introduces the generic Building Product Ontology as a potential core schema of the concept. To demonstrate the feasibility of Linked Building Product Data and the Building Product Ontology, the authors apply both the concept and the data schema to innovative and multi-functional example products that cannot be described with the existing approaches for product descriptions. The evaluation demonstrates the flexibility, modularity and overall suitability of the presented concepts, meeting all collected requirements for digital product descriptions. Hence, Linked Building Product Data may solve existing issues with rigid product description schemas. At the same time, this approach complements the current research trend of Linked Building Data.}
}
@article{PAN2025103422,
title = {A context-aware KG-LLM collaborated conceptual design approach for personalized products: A case in lower limbs rehabilitation assistive devices},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103422},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103422},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003155},
author = {Xinyu Pan and Weibin Zhuang and Sijie Wen and Weigang Yu and Jinsong Bao and Xinyu Li},
keywords = {Conceptual design, Knowledge graph, Large language model, Lower limb rehabilitation assistive devices, User requirement mining},
abstract = {With the rapid increase in demand for personalized Rehabilitation Assistive Devices (RADs), significant challenges have emerged in their design processes. Particularly in practical applications, designers face challenges such as ambiguity in user requirements, inefficiencies in cross-domain knowledge sharing, and deviations of generated solutions from actual user needs. To address these issues, this paper proposes a Context-Aware Conceptual design method based on Knowledge graph (KG) and Large language models (LLM), named CACKL. Firstly, to address the high complexity involved in eliciting user requirements, user profiles are constructed by integrating multi-source data, and fine-grained “requirement-function” mappings are extracted using fine-tuned LLM, thereby reducing the cost associated with manual intervention. Secondly, a KG-LLM collaborated reasoning mechanism guided by a Chain-of-Thought (CoT) prompting approach is proposed to align structured domain knowledge with implicit semantic representations from LLM, thus enhancing the contextual relevance and practical effectiveness of concept generation, aiming to improve the efficiency of personalized conceptual design. In a practical case involving lower-limb RADs, the proposed CACKL method was evaluated regarding user requirement mining and conceptual design. Experimental results demonstrated significant advantages in the automatic generation of personalized design solutions, particularly in enhancing design efficiency and meeting user requirements, thereby validating its effectiveness in real-world applications. This study provides an innovative paradigm for the intelligent design of RADs by integrating dynamic knowledge constraints with natural language interaction.}
}
@article{AZIZ201987,
title = {An ontology-based methodology for hazard identification and causation analysis},
journal = {Process Safety and Environmental Protection},
volume = {123},
pages = {87-98},
year = {2019},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2018.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S095758201831365X},
author = {Abdul Aziz and Salim Ahmed and Faisal I. Khan},
keywords = {Hazard identification, Probabilistic ontology, Web ontology language, Multi-entity Bayesian network, Expert system},
abstract = {This article presents a dynamic hazard identification methodology founded on an ontology-based knowledge modeling framework coupled with probabilistic assessment. The objective is to develop an efficient and effective knowledge-based tool for process industries to screen hazards and conduct rapid risk estimation. The proposed generic model can translate an undesired process event (state of the process) into a graphical model, demonstrating potential pathways to the process event, linking causation to the transition of states. The Semantic web-based Web Ontology Language (OWL) is used to capture knowledge about unwanted process events. The resulting knowledge model is then transformed into Probabilistic-OWL (PR-OWL) based Multi-Entity Bayesian Network (MEBN). Upon queries, the MEBNs produce Situation Specific Bayesian Networks (SSBN) to identify hazards and their pathways along with probabilities. Two open-source software programs, Protégé and UnBBayes, are used. The developed model is validated against 45 industrial accidental events extracted from the U.S. Chemical Safety Board's (CSB) database. The model is further extended to conduct causality analysis.}
}
@article{YU2022347,
title = {Disassembly task planning for end-of-life automotive traction batteries based on ontology and partial destructive rules},
journal = {Journal of Manufacturing Systems},
volume = {62},
pages = {347-366},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521002557},
author = {Jianping Yu and Hua Zhang and Zhigang Jiang and Wei Yan and Yan Wang and Qi Zhou},
keywords = {Automotive traction battery, Disassembly task planning, Partial destructive disassembly rules, Ontology model},
abstract = {Disassembly is an essential step for the cascade utilization of end-of-life automobile power batteries. The diversity in types and structures of end-of-life automobile power batteries has led to much waste in time and cost in manual disassembly processes. With the incoming large-scale retirement of automobile power batteries, it is urgent to use artificial intelligence technology to enable the automation of battery disassembly planning. In order to establish a complete and open product information model to realize the automatic disassembly task planning of end-of-life automobile power battery, a disassembly task planning method of automobile power batteries is proposed based on ontology and partial destructive rules. This presented approach is in combination with case-based reasoning/rule-based reasoning, which is utilized as the mechanism for disassembly knowledge reuse and reasoning. Firstly, a disassembly information ontology of automobile power batteries is constructed to describe the components information and assembly relation. Then, a set of partial destructive rules are formulated to guide the dismount of parts with destructive connection. Thirdly, a disassembly sequence generation method is presented to infer feasible planning schemes from the rule base. Finally, the effectiveness of the method is tested with a case study of a power lithium-ion battery pack. The case study has indicated that this presented method can generate the disassembly task schemes quickly and effectively, when applied to the disassembly of large-scale heterogeneous automobile power batteries.}
}
@article{LAURAMARIA2022309,
title = {An ontology and neural networks based platform for improving impact policy assessment},
journal = {Procedia Computer Science},
volume = {207},
pages = {309-318},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.064},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922009371},
author = {Cornei Laura-Maria and Alboaie Lenuța},
keywords = {Open Data, Governmental Data Management, Ontological Modelling, Neural Networks},
abstract = {Research and development along with technology transfer play an essential role in the long-lasting development of any country. However, governmental institutions may struggle nowadays to adopt convenient innovation policies due to the exponential growth of data as well as the variety of information formats. A relevant example is Romania, a European country, which faces a challenge in processing the huge amount of unstructured, unlinked and heterogeneous data related to the above-mentioned fields. Using data from official sources, we aim to build a system that can enable experts in national and international policies to identify, analyze and visualize dependencies between the gross domestic expenditure on the research and development sector and the results in research, innovation and technology transfer fields. In order to achieve this, we propose an ontology-based platform that can allow policy-makers to ask queries, explore datasets, view statistics and study predictions using neural networks related to the evolution of certain research and technology transfer indicators.}
}
@article{VENKATASUBRAMANIAN2025108895,
title = {Quo Vadis ChatGPT? From large language models to Large Knowledge Models},
journal = {Computers & Chemical Engineering},
volume = {192},
pages = {108895},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108895},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424003132},
author = {Venkat Venkatasubramanian and Arijit Chakraborty},
keywords = {Artificial intelligence, Large language model, Knowledge graph, Domain-specific language processing, Machine learning, Hybrid AI},
abstract = {The startling success of ChatGPT and other large language models (LLMs) using transformer-based generative neural network architecture in applications such as natural language processing and image synthesis has many researchers excited about potential opportunities in process systems engineering (PSE). The almost human-like performance of LLMs in these areas is indeed very impressive, surprising, and a major breakthrough. Their capabilities are very useful in certain tasks, such as writing first drafts of documents, code writing assistance, text summarization, etc. However, their success is limited in highly scientific domains as they cannot yet reason, plan, or explain due to their lack of in-depth mechanistic domain knowledge. This is a problem in domains such as chemical engineering as they are governed by fundamental laws of physics and chemistry (and biology), constitutive relations, and highly technical knowledge about materials, processes, and systems. Although purely data-driven machine learning has its immediate uses, the long-term success of AI in scientific and engineering domains would depend on developing hybrid AI systems that combine first principles and technical knowledge effectively. We call these hybrid AI systems Large Knowledge Models (LKMs), as they will not be limited to only NLP-based techniques or NLP-like applications. In this paper, we discuss the challenges and opportunities in developing such systems in chemical engineering.}
}
@article{GAYATHRI2020511,
title = {Ontology based Concept Extraction and Classification of Ayurvedic Documents},
journal = {Procedia Computer Science},
volume = {172},
pages = {511-516},
year = {2020},
note = {9th World Engineering Education Forum (WEEF 2019) Proceedings : Disruptive Engineering Education for Sustainable Development},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.05.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920313909},
author = {M. Gayathri and R. Jagadeesh Kannan},
keywords = {Ayurveda medicine, text mining, domain ontology, natural language processing, semantic web},
abstract = {India is rich with its culture and heritage. It is known for its traditional medicinal system and it is mentioned even in the ancient Vedas and other scriptures also. In India, Traditional Medical system includes Ayurveda, yoga, siddha, Unani, and homeopathy. Biomedical Text Mining (BioTM) is aiming at the extraction of novel, non-trivial information from the large amounts of biomedical related documents. This unstructured bio medical document holds greater knowledge about medical diagnostics, treatment, and prevention. Ontology plays a vital role in deep understanding of information. It is the building block of the Semantic Web and considers it’s important on the semantic clarity of concepts and entities. The main objective is to search the most relevant content from this huge set of text by understanding the meaning of conceptual terms. We proposed Ontology based Concept Extraction and Classification in which the domain ontology and semantic document description was used to improve classification accuracy. The results show that the classification accuracy of proposed algorithm outperforms than the other existing supervised machine learning algorithm. To further prove the efficiency of the model, experiments were conducted by giving different queries and the results are compared with other existing methods. The results show that the content retrieved by the proposed model was most relevant when compared with existing system.}
}
@article{CHEN2020580,
title = {Modeling and reasoning of IoT architecture in semantic ontology dimension},
journal = {Computer Communications},
volume = {153},
pages = {580-594},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419318912},
author = {Guang Chen and Tonghai Jiang and Meng Wang and Xinyu Tang and Wenfei Ji},
keywords = {IoT, Architecture, Ontology, Modeling, Reasoning},
abstract = {The architecture for IoT is the primary foundation for designing and implementing the System of Internet of things. This paper discusses the theory, method, tools and practice of modeling and reasoning the architecture of the Internet of Things system from the dimension of semantic ontology. This paper breaks the way of static ontology modeling, and proposes an implementation framework for real-time and dynamic ontology modeling of the IoT system from the running system. According to the actual needs of the health cabin IoT system and the combination of theory and practice, the system architecture model of the semantic ontology dimension of IoT is built. Then, based on the reasoning rules of the ontology model, the model is reasoned by Pellet reasoning engine which injects the atom of the custom reasoning built-ins into the source code. In this way we have realized the automatic classification and attribute improvement of resources and behaviors of the IoT system, the real-time working state detection and fault diagnosis of the IoT system, and the automatic control of the IoT system and resources.}
}
@article{PEREIRA20239831,
title = {Interoperability Middleware for IIoT Gateways based on Standard Ontologies and AAS},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {9831-9836},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.403},
url = {https://www.sciencedirect.com/science/article/pii/S240589632300770X},
author = {Pedro Henrique Morgan Pereira and Gustavo Cainelli and Carlos Eduardo Pereira and Edison Pignaton {de Freitas}},
keywords = {Interoperability, Industry 4.0, IIoT, Middleware, Gateway, Asset Administration Shell, Information Integration, Industrial Ontology},
abstract = {Recent advancements in microelectronics, information technology, and communication protocols have led to smaller devices with greater processing capabilities and lower energy consumption. These developments have contributed to the growing number of physical devices used in industrial environments that are interconnected and communicate through the internet, allowing for the realization of Industry 4.0 and the Industrial Internet of Things (IIoT). Numerous companies are involved in producing these devices, each using different communication protocols, data structures, and IoT platforms. This has resulted in interoperability issues that need to be addressed. To mitigate these issues, this paper proposes an interoperability middleware for IIoT gateways that adopts ontologies based on international standards (IEEE 1872-2015) and the asset administration shell (AAS) using industrial frameworks.}
}
@article{BOROUKHIAN2025104354,
title = {Semantic middleware for demand response systems: Enhancing data interoperability in green electricity management for manufacturing},
journal = {Computers in Industry},
volume = {172},
pages = {104354},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104354},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525001198},
author = {Tina Boroukhian and Kritkorn Supyen and Christopher William Mclaughlan and Atit Bashyal and Tuan Pham and Hendro Wicaksono},
keywords = {Demand response system, Energy transition, Knowledge graph creation, Querying system, Semantic middleware},
abstract = {Optimizing the consumption of green electricity across sectors, including manufacturing, is a critical strategy for achieving net-zero emissions and advancing clean production in Europe by 2050. Demand Response (DR) represents a promising approach to engaging power consumers from all sectors in the transition toward increased utilization of renewable energy sources. A functional DR system for manufacturing power consumers requires seamless data integration and communication between information systems across multiple domains, including both power consumption and generation. This paper introduces a semantic middleware specifically designed for DR systems in the manufacturing sector, using an ontology as the central information model. To develop this ontology, we adopted a strategy that reuses and unifies existing ontologies from multiple domains, ensuring comprehensive coverage of the data requirements for DR applications in manufacturing. To operationalize this strategy, we designed novel methods for effective ontology unification and implemented them within a dedicated unification tool. This process was followed by data-to-ontology mapping to construct a knowledge graph, and was further extended through the development of a querying system equipped with a natural language interface. Additionally, this paper offers insights into the deployment environment of the semantic middleware, encompassing multiple data sources and the applications that utilize this data. The proposed approach is implemented in multiple German manufacturing small and medium-sized enterprises connected to a utility company, demonstrating consistent data interpretation and seamless information integration. Consequently, the method offers practical potential for optimizing green electricity usage in the manufacturing sector and supporting the transition toward a more sustainable and cleaner future.}
}
@article{LIU2022108495,
title = {A novel focused crawler combining Web space evolution and domain ontology},
journal = {Knowledge-Based Systems},
volume = {243},
pages = {108495},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108495},
url = {https://www.sciencedirect.com/science/article/pii/S095070512200212X},
author = {Jingfa Liu and Xin Li and Qiansheng Zhang and Guo Zhong},
keywords = {Focused crawler, Web space evolution, Multi-objective optimization, Pareto optimal, Ontology},
abstract = {In many fields, how to catch the related-topic Web resources is crucial. As a vertical search method, focused crawler has received great attention in recent years. Currently, most focused crawlers consider multiple evaluating factors of the hyperlinks and use the weighted sum approach to compute the priorities of unvisited hyperlinks. However, the proper weighted coefficients are hard to determine, and their unsuitable values may even cause the direction of crawlers to deviate seriously from the topic. To overcome this issue, this article builds a multi-objective optimization model based on Web text and link structure and designs a crawler framework called the Web space evolution (WSE), where a hyperlink bank whose radius is gradually increased is introduced to extend the search scape of crawlers in Web space. To improve the uniformity and diversity of hyperlinks, a nearest and farthest candidate solution method is combined with the fast non-dominated sorting to choose Pareto-optimal solutions (hyperlinks). A domain ontology based on the formal concept analysis is applied to establish the topic model. By incorporating the WSE and the domain ontology into the focused crawling, a novel focused crawler called FCWSEO is proposed to collect topic-relevant webpages. The experimental results on the rainstorm disaster domain show that the FCWSEO outperforms other focused crawler strategies in terms of the quantity and quality of retrieved relevant webpages.}
}
@article{HUA2025103613,
title = {Integration of dynamic knowledge and LLM for adaptive human-robot collaborative assembly solution generation},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103613},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103613},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005063},
author = {Yiwei Hua and Kerun Li and Ru Wang and Yingjie Li and Guoxin Wang and Yan Yan},
keywords = {Dynamic knowledge, Large language model, Adaptive, Human-robot collaboration, Solution generation},
abstract = {The active adaptability of robots has always been a central challenge and focus in human-robot collaboration research. In complex product assembly scenarios, humans often struggle to provide robots with clear or sufficient natural language instructions. To enhance a robot’s adaptive capability, it is essential to incorporate dynamic collaborative context information, such as assembly requirement changes, object positional adjustments, and product status evolution. Traditional approaches that establish static knowledge bases are more suitable for simple and localized context tasks but overlook the fact that in collaborative assembly tasks, historical contextual knowledge accumulates rapidly. This accumulation makes it increasingly challenging to extract effective knowledge from large volumes of historical data and reduces interference from irrelevant contextual prompts. To address this issue, this paper proposes an adaptive method for intelligent generation of Human-Robot Collaborative Assembly (HRCA) programs by fusing dynamic knowledge and Large Language Models (LLMs). The method generates collaborative assembly solutions based on the logic of knowledge modeling, knowledge evolution, and knowledge enhancement. Specifically, a dynamic knowledge evolution mechanism for HRCA is designed, which establishes a memory iteration loop for dynamic contextual requirements and historical scene states. This loop provides LLMs with comprehensive prompt texts that balance current contextual demands with scene states, reducing the interference of irrelevant information and improving the accuracy and consistency of the generated solutions. The proposed method is applied to a complex product’s HRCA, and its performance is compared with various baseline methods. The results show that the proposed method significantly enhances the accuracy of multi-step reasoning in HRCA, with accuracy and consistency in the generated solutions for different collaboration modes approaching 90%, thereby validating the effectiveness of the proposed method.}
}
@article{ZALAMEAPATINO2018162,
title = {Merging and expanding existing ontologies to cover the Built Cultural Heritage domain},
journal = {Journal of Cultural Heritage Management and Sustainable Development},
volume = {8},
number = {2},
pages = {162-178},
year = {2018},
issn = {2044-1266},
doi = {https://doi.org/10.1108/JCHMSD-05-2017-0028},
url = {https://www.sciencedirect.com/science/article/pii/S2044126618000172},
author = {Olga Piedad {Zalamea Patino} and Jos {Van Orshoven} and Thérèse Steenberghen},
keywords = {CIDOC-CRM, BCH-ontology, CityGML, Mondis},
abstract = {Purpose
The purpose of this paper is to present the development of an ontological model consisting of terms and relationships between these terms, creating a conceptual information model for the Built Cultural Heritage (BCH) domain, more specifically for preventive conservation.
Design/methodology/approach
The On-To-Knowledge methodology was applied in the ontology development process. Terms related to preventive conservation were identified by means of a taxonomy which was used later to identify related existing ontologies. Three ontologies were identified and merged, i.e. Geneva City Geographic Markup Language (Geneva CityGML), Monument Damage ontology (Mondis) and CIDOC Conceptual Reference Model (CIDOC-CRM). Additional classes and properties were defined as to provide a complete semantic framework for management of BCH.
Findings
A BCH-ontology for preventive conservation was created. It consists of 143 classes from which 38 originate from the Mondis ontology, 38 from Geneva CityGML, 37 from CIDOC-CRM and 30 were newly created. The ontology was applied in a use case related to the New cathedral in the city of Cuenca, Ecuador. Advantages over other type of systems and for the BCH-domain were discussed based on this example.
Research limitations/implications
The proposed ontology is in a testing stage through which a number of its aspects are being verified.
Originality/value
This ontological model is the first one to focus on the preventive conservation of BCH.}
}
@article{ZHENG2021103930,
title = {A shared ontology suite for digital construction workflow},
journal = {Automation in Construction},
volume = {132},
pages = {103930},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103930},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003812},
author = {Yuan Zheng and Seppo Törmä and Olli Seppänen},
keywords = {Construction workflow, Ontology, Digital construction, Information management},
abstract = {With ongoing advancements in information and communication technologies (ICTs) in all stages of the construction lifecycle, information from entities related to construction workflow (CW) can now be automatically collected. These implementations are point solutions, which require systematic integration to combine their information to enable a holistic picture of CW. The major barrier to such integration is information heterogeneity, where the information is collected from different systems under multiple contexts. Scholars in the construction domain have explored the use of ontology to solve the information-integration problem, although an ontology that both adequately represents the CW and integrates the digitalized information of CW via various systems and multiple contexts is currently missing from the existing literature. This research thus presents an ontology set for formalizing and integrating CW information within the digital construction context. The proposed digital construction ontologies (DiCon) are shared representations of construction domain knowledge that specify the terms and relations of CWs and their related information. We developed the DiCon based on a hybrid ontology development approach. The DiCon includes six modules: Entities, Processes, Information, Agents, Variables, and Contexts. The developed DiCon was further evaluated by approaches including automatic consistency checking, criteria-based evaluation, expert workshops, and task-based evaluation and involved two use cases by answering relevant competency questions via SPARQL queries. The results of the evaluation demonstrate that the DiCon ontologies are sufficient to represent domain knowledge and can formalize and integrate CW information within the digital construction context.}
}
@article{CHAOUCH2021890,
title = {Model for the classification of scheduling problems based on ontology},
journal = {Procedia Computer Science},
volume = {181},
pages = {890-896},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.244},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002878},
author = {Rihab Chaouch and Hanen Ghorbel and Soulef Khalfallah},
keywords = {Scheduling, classification of scheduling problems, ontology, knowledge-base;},
abstract = {In this paper, we propose an ontology-based architecture for the classification of a scheduling problem. In fact, planning and scheduling is a major concern for the production enterprise as it contributes to its profit. Most of the time, researchers propose algorithms that relax some reel situations’ constraints; given that reel shop description are not taken directly from the floor supervisor, or given the complexity of the problem. The objective of ontology for the classification of scheduling problem is to build a bridge between practitioners and researchers in order to identify the reel classification of the problem. Thus, in this paper, we start by presenting a general model for the classification of scheduling problem. On this model, we build specific domain ontology. The specific domain ontology functions as a knowledge base and a data access point for the scheduling problem classification system.}
}
@article{JINDAL202062,
title = {Construction of domain ontology utilizing formal concept analysis and social media analytics},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {1},
pages = {62-69},
year = {2020},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2020.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666307420300103},
author = {Rajni Jindal and K.R. Seeja and Shivani Jain},
keywords = {Ontology, Semantic web, Formal concept analysis, Fluent editor, Twitter, Social media analysis},
abstract = {Semantic Web, deals with the meaning of information in a defined domain and Ontologies are the backbone of Semantic Web. Domain Ontologies are crucial source of information for knowledge-based system. Still, domain ontology development is a labor- intensive process and is highly dependent on developer's knowledge. In this work, a novel semiautomatic method is proposed to build an ontology on terrorism domain. Terrorism activities provide crucial information to enhance security system for any country worldwide. Social media data, namely, Twitter text data is extracted to attain latest information related to domain and next, concepts and relationships are identified and mapped using formal concept analysis. Several user-defined relationships are presented through fluent editor tool. Also, knowledge is extracted with the help of query-based system through a reasoner window of fluent editor. The developed domain ontology is published on web using ontology web language which can be utilized in other related application areas. The proposed work is significant as it develops a wide-coverage domain ontology for terrorism domain using a tool named Fluent Editor, in place of standard tool protégé and semantic information is extracted similar to query-based system with 100% accuracy.}
}
@article{PANKOWSKI2021497,
title = {Modeling and Querying Data in an Ontology-Based Data Access System},
journal = {Procedia Computer Science},
volume = {192},
pages = {497-506},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015386},
author = {Tadeusz Pankowski},
keywords = {Ontology-Based Data Access, Description Logic, query rewriting, faceted queries},
abstract = {The Ontology-Based Data Access (OBDA) systems allows users to access external databases through a conceptual domain view, given in terms of an ontology. This semantic technology addresses such problems as conceptual modeling, query rewriting, an source-to-target mapping. We show how these issues have been defined and implemented in the DAFO system, highlighting those features that distinguish it from the standard OBDA system. We propose an original approach to cope with the trade-off between expressiveness of the ontology-based conceptual modelling and ontology-mediated query rewriting in the DAFO systems. For this purpose, we divide ontological rules into three parts: (a) rewriting rules, (b) rules defining intensional predicates (views), and (c) constraint rules (satisfied in the database). This allows to balance the semantic power of the conceptual modeling and the effciency of query answering in the DAFO system, where queries are based on the faceted query paradigm.}
}
@article{MANTOVANI2020104446,
title = {Ontology-driven representation of knowledge for geological maps},
journal = {Computers & Geosciences},
volume = {139},
pages = {104446},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104446},
url = {https://www.sciencedirect.com/science/article/pii/S0098300419302420},
author = {Alizia Mantovani and Fabrizio Piana and Vincenzo Lombardo},
keywords = {Geology, Geologic knowledge encoding, Geological map, Geographical information science & systems, Geological structure ontology, Geological unit ontology},
abstract = {This paper presents an ontology-driven representation of knowledge for geological maps. The ontological formal language allows for a machine-readable encoding of the Earth scientist's interpretation through semantic categories and properties and is credited to support knowledge sharing and interoperability. We introduce an ontology-driven method for the interpretation and the encoding of the map data that employs shared vocabularies and resources encoded through ontologies in order to prevent the use of ambiguous terms. The approach relies on a computational ontology of the geological knowledge (OntoGeonous), which formalizes a number of geological knowledge sources (including GeoScienceML), to guide the interpretation process. The design of the database underlying the map (OntoGeoBase) constrains the process of data entry to refer to the terminology conveyed by the taxonomic-axiomatic nature of the ontology. This reduces the amount of implicit knowledge favouring a conceptual alignment of the ancillary documentation with the map, leading to a better comprehension of map and allowing the traceability of the interpretation.}
}
@article{LONGO2022594,
title = {An ontology-based, general-purpose and Industry 4.0-ready architecture for supporting the smart operator (Part I – Mixed reality case)},
journal = {Journal of Manufacturing Systems},
volume = {64},
pages = {594-612},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522001303},
author = {Francesco Longo and Giovanni Mirabelli and Letizia Nicoletti and Vittorio Solina},
keywords = {Mixed reality, Ontology, Internet of things, Smart operator, Smart factory},
abstract = {The advent of novel industry 4.0-driven technologies is offering significant opportunities to manufacturing systems, but at the same time it is posing new great challenges. The growing number of connected and interconnected devices is enormously increasing the amount of data generated, which must be properly organized to give value to the business. Basically, the need for approaches that are able to guarantee compliance with FAIR data principles is significantly emerging. Recently, the KNOW4I platform has been proposed in the literature to support the smart operator through a suite of Smart Utilities and Objects (Longo et al., 2022). The main purpose of this paper is to extend such platform, in the form of an ontology-based, general-purpose and industry 4.0-ready architecture, capable of improving the capabilities of the smart operator, with a focus on mixed reality. The novel proposal is based on two fundamental aspects: (1) a new general ontology, developed through the ontology engineering methodology; (2) the adoption of FIWARE, an open-source infrastructure, capable of enabling interoperability between different systems. The proposed architecture is implemented and validated on two case studies belonging to the manufacturing sector, which respectively concern (1) scheduled maintenance and alarm management and (2) customer order management. The experimental phase shows that the architecture is able to effectively and efficiently support the smart operator.}
}
@article{LI2025107173,
title = {Supporting vision-language model few-shot inference with confounder-pruned knowledge prompt},
journal = {Neural Networks},
volume = {185},
pages = {107173},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2025.107173},
url = {https://www.sciencedirect.com/science/article/pii/S0893608025000528},
author = {Jiangmeng Li and Wenyi Mo and Fei Song and Chuxiong Sun and Wenwen Qiang and Bing Su and Changwen Zheng},
keywords = {Multi-modal model, Large-scale pre-training, Prompt learning, Maximum entropy, Knowledge graph},
abstract = {Vision-language models are pre-trained by aligning image-text pairs in a common space to deal with open-set visual concepts. Recent works adopt fixed or learnable prompts, i.e., classification weights are synthesized from natural language descriptions of task-relevant categories, to reduce the gap between tasks during the pre-training and inference phases. However, how and what prompts can improve inference performance remains unclear. In this paper, we explicitly clarify the importance of incorporating semantic information into prompts, while existing prompting methods generate prompts without sufficiently exploring the semantic information of textual labels. Manually constructing prompts with rich semantics requires domain expertise and is extremely time-consuming. To cope with this issue, we propose a knowledge-aware prompt learning method, namely Confounder-pruned Knowledge Prompt (CPKP), which retrieves an ontology knowledge graph by treating the textual label as a query to extract task-relevant semantic information. CPKP further introduces a double-tier confounder-pruning procedure to refine the derived semantic information. Adhering to the individual causal effect principle, the graph-tier confounders are gradually identified and phased out. The feature-tier confounders are eliminated by following the maximum entropy principle in information theory. Empirically, the evaluations demonstrate the effectiveness of CPKP in few-shot inference, e.g., with only two shots, CPKP outperforms the manual-prompt method by 4.64% and the learnable-prompt method by 1.09% on average.}
}
@article{ALGHERAIRY2025101697,
title = {Prompting large language models for user simulation in task-oriented dialogue systems},
journal = {Computer Speech & Language},
volume = {89},
pages = {101697},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101697},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000809},
author = {Atheer Algherairy and Moataz Ahmed},
keywords = {Task Oriented Dialogue, User simulator, Large language models, Prompting, Agenda-based simulator},
abstract = {Large Language Models (LLMs) have gained widespread popularity due to their instruction-following abilities. In this study, we evaluate their ability in simulating user interactions for task-oriented dialogue (TOD) systems. Our findings demonstrate that prompting LLMs reveals their promising capabilities for training and testing dialogue policies, eliminating the need for domain expertise in crafting complex rules or relying on large annotated data, as required by traditional simulators. The results show that the dialogue system trained with the ChatGPT simulator achieves a success rate of 59%, comparable to a 62% success rate of the dialogue system trained with the manual-rules, agenda-based user simulator (ABUS). Furthermore, the dialogue system trained with the ChatGPT simulator demonstrates better generalization ability compared to the dialogue system trained with the ABUS. Its success rate outperforms that of the dialogue system trained with the ABUS by 4% on GenTUS, 5% on the ChatGPT Simulator, and 3% on the Llama simulator. Nevertheless, LLM-based user simulators provide challenging environment, lexically rich, diverse, and random responses. Llama simulator outperforms the human reference in all lexical diversity metrics with a margin of 0.66 in SE, 0.39 in CE, 0.01 in MSTTR, 0.04 in HDD, and 0.55 in MTLD, while the ChatGPT simulator achieves comparable results. This ultimately contributes to enhancing the system’s ability to generalize more effectively.}
}
@article{HUITZIL2021107158,
title = {Minimalistic fuzzy ontology reasoning: An application to Building Information Modeling},
journal = {Applied Soft Computing},
volume = {103},
pages = {107158},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107158},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621000818},
author = {Ignacio Huitzil and Miguel Molina-Solana and Juan Gómez-Romero and Fernando Bobillo},
keywords = {Fuzzy ontologies, Flexible querying, Building Information Modeling},
abstract = {This paper presents a minimalistic reasoning algorithm to solve imprecise instance retrieval in fuzzy ontologies with application to querying Building Information Models (BIMs)—a knowledge representation formalism used in the construction industry. Our proposal is based on a novel lossless reduction of fuzzy to crisp reasoning tasks, which can be processed by any Description Logics reasoner. We implemented the minimalistic reasoning algorithm and performed an empirical evaluation of its performance in several tasks: interoperation with classical reasoners (Hermit and TrOWL), initialization time (comparing TrOWL and a SPARQL engine), and use of different data structures (hash tables, databases, and programming interfaces). We show that our software can efficiently solve very expressive queries not available nowadays in regular or semantic BIMs tools.}
}
@article{AMITH20181,
title = {Assessing the practice of biomedical ontology evaluation: Gaps and opportunities},
journal = {Journal of Biomedical Informatics},
volume = {80},
pages = {1-13},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300285},
author = {Muhammad Amith and Zhe He and Jiang Bian and Juan Antonio Lossio-Ventura and Cui Tao},
keywords = {Ontology evaluation, Biomedical ontologies, Knowledge representation, Quality assurance},
abstract = {With the proliferation of heterogeneous health care data in the last three decades, biomedical ontologies and controlled biomedical terminologies play a more and more important role in knowledge representation and management, data integration, natural language processing, as well as decision support for health information systems and biomedical research. Biomedical ontologies and controlled terminologies are intended to assure interoperability. Nevertheless, the quality of biomedical ontologies has hindered their applicability and subsequent adoption in real-world applications. Ontology evaluation is an integral part of ontology development and maintenance. In the biomedicine domain, ontology evaluation is often conducted by third parties as a quality assurance (or auditing) effort that focuses on identifying modeling errors and inconsistencies. In this work, we first organized four categorical schemes of ontology evaluation methods in the existing literature to create an integrated taxonomy. Further, to understand the ontology evaluation practice in the biomedicine domain, we reviewed a sample of 200 ontologies from the National Center for Biomedical Ontology (NCBO) BioPortal—the largest repository for biomedical ontologies—and observed that only 15 of these ontologies have documented evaluation in their corresponding inception papers. We then surveyed the recent quality assurance approaches for biomedical ontologies and their use. We also mapped these quality assurance approaches to the ontology evaluation criteria. It is our anticipation that ontology evaluation and quality assurance approaches will be more widely adopted in the development life cycle of biomedical ontologies.}
}
@article{ESFAHANI202273,
title = {Enabling automated checking of information in factory planning with ontologies – a case study},
journal = {Procedia CIRP},
volume = {112},
pages = {73-78},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.047},
url = {https://www.sciencedirect.com/science/article/pii/S221282712201201X},
author = {Matthias Ebade Esfahani and Peter Burggräf and Tobias Adlon and Stephan Matoni},
keywords = {Factory planning, Building Information Modeling, Ontologies, Expert systems, Automated checking},
abstract = {Planning a manufacturing factory involves a high number of planners who deliver their domain-specific planning information. In Building Information Modeling, a central entity needs to check the information from different planning domains for coherence to each other. Especially for non-geometrical information, this is a manual, time-consuming and error-prone approach. The paper at hand therefore aimed at developing an expert system that automates the decision making in the checking process of factory planning information. On the basis of a case study, we have applied knowledge engineering methods from the Semantic Web and calculated an F1 score for validating our approach.}
}
@article{JABALAMELI2021114922,
title = {Denoising distant supervision for ontology lexicalization using semantic similarity measures},
journal = {Expert Systems with Applications},
volume = {177},
pages = {114922},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.114922},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421003638},
author = {Mehdi Jabalameli and Mohammadali Nematbakhsh and Reza Ramezani},
keywords = {Ontology lexicalization, Distant supervision, Denoising, Word embeddings},
abstract = {Ontology lexicalization aims to provide information about how the elements of an ontology are verbalized in a given language. Most ontology lexicalization techniques require labeled training data, which are usually generated automatically using the distant supervision technique. This technique is based upon the assumption that if a sentence contains two entities of a triple in a knowledge base, it expresses the relation stated in that triple. This assumption is very simplistic and would lead to generating wrong mappings between sentences and knowledge base triples. In other words, a sentence may contain two entities of a triple, but the relation of entities in the sentence differs from the relation of the triple. Such wrong mappings cause to generating wrong ontology lexicon entries. In this paper, a new method, called denoising distant supervision, is presented to reduce the wrong mappings between sentences and triples by taking the semantic similarity between sentences and the label of triples’ predicate into account. For this purpose, different semantic similarity measures are proposed, which use pre-trained word embeddings to calculate the semantic similarity between the sentences and the label of the triples relation. Then, the sentences whose semantic similarity is low are removed from the mapping. The proposed solution is evaluated in the M−ATOLL framework. The experimental results show that the quality of the generated ontology lexicon under the proposed solution is improved compared to state-of-the-art techniques.}
}
@article{WANG2025101364,
title = {A multimodal LLM-agent framework for personalized clinical decision-making in hepatocellular carcinoma},
journal = {Patterns},
pages = {101364},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101364},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925002120},
author = {Liyang Wang and Fa Tian and Chengquan Li and Jitao Wang and Jiahong Dong and Jiabin Cai and Shizhong Yang and Xiaobin Feng},
keywords = {hepatocellular carcinoma, radiomics, deep learning, LLM-based decision-making, pathological marker prediction},
abstract = {Summary
Hepatocellular carcinoma (HCC) treatment is challenging due to tumor heterogeneity and patient variability. Current guidelines often overlook individual factors, limiting treatment precision. We developed an integrated framework combining radiomics, deep learning, and large language model (LLM)-based decision agents to generate personalized HCC treatment recommendations. A modified GhostNet incorporating dilated convolutions, channel and spatial attention mechanism (CBAM), and residual channel attention (RCA) modules was trained on MRI to predict pathological markers such as microvascular invasion (MVI), capsule presence, and tumor differentiation. A fusion model integrating radiomics and deep learning enhanced prediction accuracy. Six AI agents processed structured multimodal data and generated individualized treatment strategies, which were evaluated by hepatobiliary surgeons. The fusion model significantly improved prediction accuracy, with MVI and capsule presence reaching 0.8902 and 0.8765, respectively. DeepSeek-R1 achieved the highest clinical relevance score, followed by GPT-4 and Med-PaLM 2. This framework demonstrates the feasibility of AI-assisted, patient-specific HCC decision-making, offering a promising direction for precision oncology.}
}
@article{BENEDETTO2025100353,
title = {Assessing how accurately large language models encode and apply the common European framework of reference for languages},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100353},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100353},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001565},
author = {Luca Benedetto and Gabrielle Gaudeau and Andrew Caines and Paula Buttery},
keywords = {Large language models, Language learning, Common European Framework of Reference for Languages},
abstract = {Large Language Models (LLMs) can have a transformative effect on a variety of domains, including education, and it is therefore pressing to understand whether these models have knowledge of – or, in other words, how they have encoded – the specific pedagogical requirements of different educational domains, and whether they use this when performing educational tasks. In this work, we propose an approach to evaluate the knowledge – or encoding – that the LLMs have of the Common European Framework of Reference for Languages (CEFR), and use it to evaluate five modern LLMs. Our study shows that the suite of tasks we propose is quite challenging for all the LLMs, and they often provide results which are not satisfactory and would be unusable in educational applications, suggesting that – even if they encode some information about the CEFR – this knowledge is not really leveraged when performing downstream tasks.}
}
@article{DOREA201939,
title = {Drivers for the development of an Animal Health Surveillance Ontology (AHSO)},
journal = {Preventive Veterinary Medicine},
volume = {166},
pages = {39-48},
year = {2019},
issn = {0167-5877},
doi = {https://doi.org/10.1016/j.prevetmed.2019.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167587718301703},
author = {Fernanda C. Dórea and Flavie Vial and Karl Hammar and Ann Lindberg and Patrick Lambrix and Eva Blomqvist and Crawford W. Revie},
keywords = {Syndromic surveillance, Classification, Vocabulary, Terminology, Standards},
abstract = {Comprehensive reviews of syndromic surveillance in animal health have highlighted the hindrances to integration and interoperability among systems when data emerge from different sources. Discussions with syndromic surveillance experts in the fields of animal and public health, as well as computer scientists from the field of information management, have led to the conclusion that a major component of any solution will involve the adoption of ontologies. Here we describe the advantages of such an approach, and the steps taken to set up the Animal Health Surveillance Ontological (AHSO) framework. The AHSO framework is modelled in OWL, the W3C standard Semantic Web language for representing rich and complex knowledge. We illustrate how the framework can incorporate knowledge directly from domain experts or from data-driven sources, as well as by integrating existing mature ontological components from related disciplines. The development and extent of AHSO will be community driven and the final products in the framework will be open-access.}
}
@article{POLENGHI2022100286,
title = {Ontology-augmented Prognostics and Health Management for shopfloor-synchronised joint maintenance and production management decisions},
journal = {Journal of Industrial Information Integration},
volume = {27},
pages = {100286},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100286},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000832},
author = {Adalberto Polenghi and Irene Roda and Marco Macchi and Alessandro Pozzetti},
keywords = {Ontology, Reasoning, Prognostics and health management, PHM, maintenance, production},
abstract = {In smart factories, guaranteeing shopfloor-synchronised and real-time decision-making is essential to be responsive to the ever-changing internal environment, namely the shopfloor of the production system and assets. At operational level, decisions should balance counter acting objectives of maintenance and production; therefore, their decision-making processes should be joint and coordinated, to fulfil production requirements considering the health state of the assets. The knowledge of the current state is promoted by the application of Prognostics and Health Management (PHM) as an aid to support informed decision-making. Nevertheless, PHM-purposed information is usually not complete in terms of production requirements. To support joint maintenance and production management decisions, an ontological approach is proposed. The ontology, called ORMA (Ontology for Reliability-centred MAintenance), has a modular structure, including formalisation of asset, process, and product knowledge. Via suitable relationships, rules, and axioms, ORMA can infer product feasibility based on the current health state of the assets and their functional units. ORMA is implemented in a Flexible Manufacturing Line at a laboratory scale. Therein, an integrated solution, involving a health state detection algorithm that interacts with the ontology, supports human decision-making via a web-based dashboard; joint maintenance and production management decisions can be then taken, relying on diversified information provided by the PHM algorithm as well as the augmentation via ontology reasoning. The proposed ontology-based solution represents a step towards reconfigurability of smart factories where human and automated decision-making processes work in synergy.}
}
@article{ZHULEGO2022602,
title = {On the Possibility of Ontology Map Constructing on the Cerebral Cortex},
journal = {Procedia Computer Science},
volume = {213},
pages = {602-609},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.110},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922018099},
author = {Vladimir Zhulego and Vadim Ushakov and Artem Balyakin and Olga Chernavskaya},
keywords = {thinking mathematical modeling, interaction matrix, semantic map, ontological map, cerebral cortex},
abstract = {A hypothesis has been put forward about the possibility of setting up an experiment to test the correlation between the factorial mathematical model of the subject area, and the "thinking model of the subject area" on the cerebral cortex. By analogy with the semantic map of the cerebral cortex, it is proposed to build an ontological map of the cerebral cortex, i.e. to localize denotations of the phenomena. A number of arguments are given in favor of such a hypothesis and the possibilities of setting up an experiment are discussed.}
}
@article{CHANG2022107119,
title = {Integrated ontology-based approach with navigation and content representation for health care website design},
journal = {Computers in Human Behavior},
volume = {128},
pages = {107119},
year = {2022},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2021.107119},
url = {https://www.sciencedirect.com/science/article/pii/S0747563221004428},
author = {Te-Min Chang and Hao-Yun Kao and Jen-Her Wu and Kai-Wen Hsiao and Te-Fu Chan}
}
@article{CHEN2020101148,
title = {Ontology-based requirement verification for complex systems},
journal = {Advanced Engineering Informatics},
volume = {46},
pages = {101148},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101148},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620301191},
author = {Ruirui Chen and Chun-Hsien Chen and Yusheng Liu and Xiaoping Ye},
keywords = {Ontology, Verification, Systems design, Reasoning},
abstract = {Verification is a necessary part of Model-based systems engineering (MBSE) which is becoming a mainstream methodology for the design of complex systems. Verification in the early design stage has aroused widespread attention for its efficiency and cost-saving. Although there are numbers of researches on verification, some deficiencies still exist, such as the integration of design and verification needs to improve, the design problems are hard to trace and the behavior verification in the early design stage is often omitted. In this study, a novel ontology-based requirement verification method for complex systems is proposed to solve the above-mentioned problems. First, a requirement formalization method is proposed to avoid the ambiguousness of natural language, to make requirements easier to verify, and to make design problems easier to trace. Second, some transformation rules are defined to realize the automatic design ontology and rule generation. Based on these two steps, automated verification can be done through reasoning with ontology models and rules. This verification method is fully integrated with design tools and no additional expertise is needed for designers. To validate its feasibility and advantages, an example of a smart traffic light system is provided.}
}
@article{BITSCH2022577,
title = {Dynamic adaption in cyber-physical production systems based on ontologies},
journal = {Procedia Computer Science},
volume = {200},
pages = {577-584},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.255},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922002642},
author = {Günter Bitsch and Pascal Senjic and Jeremy Askin},
keywords = {Cyber-Physical Production Systems (CPPSs), Ontology, Adaptability, Flexibility},
abstract = {The paradigmatic shift of production systems towards Cyber-Physical Production Systems (CPPSs) requires the development of flexible and decentralized approaches. In this way, such systems enable manufacturers to respond quickly and accurately to changing requirements. However, domain-specific applications require the use of suitable conceptualizations. The issue at hand, when using various conceptualizations is the interoperability of different ontologies. To achieve flexibility and adaptability in CPPSs though requires overcoming interoperability issues within CPPSs. This paper presents an approach to increase flexibility and adaptability in CPPSs while addressing the interoperability issue. In this work, OWL ontologies conceptualize domain knowledge. The Intelligent Manufacturing Knowledge Ontology Repository (IMKOR) connects the domain knowledge in different ontologies. Testing if adaptions in one ontology within the IMKOR provide knowledge to the whole IMKOR. The tests showed, positive results and the repository makes the knowledge available to the whole CPPS. Furthermore, an increase in flexibility and adaptability was noticed.}
}
@article{KATTI2020197,
title = {Bidirectional Transformation of MES Source Code and Ontologies},
journal = {Procedia Manufacturing},
volume = {42},
pages = {197-204},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.070},
url = {https://www.sciencedirect.com/science/article/pii/S235197892030634X},
author = {Badarinath Katti and Christiane Plociennik and Martin Ruskowski and Michael Schweitzer},
keywords = {Cloud based MES, Production Automation, Knowledge Representation, OPC-UA, Ontology, OWL, SWRL, SQWRL, SPARQL, Edge Computing},
abstract = {Future production environments must be flexible and reconfigurable. To achieve this, the devices and services to fulfill the different steps of a production order (PO) should not be selected in the manufacturing execution system (MES), but in an edge component close to the shop floor. To enable this, abstract services in the PO and concrete services provided by the field devices on the shop floor need to refer to a production ontology. The creation of this ontology is a challenge of its own. This research proposes a pragmatic automation of an encoding of a primary and light weight production ontology based on the source code of MES. The transformation procedure of source code to resource, product and generic concepts of the manufacturing plant ontology is described. To this end, the knowledge of OPC UA collaborations are also exploited during the creation of resource ontologies. Due to a fundamental difference between source code implementation (imperative paradigm) and ontology representation (declarative paradigm), the problem of information loss is inevitable. This problem is overcome by formulation of production and business rules that encapsulate the logic of the MES. The foundation of ontology is exploited to formulate these rulesets using OWL based constructs and OWL based rule languages such as Semantic Web Rule Language (SWRL), Semantic Query-Enhanced Web Rule Language (SQWRL) and SPARQL Protocol and RDF Query Language (SPARQL) based on feasibility and requirements of specific rules. Further, these rulesets are either run on the automatically generated ontology at design time with an intention to enrich the knowledge base, or production runtime to validate the pre-defined business rules between the production steps. The generated ontology also acts as basis for automatically generating the OWL-S ontologies for the OPC UA application methods for the purpose of dynamic manufacturing service discovery and orchestration. The generated ontology and an abstract PO hooked with formulated rules are cached to the shop-floor network for consequent production control to enable smart edge production. An implementation is conducted on an industrial use-case demonstrator to evaluate the applicability of the proposed approach.}
}
@article{GONZALEZ20211,
title = {An approach based on the ifcOWL ontology to support indoor navigation},
journal = {Egyptian Informatics Journal},
volume = {22},
number = {1},
pages = {1-13},
year = {2021},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2020.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S1110866520301122},
author = {Evelio González and José Demetrio Piñeiro and Jonay Toledo and Rafael Arnay and Leopoldo Acosta},
keywords = {Applications, Decision support, Ontology, Software experience, Indoor navigation},
abstract = {This paper presents an indoor navigation support system based on the Building Information Models (BIM) paradigm. Although BIM is initially defined for the Architecture, Engineering and Construction/Facility Management (AEC/FM) industry, the authors believe that it can provide added value in this context. To this end, the authors will focus on the Industry Foundation Classes (IFC) standard for the formal representation of BIM. The approach followed in this paper will be based on the ifcOWL ontology, which translates the IFC schemas into Ontology Web Language (OWL). Several modifications of this ontology have been proposed, consisting of the inclusion of new items, SWRL rules and SQWRL searches. This way of expressing the elements of a building can be used to code information that is very useful for navigation, such as the location of elements related to the actions desired by the user. It is important to note that this design is intended to be used as a complement to other well-known tools and techniques for indoor navigation. The proposed modifications have been successfully tested in a variety of simulated and real scenarios. The main limitation of the proposal is the immense amount of information contained in the ifcOWL ontology, which causes difficulties involving its processing and the time necessary to perform operations on it. Those elements that are considered important have been selected, removing those that seem secondary to navigation. This procedure will result in a significant reduction in the storage and semantic processing of the information. Thus, for a system with 1000 individuals (in the ontological sense), the processing time is about 90 s. The authors regard this time as acceptable, since in most cases the tasks involved can be considered part of the system initialization, meaning they will only be executed once at the beginning of the process.}
}
@article{KUMAR2024122493,
title = {Aspect-based sentiment score and star rating prediction for travel destination using Multinomial Logistic Regression with fuzzy domain ontology algorithm},
journal = {Expert Systems with Applications},
volume = {240},
pages = {122493},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122493},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423029950},
author = {Niranjan Kumar and Bhagyashri R. Hanji},
keywords = {Aspect sentiment analysis, GloVe, Tourism destination, Trip advisor, Cumulative average, Multinomial Logistic Regression},
abstract = {Due to its significant advantages for downstream applications, such as recommender systems, In the context of travel and tourism, user reviews on TripAdvisor have an impact on other travellers' judgments regarding a variety of travel-related issues, including the choice of a vacation spot, lodging, and places to visit. In graded user reviews, the model must specifically forecast the user's review score after receiving the textual review. The purpose of this article is to present a predictive outline for aspect-based extraction and classification that can estimate the users' optimal travel destination. This impression helps travellers in a variety of ways, such as by recommending better locations that also include expensive destinations. The underlying classification algorithm's processing time is significantly impacted by more dimensions. The term “curse of dimensionality” is often used in statistics and machine learning to describe these issues. By projecting the high-dimensional input data into the low-dimensional subspace while roughly maintaining the distance between the data points with a higher probability, the Random Projection (RP) ensemble classifier decreases the complexity of multivariate data. Extract the crucial information from the reviews, then use Glove word vector representation to categorise the relevant emotions. Further, the article proposed Multinomial Logistic Regression (MNLR) with a Fuzzy Domain Ontology (FDO) algorithm for aspect-based sentiment analysis. More intricate aspects than just the products themselves impact how satisfied people are with tourist destinations. The most important factor in evaluating how convenient a tourist route will be is typically the weather. The combined form of predicted sentiment score, start ratings and environment factor has to be calculated to predict the travel destination based on the measurement of personalized search results. The simulation was processed in Python software. The presented work has utilized some performance measures to evaluate the classification model such as F1-score, Recall, Precision, Mean Absolute Error (MAE), Mean Squared Error (MSE), Cohen Score and Matthew Score. The accuracy with GloVe word vector representation was 90% and after the GloVe representation, the classification model accuracy was about 94%. The proposed strategy outperforms in terms of classification accuracy, according to simulated results and analyses of real-world data.}
}
@article{HE2025102963,
title = {A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics},
journal = {Information Fusion},
volume = {118},
pages = {102963},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102963},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000363},
author = {Kai He and Rui Mao and Qika Lin and Yucheng Ruan and Xiang Lan and Mengling Feng and Erik Cambria},
keywords = {Healthcare application, Large language model, Medicine, Pretrained language model},
abstract = {The utilization of large language models (LLMs) for Healthcare has generated both excitement and concern due to their ability to effectively respond to free-text queries with certain professional knowledge. This survey outlines the capabilities of the currently developed Healthcare LLMs and explicates their development process, to provide an overview of the development road map from traditional Pretrained Language Models (PLMs) to LLMs. Specifically, we first explore the potential of LLMs to enhance the efficiency and effectiveness of various Healthcare applications highlighting both the strengths and limitations. Secondly, we conduct a comparison between the previous PLMs and the latest LLMs, and summarize related Healthcare training data, learning methods, and usage. Finally, the unique concerns associated with deploying LLMs are investigated, particularly regarding fairness, accountability, transparency, and ethics. Besides, we support researchers by compiling a collection of open-source resources11https://github.com/KaiHe-CatOwner/LLM-for-Healthcare.. Summarily, we contend that a significant paradigm shift is underway, transitioning from PLMs to LLMs. This shift encompasses a move from discriminative AI approaches to generative AI approaches, as well as a move from model-centered methodologies to data-centered methodologies. We determine that the biggest obstacle of using LLMs in Healthcare are fairness, accountability, transparency and ethics.}
}
@article{JIANG2025103312,
title = {AutoTRIZ: Automating engineering innovation with TRIZ and large language models},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103312},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103312},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625002058},
author = {Shuo Jiang and Weifeng Li and Yuping Qian and Yangjun Zhang and Jianxi Luo},
keywords = {Large language models, TRIZ, Engineering design, Innovation, Problem solving, Artificial intelligence},
abstract = {Various ideation methods, such as morphological analysis and design-by-analogy, have been developed to aid creative problem-solving and innovation. Among them, the Theory of Inventive Problem Solving (TRIZ) stands out as one of the best-known methods. However, the complexity of TRIZ and its reliance on users’ knowledge, experience, and reasoning capabilities limit its practicality. To address this, we introduce AutoTRIZ, an artificial ideation system that integrates Large Language Models (LLMs) to automate and enhance the TRIZ methodology. By leveraging LLMs’ vast pre-trained knowledge and advanced reasoning capabilities, AutoTRIZ offers a novel, generative, and interpretable approach to engineering innovation. AutoTRIZ takes a problem statement from the user as its initial input, automatically conduct the TRIZ reasoning process and generates a structured solution report. We demonstrate and evaluate the effectiveness of AutoTRIZ through comparative experiments with textbook cases and a real-world application in the design of a Battery Thermal Management System (BTMS). Moreover, the proposed LLM-based framework holds the potential for extension to automate other knowledge-based ideation methods, such as SCAMPER, Design Heuristics, and Design-by-Analogy, paving the way for a new era of AI-driven innovation tools.}
}
@article{KALITA2020785,
title = {Ontology for preserving the knowledge base of traditional dances (OTD)},
journal = {The Electronic Library},
volume = {38},
number = {4},
pages = {785-803},
year = {2020},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-11-2019-0258},
url = {https://www.sciencedirect.com/science/article/pii/S0264047320000090},
author = {Deepjyoti Kalita and Dipen Deka},
keywords = {Semantic Web, Digital humanities, Web 3.0, Folk dance, Culture preservation, Dance ontology},
abstract = {Purpose
Systematic organization of domain knowledge has many advantages in archiving, sharing and retrieval of information. Ontologies provide a cushion for such practices in the semantic Web environment. This study aims to develop an ontology that can preserve the knowledge base of traditional dance practices.
Design/methodology/approach
It is hypothesized that an ontology-based approach for the chosen domain might boost collaborative research prospects in the domain. A systematic methodology was developed for modeling the ontology based on the analytico-synthetic rule of library classification. Protégé 5.2 was used as an editor for the ontology using the Web ontology language combined with description logic axioms. Ontology was later implemented in a local GraphDB repository to run queries over it.
Findings
The developed ontology on traditional dances (OTD) was tested using the dances of the Rabha tribes of North East India. Rabha tribes are from an indigenous mongoloid community and have a robust presence in Southeast Asian countries, such as Myanmar, Thailand, Bangladesh, Bhutan and Nepal. The result from HermiT reasoner found the presence of no logical inconsistency in the ontology, while the OOPS! pitfall checker tool reported no major internal inconsistency. The induced knowledge base of traditional dances of the Rabha’s in the developed OTD was further validated based on some competency questions.
Research limitations/implications
In the growing trend of globalization, preservation of the cultural knowledge base of human societies is an important issue. Traditional dances reflect a strong base of the cultural heritage of human societies as they are closely related to the lifestyle, habitat, religious practices and festivals of a specific community.
Originality/value
The current study is exclusively designed, keeping in mind the variables of traditional dance domain based on a survey of the user- and domain-specific needs. The ontology finds probable uses in traditional knowledge information systems, lifestyle-based e-commerce sites and e-learning platforms.}
}
@article{KHALED2025103682,
title = {UniTextFusion: A low-resource framework for Arabic multimodal sentiment analysis using early fusion and LoRA-tuned language models},
journal = {Ain Shams Engineering Journal},
volume = {16},
number = {11},
pages = {103682},
year = {2025},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2025.103682},
url = {https://www.sciencedirect.com/science/article/pii/S209044792500423X},
author = {Salma Khaled and Walaa Medhat and Ensaf Hussein Mohamed},
keywords = {Arabic sentiment analysis, Multimodal, Early fusion, LLMs, LLama, SILMA, LoRA},
abstract = {Multimodal Sentiment Analysis (MuSA) seeks to interpret human emotions by combining textual, auditory, and visual cues. While this field has advanced significantly in English, Arabic MuSA remains underdeveloped due to limited large language models (LLMs), scarce annotated datasets, dialectal variation, and the complexity of fusing multiple modalities. Cultural elements such as sarcasm and emotional nuance are particularly difficult to capture without multimodal context. An early fusion approach, UniTextFusion, is introduced as a means of overcoming these challenges. This fusion strategy transforms audio and visual inputs into descriptive text, allowing seamless integration with Arabic-compatible LLMs. We apply parameter-efficient Low-Rank Adaption (LoRA) fine-tuning to two generative models—LLaMA 3.1-8B Instruct and SILMA AI 9B. Experiments on our Arabic MuSA dataset show that UniTextFusion improves sentiment classification performance by up to 34% in F1-score over strong unimodal and multimodal baselines, reaching 68% with LLaMA and 71% with SILMA. These results validate our hypothesis that modality textualization combined with lightweight fine-tuning is effective for Arabic MuSA and offers a scalable solution for sentiment analysis in low-resource settings.}
}
@article{ABBASIMOUD2022116877,
title = {CAFOB: Context-aware fuzzy-ontology-based tourism recommendation system},
journal = {Expert Systems with Applications},
volume = {199},
pages = {116877},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116877},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422003232},
author = {Zahra Abbasi-Moud and Saeed Hosseinabadi and Manoochehr Kelarestaghi and Farshad Eshghi},
keywords = {Recommendation system, Tourism, Context-aware, Fuzzy ontology, User preferences, Sentiment analysis},
abstract = {The ever-increasing volume of information available on tourist attractions in cyberspace has made the tourist decision-making process a crucial task. Therefore, tourism recommendation systems can significantly benefit tourists in terms of comfort and satisfaction. In this paper, a context-aware fuzzy-ontology-based tourism recommendation system is proposed. In the proposed system, we have two new propositions that can be individually used in other tourism recommendation systems: a fuzzy-weighted ontology and a new sentiment/emotion score scheme. In CAFOB, the ontology-based scores of a user’s reviews are then multiplicatively modulated by sentiment/emotion scores to generate the total scores of the reviews’ ontology words, representing the user preferences. Then, the nearby-open-3+-bubbled touristic attractions’ characteristics are extracted based on their past visitors’ reviews. Finally, a recommendation list is produced using a maximum hybrid semantic similarity between the user preferences and attractions’ characteristics. The employment of contextual information, including weather, location, and time makes CAFOB context-aware and improves the accuracy and quality of the recommendations. The F-measure, NDCG, and MRR results show the outperformance of CAFOB against the state-of-the-art tourism recommendation systems for all relevant Top N recommendation options and different geographical spans of interest.}
}
@article{MOKOS2025112231,
title = {Model-based safety analysis of requirement specifications},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112231},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112231},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002759},
author = {Konstantinos Mokos and Panagiotis Katsaros and Preben Bohn},
keywords = {Model-based design, Requirements formalization, Ontology-based specification, Formal verification, Safety analysis},
abstract = {Model-based design primarily aims to establish a communication framework throughout a system’s design. Moreover, models with formal semantics allow verification based on rigorous methods, including the analysis of system safety. However, building formal models is a tedious manual process and cannot be easily applied to real problems. A key gap that hinders automation of model development is that there is no systematic way to connect system requirements with the activity of model-based design. In this article, we introduce a workflow to tackle this gap and ultimately automate the analysis of system safety using formal methods. We extend our previous work on boilerplate-based specification of system requirements with ontological semantics towards specifying FDIR (Failure, Detection, Isolation, Recovery) requirements. The workflow is centered around the automated generation of a model skeleton in SLIM, a component-based formal modeling language, from a set of ontology-based requirement specifications. Our approach has been implemented into a dedicated tool, which not only provides visualization of the ontology relations, but also supports traceability of the analysis findings back to the requirements specification. Finally, we provide results on the safety analysis of a real star-tracker system based on a SLIM model derived by minimally changing the auto-generated model skeleton.}
}
@article{MCDONNELL2023101764,
title = {Armchair citizenship and ontological insecurity: Uncovering styles of media and political behavior},
journal = {Poetics},
volume = {98},
pages = {101764},
year = {2023},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2023.101764},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X23000049},
author = {Terence E. McDonnell and Sarah M. Neitz and Marshall A. Taylor},
keywords = {Social media, Political participation, Ontological insecurity, Cultural styles, Latent class analysis},
abstract = {Media effects research has established a positive relationship between media and news consumption and political action—a “more-more” pattern. This paper identifies a coexistent “more-less” pattern in which more political engagement on social media is associated with limited political behavior offline. Traditional approaches that treat media behavior as an independent variable and political behavior as dependent miss this finding. Instead, this paper treats configurations of media and political behavior as styles, identified through latent class analysis (LCA). The data come from an original survey of American citizens about their encounters with political content on social media, their media habits, and their online expression of political attitudes to assess whether and how these media behaviors undermine their political and civic engagement offline. The latent class analysis of these data reveals three styles, which we label “actives,” “passives,” and “armchair citizens” (i.e., more-less). We then seek to make sense of this armchair citizen style—who would choose this style of media and political participation? An LCA regression model predicting membership into the armchair citizen class suggests that armchair citizen style is associated with experiencing what Giddens’ called “ontological insecurity.”}
}
@article{BZDOK2024698,
title = {Data science opportunities of large language models for neuroscience and biomedicine},
journal = {Neuron},
volume = {112},
number = {5},
pages = {698-717},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324000424},
author = {Danilo Bzdok and Andrew Thieme and Oleksiy Levkovskyy and Paul Wren and Thomas Ray and Siva Reddy},
abstract = {Large language models (LLMs) are a new asset class in the machine-learning landscape. Here we offer a primer on defining properties of these modeling techniques. We then reflect on new modes of investigation in which LLMs can be used to reframe classic neuroscience questions to deliver fresh answers. We reason that LLMs have the potential to (1) enrich neuroscience datasets by adding valuable meta-information, such as advanced text sentiment, (2) summarize vast information sources to overcome divides between siloed neuroscience communities, (3) enable previously unthinkable fusion of disparate information sources relevant to the brain, (4) help deconvolve which cognitive concepts most usefully grasp phenomena in the brain, and much more.}
}
@article{USIP2022487,
title = {Applied personal profile ontology for personnel appraisals},
journal = {International Journal of Web Information Systems},
volume = {18},
number = {56},
pages = {487-500},
year = {2022},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-03-2022-0048},
url = {https://www.sciencedirect.com/science/article/pii/S1744008422000064},
author = {Patience Usoro Usip and Edward N. Udo and Ini J. Umoeka},
keywords = {Semantic Web, Personal profile ontology, Personnel appraisal, Smart resume},
abstract = {Purpose
The purpose of this paper is to apply the earlier enhanced personal profile ontology (e-PPO) developed by the authors as a case study for the appraisal of the lecturers in the department of computer science, University of Uyo, Uyo for the purposes of promotions. The developed e-PPO was a sample smart résumé for the selection of the best among three personnel using linguistic variables and formal rules representing the combination of the criteria and subcriteria was illustrated which was used to allocate competent personnel for software requirement engineering tasks. The need for the use of the smart resume for appraisal purposes was pointed out in the conference paper, calling for the applicant’s data to be inputted into the enhanced personal profile ontology (e-PPO) for personnel appraisa.
Design/methodology/approach
Appraisal is a regular review of employees’ performances and their overall contribution to the organization they are working for. The availability of a web application for personnel appraisal requires PPO which includes both static and dynamic features. Personal profile is often modified for several purposes calling for augmentation and annotation when needs arise. Resume is one resulting extract from personal profile and often contain slightly different information based on needs. The urgent preparation of resume may introduce bias and incorrect information for the sole aim of projecting the personnel as being qualified for the available job. Religious and gender biases may sometimes be observed during appointments of new personnel, which may not be the case during appraisals for promotions or reassignment of tasks because such biases become insignificant given the fact that job targets and the skills needed are already set and the appraisals passes through several phases that are not determined by a single individual. This work therefore applied the earlier developed e-PPO for appraisal of the academic staff of the department of computer science, university of Uyo, Uyo, Nigeria. A mixed approach of existing ontologies like Methontology and Neon have been followed in the creation of the e-PPO, which is a constraint-based semantic data model tested using Protégé inbuilt reasoner with its updated plugins. Upon application of e-PPO on personnel appraisals, promotion and selection of employee for specific assignments in any organization is possible using the smart resume.
Findings
The use of the smart resume reduces the numerous task that would have been taken up by the human resource team, thereby reducing the processing time for the appraisals. The appraisal task is done void of biases of any kind such as gender and religion.
Originality/value
This work is an extension of the original work done by the authors.}
}
@article{VERDONCK2020101568,
title = {Comprehending 3D and 4D ontology-driven conceptual models: An empirical study},
journal = {Information Systems},
volume = {93},
pages = {101568},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101568},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300582},
author = {Michaël Verdonck and Frederik Gailly and Sergio de Cesare},
keywords = {Ontology, Conceptual modeling, Empirical study, Ontology-driven conceptual modeling, Experiment, UFO, BORO},
abstract = {This paper presents an empirical study that investigates the extent to which the pragmatic quality of ontology-driven models is influenced by the choice of a particular ontology, given a certain understanding of that ontology. To this end, we analyzed previous research efforts and distilled three hypotheses based on different metaphysical characteristics. An experiment based on two foundational ontologies (UFO and BORO) involving 158 participants was then carried out, followed by a protocol analysis to gain further insights into the results of experiment. We then extracted five derivations from the results of the empirical study in order to summarize our findings. Overall, the results confirm that the choice of a foundational ontology can lead to significant differences in the interpretation and comprehension of the conceptual models produced. Moreover, the effect of applying a certain foundational ontology can cause considerable variations in the effort required to comprehend these models.}
}
@article{LIN2025100868,
title = {Roles and Potential of Large Language Models in Healthcare: A Comprehensive Review},
journal = {Biomedical Journal},
pages = {100868},
year = {2025},
issn = {2319-4170},
doi = {https://doi.org/10.1016/j.bj.2025.100868},
url = {https://www.sciencedirect.com/science/article/pii/S2319417025000423},
author = {Chihung Lin and Chang-Fu Kuo},
keywords = {Large language models, healthcare, artificial intelligence, clinical decision support, patient communication},
abstract = {Large Language Models (LLMs) are capable of transforming healthcare by demonstrating remarkable capabilities in language understanding and generation. They have matched or surpassed human performance in standardized medical examinations and assisted in diagnostics across specialties like dermatology, radiology, and ophthalmology. LLMs can enhance patient education by providing accurate, readable, and empathetic responses, and they can streamline clinical workflows through efficient information extraction from unstructured data such as clinical notes. Integrating LLM into clinical practice involves user interface design, clinician training, and effective collaboration between Artificial Intelligence (AI) systems and healthcare professionals. Users must possess a solid understanding of generative AI and domain knowledge to assess the generated content critically. Ethical considerations to ensure patient privacy, data security, mitigating biases, and maintaining transparency are critical for responsible deployment. Future directions for LLMs in healthcare include interdisciplinary collaboration, developing new benchmarks that incorporate safety and ethical measures, advancing multimodal LLMs that integrate text and imaging data, creating LLM-based medical agents capable of complex decision-making, addressing underrepresented specialties like rare diseases, and integrating LLMs with robotic systems to enhance precision in procedures. Emphasizing patient safety, ethical integrity, and human-centered implementation is essential for maximizing the benefits of LLMs, while mitigating potential risks, thereby helping to ensure that these AI tools enhance rather than replace human expertise and compassion in healthcare.}
}
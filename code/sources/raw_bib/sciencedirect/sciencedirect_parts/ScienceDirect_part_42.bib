@article{GROZA2025100057,
title = {Realising the potential impact of artificial intelligence for rare diseases – A framework},
journal = {Rare},
volume = {3},
pages = {100057},
year = {2025},
issn = {2950-0087},
doi = {https://doi.org/10.1016/j.rare.2024.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2950008724000401},
author = {Tudor Groza and Chun-Hung Chan and David A. Pearce and Gareth Baynam},
keywords = {Rare diseases, Patient journey, Generative artificial intelligence, Diagnosis, Care coordination},
abstract = {Rare diseases (RD) are conditions affecting fewer than 1 in 2000 persons, with over 7000 largely genetic RDs affecting 3.5 %-5.9 % of the global population, or approximately 262.9–446.2 million people. The substantial healthcare burden and costs, such as the $1 trillion annual expense in the USA, highlight the urgent need for improved RD management. The International Rare Diseases Research Consortium (IRDiRC) addresses this need through global collaboration, aiming for timely and accurate diagnosis, development of 1000 new therapies, and methodologies to measure impact by 2027. IRDiRC's initiatives include biannual meetings and workshops, like the AI-focused workshop in October 2023. This identified AI as crucial for advancing RD research and proposed a Framework for AI to enhance the RD patient journey by addressing efficiency and quality of life through modular solutions mapped to critical stages. The Framework integrates diverse data sources to improve diagnosis, treatment, and impact assessment, reflecting a holistic, cross-sector approach. By guiding multi-stakeholder efforts, the Framework aims to harness AI’s potential to significantly improve rare disease care.}
}
@incollection{SINGH2024131,
title = {Chapter 6 - Biological interaction networks and their application for microbial pathogenesis},
editor = {Mohd. Tashfeen Ashraf and Abdul Arif Khan and Fahad M. Aldakheel},
booktitle = {Systems Biology Approaches for Host-Pathogen Interaction Analysis},
publisher = {Academic Press},
pages = {131-143},
year = {2024},
series = {Developments in Microbiology},
isbn = {978-0-323-95890-5},
doi = {https://doi.org/10.1016/B978-0-323-95890-5.00009-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323958905000090},
author = {Nirupma Singh and Sonika Bhatnagar},
keywords = {Host-pathogen interactions, biological network, centrality, cytoscape, R studio, host proteins, pathogen proteins, pathways, functional annotation, MorCVD},
abstract = {Host and pathogen molecular interactions are crucial for pathogenesis and are aptly portrayed as biological networks. Experimental proteomics tools have generated a large amount of data for common pathogens, and databases of host-pathogen interactions are available for their easy access. Several tools such as Cytoscape 3.5.1 and R studio are used for network construction, visualization, and calculation of topological parameters. Web servers like DAVID and KOBAS 3.0 servers are used for functional annotation and enrichment. Various biological features of critical nodes of the network can be annotated, for example, immune-related genes, essential genes, and host factors can be annotated using Reactome, DEG 10, and vhfRNAi databases, respectively, while virulence factors of pathogens can be predicted using VirulentPred. The network structure and properties of host-pathogen protein–protein interactions (HP-PPIs) can elucidate the pathways and mechanisms of microbial pathogenesis ontology, pathway, and crosstalk analysis. This chapter introduces the basic principles and structure of biological interaction networks. It further highlights the main databases and methods for HP-PPI determination and construction as well as analysis. Salient findings from host-pathogen interaction network studies are highlighted. With the help of a case study, we highlight how a correlation can be drawn between biological properties and network behavior of pathogen/host proteins leading to the inference of novel drug targets.}
}
@article{QURESHI2025108158,
title = {A survey on security enhancing Digital Twins: Models, applications and tools},
journal = {Computer Communications},
volume = {238},
pages = {108158},
year = {2025},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2025.108158},
url = {https://www.sciencedirect.com/science/article/pii/S014036642500115X},
author = {Abdul Rehman Qureshi and Adrián Asensio and Muhammad Imran and Jordi Garcia and Xavi Masip-Bruin},
keywords = {Digital Twins, CyberSecurity, IoT, Behavior modeling, Cyberphysical systems, Security applications, Security tools},
abstract = {A Digital Twin (DT) is a virtual representation of both cyber and physical objects. Initially, the concept was introduced to optimize manufacturing products, aimed at validating and verifying their behavior along their lifecycle. However, in the recent years and with the advent of Industry 4.0, it has also gained huge attention in the cybersecurity arena. Indeed, using a DT is seen as an strategy to automate, monitor, simulate, and diagnose cyber–physical systems behavior, aimed at making them to be secure. The DT can provide security in multiple domains such as manufacturing, aerospace, automotive, construction, smart grid, smart cities, and smart transportation. The fundamentals of a DT is a combination of data and a model of the system. To this end, this survey aims to explore current efforts on DT modeling techniques, and how these DT models capture the behavior of the system to enhance cybersecurity. The study focuses on the construction of behavioral models, techniques, tools, and technologies of DT used for specific security applications. The purpose of the discussion carried out in this survey is to illustrate the reader on how these behavioral models adapt to different security applications. In addition, the paper also analyzes and classifies current DT literature based on use cases, type of DT study, tools, and various DT operation modes with their specific security application. The findings of this survey describe security applications being enhanced by DT, and how the combination of key technologies facilitating Industry 4.0 (i.e., AI, IoT, Big Data and Cloud Computing, among others), are accelerating the adoption of DT as a solution for next generation security applications. Finally, this survey highlights relevant research gaps that need to be addressed before the wide adoption of DT in cybersecurity.}
}
@article{BILDSTEIN2019139,
title = {Combining Channel Theory and Semantic Web Technology to build up a Production Capability Matching Framework},
journal = {Procedia CIRP},
volume = {81},
pages = {139-144},
year = {2019},
note = {52nd CIRP Conference on Manufacturing Systems (CMS), Ljubljana, Slovenia, June 12-14, 2019},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.03.025},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119303300},
author = {Andreas Bildstein and Junkang Feng and Thomas Bauernhansl},
keywords = {Channel Theory, Capability Matching Framework, Semantic Web Technology},
abstract = {One of the main theories behind the worldwide ‘Factories of the Future’ movement is the idea that machines and systems, and thus the production system, should reach a certain level of self-organisation. We support this self-organisation of the production system by introducing a production capability matching framework that is based on an extended application of Channel Theory combined with the extensive use of Semantic Web technologies like OWL ontologies and rules based on SWRL. This approach shows that the combination of Channel Theory and Semantic Web technology is able to lay the groundwork for an automated equipment assignment process in global and local value chains.}
}
@article{BREITFUSS2021715,
title = {Representing emotions with knowledge graphs for movie recommendations},
journal = {Future Generation Computer Systems},
volume = {125},
pages = {715-725},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001953},
author = {Arno Breitfuss and Karen Errou and Anelia Kurteva and Anna Fensel},
keywords = {Knowledge graph, Artificial intelligence, Machine learning, Bias, Emotions, Ontology, Movies, Recommender system},
abstract = {Consumption of media, and movies in particular, is increasing and is influenced by a number of factors. One important and overlooked factor that affects the media consumption choices is the emotional state of the user and the decision making based on it. To include this factor in movie recommendation processes, we propose a knowledge graph representing human emotions in the domain of movies. The knowledge graph has been built by extracting emotions out of pre-existing movie reviews using machine learning techniques. To show how the knowledge graph can be used, a chatbot prototype has been developed. The chatbot’s reasoning mechanism derives movie recommendations for the user by combining the user’s emotions, which have been extracted from chat messages, with the knowledge graph. The developed approach for movie recommendations based on sentiment represented as a knowledge graph has been proven to be technically feasible, however, it requires more information about the emotions associated with the movies than currently available online.}
}
@article{KANAGASINGAM2022100137,
title = {Addressing the complexity of equitable care for larger patients: A critical realist framework},
journal = {SSM - Qualitative Research in Health},
volume = {2},
pages = {100137},
year = {2022},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2022.100137},
url = {https://www.sciencedirect.com/science/article/pii/S2667321522000993},
author = {Deana Kanagasingam},
keywords = {Critical realism, Healthcare professionals, Obesity, Patients, Social justice},
abstract = {The notion of obesity as a pathological state within the individual remains the dominant perspective in public health and biomedicine. However, there has been a growing call to re-examine this assumption from a social justice lens. Given that obesity is itself a contested term, there is a lack of consensus on what constitutes a social justice approach to addressing weight and health. Underpinning such debates amongst social justice researchers is a divide between realist and constructionist framings of obesity. The realist framing considers obesity to be a biomedical fact posing health and social consequences, and proposes collective, systems-based solutions to preventing and managing obesity. In contrast, the constructionist framing challenges the taken-for-granted assumptions that obesity is an epidemic and that fat necessarily signifies poor health. Despite such theorizing about social justice and obesity, to-date no empirical research has explored the views and experiences of 1) social justice-oriented healthcare practitioners who work with larger patients or 2) larger patients who receive social justice-informed care. This article features interviews with practitioners (n ​= ​22) across multiple professions in Canada who describe themselves as adopting a social justice approach to caring for larger patients, as well as with practitioners' patients (n ​= ​20) who self-identify as larger bodied. Drawing on a critical realist theoretical framework, the analysis uncovers the ontological and ideological assumptions driving participants' varying conceptualizations of obesity. To conclude, the article considers how participants’ different understandings of weight and health impact clinical practice and interactions, particularly in terms of whether social justice goals are fulfilled.}
}
@article{FATIMA2024466,
title = {Advanced network pharmacology and molecular docking-based mechanism study to explore the multi-target pharmacological mechanism of Cymbopogon citratus against Alzheimer's disease},
journal = {South African Journal of Botany},
volume = {165},
pages = {466-477},
year = {2024},
issn = {0254-6299},
doi = {https://doi.org/10.1016/j.sajb.2024.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0254629924000036},
author = {Kinza Fatima and Usman Ali Ashfaq and Muhammad {Tahir ul Qamar} and Muhammad Asif and Asma Haque and Muhammad Qasim and Mubarak A. Alamri and Ziyad Tariq Muhseen and Fatima Noor and Muhammad Sadaqat},
keywords = {Lemongrass, Alzheimer's disease, Network pharmacology, Network construction, Gene ontology, SRC, Molecular docking},
abstract = {Background
Alzheimer's disease (AD) is the most common type of dementia, accounting for at least two-thirds of cases of dementia in people age 65 and older. The limited amount of available data and multiple spectra of pathophysiological mechanisms of AD make it a challenging task and a serious economic load on the community health sector. Cymbopogon citratus is a tall perennial fast-growing grass with tuft of lemon-scented leaves from the annulate and sparingly branched rhizomes. A list of studies evidences the therapeutic efficacy of C. citratus against AD, but the precise molecular mechanism is yet to be discovered.
Aim of the study
This research utilizes network pharmacology and molecular docking approaches to elucidate the multi-target effects of C. Citratus on AD, identifying its active compounds, potential therapeutic targets, and associated signaling pathways.
Materials and methods
Initially the active compounds of C. citratus, the target genes related to both the C. citratus and the AD were retrieved from literature as well as databases. The PPI network of these target genes was constructed using STRING database and later the hub genes were screened by importing the network into the Cytoscape. The interactions among the target genes and the compounds were also analyzed using Cytoscape. The Gene Ontology (GO) and KEGG pathways of these hub genes were explored to reveal the genes involved in AD-related pathways. These compound-target, genes-pathway networks were explored which uncovered that bioactive compounds of C. citratus may serve as a magic bullet against AD by influencing the target genes involved in the disease pathogenesis. Further, the microarray data was analyzed to explore the expression level of core target genes. Lastly, docking analysis further strengthened the current findings by validating the effective activity of the bioactive compounds against putative target genes.
Results
Network pharmacology based analysis highlighted the pathways related to AD, the ten target genes, and the compounds involved in the AD disease mechanism. Through the analysis of microarray expression data, the two genes SRC and IL6 were found to have differential expression. Moreover, the molecular docking analysis predicted the string binding affinity among these genes and active compounds.
Conclusion
In summary, our study proposed that five key compounds catechin, apigenin, 8-prenylnaringenin, quercetin, and luteolin contributed to the development of AD by affecting IL6 and SRC genes. The overall integration of network pharmacology with molecular docking unveiled the multi-target pharmacological mechanisms of C. citratus against AD. This study provides convincing evidence that C. citratus might partially alleviate the AD and ultimately lays a foundation for further experimental research on the anti-AD activity of C. citratus.}
}
@article{JELISIC2022100385,
title = {A novel business context-based approach for improved standards-based systems integration—a feasibility study},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100385},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100385},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X2200053X},
author = {Elena Jelisic and Nenad Ivezic and Boonserm Kulvatunyou and Pavle Milosevic and Sladjan Babarogic and Zoran Marjanovic},
keywords = {Enterprise systems integration, Business context model, Digitalization, Data exchange standard development, Data exchange standard usage},
abstract = {Systems integration processes need to become more efficient and effective in order to allow enterprises to be nimbler and more responsive in today's dynamic markets. Systems integration typically depends on data exchange standards (DESes) and the associated DES usage specification that provides precise standard implementation requirements. However, there are significant inefficiencies in DES usage specification management today. Therefore, to achieve the objective of more responsive enterprises, DES usage specification management, particularly reuse, needs to advance. The Core Component Technical Specification carries the promise to advance the reuse by introducing the notions of Core Components (CCs), as DES building blocks, and Business Information Entities (BIEs), as DES usage specification. While the CCs idea has been successfully implemented in industry DES, the BIEs idea has been implemented only in a basic form, falling short of enabling the BIE reuse to its full potential. To realize the full potential of the BIE reuse, BIE development in industry standard usage needs to utilize the notion of business context better. In this paper, we reviewed existing business context models including UN/CEFACT Context Model (UCM), Enhanced UCM (E-UCM), and Business Context Ontology (BCOnt) and found that they were promising tools to improve the effectiveness of the BIE development and reuse. In addition to that contribution, this research took a closer look at E-UCM in particular. Two novel assessment criteria called expressiveness and effectiveness were defined. Using an industry use case and the two assessments, we showed short-comings of E-UCM such as semantic ambiguity and business rule disconnection. From there, improvements were outlined for future work to device them into E-UCM to enable a more efficient and effective BIE development and reuse process.}
}
@article{CHELLA20181,
title = {Knowledge acquisition through introspection in Human-Robot Cooperation},
journal = {Biologically Inspired Cognitive Architectures},
volume = {25},
pages = {1-7},
year = {2018},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2018.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X18300847},
author = {Antonio Chella and Francesco Lanza and Arianna Pipitone and Valeria Seidita},
keywords = {Cognitive agent, Knowledge acquisition, Ontology, Cognitive architecture, Introspection},
abstract = {When cooperating with a team including humans, robots have to understand and update semantic information concerning the state of the environment. The run-time evaluation and acquisition of new concepts fall in the critical mass learning. It is a cognitive skill that enables the robot to show environmental awareness to complete its tasks successfully. A kind of self-consciousness emerges: the robot activates the introspective mental processes inferring if it owns a domain concept or not, and correctly blends the conceptual meaning of new entities. Many works attempt to simulate human brain functions leading to neural network implementation of consciousness; regrettably, some of these produce accurate model that however do not provide means for creating virtual agents able to interact with a human in a teamwork in a human-like fashion, hence including aspects such as self-conscious abilities, trust, emotions and motivations. We propose a method that, based on a cognitive architecture for human-robot teaming interaction, endows a robot with the ability to model its knowledge about the environment it is interacting with and to acquire new knowledge when it occurs.}
}
@article{ALTNER2022538,
title = {Improving engineering change management by introducing a standardised description for engineering changes for the automotive wiring harness},
journal = {Procedia CIRP},
volume = {109},
pages = {538-543},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.291},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007405},
author = {Moritz Altner and Hans Redinger and Benjamin Valeh and Eder Kevin and Jonas Neckenich and Simon Rapp and Roland Winter and Albert Albers},
keywords = {Engineering Change Management, Standardised Engineering Change Description, Automotive Wiring Harness, Product Development},
abstract = {Engineering change management is a key part in the development of products that requires a lot of resources and time. A key problem is the lack of a shared ontology to describe engineering changes. This creates problems, additional effort and hinders the digitalisation of the engineering change management. This is especially true for the development of the automotive wiring harness where a low degree of automation together with the occurrence of many changes in a multi-variant system poses a big challenge. A description that is unambiguous, comprehensive and coherent is needed. The research presented in this paper tackles this problem. A standardised description for the engineering change management for the automotive wiring harness is introduced in this publication. The authors outline the approach that has been used to create a systematic description. The validation of the standardised description is based on two approaches: a case study of a development project and an ongoing development project. The validation shows that 94% of all engineering changes can be described in the proposed standardised way. Concepts where the standardised descriptions can be used to improve the engineering change process are outlined at the end of the paper. The paper thereby presents a way that directly improves the engineering change process and the product development process. It enables the further improvement of the engineering change management by providing a basis for an automatic processing, evaluation and implementation of engineering changes.}
}
@incollection{PIRRO2019877,
title = {Semantic Similarity Functions and Measures},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {877-888},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20402-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338204020},
author = {Giuseppe Pirrò},
keywords = {Gene ontology, Information content, Intrinsic information content, MeSH, Normalized google distance, PMI-IR, Semantic relatedness, Semantic similarity, Similarity library, WordNet},
abstract = {Similarity is the process through which two or more objects are compared in order to identify to what extent they are alike. This is useful, for instance, when there is the need to identify common elements among a set of objects or when a new object has to be classified w.r.t. the others. Similarity is a very generic notion spanning among different disciplines from psychology to computer science. In this paper, the focus will be on computational methods that exploit ontologies or search engines as primary source of “background” knowledge. In particular, after surveying on the most popular similarity measures recently proposed, the Similarity Library will be presented, which has the aim to provide researchers and practitioners with a flexible tool encompassing several similarity measures both between words and sentences.}
}
@article{OSHAUGHNESSY2024104528,
title = {The recovery experiences of homeless service users with substance use disorder: A systematic review and qualitative meta-synthesis},
journal = {International Journal of Drug Policy},
volume = {130},
pages = {104528},
year = {2024},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2024.104528},
url = {https://www.sciencedirect.com/science/article/pii/S0955395924002135},
author = {Branagh R. O'Shaughnessy and Paula Mayock and Aimen Kakar},
keywords = {homelessness, Substance use disorder, Recovery, Systematic review, Qualitative meta-synthesis},
abstract = {Background
The relationship between homelessness and substance use disorder (SUD) is layered and complex. Adults pursuing recovery while dealing with homelessness and SUD face many challenges. Little research has inspected qualitative first-person accounts of recovery in the context of homelessness and SUD, and few studies have employed conceptualisations of recovery beyond abstinence. In this systematic review study, we examine the qualitative literature on the recovery experiences of adult homeless service users with SUD.
Methods
2,042 records were identified via database and secondary searching strategy. After title and abstract and full text screening, 15 eligible studies remained. Critical Appraisal Skills Programme quality appraisal criteria was used to assess potential bias in the studies. Meta-ethnography was employed to synthesise extracted data.
Results
Four themes were generated from the extracted data: Two sides of the Service Coin; Navigating Relationships; Recovery Practices and Personal Attributes; and Housing as Foundational for Recovery.
Conclusion
Unconditional housing, a broad array of supports, opportunities to contribute to society, and family reunification supports all facilitate the development of recovery for adults with SUD experiencing homelessness. Implications for policy are discussed.}
}
@article{FAVIEZ2025106021,
title = {Enhancing rare disease detection with deep phenotyping from EHR narratives: evaluation on Jeune syndrome},
journal = {International Journal of Medical Informatics},
volume = {203},
pages = {106021},
year = {2025},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2025.106021},
url = {https://www.sciencedirect.com/science/article/pii/S1386505625002382},
author = {Carole Faviez and Xiaomeng Wang and Marc Vincent and Nicolas Garcelon and Sophie Saunier and Valérie Cormier-Daire and Xiaoyi Chen and Anita Burgun},
keywords = {Diagnosis support, Rare disease, Electronic health record, Supervised machine learning, Named entity recognition, Deep phenotyping},
abstract = {Background
Patients with rare diseases frequently experience misdiagnoses and long diagnostic delays. Accelerating their diagnosis is essential to ensure timely access to appropriate care. Given the increasing availability of EHRs, combining artificial intelligence and deep phenotyping from large-scale clinical databases offers a promising approach to identify undiagnosed patients. This study assesses the impact of improved phenotype extraction on a screening algorithm for Jeune syndrome, a rare ciliopathy characterized by skeletal abnormalities.
Methods
Phenotypes from Jeune syndrome patients and controls were automatically extracted from patient unstructured EHRs relying on two thesauri separately: the standard UMLS Metathesaurus and the UMLS+, an enhanced version incorporating additional terms identified through deep learning. The machine learning pipeline that we designed for classifying patients with renal ciliopathy was adapted for Jeune syndrome detection. The model was trained and tested on both the datasets created using the two phenotyping strategies.
Results
Using UMLS+ strongly improved the classification of patients with Jeune syndrome, increasing the sensitivity from 49 % to 95 % while maintaining a 90 % specificity. The review of a subset of misclassified controls showed that most of them (69 %) had other genetic skeletal disorders, indicating that the model also captured patients who would benefit from referral to a bone disease geneticist.
Conclusion
AI-based screening combined with high-quality deep phenotyping can help reduce diagnostic delay in rare diseases. The completeness and accuracy of phenotyping from EHRs have a strong impact on screening performances.}
}
@article{HIMLEY2021102373,
title = {The future lies beneath: Mineral science, resource-making, and the (de)differentiation of the Peruvian underground},
journal = {Political Geography},
volume = {87},
pages = {102373},
year = {2021},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2021.102373},
url = {https://www.sciencedirect.com/science/article/pii/S0962629821000330},
author = {Matthew Himley},
keywords = {Earth science, Mining, Peru, Resources, Territory},
abstract = {Engaging with recent scholarship on the relations among science, resources, and territory, this paper examines the role of late-nineteenth-century mineral science in the rendering of the Peruvian underground as a knowable and actionable space. I focus on the mineralogical work of Italian-born naturalist Antonio Raimondi, identifying in it a dual dynamic of geo-political differentiation and ontological dedifferentiation. On the one hand, Raimondi constructs Peruvian national territory as distinct due to the abundance and diversity of minerals contained in its subsurface. On the other, his mineralogy was undergirded by the idea that Peru's minerals were ontologically equivalent to minerals anywhere else, and thus knowable through ‘universal’ scientific knowledge practices. Both aspects of Raimondi's mineral science reflected an underlying aim: stimulating the ‘rebirth’ of Peruvian mining. Yet, I suggest that the influence of Raimondi's mineralogical work on the history of resource-making in Peru lay not in its immediate utility for political-economic calculation, but rather in its contribution to an imaginary of Peru as mineral-rich and underexploited. In conclusion, I call attention to the need for research on the political economies of Earth science to approach science as an historically and geographically situated practice, to attend to the multiple (e.g., calculative and symbolic) dimensions of scientific activities, and to be attentive to ways in which the logics and aims of Earth science may not fully intersect with those of capital and the state.}
}
@article{KERSLOOT2021103897,
title = {De-novo FAIRification via an Electronic Data Capture system by automated transformation of filled electronic Case Report Forms into machine-readable data},
journal = {Journal of Biomedical Informatics},
volume = {122},
pages = {103897},
year = {2021},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103897},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421002264},
author = {Martijn G. Kersloot and Annika Jacobsen and Karlijn H.J. Groenen and Bruna {dos Santos Vieira} and Rajaram Kaliyaperumal and Ameen Abu-Hanna and Ronald Cornet and Peter A.C. {‘t Hoen} and Marco Roos and Leo {Schultze Kool} and Derk L. Arts},
keywords = {Electronic Case Report Forms, FAIR Data, Machine-readable data, Interoperability, Patient registry},
abstract = {Introduction
Existing methods to make data Findable, Accessible, Interoperable, and Reusable (FAIR) are usually carried out in a post hoc manner: after the research project is conducted and data are collected. De-novo FAIRification, on the other hand, incorporates the FAIRification steps in the process of a research project. In medical research, data is often collected and stored via electronic Case Report Forms (eCRFs) in Electronic Data Capture (EDC) systems. By implementing a de novo FAIRification process in such a system, the reusability and, thus, scalability of FAIRification across research projects can be greatly improved. In this study, we developed and implemented a novel method for de novo FAIRification via an EDC system. We evaluated our method by applying it to the Registry of Vascular Anomalies (VASCA).
Methods
Our EDC and research project independent method ensures that eCRF data entered into an EDC system can be transformed into machine-readable, FAIR data using a semantic data model (a canonical representation of the data, based on ontology concepts and semantic web standards) and mappings from the model to questions on the eCRF. The FAIRified data are stored in a triple store and can, together with associated metadata, be accessed and queried through a FAIR Data Point. The method was implemented in Castor EDC, an EDC system, through a data transformation application. The FAIRness of the output of the method, the FAIRified data and metadata, was evaluated using the FAIR Evaluation Services.
Results
We successfully applied our FAIRification method to the VASCA registry. Data entered on eCRFs is automatically transformed into machine-readable data and can be accessed and queried using SPARQL queries in the FAIR Data Point. Twenty-one FAIR Evaluator tests pass and one test regarding the metadata persistence policy fails, since this policy is not in place yet.
Conclusion
In this study, we developed a novel method for de novo FAIRification via an EDC system. Its application in the VASCA registry and the automated FAIR evaluation show that the method can be used to make clinical research data FAIR when they are entered in an eCRF without any intervention from data management and data entry personnel. Due to the generic approach and developed tooling, we believe that our method can be used in other registries and clinical trials as well.}
}
@article{KROBER2025552,
title = {On-demand, semantic EO data cubes – knowledge-based, semantic querying of multimodal data for mesoscale analyses anywhere on Earth},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {228},
pages = {552-565},
year = {2025},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2025.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S092427162500276X},
author = {Felix Kröber and Martin Sudmanns and Lorena Abad and Dirk Tiede},
keywords = {Earth observation, Remote sensing, Big data analyses, Data cubes, Semantic querying},
abstract = {With the daily increasing amount of available Earth Observation (EO) data, the importance of processing frameworks that allow users to focus on the actual analysis of the data instead of the technical and conceptual complexity of data access and integration is growing. In this context, we present a Python-based implementation of ad-hoc data cubes to perform big EO data analysis in a few lines of code. In contrast to existing data cube frameworks, our semantic, knowledge-based approach enables data to be processed beyond its simple numerical representation, with structured integration and communication of expert knowledge from the relevant domains. The technical foundations for this are threefold: Firstly, on-demand fetching of data in cloud-optimized formats via SpatioTemporal Asset Catalog (STAC) standardized metadata to regularized three-dimensional data cubes. Secondly, provision of a semantic language along with an analysis structure that enables to address data and create knowledge-based models. And thirdly, chunking and parallelization mechanisms to execute the created models in a scalable and efficient manner. From the user’s point of view, big EO data archives can be analyzed both on local, commercially available devices and on cloud-based processing infrastructures without being tied to a specific platform. Visualization options for models enable effective exchange with end users and domain experts regarding the design of analyses. The concrete benefits of the presented framework are demonstrated using two application examples relevant for environmental monitoring: querying cloud-free data and analyzing the extent of forest disturbance areas.}
}
@incollection{FERMULLER2022373,
title = {Chapter 11 - Learning for action-based scene understanding},
editor = {E.R. Davies and Matthew A. Turk},
booktitle = {Advanced Methods and Deep Learning in Computer Vision},
publisher = {Academic Press},
pages = {373-403},
year = {2022},
series = {Computer Vision and Pattern Recognition},
isbn = {978-0-12-822109-9},
doi = {https://doi.org/10.1016/B978-0-12-822109-9.00020-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128221099000205},
author = {Cornelia Fermüller and Michael Maynord},
keywords = {Affordances, Actions, Activities, Action-based representations, Vector space embeddings},
abstract = {Action plays a central role in our lives and environments, yet most Computer Vision methods do not explicitly model action. In this chapter we outline an action-centric framework which spans multiple time scales and levels of abstraction, producing both action and scene interpretations constrained towards action consistency. At the lower level of the visual hierarchy we detail affordances – object characteristics which afford themselves to different actions. At mid-levels we model individual actions, and at higher levels we model activities through leveraging knowledge and longer term temporal relations. We emphasize the use of grasp characteristics, geometry, ontologies, and physics based constraints for generalizing to new scenes. Such explicit representations avoid overtraining on appearance characteristics. To integrate signal based perception with symbolic knowledge we align vectorized knowledge with visual features. We finish with a discussion on action and activity understanding, and discuss implications for future work.}
}
@article{MISIRLI20191548,
title = {A Computational Workflow for the Automated Generation of Models of Genetic Designs},
journal = {ACS Synthetic Biology},
volume = {8},
number = {7},
pages = {1548-1559},
year = {2019},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.7b00459},
url = {https://www.sciencedirect.com/science/article/pii/S2161506319001438},
author = {Göksel Misirli and Tramy Nguyen and James Alastair McLaughlin and Prashant Vaidyanathan and Timothy S. Jones and Douglas Densmore and Chris Myers and Anil Wipat},
keywords = {SBOL, SBML, data standards, VPR API, modeling, genetic design automation},
abstract = {Computational models are essential to engineer predictable biological systems and to scale up this process for complex systems. Computational modeling often requires expert knowledge and data to build models. Clearly, manual creation of models is not scalable for large designs. Despite several automated model construction approaches, computational methodologies to bridge knowledge in design repositories and the process of creating computational models have still not been established. This paper describes a workflow for automatic generation of computational models of genetic circuits from data stored in design repositories using existing standards. This workflow leverages the software tool SBOLDesigner to build structural models that are then enriched by the Virtual Parts Repository API using Systems Biology Open Language (SBOL) data fetched from the SynBioHub design repository. The iBioSim software tool is then utilized to convert this SBOL description into a computational model encoded using the Systems Biology Markup Language (SBML). Finally, this SBML model can be simulated using a variety of methods. This workflow provides synthetic biologists with easy to use tools to create predictable biological systems, hiding away the complexity of building computational models. This approach can further be incorporated into other computational workflows for design automation.
}
}
@article{CESCO2024105015,
title = {Smart management of emergencies in the agricultural, forestry, and animal production domain: Tackling evolving risks in the climate change era},
journal = {International Journal of Disaster Risk Reduction},
volume = {114},
pages = {105015},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.105015},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924007775},
author = {Stefano Cesco and Davide Ascoli and Lucia Bailoni and Gian Battista Bischetti and Pietro Buzzini and Monica Cairoli and Luisella Celi and Giuseppe Corti and Marco Marchetti and Giacomo Scarascia Mugnozza and Simone Orlandini and Andrea Porceddu and Giovanni Gigliotti and Fabrizio Mazzetto},
keywords = {Climate change, Sustainability, Emergency, Agriculture, Forestry, Livestock},
abstract = {The agricultural, forestry, and animal production domain (AFA domain) plays an essential role in meeting global needs and supporting livelihoods while facing escalating challenges from climate change-induced impacts and extreme natural events. This perspective advocates for urgent strategies to enhance resilience through effective emergency management and prevention measures tailored to this critical domain. The analysis here exposed, which includes elements of ontology and the conceptual approach of an emergency management system encompassing both restoration and prevention aspects, entails three case studies across the AFA domain. Each case study, described by location, timing, nature, and consequences, critically evaluates the implemented risk prevention measures, details the emergency and recovery actions, and highlights shortcomings in response efforts. The analysis, incorporating a retrospective comparative component based on the proposed conceptual model, highlights the importance of identifying lessons learned and potential future applications. It emphasizes the urgent need for a well-structured emergency management strategy that integrates risk mapping and advanced technology to ensure timely and effective responses. The active engagement of domain professionals (agronomists, foresters, animal production doctors) and scholars of AFA domain sciences, as either farm owners or technical advisors, is crucial to optimize intervention strategies. This engagement is especially important for enhancing resilience during recovery phases, aligning with the best international practices such as making use of local knowledge and citizen engagement strategies. Comprehensive training initiatives, also adopting innovative formats and tools including micro-credentials, e-learning platforms, and the applications of generative Artificial Intelligence for learning assistance, as well as new research insights are strategic for coordinated and effective emergency responses across all stakeholders. Collaboration between the different production systems and areas of expertise, raising awareness of the distinction between Civil Protection and Production Protection and fostering their close interconnection, is essential for effective emergency response and long-term resilience.}
}
@article{QI201887,
title = {A categorical framework for formalising knowledge in additive manufacturing},
journal = {Procedia CIRP},
volume = {75},
pages = {87-91},
year = {2018},
note = {The 15th CIRP Conference on Computer Aided Tolerancing, CIRP CAT 2018, 11-13 June 2018, Milan, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.04.076},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118305924},
author = {Qunfen Qi and Luca Pagani and Paul J Scott and Xiangqian Jiang},
keywords = {Additive Manufacturing (AM), geometrical variability, process parameters},
abstract = {Additive manufacturing (AM) changes the way products are designed, manufactured and measured. It enables the fabrication of components with complex geometries and customisable material properties. However traditional design rules or guidelines are no longer applicable for AM. As a result design for additive manufacturing lacks of formal and structured design principles and guidelines. It urges a comprehensive system that can help designers and engineers understand for example how the geometrical design and process parameters will affect each other, and how to configure process parameters to meet specifications. In this paper a set of category ontologies has been developed to formalise fundamental/general knowledge of design and process for AM. A collection of design guidelines and rules are encapsulated and modelled into categorical structures. The formalisation of knowledge of AM will enable existing fundamental/general knowledge of AM process and state-of-the-art designing cases computer-readable and to be interrogated and reasoned, and then can be integrated into CAx platforms.}
}
@article{RODLER2023251,
title = {DynamicHS: Streamlining Reiter’s Hitting-Set Tree for Sequential Diagnosis},
journal = {Information Sciences},
volume = {627},
pages = {251-279},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522009124},
author = {Patrick Rodler},
keywords = {Reiter’s hitting set tree, Model-based diagnosis, Diagnosis computation, Sequential diagnosis, Diagnostic search, Ontology debugging},
abstract = {Given a system that does not work as expected, sequential diagnosis aims at suggesting a series of system measurements to isolate the true explanation for the system’s misbehavior from a potentially large set of possible explanations. To reason about the best next measurement, sequential diagnosis methods usually require a sample of possible fault explanations at each step of the iterative diagnostic process. The computation of this sample can be accomplished by various diagnostic search algorithms. Among those, Reiter’s HS-Tree is one of the most popular due to its desirable properties and general applicability. Usually, HS-Tree is used in a stateless fashion throughout the diagnosis process to (re)compute a sample of possible fault explanations per iteration, each time given the latest (updated) system knowledge including all so-far collected measurements. At this, the built search tree is discarded between two iterations, albeit often large parts of the tree have to be rebuilt in the next iteration, involving redundant operations and calls to costly reasoning services. As a remedy to this, we propose DynamicHS, a variant of HS-Tree that maintains state throughout the diagnostic session and embraces special strategies to minimize the number of expensive reasoner invocations. DynamicHS provides an answer to a longstanding question posed by Raymond Reiter in his seminal paper from 1987, where he wondered if there is a reasonable strategy to reuse an existing search tree to compute fault explanations after new system information is obtained. We conducted extensive evaluations on real-world diagnosis problems from the domain of knowledge-based systems—a field where the usage of HS-Tree is state-of-the-art—under various diagnosis scenarios in terms of the number of fault explanations computed and the heuristic for measurement selection used. The results prove the reasonability of the novel approach and testify its clear superiority to HS-Tree wrt.computation time. More specifically: (1) DynamicHS required less time than HS-Tree in 96% of the executed sequential diagnosis sessions. (2) DynamicHS exhibited substantial and statistically significant time savings over HS-Tree in most scenarios, with median and maximal savings of 52% and 75%, respectively. (3) The relative amount of saved time appears to neither depend on the number of computed fault explanations nor on the used measurement selection heuristic. (4) In the hardest (most time-intensive) cases per diagnosis scenario, DynamicHS achieved even higher savings than on average, and could avoid median and maximal time overheads of over 175% and 800%, respectively, as opposed to a usage of HS-Tree. Remarkably, DynamicHS achieves these performance improvements while preserving all desirable properties as well as the general applicability of HS-Tree.}
}
@article{WHITE2025S30,
title = {S15-03 The Utility of Exposure led NGRA case studies – challenges and progress in integrating non-animal data, tools and models for human relevant safety assessments},
journal = {Toxicology Letters},
volume = {411},
pages = {S30},
year = {2025},
note = {Abstracts of the 59th Congress of the European Societies of Toxicology (EUROTOX 2025) TOXICOLOGY ADDRESSES SOCIETY'S REAL LIFE RISKS FOR SUSTAINABLE HEALTH AND WELL BEING},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2025.07.093},
url = {https://www.sciencedirect.com/science/article/pii/S0378427425016765},
author = {A. White},
abstract = {No abstract has been submitted.}
}
@article{JU2018881,
title = {Citizen-centered big data analysis-driven governance intelligence framework for smart cities},
journal = {Telecommunications Policy},
volume = {42},
number = {10},
pages = {881-896},
year = {2018},
note = {Smart Cities: Governance and Economics},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2018.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0308596117301556},
author = {Jingrui Ju and Luning Liu and Yuqiang Feng},
keywords = {Citizen-centered big data, Governance intelligence, Smart cities, Data-analysis algorithm, Data merging, Citizen profile, Citizen persona, Ontology model},
abstract = {Sensors and systems within rapidly expanding smart cities produce citizen-centered big data which have potential value to support citizen-centered urban governance decision-making. There exists a wealth of extant conceptual studies, however, further operational studies are needed to establish a specific path towards implementation of such data to governance decision-making with analytical algorithms that are appropriate for each step of the path. This paper proposes a framework for the use of citizen-centered big data analysis to drive governance intelligence in smart cities from two perspectives: urban governance issues and data-analysis algorithms. The framework consists of three layers: 1) A data-merging layer, which builds a citizen-centered panoramic data set for each citizen by merging citizen-related big data from multiple sources in collaborative urban governance via similarity calculation and conflict resolution; 2) a knowledge-discovery layer, which plots the citizen profile and citizen persona at both individual and group levels in terms of urban public service delivery and citizen participation via simple statistical analysis techniques, machine learning, and econometrics methods; and 3) a decision-making layer, which uses ontology models to standardize urban governance-related attributes, personas, and associations to support governance decision-making via data mining and Bayesian Net techniques. Finally, the proposed framework is validated in a case study on blood donation governance in China. This research highlights the value of citizen-centered big data, pushes data-to-decision research from conceptual to operational, synthesizes previously published frameworks for citizen-centered big data analysis in smart cities, and enhances the mutual supplement cross multiple disciplinaries.}
}
@article{WESTPHAL2022108233,
title = {Spatial concept learning and inference on geospatial polygon data},
journal = {Knowledge-Based Systems},
volume = {241},
pages = {108233},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108233},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122000673},
author = {Patrick Westphal and Tobias Grubenmann and Diego Collarana and Simon Bin and Lorenz Bühmann and Jens Lehmann},
keywords = {Spatial analytics, Concept learning, Description logics, Spatial knowledge graphs},
abstract = {Geospatial knowledge has always been an essential driver for many societal aspects. This concerns in particular urban planning and urban growth management. To gain insights from geospatial data and guide decisions usually authoritative and open data sources are used, combined with user or citizen sensing data. However, we see a great potential for improving geospatial analytics by combining geospatial data with the rich terminological knowledge, e.g., provided by the Linked Open Data Cloud. Having semantically explicit, integrated geospatial and terminological knowledge, expressed by means of established vocabularies and ontologies, cross-domain spatial analytics can be performed. One analytics technique working on terminological knowledge is inductive concept learning, an approach that learns classifiers expressed as logical concept descriptions. In this paper, we extend inductive concept learning to infer and make use of the spatial context of entities in spatio-terminological data. We propose a formalism for extracting and making spatial relations explicit such that they can be exploited to learn spatial concept descriptions, enabling ‘spatially aware’ concept learning. We further provide an implementation of this formalism and demonstrate its capabilities in different evaluation scenarios.}
}
@article{TRAN201948,
title = {Gating mechanism based Natural Language Generation for spoken dialogue systems},
journal = {Neurocomputing},
volume = {325},
pages = {48-58},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.09.069},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218311500},
author = {Van-Khanh Tran and Le-Minh Nguyen},
keywords = {Natural Language Generation, Gating mechanism, Attention mechanism, Recurrent Neural Network, Gated recurrent unit, Dialogue system},
abstract = {Recurrent Neural Network (RNN) based approaches have recently shown promising in tackling Natural Language Generation (NLG) problems. This paper presents an approach to leverage gating mechanisms, in which we incrementally propose three additional semantic cells into a traditional RNN model: a Refinement cell to filter the sequential inputs before RNN computations, an Adjustment cell, and an Output cell to select semantic elements and gate a feature vector during generation. The proposed gating-based generators can learn from unaligned data by jointly training both sentence planning and surface realization to generate natural language utterances. We conducted extensive experiments on four different NLG domains in which the results empirically show that the proposed methods not only achieved better performance on all the NLG domains in comparison with previous gating-based, attention-based methods, but also obtained highly competitive results compared to a hybrid generator.}
}
@article{DALY20183,
title = {Reproducible model development in the cardiac electrophysiology Web Lab},
journal = {Progress in Biophysics and Molecular Biology},
volume = {139},
pages = {3-14},
year = {2018},
note = {Quantitative Systems Pharmacology (QSP): Methods and Tools},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2018.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0079610718300257},
author = {Aidan C. Daly and Michael Clerx and Kylie A. Beattie and Jonathan Cooper and David J. Gavaghan and Gary R. Mirams},
abstract = {The modelling of the electrophysiology of cardiac cells is one of the most mature areas of systems biology. This extended concentration of research effort brings with it new challenges, foremost among which is that of choosing which of these models is most suitable for addressing a particular scientific question. In a previous paper, we presented our initial work in developing an online resource for the characterisation and comparison of electrophysiological cell models in a wide range of experimental scenarios. In that work, we described how we had developed a novel protocol language that allowed us to separate the details of the mathematical model (the majority of cardiac cell models take the form of ordinary differential equations) from the experimental protocol being simulated. We developed a fully-open online repository (which we termed the Cardiac Electrophysiology Web Lab) which allows users to store and compare the results of applying the same experimental protocol to competing models. In the current paper we describe the most recent and planned extensions of this work, focused on supporting the process of model building from experimental data. We outline the necessary work to develop a machine-readable language to describe the process of inferring parameters from wet lab datasets, and illustrate our approach through a detailed example of fitting a model of the hERG channel using experimental data. We conclude by discussing the future challenges in making further progress in this domain towards our goal of facilitating a fully reproducible approach to the development of cardiac cell models.}
}
@article{ZHANG2024119434,
title = {A knowledge graph-based inspection items recommendation method for port state control inspection of LNG carriers},
journal = {Ocean Engineering},
volume = {313},
pages = {119434},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119434},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824027720},
author = {Xiyu Zhang and Chengyong Liu and Yi Xu and Beiyan Ye and Langxiong Gan and  {Yaqing Shu}},
keywords = {Port state control inspection, Liquefied natural gas carriers, Knowledge graph, Knowledge graph embedding, Knowledge graph-based recommendation model},
abstract = {The safe navigation and operation of Liquified natural gas (LNG) carriers is crucial in maritime transportation. The Port State Control (PSC) inspection is the primary method for identifying deficiencies in vessels. Different from other vessels, the PSC inspection of LNG carriers requires knowledge of specialized transport equipment. Therefore, effective management of inspection knowledge and precise targeting of inspection items are particularly crucial. Knowledge graph is an efficient way to manage knowledge in the context of artificial intelligence. In this study, an LNG carrier PSC inspection knowledge graph is constructed for fusing multisource knowledge data from PSC inspection. Additionally, a knowledge graph-based recommendation model, namely the improved knowledge graph convolutional network combined with pretrained translation embedding (PT-KGCN), is developed to recommend inspection items and knowledge. The PT-KGCN model first carries out knowledge graph embedding. It then predicts possible deficiencies on the basis of historical PSC inspection data. Finally, it recommends inspection items by correlating the predicted deficiencies with knowledge from the knowledge graph. The results show that more than 87% of defective items in historical data are correctly predicted. The research findings can provide ideas for the practical application of knowledge data in the inspection fields.}
}
@incollection{POLANSKI2020635,
title = {4.26 - Chemoinformatics☆},
editor = {Steven Brown and Romà Tauler and Beata Walczak},
booktitle = {Comprehensive Chemometrics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {635-676},
year = {2020},
isbn = {978-0-444-64166-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.14327-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472143276},
author = {J. Polanski},
keywords = {Big data, Chemical ontology, Chemoinformatics, Database mining, Drug design, In silico chemistry, Internet, Materials discovery, Molecular design, Molecular modeling, Retro-synthesis, Synthons},
abstract = {Chemometrics originated from analytical chemistry as one of the first organized fields of computer applications in chemistry. With the greater and greater potential of informatics in silico chemistry has significantly increased the scope of interest and the available field of investigations. Although chemoinformatics is a branch of chemical science at the lexical level, chemistry changes its role from a root to a modifier. Does this relation have any deeper meaning or should we consider this merely to a play on words? Actually, the broadest definition of chemoinformatics today can simply be “in silico chemistry.” Interestingly, however, at its origins, the scope of this discipline was not so broadly designed. Brown, who coined the term in 1988, focused solely on the “the combination of all the information resources that a scientist needs to optimize the properties of a ligand to become a drug.” In the 1990s, chemistry began to be slightly old fashioned. In turn, the potential, reputation and popularity computers was steadily increasing. Currently, chemoinformatics focuses on drug design, molecular engineering and organic chemistry. The application of computer assisted methods for molecular manipulation and prediction, synthesis design and property oriented synthesis are illustrative examples that are discussed in this chapter. Big data (“Big Data: A New Opportunity and Challenge” section) and chemical ontology (“Structuring Chemical Data Into an Ontology” section) are examples of new trends.}
}
@article{LEONIDOU2025,
title = {Genome-scale metabolic model of Staphylococcus epidermidis ATCC 12228 matches in vitro conditions},
journal = {mSystems},
volume = {10},
number = {6},
year = {2025},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.00418-25},
url = {https://www.sciencedirect.com/science/article/pii/S2379507725001242},
author = {Nantia Leonidou and Alina Renz and Benjamin Winnerling and Anastasiia Grekova and Fabian Grein and Andreas Dräger},
keywords = {genome-scale metabolic modeling, , Gram positive, skin mcirobiota, nasal microbiota, systems biology, flux balance analysis, linear programming, SBML, FAIR principles},
abstract = {ABSTRACT

Staphylococcus epidermidis, a commensal bacterium inhabiting collagen-rich areas like human skin, has gained significance due to its probiotic potential in the nasal microbiome and as a leading cause of nosocomial infections. While infrequently leading to severe illnesses, S. epidermidis exerts a significant influence, particularly in its close association with implant-related infections and its role as a classic opportunistic biofilm former. Understanding its opportunistic nature is crucial for developing novel therapeutic strategies, addressing both its beneficial and pathogenic aspects, and alleviating the burdens it imposes on patients and healthcare systems. Here, we employ genome-scale metabolic modeling as a powerful tool to elucidate the metabolic capabilities of S. epidermidis. We created a comprehensive computational resource for understanding the organism’s growth conditions within diverse habitats by reconstructing and analyzing a manually curated and experimentally validated metabolic model. The final network, iSep23, incorporates 1,415 reactions, 1,051 metabolites, and 705 genes, adhering to established community standards and modeling guidelines. Benchmarking with the Metabolic Model Testing suite yields a high score, indicating the model’s remarkable semantic quality. Following the findable, accessible, interoperable, and reusable (FAIR) data principles, iSep23 becomes a valuable and publicly accessible asset for subsequent studies. Growth simulations and carbon source utilization predictions align with experimental results, showcasing the model’s predictive power. Ultimately, this work provides a robust foundation for future research aimed at both exploiting the probiotic potential and mitigating the pathogenic risks posed by S. epidermidis.
IMPORTANCE
Staphylococcus epidermidis, a bacterium commonly found on human skin, has shown probiotic effects in the nasal microbiome and is a notable causative agent of hospital-acquired infections. While these infections are typically non-life-threatening, their economic impact is considerable, with annual costs reaching billions of dollars in the United States. To better understand its opportunistic nature, we employed genome-scale metabolic modeling to construct a detailed network of S. epidermidis’s metabolic capabilities. This model, comprising over a thousand reactions, metabolites, and genes, adheres to established standards and demonstrates solid benchmarking performance. Following the findable, accessible, interoperable, and reusable (FAIR) data principles, the model provides a valuable resource for future research. Growth simulations and predictions closely match experimental data, underscoring the model’s predictive accuracy. Overall, this work lays a solid foundation for future studies aimed at leveraging the beneficial properties of S. epidermidis while mitigating its pathogenic potential.
Staphylococcus epidermidis, a bacterium commonly found on human skin, has shown probiotic effects in the nasal microbiome and is a notable causative agent of hospital-acquired infections. While these infections are typically non-life-threatening, their economic impact is considerable, with annual costs reaching billions of dollars in the United States. To better understand its opportunistic nature, we employed genome-scale metabolic modeling to construct a detailed network of S. epidermidis’s metabolic capabilities. This model, comprising over a thousand reactions, metabolites, and genes, adheres to established standards and demonstrates solid benchmarking performance. Following the findable, accessible, interoperable, and reusable (FAIR) data principles, the model provides a valuable resource for future research. Growth simulations and predictions closely match experimental data, underscoring the model’s predictive accuracy. Overall, this work lays a solid foundation for future studies aimed at leveraging the beneficial properties of S. epidermidis while mitigating its pathogenic potential.}
}
@incollection{VASILAKES2021123,
title = {Chapter 6 - Natural language processing},
editor = {Subhi J. Al'Aref and Gurpreet Singh and Lohendran Baskaran and Dimitris Metaxas},
booktitle = {Machine Learning in Cardiovascular Medicine},
publisher = {Academic Press},
pages = {123-148},
year = {2021},
isbn = {978-0-12-820273-9},
doi = {https://doi.org/10.1016/B978-0-12-820273-9.00006-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128202739000063},
author = {Jake Vasilakes and Sicheng Zhou and Rui Zhang},
keywords = {Artificial intelligence, Biomedical informatics, Clinical informatics, Information extraction, Machine learning, Natural language processing},
abstract = {Natural language processing (NLP) is a subfield of artificial intelligence that is concerned with the automatic understanding of human language by computers. NLP has seen much success in recent years due to increased computing power and the rise of deep learning, and this success has extended into the domains of biomedical and clinical text. NLP has contributed to tasks such as the discovery of drug interactions, the development of clinical decision support systems, and the facilitation of chart review. Part I of this chapter provides an introduction to NLP, some common tasks in the biomedical domain, and the methods for accomplishing these tasks. Part II gives a survey of recent applications of NLP in cardiovascular medicine.}
}
@article{IHLEN2019101824,
title = {Ethical grounds for public relations as organizational rhetoric},
journal = {Public Relations Review},
volume = {45},
number = {4},
pages = {101824},
year = {2019},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2019.101824},
url = {https://www.sciencedirect.com/science/article/pii/S0363811118302972},
author = {Oyvind Ihlen and Robert L. Heath},
keywords = {Organizational rhetoric, Self-governance, Civil society, Social capital, Fairness, Caring, Rhetorical citizenship},
abstract = {Organizational rhetoric is critically questioned for ethics of its strategic processes and aspirational goal of persuasive, inescapably self-interested influence. Such critique pits strategic engagement needed for self-governance against self-interested framing (spin) and other dysfunctions. This theoretical essay takes stock of research literature to evaluate the ethics of organizational rhetoric, as rationale for public relations, and justify shifting from a strategic functional to an ontological, agonistic view of public relations. Relevant literature justifies the ethics of fairness (which features regard for others’ interests) to guide rhetorical processes and prefer outcomes as societally responsible. From classical Greece to postmodern theory of agonism, analysis of rhetoric centers on self-governance: achieved by stakeholders addressing rhetorical problems in rhetorical situations to deliberate strategic legitimatization. The discursive role of public relations intersects ethics of fairness and rhetorical citizenship, advocacy and dialogue, discourse and engagement at individual and societal levels. Public relations should be linked to an ontological ethics regarding strategic means of rhetorical influence toward ends accomplished collectively by agonistic pursuit.}
}
@article{MILLET2023107707,
title = {Defending humankind: Anthropocentric bias in the appreciation of AI art},
journal = {Computers in Human Behavior},
volume = {143},
pages = {107707},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107707},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223000584},
author = {Kobe Millet and Florian Buehler and Guanzhong Du and Michail D. Kokkoris},
keywords = {Anthropocentrism, Speciesism, Artificial intelligence (AI), Computational creativity, Computer-generated art, Awe},
abstract = {We argue that recent advances of artificial intelligence (AI) in the domain of art (e.g., music, painting) pose a profound ontological threat to anthropocentric worldviews because they challenge one of the last frontiers of the human uniqueness narrative: artistic creativity. Four experiments (N = 1708), including a high-powered preregistered experiment, consistently reveal a pervasive bias against AI-made artworks and shed light on its psychological underpinnings. The same artwork is preferred less when labeled as AI-made (vs. human-made) because it is perceived as less creative and subsequently induces less awe, an emotional response typically associated with the aesthetic appreciation of art. These effects are more pronounced among people with stronger anthropocentric creativity beliefs (i.e., who believe that creativity is a uniquely human characteristic). Systematic depreciation of AI-made art (assignment of lower creative value, suppression of emotional reactions) appears to serve a shaken anthropocentric worldview whereby creativity is exclusively reserved for humans.}
}
@article{HEATHKELLY201863,
title = {Survivor Trees and memorial groves: Vegetal commemoration of victims of terrorism in Europe and the United States},
journal = {Political Geography},
volume = {64},
pages = {63-72},
year = {2018},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0962629816302694},
author = {Charlotte Heath-Kelly},
abstract = {In commemorations of human lives lost in terrorism, European and American memorials increasingly appeal to the aesthetics of ‘nature’ to symbolise societal regrowth. This article interrogates the ironic and ontological registers involved in commemorating human life through vegetal symbols, paying particular attention to the World Trade Center site in Manhattan. Memorials traditionally conceive of human life as distinct from material and living ecologies, rarely commemorating the deaths of non-humans. As such, the use of trees and vegetal landscaping to represent and memorialise the dead human involves a complex and ironic ontological relationship. Post disaster place-making through vegetal symbolism equates vegetal and human being, on one level, but it also ironically emphasises the fundamental gulf between them. Survivors and visitors are confronted with regenerating vegetal life which evokes idealised ecological conceptions of networked human and non-human lives. But we do not live or die in the same way as a plant, so vegetal symbolism simultaneously invokes human alienation from the natural world. The aesthetic registers of the survivor trees bring a complex, unresolved and ironic reflection on human mortality to memorial landscapes.}
}
@article{KANG2023107588,
title = {BBLN: A bilateral-branch learning network for unknown protein–protein interaction prediction},
journal = {Computers in Biology and Medicine},
volume = {167},
pages = {107588},
year = {2023},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2023.107588},
url = {https://www.sciencedirect.com/science/article/pii/S0010482523010533},
author = {Yan Kang and Xinchao Wang and Cheng Xie and Huadong Zhang and Wentao Xie},
keywords = {Protein–protein interaction (PPI), Multi-modal learning, Graph neural network, Contrastive learning, Unknown PPI prediction},
abstract = {Unknown Protein–Protein Interactions (PPIs) prediction has a huge demand in the biological analysis field. Since the effect of the limited availability of protein data is severe, transferable representations are highly demanded to be learned from various data. The latest works enhance the model performance on unknown PPIs prediction and have achieved certain improvements by combining protein information and relation information on PPI graph. However, such methods inevitably suffer from a so-called information monotonicity problem that limits the improvements when encountering large amounts of unknown PPIs. The prediction performance cannot be actually increased without considering the complementary information and relationship information among various modalities of protein data. To this end, we propose a bilateral-branch learning network to deeply enhance the both complementary and relationship information based on the amino acid sequence and gene ontology from multi- and cross-modal views. Experimental results on massive real-world datasets show that our method significantly outperforms the previous state-of-the-art on both traditional and novel unknown PPIs prediction.}
}
@article{DEDIU201917,
title = {Third International Conference on Statistical Language and Speech Processing, SLSP 2015 Preface},
journal = {Computer Speech & Language},
volume = {58},
pages = {17-18},
year = {2019},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0885230819300968},
author = {Adrian-Horia Dediu and Carlos Martín-Vide}
}
@article{KATASE2023334,
title = {DKK3 expression is correlated with poorer prognosis in head and neck squamous cell carcinoma: A bioinformatics study based on the TCGA database},
journal = {Journal of Oral Biosciences},
volume = {65},
number = {4},
pages = {334-346},
year = {2023},
issn = {1349-0079},
doi = {https://doi.org/10.1016/j.job.2023.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1349007923001263},
author = {Naoki Katase and Shin-ichiro Nishimatsu and Akira Yamauchi and Shinji Okano and Shuichi Fujita},
keywords = {Head and neck squamous cell carcinoma, DKK3 protein, human, Bioinformatics},
abstract = {Objective
We previously reported that dickkopf WNT signaling pathway inhibitor 3 (DKK3) expression is correlated with poorer prognosis in head and neck squamous cell carcinoma (HNSCC). Here we investigated DKK3 expression by using The Cancer Genome Atlas (TCGA) public database and bioinformatic analyses.
Methods
We used the RNA sequence data and divided the tumor samples into “DKK3-high” and “DKK3-low” groups according to median DKK3 expression. The correlations between DKK3 expression and the clinical data were investigated. Differentially expressed genes (DEGs) were detected using DESEq2 and analyzed by ShinyGO 0.77. A gene set enrichment analysis (GSEA) was also performed using GSEA software. The DEGs were also analyzed with TargetMine to establish the protein–protein interaction (PPI) network.
Results
DKK3 expression was significantly increased in cancer samples, and a high DKK3 expression was significantly associated with shorter overall survival. We identified 854 DEGs, including 284 up-regulated and 570 down-regulated. Functional enrichment analyses revealed several Gene Ontology (GO) terms and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways associated with extracellular matrix remodeling. The PPI network identified COL8A1, AGTR1, FN1, P4HA3, PDGFRB, and CEP126 as the key genes.
Conclusions
These results suggested the cancer-promoting ability of DKK3, the expression of which is a promising prognostic marker and therapeutic target for HNSCC.}
}
@article{RONGEN2023103910,
title = {Modelling with AAS and RDF in Industry 4.0},
journal = {Computers in Industry},
volume = {148},
pages = {103910},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103910},
url = {https://www.sciencedirect.com/science/article/pii/S016636152300060X},
author = {Sjoerd Rongen and Nikoletta Nikolova and Mark {van der Pas}},
keywords = {Industry 4.0, Modelling, Knowledge representation, RDF, AAS, Semantic interoperability},
abstract = {Industry 4.0 has proposed the Asset Administration Shell (AAS) model for digital twins. This model should help to solve interoperability issues, a topic that is also addressed by the Semantic Web and its Resource Description Framework (RDF). AAS and RDF-based models have their own strengths. AAS models are easier to integrate with operational technologies in a production environment, whereas RDF-based models offer more semantic expressiveness and advanced querying. In the Horizon MAS4AI project we found that both modelling paradigms can complement each other to develop agentbased digital twins for modular production environments. In this work we propose two different approaches to bridge both modelling paradigms. First we define a set of mapping rules to generate an AAS model from a given RDF-based model, supporting model development. Secondly, we propose to use RDF-based models to generate a digital shadow of AASs to improve semantic discoverability. Preliminary results demonstrate that heterogeneity of metamodels does not exclude achieving semantic interoperability, as well as that greater functionality can be obtained compared to using both models in isolation. The solutions will be further developed in collaboration with pilot lines in the MAS4AI project.}
}
@article{ABDELMAGEED2022,
title = {BiodivNERE: Gold standard corpora for named entity recognition and relation extraction in the biodiversity domain},
journal = {Biodiversity Data Journal},
volume = {10},
year = {2022},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.10.e89481},
url = {https://www.sciencedirect.com/science/article/pii/S131428362200118X},
author = {Nora Abdelmageed and Felicitas Löffler and Leila Feddoul and Alsayed Algergawy and Sheeba Samuel and Jitendra Gaikwad and Anahita Kazem and Birgitta König-Ries},
keywords = {entity annotation, relation annotation, Named Entity Recognition (NER), Relation Extraction (RE), Information Extraction (IE), biodiversity research, gold standard},
abstract = {Background
Biodiversity is the assortment of life on earth covering evolutionary, ecological, biological, and social forms. To preserve life in all its variety and richness, it is imperative to monitor the current state of biodiversity and its change over time and to understand the forces driving it. This need has resulted in numerous works being published in this field. With this, a large amount of textual data (publications) and metadata (e.g. dataset description) has been generated. To support the management and analysis of these data, two techniques from computer science are of interest, namely Named Entity Recognition (NER) and Relation Extraction (RE). While the former enables better content discovery and understanding, the latter fosters the analysis by detecting connections between entities and, thus, allows us to draw conclusions and answer relevant domain-specific questions. To automatically predict entities and their relations, machine/deep learning techniques could be used. The training and evaluation of those techniques require labelled corpora.
New information
In this paper, we present two gold-standard corpora for Named Entity Recognition (NER) and Relation Extraction (RE) generated from biodiversity datasets metadata and abstracts that can be used as evaluation benchmarks for the development of new computer-supported tools that require machine learning or deep learning techniques. These corpora are manually labelled and verified by biodiversity experts. In addition, we explain the detailed steps of constructing these datasets. Moreover, we demonstrate the underlying ontology for the classes and relations used to annotate such corpora.}
}
@article{MISIRLI20213304,
title = {Virtual Parts Repository 2: Model-Driven Design of Genetic Regulatory Circuits},
journal = {ACS Synthetic Biology},
volume = {10},
number = {12},
pages = {3304-3315},
year = {2021},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.1c00157},
url = {https://www.sciencedirect.com/science/article/pii/S2161506321000395},
author = {Göksel Mısırlı and Bill Yang and Katherine James and Anil Wipat},
keywords = {genetic circuits, model-driven design, genetic design automation, modular models, computational simulation},
abstract = {Engineering genetic regulatory circuits is key to the creation of biological applications that are responsive to environmental changes. Computational models can assist in understanding especially large and complex circuits for which manual analysis is infeasible, permitting a model-driven design process. However, there are still few tools that offer the ability to simulate the system under design. One of the reasons for this is the lack of accessible model repositories or libraries that cater to the modular composition of models of synthetic systems. Here, we present the second version of the Virtual Parts Repository, a framework to facilitate the model-driven design of genetic regulatory circuits, which provides reusable, modular, and composable models. The new framework is service-oriented, easier to use in computational workflows, and provides several new features and access methods. New features include supporting hierarchical designs via a graph-based repository or compatible remote repositories, enriching existing designs, and using designs provided in Synthetic Biology Open Language documents to derive system-scale and hierarchical Systems Biology Markup Language models. We also present a reaction-based modeling abstraction inspired by rule-based modeling techniques to facilitate scalable and modular modeling of complex and large designs. This modeling abstraction enhances the modeling capability of the framework, for example, to incorporate design patterns such as roadblocking, distributed deployment of genetic circuits using plasmids, and cellular resource dependency. The framework and the modeling abstraction presented in this paper allow computational design tools to take advantage of computational simulations and ultimately help facilitate more predictable applications.
}
}
@article{CHEN20231317,
title = {Exploring the mechanisms of magnolol in the treatment of periodontitis by integrating network pharmacology and molecular docking},
journal = {Biocell},
volume = {47},
number = {6},
pages = {1317-1327},
year = {2023},
issn = {0327-9545},
doi = {https://doi.org/10.32604/biocell.2023.028883},
url = {https://www.sciencedirect.com/science/article/pii/S0327954523000683},
author = {DER-JEU CHEN and CHENG-HUNG LAI},
keywords = {Magnolol, Periodontitis, Network pharmacology, Molecular docking},
abstract = {Background
Magnolol, a bioactive extract of the Chinese herb Magnolia officinalis has a protective effect against periodontitis. This study is aimed to explore the mechanisms involved in the functioning of magnolol against periodontitis and provide a basis for further research.
Methods
Network pharmacology analysis was performed based on the identification of related targets from public databases. The Protein-protein interaction (PPI) network was constructed to visualize the significance between the targets of magnolol and periodontitis. Subsequently, Gene ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) analysis were performed to predict the functions and the signal regulatory pathways involved in the action of magnolol against periodontitis. The “function-target-pathway” networks were constructed to analyze the core targets and pathways of magnolol against periodontitis. Molecular docking was used to verify the interaction of magnolol and core targets.
Results
A total of 58 active targets of magnolol and 644 periodontitis-related targets were collected from public databases. A total of 25 targets of magnolol against periodontitis were identified based on the Venn diagram. GO analysis showed that magnolol has a role in the response to oxidative stress, nicotine, and lipopolysaccharide. KEGG enrichment analysis indicated that the mechanism of magnolol against periodontitis was mainly related to the tumor necrosis factor (TNF), phosphoinositide 3-kinase (PI3K/Akt), and mitogen-activated protein kinase (MAPK) signaling pathways. Combined with PPI network and molecular docking results, the core targets of magnolol against periodontitis included AKT1, MAPK8, MAPK14, TNF, and TP53.
Conclusion
To summarize, the anti-periodontitis mechanisms of magnolol are potentially through regulating the TNF, PI3K/Akt, and MAPK signaling pathways.}
}
@article{QIU2024155688,
title = {Evaluating the diagnostic potential of SOCS3 in copper metabolism for acute myocardial infarction},
journal = {Pathology - Research and Practice},
volume = {264},
pages = {155688},
year = {2024},
issn = {0344-0338},
doi = {https://doi.org/10.1016/j.prp.2024.155688},
url = {https://www.sciencedirect.com/science/article/pii/S0344033824005995},
author = {Duixin Qiu and Xinrong Jia and Ye Ding and Yating Gao and Xiaodong Chen and Dan Huang},
keywords = {Acute myocardial infarction, Copper homeostasis, SOCS3, Differentially expressed genes, Diagnostic value},
abstract = {Acute myocardial infarction (AMI) represents a critical cardiovascular condition necessitating rapid and precise diagnostic strategies. This study investigates the diagnostic implications of genes involved in copper metabolism homeostasis in AMI. We identified genes related to copper metabolism and AMI from Genecards and GEO databases, conducting differential gene analysis via R software. Gene function was annotated through Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses, while the STRING database facilitated key gene identification via topological analysis. The diagnostic value of these genes, particularly cytokine signaling 3 (SOCS3), was assessed using ROC curve analysis. SOCS3 expression was validated using in-vitro and in-vivo models, including cardiomyocyte hypoxia/reoxygenation (H/R) and rat myocardial infarction (MI) model. Further, we examined the effects of SOCS3 knockout on cell proliferation, apoptosis, and myocardial infarction severity. 77 genes were identified, with 73 showing upregulation and 4 downregulation. These genes mainly participated in pathways related to cytokine activation, inflammation regulation, and lipid metabolism. Network analysis highlighted 10 key genes, with SOCS3 exhibiting significant diagnostic potential (AUC > 0.9). Validation experiments confirmed SOCS3 overexpression in disease models, with its knockout leading to decreased apoptosis, reduced infarct size, and improved cardiac function. This study highlights the diagnostic relevance of genes associated with copper metabolism, particularly SOCS3, in AMI. These findings offer novel insights into the molecular mechanisms of AMI, supporting the development of targeted diagnostic and therapeutic strategies.}
}
@article{ORIOL2023179,
title = {Generating valid test data through data cloning},
journal = {Future Generation Computer Systems},
volume = {144},
pages = {179-191},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.02.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23000614},
author = {Xavier Oriol and Ernest Teniente and Marc Maynou and Sergi Nadal},
keywords = {Database testing, Test data, Data cloning},
abstract = {One of the most difficult, time-consuming and error-prone tasks during software testing is that of manually generating the data required to properly run the test. This is even harder when we need to generate data of a certain size and such that it satisfies a set of conditions, or business rules, specified over an ontology. To solve this problem, some proposals exist to automatically generate database sample data. However, they are only able to generate data satisfying primary or foreign key constraints but not more complex business rules in the ontology. We propose here a more general solution for generating test data which is able to deal with expressive business rules. Our approach, which is entirely based on the chase algorithm, first generates a small sample of valid test data (by means of an automated reasoner), then clones this sample data, and finally, relates the cloned data with the original data. All the steps are performed iteratively until a valid database of a certain size is obtained. We theoretically prove the correctness of our approach, and experimentally show its practical applicability.}
}
@incollection{PIRRO2019446,
title = {Semantic Similarity Functions and Measures},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {446-458},
year = {2019},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00345-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027003456},
author = {Giuseppe Pirrò},
keywords = {Gene ontology, Information content, Intrinsic information content, MeSH, Normalized google distance, PMI-IR, Semantic relatedness, Semantic similarity, Similarity library, WordNet},
abstract = {Similarity is the process through which two or more objects are compared in order to identify to what extent they are alike. This is useful, for instance, when there is the need to identify common elements among a set of objects or when a new object has to be classified w.r.t. the others. Similarity is a very generic notion spanning among different disciplines from psychology to computer science. In this paper, the focus will be on computational methods that exploit ontologies or search engines as primary source of “background” knowledge. In particular, after surveying on the most popular similarity measures recently proposed, the Similarity Library will be presented, which has the aim to provide researchers and practitioners with a flexible tool encompassing several similarity measures both between words and sentences.}
}
@article{KONG2021323,
title = {Data Construction Method for the Applications of Workshop Digital Twin System},
journal = {Journal of Manufacturing Systems},
volume = {58},
pages = {323-328},
year = {2021},
note = {Digital Twin towards Smart Manufacturing and Industry 4.0},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2020.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612520300133},
author = {Tianxiang Kong and Tianliang Hu and Tingting Zhou and Yingxin Ye},
keywords = {Digital Twin, Workshop, Application-oriented, Data construction},
abstract = {Data is the key to the operation of manufacturing workshop Digital Twin System (DTS) and also supports the top-level applications of DTS. However, manufacturing data has the characteristics of coupling and large-amount, which lead to inefficient and inaccurate operations of applications. In order to provide stable and efficient data support for the applications of DTS, this paper proposed a data construction method. The framework of data construction is designed based on the functional requirements, which are analyzed according to the characteristics of manufacturing data. Then, module implementation of the framework, including data representation module, data organization module and data management module, is introduced in detail. Finally, application of cutting tool wear prediction is taken as a case study to show the feasibility and effectiveness of the proposed data construction method.}
}
@article{MERONOPENUELA2025100847,
title = {KG.GOV: Knowledge graphs as the backbone of data governance in AI},
journal = {Journal of Web Semantics},
volume = {85},
pages = {100847},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100847},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000337},
author = {Albert Meroño-Peñuela and Elena Simperl and Anelia Kurteva and Ioannis Reklos},
keywords = {Knowledge graphs, AI, Governance},
abstract = {As (generative) Artificial Intelligence continues to evolve, so do the challenges associated with governing the data that powers it. Ensuring data quality, privacy, security, and ethical use become more and more challenging due to the increasing volume and variety of the data, the complexity of AI models, and the rapid pace of technological advancement. Knowledge graphs have the potential to play a significant role in enabling data governance in AI, as we move beyond their traditional use as data organisational systems. To address this, we present KG.gov, a framework that positions KGs at a higher abstraction level within AI workflows, and enables them as a backbone of AI data governance. We illustrate the three dimensions of KG.gov: modelling data, alternative representations, and describing behaviour; and describe the insights and challenges of three use cases implementing them: Croissant, a vocabulary to model and document ML datasets; WikiPrompts, a collaborative KG of prompts and prompt workflows to study their behaviour at scale; and Multimodal transformations, an approach for multimodal KGs harmonisation and completion aiming at broadening access to knowledge.}
}
@article{KURNIAWAN2022102828,
title = {KRYSTAL: Knowledge graph-based framework for tactical attack discovery in audit data},
journal = {Computers & Security},
volume = {121},
pages = {102828},
year = {2022},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2022.102828},
url = {https://www.sciencedirect.com/science/article/pii/S016740482200222X},
author = {Kabul Kurniawan and Andreas Ekelhart and Elmar Kiesling and Gerald Quirchmayr and A Min Tjoa},
keywords = {Attack graph construction, Log analysis, Knowledge graph, Attack discovery, Cybersecurity, Information security},
abstract = {Attack graph-based methods are a promising approach towards discovering attacks and various techniques have been proposed recently. A key limitation, however, is that approaches developed so far are monolithic in their architecture and heterogeneous in their internal models. The inflexible custom data models of existing prototypes and the implementation of rules in code rather than declarative languages on the one hand make it difficult to combine, extend, and reuse techniques, and on the other hand hinder reuse of security knowledge – including detection rules and threat intelligence. KRYSTAL tackles these challenges by providing a knowledge graph-based, modular framework for threat detection, attack graph and scenario reconstruction, and analysis based on RDF as a standard model for knowledge representation. This approach provides query options that facilitate contextualization over internal and external background knowledge, as well as the integration of multiple detection techniques, including tag propagation, attack signatures, and graph queries. We implemented our framework in an openly available prototype and demonstrate its applicability on multiple scenarios of the DARPA Transparent Computing dataset. Our evaluation shows that the combination of different threat detection techniques within our framework improved detection capabilities. Furthermore, we find that RDF provenance graphs are scalable and can efficiently support a variety of threat detection techniques.}
}
@article{CABADA2018611,
title = {An affective and Web 3.0-based learning environment for a programming language},
journal = {Telematics and Informatics},
volume = {35},
number = {3},
pages = {611-628},
year = {2018},
note = {SI: EduWebofData},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0736585316304907},
author = {Ramón Zataraín Cabada and María Lucía Barrón Estrada and Francisco González Hernández and Raúl Oramas Bustillos and Carlos Alberto Reyes-García},
keywords = {Web 3.0, Intelligent learning environment, Educational applications},
abstract = {We present a Web-based environment for learning Java programming that aims to provide adapted and individualized programming instruction to students by using modern learning technologies as a recommender and mining system, an affect recognizer, a sentiment analyzer, and an authoring tool. All these components interact in real time to provide an educational setting where the student learn to develop Java programs. The recommender system is an E-Learning 3.0 software component that recommends new exercises to a student based on the actions (ratings) of previous learners. The affect recognizer analyze pictures of the student to recognize learning-centered emotions (frustration, boredom, engagement, and excitement) that are used to provide personalized instruction. Sentiment text analysis determines the quality of the programming exercises based on the opinions of the students. The authoring tool is used to create new exercises with no programming work. We conducted two evaluations: one evaluation used the Technology Acceptance Model to assess the impact of our software tool on student behavior. The second evaluation calculated the student’s t-test to assess the learning gain after a student used the tool. The results of the evaluations show the students perceived enjoyment and are willing to use the tool. The study also show that students using the tool have a greater learning gain than those who learn using a traditional method.}
}
@article{FUCHS2022103590,
title = {A collaborative knowledge-based method for the interactive development of cabin systems in virtual reality},
journal = {Computers in Industry},
volume = {136},
pages = {103590},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103590},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521001974},
author = {Mara Fuchs and Florian Beckert and Jörn Biedermann and Björn Nagel},
keywords = {Aerospace, Aircraft cabin, Virtual reality, Knowledge-based modeling, Digital design},
abstract = {Progressive digitization in the development phase of systems is leading to shorter development times and lower costs. At the same time, the interactions in more complex systems are increasing and become more nested, which affects the understanding of system dependencies for humans as well as modeling these. This results in the challenge of digitizing the knowledge (rules, regulations, requirements, etc.) required to describe the system and its interrelationships. An example of such a system is the aircraft. In practice, usually, the technical design of the cabin and its systems is done separately from the preliminary aircraft design and the cabin results will be integrated late in the aircraft development process. In this paper, a proposal is given for a conceptual design method that enables a cabin systems layout based on preliminary aircraft design data (parameter set). Therefore, a central data model is developed that links cabin components to several disciplines to enable an automated layout. Here, knowledge is stored in an ontology. Linking the ontology with design rules and importing external parameters, missing information needed for preliminary design of cabin systems can be generated. The design rules are based on requirements, safety regulations as well as expert knowledge for design interpretation that has been collected and formalized. Using the ontology, an XML data structure can be instantiated which contains all information about properties, system relationships and requirements. So, the metadata and results of heterogenous domain-specific models and software tools are accessible for all experts of the layout process in a holistic manner and ensure data consistency. Using this XML data structure, a 3D virtual cabin mockup is created in which users have the possibility to interact with cabin modules and system components via controllers. This virtual development platform enables an interaction with complex product data sets like the XML file by visualizing metadata and analysis results along with the cabin geometry, making it even better comprehensible and processable for humans. So, various new cabin system designs can be iterated, evaluated, and optimized at low cost before the concepts are validated in a real prototype. For this, the virtual environment provides a platform that integrates all related disciplines, experts, research partners or the entire supply chain to improve communication among all stakeholders by directly participating and intervening in the evaluation and optimization process. Moreover, the use of VR is being investigated as a new technology in pre-design phase to exploit the potential of knowledge acquisition in immersive environments early in the development stage.}
}
@article{JACKSON202044,
title = {Australia’s mass fish kills as a crisis of modern water: Understanding hydrosocial change in the Murray-Darling Basin},
journal = {Geoforum},
volume = {109},
pages = {44-56},
year = {2020},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2019.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0016718519303641},
author = {Sue Jackson and Lesley Head},
abstract = {In the summer of 2018/19, a series of fish kills on the Darling River attracted international attention. We analyse the disaster as a crisis of modern water within the hydrosocial cycle framework formulated byLinton and Budds. Using archival analysis we identify four phases in the emergence and transformation of modern water in the Murray-Darling Basin generally and the lower Darling specifically; navigation flows (1850–1900s), entitlement flows (1880–1940s), exchange water (1950–1990s) and saved water (2000s-present). The phases are driven by conceptual abstraction and commensuration, leading in turn to the material abstraction of water from the lower Darling, rendering the river and its communities vulnerable. We reveal three previously unidentified social processes contributing to the current crisis. First, the development of a model of hydrological productivity that rationalised the Basin scale as a unit of governance and deemed some places ‘effective’ and others, like the Darling, ‘ineffective’. Second, an early form of offsetting in ‘exchange water’ that disembedded water at least three decades before market environmentalism took hold. Third, accounting technologies that enrol evaporative water into basin water governance and politics. A crisis like the fish kills reveals the ways in which the hydrological cycle overflows with social content, internalising scientific expertise and dominant modes of water governance that include settler colonialism. However, hydrosocial framings need to better capture the diversity and complexity of co-existing Indigenous ontologies, and their different expressions of the social and experiences of the material. These ontologies both intersect with and exceed modern water.}
}
@incollection{BEATTYMARTINEZ2025,
title = {Codeswitching},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00503-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041005032},
author = {Anne L. Beatty-Martínez and M. Carmen {Parafita Couto} and Felix K. Ameka and Enoch O. Aboh},
keywords = {Bilingualism, Codemixing, Codeswitching, Language contact, Multilingualism},
abstract = {Codeswitching is the practice of using more than one language in a conversation, influenced by linguistic, cognitive, and social factors. Historically misunderstood as a sign of laziness or confusion, it is now recognized as a sophisticated, systematic linguistic behavior. We celebrate the milestones in codeswitching research, emphasizing how community norms and individual language experience shape its use, offering valuable insights into multilingualism and language processing across cultures.}
}
@article{BACQUET2024289,
title = {Requirements Structure for System Requirements Formal Modelling, Verification and Validation},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {19},
pages = {289-294},
year = {2024},
note = {18th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.09.194},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324016136},
author = {Cyril Bacquet and Pascale Marangé and Éric Bonjour and Alain Kerbrat},
keywords = {Requirements, Boilerplates, Pattern, Model, Verification, Validation},
abstract = {Requirements engineering (RE) is essential for system design because incorrect or incomplete RE can lead to misunderstandings, gaps, and mistakes that can negatively affect projects. Higher-quality requirements can reduce errors. However, verification and validation (V&V) of requirements qualities is challenging. Formal modeling enables automation for early V&V of large requirements sets. The use of patterns improves requirements quality, but gaps still exist in formal modeling and V&V methodologies. This paper presents a system requirements conceptual model with requirements writing patterns that enable the systems requirements modeling and V&V, along with a comparative analysis with existing concepts in the literature.}
}
@article{COWLEY2019104025,
title = {Wide coding: Tetris, Morse and, perhaps, language},
journal = {Biosystems},
volume = {185},
pages = {104025},
year = {2019},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2019.104025},
url = {https://www.sciencedirect.com/science/article/pii/S0303264719301820},
author = {S J Cowley},
keywords = {Organic codes, Distributed language, Adaptors, Wide cognition, Reading, Languaging},
abstract = {Code biology uses protein synthesis to pursue how living systems fabricate themselves. Weight falls on intermediary systems or adaptors that enable translated DNA to function within a cellular apparatus. Specifically, code intermediaries bridge between independent worlds (e.g. those of RNAs and proteins) to grant functional lee-way to the resulting products. Using this Organic Code (OC) model, the paper draws parallels with how people use artificial codes. As illustrated by Tetris and Morse, human players/signallers manage code functionality by using bodies as (or like) adaptors. They act as coding intermediaries who use lee-way alongside “a small set of arbitrary rules selected from a potentially unlimited number in order to ensure a specific correspondence between two independent worlds” (Barbieri, 2015). As with deep learning, networked bodily systems mesh inputs from a coded past with current inputs. Received models reduce ‘use’ of codes to a run-time or program like process. They overlook how molecular memory is extended by living apparatuses that link codes with functioning adaptors. In applying the OC model to humans, the paper connects Turing’s (1937) view of thinking to Wilson’s (2004) appeal to wide cognition. The approach opens up a new view of Kirsh and Maglio’s (1994) seminal studies on Tetris. As players use an interface that actualizes a code or program, their goal-directed (i.e. ‘pragmatic’) actions co-occur with adaptor-like ‘filling in’ (i.e. ‘epistemic’ moves). In terms of the OC model, flexible functions derive from, not actions, but epistemic dynamics that arise in the human-interface-computer system. Second, I pursue how a Morse radio operator uses dibs and dabs that enable the workings of an artificial code. While using knowledge (‘the rules’) to resemiotize by tapping on a transmission key, bodily dynamics are controlled by adaptor-like resources. Finally, turning to language, I sketch how the model applies to writing and reading. Like Morse operators, writers resemiotize a code-like domain of alphabets, spelling-systems etc. by acting as (or like) bodily adaptors. Further, in attending to a text-interface (symbolizations), a reader relies on filling-in that is (or feels) epistemic. Given that humans enact or mimic adaptor functions, it is likely that the OC model also applies to multi-modal language.}
}
@article{LANZA2025101456,
title = {Establishment of good practices for the usage of machine-actionable core metrological terminology},
journal = {Measurement: Sensors},
volume = {38},
pages = {101456},
year = {2025},
note = {Proceedings of the XXIV IMEKO World Congress},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2024.101456},
url = {https://www.sciencedirect.com/science/article/pii/S266591742400432X},
author = {Giacomo Lanza and Martin Koval and Federico {Grasso Toro} and Mark Kuster and Hugo {Gasca Aragón} and Diego {Nahuel Coppa} and Maitane Iturrate-García and Michaela Küpferling and Maximilian Gruber and Jean-Laurent Hippolyte and Luca Mari},
keywords = {Metrology, Metadata, Terminology, VIM, FAIR data principles},
abstract = {With progressing digitalisation of scientific research and metrological services, the exchange and reuse of data benefits from the usage of standard metadata schemas and controlled terminologies as a “common language” to describe, search and filter data. However, no framework yet unifies metrology-related endeavours to rigorously deal with machine-actionable data. Normative terminology documents often lack machine-readability, while community-driven machine-readable terminologies lack both metrological rigour and comprehensiveness. The resulting fragmented landscape makes it difficult for end users to adopt a good practice in registering data. Our contribution takes a step to cover this gap, by providing a unified overview of all relevant recommendations in a comprehensive and accessible way, in agreement with authoritative regulation bodies, current good practices, and existing digital-object validation platforms. Our activity will focus on a machine-readable version of the International Vocabulary of Metrology (VIM).}
}
@article{SUN2023,
title = {Artificial Intelligence Method for Accurate Translation of Fuzzy Semantics in English Language and Literature},
journal = {International Journal on Semantic Web and Information Systems},
volume = {19},
number = {1},
year = {2023},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.331033},
url = {https://www.sciencedirect.com/science/article/pii/S1552628323000406},
author = {Ying Sun},
keywords = {Artificial Intelligence, English Words, Fuzzy Semantics},
abstract = {ABSTRACT
In order to address the drawbacks of semantic ambiguity, inaccurate quantifiers, and low translation accuracy in traditional grammar-based translation methods, this paper proposes an artificial intelligence translation method based on semantic analysis for English fuzzy semantics. Firstly, a comprehensive analysis of English language semantics was carried out from different semantic levels such as language, knowledge, and pragmatics, and the key points of fuzzy semantics were identified. Then, key feature quantities for accurate translation of fuzzy semantics in English vocabulary and literature were constructed, and artificial intelligence methods were used to optimize fuzzy semantics. The experimental results show that the proposed method can avoid semantic understanding ambiguity and improve the accuracy of English language translation.}
}
@article{HARPER2020196,
title = {Howard Stein on sophisticated practice of philosophers/scientists},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {71},
pages = {196-208},
year = {2020},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2020.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1355219818302053},
author = {William L. Harper}
}
@article{SORMAZ20181242,
title = {IMPlanner-MAS: A Multiagent System for Distributed Manufacturing Process Planning},
journal = {Procedia Manufacturing},
volume = {26},
pages = {1242-1254},
year = {2018},
note = {46th SME North American Manufacturing Research Conference, NAMRC 46, Texas, USA},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.07.161},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918308370},
author = {Dusan N. Sormaz and Arkopaul Sarkar and Subhabrata Ghosal},
keywords = {Manufacturing process planning, CAPP, Agent based modeling, Distributed planning, JADE framework},
abstract = {The paper reports development of the multiagent-based framework for distributed manufacturing process planning. The research builds on previous results in developing intelligent manufacturing planning platform and extends it into agent-based modeling. Both, design and manufacturing tasks are defined as sequences of steps to be performed by highly intelligent autonomous agents. Autonomous agents are classified into two categories: task agents, which are responsible for completing design or manufacturing tasks, and service agents, which provide services for individual steps of those tasks. For multiagent implementation, we have chosen JADE framework and both task agents and service agents have been implemented by extending agents from JADE. Thus, the JADE’s message passing mechanism has been adopted for inter-agent communication and appropriate ontologies for the message passing have been built from planning classes and objects. The brief overview of IMPlanner functionalities has been provided and development and implementation of service agents for several typical manufacturing planning tasks have been explained. The proposed framework has been executed in the context of the simulation model for evaluation of dynamic routing policies.}
}
@article{SINFIELD2020100037,
title = {Framing the Intractable: Comprehensive Success Factor Analysis for Grand Challenges},
journal = {Sustainable Futures},
volume = {2},
pages = {100037},
year = {2020},
issn = {2666-1888},
doi = {https://doi.org/10.1016/j.sftr.2020.100037},
url = {https://www.sciencedirect.com/science/article/pii/S2666188820300307},
author = {Joseph V. Sinfield and Ananya Sheth and Romika R. Kotian},
keywords = {Grand challenges, Complex problem framing, Comprehensive success factor analysis (CSFA), Complex problem structuring, Knowledge representation, International development},
abstract = {Complex socio-technical challenges, often referred to as grand challenges or wicked problems, lack a robust method for their holistic framing. Current approaches to framing fall into two primary categories. On one hand, models grounded in reductionist perspectives tend to oversimplify the problems and thus fall short of capturing the true complexity that must be understood to make tangible progress. On the other, notable attempts to achieve holism are more effective at incorporating contextual nuance, but still lack systematicity to identify and drive effective inclusion of critical issues, and also tend to suffer from the inherent bias of select expert input. In this article, we report on an extension of holistic problem framing techniques called comprehensive success factor analysis (CSFA) that makes-sense of web-mined information reflective of both expert and general population perspectives as well as pattern-informed ontological knowledge organization structure, to yield ‘richer pictures’ of grand challenges. This method has been developed and refined over a seven-year period by application to a variety of distinct socio-technical challenges, and emphasizes that framing complex problems requires one to embrace multiple levels of abstraction, a plurality of perspectives, careful contextualization, and an overarching system view. The CSFA method results in ‘success factor trees’ that are more comprehensive than seen otherwise and present a holistic view of the essential factors that need to be considered when engaging in large scale socio-technical problems. The success factor trees provide common grounds for meaningful collaboration and discourse on grand challenges, facilitate more informed resource allocation decisions, and provide guidance for designing solutions through careful consideration of system factors that are not always apparent. The paper illustrates CSFA applied to the challenge of ‘food security for a nation in a low- to middle-income country context’ to ascertain the value of the approach and finds that it results in a robust view of the challenge that greatly exceeds perspectives arrived at in the literature using current framing methods, on dimensions of scope, levels of abstraction, plurality, and context detail.}
}
@article{ANTONIOU2019284,
title = {Enabling the use of a planning agent for urban traffic management via enriched and integrated urban data},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {98},
pages = {284-297},
year = {2019},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X18317911},
author = {Grigoris Antoniou and Sotiris Batsakis and John Davies and Alistair Duke and Thomas L. McCluskey and Evtim Peytchev and Ilias Tachmazidis and Mauro Vallati},
keywords = {Planning agent, Traffic control, Data hub, Ontology},
abstract = {Improving a city’s infrastructure is seen as a crucial part of its sustainability, leading to efficiencies and opportunities driven by technology integration. One significant step is to support the integration and enrichment of a broad variety of data, often using state of the art linked data approaches. Among the many advantages of such enrichment is that this may enable the use of intelligent processes to autonomously manage urban facilities such as traffic signal controls. In this paper we document an attempt to integrate sets of sensor and historical data using a data hub and a set of ontologies for the data. We argue that access to such high level integrated data sources leads to the enhancement of the capabilities of an urban transport operator. We demonstrate this by documenting the development of a planning agent which uses such data as inputs in the form of logic statements, and when given traffic goals to achieve, outputs complex traffic signal strategies which help transport operators deal with exceptional events such as road closures or road traffic saturation. The aim is to create an autonomous agent which reacts to commands from transport operators in the face of exceptional events involving saturated roads, and creates, executes and monitors plans to deal with the effects of such events. We evaluate the intelligent agent in a region of a large urban area, under the direction of urban transport operators.}
}
@article{VARSHNEY2025110929,
title = {Med-KGMA: A novel AI-driven medical support system leveraging knowledge graphs and medical advisors},
journal = {Computers in Biology and Medicine},
volume = {197},
pages = {110929},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110929},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525012818},
author = {Sona Varshney and Bhawna Jain and Prerna Singh and Drishti Rani and Saumya Mehra},
keywords = {Medical decision support system, Healthcare, Machine learning, Knowledge graph},
abstract = {Healthcare systems worldwide face a growing burden, struggling to provide timely diagnosis and personalized care due to resource constraints. The increasing demand for medical expertise often results in delayed interventions, making automated decision support crucial. However, existing medical question-answering (QnA) systems struggle with hallucinations, limited contextual understanding, and difficulty handling complex queries, which often results in unreliable responses. To address these challenges, this study proposes Med-KGMA, an artificial intelligence-driven medical QnA system that leverages the proposed SequentialRotatE, a knowledge graph embedding model, along with the Mixture-of-Medical-Advisors (MoMA) framework to enhance diagnostic accuracy and treatment recommendations. Unlike conventional methods, SequentialRotatE effectively captures contextual relationships between medical entities, improving the system’s reasoning capabilities. Additionally, the MoMA framework, which dynamically routes queries to specialized advisors based on complexity and relevance, ensures more precise recommendations. Experimental results demonstrate that Med-KGMA achieves 91.32% accuracy, outperforming state-of-the-art baselines. This approach advances medical knowledge representation through optimized query processing, tailored knowledge graphs, and intelligent advisor selection, providing an efficient, scalable solution. By addressing initial symptom analysis and reducing healthcare load, Med-KGMA empowers users with reliable medical insights, bridging the gap between patients and timely care.}
}
@article{CARVALHO2023107714,
title = {DNA methylation epi-signature and biological age in attention deficit hyperactivity disorder patients},
journal = {Clinical Neurology and Neurosurgery},
volume = {228},
pages = {107714},
year = {2023},
issn = {0303-8467},
doi = {https://doi.org/10.1016/j.clineuro.2023.107714},
url = {https://www.sciencedirect.com/science/article/pii/S0303846723001300},
author = {Gleyson Francisco da Silva Carvalho and Thais Virginia Moura Machado Costa and Amom Mendes Nascimento and Beatriz Martins Wolff and Julian Gabriel Damasceno and Lucas Liro Vieira and Vanessa Tavares Almeida and Yanca Gasparini de Oliveira and Claudia Berlim de Mello and Mauro Muszkat and Leslie Domenici Kulikowski},
keywords = {ADHD, Epigenetic, Methylation, Epi-signature, DNAmAge},
abstract = {Objective
Attention Deficit/Hyperactivity Disorder (ADHD) is a common behavioral syndrome that begins in childhood and affects 3.4% of children worldwide. Due to its etiological complexity, there are no consistent biomarkers for ADHD, however the high heritability presented by the disorder indicates a genetic/epigenetic influence. The main epigenetic mechanism is DNA methylation, a process with an important role in gene expression and in many psychiatric disorders. Thus, our study sought to identify epi-signatures biomarkers in 29 children clinically diagnosed with ADHD.
Methods
After DNA extraction and bisulfite conversion, we performed methylation array experiment for differential methylation, ontological and biological age analysis.
Results
The biological response in ADHD patients was not sufficient to determine a conclusive epi-signature in our study. However, our results highlighted the interaction of energy metabolism and oxidative stress pathways in ADHD patients detected by differential methylation patterns. Furthermore, we were able to identify a marginal association between the DNAmAge and ADHD.
Conclusion
Our study present new methylation biomarkers findings associated with energy metabolism and oxidative stress pathways, in addition to DNAmAge in ADHD patients. However, we propose that further multiethnic studies, with larger cohorts and including maternal conditions, are necessary to demonstrate a definitive association between ADHD and these methylation biomarkers.}
}
@article{ZUBKOV20221804,
title = {MOOCs in competence approach to EFL Training of Transport Professionals},
journal = {Transportation Research Procedia},
volume = {63},
pages = {1804-1809},
year = {2022},
note = {X International Scientific Siberian Transport Forum — TransSiberia 2022},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.06.197},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522004525},
author = {Artyom Zubkov},
keywords = {massive open online course, foreign language, methodological model, for-eign language communicative competence, economists},
abstract = {The article explores the possibilities of massive open online courses for competence-based teaching of professional foreign language. The relevance of the study is determined by the need to form the foreign language communicative competence of transport professionals and the need to find new ways to form it among the digital generation of specialists. A content analysis of the Coursera educational platform in relation to the possibility of forming components of a foreign language communicative competence was carried out. A model of the formation of foreign language communicative competence of students majoring in International Business has been developed and its effectiveness has been assessed. A conclusion about the effectiveness of the developed model and the presence of the didactic and content potential of MOOCs in relation to the competence-based approach to EFL-training of transport professionals is made.}
}
@article{RAO2024140173,
title = {Unlocking the molecular modifications of plasma-activated water-induced oxidation through redox proteomics: In the case of duck myofibrillar protein (Anas platyrhynchos)},
journal = {Food Chemistry},
volume = {458},
pages = {140173},
year = {2024},
issn = {0308-8146},
doi = {https://doi.org/10.1016/j.foodchem.2024.140173},
url = {https://www.sciencedirect.com/science/article/pii/S0308814624018235},
author = {Wei Rao and Shilong Ju and Yangying Sun and Qiang Xia and Changyu Zhou and Jun He and Wei Wang and Daodong Pan and Lihui Du},
keywords = {Label-free proteomics, Plasma, Myofibrillar protein, Cysteine sites},
abstract = {Plasma-activated water (PAW) contains multiple active species that alter the structure of myofibrillar protein (MP) to enhance their gel properties. This work investigated the impact of PAW on the oxidation of cysteine in MP by label-free quantitative proteomics. PAW treatment caused the oxidation of 8241 cysteine sites on 2815 proteins, and structural proteins such as nebulin, myosin XVIIIB, myosin XVIIIA, and myosin heavy chain were susceptible to oxidation by PAW. Bioinformatics analysis, including Gene Ontology (GO), Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway, subcellular localization, and STRING analysis, indicated that these proteins with differential oxidation sites were mainly derived from the cytoplasm and membrane, and were involved in multiple GO terms and KEGG pathways. This is one of the first reports of the redox proteomic changes induced by PAW treatment, and the results are useful for understanding the possible mechanism of PAW-induced oxidation of MP.}
}
@article{STETTER2024168,
title = {A Concept for an Integrated Framework for Abstract Physics Modelling (IF4APM)},
journal = {Procedia CIRP},
volume = {128},
pages = {168-173},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124006796},
author = {Ralf Stetter and Markus Till},
keywords = {abstract product representation, modelling of physical effects, model-based systems engineering},
abstract = {Abstract physics modelling intends to describe the physical phenomena, working surfaces, kinematic behavior, etc. which realize the functionality of a technical system. In this context, the notion “abstract” means that particular detailed structures, components, geometry etc. are consciously not described. The advantages of an abstract modelling are both a concentration on essential information and an avoidance of a limitation of the solution space. In the last years, several approaches and perspectives for abstract physics modelling were investigated and the need to integrate those approaches in engineering frameworks was pointed out. This intermediate level is especially important for model-based system engineering (MBSE), because in MBSE product models are the primary process management objects instead of not product related documents. However, an integrated framework which combines the advantages of the different approaches and allows an integration in engineering frameworks was not yet proposed. This paper presents an initial concept of an Integrated Framework for Abstract Physics Modelling (IF4APM). The underlying investigation is based on the product development of agricultural technology systems. The general approach and the different views are presented based on a certain use case of one of these systems.}
}
@article{WALTERSDORFER2025100849,
title = {Leveraging Knowledge Graphs for AI System Auditing and Transparency},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100849},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100849},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000350},
author = {Laura Waltersdorfer and Marta Sabou},
keywords = {AI auditing, Knowledge Graphs, AI transparency},
abstract = {Auditing complex Artificial Intelligence (AI) systems is gaining importance in light of new regulations and is particularly challenging in terms of system complexity, knowledge integration, and differing transparency needs. Current AI auditing tools however, lack semantic context, resulting in difficulties for auditors in effectively collecting and integrating, but also for analysing and querying audit data. In this position paper, we explore how Knowledge Graphs (KGs) can address these challenges by offering a structured and integrative approach to collecting and transforming audit traces. This work discusses the current limitations in both AI auditing processes and tools. Furthermore, we examine how KGs can play a transformative role in overcoming these obstacles to achieve improved auditability and transparency of AI systems.}
}
@article{AMINIFARSANI2025101208,
title = {Charting L2 argumentative writing: A systematic review},
journal = {Journal of Second Language Writing},
volume = {68},
pages = {101208},
year = {2025},
issn = {1060-3743},
doi = {https://doi.org/10.1016/j.jslw.2025.101208},
url = {https://www.sciencedirect.com/science/article/pii/S1060374325000335},
author = {Mohammad {Amini Farsani} and Paul Stapleton and Hamid R. Jamali},
keywords = {Argumentative writing, Research synthesis, L2},
abstract = {Instilling in students the ability to argue effectively is one of the most important responsibilities of educators at any level. This may be why so much focus is put on the quality of reasoning in schools and universities both in L1 and L2 contexts, especially when arguments appear in written form. Research conducted on L2 written argumentation has covered myriad aspects in educational contexts which calls for the need to provide a big picture view of the types of studies conducted and their associated scholars. Accordingly, in the present study, we reviewed 108 articles on L2 argumentative writing research from 2003 to 2023 adopting a synthetic approach to investigate the impact of theoretical orientations, research methodology, and the main topics of interest. The findings revealed L2 written argumentation has gathered global interest across all educational levels. Primary topic focuses were on language usage, pedagogy, and assessment, while there was less interest in the actual quality of the argumentative content. The most frequently used theoretical frameworks were theories related to modified Toulmin models, cognition, society and culture (i.e., sociocultural theory), linguistic complexity, and genre. Research methodologies were mostly quantitative. Implications and recommendations for those working on L2 argumentative writing are discussed.}
}
@article{ANDERSSON2018118,
title = {Wickedness and the anatomy of complexity},
journal = {Futures},
volume = {95},
pages = {118-138},
year = {2018},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0016328717300034},
author = {Claes Andersson and Petter Törnberg},
keywords = {Wicked problems, Future, Sustainability, Sociotechnical systems, Complexity, Innovation},
abstract = {Traditional scientific policy approaches and tools are increasingly seen as inadequate, or even counter-productive, for many purposes. In response to these shortcomings, a new wave of approaches has emerged based on the idea that societal systems are irreducibly complex. The new categories that are thereby introduced – like “complex” or “wicked” – suffer, however, by a lack of shared understanding. We here aim to reduce this confusion by developing a meta-ontological map of types of systems that have the potential to “overwhelm us”: characteristic types of problems, attributions of function, manners of design and governance, and generating and maintaining processes and phenomena. This permits us, in a new way, to outline an inner anatomy of the motley collection of system types that we tend to call “complex”. Wicked problems here emerge as the product of an ontologically distinct and describable type of system that blends dynamical and organizational complexity. The framework is intended to provide systematic meta-theoretical support for approaching complexity and wickedness in policy and design. We also points to a potential causal connection between innovation and wickedness as a basis for further theoretical improvement.}
}
@article{BUCHE2019104843,
title = {Expertise-based decision support for managing food quality in agri-food companies},
journal = {Computers and Electronics in Agriculture},
volume = {163},
pages = {104843},
year = {2019},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2019.05.052},
url = {https://www.sciencedirect.com/science/article/pii/S0168169918317927},
author = {Patrice Buche and Bernard Cuq and Jérôme Fortin and Clément Sipieter},
keywords = {Knowledge acquisition, Knowledge extraction, Knowledge representation, Conceptual graphs, Decision support systems},
abstract = {In many agri-food companies, food quality is often managed using expertise gained through experience. Overall quality enhancement may come from sharing collective expertise. In this paper, we describe the design and implementation of a complete methodology allowing an expert knowledge base to be created and used to recommend the technical action to take to maintain food quality. We present its functional specifications, defined in cooperation with several industrial partners and technical centres over the course of several projects carried out in recent years. We propose a systematic methodology for collecting the knowledge on a given food process, from the design of a questionnaire to the synthesis of the information from completed questionnaires using a mind map approach. We then propose an original core ontology for structuring knowledge as possible causal relationships between situations of interest. We describe how mind map files generated by mind map tools are automatically imported into a conceptual graph knowledge base, before being validated and finally automatically processed in a graph-based visual tool. A specific end-user interface has been designed to ensure that end-user experts in agri-food companies can use the tool in a convenient way. Finally, our approach is compared with current research.}
}
@article{STRATHEARN2025127304,
title = {From documents to dialogue: Context matters in common sense-enhanced task-based dialogue grounded in documents},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127304},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127304},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425009261},
author = {Carl Strathearn and Yanchao Yu and Dimitra Gkatzia},
keywords = {Common sense, NLP, nlg, ptlms, Chatbot},
abstract = {Humans can engage in a conversation to collaborate on multi-step tasks and divert briefly to complete essential sub-tasks, such as asking for confirmation or clarification, before resuming the overall task. This communication is necessary as some knowledge in instructional documents can be implicit rather than grounded in the dialogue, meaning that people must rely on their own and others’ knowledge for problem-solving. We often attribute this capability to common sense, i.e., the assumption that interlocutors perceive behaviours, temporality, context, space and object properties in a similar way. To explore the significance of emulating such problem-solving capabilities, we developed a novel hybrid document-grounded dialogue system (DGDS) called ChefBot11https://github.com/NapierNLP/CiViL. leveraging the contextual understanding of a pre-trained language model and the structuring of a sequence-to-sequence model trained on a series of commonsense knowledge databases. In a human evaluation, the hybrid system proved more effective in capturing object knowledge (utility, appearance, storage, relationships, handling) and contextual knowledge (understanding of events and situations) compared to a rule-based baseline. A key finding of this paper is demonstrating how inferring context from different document sources enhances the dialogue by allowing richer and more fluid interaction. To our knowledge, this research is innovative in its scope as the first effort to model task-based dialogue grounded in commonsense knowledge across multiple documents.}
}
@article{ILYAS2025105487,
title = {A systematic review of social media-based sentiment analysis in disaster risk management},
journal = {International Journal of Disaster Risk Reduction},
volume = {123},
pages = {105487},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2025.105487},
url = {https://www.sciencedirect.com/science/article/pii/S2212420925003115},
author = {Bilal Ilyas and Ayyoob Sharifi},
keywords = {Disaster risk management, Social media, Sentiment analysis, Natural language processing, Machine learning algorithms, Lexicon-based methods},
abstract = {This study seeks to enhance understanding of how social media-based sentiment analysis can contribute to disaster risk management. As a natural language processing tool, sentiment analysis is increasingly used to extract public emotions, concerns, and responses during disasters. This systematic review analyzes 139 studies and employs the PRISMA framework for literature search and selection. It uses both inductive and deductive techniques for content analysis. The review offers an overview of the field, identifying key themes and prominent tools adopted by researchers. It reveals that various techniques, including machine learning algorithms, lexicon-based methods, and hybrid models, are employed to assess public sentiments across platforms such as X, Weibo, and Facebook. Furthermore, the review highlights the merits of social media and sentiment analysis and the significant limitations of these platforms and sentiment analysis tools. The review reveals that despite limitations, social media is a meaningful resource for gauging societal sentiments for all phases of disasters. It finds that sentiment analysis uncovers emotions essential for devising effective strategies in disaster risk management. A major gap in the existing literature is the reliance on English and Chinese sources, leading to research skewed toward data-rich countries. Another gap is the predominant use of platforms such as X and Weibo. The findings suggest the need to consider broader language inclusivity, geographic diversity, and the use of multiple data sources to enhance the effectiveness of sentiment analysis for disaster risk management. Insights from this study can be used to create more resilient communities in the face of climate change and other stressors.}
}
@article{GRIFFIER2025,
title = {Integrating Health Care Data in an Informatics for Integrating Biology & the Bedside (i2b2) Model Persisted Through Elasticsearch: Design, Implementation, and Evaluation in a French University Hospital},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/65753},
url = {https://www.sciencedirect.com/science/article/pii/S229196942500081X},
author = {Romain Griffier and Fleur Mougin and Vianney Jouhet},
keywords = {clinical data warehouse, health data integration, i2b2, Elasticsearch, medical informatics, data persistence},
abstract = {Background
The volume of digital data in health care is continually growing. In addition to its use in health care, the health data collected can also serve secondary purposes, such as research. In this context, clinical data warehouses (CDWs) provide the infrastructure and organization necessary to enhance the secondary use of health data. Various data models have been proposed for structuring data in a CDW, including the Informatics for Integrating Biology & the Bedside (i2b2) model, which relies on a relational database. However, this persistence approach can lead to performance issues when executing queries on massive data sets.
Objective
This study aims to describe the necessary transformations and their implementation to enable i2b2’s search engine to perform the phenotyping task using data persistence in a NoSQL Elasticsearch database.
Methods
This study compares data persistence in a standard relational database with a NoSQL Elasticsearch database in terms of query response and execution performance (focusing on counting queries based on structured data, numerical data, and free text, including temporal filtering) as well as material resource requirements. Additionally, the data loading and updating processes are described.
Results
We propose adaptations to the i2b2 model to accommodate the specific features of Elasticsearch, particularly its inability to perform joins between different indexes. The implementation was tested and evaluated within the CDW of Bordeaux University Hospital, which contains data on 2.5 million patients and over 3 billion observations. Overall, Elasticsearch achieves shorter query execution times compared with a relational database, with particularly significant performance gains for free-text searches. Additionally, compared with an indexed relational database (including a full-text index), Elasticsearch requires less disk space for storage.
Conclusions
We demonstrate that implementing i2b2 with Elasticsearch is feasible and significantly improves query performance while reducing disk space usage. This implementation is currently in production at Bordeaux University Hospital.}
}
@article{MELLINAANDREU2025103177,
title = {PhenoLinker: Phenotype-gene link prediction and explanation using heterogeneous graph neural networks},
journal = {Artificial Intelligence in Medicine},
volume = {167},
pages = {103177},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103177},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725001125},
author = {Jose L. {Mellina Andreu} and Luis Bernal and Antonio F. Skarmeta and Mina Ryten and Sara Álvarez and Alejandro Cisterna García and Juan A. Botía},
abstract = {The association of a given human phenotype with a genetic variant remains a critical challenge in biomedical research. We present PhenoLinker, a novel graph-based system capable of associating a score to a phenotype-gene relationship by using heterogeneous information networks and a convolutional neural network-based model for graphs, which can provide an explanation for the predictions. Unlike previous approaches, PhenoLinker integrates gene and phenotype attributes, while maintaining explainability through Integrated Gradients. PhenoLinker consistently outperforms existing models in both retrospective and temporal validation tasks. This system can aid in the discovery of new associations and in understanding the consequences of human genetic variation.}
}
@article{IBRAHIM2020106791,
title = {A fog based recommendation system for promoting the performance of E-Learning environments},
journal = {Computers & Electrical Engineering},
volume = {87},
pages = {106791},
year = {2020},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2020.106791},
url = {https://www.sciencedirect.com/science/article/pii/S0045790620306455},
author = {Taghreed S. Ibrahim and Ahmed I. Saleh and Nehad Elgaml and Mohamed M. Abdelsalam},
keywords = {Fog computing, Recommendation system, Association rules mining, Information gain, Weighting method, Ontology and fuzzy logic},
abstract = {Recently, Recommendation Systems (RSs) have gained a great interest. E-Learning is one of the most important working fields of RS in which many challenges that hinder users in discovering the most appropriate materials can be overcome. The fog computing technique can enrich E-Learning based RS as it bridges the gap between; the cloud and end devices. In this paper, we propose Fog based Recommendation System (FBRS), which can be successfully utilized for promoting the performance of the E-Learning environment. We discuss a framework to consolidate and improve EL environment through defining three modules of FBRS: (i) Class Identification Module (CIM), (ii) Subclass Identification Module (SIM), and (iii) Matchmaking Module (MM). Moreover, the FBRS approach achieves a high response time and security to overcome both personalization and synonymy. Experimental results show that FBRS outperforms are recent techniques in terms of recommendation accuracy.}
}
@article{WANG2020,
title = {Systematic Evaluation of Research Progress on Natural Language Processing in Medicine Over the Past 20 Years: Bibliometric Study on PubMed},
journal = {Journal of Medical Internet Research},
volume = {22},
number = {1},
year = {2020},
issn = {1438-8871},
doi = {https://doi.org/10.2196/16816},
url = {https://www.sciencedirect.com/science/article/pii/S143888712000031X},
author = {Jing Wang and Huan Deng and Bangtao Liu and Anbin Hu and Jun Liang and Lingye Fan and Xu Zheng and Tong Wang and Jianbo Lei},
keywords = {natural language processing, clinical, medicine, information extraction, electronic medical record},
abstract = {Background
Natural language processing (NLP) is an important traditional field in computer science, but its application in medical research has faced many challenges. With the extensive digitalization of medical information globally and increasing importance of understanding and mining big data in the medical field, NLP is becoming more crucial.
Objective
The goal of the research was to perform a systematic review on the use of NLP in medical research with the aim of understanding the global progress on NLP research outcomes, content, methods, and study groups involved.
Methods
A systematic review was conducted using the PubMed database as a search platform. All published studies on the application of NLP in medicine (except biomedicine) during the 20 years between 1999 and 2018 were retrieved. The data obtained from these published studies were cleaned and structured. Excel (Microsoft Corp) and VOSviewer (Nees Jan van Eck and Ludo Waltman) were used to perform bibliometric analysis of publication trends, author orders, countries, institutions, collaboration relationships, research hot spots, diseases studied, and research methods.
Results
A total of 3498 articles were obtained during initial screening, and 2336 articles were found to meet the study criteria after manual screening. The number of publications increased every year, with a significant growth after 2012 (number of publications ranged from 148 to a maximum of 302 annually). The United States has occupied the leading position since the inception of the field, with the largest number of articles published. The United States contributed to 63.01% (1472/2336) of all publications, followed by France (5.44%, 127/2336) and the United Kingdom (3.51%, 82/2336). The author with the largest number of articles published was Hongfang Liu (70), while Stéphane Meystre (17) and Hua Xu (33) published the largest number of articles as the first and corresponding authors. Among the first author’s affiliation institution, Columbia University published the largest number of articles, accounting for 4.54% (106/2336) of the total. Specifically, approximately one-fifth (17.68%, 413/2336) of the articles involved research on specific diseases, and the subject areas primarily focused on mental illness (16.46%, 68/413), breast cancer (5.81%, 24/413), and pneumonia (4.12%, 17/413).
Conclusions
NLP is in a period of robust development in the medical field, with an average of approximately 100 publications annually. Electronic medical records were the most used research materials, but social media such as Twitter have become important research materials since 2015. Cancer (24.94%, 103/413) was the most common subject area in NLP-assisted medical research on diseases, with breast cancers (23.30%, 24/103) and lung cancers (14.56%, 15/103) accounting for the highest proportions of studies. Columbia University and the talents trained therein were the most active and prolific research forces on NLP in the medical field.}
}
@article{YUAN2023103705,
title = {Research on the standardization model of data semantics in the knowledge graph construction of Oil&Gas industry},
journal = {Computer Standards & Interfaces},
volume = {84},
pages = {103705},
year = {2023},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2022.103705},
url = {https://www.sciencedirect.com/science/article/pii/S0920548922000721},
author = {Jingshu Yuan and Hongqi Li},
keywords = {Concept, MDR, Data semantic standardization, Knowledge Graph, Smart Field},
abstract = {At present, knowledge graphs in both general and vertical fields rarely consider the problem of data semantic standardization in the process of constructing from a global perspective, which brings trouble and loss to the sharing and interoperability of domain information. Therefore, it is very important to construct a set of standardized and semantically consistent knowledge organization model, and to propose a data semantics standardization methodology on this basis, so as to fundamentally solve this serious problem that hinders data sharing. This paper first proposes a model of data semantic standardization theoretically through the research on knowledge organization, concept triangle and concept logic and other theories, combined with ISO 1087 international standard, and proposes the important idea that the key issue of data semantic standardization is conceptual standardization as the core. Then, based on this theoretical model, a comprehensive study and combing of various vocabulary standards in the international Oil&Gas industry was carried out, and the Oil&Gas industry reference vocabulary standardization model (OIRVSM) used to guide the construction of data semantic standards in the oil and gas industry is proposed. Finally, under the guidance of the W3C best practice framework, combined with the proposed model, an operational algorithm for constructing standardized knowledge graph (ACSKG) in Oil&Gas industry and a construction example are given, which verifies the rationality and correctness of data semantic standardization model and vocabulary standard system model. The two models proposed in this paper are innovative. Although they are proposed in the context of Oil&Gas industry, they also have application and reference significance in other fields.}
}
@article{TVARDIK201896,
title = {Accuracy of using natural language processing methods for identifying healthcare-associated infections},
journal = {International Journal of Medical Informatics},
volume = {117},
pages = {96-102},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618304362},
author = {Nastassia Tvardik and Ivan Kergourlay and André Bittar and Frédérique Segond and Stefan Darmoni and Marie-Hélène Metzger},
keywords = {Epidemiology, Healthcare-associated infections, Decision support systems, Clinical, Medical records systems, computerized, Natural language processing},
abstract = {Objective
There is a growing interest in using natural language processing (NLP) for healthcare-associated infections (HAIs) monitoring. A French project consortium, SYNODOS, developed a NLP solution for detecting medical events in electronic medical records for epidemiological purposes. The objective of this study was to evaluate the performance of the SYNODOS data processing chain for detecting HAIs in clinical documents.
Materials and methods
The collection of textual records in these hospitals was carried out between October 2009 and December 2010 in three French University hospitals (Lyon, Rouen and Nice). The following medical specialties were included in the study: digestive surgery, neurosurgery, orthopedic surgery, adult intensive-care units. Reference Standard surveillance was compared with the results of automatic detection using NLP. Sensitivity on 56 HAI cases and specificity on 57 non-HAI cases were calculated.
Results
The accuracy rate was 84% (n = 95/113). The overall sensitivity of automatic detection of HAIs was 83.9% (CI 95%: 71.7–92.4) and the specificity was 84.2% (CI 95%: 72.1–92.5). The sensitivity varies from one specialty to the other, from 69.2% (CI 95%: 38.6–90.9) for intensive care to 93.3% (CI 95%: 68.1–99.8) for orthopedic surgery. The manual review of classification errors showed that the most frequent cause was an inaccurate temporal labeling of medical events, which is an important factor for HAI detection.
Conclusion
This study confirmed the feasibility of using NLP for the HAI detection in hospital facilities. Automatic HAI detection algorithms could offer better surveillance standardization for hospital comparisons.}
}
@article{ONDITI2022102992,
title = {Futuring an ‘Inclusive Knowledge Futures’ framework beyond IR theories},
journal = {Futures},
volume = {142},
pages = {102992},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2022.102992},
url = {https://www.sciencedirect.com/science/article/pii/S0016328722000921},
author = {Francis Onditi},
keywords = {IKF, Afric-rhektology, Futures studies, , , Globality, Knowledge disruption},
abstract = {This article proposes an ‘inclusive knowledge futures’ (IKF) analytical framework as an alternative for integrating African home-grown knowledge architecture (henceforth ‘Afric-rhektology’) into the west-dominated international relations (IR) thought and practice. On the basis of this Afric-rhektological knowledge mantra, I argue that to remain complacent to the current IR studies order, altogether, and to insist on resting in the moment of simple difference, is only to recoil into the obverse of a colonial universalism. It is in fact a purely deconstructive project that cannot offer an alternative to concrete forms of knowledge hegemony. The article factors in ‘disruption’ and brings new tools of analysis based on philosophies rooted in the African socio-cultural context. Disrupting the existing west-dominated fabric of IR theories allows borderless diffusion of knowledge, a reciprocal sharing of resources, cultures and technologies. By so doing, it renders hierarchies and knowledge hegemony utterly useless. Within this knowledge disruption thinking, I evoke the Afric-rhektology tools to mainstream some of the profound Africa based philosophies (Ujamaa and Ubaraza), as a way of accepting multiplicity, hybridity and inclusivity of IR and Futures Studies.}
}
@article{HU2024104599,
title = {CMBEE: A constraint-based multi-task learning framework for biomedical event extraction},
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104599},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104599},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000170},
author = {Jingyue Hu and Buzhou Tang and Nan Lyu and Yuxin He and Ying Xiong},
keywords = {Biomedical event extraction, Event constraint information, Multi-task learning},
abstract = {Objective:
Event extraction plays a crucial role in natural language processing. However, in the biomedical domain, the presence of nested events adds complexity to event extraction compared to single events, and these events usually have strong semantic relationships and constraints. Previous approaches ignored the binding connections between these complex nested events. This study aims to develop a unified framework based on event constraint information that jointly extract biomedical event triggers and arguments and enhance the performance of nested biomedical event extraction.
Material and Methods:
We propose a multi-task learning framework based on constraint information called CMBEE for the task of biomedical event extraction. The N-tuple form of event patterns is used to represent the constrained information, which is integrated into role detection and event type classification tasks. The framework use attention mechanism and gating mechanism to explore the fusion of multiple tuple information, as well as local and global constrained information fusion methods to dig further into the connections between events.
Results:
Experimental results demonstrate that our proposed method achieves the highest F1 score on a multilevel event extraction biomedical (MLEE) corpus and performs favorably on the biomedical natural language processing shared task 2013 Genia event corpus (GE 13).
Conclusions:
The experimental results indicate that modeling event patterns and constraints for multi-event extraction tasks is effective for complex biomedical event extraction. The fusion strategy proposed in this study, which incorporates different constraint information, helps to better express semantic information.}
}
@article{PUNIYA2025169181,
title = {Artificial-intelligence-driven Innovations in Mechanistic Computational Modeling and Digital Twins for Biomedical Applications},
journal = {Journal of Molecular Biology},
volume = {437},
number = {17},
pages = {169181},
year = {2025},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2025.169181},
url = {https://www.sciencedirect.com/science/article/pii/S0022283625002475},
author = {Bhanwar Lal Puniya},
keywords = {artificial intelligence, mechanistic modeling, biomedicine, medical digital twins, systems biology},
abstract = {Understanding of complex biological systems remains a significant challenge due to their high dimensionality, nonlinearity, and context-specific behavior. Artificial intelligence (AI) and mechanistic modeling are becoming essential tools for studying such complex systems. Mechanistic modeling can facilitate the construction of simulatable models that are interpretable but often struggle with scalability and parameters estimation. AI can integrate multi-omics data to create predictive models, but it lacks interpretability. The gap between these two modeling methods limits our ability to develop comprehensive and predictive models for biomedical applications. This article reviews the most recent advancements in the integration of AI and mechanistic modeling to fill this gap. Recently, with omics availability, AI has led to new discoveries in mechanistic computational modeling. The mechanistic models can also help in getting insight into the mechanism for prediction made by AI models. This integration is helpful in modeling complex systems, estimating the parameters that are hard to capture in experiments, and creating surrogate models to reduce computational costs because of expensive mechanistic model simulations. This article focuses on advancements in mechanistic computational models and AI models and their integration for scientific discoveries in biology, pharmacology, drug discovery and diseases. The mechanistic models with AI integration can facilitate biological discoveries to advance our understanding of disease mechanisms, drug development, and personalized medicine. The article also highlights the role of AI and mechanistic model integration in the development of more advanced models in the biomedical domain, such as medical digital twins and virtual patients for pharmacological discoveries.}
}
@article{HOSSEN2024101527,
title = {Exploring potential pathways and biomarkers of pancreatic cancer associated with lynch syndrome and type 2 diabetes: An integrated bioinformatics analysis},
journal = {Informatics in Medicine Unlocked},
volume = {48},
pages = {101527},
year = {2024},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2024.101527},
url = {https://www.sciencedirect.com/science/article/pii/S2352914824000832},
author = {Md. Arif Hossen and Md Tanvir Yeasin and Md. Arju Hossain and Umme Mim Sad Jahan and Moshiur Rahman and Anik Hasan Suvo and Md Sohel and Mahmuda Akther Moli and Md. Khairul Islam and Mohammad Nasir Uddin and Md Habibur Rahman},
keywords = {Inflammatory response, , Pancreatic cancer, TNF, Thujopsene, Type 2 diabetes},
abstract = {Pancreatic cancer (PC) is a devastating malignancy with intricate genetic underpinnings and a complex etiology. Emerging evidence suggests the presence of lynch syndrome (LS) and type 2 diabetes (T2D) associated susceptibility to PC. This study presents integrated computational and systems biology approaches to identify the genetic risk factors underlying the association between PC, LS, and T2D. Patient data for these three diseases have been collected from NCBI and differentially expressed genes (DEGs) identified by the GREIN web platform. Furthermore, protein-protein interaction (PPI), gene ontology (GO), and signaling pathway networks were analyzed through STRING and DAVID databases, respectively. Autodock Vina has been used for prospective analysis of ligand-protein interaction. About 60 unique common DEGs were identified by statistical analysis. In addition to the utilization of five distinct algorithms within the Cytoscape framework, we have reported three potential target candidates: TNF, CXCL1, and TNFSF10. In particular, the immune and inflammatory response, the chemokine-mediated signaling pathway, rheumatoid arthritis, and IL-17 signaling pathways emerged as prominently enriched pathways. Furthermore, the interaction of 162 phytochemicals from Nigella sativa was assessed with the identified hub proteins. Among these, thujopsene emerged as a notable ligand candidate, demonstrating the most favorable binding energy against the TNF (−9.6 kca/mol TNFSF10 (−8.5 kcal/mol), and CXCL1 (−9.1 kcal/mol) proteins. Besides, pharmacokinetics, toxicity, and drug-likeness properties of the thujopsene ligand showed an acceptable range for selection of a drug candidate. Collectively, these findings shed light on the intricate interplay of genes, pathways, and potential therapeutic compounds, providing a basis for further exploration and validation in the context of relevant diseases.}
}
@article{STRIDSLAND2024e30370,
title = {Collaborative approaches to greenhouse gas inventory in higher education: Insights from the Universities Denmark Group},
journal = {Heliyon},
volume = {10},
number = {10},
pages = {e30370},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e30370},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024064016},
author = {Thomas Stridsland and Timen M. Boeve and Søren Løkke and Hans Sanderson},
abstract = {The challenges faced by universities and private industry in estimating their emissions for decarbonisation are similar, but the task of linking emissions to university operations, namely procurement, is complex and time-consuming due to the wide range and types of purchases. Here, participatory action research is used to address these challenges, with a group of Danish universities to investigate the functions and improve the completeness of university GHG inventories. The research enabled knowledge sharing and collaboration, leading to a better understanding of the complexities and possibilities of GHG inventories. The main conclusions drawn from discussions are that the GHG inventory should serve multiple functions; an inward-facing decision support material, and an externally-facing communication tool. EXIOBASE, an environmentally extended input-output model, was identified as a useful tool for future inventories, particularly in procurement, due to its comprehensive spend-based assessment of purchases also relevant to universities. With more universities adopting spend-based practices, the presented conclusions shed light on potential risks of this method that have not yet been discussed in this context. A consensus on methodological trade-offs, relevant activities, and data considerations for a GHG inventory are reached and reflected on. As suppliers can increasingly deliver product specific climate related information, a data ontology is needed to appropriately incorporate supplier-specific data into consistent inventories without conflicting with methodological principles and upholding proprietary requirements of suppliers. Addressing challenges identified through this collaborative investigation will expand on the dialogue in the literature and help shape how universities conduct and use GHG inventories. Keywords: Sustainable university; Action Research; Consequential Attributional; GHG Protocol.}
}
@article{STERCKX2020103544,
title = {Clinical information extraction for preterm birth risk prediction},
journal = {Journal of Biomedical Informatics},
volume = {110},
pages = {103544},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103544},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420301726},
author = {Lucas Sterckx and Gilles Vandewiele and Isabelle Dehaene and Olivier Janssens and Femke Ongenae and Femke {De Backere} and Filip {De Turck} and Kristien Roelens and Johan Decruyenaere and Sofie {Van Hoecke} and Thomas Demeester},
keywords = {Clinical information extraction, Clinical decision support models, Preterm birth, Text mining},
abstract = {This paper contributes to the pursuit of leveraging unstructured medical notes to structured clinical decision making. In particular, we present a pipeline for clinical information extraction from medical notes related to preterm birth, and discuss the main challenges as well as its potential for clinical practice. A large collection of medical notes, created by staff during hospitalizations of patients who were at risk of delivering preterm, was gathered and analyzed. Based on an annotated collection of notes, we trained and evaluated information extraction components to discover clinical entities such as symptoms, events, anatomical sites and procedures, as well as attributes linked to these clinical entities. In a retrospective study, we show that these are highly informative for clinical decision support models that are trained to predict whether delivery is likely to occur within specific time windows, in combination with structured information from electronic health records.}
}
@article{GARLET2025116295,
title = {Enriching BIM-BEPS workflows to support natural ventilation simulations using AirflowNetwork},
journal = {Energy and Buildings},
volume = {347},
pages = {116295},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2025.116295},
url = {https://www.sciencedirect.com/science/article/pii/S0378778825010254},
author = {Liége Garlet and Carlos Alexandre Dias and James O’Donnell and Ana Paula Melo},
keywords = {IFC Enrichment, IFC, BIM, BEPS, Natural Ventilation, AirflowNetwork},
abstract = {Natural ventilation is a key strategy of building design that can reduce energy consumption and greenhouse gas emissions, while improving indoor air quality and thermal comfort. Although BEPS (Building Energy Performance Simulation) tools assess such systems and BIM (Building Information Modeling) facilitates data exchange via the IFC (Industry Foundation Classes) standard, current BIM-BEPS workflows present challenges like data loss and limited support for natural ventilation. This study proposes a methodology to enhance BIM-BEPS workflows by providing robust support for natural ventilation simulations using the AirflowNetwork model. The approach involved applying the Information Delivery Manual (IDM) methodology to structure the processes and organize the information requirements. Based on this structure, a semantic data dictionary aligned with the BuildingSMART Data Dictionary (bSDD) is developed, incorporating AirflowNetwork-supported systems in EnergyPlus. A Python-based add-on is created to enrich IFC models with data from the dictionary ensuring compatibility for simulation purposes. Model validation is performed through Information Delivery Specification (IDS). The proposed methodology addresses current limitations and establishes a foundation for broader applications in the energy domain, expanding its usefulness beyond this area. It is fully structured within an openBIM workflow, relying exclusively on open data standards and freely accessible tools for both development and implementation.}
}
@article{APOSTOLAKIS2023102241,
title = {Simple querying service for OpenAPI descriptions with semantic extensions},
journal = {Information Systems},
volume = {117},
pages = {102241},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2023.102241},
url = {https://www.sciencedirect.com/science/article/pii/S0306437923000777},
author = {Ioannis Apostolakis and Nikolaos Mainas and Euripides G.M. Petrakis},
keywords = {OpenAPI, REST, Web service, Service discovery, Query language},
abstract = {We introduce OpenAPI Query Language 2 (OAQL2), an SQL-like language for querying OpenAPI documents. OpenAPI is a standard format for the description of REST services, based on JSON (or YAML). A Web service capable of executing OAQL2 queries is implemented. The service stores metadata for each OpenAPI document and executes the queries on metadata to avoid the overhead of expensive join operations. It builds indexes to speed up queries, handles composite schema objects, and uses reasoning to support searching on semantic models referred to by OpenAPI properties. The run-time performance of OAQL2 has been assessed experimentally on a large database of 10,000 real service descriptions downloaded from SwaggerHub. Compared to the system implemented in our previous work, OAQL2 is proven to be faster and more complete in terms of syntax and compatibility with OpenAPI. To the best of our knowledge, there is nothing similar to SQL for OpenAPI. All other query methods are not designed for OpenAPI and require the user to be familiar with the peculiarities of OpenAPI syntax. This results in long complicated queries.}
}
@article{ZHANG2025270,
title = {Knowledge Understanding and Citation Ability Improvement Strategy Based on End-to-end GenIR Model},
journal = {Procedia Computer Science},
volume = {261},
pages = {270-278},
year = {2025},
note = {The 5th International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy (SPIoT2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.203},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925013055},
author = {Wenyan Zhang},
keywords = {End-To-End Genir Model, Knowledge Understanding, Citation Capability, Information Retrieval},
abstract = {In order to improve the machine’s knowledge understanding and citation capabilities in complex and heterogeneous information environments and ensure accuracy and efficiency, this paper constructs and optimizes an end-to-end GenIR model. The model combines the advantages of generative AI technology and information retrieval technology, aiming to automatically extract, understand and cite relevant knowledge from large amounts of text data. The paper first collects a large amount of text data in related fields, then preprocesses the data, and then builds an end-to-end GenIR model. The model is then trained and the model parameters are optimized using the back propagation algorithm. During the training process, the attention mechanism is used to enhance the model’s ability to capture key information, and adversarial training is used to improve the robustness of the model. After training, the model is fine-tuned. Experimental results show that the GenIR model reaches a maximum MAP value of 99.9% and also has significant advantages in nDCG. The average inference time is about 29% faster than the BM25 model, demonstrating dual optimization in accuracy and efficiency. This study successfully improves the machine’s knowledge understanding and citation capabilities in complex and heterogeneous information environments by building and optimizing an end-to-end GenIR model. It not only achieves optimization in accuracy and efficiency, but also provides a new solution for the field of information retrieval.}
}
@article{DIWAN2023100110,
title = {AI-based learning content generation and learning pathway augmentation to increase learner engagement},
journal = {Computers and Education: Artificial Intelligence},
volume = {4},
pages = {100110},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100110},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000650},
author = {Chaitali Diwan and Srinath Srinivasa and Gandharv Suri and Saksham Agarwal and Prasad Ram},
keywords = {Learner engagement, Open educational resources, Curating learning pathways, Educational content generation, Language models, Definition generation, Automatic question generation, Multiple choice question},
abstract = {Retaining learner engagement is a major challenge in online learning environments, which is even more intensified with learning spaces increasingly built by combining resources from multiple independent sources. Narrative-centric learning experience has been found to improve learner engagement by several researchers. Towards this end, we propose an AI-based approach that generates auxiliary learning content called narrative fragments which are interspersed into the learning pathways to create interactive learning narratives. The proposed approach consists of the automatic generation of two types of narrative fragments– overviews of the learning pathway segments and reflection quizzes or formative assessments from learning resources in any format including open educational resources. The pipeline for the generation of the narrative fragments consists of various components based on different semantic models and a natural language generation (NLG) component based on a pre-trained language model GPT-2 (Generative Pre-trained Transformer 2). Automation enables the generation of narrative fragments on the fly whenever there are changes in the learning pathway due to the need for reiteration of concepts, pre-requisite knowledge acquisition, etc., enabling adaptability in the learning pathways. The proposed approach is domain agnostic which makes it easily adaptable to different domains. The NLG model is evaluated using ROUGE scores against several baselines. Automatically generated narrative fragments are evaluated by human evaluators. We obtained encouraging results in both cases.}
}
@article{WANG2020102285,
title = {Mapping the subaquatic animals in the Aquatocene: Offshore wind power, the materialities of the sea and animal soundscapes},
journal = {Political Geography},
volume = {83},
pages = {102285},
year = {2020},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2020.102285},
url = {https://www.sciencedirect.com/science/article/pii/S0962629820303486},
author = {Chi-Mao Wang and Ker-Hsuan Chien},
keywords = {Aquatocene, Marine animal soundscapes, Volume, Sea, Offshore wind power},
abstract = {This paper examines the controversy around marine animal management in Taiwan Strait, where the sea has been territorialized by offshore wind power developers. Concerns have been focused on the impacts of pile-driving noise on the Taiwanese white dolphin which is on the IUCN list of extremely endangered species. To deal with the problem, both state and developers have been involved in volumetric practices which attempt to render the marine mammals knowable, and in turn, governable. While recent work on volumetric thinking has revealed that power is exercised through volume, we contend that insufficient attention has been given to those lively ‘non-human’ subjects living in the volumetric spaces. Inspired by recent scholarship on animal atmospheres and the wet ontologies, we argue that marine animals are sentient beings that cannot be known, or mapped, by the state-corporate volumetric practices which are mainly based on scientific experiments, conducted in isolated social contexts. To illustrate this, we draw on assemblage thinking and develop the idea of marine animal soundscapes. We suggest that marine animal soundscapes exist only through the embodied experience of a sensory, marine animal body, which is able to affect, and learn to be affected by, others, through non-linguistic ‘signs’, such as sound. We maintain that animal soundscapes are shaped by social, ecological and material circumstances, of which, the materialities of the sea, such as its liquidity, rapidity and fluidity, are of great importance. With an emphasis on the subaquatic animal soundscapes, our approach intends to extend social relationality to non-humans and calls for an ontology, distinct from the one with which existing volumetric analysts work.}
}
@article{EIDEN2021690,
title = {Supporting semantic PLM by using a lightweight engineering metadata mapping engine},
journal = {Procedia CIRP},
volume = {100},
pages = {690-695},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.146},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121006211},
author = {Andreas Eiden and Thomas Eickhoff and Jonas Gries and Jens C. Göbel and Thomas Psota},
keywords = {PLM, Data Integration, Data Modeling, Data Mapping, Interoperability, Open Web Standards},
abstract = {In order to handle a high variety of interdisciplinary processes and complex smart products, integration platforms are a useful approach to view and access data all along the product lifecycle, which is stored in different data management solutions like Product Lifecycle Management (PLM), Enterprise Resource Planning (ERP) and authoring systems. Here, the Metadata Repository for Semantic Product Lifecycle Management (SP²IDER) could serve as a supporting integration platform. An additional information layer on top of data source systems like PLM and ERP provides additional information, links data objects from different source systems, and provides access to these data. The SP²IDER platform consists of three basic parts: Connector units to fetch data from the source systems, a core unit with a Service Directory and a Mapping Engine, and a Metadata Store, where information about data objects is stored. This paper focuses on the view inside the Mapping Engine and the Metadata Store. The mapping engine has a three-way approach for the mapping of data objects and data types: The initial manual mapping at the data type level, second a rule-based, and third a machine-learning mapping at the data object level. This paper describes the manual mapping process, how the mapped data objects are stored inside the Metadata Store, and how this leads to a newly formed lightweight data model, that does not need heavyweight ontologies.}
}
@article{DIXIT2022100314,
title = {Towards user-centered and legally relevant smart-contract development: A systematic literature review},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100314},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100314},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21001072},
author = {Abhishek Dixit and Vipin Deval and Vimal Dwivedi and Alex Norta and Dirk Draheim},
keywords = {Blockchain, Smart contract, Ricardian contract, Business collaboration, Legal relevance},
abstract = {Smart contracts (SC) run on blockchain technology (BCT) to implement agreements between several parties. As BCT grows, organizations aim to automate their processes and engage in business collaborations using SCs. The translation of contract semantics into SC language semantics is difficult due to ambiguous contractual interpretation by the several parties and the developers. Also, an SC language itself misses the language constructs needed for semantically expressing collaboration terms. This leads to SC coding errors that result in contractual conflicts over transactions during the performance of SCs and thus, novel SC solutions incur high development and maintenance costs. Various model-based and no/low code development approaches address this issue by enabling higher abstractions in SC development. Still, the question remains unanswered how contractual parties, i.e., end-users with non-IT skills, manage to develop legally relevant SCs with ease. This study aims to (1) identify and categorize the state of the art of SC automation models, in terms of their technical features, and their legal significance, and to (2) identify new research opportunities. The review has been conducted as a systematic literature review (SLR) that follows the guidelines proposed by Kitchenham for performing SLRs in software-engineering. As a result of the implementation of the review protocol, 1367 papers are collected, and 33 of them are selected for extraction and analysis. The contributions of this article are threefold: (1) 10 different SC automation models/frameworks are identified and classified according to their technical and implementation features; (2) 11 different legal contract parameters are identified and categorized into 4 legal criteria classes; (3) a comparative analysis of SC-automation models in the context of their legal significance is conducted that identifies the degrees to which the SC-automation models are considered legally relevant. As a conclusion, we produce a comprehensive and replicable overview of the state of the art of SC automation models and a systematic measure of their legal significance to benefit practitioners in the field.}
}
@article{LIMA2025125,
title = {A Deep Learning Approach for Identifying Similarities in Contracted Object Description in Public Procurement},
journal = {Procedia Computer Science},
volume = {264},
pages = {125-136},
year = {2025},
note = {International Neural Network Society Workshop on Deep Learning Innovations and Applications 2025},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.07.124},
url = {https://www.sciencedirect.com/science/article/pii/S187705092502174X},
author = {Weslley E.M. Lima and Victor R. {da Silva} and Jasson C. {da Silva} and Ricardo de {A.L. Rabelo} and Anselmo C. {de Paiva}},
keywords = {Textual similarity, Public Procurement, NLP},
abstract = {The digitization of public procurement has led to the automation of data analysis in public administration. However, using natural language in contract details still poses challenges in analysis and auditing. The large volume of unstructured data in bidding notices calls for tools that automate data analysis for citizens and auditing authorities. The description of the tender object is a critical problem in data analysis, as it lacks standardization and often contains unclear content. Detecting similarities between contract objects is crucial for analyzing public purchases and identifying fraudulent practices. However, manually reading all contracts to notice similarities is tedious and impractical. Therefore, automating the identification of similarities is essential. We propose a deep learning-based solution that uses the Euclidean distance between contextualized embeddings of contracted object descriptions to detect similarity. To improve the performance of the models, we used Named Entity Recognition (NER) to extract the central idea of the contract object, excluding confusing words and expressions. This approach improves the detection of textual similarity in public procurement objects. In general, these advances are designed to facilitate the timely analysis of public contracts.}
}
@article{CHEN20241087,
title = {Mechanism of Polygala-Acorus in Treating Autism Spectrum Disorder Based on Network Pharmacology and Molecular Docking},
journal = {Current Computer-Aided Drug Design},
volume = {20},
number = {7},
pages = {1087-1099},
year = {2024},
issn = {1573-4099},
doi = {https://doi.org/10.2174/0115734099266308231108112058},
url = {https://www.sciencedirect.com/science/article/pii/S1573409924000144},
author = {Haozhi Chen and Changlin Zhou and Wen Li and Yaoyao Bian},
keywords = {Network pharmacology, molecular docking,   , autism spectrum disorder, drugs and disease, molecular mechanism},
abstract = {Background
Recent epidemic survey data have revealed a globally increasing prevalence of autism spectrum disorders (ASDs). Currently, while Western medicine mostly uses a combination of comprehensive intervention and rehabilitative treatment, patient outcomes remain unsatisfactory. Polygala-Acorus, used as a pair drug, positively affects the brain and kidneys, and can improve intelligence, wisdom, and awareness; however, the underlying mechanism of action is unclear.
Objectives
We performed network pharmacology analysis of the mechanism of Polygala–Acorus in treating ASD and its potential therapeutic effects to provide a scientific basis for the pharmaceutical’s clinical application.
Methods
The chemical compositions and targets corresponding to Polygala–Acorus were obtained using the Traditional Chinese Medicine Systematic Pharmacology Database and Analysis Platform, Chemical Source Website, and PharmMapper database. Disease targets in ASD were screened using the DisGeNET, DrugBank, and GeneCards databases. Gene Ontology functional analysis and metabolic pathway analysis (Kyoto Encyclopedia of Genes and Genomes) were performed using the Metascape database and validated via molecular docking using AutoDock Vina and PyMOL software.
Results
Molecular docking analysis showed that the key active components of Polygala-Acorus interacted with the following key targets: EGFR, SRC, MAPK1, and ALB. Thus, the key active components of Polygala-Acorus (sibiricaxanthone A, sibiricaxanthone B tenuifolin, polygalic acid, cycloartenol, and 8-isopentenyl-kaempferol) have been found to bind to EGFR, SRC, MAPK1, and ALB.
Conclusion
This study has preliminarily revealed the active ingredients and underlying mechanism of Polygala-Acorus in the treatment of ASD, and our predictions need to be proven by further experimentation.}
}
@article{SCHNEIDER20241210,
title = {Approach for Linking System Architecture and Business Model based on the Example of Circular Value Propositions},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {19},
pages = {1210-1215},
year = {2024},
note = {18th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.09.088},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324014885},
author = {Benjamin Schneider and Helge Spindler and Mehmet Kürümlüoglu},
keywords = {Advanced Systems Engineering, Business Model, Requirements Management, Sustainability, Circularity},
abstract = {Dynamic and volatile market environments influenced by sustainability and circularity are core topics impacting today’s value creation. They impact business on different levels, ranging from strategy and management to part and production design. In the common systems engineering RFLP-understanding they can be seen as additional requirements to be considered during product development and to be balanced with other requirements derived from stakeholders or regulation. Additionally, circular product design can offer new opportunities for design of business models and strategy on product or company level. This paper presents an approach for coupling of two levels of product related decision making utilizing the concepts of model-based systems engineering. The levels are (1) strategy and business-model related decisions, (2) product architecture including requirements. The presented approach can serve as a support for system architects and business model designers.}
}
@article{LU2018128,
title = {Resource virtualization: A core technology for developing cyber-physical production systems},
journal = {Journal of Manufacturing Systems},
volume = {47},
pages = {128-140},
year = {2018},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0278612518300657},
author = {Yuqian Lu and Xun Xu},
keywords = {Smart factory, Cyber-physical production system, Resource virtualization, Digital twin, Ontology, Semantic web},
abstract = {Smart factory in the context of Industry 4.0 is the next wave of smart manufacturing solution to empower companies to rapidly configure manufacturing facilities and processes to enable the fast production of individualized products at change scales. A key enabling technology for developing a smart factory is resource virtualization or creation of digital twins. The presented research fills the gap that the industry needs a practical methodology to enable themselves to easily virtualize their manufacturing assets for developing a smart factory solution. A test-driven resource virtualization framework is proposed as the recommendation for the industry to adopt to create digital twins for a smart factory. The proposed framework draws inspiration from past resource virtualization outcomes with special attention paid to the usability of the proposed framework in a business environment. It provides a straightforward process for companies to create digital twins by specifying the digital twin hierarchy, the information to be modeled, and the modeling method. To validate the proposed framework, a case study was undertaken at an international company, to create digital twins for all their manufacturing resources. The testing result showed that the proposed resource virtualization framework and developed tools are easy to use in a practical business environment to virtualize complex factory setups in the cyberspace.}
}
@article{ZHAO2023e18622,
title = {Luteolin and triptolide: Potential therapeutic compounds for post-stroke depression via protein STAT},
journal = {Heliyon},
volume = {9},
number = {8},
pages = {e18622},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e18622},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023058309},
author = {Tianyang Zhao and Siqi Sun and Yueyue Gao and Yuting Rong and Hanwenchen Wang and Sihua Qi and Yan Li},
keywords = {Post-stroke depression (PSD), Text mining, Network, DeepPurpose, GO function, Compound},
abstract = {Post stroke depression (PSD) is a common neuropsychiatric complication following stroke closely associated with the immune system. The development of medications for PSD remains to be a considerable challenge due to the unclear mechanism of PSD. Multiple researches agree that the functions of gene ontology (GO) are efficient for the investigation of disease mechanisms, and DeepPurpose (DP) is extremely valuable for the mining of new drugs. However, GO terms and DP have not yet been applied to explore the pathogenesis and drug treatment of PSD. This study aimed to interpret the mechanism of PSD and discover important drug candidates targeting risk proteins, based on immune-related risk GO functions and informatics algorithms. According to the risk genes of PSD, we identified 335 immune-related risk GO functions and 37 compounds. Based on the construction of the GO function network, we found that STAT protein may be a pivot protein in underlying the mechanism of PSD. Additionally, we also established networks of Protein-Protein Interaction as well as Gene-GO function to facilitate the evaluation of key genes. Based on DP, a total of 37 candidate compounds targeting 7 key proteins were identified with a potential for the therapy of PSD. Furthermore, we noted that the mechanisms by which luteolin and triptolide acting on STAT-related GO function might involve three crucial pathways, including specifically hsa04010 (MAPK signaling pathway), hsa04151 (PI3K-Akt signaling pathway) and hsa04060 (Cytokine-cytokine receptor interaction). Thus, this study provided fresh and powerful information for the mechanism and therapeutic strategies of PSD.}
}
@article{LI2022479,
title = {Automated Radiology-Arthroscopy Correlation of Knee Meniscal Tears Using Natural Language Processing Algorithms},
journal = {Academic Radiology},
volume = {29},
number = {4},
pages = {479-487},
year = {2022},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2021.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S107663322100026X},
author = {Matthew D. Li and Francis Deng and Ken Chang and Jayashree Kalpathy-Cramer and Ambrose J. Huang},
keywords = {Natural language processing, Machine learning, Radiology-arthroscopy correlation, Knee MRI, Meniscal tear},
abstract = {Rationale and Objectives
Train and apply natural language processing (NLP) algorithms for automated radiology-arthroscopy correlation of meniscal tears.
Materials and Methods
In this retrospective single-institution study, we trained supervised machine learning models (logistic regression, support vector machine, and random forest) to detect medial or lateral meniscus tears on free-text MRI reports. We trained and evaluated model performances with cross-validation using 3593 manually annotated knee MRI reports. To assess radiology-arthroscopy correlation, we then randomly partitioned this dataset 80:20 for training and testing, where 108 test set MRIs were followed by knee arthroscopy within 1 year. These free-text arthroscopy reports were also manually annotated. The NLP algorithms trained on the knee MRI training dataset were then evaluated on the MRI and arthroscopy report test datasets. We assessed radiology-arthroscopy agreement using the ensembled NLP-extracted findings versus manually annotated findings.
Results
The NLP models showed high cross-validation performance for meniscal tear detection on knee MRI reports (medial meniscus F1 scores 0.93–0.94, lateral meniscus F1 scores 0.86–0.88). When these algorithms were evaluated on arthroscopy reports, despite never training on arthroscopy reports, performance was similar, though higher with model ensembling (medial meniscus F1 score 0.97, lateral meniscus F1 score 0.99). However, ensembling did not improve performance on knee MRI reports. In the radiology-arthroscopy test set, the ensembled NLP models were able to detect mismatches between MRI and arthroscopy reports with sensitivity 79% and specificity 87%.
Conclusion
Radiology-arthroscopy correlation can be automated for knee meniscal tears using NLP algorithms, which shows promise for education and quality improvement.}
}
@incollection{JIN2018103,
title = {Chapter 7 - Effect-Oriented System Capability},
editor = {Zhi Jin},
booktitle = {Environment Modeling-Based Requirements Engineering for Software Intensive Systems},
publisher = {Morgan Kaufmann},
address = {Oxford},
pages = {103-126},
year = {2018},
isbn = {978-0-12-801954-2},
doi = {https://doi.org/10.1016/B978-0-12-801954-2.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019542000078},
author = {Zhi Jin},
keywords = {Effect-oriented capability profile, Environment ontology, Environment entity state transition, System entity capability},
abstract = {In this chapter, the capability description of system entity is proposed based on environment ontology. System capabilities can be grounded on their effects on environment entities. Finally, such a capability description allows system entities to aggregate autonomously for capability composition.}
}
@article{FELDMAN2022105058,
title = {Meaning and reference from a probabilistic point of view},
journal = {Cognition},
volume = {223},
pages = {105058},
year = {2022},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2022.105058},
url = {https://www.sciencedirect.com/science/article/pii/S0010027722000464},
author = {Jacob Feldman and Lee-Sun Choi},
keywords = {Bayesian cognitive science, Meaning, Reference, Information theory, Kullback-Leibler divergence},
abstract = {The rise of Bayesian models of cognition requires that traditional questions in epistemology and metaphysics, such as how models relate to reality and how one observer's models relate to another's, be reframed in probabilistic terms. In this paper we take up these questions beginning from a subjective (Bayesian) conception of probability, in which distinct observers hold potentially different probabilistic models of the world, with no one observer necessarily possessing the “true” one. The key question is what terms in a probabilistic theory mean—that is, what they refer to and what their truth conditions are. We address this question with tools from information theory. We introduce the translation uncertainty, a generalization of the Kullback–Leibler divergence that expresses the discrepancy between two observers’ probabilistic models of a common environment. We derive a number of basic information-theoretic relationships among observers, showing for example that the probability that two Bayesian observers will classify the world similarly (called the concordance) depends on the translation uncertainty between their respective models of the world. Our framework suggests a pathway to a semantics for a “probabilistic language of thought.”}
}
@article{FELICETTI201812,
title = {The Molecular Communications Markup Language (MolComML)},
journal = {Nano Communication Networks},
volume = {16},
pages = {12-25},
year = {2018},
issn = {1878-7789},
doi = {https://doi.org/10.1016/j.nancom.2018.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1878778917301552},
author = {Luca Felicetti and Simon S. Assaf and Mauro Femminella and Gianluca Reali and Eduard Alarcon and Josep Sole-Pareta},
keywords = {Molecular communications, Markup language, Simulation, Standardization},
abstract = {Molecular Communications is a research area which combines the expertise of two main fields of science: biology and engineering. This multidisciplinarity, in addition to being a fruitful strength, makes knowledge exchange between researchers difficult. This issue has led to the definition of a new markup language, named Molecular Communication Markup Language (MolComML). It contributes to the harmonization of cross-disciplinary research activities. By leveraging the typical flexibility of markup languages, it specifies the essential components of generic molecular communication scenarios, by combining both numerical analysis and experimental synthesis. In this paper we illustrate the architecture of MolComML and focus on its peculiarities that allow describing molecular communications systems. In addition, we evaluate the performance of some molecular communication systems by using different simulation packages, configured by using the MolComML, in order to demonstrate both its portability and the possibility of combining different platforms for creating complex simulation tools.}
}
@article{BRUNET2021834,
title = {Studying Projects Processually},
journal = {International Journal of Project Management},
volume = {39},
number = {8},
pages = {834-848},
year = {2021},
issn = {0263-7863},
doi = {https://doi.org/10.1016/j.ijproman.2021.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S026378632100123X},
author = {Maude Brunet and Fernando Fachin and Ann Langley},
keywords = {process research, project studies, qualitative research, time},
abstract = {Projects are inherently processual, and many project management scholars have begun to mobilize a process view, albeit sometimes implicitly. This methodological article presents four distinct process perspectives (process as evolution, process as narrative, process as activity and process as “withness”) that are grounded in different ontologies. We contribute to the literature on project management by identifying the distinctive implications of each perspective for research designs, data collection and analysis and by exploring their potential contributions to project management knowledge, drawing on examples from the project management and organization studies literatures. We further show how different perspectives can offer insight into different facets of project temporality (linear vs. momentary; objective vs. subjective, past, present or future), and we explore the varied visual representations they give rise to. We also reveal the multiple ways in which the notion of performance may be treated in process studies (as outcome, as input, as performativity, as theatre, as accomplishment), highlighting new modes of thinking about what performance might mean. Finally, the paper considers the dilemmas and limitations of each perspective and proposes opportunities for future development.}
}
@article{MENTZINGEN2024100247,
title = {Textual similarity for legal precedents discovery: Assessing the performance of machine learning techniques in an administrative court},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {2},
pages = {100247},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100247},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000363},
author = {Hugo Mentzingen and Nuno António and Fernando Bacao and Marcio Cunha},
keywords = {Language processing, Court automation, Case similarity, Imbalanced data},
abstract = {The importance of legal precedents in ensuring consistent jurisprudence is undisputed. Particularly in jurisdictions following the Common law, but even in Civil law systems, uniformity in case law requires adherence to precedents. However, with the growing volume of cases, manual identification becomes a bottleneck, prompting the need for automation. Leveraging the capabilities of natural language processing (NLP) and machine learning (ML), our study delves into the potential of automation in identifying similar cases indicative of precedents. Drawing from a unique, substantial dataset of legal cases from an administrative court in Brazil, we extensively evaluated over one hundred combinations of document representations and text vectorizations. Contrary to earlier studies that relied on minimal validation samples, ours employed a statistically significant sample vetted by legal experts. Our findings reveal that models focusing on granular text representations perform optimally, especially when extracting concepts and relations. Notably, while intricate models may not always guarantee superior outcomes, the importance of refining textual features cannot be understated. These findings pave the way for creating efficient decision support systems in judicial contexts and set a direction for future research aiming to integrate technology in legal decision-making.}
}
@article{MEDITSKOS2018169,
title = {Multi-modal activity recognition from egocentric vision, semantic enrichment and lifelogging applications for the care of dementia},
journal = {Journal of Visual Communication and Image Representation},
volume = {51},
pages = {169-190},
year = {2018},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318300154},
author = {Georgios Meditskos and Pierre-Marie Plans and Thanos G. Stavropoulos and Jenny Benois-Pineau and Vincent Buso and Ioannis Kompatsiaris},
keywords = {Instrumental activity recognition, Egocentric camera, Mechanical measurements, Visual cues, Ontologies, Semantic knowledge graphs},
abstract = {We describe a framework for lifelogging monitoring in the scope of dementia care, based on activity recognition from egocentric vision and semantic context-enrichment. As pure vision-based approaches appear to be already saturating in terms of recognition accuracy, we propose their enhancement with wearable bracelet accelerometer information. For that purpose, we design and study appropriate early and late fusion schemes to increase accuracy. The incorporation of mechanical variables, such as jerk, improves the recognition accuracy of activities that require fine motion. In addition, we describe a framework for semantic activity representation and interpretation, using Semantic Web technologies for building interoperable activity graphs. The system is personalized, as deployment-specific activity models are authored, while problems related to the disease are detected by rules. Complemented by lifelogging applications, the system is able to support interventions by clinicians, and endorse a feeling of safety and inclusion for end-users and their carers.}
}
@article{BOBILLO20181,
title = {Reasoning within Fuzzy OWL 2 EL revisited},
journal = {Fuzzy Sets and Systems},
volume = {351},
pages = {1-40},
year = {2018},
note = {Theme: Computer Science},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2018.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0165011418301064},
author = {Fernando Bobillo and Umberto Straccia},
keywords = {Fuzzy ontologies, Fuzzy description logics, Tractable reasoning},
abstract = {Description Logics (DLs) are logics with interesting representational and computational features and are at the core of the Web Ontology Language OWL 2 and its profiles among which there is OWL 2 EL. The main feature of OWL 2 EL is that instance/subsumption checking can be decided in polynomial time. On the other hand, fuzzy DLs have been proposed as an extension to classical DLs with the aim of dealing with fuzzy concepts and we focus here on Fuzzy OWL 2 EL under standard and Gödel semantics. We provide some reasoning algorithms showing that instance/subsumption checking decision problems remain polynomial time for Fuzzy OWL 2 EL. We also identify some issues in previous related work (essentially incompleteness problems).}
}
@article{BANEGAS2023102978,
title = {“What if it's been space all this time?”: Understanding the spatiality of language teacher education},
journal = {System},
volume = {113},
pages = {102978},
year = {2023},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2022.102978},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X22002615},
author = {Darío Luis Banegas},
keywords = {Space, Spatiality, Language teacher education, Matter, Covid-19, Walking methodologies},
abstract = {Space is critical in teacher education because it shapes and is shaped by different aspects of teacher learning. This study examines how a group of language teacher educators and student-teachers understood space and the effects of that understanding on their practices and conceptions before, during, and after Covid-19 restrictions. Set in Argentina, the study collected data by means of walking interviews, drawings, photos, and follow-up interviews. Qualitative content analysis shows that space and matter can exert a powerful influence on educational trajectories as they affect educational actors’ wellbeing, perceptions, and practices. The study proposes a model of spatiality of language teacher education.}
}
@article{GUO2024121448,
title = {A method for constructing a machining knowledge graph using an improved transformer},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121448},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121448},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019504},
author = {Liang Guo and Xinling Li and Fu Yan and Yuqian Lu and Wenping Shen},
keywords = {Knowledge Engineering, Machining, Transformer, Knowledge Graph},
abstract = {Process knowledge base is a core component in the intelligent process, which determines the intelligent degree of product manufacturing and directly affects the production efficiency of products. However, traditional process knowledge base is often constructed manually, which is difficult and time-consuming. In addition, in the field of machining, there is a large amount of unstructured invisible process knowledge, which is not effectively organized and managed. To make use of this knowledge and provide knowledge support for downstream production and maintenance, a process knowledge base construction framework is proposed by using Knowledge Graph (KG) technology. Firstly, the ontology rules of process knowledge are designed from the perspective of the processing method of process characteristics according to the particularity of knowledge in the machining field. The process KG schema layer is then constructed. Secondly, a neural network BERT–Improved TRANSFORMER–CRF (BITC) model is proposed for the machining knowledge extraction task, and the data layer is constructed. Then, entity linking and knowledge fusion are performed by using the word vector cosine similarity algorithm and stored in Neo4j. The process KG is then constructed. Finally, the effectiveness of the proposed method is verified by using an aero-engine casing of an enterprise as an example. Under the same dataset, the BITC model scored higher than several other classical models. The Precision, Recall, and F1-score were 85.27%, 86.40%, and 85.83 %, respectively.}
}
@article{SANCHEZ2020879,
title = {Semantic-based privacy settings negotiation and management},
journal = {Future Generation Computer Systems},
volume = {111},
pages = {879-898},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.10.024},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18317035},
author = {Odnan Ref Sanchez and Ilaria Torre and Bart P. Knijnenburg},
keywords = {Privacy Preference Manager, Privacy ontology, Data sharing and permission management, IoT},
abstract = {By 2020, an individual is expected to own an average of 6.58 devices that share and integrate a wealth of personal user data. The management of privacy preferences across these devices is a complex task for which users are ill-equipped, which increases privacy risks. In this paper we propose an approach that exploits Semantic Web (SW) technology to manage the user’s IoT privacy preferences and negotiate the permissions for data sharing with third parties. SW technology comprises a web of data that can be processed by machines through a formal, universally shared representation. In our approach, SW enables a lightweight and interoperable communication between a Personal Data Manager (PDM) and the Third Parties (TPs) that request access to the user’s personal data. The PDM can handle multiple heterogeneous personal IoT devices and manages the negotiation process between the user and the TPs in a way that can relieve users from the burden of specifying their privacy requirement for each TP. The core of the approach is the definition of the Privacy Preference for IoT (PPIoT) Ontology which is based on the Privacy Preference Ontology, the W3C Semantic Sensor Network Ontology, the Fair Information Practices (FIP) principles, and state-of-the-art recommendation techniques for privacy protection in the IoT. This ontology aims to capture the complexity of privacy management in the IoT paradigm in light of the recent General Data Protection Regulation (GDPR) of the European Union. Along with presenting the ontology, in this paper we will provide an example on how to use the PPIoT ontology for the management of privacy preferences in the fitness IoT domain and we will show how the PDM handles the process of negotiation between the user and the TPs. The approach is based on an interactive PPIoT-based Privacy Preference Model (PPM) that meets the requirements of the GDPR to have transparent and simple TP privacy policies. Finally, we will report the results of an evaluation on a mockup fitness app that implements this PPM. The main contributions of this paper are: (i) to propose an ontology for privacy preference in the IoT context, which covers a knowledge gap in existing literature and can be used for IoT privacy management, (ii) to propose an interactive PPIoT-based Privacy Preference Model, which is in accordance with the GDPR objectives.}
}
@incollection{LAMURIAS202550,
title = {Text Mining for Bioinformatics Using Biomedical Literature},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {50-61},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000178},
author = {Andre Lamurias and Diana F. Sousa and Francisco M. Couto},
keywords = {Biomedical literature, Distant supervision, Event extraction, Machine Learning, Named entity recognition, Normalization, Relation extraction, Text mining},
abstract = {Biomedical literature is a large and rich source of information for various applications. Text mining tools aim at extracting information from the literature in an efficient manner since processing scientific texts is a complex task given the formal and highly specialized language. Text mining tools tackle these challenges using different approaches, such as rule-based methods and machine learning algorithms including deep learning. This document overviews the current biomedical text mining tools by describing their approaches, tasks (e.g., Named Entity Recognition, Relation Extraction, Event Extraction, Question Answering), available corpora, toolkits and applications, and community challenges.}
}
@article{ZERMATTEN2025621,
title = {Learning transferable land cover semantics for open vocabulary interactions with remote sensing images},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {220},
pages = {621-636},
year = {2025},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2025.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0924271625000061},
author = {Valérie Zermatten and Javiera Castillo-Navarro and Diego Marcos and Devis Tuia},
keywords = {Land cover mapping, Open vocabulary semantic segmentation, Vision-language model for remote sensing},
abstract = {Why should we confine land cover classes to rigid and arbitrary definitions? Land cover mapping is a central task in remote sensing image processing, but the rigorous class definitions can sometimes restrict the transferability of annotations between datasets. Open vocabulary recognition, i.e. using natural language to define a specific object or pattern in an image, breaks free from predefined nomenclature and offers flexible recognition of diverse categories with a more general image understanding across datasets and labels. The open vocabulary framework opens doors to search for concepts of interest, beyond individual class boundaries. In this work, we propose to use Text As supervision for COntrastive Semantic Segmentation (TACOSS), and we design an open vocabulary semantic segmentation model that extends its capacities beyond that of a traditional model for land cover mapping: In addition to visual pattern recognition, TACOSS leverages the common sense knowledge captured by language models and is capable of interpreting the image at the pixel level, attributing semantics to each pixel and removing the constraints of a fixed set of land cover labels. By learning to match visual representations with text embeddings, TACOSS can transition smoothly from one set of labels to another and enables the interaction with remote sensing images in natural language. Our approach combines a pretrained text encoder with a visual encoder and adopts supervised contrastive learning to align the visual and textual modalities. We explore several text encoders and label representation methods and compare their abilities to encode transferable land cover semantics. The model’s capacity to predict a set of different land cover labels on an unseen dataset is also explored to illustrate the generalization capacities across domains of our approach. Overall, TACOSS is a general method and permits adapting between different sets of land cover labels with minimal computational overhead. Code is publicly available online11https://github.com/eceo-epfl/RS-OVSS..}
}
@article{ALFOUDARI2021104282,
title = {Understanding socio-technological challenges of smart classrooms using a systematic review},
journal = {Computers & Education},
volume = {173},
pages = {104282},
year = {2021},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2021.104282},
url = {https://www.sciencedirect.com/science/article/pii/S0360131521001597},
author = {Aisha M. Alfoudari and Christopher M. Durugbo and Fairouz M. Aldhmour},
keywords = {Smart classroom, Systematic review, Socio-technological challenges, Technology-supported learning},
abstract = {Smart classrooms are paradigm innovations for enhanced learning behavior in digital learning environments. These environments offer benefits for inclusive and virtual learning, underscoring the need for assessments of current practices. Although research on smart classrooms propose models and systems for enhancing socio-technological integration, knowledge on socio-technological challenges of smart classrooms remains limited. This article applies a systematic review methodology in line with the PRISMA protocol and analyzes current social and technological challenges based on 105 articles published between 2000 and 2019. The review identifies social challenges that facilitate personalization for external factors and teaching methods, stimulate learner-oriented content, instructor, peer, and technology forms of engagement, and boost interactivity depending on the willingness of learners and instructors. The review also finds technological challenges that concern designing learning environments and integrating intelligent systems, analytical tools and analysis, system models and ontology, and mobile and social media applications. The review suggests areas for future research involving smart classroom design for continuity and consistency, quality attributes of smart classrooms, efficiency and sustainability of smart classroom infrastructure, and the development of a smart classroom modelling language.}
}
@article{QI2025105873,
title = {Linking geo-models for geomorphological classification using knowledge graphs},
journal = {Computers & Geosciences},
volume = {196},
pages = {105873},
year = {2025},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2025.105873},
url = {https://www.sciencedirect.com/science/article/pii/S0098300425000238},
author = {Yanmin Qi and Yunqiang Zhu and Shu Wang and Yutao Zhong and Stuart Marsh and Amin Farjudian and Heshan Du},
keywords = {Knowledge graph, Geographic computation, Geographic model, Geomorphological classification},
abstract = {Geographic computation is an important process in geographic information systems to detect, predict, and simulate geographic entities, events, and phenomena, which is performed through a series of geographic models over geographic data. However, selecting and sequencing appropriate models is challenging for users with limited knowledge. To automate the process of linking models into workflows, a knowledge graph-based approach is proposed. In this approach, the first part is to construct a knowledge graph that integrates knowledge from geographic models and domain experts. Then, an algorithm is designed to assist the constructed knowledge graph in automating model linking. This paper takes the geomorphological classification of the Hengduan Mountains in China as a case study, which geomorphological classification maps are generated by performing querying and computing through the geomorphological classification knowledge graph. Experimental results demonstrate that the proposed knowledge graph-based approach links the models into workflows automatically and generates reliable classification results.}
}
@article{JENNINGSDOBBS2023102006,
title = {Visualizing Data Interoperability for Food Systems Sustainability Research—From Spider Webs to Neural Networks},
journal = {Current Developments in Nutrition},
volume = {7},
number = {11},
pages = {102006},
year = {2023},
issn = {2475-2991},
doi = {https://doi.org/10.1016/j.cdnut.2023.102006},
url = {https://www.sciencedirect.com/science/article/pii/S2475299123265907},
author = {Emily M. Jennings-Dobbs and Shavawn M. Forester and Adam Drewnowski},
keywords = {interoperability, ontologies, USDA FoodData Central, agriculture, climate change, food prices, nutrition, population health},
abstract = {Food systems represent all elements and activities needed to feed the growing global population. Research on sustainable food systems is transdisciplinary, relying on the interconnected domains of health, nutrition, economics, society, and environment. The current lack of interoperability across databases poses a challenge to advancing research on food systems transformation. Crosswalks among largely siloed data on climate change, soils, agricultural practices, nutrient composition of foods, food processing, prices, dietary intakes, and population health are not fully developed. Starting with US Department of Agriculture FoodData Central, we assessed the interoperability of databases from multiple disciplines by identifying existing crosswalks and corresponding visualizations. Our visual demonstration serves as proof of concept, identifying databases in need of expansion, integration, and harmonization for use by researchers, policymakers, and the private sector. Interoperability is the key: ontologies and well-defined crosswalks are necessary to connect siloed data, transcend organizational barriers, and draw pathways from agriculture to nutrition and health.}
}
@article{MOUKRIM2021476,
title = {An innovative approach to autocorrecting grammatical errors in Arabic texts},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {4},
pages = {476-488},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818310012},
author = {Chouaib Moukrim and Tragha Abderrahim and El Habib Benlahmer and Almalki Tarik},
keywords = {Arabic, Syntactic errors, Natural language processing, Ontology},
abstract = {Natural Language Processing (NLP) has been a growing area of research in computer and cognitive sciences, using experimental approaches. Morphology and syntax play specifically a vital role in the correct interpretation of a sentence. In this paper, we will present a syntactic error correction system based on the automatic generation of correct sentences in Arabic. First, we extract the words from the considered sentence and we then generate all the possible sentences that are syntactically correct; based on a logical description of the rules of Arabic grammar in the ontology. We will afterwards compare the original sentence with the generated sentences to detect any eventual errors followed by the correction phase. In case the system has not found a sentence that looks similar to the original sentence, the correct alternative sentences are automatically offered. The use of the Arabic syntactic corrector can increase productivity and improve the quality of the text for anyone who writes in the Arabic language. Successful tests have been performed using a set of Arabic sentences. The implemented system achieved a precision rate of about 92% and a recall rate of about 84%. By observing the achieved results, it is concluded that our approach is promising.}
}
@article{CAROLAN2022208,
title = {Digitization as politics: Smart farming through the lens of weak and strong data},
journal = {Journal of Rural Studies},
volume = {91},
pages = {208-216},
year = {2022},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2020.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S0743016720306951},
author = {Michael Carolan},
keywords = {Precision agriculture, Automation, Robotics, Big data, Agriculture 4.0, Responsible innovation},
abstract = {This paper offers an alternative approach for engaging sociologically, ontologically, and politically with digital farming platforms. A challenge faced by any approach looking to upend intellectual conventions, especially ontological ones, lies in the question of representation, namely, how do we talk about digitization in novel theoretical ways using language rooted in more than two millennia of western human-centered thought? One way to deal with this challenge, the one adopted in this paper, involves decentering familiar terms and repurposing them. Thus, rather than organizing the argument around familiar terms like small and big data, I argue instead that we organize data-assemblages according to what they do, which makes this a political ontological project. To do this, I offer the relational predicate of weak data and strong data, suggesting that they are good to think with in this regard. The argument ultimately lands at a place for analyzing these platforms that offers the potential for critique and perhaps even a degree of optimism, as I suggest a framework for normatively evaluating these practices. The paper draws upon various empirical studies of digital agriculture conducted by the author, which include interviews with farmers, farm laborers, hacktivists, investors, and engineers from numerous countries and locales.}
}
@article{MAIELLA2018706,
title = {Harmonising phenomics information for a better interoperability in the rare disease field},
journal = {European Journal of Medical Genetics},
volume = {61},
number = {11},
pages = {706-714},
year = {2018},
note = {Focus on rare disease research projects supported by the E-Rare ERA-Net program},
issn = {1769-7212},
doi = {https://doi.org/10.1016/j.ejmg.2018.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S1769721217305062},
author = {Sylvie Maiella and Annie Olry and Marc Hanauer and Valérie Lanneau and Halima Lourghi and Bruno Donadille and Charlotte Rodwell and Sebastian Köhler and Dominik Seelow and Simon Jupp and Helen Parkinson and Tudor Groza and Michael Brudno and Peter N. Robinson and Ana Rath},
keywords = {Knowledge bases, Biological ontologies, Controlled vocabulary, Diagnosis, Differential},
abstract = {HIPBI-RD (Harmonising phenomics information for a better interoperability in the rare disease field) is a three-year project which started in 2016 funded via the E-Rare 3 ERA-NET program. This project builds on three resources largely adopted by the rare disease (RD) community: Orphanet, its ontology ORDO (the Orphanet Rare Disease Ontology), HPO (the Human Phenotype Ontology) as well as PhenoTips software for the capture and sharing of structured phenotypic data for RD patients. Our project is further supported by resources developed by the European Bioinformatics Institute and the Garvan Institute. HIPBI-RD aims to provide the community with an integrated, RD-specific bioinformatics ecosystem that will harmonise the way phenomics information is stored in databases and patient files worldwide, and thereby contribute to interoperability. This ecosystem will consist of a suite of tools and ontologies, optimized to work together, and made available through commonly used software repositories. The project workplan follows three main objectives:1.Full integration of the HPO and ORDO in order to create an ontological resource that allows semantic interoperability in a computable, re-usable fashion, including information on the frequency of occurrence of phenotypic traits in each RD, and allows for the identification of features that are pathognomonic or diagnostic criteria for the diagnosis of a RD. These ontologies can be integrated into computational frameworks for automated diagnosis assistance and identification of causative variants in a patient genome. The ontological resource will be made available in several languages.2.Enriching rare disease knowledge via automated concept recognition in order enable the enrichment of the Orphanet base via novel phenotypic abnormalities-disease associations. This knowledge will be integrated into the Orphanet community-driven curation process, enabling experts to manually validate the output of the concept recognition process, and thus ensuring that the automatically extracted knowledge from publications conforms to Orphanet's high quality standards. Analysis of manually validated entries will also allow improvement of the concept recognition mechanism.3.Develop crowdsourcing for HPO/ORDO annotations in order to further improve these linkages through the efforts of clinicians, citizen scientists, RD patients, and medical students. While the annotation of any one of these persons may be inaccurate or imprecise, it has been shown in many other domains that intelligent combination of low-quality annotation by hundreds of non-experts can perform similarly to expert-level annotation. The HIPBI-RD ecosystem will contribute to the interpretation of variants identified through exome and full genome sequencing by harmonising the way phenotypic information is collected, thus improving diagnostics and delineation of RD. The ultimate goal of HIPBI-RD is to provide a resource that will contribute to bridging genome-scale biology and a disease-centered view on human pathobiology. Achievements in Year 1:-annotation of 2700 Orphanet disorder entities with relevant HPO terms together with information on frequency of occurrence of each of these phenotypic features. This represents a 44% coverage of Orphanet rare disorders. Furthermore, the available integrated Orphanet/HPO linkages have been implemented into PhenoTips, and PhenoTips is now available in Spanish and French, as well as English.-creation of Phenotate (phenotate.org), a tool that allows users to annotate ORDO nosological entities with HPO terms. This crowdsourced data will be collected and curated on an annual basis, creating a new source of HPO/ORDO linkages that will further improve the ontology.}
}
@article{GRENCZUK20245545,
title = {AI-Supported Translation Tools for Legal Texts: A Comparative Analysis},
journal = {Procedia Computer Science},
volume = {246},
pages = {5545-5554},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.707},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924027686},
author = {Andrzej Greńczuk and Iwona Chomiak-Orsa and Katarzyna Tryczyńska},
keywords = {Artificial intelligence, legal translation, legal act, machine translation},
abstract = {One of the effects of globalization is the increase in the intensity and importance of international cooperation. The context of internationalization of the functioning of organizations and international contracts has influenced the need to popularize translation services. In the case of everyday language or basic communication processes, the lack of clarity and an appropriate level of quality of translations between any language of the world can cause minor problems and communication problems. However, in the case of contracts, political protocol or legal regulations, the quality of translation processes between languages is very important. Despite the high popularity of IT translation tools, there is still a need for the services of professional, traditional translators, especially when translation processes involve highly specialized vocabulary or highly formalized studies, such as legal regulations. The aim of the article is a comparative analysis of two tools using LLM in the processes of translating legal texts into less popular languages, such as Dutch and Polish. In order to assess the possibility and quality of translation of popular translators such as DeepL and Google Translate, the authors used a scientific experiment in which a sworn translator from Dutch took part, assessing the quality and unambiguity of the translations made by IT tools.}
}
@article{DRAGONI2020101840,
title = {Explainable AI meets persuasiveness: Translating reasoning results into behavioral change advice},
journal = {Artificial Intelligence in Medicine},
volume = {105},
pages = {101840},
year = {2020},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2020.101840},
url = {https://www.sciencedirect.com/science/article/pii/S0933365719310140},
author = {Mauro Dragoni and Ivan Donadello and Claudio Eccher},
keywords = {Explainable AI, Explainable reasoning, Natural Language Generation, MHealth, Ontologies},
abstract = {Explainable AI aims at building intelligent systems that are able to provide a clear, and human understandable, justification of their decisions. This holds for both rule-based and data-driven methods. In management of chronic diseases, the users of such systems are patients that follow strict dietary rules to manage such diseases. After receiving the input of the intake food, the system performs reasoning to understand whether the users follow an unhealthy behavior. Successively, the system has to communicate the results in a clear and effective way, that is, the output message has to persuade users to follow the right dietary rules. In this paper, we address the main challenges to build such systems: (i) the Natural Language Generation of messages that explain the reasoner inconsistency; and, (ii) the effectiveness of such messages at persuading the users. Results prove that the persuasive explanations are able to reduce the unhealthy users’ behaviors.}
}
@article{MASSON2025114001,
title = {Optimal strategies to perform multilingual analysis of social content for a novel dataset in the tourism domain},
journal = {Knowledge-Based Systems},
volume = {326},
pages = {114001},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114001},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125010469},
author = {Maxime Masson and Rodrigo Agerri and Christian Sallaberry and Marie-Noelle Bessagnet and Annig {Le Parc Lacayrelle} and Philippe Roose},
keywords = {Tourism, Few-shot learning, Large language models, Masked language models, Multilinguality, Computational social science, Natural language processing},
abstract = {The rising influence of social media platforms in various domains, including tourism, has highlighted the growing need for efficient and automated Natural Language Processing (NLP) strategies to take advantage of this valuable resource. However, the transformation of multilingual, unstructured, and informal texts into structured knowledge still poses significant challenges, most notably the never-ending requirement for manually annotated data to train deep learning classifiers. In this work, we study different NLP techniques to establish the best ones to obtain competitive performances while keeping the need for training annotated data to a minimum. To do so, we built the first publicly available multilingual dataset (French, English, and Spanish) for the tourism domain, composed of tourism-related tweets. The dataset includes multilayered, manually revised annotations for Named Entity Recognition (NER) for Locations and Fine-grained Thematic Concepts Extraction mapped to the Thesaurus of Tourism and Leisure Activities of the World Tourism Organization, as well as for Sentiment Analysis at the tweet level. Extensive experimentation comparing various few-shot and fine-tuning techniques with modern language models demonstrate that modern few-shot techniques allow us to obtain competitive results for all three tasks with very little annotation data: 5 tweets per label (15 in total) for Sentiment Analysis, 30 tweets for Named Entity Recognition of Locations and 1K tweets annotated with fine-grained thematic concepts, a highly fine-grained sequence labeling task based on an inventory of 315 classes. We believe that our results, grounded in a novel dataset, pave the way for applying NLP to new domain-specific applications, reducing the need for manual annotations and circumventing the complexities of rule-based, ad-hoc solutions.}
}
@article{YANG2024242,
title = {Vision transformer-based visual language understanding of the construction process},
journal = {Alexandria Engineering Journal},
volume = {99},
pages = {242-256},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824004873},
author = {Bin Yang and Binghan Zhang and Yilong Han and Boda Liu and Jiniming Hu and Yiming Jin},
keywords = {Intelligent construction, Computer vision, Vision transformer, Natural language processing, Visual question answering},
abstract = {The widespread implementation of surveillance systems on construction sites has led to the accumulation of vast amounts of visual data, highlighting the need for an effective semantic analysis methodology. Natural language, as the most intuitive mode of expression, can significantly enhance the interpretability of such data. The adoption of multi-modality models promotes the interaction between surveillance video and textual data, thereby enabling managers to swiftly comprehend on-site dynamics. This study introduces a Visual Question Answering (VQA) approach for the construction industry and presents a specialized dataset to address the unique requirements of on-site management. Utilizing a Vision Transformer (ViT) architecture, the proposed model conducts feature extraction, fusion and interaction between visual and textual features. An additional projection layer is added to establish a transfer learning strategy that is optimized for construction site data. This novel approach facilitates rapid alignment of visual and language features in the model and is validated through ablation studies. The proposed approach achieves a testing accuracy of 83.8%, effectively converting image data from construction sites into natural language descriptions that enhance the analysis of construction processes. Compared to existing methods, this approach does not rely on object detection and allows for the direct extraction of deep-level semantic information from the on-site images. This study further discusses the feasibility of applying VQA within the architecture, engineering and construction (AEC) industry, examines its limitations, and offers suggestions for viable future directions of development.}
}
@article{VALCAMONICO2024109638,
title = {Combining natural language processing and bayesian networks for the probabilistic estimation of the severity of process safety events in hydrocarbon production assets},
journal = {Reliability Engineering & System Safety},
volume = {241},
pages = {109638},
year = {2024},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109638},
url = {https://www.sciencedirect.com/science/article/pii/S0951832023005525},
author = {Dario Valcamonico and Piero Baraldi and Enrico Zio and Luca Decarli and Anna Crivellari and Laura La Rosa},
keywords = {Quantitative risk assessment, Text mining, Taxonomy, Bayesian network, Textual reports, Oil and Gas},
abstract = {This work investigates the possibility of using the information contained in reports describing Process Safety Events (PSEs) occurred in hydrocarbon production assets to support Quantitative Risk Assessment (QRA). Specifically, a novel methodology combining Natural Language Processing (NLP) and Bayesian Networks (BNs) is proposed to estimate the probabilities of having PSEs of various classes of severity and identifying the factors that have mostly influenced their variation along the monitored period. A repository of reports of PSEs of hydrocarbons plants is considered to show the potentialities of the developed methodology. An application to a repository of reports of PSEs of hydrocarbons plants is considered to show the potentialities of the developed methodology. The results obtained in the application show that the proposed methodology allows identifying the critical factors for the severity of the consequences of PSEs. These results show that the framework can be used to inform and guide decisions about possible improvements of the system safety by mitigative and preventive barriers.}
}
@article{BALAKRISHNAN2024100188,
title = {Bioassay protocol metadata annotation: Proposed standards adoption},
journal = {SLAS Discovery},
volume = {29},
number = {8},
pages = {100188},
year = {2024},
issn = {2472-5552},
doi = {https://doi.org/10.1016/j.slasd.2024.100188},
url = {https://www.sciencedirect.com/science/article/pii/S2472555224000509},
author = {Rama Balakrishnan and Ellen L. Berg and Christopher C. Butler and Alex M. Clark and Sheryl P. Denker and Isabella Feierberg and Jason Harris and Timothy P. Ikeda and Samantha Jeschonek and Vladimir A. Makarov and Christopher Southan and Dana Vanderwall and Peter Winstanley},
keywords = {Bioassay, FAIR, Metadata, Ontology, Data standards, Assay registration, Protocol, Drug discovery},
abstract = {We present a standardized metadata template for assays used in pharmaceutical drug discovery research, according to the FAIR principles. We also describe the use of an automated tool for annotating assays from a variety of sources, including PubChem, commercial assay providers, and the peer-reviewed literature, to this metadata template. Adoption of a standardized metadata template will allow drug discovery scientists to better understand and compare the increasing amounts of assay data becoming available, and will facilitate the use of artificial intelligence tools and other computational methods for analysis and prediction. Since bioassays drive advances in biomedical research, improvements in assay metadata can improve productivity in discovery of new therapeutics, platform technologies, and assay methods.}
}
@article{VANVEEN2025100239,
title = {The emancipatory inclusion of nonhuman animals in the Dutch sustainable food system transition: An embodied critical discourse analysis},
journal = {Earth System Governance},
volume = {23},
pages = {100239},
year = {2025},
issn = {2589-8116},
doi = {https://doi.org/10.1016/j.esg.2025.100239},
url = {https://www.sciencedirect.com/science/article/pii/S2589811625000059},
author = {Anne {van Veen} and Ingrid Visseren-Hamakers},
keywords = {(1–7): animals, Interspecies democracy, Transformative governance, Critical discourse analysis, Agriculture},
abstract = {There is increasing agreement among scientists and policy makers that we need transformative change, change at the deepest levels of society, to become sustainable. There is also increasing attention for politics, power relations and justice in studying and governing sustainability transitions. This attention has however largely been limited to human politics, power relations and justice, even though other animals are also stakeholders in these transitions. This article focuses on the inclusion of nonhuman animals in the food system transition in the Netherlands. Through an embodied critical discourse analysis of academic papers and policy texts we show that there is a lack of inclusion of other animals as stakeholders, and how anthropocentric ontological and epistemological assumptions lead to reproducing power inequalities and the perpetuation of practices of nonhuman animal exclusion and oppression. In addition, our analysis demonstrates how terms such as ‘intrinsic value’ serve human rather than nonhuman interests because they are interpreted in such a way that they simultaneously assuage guilt and legitimize continued owning and killing of other animals by humans. We also contribute methodologically by adding the practice of re-embodiment of texts to the close reading method generally used in Critical Discourse Analysis, thereby making this method less anthropocentric. Finally, we propose steps towards the emancipatory inclusion of nonhuman animals in environmental governance.}
}
@article{XIE2025,
title = {Identifying Asthma-Related Symptoms From Electronic Health Records Using a Hybrid Natural Language Processing Approach Within a Large Integrated Health Care System: Retrospective Study},
journal = {JMIR AI},
volume = {4},
year = {2025},
issn = {2817-1705},
doi = {https://doi.org/10.2196/69132},
url = {https://www.sciencedirect.com/science/article/pii/S2817170525000353},
author = {Fagen Xie and Robert S Zeiger and Mary Marycania Saparudin and Sahar Al-Salman and Eric Puttock and William Crawford and Michael Schatz and Stanley Xu and William M Vollmer and Wansu Chen},
keywords = {asthma, symptom extraction, electronic health record, natural language processing, transformer-based algorithm, rule-based algorithm},
abstract = {Background
Asthma-related symptoms are significant predictors of asthma exacerbation. Most of these symptoms are documented in clinical notes in a free-text format, and effective methods for capturing asthma-related symptoms from unstructured data are lacking.
Objective
The study aims to develop a natural language processing (NLP) algorithm for identifying symptoms associated with asthma from clinical notes within a large integrated health care system.
Methods
We analyzed unstructured clinical notes within 2 years before a visit with asthma diagnosis in 2013‐2018 and 2021‐2022 to identify 4 common asthma-related symptoms. Related terms and phrases were initially compiled from publicly available resources and then refined through clinician input and chart review. A rule-based NLP algorithm was iteratively developed and refined via multiple rounds of chart review followed by adjudication. Subsequently, transformer-based deep learning algorithms were trained using the same manually annotated datasets. A hybrid NLP algorithm was then generated by combining rule-based and transformer-based algorithms. The hybrid NLP algorithm was finally applied to the implementation notes.
Results
A total of 11,374,552 eligible clinical notes with 128,211,793 sentences were analyzed. After applying the hybrid algorithm to implementation notes, at least 1 asthma-related symptom was identified in 1,663,450 out of 127,763,086 (1.3%) sentences and 858,350 out of 11,364,952 (7.55%) notes, respectively. Cough was the most frequently identified at both the sentence (1,363,713/127,763,086, 1.07%) and note (660,685/11,364,952, 5.81%) levels, while chest tightness was the least frequent at both the sentence (141,733/127,763,086, 0.11%) and note (64,251/11,364,952, 0.57%) levels. The frequency of multiple symptoms ranged from 0.03% (36,057/127,763,086) to 0.38% (484,050/127,763,086) at the sentence level and 0.10% (10,954/11,364,952) to 1.85% (209,805/11,364,952) at the note level. Validation against 1600 manually annotated clinical notes yielded a positive predictive value ranging from 96.53% (wheezing) to 97.42% (chest tightness) at the sentence level and 96.76% (wheezing) to 97.42% (chest tightness) at the note level. Sensitivity ranged from 93.9% (dyspnea) to 95.95% (cough) at the sentence level and 96% (chest tightness) to 99.07% (cough) at the note level. All 4 symptoms had F1-scores greater than 0.95 at both the sentence and note levels, regardless of NLP algorithms.
Conclusions
The developed NLP algorithms could effectively capture asthma-related symptoms from unstructured clinical notes. These algorithms could be used to facilitate early asthma detection and predict exacerbation risk.}
}
@article{JONES2025100557,
title = {I gotta use words when I talk to you: Primed suspension of disbelief in views on agency in relation to Artificial Intelligence},
journal = {Information and Organization},
volume = {35},
number = {1},
pages = {100557},
year = {2025},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2025.100557},
url = {https://www.sciencedirect.com/science/article/pii/S147177272500003X},
author = {Matthew Jones},
keywords = {Agency, Artificial intelligence},
abstract = {The public launch of generative AI systems in 2022 has prompted considerable popular interest in their potential to transform work and to achieve the long-sought goal of machine intelligence. While the uncanny abilities of Large Language Models to produce fluent text on almost any subject lends these ideas some plausibility, they are based on debatable, if not erroneous, claims about the capabilities of generative AI. Although rarely presented in these terms, these claims may be seen to reflect a limited, substantive conception of the agency of AI systems. Alternative conceptualisations of agency from other disciplines are presented that may offer potentially more fruitful ways of thinking about agency in relation to AI. The way that conceptions of the agency of AI systems is shaped by the language used to describe their behaviour is highlighted and opportunities for alternative conceptions of agency to inform a more critical analysis of generative AI are identified.}
}
@article{LEE2024111542,
title = {DSTEA: Improving Dialogue State Tracking via Entity Adaptive pre-training},
journal = {Knowledge-Based Systems},
volume = {290},
pages = {111542},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111542},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124001771},
author = {Yukyung Lee and Takyoung Kim and Hoonsang Yoon and Pilsung Kang and Junseong Bang and Misuk Kim},
keywords = {Task-oriented dialogue, Dialogue State Tracking, ERNIE, Adaptive pre-training, Knowledge-augmented method},
abstract = {Dialogue State Tracking (DST) is critical for comprehensively interpreting user and system utterances, thereby forming the cornerstone of efficient dialogue systems. Despite past research efforts focused on enhancing DST performance through alterations to the model structure or integrating additional features like graph relations, they often require additional pre-training with external dialogue corpora. In this study, we propose DSTEA, improving Dialogue State Tracking via Entity Adaptive pre-training, which can enhance the encoder through by intensively training key entities in dialogue utterances. DSTEA identifies these pivotal entities from input dialogues utilizing four different methods: ontology information, named-entity recognition, the spaCy toolkit, and the flair library. Subsequently, it employs selective knowledge masking to train the model effectively. Remarkably, DSTEA only requires pre-training without the direct infusion of extra knowledge into the DST model. This approach results in substantial performance improvements of four robust DST models on MultiWOZ 2.0, 2.1, and 2.2, with joint goal accuracy witnessing an increase of up to 2.69% (from 52.41% to 55.10%). Comparative experiments considering various entity types and different entity adaptive pre-training configurations, such as masking strategy and masking rate, further validated the efficacy of DSTEA.}
}
@article{LATSOU2024102567,
title = {A unified framework for digital twin development in manufacturing},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102567},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102567},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002155},
author = {Christina Latsou and Dedy Ariansyah and Louis Salome and John {Ahmet Erkoyuncu} and Jim Sibson and John Dunville},
keywords = {Digital twins, Digital Twin development framework, Ontology, Manufacturing},
abstract = {The concept of digital twin (DT) is undergoing rapid transformation and attracting increased attention across industries. It is recognised as an innovative technology offering real-time monitoring, simulation, optimisation, accurate forecasting and bi-directional feedback between physical and digital objects. Despite extensive academic and industrial research, DT has not yet been properly understood and implemented by many industries, due to challenges identified during its development. Existing literature shows that there is a lack of a unified framework to build DT, a lack of standardisation in the development, and challenges related to coherent goals of DT in a multi-disciplinary team engaged in the design, development and implementation of DT to a larger scale system. To address these challenges, this study introduces a unified framework for DT development, emphasising reusability and scalability. The framework harmonises existing DT frameworks by unifying concepts and process development. It facilitates the integration of heterogeneous data types and ensures a continuous flow of information among data sources, simulation models and visualisation platforms. Scalability is achieved through ontology implementation, while employing an agent-based approach, it monitors physical asset performance, automatically detects faults, checks repair status and offers operators feedback on asset demand, availability and health conditions. The effectiveness of the proposed DT framework is validated through its application to a real-world case study involving five interconnected air compressors located at the Connected Facility at Devonport Royal Dockyard, UK. The DT automatically and remotely monitors the performance and health status of compressors, providing guidance to humans on fault repair. This guidance dynamically adapts based on feedback from the DT. Analyses of the results demonstrate that the proposed DT increases the facility’s operation availability and enhances decision-making by promptly and accurately detecting faults.}
}
@article{TRASMUNDI2024101629,
title = {Introduction to the Festschrift for Per Linell: Dialogism as a General Epistemology for the Language Sciences},
journal = {Language Sciences},
volume = {103},
pages = {101629},
year = {2024},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101629},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000184},
author = {Sarah Bro Trasmundi}
}
@article{CAPUANO2019459,
title = {Experimentation of a smart learning system for law based on knowledge discovery and cognitive computing},
journal = {Computers in Human Behavior},
volume = {92},
pages = {459-467},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.03.034},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218301390},
author = {Nicola Capuano and Daniele Toti},
keywords = {Adaptive learning, Knowledge discovery, Cognitive computing, Natural language processing, Ontology integration, Online dispute resolution},
abstract = {This work presents a Smart Learning system based on Knowledge Discovery and Cognitive Computing techniques aimed at citizens, legal students and experts alike, providing them with the possibility of submitting legal cases expressed in natural language and obtaining legal insight and advice in return. Advanced features implemented within the system include the automatic conceptualization and classification of textual legal cases via natural language processing, the generation of learning paths by relying upon legal ontologies, and additional features for managing legal knowledge bases, including editing, versioning, integration and enrichment. The system has been experimented on a diversified user-base and succeeded in obtaining a positive evaluation with respect to the aspects that were subject of the investigation, including effectiveness, efficiency and usability, thus paving the way to make the system a successful cognitive learning platform for future law professionals and knowledgeable citizens.}
}
@article{ZHANG2022101557,
title = {Natural language generation and deep learning for intelligent building codes},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101557},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101557},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622000313},
author = {Ruichuan Zhang and Nora El-Gohary},
keywords = {Intelligent building code, Natural language generation, Deep learning, Automated compliance checking, Requirement representation},
abstract = {Many existing automated compliance checking (ACC) systems require the processes of extracting regulatory information from natural-language building-code requirements and transforming the extracted information into computer-processable semantic representations. These processes could, however, be jeopardized by the ambiguous nature of the natural language and the hierarchically complex structures of building-code requirements. To address this problem, this paper proposes the concept of intelligent building code for bypassing the error-prone information extraction and transformation processes. In the proposed intelligent code, the natural-language requirements in the code are connected with highly structured computer-understandable semantic information, which is represented in the form of semantic requirement hierarchies and can be readily used by computers for ACC. The paper also proposes a deep learning-based method to automatically generate such intelligent code. The method leverages the requirement hierarchy representation, a proposed deep learning unit-to-text model for generating requirement sentence segments, and a proposed semantic correspondence score for configuring the segments into requirement sentences. The method was implemented and tested on a dataset from multiple regulatory documents. The generated intelligent requirements were evaluated in terms of both natural-language requirement comprehensibility and correspondence between the natural language and the semantic representation, with the results indicating high performance for the proposed representation and method. The proposed intelligent code will help reduce ACC errors, improve requirement comprehensibility, and facilitate intelligent code analytics.}
}
@incollection{WICKMAN2024,
title = {Language in Corporate Discourse},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00242-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041002428},
author = {Chad Wickman},
keywords = {Language in corporate discourse, Corporate communication, Corporate reporting, Corporate social responsibility, Business ethics, Discursive legitimation, Critical linguistics, Critical discourse analysis, Environmental sustainability},
abstract = {This article reviews key concepts and approaches to the study of language in corporate discourse. It begins by conceptualizing the subject based on scholarship in language and discourse studies; it follows with a selective review of research that explores linguistic and rhetorical dimensions of corporate accountability and legitimacy; and it concludes by suggesting directions for research and application, specifically highlighting the potential for language study to bring corporate activity into line with ideals of ethics and environmental sustainability.}
}
@article{DOMBROWSKI20219,
title = {Neural Machine Translation for Semantic-Driven Q&A Systems in the Factory Planning},
journal = {Procedia CIRP},
volume = {96},
pages = {9-14},
year = {2021},
note = {8th CIRP Global Web Conference – Flexible Mass Customisation (CIRPe 2020)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.01.044},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121000676},
author = {Uwe Dombrowski and Alexander Reiswich and Raphael Lamprecht},
keywords = {factory planning, semantic web stack, knowledge graph, artificial neural networks, natural language processing, question, answering models},
abstract = {Shorter lifecycles, increasing product variance and the integration of new products and technologies into existing factories lead to a high complexity in today’s factory planning. In order to master this complexity, many companies attempt to improve their processes by using digitalization tools. This generates enormous amounts of data, which are currently only partially managed centrally in the company. In order to simplify the associated difficulties regarding the access to information, semantic technologies for the generation and application of knowledge graphs (KG) are currently being investigated intensively by research and industry. To retrieve information - stored in the KG - query languages such as SPARQL are used which require knowledge regarding the structure of the KG as well as a profound knowledge of query languages. To also enable non-expert users to retrieve the stored information in the KG, an intuitive user interface for question answering (Q&A) is needed. In this context, we propose a translation model using artificial neural networks (ANN), which translates questions in german into the query language SPARQL. Due to a lack of suitable datasets, we first developed a method to automatically generate synthetic data sets for training the translation model based on existing ontologies. Based on similar approaches from research, we develop an ANN architecture for the translation model. The method for data generation and the ANN were tested and validated at a german car manufacturer using a knowledge graph. It was shown that the developed architecture is particularly suitable for our field of application.}
}
@article{FERRETTI202482,
title = {The Mediterranean metaphor and Léon Metchnikoff's Great Historical Rivers: anarchist geographies of water-land hybridity},
journal = {Journal of Historical Geography},
volume = {86},
pages = {82-92},
year = {2024},
issn = {0305-7488},
doi = {https://doi.org/10.1016/j.jhg.2024.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0305748824000604},
author = {Federico Ferretti},
keywords = {Mediterranean metaphor, Historical Rivers, Coastal indentation, Hybrid geographies, Anarchism},
abstract = {This paper discusses ideas of anarchist (historical) geographies of rivers and seas. It does so by addressing works of early anarchist geographer Lev Ilich Mechnikov (mentioned here with the more known French spelling Léon Metchnikoff) (1838–1888), which lie at the origin of broader ‘Mediterranean metaphors’ comparing the globalising role of oceanic navigation to early Mediterranean connectedness, mainly discussed by Metchnikoff in his key book La civilisation et les grands fleuves historiques [Civilisation and Great Historical Rivers]. A close collaborator of Elisée Reclus and Peter Kropotkin and a multifarious scholarly talent, Metchnikoff provided contributions that still need to be fully rediscovered. Based on a systematic reading of Metchnikoff's archives and works, I argue that, starting from historical rivers and the early Mediterranean, his ideas on the historical roles that can be possibly (and relationally) played by water-land assemblages can nourish current notions of more-than-wet ontologies and critical geopolitics. Eventually, these ideas provide models for understanding spatialities that are alternative to those of state borders, bounded land and terracentric territorialities, contributing to shape the open and boundless world that is currently conceived by scholarship informed to pluriversal notions of critical Mediterraneanism.}
}
@article{NGO2022102540,
title = {A transformer-Based neural language model that synthesizes brain activation maps from free-form text queries},
journal = {Medical Image Analysis},
volume = {81},
pages = {102540},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102540},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522001876},
author = {Gia H. Ngo and Minh Nguyen and Nancy F. Chen and Mert R. Sabuncu},
keywords = {Coordinate-based meta-analysis, Transformers, Information retrieval, Image generation},
abstract = {Neuroimaging studies are often limited by the number of subjects and cognitive processes that can be feasibly interrogated. However, a rapidly growing number of neuroscientific studies have collectively accumulated an extensive wealth of results. Digesting this growing literature and obtaining novel insights remains to be a major challenge, since existing meta-analytic tools are constrained to keyword queries. In this paper, we present Text2Brain, an easy to use tool for synthesizing brain activation maps from open-ended text queries. Text2Brain was built on a transformer-based neural network language model and a coordinate-based meta-analysis of neuroimaging studies. Text2Brain combines a transformer-based text encoder and a 3D image generator, and was trained on variable-length text snippets and their corresponding activation maps sampled from 13,000 published studies. In our experiments, we demonstrate that Text2Brain can synthesize meaningful neural activation patterns from various free-form textual descriptions. Text2Brain is available at https://braininterpreter.com as a web-based tool for efficiently searching through the vast neuroimaging literature and generating new hypotheses.}
}
@article{PRADEEP202133,
title = {Leveraging context-awareness for Internet of Things ecosystem: Representation, organization, and management of context},
journal = {Computer Communications},
volume = {177},
pages = {33-50},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421002280},
author = {Preeja Pradeep and Shivsubramani Krishnamoorthy and Rahul Krishnan Pathinarupothi and Athanasios V. Vasilakos},
keywords = {Context-aware computing, Context model, Context ontology, Internet of Things, Situational context},
abstract = {Present-day devices are becoming increasingly smarter than their predecessors. From a simple passive light switch to an intelligent wristwatch, great strides have been made in networking smart devices, creating an autonomous ecosystem, the so-called Internet of Things. In an increasingly information-driven world, context-awareness supports the intended applications as well as their constituent devices, making them conscious of and adaptive to the specific scenario in real-time. Moreover, heterogeneous devices in the Internet of Things ecosystem peruse disparate data formats and semantics, giving rise to interoperability and information sharing challenges. Context modeling is a core feature that facilitates interoperability and information sharing between applications. Although generic context models exist, they do not consider pertinent dimensions of context to provide a generic vocabulary, and therefore, they cannot be extended to generalize situations commonly encountered in the Internet of Things environment. An extensible, generic modeling and representation of context is required to manage pertinent context dimensions in various ecosystems by being dynamically aware of the situation. This paper presents Context Model for Internet of Things, an extensible and generic ontology-based context modeling approach that provides relevant information at the right time. This work encompasses Context Ontology for Internet of Things, an ontology-based context organization approach, which provides an abstract and overarching vocabulary that fosters knowledge reusability and sharing. The proposed model has been implemented and evaluated with a use case to validate its adaptability, effectiveness, and viability. Our evaluation based on generality, effectiveness, and consistency shows that the proposed model can effectively represent, organize, and manage the context in different Internet of Things ecosystems.}
}
@article{NAHAR2022102038,
title = {Integrated identity and access management metamodel and pattern system for secure enterprise architecture},
journal = {Data & Knowledge Engineering},
volume = {140},
pages = {102038},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102038},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X22000428},
author = {Kamrun Nahar and Asif Qumer Gill},
keywords = {Identity management, Access control management, Metamodel, Ontology, Enterprise architecture, Design science research},
abstract = {Identity and access management (IAM) is one of the key components of the secure enterprise architecture for protecting the digital assets of the information systems. The challenge is: How to model an integrated IAM for a secure enterprise architecture to protect digital assets? This research aims to address this question and develops an ontology based integrated IAM metamodel for the secure digital enterprise architecture (EA). Business domain and technology agnostic characteristics of the developed IAM metamodel will allow it to develop IAM models for different types of information systems. Well-known design science research (DSR) methodology was adopted to conduct this research. The developed IAM metamodel is evaluated by using the demonstration method. Furthermore, as a part of the evaluation, a pattern system has been developed, consisting of eight IAM patterns. Each pattern offers a solution to a specific IAM related problem. The outcome of this research indicates that enterprise, IAM and information systems architects and academic researchers can use the proposed IAM metamodel and the pattern system to design and implement situation-specific IAM models within the overall context of a secure EA for information systems.}
}
@article{JABLA20221871,
title = {A knowledge-driven activity recognition framework for learning unknown activities},
journal = {Procedia Computer Science},
volume = {207},
pages = {1871-1880},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.245},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922011310},
author = {Roua Jabla and Maha Khemaja and Félix Buendia and Sami Faiz},
keywords = {Activity recognition, Learning, Knowledge-driven, Ontology reasoning, Rule, Smartphone},
abstract = {Human activity recognition has increasingly received attention in recent years to track regular activities of people. Existing activity recognition approaches considerably contributed to the analysis of human behavior. However, they still confront numerous issues related to the variability of activities performed by people within dynamic environments. Generally, this variability renders the used training or ontology models with predefined activities unsuitable. Therefore, creating an activity recognition approach that is able to leverage dynamically new and unknown activities at runtime becomes important. In this paper, we propose a novel knowledge-driven activity recognition framework using smartphone. This framework envisions taking a knowledge-driven approach to reinforce the recognition accuracy and people's quality of life in the context of dynamic environments at runtime. More specifically, we propose an ontology-based context evolution along with a dynamic decision-making, so that new and unknown performed activities can be accurately recognized. Furthermore, we use a public activity recognition dataset to demonstrate the effectiveness of the proposed framework and show its advantage over a data-driven baselines in terms of accuracy. Experimental results reveal that our framework not only reinforces the accuracy, but also enables an effective activity learning when facing unknown activities at runtime.}
}
@article{SIM2023102701,
title = {Natural language processing with machine learning methods to analyze unstructured patient-reported outcomes derived from electronic health records: A systematic review},
journal = {Artificial Intelligence in Medicine},
volume = {146},
pages = {102701},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102701},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723002154},
author = {Jin-ah Sim and Xiaolei Huang and Madeline R. Horan and Christopher M. Stewart and Leslie L. Robison and Melissa M. Hudson and Justin N. Baker and I-Chan Huang},
keywords = {Natural language processing, Machine learning, Patient-reported outcomes, Electronic health records, Unstructured clinical narrative},
abstract = {Objective
Natural language processing (NLP) combined with machine learning (ML) techniques are increasingly used to process unstructured/free-text patient-reported outcome (PRO) data available in electronic health records (EHRs). This systematic review summarizes the literature reporting NLP/ML systems/toolkits for analyzing PROs in clinical narratives of EHRs and discusses the future directions for the application of this modality in clinical care.
Methods
We searched PubMed, Scopus, and Web of Science for studies written in English between 1/1/2000 and 12/31/2020. Seventy-nine studies meeting the eligibility criteria were included. We abstracted and summarized information related to the study purpose, patient population, type/source/amount of unstructured PRO data, linguistic features, and NLP systems/toolkits for processing unstructured PROs in EHRs.
Results
Most of the studies used NLP/ML techniques to extract PROs from clinical narratives (n = 74) and mapped the extracted PROs into specific PRO domains for phenotyping or clustering purposes (n = 26). Some studies used NLP/ML to process PROs for predicting disease progression or onset of adverse events (n = 22) or developing/validating NLP/ML pipelines for analyzing unstructured PROs (n = 19). Studies used different linguistic features, including lexical, syntactic, semantic, and contextual features, to process unstructured PROs. Among the 25 NLP systems/toolkits we identified, 15 used rule-based NLP, 6 used hybrid NLP, and 4 used non-neural ML algorithms embedded in NLP.
Conclusions
This study supports the potential utility of different NLP/ML techniques in processing unstructured PROs available in EHRs for clinical care. Though using annotation rules for NLP/ML to analyze unstructured PROs is dominant, deploying novel neural ML-based methods is warranted.}
}
@article{HIDRI2019177,
title = {A Meta-model for context-aware adaptive Business Process as a Service in collaborative cloud environment},
journal = {Procedia Computer Science},
volume = {164},
pages = {177-186},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.170},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919322094},
author = {Wafa Hidri and Riadh Hadj M’tir and Narjès Bellamine {Ben Saoud} and Chirine Ghedira-Guegan},
keywords = {SaaS, Business Process, service composition, adaptation, meta-model, ontology},
abstract = {With the emergence of cloud computing, building and developing a new Software by SaaS composition becomes a major concern in today’s cloud business. First, the business analyst establishes the Business Process (BP) of the new SaaS. Then, the SaaS composer, based on functional and non-functional requirements, discovers and selects the potential SaaS components that can fill into each of the defined tasks in the BP. Currently, enterprises need more and more flexible SaaS business process as an adaptive composite SaaS from process vendors, which interest on context changes and needs evolutions. A meta-model is proposed in this paper dealing with different concepts involved in business cloud layer. This meta-model, in which both business process and service composition are considered, is accompanied by a case study. Then, an ontology has been developed and an extract of instances has been presented to demonstrate the adaptation process.}
}
@article{XU2018441,
title = {Knowledge-driven intelligent quality problem-solving system in the automotive industry},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {441-457},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618301861},
author = {Zhaoguang Xu and Yanzhong Dang and Peter Munro},
keywords = {Quality management, Intelligent quality problem-solving, Knowledge management, Automotive industry, Digital fishbone diagram, Ontology},
abstract = {In the current automotive industry, quality management, especially quality problem-solving (QPS), plays an important role in fulfilling the expectations of demanding customers who seek high-quality products at low-cost. During the problem-solving process, various real-time and historical quality data are often not fully used, yet these data are of high value. This paper provides a comprehensive quality data mining process and method, as well as an intelligent quality problem-solving system (IQPSS). First, based on original quality problem data, an ontology library is constructed using the ontology generating module (OGM). Second, based on the generated ontology and the textual data of the original quality problem, this study builds a quality problem-solving knowledge base (QPSKB) by employing relevant algorithms in the knowledge transformation module (KTM). The component and fault relational matrix mining (CFRMM) algorithm is designed to extract the relationship matrix between the components and faults. The semi-supervised classification algorithm based on the K-nearest neighbor algorithm (KNN) is used to classify the immediate measures, causes and long-term measures into the corresponding ontology and express the ontology as their knowledge. Furthermore, the binary tree-based support vector machine (SVM) approach is applied to classify the cause texts into the factors of Man, Machine, Material, Method, and Environment (4M1E), which are the five factors in a fishbone diagram. In particular, the digital fishbone diagram is a brand-new type of fishbone diagram that subverts the traditional method of fishbone diagram analysis through brainstorming. A pilot run of the IQPSS has been undertaken in an automotive manufacturing company to demonstrate how quality management employees obtain this knowledge by searching in the IQPSS. The results show that the IQPSS contributes appreciably to the quality problem-solving in the manufacturing industry.}
}
@article{PUTRAMA2024110853,
title = {Heterogeneous data integration: Challenges and opportunities},
journal = {Data in Brief},
volume = {56},
pages = {110853},
year = {2024},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2024.110853},
url = {https://www.sciencedirect.com/science/article/pii/S2352340924008175},
author = {I Made Putrama and Péter Martinek},
keywords = {Big data, Integration, Ontology, Heterogeneous, Data sources, Review},
abstract = {Integrating multiple data source technologies is essential for organizations to respond to highly dynamic market needs. Although physical data integration systems have been considered to have better query processing systems, they pose higher implementation and maintenance costs. Meanwhile, virtual data integration has become an alternative topic that is increasingly attracting the attention of researchers in the current era of big data. Various data integration methodologies have been developed and used in various domains, processing heterogeneous data using various approaches. This review article aims to provide an overview of heterogeneous data integration research focusing on methodology and approaches. It surveys existing publications, highlighting key trends, challenges, and open research topics. The main findings are: (i) Research has been conducted in various domains. However, most focus on big data rather than specific study domains; (ii) researchers primarily focus on semantics challenges, and (iii) gaps still need to be addressed and related to integration issues involving semantics and unstructured data formats that must be thoroughly addressed. Furthermore, considering elements of cutting-edge technology, such as machine learning and data integration, about privacy concerns provides a chance for additional investigation. Finally, we provide insight into the potential for a broader review of integration challenges based on case studies.}
}
@article{WAJZER2023100874,
title = {The reductionism of genopolitics in the context of the relationships between biology and political science},
journal = {Endeavour},
volume = {47},
number = {3},
pages = {100874},
year = {2023},
issn = {0160-9327},
doi = {https://doi.org/10.1016/j.endeavour.2023.100874},
url = {https://www.sciencedirect.com/science/article/pii/S0160932723000315},
author = {Mateusz Wajzer},
keywords = {Reductionism, Genopolitics, Biology, Political science, Political attitudes and behaviours},
abstract = {The past two decades have seen an increase in the use of theories, data, assumptions and methods of the biological sciences in studying political phenomena. One of the approaches that combine biology with political science is genopolitics. The goal of the study was to analyse the basic ontological, methodological and epistemological assumptions for the reductionism of genopolitics. The results show that genopolitics assumes methodological reductionism but rejects ontological and epistemological reductionism. The key consequences of the findings are the irreducibility of political science to biology and the complementarity of genopolitical explanations and political science explanations based on culturalism. If my findings prove to be correct, they give rise to the formation of a hypothesis regarding the anti-reductionist orientation of the contemporary links between political science and biology. An important step towards confirming or falsifying such a hypothesis will be exploring the reductionism of contemporary biopolitical approaches such as neuropolitics or evolutionary political psychology.}
}
@article{OLANREWAJU2022103865,
title = {Building information modelling and green building certification systems: A systematic literature review and gap spotting},
journal = {Sustainable Cities and Society},
volume = {81},
pages = {103865},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103865},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722001925},
author = {Oludolapo Ibrahim Olanrewaju and Wallace Imoudu Enegbuma and Michael Donn and Nicholas Chileshe},
keywords = {Building information modelling, Green building certification system, Systematic literature review, Sustainability},
abstract = {There has been an increasing focus on using Building Information Modelling (BIM) for sustainability assessment. Especially in the aspect of applying BIM-based tools to provide documentation for Green Building Certification Systems (GBCS) credits in sustainability assessment. Despite the huge potential of BIM for GBCS, there is limited literature that has established this link. Hence, the purpose of this paper is to explore the synergies between BIM and GBCS to achieve a deeper understanding of the GBCS currently BIM-enabled, level of BIM implementation in GBCS sustainability areas, challenges and gaps in integrating BIM and GBCS, BIM and GB tools currently in use and areas for further research in the BIM-GBCS domain. The research methodology includes a combination of systematic literature review (SLR) and gap spotting. The SLR includes a total of 84 papers from highly ranked journals between 2009 and 2020. In terms of sustainability areas, energy (71%) has the highest amount of literature in the environmental sustainability dimension while social and economic dimensions had a representation of 15% and 11%, respectively. The knowledge gaps and areas for proposed future research directions are critical to developing work in the BIM-GBCS domain.}
}
@article{YADAV2024165,
title = {TCR-ESM: Employing protein language embeddings to predict TCR-peptide-MHC binding},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {165-173},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S200103702300452X},
author = {Shashank Yadav and Dhvani Sandip Vora and Durai Sundar and Jaspreet Kaur Dhanjal},
keywords = {T-cell therapy, TCR-pMHC interactions, Protein language models, TCR specificity, Peptide embeddings},
abstract = {Cognate target identification for T-cell receptors (TCRs) is a significant barrier in T-cell therapy development, which may be overcome by accurately predicting TCR interaction with peptide-bound major histocompatibility complex (pMHC). In this study, we have employed peptide embeddings learned from a large protein language model- Evolutionary Scale Modeling (ESM), to predict TCR-pMHC binding. The TCR-ESM model presented outperforms existing predictors. The complementarity-determining region 3 (CDR3) of the hypervariable TCR is located at the center of the paratope and plays a crucial role in peptide recognition. TCR-ESM trained on paired TCR data with both CDR3α and CDR3β chain information performs significantly better than those trained on data with only CDR3β, suggesting that both TCR chains contribute to specificity, the relative importance however depends on the specific peptide-MHC targeted. The study illuminates the importance of MHC information in TCR-peptide binding which remained inconclusive so far and was thought dependent on the dataset characteristics. TCR-ESM outperforms existing approaches on external datasets, suggesting generalizability. Overall, the potential of deep learning for predicting TCR-pMHC interactions and improving the understanding of factors driving TCR specificity are highlighted. The prediction model is available at http://tcresm.dhanjal-lab.iiitd.edu.in/ as an online tool.}
}
@article{CERAMI20221,
title = {On decidability of concept satisfiability in Description Logic with product semantics},
journal = {Fuzzy Sets and Systems},
volume = {445},
pages = {1-21},
year = {2022},
note = {Logic and Databases},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2021.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0165011421004395},
author = {Marco Cerami and Francesc Esteva},
abstract = {The aim of the present paper is to prove that concept validity and positive satisfiability with an empty ontology in the Fuzzy Description Logic IALE, under standard product semantics and with respect to quasi-witnessed models, are decidable. In our framework we are not considering reasoning tasks over ontologies. The proof of our result consists in reducing the problem to a finitary consequence problem in propositional product logic with Monteiro-Baaz delta operator, which is known to be decidable. Product FDL and first order logic are known not to enjoy the finite model property, so we cannot restrict to finite interpretations. Thus, in order to obtain our result, we need to codify infinite interpretations using a finite number of propositional formulas. Such result was conjectured in [10], but the proof given was subsequently found incorrect. In the present work an improved reduction algorithm is proposed and a proof of the same result is provided.}
}
@article{GIEREND2024,
title = {Provenance Information for Biomedical Data and Workflows: Scoping Review},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/51297},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124005168},
author = {Kerstin Gierend and Frank Krüger and Sascha Genehr and Francisca Hartmann and Fabian Siegel and Dagmar Waltemath and Thomas Ganslandt and Atinkut Alamirrew Zeleke},
keywords = {provenance, biomedical research, data management, scoping review, health care data, software life cycle},
abstract = {Background
The record of the origin and the history of data, known as provenance, holds importance. Provenance information leads to higher interpretability of scientific results and enables reliable collaboration and data sharing. However, the lack of comprehensive evidence on provenance approaches hinders the uptake of good scientific practice in clinical research.
Objective
This scoping review aims to identify approaches and criteria for provenance tracking in the biomedical domain. We reviewed the state-of-the-art frameworks, associated artifacts, and methodologies for provenance tracking.
Methods
This scoping review followed the methodological framework developed by Arksey and O’Malley. We searched the PubMed and Web of Science databases for English-language articles published from 2006 to 2022. Title and abstract screening were carried out by 4 independent reviewers using the Rayyan screening tool. A majority vote was required for consent on the eligibility of papers based on the defined inclusion and exclusion criteria. Full-text reading and screening were performed independently by 2 reviewers, and information was extracted into a pretested template for the 5 research questions. Disagreements were resolved by a domain expert. The study protocol has previously been published.
Results
The search resulted in a total of 764 papers. Of 624 identified, deduplicated papers, 66 (10.6%) studies fulfilled the inclusion criteria. We identified diverse provenance-tracking approaches ranging from practical provenance processing and managing to theoretical frameworks distinguishing diverse concepts and details of data and metadata models, provenance components, and notations. A substantial majority investigated underlying requirements to varying extents and validation intensities but lacked completeness in provenance coverage. Mostly, cited requirements concerned the knowledge about data integrity and reproducibility. Moreover, these revolved around robust data quality assessments, consistent policies for sensitive data protection, improved user interfaces, and automated ontology development. We found that different stakeholder groups benefit from the availability of provenance information. Thereby, we recognized that the term provenance is subjected to an evolutionary and technical process with multifaceted meanings and roles. Challenges included organizational and technical issues linked to data annotation, provenance modeling, and performance, amplified by subsequent matters such as enhanced provenance information and quality principles.
Conclusions
As data volumes grow and computing power increases, the challenge of scaling provenance systems to handle data efficiently and assist complex queries intensifies, necessitating automated and scalable solutions. With rising legal and scientific demands, there is an urgent need for greater transparency in implementing provenance systems in research projects, despite the challenges of unresolved granularity and knowledge bottlenecks. We believe that our recommendations enable quality and guide the implementation of auditable and measurable provenance approaches as well as solutions in the daily tasks of biomedical scientists.
International Registered Report Identifier (IRRID)
RR2-10.2196/31750}
}
@article{SONG2025109572,
title = {Korean football in-game conversation state tracking dataset for dialogue and turn level evaluation},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109572},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109572},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624017305},
author = {Sangmin Song and Juhyoung Park and Juhwan Choi and Junho Lee and Kyohoon Jin and YoungBin Kim},
keywords = {Dialogue state tracking, Data annotation, Large language model},
abstract = {Recent research in dialogue state tracking has made significant progress in tracking user goals through dialogue-level and turn-level approaches, but existing research primarily focused on predicting dialogue-level belief states. In this study, we present the KICK: Korean football In-game Conversation state tracKing dataset, which introduces a conversation-based approach. This approach leverages the roles of casters and commentators within the self-contained context of sports broadcasting to examine how utterances impact the belief state at both the dialogue-level and turn-level. Towards this end, we propose a task that aims to track the states of a specific time turn and understand conversations during the entire game. The proposed dataset comprises 228 games and 2463 events over one season, with a larger number of tokens per dialogue and turn, making it more challenging than existing datasets. Experiments revealed that the roles and interactions of casters and commentators are important for improving the zero-shot state tracking performance. By better understanding role-based utterances, we identify distinct approaches to the overall game process and events at specific turns.}
}
@article{SHEN20181,
title = {CBN: Constructing a clinical Bayesian network based on data from the electronic medical record},
journal = {Journal of Biomedical Informatics},
volume = {88},
pages = {1-10},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418302041},
author = {Ying Shen and Lizhu Zhang and Jin Zhang and Min Yang and Buzhou Tang and Yaliang Li and Kai Lei},
keywords = {Bayesian network, Ontology, Probabilistic inference, Disease diagnosis},
abstract = {The process of learning candidate causal relationships involving diseases and symptoms from electronic medical records (EMRs) is the first step towards learning models that perform diagnostic inference directly from real healthcare data. However, the existing diagnostic inference systems rely on knowledge bases such as ontology that are manually compiled through a labour-intensive process or automatically derived using simple pairwise statistics. We explore CBN, a Clinical Bayesian Network construction for medical ontology probabilistic inference, to learn high-quality Bayesian topology and complete ontology directly from EMRs. Specifically, we first extract medical entity relationships from over 10,000 deidentified patient records and adopt the odds ratio (OR value) calculation and the K2 greedy algorithm to automatically construct a Bayesian topology. Then, Bayesian estimation is used for the probability distribution. Finally, we employ a Bayesian network to complete the causal relationship and probability distribution of ontology to enhance the ontology inference capability. By evaluating the learned topology versus the expert opinions of physicians and entropy calculations and by calculating the ontology-based diagnosis classification, our study demonstrates that the direct and automated construction of a high-quality health topology and ontology from medical records is feasible. Our results are reproducible, and we will release the source code and CN-Stroke knowledge graph of this work after publication.1Source code: https://github.com/anonymousBioinformatics/MedSim. Ontology: https://github.com/anonymousAuthor111/CN-Stroke-Knowledge-Graph.git.1}
}
@article{KAUR2023301,
title = {Technology-Assisted Language Learning Adaptive Systems: A Comprehensive Review},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {4},
pages = {301-313},
year = {2023},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2023.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S266630742300030X},
author = {Parneet Kaur and Harish Kumar and Sakshi Kaushal},
keywords = {Artificial Intelligence, Online learning, Technology-assisted language learning (TALL), Intelligent Tutoring Systems (ITS), Adaptive systems, Learning styles},
abstract = {Technology-aided learning is one of the important aspects of cognitive computing where education is provided through the means of digital media. This paper presents a comprehensive review of trends and the development of technology-based adaptive language learning systems. It also strives to highlight challenges and opportunities in the field of Technology-Assisted Language Learning (TALL). Articles from various electronic databases and reputed journals in the related field have been reviewed from 2011to 2021 to get an insight into state-of-the-art adaptive systems for TALL. To analyze results, the authors have proposed three dimensions viz. spatial and temporal aspects, system and targeted learners’ characteristics, and adaptation provided. The findings of the review indicate that research in this field has been gaining popularity since 2015, especially in the Asian region. English is the most frequently investigated language in TALL systems which have been employed majorly for university students. The study also analyzes various methods and sources of adaptation provided in such systems. Challenges, opportunities, limitations, and implications of research in this direction have also been discussed. The study also proposes new ways and ideas that can be taken up and implemented by language educators and local governments to make the research in the field of TALL commercially feasible and penetrable to various levels of society.}
}
@article{MILANESE2024108800,
title = {Fuzzy order-sorted feature logic},
journal = {Fuzzy Sets and Systems},
volume = {477},
pages = {108800},
year = {2024},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2023.108800},
url = {https://www.sciencedirect.com/science/article/pii/S0165011423004451},
author = {Gian Carlo Milanese and Gabriella Pasi},
keywords = {Approximate reasoning, Fuzzy unification, Fuzzy subsumption, Order-sorted feature logic, Knowledge representation, Fuzzy ontologies},
abstract = {Order-Sorted Feature (OSF) logic is a knowledge representation and reasoning language based on function-denoting feature symbols and set-denoting sort symbols ordered in a subsumption lattice. OSF logic allows the construction of record-like terms that represent classes of entities and that are themselves ordered in a subsumption relation. The unification algorithm for such structures provides an efficient calculus of type subsumption, which has been applied in computational linguistics and implemented in constraint logic programming languages such as LOGIN and LIFE and automated reasoners such as CEDAR. This work generalizes OSF logic to a fuzzy setting. We give a flexible definition of a fuzzy subsumption relation which generalizes Zadeh's inclusion between fuzzy sets. Based on this definition we define a fuzzy semantics of OSF logic where sort symbols and OSF terms denote fuzzy sets. We extend the subsumption relation to OSF terms and prove that it constitutes a fuzzy partial order with the property that two OSF terms are subsumed by one another in the crisp sense if and only if their subsumption degree is greater than 0. We show how to find the greatest lower bound of two OSF terms by unifying them and how to compute the subsumption degree between two OSF terms, and we provide the complexity of these operations.}
}
@article{KAUR2022102620,
title = {Performed worlding: Political praxis of Sikh nagar kirtan assemblies in Canada},
journal = {Political Geography},
volume = {96},
pages = {102620},
year = {2022},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2022.102620},
url = {https://www.sciencedirect.com/science/article/pii/S0962629822000348},
author = {Amardeep Kaur},
keywords = {Worlding, Anti-colonial movements, Decolonial politics, Race, Religion, Postsecular, Multiculturalism, Cosmopolitanism, Mobilities, Political ontology, Spiritual politics, Nagar kirtan, Vaisakhi, Sikh, Ghadar},
abstract = {Nagar kirtans are street assemblies with Sikh spiritual music, political marches, and free food distribution. They are especially large in April during the Punjabi harvest festival of Vaisakhi but also take place in other months. This paper explores the political praxis of nagar kirtans in Canada to contribute to critiques of secularism and theorize the political ontology in the Sikh context. I conceptualize nagar kirtan as a mobile court of both poetic pilgrimage and politics, which engage an embodied and performative world-making process, termed performed worlding, that is at odds with the modern ontology of the secular state. The empirical study analyzes early 20th century nagar kirtans in Vancouver and contemporary ones in urban Canada in the 2010-decade. Ethnographic fieldwork was carried out between 2017 and 2018 including analysis of interviews and archival and media content. Early nagar kirtans in Vancouver were aligned with the anti-colonial Ghadar Movement to protest the racial state of Canada. In the contemporary context, the study reveals competing ways of worlding the political ontologies of the mobile court. On the one hand, there is an increased commodification and state presence that refashions the practice at larger Vaisakhi nagar kirtan to be marketable to Canadian multiculturalism. On the other hand, the bottom-up assembling of participants align a spiritual politics of Sikh and Punjabi world-making, one that is grounded in practices of horizontal organizing and mutual care and sustain a form of embodied expression against racial Canada.}
}
@article{TIRLET20244232,
title = {Generic and queryable data integration schema for transcriptomics and epigenomics studies},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {4232-4241},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024003933},
author = {Yael Tirlet and Matéo Boudet and Emmanuelle Becker and Fabrice Legeai and Olivier Dameron},
keywords = {Multi-omics analysis, Data integration, Integration schema, Semantic web},
abstract = {The expansion of multi-omics datasets raises significant challenges for data integration and querying. To overcome these challenges, we developed a generic RDF-based integration schema that connects various types of differential -omics data, epigenomics, and regulatory information. This schema employs the FALDO ontology to enable querying based on genomic locations. It is designed to be fully or partially populated, providing both flexibility and extensibility while supporting complex queries. We validated the schema by reproducing two recently published studies, one in biomedicine and the other in environmental science, proving its genericity and its ability to integrate data efficiently. This schema serves as an effective tool for managing and querying a wide range of multi-omics datasets.}
}
@article{KURBATOV2021,
title = {Linguistic Processor Integration for Solving Planimetric Problems},
journal = {International Journal of Cognitive Informatics and Natural Intelligence},
volume = {15},
number = {4},
year = {2021},
issn = {1557-3958},
doi = {https://doi.org/10.4018/IJCINI.20211001.oa37},
url = {https://www.sciencedirect.com/science/article/pii/S1557395821000166},
author = {Sergeyi S. Kurbatov},
keywords = {Applied Ontology, Interactive Visualization, Linguistic Processor, Semantic Representation, Solving Planimetric Problems, Syntactic Structures},
abstract = {ABSTRACT
The research deals with the original algorithms of the linguistic processor integration for solving planimetric problems. The linguistic processor translates the natural language description of the problem into a semantic representation based on the ontology that supports the axiomatics of geometry. The linguistic processor synthesizes natural-language comments to the solution and drawing objects. The method of interactive visualization of the linguistic processor functioning is proposed. The method provides a step-by-step dialog control of syntactic structure construction and its display in semantic representation. During the experiments, several dozens standard syntactic structures correctly displayed in the semantic structures of the subject area were obtained. The direction of further research related to the development of the proposed approach is outlined.}
}
@article{STINTON2023100007,
title = {Clinical free text to HPO codes},
journal = {Rare},
volume = {1},
pages = {100007},
year = {2023},
issn = {2950-0087},
doi = {https://doi.org/10.1016/j.rare.2023.100007},
url = {https://www.sciencedirect.com/science/article/pii/S2950008723000078},
author = {Gabrielle Stinton and Jane A. Lieviant and Sylvia Kam and Jiin Ying Lim and Jasmine Chew-Yin Goh and Weng Khong Lim and Gareth Baynam and Tele Tan and Duc-Son Pham and Saumya Shekhar Jamuar},
keywords = {Rare disease, Human phenotype ontology, Phenotypic concept extraction, Named entity recognition, Human-in-the-loop},
abstract = {Leveraging Artificial Intelligence (AI) within the rare disease diagnostic odyssey can facilitate a decrease in diagnostic times and an increase in diagnostic rates. Among the steps involved in the odyssey, this project focused on utilizing AI to automate the standardized capturing of clinical free text into Human Phenotype Ontology (HPO) codes. This research project was conducted at both the KK Women’s and Children’s Hospital (KKH), Singapore and the Rare Care Centre at Perth Children’s Hospital, Western Australia (WA), via the Curtin New Colombo Plan (NCP) Scholarship. The outcome of the project saw the development of a Streamlit web application that utilized two (2) pre-trained AI models – PhenoTagger and PhenoBERT – with a human-in-the-loop design. A case study conducted with ten (10) de-identified clinical reports demonstrated a reduction in the HPO extraction task time from ten (10) to twenty (20) minutes per report to less than five (5) minutes.}
}
@article{GONZALEZACOSTA2025149564,
title = {Functional location of the language cortical areas in focal refractory epilepsy using the conventional, selective, and supraselective Wada test},
journal = {Brain Research},
volume = {1854},
pages = {149564},
year = {2025},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2025.149564},
url = {https://www.sciencedirect.com/science/article/pii/S0006899325001222},
author = {Carlos A. González-Acosta and Carlos R. Tolosa-Gaviria and Alejandro Herrera-Trujillo and Carlos A. Dorado-Ramírez and William Escobar-Rojas and Christian A. Rojas-Cerón and Lina V. Becerra-Hernández and Efraín Buriticá-Ramírez and Alfredo Pedroza-Campo},
keywords = {Refractory epilepsy, Epileptogenic zone, WADA test, Selective technique, Supraselective technique, Atypical language lateralization},
abstract = {In refractory focal epilepsy, resective surgery offers an alternative for seizure control. However, there is a risk of language deterioration when the epileptogenic zone involves an eloquent cortical region. The Wada test involves the insertion of a catheter through the internal carotid artery and the injection of a short-acting anesthetic, resulting in transient loss of hemisphere function. While its specificity is high, its sensitivity is reduced, despite its limited or absent spatial resolution. Additionally, the generalized action of the anesthetic may lead to misinterpretations due to global cognitive arrest, particularly in patients with baseline deficits. The aim of this report was to prove the refinement of the selective and supraselective protocols, as well as their contribution to overcoming these disadvantages. The procedure began by placing a microcatheter in progressively more distal irrigation sites, according to the required technique, gradually performing angiography with contrast medium. Tissue perfusion allowed the identification of the cerebral parenchyma where the anesthetic would act. After injection, the assessment of neurocognitive changes was conducted. The characterization of language patterns was performed, delineating indispensable eloquent zones and dispensable eloquent zones, irrespective of the patients’ cognitive condition. There was concordance between the findings and post-surgical results. The selective and supraselective Wada test surpasses the disadvantages of the conventional method and proves decisive in surgical planning and decision-making.}
}
@article{ALMUHAIMEED2022116466,
title = {A modern semantic similarity method using multiple resources for enhancing influenza detection},
journal = {Expert Systems with Applications},
volume = {193},
pages = {116466},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116466},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421017462},
author = {Abdullah Almuhaimeed and Mohammed A. Alhomidi and Mohammed N. Alenezi and Emad Alamoud and Saad Alqahtani},
keywords = {Semantic web, Semantic similarity, Ontologies, Infectious diseases},
abstract = {With the widespread of the data resources over the World Wide Web (WWW), there is an overlapping between these resources that contributes to helping researchers in discovering more information and facts. However, extracting information and data and then, calculating the semantic similarity between them is a non-trivial task as such resources have varying ways to describe the information. Thus, such a problem can be overcome by designing a new semantic similarity method which takes into account different factors and not exclusive on the syntactical description of the data. The paper describes a new semantic similarity method which exploits different factors to calculate the semantic similarity between different resources. The factors are the description of the node and the surrounded relations (i.e. ascending and descending) from multiple ontologies. This will contribute to calculating semantic similarity based on many perspectives and will help to strengthen the similarity relations between different resources to discover new semantic similarities between influenza symptoms. This may lead to control the expansion of the influenza outbreak. The method has been applied over a set of influenza ontologies to find new cases of similarity that may not be explicit in the original resources. An off-line evaluation method has been also conducted on a set of ontologies and compared with the baseline and specialist methods where it exceeded both methods and satisfied more accurate similarity scores.}
}
@article{OBRIEN2024249,
title = {Machine learning for hypothesis generation in biology and medicine: exploring the latent space of neuroscience and developmental bioelectricity},
journal = {Digital Discovery},
volume = {3},
number = {2},
pages = {249-263},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00185g},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X2400024X},
author = {Thomas O'Brien and Joel Stremmel and Léo Pio-Lopez and Patrick McMillen and Cody Rasmussen-Ivey and Michael Levin},
abstract = {Artificial intelligence is a powerful tool that could be deployed to accelerate the scientific enterprise. Here we address a major unmet need: use of existing scientific literature to generate novel hypotheses. We use a deep symmetry between the fields of neuroscience and developmental bioelectricity to evaluate a new tool, FieldSHIFT. FieldSHIFT is an in-context learning framework using a large language model to facilitate candidate scientific research from existing published studies, serving as a tool to generate hypotheses at scale. We release a new dataset for translating between the neuroscience and developmental bioelectricity domains and show how FieldSHIFT helps human scientists explore a latent space of papers that could exist, providing a rich field of suggested future research. We demonstrate the performance of FieldSHIFT for hypothesis generation relative to human-generated developmental biology research directions then test a key prediction of this model using bioinformatics, showing a surprising conservation of molecular mechanisms involved in cognitive behavior and developmental morphogenesis. By allowing scientists to rapidly explore symmetries and meta-parameters that exist in a corpus of scientific papers, we show how machine learning can potentiate human creativity and assist with one of the most interesting and crucial aspects of research: identifying insights from data and generating potential candidates for research agendas.}
}
@article{TRAPPEY2023101354,
title = {Patent landscape and key technology interaction roadmap using graph convolutional network – Case of mobile communication technologies beyond 5G},
journal = {Journal of Informetrics},
volume = {17},
number = {1},
pages = {101354},
year = {2023},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2022.101354},
url = {https://www.sciencedirect.com/science/article/pii/S1751157722001079},
author = {Amy J.C. Trappey and Ann Y.E. Wei and Neil K.T. Chen and Kuo-An Li and L.P. Hung and Charles V. Trappey},
keywords = {Tech-mining analysis, Patent analysis, Keyword extraction, Graph convolution network (GCN)},
abstract = {Beyond 5G (B5G) in mobile network technologies is the latest communication technology currently under development. B5G is expected to achieve superior capabilities in ultra-high network transmission speed, low latency, low energy consumption, and high coverage, comparing to current 5G network performance. Although B5G is still in the development and implementation stage, there are many patents and non-patent literature depicting B5G innovative technologies and applications. The landscapes of B5G technologies are great references for governments and industries to understand the advances in mobile communication for R&D strategies. Thus, this research focuses on developing a formal tech-mining workflow integrating semantic-based patent and non-patent literature analysis for ontology building, patent technological topic clustering, and graph convolutional network (GCN) modeling for depicting key technology interactions among clusters of sub-domain topics. This research emphasizes the study of B5G patent landscape and key technology interaction roadmap in comprehensive steps as a valuable reference for B5G mobile network R&D, as well as for conducting tech-mining of other technology domains of interests.}
}
@article{PANKOWSKA201911,
title = {Business Models in CMMN, DMN and ArchiMate language},
journal = {Procedia Computer Science},
volume = {164},
pages = {11-18},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.148},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321878},
author = {Malgorzata Pankowska},
keywords = {business modelling, ArchiMate, CMMN, DMN, BPMN, business model mapping},
abstract = {Business modelling can be considered as a practice for enabling change in enterprise by defining recommended Information Communication Technology (ICT) solutions, which provide value to business stakeholders. Business modelling and business analyses can be connected with different business models, techniques, and software tools. The goal of the paper is to present and discuss which business models are already well known for enterprise architecture (EA) business analysis. The literature review was done for this purpose. Beyond that, the paper aims to present a classification of business models and their mapping in ArchiMate language into different notations and languages diagrams for information architecture modelling. Proposed in this paper the business model mapping was applied in author’s earlier projects as well as it is used in university course teaching on system analysis and modelling.}
}
@article{GUERRA2025111831,
title = {A cases and clusters framework for recording, retrieving, and reusing response plans in structured cybersecurity incident management},
journal = {Engineering Applications of Artificial Intelligence},
volume = {160},
pages = {111831},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111831},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625018330},
author = {Patrick Andrei Caron Guerra and Raul Ceretta Nunes and Luis Alvaro {de Lima Silva}},
keywords = {Cybersecurity incident response, Case-based reasoning, Clustering, Ontology, Explainable artificial intelligence, Decision-support system, Cybersecurity},
abstract = {The dynamic and increasing sophistication of cyberattacks and vulnerability exploitation creates a need for Explainable Artificial Intelligence (XAI) approaches that help maintain cyber resilience in organizations. In structured cybersecurity incident management, effective incident response demands explainable outputs from AI-based decision-support systems. To approach this problem, this work presents a framework for reusing concrete experiences of cybersecurity incident response, capturing problem-solving data and knowledge as cases for integrated Case-Based Reasoning (CBR) and Clustering. The contribution includes cluster-based query answer analysis, where cybersecurity analysts reuse clusters of retrieved incident response cases to build answers to new problems. Clustering helps analysts identify relevant groups from ranked lists of retrieved cases, making the reuse process more structured and understandable, especially when dealing with retrieval results for broad and ambiguous queries. Different clustering methods are applied to organize retrieved incident response cases from a case base, supporting the grouping of similar cases for a more straightforward interpretation. Multiple experiments, including cross-validation and real-world incident response testing, are conducted to demonstrate the effectiveness of the proposed framework in improving the decision-support system’s precision. The results indicate that exploring cases and clusters can enhance the selection of incident response procedures for reuse, mainly when analysts identify the most relevant clusters of retrieved cases for the given problem situations. The proposed framework contributes to the organization and understanding of responses to cybersecurity incidents, besides supporting more informed decision-making, ultimately improving cybersecurity incident management.}
}
@article{SERRANORUIZ2022185,
title = {Development of a multidimensional conceptual model for job shop smart manufacturing scheduling from the Industry 4.0 perspective},
journal = {Journal of Manufacturing Systems},
volume = {63},
pages = {185-202},
year = {2022},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522000462},
author = {Julio C. Serrano-Ruiz and Josefa Mula and Raúl Poler},
keywords = {Industry 4.0, Job shop, Smart manufacturing scheduling, Digital twin, Zero-defect manufacturing},
abstract = {Based on a scientific literature review in the conceptual domain defined by smart manufacturing scheduling (SMS), this article identifies the benefits and limitations of the reviewed contributions, establishes and discusses a set of criteria with which to collect and structure its main synergistic attributes, and devises a conceptual framework that models SMS around three axes: a semantic ontology context, a hierarchical agent structure, and the deep reinforcement learning (DRL) method. The main purpose of such a modelling research is to establish a conceptual and structured relationship framework to improve the efficiency of the job shop scheduling process using the approach defined by SMS. The presented model orients the job shop scheduling process towards greater flexibility, through enhanced rescheduling capability, and towards autonomous operation, mainly supported by the use of machine learning technology. To the best of our knowledge, there are no other similar conceptual models in the literature that synergistically combine the potential of the specific set of Industry 4.0 principles and technologies that model SMS. This research can provide guidance for practitioners and researchers’ efforts to move toward the digital transformation of job shops.}
}
@article{SCUTELNICU2022829,
title = {An Approach For Word Sense Alignment For Romanian Language},
journal = {Procedia Computer Science},
volume = {207},
pages = {829-836},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.138},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010195},
author = {Liviu Andrei Scutelnicu},
keywords = {Lexical resources, electronic dictionary, Romanian WordNet, word sense alignment},
abstract = {The Romanian WordNet (RoWN) database is an impressive collection of Romanian nouns, verbs, adjectives, and adverbs, which can be seen as a network of nodes where, words can be found in a certain context to have the same meaning, and in the mesh of that network are found the other words that in turn can become a network node for another word in a specific context. In this study we propose an approach for the problem of aligning the word senses form RoWordNet with those of the Thesaurus Dictionary of the Romanian Language in electronic format (eDTLR), by exploiting the collections of definitions and examples in the two linguistic thesauri, based on a statistical model, where four heuristics are proposed for solving this issue.}
}
@article{HOSSEINI2021106608,
title = {Analyzing privacy policies through syntax-driven semantic analysis of information types},
journal = {Information and Software Technology},
volume = {138},
pages = {106608},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106608},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000859},
author = {Mitra Bokaei Hosseini and Travis D. Breaux and Rocky Slavin and Jianwei Niu and Xiaoyin Wang},
keywords = {Privacy policy, Ambiguity, Generality, Ontology},
abstract = {Context:
Several government laws and app markets, such as Google Play, require the disclosure of app data practices to users. These data practices constitute critical privacy requirements statements, since they underpin the app’s functionality while describing how various personal information types are collected, used, and with whom they are shared.
Objective:
Abstract and ambiguous terminology in requirements statements concerning information types (e.g., “we collect your device information”), can reduce shared understanding among app developers, policy writers, and users.
Method:
To address this challenge, we propose a syntax-driven method that first parses a given information type phrase (e.g. mobile device identifier) into its constituents using a context-free grammar and second infers semantic relationships between constituents using semantic rules. The inferred semantic relationships between a given phrase and its constituents generate a hierarchy that models the generality and ambiguity of phrases. Through this method, we infer relations from a lexicon consisting of a set of information type phrases to populate a partial ontology. The resulting ontology is a knowledge graph that can be used to guide requirements authors in the selection of the most appropriate information type terms.
Results:
We evaluate the method’s performance using two criteria: (1) expert assessment of relations between information types; and (2) non-expert preferences for relations between information types. The results suggest performance improvement when compared to a previously proposed method. We also evaluate the reliability of the method considering the information types extracted from different data practices (e.g., collection, usage, sharing, etc.) in privacy policies for mobile or web-based apps in various app domains.
Contributions:
The method achieves average of 89% precision and 87% recall considering information types from various app domains and data practices. Due to these results, we conclude that the method can be generalized reliably in inferring relations and reducing the ambiguity and abstraction in privacy policies.}
}
@article{LI2021100247,
title = {An Analytic Graph Data Model and Query Language for Exploring the Evolution of Science},
journal = {Big Data Research},
volume = {26},
pages = {100247},
year = {2021},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2021.100247},
url = {https://www.sciencedirect.com/science/article/pii/S2214579621000642},
author = {Ke Li and Hubert Naacke and Bernd Amann},
keywords = {Topic modeling, Topic evolution networks, LDA, Science evolution, Big data},
abstract = {In this article we propose a data model and query language for the visualisation and exploration of topic evolution networks representing the research progress in scientific document archives. Our model is independent of a particular topic extraction and alignment method and proposes a set of semantic and structural metrics for characterizing and filtering meaningful topic evolution patterns. These metrics are particularly useful for the visualization and the exploration of large topic evolution graphs. We also present a first implementation of our model on top of Apache Spark and experimental results obtained for four real-world document archives.}
}
@article{MUNIYAPPAN2023104528,
title = {EGeRepDR: An enhanced genetic-based representation learning for drug repurposing using multiple biomedical sources},
journal = {Journal of Biomedical Informatics},
volume = {147},
pages = {104528},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104528},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423002496},
author = {Saranya Muniyappan and Arockia Xavier Annie Rayan and Geetha Thekkumpurath Varrieth},
keywords = {Drug repurposing, Genetic factors, Multimodal, Heterogeneous biological network, Pattern-based bootstrapping, Ontology-based representation learning},
abstract = {Motivation
Drug repurposing (DR) is an imminent approach for identifying novel therapeutic indications for the available drugs and discovering novel drugs for previously untreatable diseases. Nowadays, DR has major attention in the pharmaceutical industry due to the high cost and time of launching new drugs to the market through traditional drug development. DR task majorly depends on genetic information since the drugs revert the modified Gene Expression (GE) of diseases to normal. Many of the existing studies have not considered the genetic importance of predicting the potential candidates.
Method
We proposed a novel multimodal framework that utilizes genetic aspects of drugs and diseases such as genes, pathways, gene signatures, or expression to enhance the performance of DR using various data sources. Firstly, the heterogeneous biological network (HBN) is constructed with three types of nodes namely drug, disease, and gene, and 4 types of edges similarities (drug, gene, and disease), drug-gene, gene-disease, and drug-disease. Next, a modified graph auto-encoder (GAE*) model is applied to learn the representation of drug and disease nodes using the topological structure and edge information. Secondly, the HBN is enhanced with the information extracted from biomedical literature and ontology using a novel semi-supervised pattern embedding-based bootstrapping model and novel DR perspective representation learning respectively to improve the prediction performance. Finally, our proposed system uses a neural network model to generate the probability score of drug-disease pairs.
Results
We demonstrate the efficiency of the proposed model on various datasets and achieved outstanding performance in 5-fold cross-validation (AUC = 0.99, AUPR = 0.98). Further, we validated the top-ranked potential candidates using pathway analysis and proved that the known and predicted candidates share common genes in the pathways.}
}
@article{HARGREAVES2025301864,
title = {SOLVE-IT: A proposed digital forensic knowledge base inspired by MITRE ATT&CK},
journal = {Forensic Science International: Digital Investigation},
volume = {52},
pages = {301864},
year = {2025},
note = {DFRWS EU 2025 - Selected Papers from the 12th Annual Digital Forensics Research Conference Europe},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2025.301864},
url = {https://www.sciencedirect.com/science/article/pii/S2666281725000034},
author = {Christopher Hargreaves and Harm {van Beek} and Eoghan Casey},
keywords = {Digital forensic techniques, Digital forensic science, Quality assurance, Error-focused datasets, AI applications},
abstract = {This work presents SOLVE-IT (Systematic Objective-based Listing of Various Established (Digital) Investigation Techniques), a digital forensics knowledge base inspired by the MITRE ATT&CK cybersecurity resource. Several applications of the knowledge-base are demonstrated: strengthening tool testing by scoping error-focused data sets for a technique, reinforcing digital forensic techniques by cataloguing available mitigations for weaknesses (a systematic approach to performing Error Mitigation Analysis), bolstering quality assurance by identifying potential weaknesses in a specific digital forensic investigation or standard processes, structured consideration of potential uses of AI in digital forensics, augmenting automation by highlighting relevant CASE ontology classes and identifying ontology gaps, and prioritizing innovation by identifying academic research opportunities. The paper provides the structure and partial implementation of a knowledge base that includes an organised set of 104 digital forensic techniques, organised over 17 objectives, with detailed descriptions, errors, and mitigations provided for 33 of them. The knowledge base is hosted on an open platform (GitHub) to allow crowdsourced contributions to evolve the contents. Tools are also provided to export the machine readable back-end data into usable formats such as spreadsheets to support many applications, including systematic error mitigation and quality assurance documentation.}
}
@article{NNAMOKO2024100094,
title = {Automatic language ability assessment method based on natural language processing},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100094},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100094},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000426},
author = {Nonso Nnamoko and Themis Karaminis and Jack Procter and Joseph Barrowclough and Ioannis Korkontzelos},
keywords = {Cognitive assessment, Natural Language Processing, Language ability test, Cosine similarity, WASI-II, Word embedding},
abstract = {Background and Objectives:
The Wechsler Abbreviated Scales of Intelligence second edition (WASI-II) is a standardised assessment tool that is widely used to assess cognitive ability in clinical, research, and educational settings. In one of the components of this assessment, referred to as the Vocabulary task, the assessed individuals are presented with words (called stimulus items), and asked to explain what each word mean. Their responses are hand-scored based on a list of pre-rated sample responses [0-Point (poor), 1-Point (moderate), or 2-Point (excellent)] that is provided in the accompanying manual of WASI-II. This scoring method is time-consuming, and scoring of responses that do not fully match the pre-rated ones may vary between individual scorers. In this study, we aim to use natural language processing techniques to automate the scoring procedure and make it more time-efficient and reliable (objective).
Methods:
Utilising five different word embeddings (Word2vec, Global Vectors, Bidirectional Encoder Representations from Transformers, Generative Pre-trained Transformer 2, and Embeddings from Language Model), we transformed stimulus items and pre-rated responses from the WASI-II Vocabulary task into machine-readable vectors. We measured distance with cosine similarity, evaluating each model against a rational-expectations hypothesis that vector representations for stimuli should align closely with 2-Point responses and diverge from 0-Point responses. Assessment involved frequency of consistent representation and the Pearson correlation coefficient, examining overall consistency with the manual’s ranking across all items and sample responses.
Results:
The Word2vec model showed the highest consistency with the WASI-II manual (frequency = 20 out of 27; Pearson Correlation coefficient = 0.61) while Bidirectional Encoder Representations from Transformers was the worst performing model (frequency = 5; Pearson Correlation coefficient = 0.05). The consistency of these two models with the WASI-II manual differed significantly, Z = 2.282, p = 0.022.
Conclusions:
Our results showed that the scoring of the WASI-II Vocabulary task can be automated with moderate accuracy relying upon off-the-shelf embedding models. These results are promising, and could be improved further by considering alternative vector dimensions, similarity metrics, and data preprocessing techniques to those used in this study.}
}
@article{ZHANG2021111185,
title = {Semantic standards of external exposome data},
journal = {Environmental Research},
volume = {197},
pages = {111185},
year = {2021},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2021.111185},
url = {https://www.sciencedirect.com/science/article/pii/S0013935121004795},
author = {Hansi Zhang and Hui Hu and Matthew Diller and William R. Hogan and Mattia Prosperi and Yi Guo and Jiang Bian},
keywords = {Semantic standard, External exposome, Environmental exposure, Ontology},
abstract = {An individual’s health and conditions are associated with a complex interplay between the individual’s genetics and his or her exposures to both internal and external environments. Much attention has been placed on characterizing of the genome in the past; nevertheless, genetics only account for about 10% of an individual’s health conditions, while the remaining appears to be determined by environmental factors and gene-environment interactions. To comprehensively understand the causes of diseases and prevent them, environmental exposures, especially the external exposome, need to be systematically explored. However, the heterogeneity of the external exposome data sources (e.g., same exposure variables using different nomenclature in different data sources, or vice versa, two variables have the same or similar name but measure different exposures in reality) increases the difficulty of analyzing and understanding the associations between environmental exposures and health outcomes. To solve the issue, the development of semantic standards using an ontology-driven approach is inevitable because ontologies can (1) provide a unambiguous and consistent understanding of the variables in heterogeneous data sources, and (2) explicitly express and model the context of the variables and relationships between those variables. We conducted a review of existing ontology for the external exposome and found only four relevant ontologies. Further, the four existing ontologies are limited: they (1) often ignored the spatiotemporal characteristics of external exposome data, and (2) were developed in isolation from other conceptual frameworks (e.g., the socioecological model and the social determinants of health). Moving forward, the combination of multi-domain and multi-scale data (i.e., genome, phenome and exposome at different granularity) and different conceptual frameworks is the basis of health outcomes research in the future.}
}
@article{DU2022108560,
title = {Post-hoc recommendation explanations through an efficient exploitation of the DBpedia category hierarchy},
journal = {Knowledge-Based Systems},
volume = {245},
pages = {108560},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108560},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122002490},
author = {Yu Du and Sylvie Ranwez and Nicolas Sutton-Charani and Vincent Ranwez},
keywords = {Linked Open Data (LOD), Knowledge graph, Recommender system, Recommendation explanation, DBpedia, Ontology},
abstract = {Leveraging knowledge graphs for post-hoc recommendation explanations has been investigated in recent years. Existing approaches rely mainly on the overlap properties (encoded by knowledge graphs) that characterize both user liked items and the recommended ones. These approaches, however, do not fully leverage the property hierarchy of knowledge graphs which may lead to flawed explanations. In this paper we introduce an approach that takes the whole property hierarchy into account. This is done with a limited computation time overhead thanks to efficient algorithmic optimizations relying on sub-ontology extraction. The hierarchical relationships among properties are also considered to avoid redundant properties for explanation. We carried out a user study of 155 participants in the movie recommendation domain and used both offline and online metrics to assess the proposed approach. Significant improvements, in terms of informativeness (by 39%), persuasiveness (by 22%), engagement (by 29%) and user trust (by 26%), are suggested by the obtained results, as compared to the state-of-the-art property-based explanation model. Our findings indicate the superiority of accounting for the whole property hierarchy when dealing with post-hoc recommendation explanations.}
}
@article{CASTANO2022101842,
title = {A knowledge-centered framework for exploration and retrieval of legal documents},
journal = {Information Systems},
volume = {106},
pages = {101842},
year = {2022},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101842},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000788},
author = {Silvana Castano and Mattia Falduti and Alfio Ferrara and Stefano Montanelli},
keywords = {Legal knowledge model, Legal knowledge extraction, Legal document retrieval and exploration},
abstract = {Automated legal knowledge extraction systems are strongly demanded, to support annotation of legal documents as well as knowledge extraction from them, to provide useful and relevant suggestions to legal actors (e.g., judges, lawyers) for managing incoming new cases. In this paper, we propose CRIKE (CRIme Knowledge Extraction), a knowledge-based framework conceived to support legal knowledge extraction from a collection of legal documents, based on a reference legal ontology called LATO (Legal Abstract Term Ontology). We first introduce LATO-KM, the knowledge model of LATO where legal knowledge featuring documents in the collection is properly formalized as conceptual knowledge, in form of legal concepts and relationships, and terminological knowledge, in form of term-sets associated with legal concepts. Then, we present the bootstrapping cycle of CRIKE that aims to progressively enrich the terminological knowledge layer of LATO by extracting new terms from legal documents to be used for enriching the term-set associated with a corresponding legal concept. Finally, to evaluate the results obtained through CRIKE, we discuss experimental results on a real dataset of 180,000 court decisions of the State of Illinois taken from the Caselaw Access Project (CAP).}
}
@article{ZANOTTO2021,
title = {Stroke Outcome Measurements From Electronic Medical Records: Cross-sectional Study on the Effectiveness of Neural and Nonneural Classifiers},
journal = {JMIR Medical Informatics},
volume = {9},
number = {11},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/29120},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421000569},
author = {Bruna Stella Zanotto and Ana Paula {Beck da Silva Etges} and Avner {dal Bosco} and Eduardo Gabriel Cortes and Renata Ruschel and Ana Claudia {De Souza} and Claudio M V Andrade and Felipe Viegas and Sergio Canuto and Washington Luiz and Sheila {Ouriques Martins} and Renata Vieira and Carisi Polanczyk and Marcos {André Gonçalves}},
keywords = {natural language processing, stroke, outcomes, electronic medical records, EHR, electronic health records, text processing, data mining, text classification, patient outcomes},
abstract = {Background
With the rapid adoption of electronic medical records (EMRs), there is an ever-increasing opportunity to collect data and extract knowledge from EMRs to support patient-centered stroke management.
Objective
This study aims to compare the effectiveness of state-of-the-art automatic text classification methods in classifying data to support the prediction of clinical patient outcomes and the extraction of patient characteristics from EMRs.
Methods
Our study addressed the computational problems of information extraction and automatic text classification. We identified essential tasks to be considered in an ischemic stroke value-based program. The 30 selected tasks were classified (manually labeled by specialists) according to the following value agenda: tier 1 (achieved health care status), tier 2 (recovery process), care related (clinical management and risk scores), and baseline characteristics. The analyzed data set was retrospectively extracted from the EMRs of patients with stroke from a private Brazilian hospital between 2018 and 2019. A total of 44,206 sentences from free-text medical records in Portuguese were used to train and develop 10 supervised computational machine learning methods, including state-of-the-art neural and nonneural methods, along with ontological rules. As an experimental protocol, we used a 5-fold cross-validation procedure repeated 6 times, along with subject-wise sampling. A heatmap was used to display comparative result analyses according to the best algorithmic effectiveness (F1 score), supported by statistical significance tests. A feature importance analysis was conducted to provide insights into the results.
Results
The top-performing models were support vector machines trained with lexical and semantic textual features, showing the importance of dealing with noise in EMR textual representations. The support vector machine models produced statistically superior results in 71% (17/24) of tasks, with an F1 score >80% regarding care-related tasks (patient treatment location, fall risk, thrombolytic therapy, and pressure ulcer risk), the process of recovery (ability to feed orally or ambulate and communicate), health care status achieved (mortality), and baseline characteristics (diabetes, obesity, dyslipidemia, and smoking status). Neural methods were largely outperformed by more traditional nonneural methods, given the characteristics of the data set. Ontological rules were also effective in tasks such as baseline characteristics (alcoholism, atrial fibrillation, and coronary artery disease) and the Rankin scale. The complementarity in effectiveness among models suggests that a combination of models could enhance the results and cover more tasks in the future.
Conclusions
Advances in information technology capacity are essential for scalability and agility in measuring health status outcomes. This study allowed us to measure effectiveness and identify opportunities for automating the classification of outcomes of specific tasks related to clinical conditions of stroke victims, and thus ultimately assess the possibility of proactively using these machine learning techniques in real-world situations.}
}
@article{LIKAVEC2019203,
title = {Sigmoid similarity - a new feature-based similarity measure},
journal = {Information Sciences},
volume = {481},
pages = {203-218},
year = {2019},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518309630},
author = {Silvia Likavec and Ilaria Lombardi and Federica Cena},
keywords = {Similarity, Properties, Feature-based similarity, Hierarchy, Ontology, Instances},
abstract = {Similarity is one of the most straightforward ways to relate objects and guide the human perception of the world. It has an important role in many areas, such as Information Retrieval, Natural Language Processing, Semantic Web and Recommender Systems. To help applications in these areas achieve satisfying results when finding similar concepts, it is important to simulate human perception of similarity and assess which similarity measure is the most adequate. We propose Sigmoid similarity, a feature-based semantic similarity measure on instances in a specific ontology, as an improvement of Dice measure. We performed two separate evaluations with real evaluators. The first evaluation includes 137 subjects and 25 pairs of concepts in the recipes domain and the second one includes 147 subjects and 30 pairs of concepts in the drinks domain. To the best of our knowledge these are some of the most extensive evaluations in the field. We also explored the performance of some hierarchy-based approaches and showed that feature-based approaches outperform them on two specific ontologies we tested. In addition, we tried to incorporate hierarchy-based information into our measures and concluded it is not worth complicating the measures only based on features with additional information since they perform comparably.}
}
@article{COSTA2021100676,
title = {Design, Application and Evaluation of PROV-SwProcess: A PROV extension Data Model for Software Development Processes},
journal = {Journal of Web Semantics},
volume = {71},
pages = {100676},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100676},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000512},
author = {Gabriella Castro Barbosa Costa and Claudia Werner and Regina Braga and Eldânae Nogueira Teixeira and Victor Ströele and Marco Antônio Pereira Araújo and Marcos Alexandre Miguel},
keywords = {Software process, Provenance data, Provenance model, Ontology, Data analysis},
abstract = {The literature defines data provenance as the description of the origins of a piece of data and the process by which it arrived in a database. It helps to audit and understand data history and bring transparency to the process. Provenance has been successfully used in scientific computing, chemical industries, and health sciences, considering that these areas require a comprehensive traceability mechanism. Meanwhile, companies have been collecting and storing more data from their systems and processes. This work investigates if the use of provenance models and techniques can support software processes execution analysis and data-driven decision-making, considering the increasing availability of process data provided by companies. PROV-SwProcess, a software development process provenance modeling proposal, was developed and evaluated by process and provenance experts. Our proposal is an extension of the W3C recommended standard model PROV, aiming to capture and store the most relevant information about software development process provenance data. The results suggest that the model’s suitability improves and assists process managers in the software process analysis and supports data-driven decision-making.}
}
@article{LUO2025131360,
title = {Bi-directional generative retrieval-augmented diffusion models for document-level informative argument extraction},
journal = {Neurocomputing},
volume = {655},
pages = {131360},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131360},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225020326},
author = {Lei Luo and Xuanzhi Chen and Liming Mao and Xinjie Yang and Yajing Xu and Jun Guo},
keywords = {Informative argument extraction, Retrieval augmentation, Diffusion models},
abstract = {Document-level Informative Argument Extraction (IAE) presents a significant challenge in the field of information extraction. This challenge stems from the necessity for implicit coreference reasoning and the linking of long-range dependencies between events within a document. Despite recent efforts to leverage generation-based document-level extraction to enhance cross-sentence inference capabilities and capture more interactions between different events, these methods often fall short in their generation quality due to difficulties in understanding the global context. Motivated by these observations and the high-quality generation results of recent diffusion models, we propose an effective model known as BGRD (Bi-directional Generative Retrieval-augmented Diffusion models) for document-level IAE. In BGRD, a text diffusion model is designed to generate high-quality target event sequences that mutually benefit the retrieval stage, leveraging previously generated events as a retrieval source. Firstly, a bi-directional retrieval mechanism is investigated to refine the denoising process, effectively exploring the knowledge from retrieved samples. This enhances the text diffusion model’s ability to capture the global context interconnecting the events. Secondly, retrieval-augmented cross-attention is employed between the retrieved samples and the target event sequences (random Gaussian noise during the inference phase) within the text diffusion model. Through this interaction, the quality of the retrieval source is improved by generating highly informative event sequences, which benefits the bi-directional retrieval stage. Extensive experiments on the publicly available argument extraction datasets demonstrate the superiority of our proposed BGRD model over existing approaches.}
}
@article{WANG2024100566,
title = {IDS-KG: An industrial dataspace-based knowledge graph construction approach for smart maintenance},
journal = {Journal of Industrial Information Integration},
volume = {38},
pages = {100566},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100566},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000104},
author = {Yanying Wang and Ying Cheng and Qinglin Qi and Fei Tao},
keywords = {Knowledge graph, Industrial dataspace, Smart maintenance, Industrial knowledge management, BERT-casualKG},
abstract = {With the development of information technology in manufacturing enterprises, a large amount of equipment maintenance data and knowledge are recorded. These rich knowledge resources contain a vast amount of semantic and physical associations that have not yet been developed, resulting in a significant gap between equipment maintenance procedures and experiential knowledge. Therefore, this paper proposes a multi-source maintenance data management method called Industrial Dataspace (IDS), and on this basis, proposes a method for constructing an equipment maintenance knowledge graph (IDS-KG) that considers the causal relationships between faults in the equipment maintenance corpus. The method fixes procedural data on the ontology model at the upper layer of the knowledge graph and automatically mines maintenance information from empirical data, and ultimately achieves the fusion management of equipment maintenance procedure knowledge and empirical knowledge. The method is validated in the practical application of nuclear power equipment maintenance, and the experiments show that the method proposed in this paper is able to effectively fuse the procedural data and empirical data and structured as triplets, and at the same time, it is able to identify the hidden causal relationship between failures in the empirical data.}
}
@article{STACEWICZ20203810,
title = {To Know we Need to Share - Information in the Context of Interactive Acquisition of Knowledge},
journal = {Procedia Computer Science},
volume = {176},
pages = {3810-3819},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318974},
author = {Paweł Stacewicz and André Włodarczyk},
keywords = {information, knowledge, ortho-information, meta-information, para-information, semantics, pragmatics, ontology},
abstract = {Natural language interface requires much more complex processing than is currently assumed. We present a general review of foundational notions of data, information and knowledge, aiming at tentatively sketching out a set of subcomponents of an integra-tive linguistic theory of man machine interaction, bearing in mind that our model will favor further research on simulation of man man interaction. Our main concern is to show that in order to acquire knowledge humans need to extract it from multiple kinds of information as distributed in the content of natural language utterances.}
}
@article{GRUBLE2024156,
title = {Combined Geometric and Kinetic Data Model in Model-Based Systems Engineering of Robotic Cells},
journal = {Procedia CIRP},
volume = {128},
pages = {156-161},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124006802},
author = {Tobias Grüble and Ralf Stetter and Timo Schuchter and Markus Till and Stephan Rudolph},
keywords = {model based systems engineering, behavior modelling, graph-based design languages},
abstract = {The main intention of the presented research is the development of an engineering framework that allows the automated generation of a combined geometric and kinetic digital twin of a robotic cell. The engineering framework is based on graph-based design languages and an appropriate compiler for their translation in order to realize a novel form of a machine-executable V-model for Model-Based Systems Engineering (MBSE). In this novel machine-executable MBSE process, the primary process management objects are no longer some documents but abstract process descriptions that can be automatically compiled into concrete product models. For such a holistic MBSE process, the models have to be enriched with physical behavior and performance information. The method is illustrated with a use-case of the engineering process of robotic cells. The starting point of the automated synthesis of the robotic cell with its resources is the abstract modelling by means of object-oriented programming. The use-case illustrates a pick-and-place operation that requires a specific geometry of a monolithic gripper. The gripper geometry synthesis is based on design automation with topology optimization, which is also realized inside the executable V-model. A large number of synthesis results from the geometry generating processes can be included in automated kinetic simulations, thus generating digital twins for a large solution spectrum to achieve an overall holistic optimization. Further, a digital twin created from geometrical data and kinetic simulation enables process monitoring based on physical values (joint forces, pressure, etc.) which then can be used to predict failure or evaluate new/optimized motion sequences.}
}
@article{MEGHATRIA2020320,
title = {Event Nugget Detection using Pre-trained Language Models},
journal = {Procedia Computer Science},
volume = {176},
pages = {320-329},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.034},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318585},
author = {Riadh Meghatria and Chiraz Latiri and Fahima Nader},
keywords = {Information retrieval, Event detection, Nugget, RoBERTa, BERT, Fine-tuning},
abstract = {This paper handles the task of event nugget detection. In fact, deep learning methods were able to manage the extraction of relevant learned features. However, these methods tend to rely on NLP-Toolkits, as they feed gradually handcrafted features into their initial model. To alleviate this dependency and offer a deeper semantic understanding of the information encompassed in data, we investigate the use of pre-trained language models. The proposed approach uses the RoBERTa model because it offers a robust context-sensitive and pertinent representation of trends in data. The results demonstrate that our approach significantly outperforms its BERT-based variants and state-of-the-art approaches.}
}
@article{TAMBORINI2024102626,
title = {The epistemic grammar of bioinspired technologies: Shifting the focus from nature to scientific practices},
journal = {Technology in Society},
volume = {78},
pages = {102626},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102626},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X2400174X},
author = {Marco Tamborini},
keywords = {Biomimetic design, Natural forms, Ecological solutions, Grammar of practices, Epistemology, Philosophy of technology, Autonomous objects, Bioinspired technologies},
abstract = {This article philosophically addresses the promise of biomimetic design, according to which mimicking natural forms can provide better, more sustainable and less risky engineering solutions. Focusing on the grammar of scientific practices, the article establishes an epistemological basis for biomimetic technology that explores the creative interaction between technology and nature. The epistemological framework shows that bioinspired objects are autonomous with specific properties. They must be evaluated in their ability to fulfill the biomimetic promise in local applications and contexts. The article presents three results. First, it presents a philosophical framework that deepens the understanding of the relationship between sustainability and bioinspired technologies and allows for a retrospective assessment of their sustainability. Second, it points out that the scientific framework emerging from bioinspired technologies is based on collaboration, interdisciplinarity, and a holistic approach to scientific inquiry. Third, it encourages a shift in perspective from the ontology of bioinspired design to the epistemic grammar of its production.}
}
@article{PICCOLI2024101835,
title = {Digital transformation requires digital resource primacy: Clarification and future research directions},
journal = {The Journal of Strategic Information Systems},
volume = {33},
number = {2},
pages = {101835},
year = {2024},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2024.101835},
url = {https://www.sciencedirect.com/science/article/pii/S0963868724000179},
author = {Gabriele Piccoli and Varun Grover and Joaquin Rodriguez},
keywords = {Digital transformation, IT-enabled transformation, Digital ontology, Digital organization, Digital resources},
abstract = {Responding to recent calls, this essay offers a commentary on the framing and definition of organizational digital transformation. We focus on the unique ontology of digital transformation and delineate it from neighboring concepts.Our contention is that, despite its volume, current research remains unclear about how the digital transformation of organizations differs from their IT-enabled transformation. We advocate definitional precision to foster knowledge accumulation and to enable scholars to pursue important research questions that are unique to digital transformation. Our perspective, grounded in the notion of digital resources, defines digital transformation as the metamorphosis of an IT-enabled organization into a digital organization – one with a specific digital architecture and design principles.A key departure from previous conceptualization is that we characterize digital transformation as a change in digital technology architecture rather than a change from digital technology use. Our paper achieves the following: describes the constructs underpinning this formulation, digital resources and digital organization; justifies their use; and describes what research directions the new perspective promotes. With sound definitions of key constructs, Information Systems scholars have the unprecedented opportunity to lead the way in digital “x” research, making our discipline the reference point for the burgeoning “digital research” literature in related business fields.}
}
@incollection{WEE2024,
title = {Language Rights},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032395504100003X},
author = {Lionel Wee},
keywords = {Collective rights, Discrimination, Gender, Individual rights, Language death, Language ecology, Linguistic citizenship, Language preservation, Linguistic human rights, Minority language rights},
abstract = {Ideological assumptions about languages and their speakers are oftentimes institutionalized in education systems, in workplace practices, and even in socio-cultural expectations about what it means to be a “good/sophisticated/respectable” person. As a consequence, there may be practices that marginalize individuals or groups simply by virtue of the language(s) that they happen to speak. The concept of language rights aims to provide one way of dealing with cases of linguistic discrimination. However, there are problems with such a concept. An alternative, linguistic citizenship, provides a more nuanced approach.}
}
@article{DIMASSI2023103824,
title = {A knowledge recommendation approach in design for multi-material 4D printing based on semantic similarity vector space model and case-based reasoning},
journal = {Computers in Industry},
volume = {145},
pages = {103824},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103824},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522002202},
author = {Saoussen Dimassi and Frédéric Demoly and Hadrien Belkebir and Christophe Cruz and Kyoung-Yun Kim and Samuel Gomes and H. Jerry Qi and Jean-Claude André},
keywords = {4D printing, Vector space model, Case-based reasoning, Knowledge recommendation},
abstract = {4D printing refers to a breakthrough technology combining additive manufacturing (AM) and smart materials (SMs) under the effect of an energy stimulation. From the numerous experiments proposed in the literature, which highlight the broad spectrum of shape- and/or property-changing capacities, an emerging research interest is to tackle 4D printing from a design perspective. The so-called design for 4D printing requires rethinking well-established models, methods, and tools to develop innovative devices with changing capacities. It is particularly suitable to focus on the computational design synthesis where decisions on AM techniques and SMs selection have an important influence on the part geometry and materials distribution. Previous research work has developed an ontology for capturing multiple perspective knowledge related to SMs, stimuli, AM, and product design. However, the construction of a complex and interdisciplinary knowledge base is not sufficient and requires knowledge reuse/recommendation mechanisms for guiding designers and engineers. Therefore, the article aims to propose a knowledge recommendation approach in computational design for 4D printing with a semantic similarity vector space model. A specific emphasis is made on multi-material 4D printing where recommendations are needed to determine active and passive materials’ distributions for achieving the desired shape change. An implementation of the approach in a computer-aided design plugin is described and illustrative cases from the literature are introduced to demonstrate its applicability.}
}
@incollection{CARDIER2025193,
title = {8 - Collaborative communication for unnamable risks: a creative writing approach to aligning human–machine situation models in an open world},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Marco Brambilla},
booktitle = {Bi-directionality in Human-AI Collaborative Systems},
publisher = {Academic Press},
pages = {193-226},
year = {2025},
isbn = {978-0-443-40553-2},
doi = {https://doi.org/10.1016/B978-0-44-340553-2.00014-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443405532000149},
author = {Beth Cardier},
keywords = {Reasoning across contexts, Open world reasoning, Bidirectional human–machine communication, Communicating novelty, Collaborative storytelling, Situated reasoning, Analogical inference},
abstract = {How can a machine warn its human collaborator about an unexpected risk if the machine does not possess the explicit language required to name it? This research transfers techniques from creative writing into a conversational format that could enable a machine to convey a novel, open world threat. Professional writers specialize in communicating unexpected conditions with inadequate language, using overlapping contextual and analogical inferences to adjust a reader's situation model. This chapter explores how a similar approach could be used in conversation by a machine to adapt its human collaborator's situation model to include unexpected information, even though it does not possess adequate language to describe it. This method is necessarily bidirectional, as the process of refining meaning on each side requires incremental adjustments to each other. A proposed method and two examples are presented, set five and ten years hence, to envisage a new kind of capability in human–machine interaction. A near-term goal is to develop foundations for autonomous communication that can adapt across different contexts, especially when a trusted outcome is critical. A larger goal is to make visible the level of communication that occurs above both explicit and implicit communication, where language can be collaboratively adapted.}
}
@incollection{SALLES2024101,
title = {Chapter Six - Anthropomorphism in social AIs: Some challenges},
editor = {Marcello Ienca and Georg Starke},
series = {Developments in Neuroethics and Bioethics},
publisher = {Academic Press},
volume = {7},
pages = {101-118},
year = {2024},
booktitle = {Brains and Machines: Towards a Unified Ethics of AI and Neuroscience},
issn = {2589-2959},
doi = {https://doi.org/10.1016/bs.dnb.2024.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S2589295924000122},
author = {Arleen Salles and Abel {Wajnerman Paz}},
keywords = {Anthropomorphism, Social AIs, Social chatbots, Ontological, Ethical, Humanness, Pragmatic strategy, Human-AI relationship, Deception},
abstract = {In this chapter, we are concerned with anthropomorphism in social AIs, particularly social chatbots. While not embodied in terms of having a human-like appearance, these chatbots are designed to induce attribution of intentionality and agency to engage users in enhanced interactions. Here we identify and present two strategies used to address the legitimacy of the anthropomorphisation, the intentional creation of human-like traits, of AIs: ontological and pragmatic. We further review some objections to pragmatic attempts to justify the anthropomorphisation of AIs. It is not our goal to argue in favour or against anthropomorphising social chatbots. Rather we examine some persistent concerns and call for more attention to and further research and reflection on the ethical, psychological, and ontological assumptions underlying them.}
}
@article{KUMAR2023120022,
title = {A novel approach to generate distractors for Multiple Choice Questions},
journal = {Expert Systems with Applications},
volume = {225},
pages = {120022},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120022},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423005249},
author = {Archana Praveen Kumar and Ashalatha Nayak and Manjula Shenoy K. and Shashank Goyal and  Chaitanya},
keywords = {Ontology, Multiple choice questions, Distractor generation, Item Response Theory},
abstract = {Multiple Choice Questions (MCQs) have been predominantly used as an assessment tool in the educational domain. The MCQ comprises a question called ‘Stem’, one correct answer called ‘Key’, and the incorrect options called ‘Distractors’. Identifying distractors is an essential step in MCQ construction because distractors need to be misleading and plausibly incorrect. Therefore the manual construction of MCQ is error-prone, and requires cumbersome efforts. Hence existing works have focused on automatic generation of MCQs but primarily towards vocabulary assessment. However, very few works inclined towards the technical domain have failed to analyze the plausibility of distractors. In this context, the proposed research DIstractor GENeration (DIGEN) is targeted to generate distractors for the MCQ in the technical domain. Hence, the novel contribution here is DIGEN takes unstructured text as well as multiple-choice questions with key as mandatory source along with ontology which may be an optional source to generate distractors automatically in the technical domain. Distractors generated have been evaluated based on Item Response Theory, which shows promising results.}
}
@article{SOLANKI2019476,
title = {Towards a knowledge driven framework for bridging the gap between software and data engineering},
journal = {Journal of Systems and Software},
volume = {149},
pages = {476-484},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302772},
author = {Monika Solanki and Bojan Božić and Christian Dirschl and Rob Brennan},
keywords = {Ontologies, Data engineering, Software engineering, Alignment, Integration},
abstract = {In this paper we present a collection of ontologies specifically designed to model the information exchange needs of combined software and data engineering. Effective, collaborative integration of software and big data engineering for Web-scale systems, is now a crucial technical and economic challenge. This requires new combined data and software engineering processes and tools. Our proposed models have been deployed to enable: tool-chain integration, such as the exchange of data quality reports; cross-domain communication, such as interlinked data and software unit testing; mediation of the system design process through the capture of design intents and as a source of context for model-driven software engineering processes. These ontologies are deployed in web-scale, data-intensive, system development environments in both the commercial and academic domains. We exemplify the usage of the suite on case-studies emerging from two complex collaborative software and data engineering scenarios: one from the legal sector and the other from the Social sciences and Humanities domain.}
}
@article{DEER2021103722,
title = {Characterizing Long COVID: Deep Phenotype of a Complex Condition},
journal = {eBioMedicine},
volume = {74},
pages = {103722},
year = {2021},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2021.103722},
url = {https://www.sciencedirect.com/science/article/pii/S2352396421005168},
author = {Rachel R Deer and Madeline A Rock and Nicole Vasilevsky and Leigh Carmody and Halie Rando and Alfred J Anzalone and Marc D Basson and Tellen D Bennett and Timothy Bergquist and Eilis A Boudreau and Carolyn T Bramante and James Brian Byrd and Tiffany J Callahan and Lauren E Chan and Haitao Chu and Christopher G Chute and Ben D Coleman and Hannah E Davis and Joel Gagnier and Casey S Greene and William B Hillegass and Ramakanth Kavuluru and Wesley D Kimble and Farrukh M Koraishy and Sebastian Köhler and Chen Liang and Feifan Liu and Hongfang Liu and Vithal Madhira and Charisse R Madlock-Brown and Nicolas Matentzoglu and Diego R Mazzotti and Julie A McMurry and Douglas S McNair and Richard A Moffitt and Teshamae S Monteith and Ann M Parker and Mallory A Perry and Emily Pfaff and Justin T Reese and Joel Saltz and Robert A Schuff and Anthony E Solomonides and Julian Solway and Heidi Spratt and Gary S Stein and Anupam A Sule and Umit Topaloglu and George D. Vavougios and Liwei Wang and Melissa A Haendel and Peter N Robinson},
keywords = {COVID-19, of post-acute sequelae of SARS-CoV-2, human phenotype ontology, long COVID, phenotyping},
abstract = {ABSTRACT
Background
Numerous publications describe the clinical manifestations of post-acute sequelae of SARS-CoV-2 (PASC or “long COVID”), but they are difficult to integrate because of heterogeneous methods and the lack of a standard for denoting the many phenotypic manifestations. Patient-led studies are of particular importance for understanding the natural history of COVID-19, but integration is hampered because they often use different terms to describe the same symptom or condition. This significant disparity in patient versus clinical characterization motivated the proposed ontological approach to specifying manifestations, which will improve capture and integration of future long COVID studies.
Methods
The Human Phenotype Ontology (HPO) is a widely used standard for exchange and analysis of phenotypic abnormalities in human disease but has not yet been applied to the analysis of COVID-19.
Findings
We identified 303 articles published before April 29, 2021, curated 59 relevant manuscripts that described clinical manifestations in 81 cohorts three weeks or more following acute COVID-19, and mapped 287 unique clinical findings to HPO terms. We present layperson synonyms and definitions that can be used to link patient self-report questionnaires to standard medical terminology. Long COVID clinical manifestations are not assessed consistently across studies, and most manifestations have been reported with a wide range of synonyms by different authors. Across at least 10 cohorts, authors reported 31 unique clinical features corresponding to HPO terms; the most commonly reported feature was Fatigue (median 45.1%) and the least commonly reported was Nausea (median 3.9%), but the reported percentages varied widely between studies.
Interpretation
Translating long COVID manifestations into computable HPO terms will improve analysis, data capture, and classification of long COVID patients. If researchers, clinicians, and patients share a common language, then studies can be compared/pooled more effectively. Furthermore, mapping lay terminology to HPO will help patients assist clinicians and researchers in creating phenotypic characterizations that are computationally accessible, thereby improving the stratification, diagnosis, and treatment of long COVID.
Funding
U24TR002306; UL1TR001439; P30AG024832; GBMF4552; R01HG010067; UL1TR002535; K23HL128909; UL1TR002389; K99GM145411.}
}
@article{RIANO2019101713,
title = {Ten years of knowledge representation for health care (2009–2018): Topics, trends, and challenges},
journal = {Artificial Intelligence in Medicine},
volume = {100},
pages = {101713},
year = {2019},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2019.101713},
url = {https://www.sciencedirect.com/science/article/pii/S0933365719304531},
author = {David Riaño and Mor Peleg and Annette {ten Teije}},
keywords = {Knowledge representation for health care, Literature review, Medical informatics},
abstract = {Background
In the last ten years, the international workshop on knowledge representation for health care (KR4HC) has hosted outstanding contributions of the artificial intelligence in medicine community pertaining to the formalization and representation of medical knowledge for supporting clinical care. Contributions regarding modeling languages, technologies and methodologies to produce these models, their incorporation into medical decision support systems, and practical applications in concrete medical settings have been the main contributions and the basis to define the evolution of this field across Europe and worldwide.
Objectives
Carry out a review of the papers accepted in KR4HC in the 2009–2018 decade, analyze and characterize the topics and trends within this field, and identify challenges for the evolution of the area in the near future.
Methods
We reviewed the title, the abstract, and the keywords of the 112 papers that were accepted to the workshop, identified the medical and technological topics involved in these works, provided a classification of these papers in medical and technological perspectives and obtained the timeline of these topics in order to determine interest growths and declines. The experience of the authors in the field and the evidences after the review were the basis to propose a list of challenges of knowledge representation in health care for the future.
Results
The most generic knowledge representation methods are ontologies (31%), semantic web related formalisms (26%), decision tables and rules (19%), logic (14%), and probabilistic models (10%). From a medical informatics perspective, knowledge is mainly represented as computer interpretable clinical guidelines (43%), medical domain ontologies (26%), and electronic health care records (22%). Within the knowledge lifecycle, contributions are found in knowledge generation (38%), knowledge specification (24%), exception detection and management (12%), knowledge enactment (8%), temporal knowledge and reasoning (7%), and knowledge sharing and maintenance (7%). The clinical emphasis of knowledge is mainly related to clinical treatments (27%), diagnosis (13%), clinical quality indicators (13%), and guideline integration for multimorbid patients (12%). According to the level of development of the works presented, we distinguished four maturity levels: formal (22%), implementation (52%), testing (13%), and deployment (2%) levels. Some papers described technologies for specific clinical issues or diseases, mainly cancer (22%) and diseases of the circulatory system (20%). Chronicity and comorbidity were present in 10% and 8% of the papers, respectively.
Conclusions
KR4HC is a stable community, still active after ten years. A persistent focus has been knowledge representation, with an emphasis on semantic-web ontologies and on clinical-guideline based decision-support. Among others, two topics receive growing attention: integration of computer-interpretable guideline knowledge for the management of multimorbidity patients, and patient empowerment and patient-centric care.}
}
@article{ZHAO2021469,
title = {An exploratory analysis: extracting materials science knowledge from unstructured scholarly data},
journal = {The Electronic Library},
volume = {39},
number = {3},
pages = {469-485},
year = {2021},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-11-2020-0320},
url = {https://www.sciencedirect.com/science/article/pii/S0264047321000357},
author = {Xintong Zhao and Jane Greenberg and Vanessa Meschke and Eric Toberer and Xiaohua Hu},
keywords = {Information science, Ontology, Knowledge, Digital scholarship, Materials science, Knowledge extraction},
abstract = {Purpose
The output of academic literature has increased significantly due to digital technology, presenting researchers with a challenge across every discipline, including materials science, as it is impossible to manually read and extract knowledge from millions of published literature. The purpose of this study is to address this challenge by exploring knowledge extraction in materials science, as applied to digital scholarship. An overriding goal is to help inform readers about the status knowledge extraction in materials science.
Design/methodology/approach
The authors conducted a two-part analysis, comparing knowledge extraction methods applied materials science scholarship, across a sample of 22 articles; followed by a comparison of HIVE-4-MAT, an ontology-based knowledge extraction and MatScholar, a named entity recognition (NER) application. This paper covers contextual background, and a review of three tiers of knowledge extraction (ontology-based, NER and relation extraction), followed by the research goals and approach.
Findings
The results indicate three key needs for researchers to consider for advancing knowledge extraction: the need for materials science focused corpora; the need for researchers to define the scope of the research being pursued, and the need to understand the tradeoffs among different knowledge extraction methods. This paper also points to future material science research potential with relation extraction and increased availability of ontologies.
Originality/value
To the best of the authors’ knowledge, there are very few studies examining knowledge extraction in materials science. This work makes an important contribution to this underexplored research area.}
}
@article{CHEN2021100625,
title = {Knowledge graph embeddings for dealing with concept drift in machine learning},
journal = {Journal of Web Semantics},
volume = {67},
pages = {100625},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100625},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300585},
author = {Jiaoyan Chen and Freddy Lécué and Jeff Z. Pan and Shumin Deng and Huajun Chen},
keywords = {Ontology, Knowledge graph embedding, Stream learning, Concept drift},
abstract = {Data stream learning has been largely studied for extracting knowledge structures from continuous and rapid data records. As data is evolving on a temporal basis, its underlying knowledge is subject to many challenges. Concept drift,11This work is addressing the challenge concept drift in machine learning as opposed to concept drift in the Semantic Web community where “concept” (class) meaning in ontology TBox shifts from versioning, iterations or modifications. Note that changes in ABox alone can also lead to concept drift in learning from ontology streams.. as one of core challenge from the stream learning community, is described as changes of statistical properties of the data over time, causing most of machine learning models to be less accurate as changes over time are in unforeseen ways. This is particularly problematic as the evolution of data could derive to dramatic change in knowledge. We address this problem by studying the semantic representation of data streams in the Semantic Web, i.e., ontology streams. Such streams are ordered sequences of data annotated with ontological vocabulary. In particular we exploit three levels of knowledge encoded in ontology streams to deal with concept drifts: i) existence of novel knowledge gained from stream dynamics, ii) significance of knowledge change and evolution, and iii) (in)consistency of knowledge evolution. Such knowledge is encoded as knowledge graph embeddings through a combination of novel representations: entailment vectors, entailment weights, and a consistency vector. We illustrate our approach on classification tasks of supervised learning. Key contributions of the study include: (i) an effective knowledge graph embedding approach for stream ontologies, and (ii) a generic consistent prediction framework with integrated knowledge graph embeddings for dealing with concept drifts. The experiments have shown that our approach provides accurate predictions towards air quality in Beijing and bus delay in Dublin with real world ontology streams.}
}
@article{ZHENG2025101406,
title = {You don’t need to prove yourself: A raciolinguistic perspective on Chinese international students’ academic language anxiety and ChatGPT use},
journal = {Linguistics and Education},
volume = {86},
pages = {101406},
year = {2025},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2025.101406},
url = {https://www.sciencedirect.com/science/article/pii/S0898589825000245},
author = {Kewen Zheng},
keywords = {Academic language, Chinese international student, ChatGPT, Linguistic insecurity, Raciolinguistic perspective},
abstract = {Adopting a raciolinguistic perspective, this study examines the Chinese international students’ anxiety on academic English writing under ChatGPT use and AI policing. As the baseline of language proficiency and a gatekeeper to academic achievement, academic English stands as the exclusive linguistic standard within academia. Chinese international students in the U.S. often experience different levels of anxiety in academic English writing as they are usually perceived as outsiders and linguistically deficient. In this context, AI language refining tools like ChatGPT have become a popular tool for such students to “standardize” their academic English writing. While these tools “improve” the quality of writing to some extent, they in fact exacerbate linguistic insecurity and language anxiety, because of their native-like response mode and readers’ biased policing. Through analysis of the interview data and human-AI interaction data provided by 18 Chinese international students in a U.S. graduate program, this study reveals the structural challenges faced by bilingual international students as they navigate academia in the era of ChatGPT. This paper contributes to a deeper understanding of the complexities surrounding requirement of academic English proficiency and racialized speakers’ experiences of being subjugated and engaging in self-policing in academic settings.}
}
@article{SEIDENBERG2023108097,
title = {Boosting autonomous process design and intensification with formalized domain knowledge},
journal = {Computers & Chemical Engineering},
volume = {169},
pages = {108097},
year = {2023},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.108097},
url = {https://www.sciencedirect.com/science/article/pii/S0098135422004306},
author = {J. Raphael Seidenberg and Ahmad A. Khan and Alexei A. Lapkin},
keywords = {Process synthesis, Process intensification, Ontology, Knowledge graph, Reinforcement learning, Machine learning},
abstract = {Conceptual process design deals with searching for optimal process flowsheets in a large design space. Effective approaches benefit from sensible search space restrictions, commonly carried out by a knowledgeable expert in the form of a superstructure optimization or heuristic rules. To achieve the goal of autonomous process design, knowledge has to be formalized in a machine-readable format. This contribution aims to incorporate an ontological representation of fundamental process knowledge to empower general-purpose design procedures while respecting problem-specific variability. Specifically, the presented framework leverages an ontology to express declarative knowledge (what-is) of processes, phenomena and design tasks to set up the search space and boost a hierarchical reinforcement learning agent which learns the required procedural knowledge (how-to) in order to find an optimal solution. The work is applied in a case study of an intensified steam methane reforming process. Results show that the automated treatment of domain knowledge allows for dynamic search space reduction and achieves better computational efficiency and solution quality, highlighting its potential in autonomous process design approaches.}
}
@incollection{ELVE20182215,
title = {A framework for multi-network modelling},
editor = {Mario R. Eden and Marianthi G. Ierapetritou and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {44},
pages = {2215-2220},
year = {2018},
booktitle = {13th International Symposium on Process Systems Engineering (PSE 2018)},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64241-7.50364-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642417503645},
author = {Arne Tobias Elve and Heinz A. Preisig},
keywords = {Graph-based Modelling, Automatic Code Generation, Model-Based Simulation, Customized Modelling, Multi-Scale Modelling},
abstract = {We present a new modelling framework called the ModelComposer. This framework combines well-known concepts such as equation-based modelling, graph-based modelling and code generation to produce simulation models automatically. Moreover, the framework uses ontologies as the bases, which means that interoperability of models produced by the modelling framework is high. The model information is stored in JSON-files, and code generation for different programming languages is used to transform the defined models into executable simulation. A result is a modelling tool that promotes the reuse of models and interoperability between networks of models of different domains. This paper will present the overall layout of the ModelComposer and how the framework handles internal conversions within a network and how different networks communicates. This is exemplified by a case study presenting the model construction of a dynamic mixing tank and the simulation of the resulting model.}
}
@article{DEVANAND2022107556,
title = {ElChemo: A cross-domain interoperability between chemical and electrical systems in a plant},
journal = {Computers & Chemical Engineering},
volume = {156},
pages = {107556},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107556},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421003343},
author = {Aravind Devanand and Gourab Karmakar and Nenad Krdzavac and Feroz Farazi and Mei Qi Lim and Y.S. {Foo Eddy} and Iftekhar A. Karimi and Markus Kraft},
keywords = {Ontology, Semantic web, Knowledge-graph, Digital twin, Industry 4.0, Reasoning},
abstract = {The paper proposes a novel framework capable of establishing machine-to-machine (M2M) interactions between chemical and electrical systems in the industry. The framework termed as ElChemo addresses the challenges in M2M interaction of entities from different silos, such as differences in the domains’ behaviour, the heterogeneities arising from different vocabularies and software. The OntoTwin ontology has been developed based on OntoPowSys and OntoEIP ontologies, which are parts of an intelligent platform called the “J-Park Simulator (JPS)”. The ElChemo framework uses Description Logic (DL) and SPIN reasoning techniques to establish the interaction between the chemical and electrical systems in a plant. This paper presents a depropaniser section of a chemical plant and its corresponding electrical system as a use case scenario to demonstrate the interoperability between the two silos within the ElChemo framework. The results from the use case demonstrate, as a proof of concept, the potential of the proposed framework and can be considered as the first step towards the development of a knowledge graph based framework capable of increasing interoperability between cross-domain interactions.}
}
@article{ALQURASHI2024374,
title = {A data-driven multi-perspective approach to cybersecurity knowledge discovery through topic modelling},
journal = {Alexandria Engineering Journal},
volume = {107},
pages = {374-389},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.07.044},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824007658},
author = {Fahad Alqurashi and Istiak Ahmad},
keywords = {Cyber security, Knowledge discovery, Semantic analysis, Bert, Topic modelling, Natural language processing},
abstract = {Cybersecurity is crucial for protecting the privacy of digital systems, maintaining economic stability, and ensuring national security. This study presents a comprehensive approach to cybersecurity knowledge discovery through topic modelling, using a multi-perspective analysis of academic and industry sources. The datasets include 15,751 articles from the Web of Science (WoS) database and 5,831 articles from Security Magazine, spanning from 2011 to 2023. We employed BERTopic for topic modelling, UMAP for dimensionality reduction, and HDBSCAN clustering algorithm for grouping and analysing distinct article clusters to uncover latent topics, enhancing the understanding of the evolving cybersecurity landscape. This study found 12 micro-clusters and three macro-clusters, namely technology, smart city and education, from the WoS database and 12 more micro-clusters and four macro-clusters, including organization, public security, governance, and education, from magazines. This study reveals key cybersecurity research and practice trends, such as the increasing focus on malware, ransomware, and cyber-attack mitigation. Additionally, temporal analysis indicates a significant rise in cybersecurity interest around 2020, followed by a diversification of topics. The results highlight the importance of integrating diverse data sources to capture a holistic view of cybersecurity developments. Future work will aim to refine the clustering algorithms to further improve topic extraction and analysis and expand the datasets to include more diverse sources and perspectives. This approach helps discover current cybersecurity trends and provides a foundation for more targeted and effective cybersecurity strategies.}
}
@article{ELLWEIN2023360,
title = {Software-defined Manufacturing: Data Representation},
journal = {Procedia CIRP},
volume = {118},
pages = {360-365},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.062},
url = {https://www.sciencedirect.com/science/article/pii/S221282712300286X},
author = {Carsten Ellwein and Rebekka Neumann and Alexander Verl},
keywords = {Software defined manufacturing, Data representation, Manufacturing ontology},
abstract = {Due to constantly decreasing product cycles, a fast adaptability of manufacturing plants is a prerequisite for profitable operation. The paradigm of Software-defined Manufacturing (SDM) pursues this overriding goal by decoupling software from mechanical and electrical components. In this way, SDM allows the free definition of functionality, purely through software, limited only by the physical limitations of the machines and systems. This paper analyzes existing approaches that pursue the goal of digitally representing manufacturing to enable flexible adaptation. Based on this analysis, a model for the data representation is proposed that allows consistent use across the entire supply chain.}
}
@article{RAMEZANZADEH2018137,
title = {The conceptualization and exploration of socially just teaching: A qualitative study on higher education English professors},
journal = {Teaching and Teacher Education},
volume = {74},
pages = {137-145},
year = {2018},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2018.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X17308405},
author = {Akram Ramezanzadeh and Saeed Rezaei},
keywords = {Socially just teaching, Inductive thematic analysis, Non-native English language professors, Higher education},
abstract = {This study explored the conceptualization of socially just teaching in higher education. Participants were English language professors who were studied through a qualitative study. The data collected through interviews and memos were analyzed using inductive thematic analysis. Three themes were extracted: a critical stance, a dialogic and emergent curriculum, and ontological turns. The findings indicated that socially just teaching requires foregrounding the questions of being. Furthermore, the findings revealed that socially just teaching necessitates critiquing othering based on essentialist stereotypes through the contextualized teaching which revolves around cultural recognition, political representation, and contextual sensitivity.}
}
@article{BEUST20243999,
title = {BioPAX in 2024: Where we are and where we are heading},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {3999-4010},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024003660},
author = {Cécile Beust and Emmanuelle Becker and Nathalie Théret and Olivier Dameron},
keywords = {Systems Biology, BioPAX, Biological pathways, Databases},
abstract = {In systems biology, the study of biological pathways plays a central role in understanding the complexity of biological systems. The massification of pathway data made available by numerous online databases in recent years has given rise to an important need for standardization of this data. The BioPAX format (Biological Pathway Exchange) emerged in 2010 as a solution for standardizing and exchanging pathway data across databases. BioPAX is a Semantic Web format associated to an ontology. It is highly expressive, allowing to finely describe biological pathways at the molecular and cellular levels, but the associated intrinsic complexity may be an obstacle to its widespread adoption. Here, we report on the use of the BioPAX format in 2024. We compare how the different pathway databases use BioPAX to standardize their data and point out possible avenues for improvement to make full use of its potential. We also report on the various tools and software that have been developed to work with BioPAX data. Finally, we present a new concept of abstraction on BioPAX graphs that would allow to specifically target areas in a BioPAX graph needed for a specific analysis, thus differentiating the format suited for representation and the abstraction suited for contextual analysis.}
}
@article{THAKUR2022100514,
title = {Cloud services selection: A systematic review and future research directions},
journal = {Computer Science Review},
volume = {46},
pages = {100514},
year = {2022},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2022.100514},
url = {https://www.sciencedirect.com/science/article/pii/S157401372200048X},
author = {Neha Thakur and Avtar Singh and A.L. Sangal},
keywords = {Cloud computing, Cloud services, Multi-criteria decision making, Ontology, Cloud services selection, Systematic review},
abstract = {Cloud computing has developed in popularity as a large-scale computing paradigm that offers a range of computing resources as a service through the internet on a pay-as-you-go basis. The expansion in demand and commercial availability of cloud services brings new challenges to cloud services selection. Several research studies have been conducted to develop enhanced methodologies to assist service consumers in selecting appropriate services. In this paper, 105 primary studies published during January, 2011 to May, 2022 has been selected using a multi-stage scrutinizing approach. The selected preliminary studies were further classified based on various variables to answer the research questions stated for this work. A systematic review of existing cloud service selection approaches is performed, which are analyzed along eight dimensions: decision-making methods, context, purposes, cloud service performance parameters, simulation/language tools, domain, datasets, and experiment/validation methods. After a thorough review and comparison of these approaches across the above-mentioned dimensions, several open research issues in the current literature have been identified. The contribution of this research is fourfold: focusing on state-of-the-art cloud services selection approaches, highlighting the benefits and drawbacks of various cloud services selection methodologies and their future directions, offering a taxonomy based on a thorough literature study, and identifying nine critical challenges in cloud services selection that require further investigation. This systematic review study is anticipated to benefit both academics and business experts.}
}
@article{SCUTELNICU2023822,
title = {Word Sense Disambiguation Corpus Development for Romanian Language},
journal = {Procedia Computer Science},
volume = {225},
pages = {822-831},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.069},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923012279},
author = {Liviu Andrei Scutelnicu},
keywords = {Lexical resources, electronic dictionary, Romanian WordNet, word sense alignment, gold corpus},
abstract = {Research in the area of ​​the interconnection of lexical resources represents a real challenge, because it addresses the difficult problem of semantic understanding and, more precisely, the disambiguation of the meaning of the words - Word Sense Disambiguation (WSD). In the current and prospective context of the information society, the existence of the digital format of the fundamental works of a national culture is strictly necessary. It is a topical issue throughout the world of creating a representative corpus of a language accessible through the Internet, the corpus being a concrete, clear picture of the use of that language. In this study we will describe the development of a Romanian language GOLD corpus, related to the multiple meanings existing for various words. We propose a corpus annotation standard, based on three lexical resources as follows: the Thesaurus Dictionary of the Romanian Language in electronic format (eDTLR), from which we extracted a list of words with multiple meanings; from the Reference Corpus for Contemporary Romanian Language (CoRoLa) we extracted contexts in which these words were founded and from the the Romanian WordNet (RoWN) resource, we took into account the sense meaning of the word from the corpus context.}
}
@incollection{AGAPITO2025160,
title = {Standards and Modals for Biological Data: BioPax},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {160-167},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000026},
author = {Giuseppe Agapito},
keywords = {Biological networks, Biological pathways, BioPAX, Ontology, OWL, RD, XML},
abstract = {The Biological Pathways Exchange Language (BioPAX) is an open-source OWL-based format representing biological pathways. BioPAX is a computer-readable OWL-based language for representing and integrating different pathways data, comprising cell signaling pathways, metabolic pathways, transportation pathways, gene regulation, genetic interactions, and many others. The adoption of BioPAX to store, exchange, and share data reduces data integration and interoperability problems. Using the BioPAX format reduces data integration to a semantic mapping between the data model on the model defined by BioPAX. As well as, the adoption of BioPAX to represent pathway data will increase the uniformity of pathway data coming from several different sources, increasing the efficiency of computational pathway research analysis. In this way, users would be better capable of spending more time on research tasks rather than struggling with data format issues.}
}
@article{KESANIEMI2022251,
title = {Challenges in managing semantic annotations in harvested research objects in a national CRIS context},
journal = {Procedia Computer Science},
volume = {211},
pages = {251-256},
year = {2022},
note = {15th International Conference on Current Research Information Systems},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.199},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922016647},
author = {Joonas Kesäniemi and Tommi Suominen and Katja Mankinen},
keywords = {CRIS, ontology, annotation, linked data, data management},
abstract = {Harvested metadata on research objects can include links between the primary domain objects such as organizational identifiers associated with dataset, persons identified with ORCIDs linked to publications and publications connected through ISSNs to publishing channels. This kind of linkage is the bread-and-butter of the CRIS systems and usually comprehensively maintained. When it comes to the more subjective description of a domain object, such as keywords, themes, or subject headings, the issues related to data management and modeling become prominent with challenges such as flexibility of free text keywords as opposed to authoritative, but rigid classification systems. Many CRIS objects also already contain an extensive description of the content, just meant for human consumption, in the form of an abstract or similar summary text.With the help of automated data mining and annotation tools, these textual representations can be processed into structured data. This paper presents the processing pipelines implemented as part of the research.fi portal for automatic linking of different research inputs based on automatically extracted ontology concepts and discusses the implications of utilizing them as part of the research.fi platform. But more than simply discussing the annotation of research objects and the creation of word clusters for representation of the semantic content of research objects, we also discuss challenges related to maintaining the automatically produced metadata}
}
@article{GRIGORYEV2025102240,
title = {Reassessing the metrics of integration: Toward eliminating the blur between theory and statistics to clarify effect sizes, measurement, and causality in acculturation psychology},
journal = {International Journal of Intercultural Relations},
volume = {108},
pages = {102240},
year = {2025},
issn = {0147-1767},
doi = {https://doi.org/10.1016/j.ijintrel.2025.102240},
url = {https://www.sciencedirect.com/science/article/pii/S0147176725001038},
author = {Dmitry Grigoryev and Albina Gallyamova and Elizaveta Komyaginskaya},
keywords = {Biculturalism, Integration hypothesis, Acculturation strategies, Adaptation, Effect size, Psychological measurement, Causality, Epistemology, Acculturation psychology},
abstract = {The debate around the integration hypothesis in acculturation research frequently centers on the interpretation of effect sizes. While critics argue that these effects are too small and inconsistent to be meaningful, supporters maintain that they reflect statistically robust and theoretically coherent patterns. This controversy reveals a broader epistemological challenge in psychology: persistent ambiguity regarding what constitutes a ‘sufficient’ effect size, rooted in limited attention to the philosophical foundations of measurement and causality. In particular, this includes neglect of the ergodic fallacy—the mistaken assumption that group-level patterns apply directly to individuals—and confusion between statistical regularities and causal explanations. This paper addresses these concerns through three interrelated discussions. First, it re-evaluates the empirical status of the integration hypothesis in light of recent meta-analyses and the epistemic weight of small effects in complex systems. Second, it analyzes how effect sizes should be interpreted across different levels of analysis—individual, inter-individual, and group—and emphasizes the need to align interpretation with the appropriate unit of explanation. Third, it explores the philosophical foundations of psychological measurement, distinguishing between data patterns, theoretical constructs, and causal inferences. Rather than viewing effect sizes as direct indicators of psychological properties or causal strength, we conceptualize them as structured regularities shaped by research design, measurement models, and ontological assumptions. By clarifying these issues, this paper offers a framework for more coherent, theoretically informed interpretations of empirical findings in acculturation psychology and calls for a shift from simplistic magnitude judgments to context-sensitive evaluation of what effect sizes represent.}
}
@article{VERES2023102208,
title = {Self supervised learning and the poverty of the stimulus},
journal = {Data & Knowledge Engineering},
volume = {147},
pages = {102208},
year = {2023},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2023.102208},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2300068X},
author = {Csaba Veres and Jennifer Sampson},
keywords = {Classification, Learnability, Text mining, Machine Learning, NLP, Language},
abstract = {Diathesis alternations are the possible expressions of the arguments of verbs in different, systematically related subcategorization frames. Semantically similar verbs such as spill and spray can behave differently with respect to the alternations they can participate in. For example one can “spill/spray water on the plant”, but while one can “spray the plant with water”, it is odd to say “spill the plant with water”. “Spray” is a verb which can alternate between syntactic frames while “spill” is not alternating. How human speakers learn the difference between such verbs is not clearly understood, because the primary linguistic data (PLD) they receive does not appear sufficient to infer the knowledge required for adult competence. More generally the poverty of the stimulus (POS) hypothesis states that the PLD is not sufficient for a learner to infer full adult competence of language. That is, learning relies on prior constraints introduced by the language faculty. We tested state-of-the-art machine learning models trained by self supervision, and found some evidence that they could in fact learn the correct pattern of acceptability judgement in the locative alternation. However, we argued that this was partially a result of fine-tuning which introduced negative evidence into the learning data, which facilitated shortcut learning. Large language models (LLMs) cannot learn some linguistic facts from normal language data, but they can compensate to some extent by learning spurious correlated features when negative feedback is introduced during the training cycle.}
}
@article{LUH2025104287,
title = {Gamifying information security: Adversarial risk exploration for IT/OT infrastructures},
journal = {Computers & Security},
volume = {151},
pages = {104287},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104287},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824005935},
author = {Robert Luh and Sebastian Eresheim and Paul Tavolato and Thomas Petelin and Simon Gmeiner and Andreas Holzinger and Sebastian Schrittwieser},
keywords = {Hacking, Security game, Model, Gamification},
abstract = {Today’s interconnected IT and OT infrastructure faces an array of cyber threats from diverse actors with varying motivations and capabilities. The increasing complexity of exposed systems, coupled with adversaries’ sophisticated technical arsenals, poses significant challenges for organizations seeking to defend against these attacks. Understanding the relationship between specific attack techniques and effective technical, organizational and human-centric mitigation measures remains elusive, as does grasping the underlying principles of information security and how they may be applied to cyber defense. In response to these challenges, we propose a gamified metamodel that combines well-established frameworks, including MITRE ATT&CK, D3FEND, CAPEC, and the NIST SP 800-53 security standard. The programmatic implementation of the model, “PenQuest”, combines elements of game theory with cybersecurity concepts to enhance risk assessment and training for IT practitioners and security engineers. In PenQuest, participants engage in a digital battle — attackers attempt to compromise an abstracted IT infrastructure, while defenders work to prevent or mitigate the threat. Bot opponents and the technical foundation for reinforcement learning enable future automated strategy inference. This paper provides an in-depth exploration of the metamodel, the game’s components and features built to translate cybersecurity principles into strategy game rules, and the technical implementation of a mature, ready-to-use education and risk exploration solution. Future work will focus on further improving the attack likelihood and detection chance algorithms for seamless risk assessment.}
}
@incollection{SARWAR2021354,
title = {CellML Model Discovery with the Physiome Model Repository},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {354-361},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11681-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383116812},
author = {Dewan M. Sarwar and David P. Nickerson},
keywords = {CellML, Protein ontology, Physiome model repository, Resource description framework, SBML, Virtual physiological human},
abstract = {In this article we present a semantics-based model discovery tool that will enable scientists to discover relevant CellML models to utilize in their research. We describe relevant tools and technologies as well as the process of annotating models to include the biological semantics needed for this approach to work. In this example, we make use of the Physiome Model Repository to store the CellML models and the semantic annotations in a way that our tool is able to leverage in assisting in model discovery tasks.}
}
@article{SHUAIB2025200569,
title = {Formal concept views for explainable boosting: A lattice-theoretic framework for Extreme Gradient Boosting and Gradient Boosting Models},
journal = {Intelligent Systems with Applications},
volume = {27},
pages = {200569},
year = {2025},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2025.200569},
url = {https://www.sciencedirect.com/science/article/pii/S266730532500095X},
author = {Sherif Eneye Shuaib and Pakwan Riyapan and Jirapond Muangprathub},
keywords = {Concept lattices, Extreme Gradient Boosting, Formal concept analysis, Gradient Boosting, Interpretability, Tree-based models},
abstract = {Tree-based ensemble methods, such as Extreme Gradient Boosting (XGBoost) and Gradient Boosting models (GBM), are widely used for supervised learning due to their strong predictive capabilities. However, their complex architectures often hinder interpretability. This paper extends a lattice-theoretic framework originally developed for Random Forests to boosting algorithms, enabling a structured analysis of their internal logic via formal concept analysis (FCA). We formally adapt four conceptual views: leaf, tree, tree predicate, and interordinal predicate to account for the sequential learning and optimization processes unique to boosting. Using the binary-class version of the car evaluation dataset from the OpenML CC18 benchmark suite, we conduct a systematic parameter study to examine how hyperparameters, such as tree depth and the number of trees, affect both model performance and conceptual complexity. Random Forest results from prior literature are used as a comparative baseline. The results show that XGBoost yields the highest test accuracy, while GBM demonstrates greater stability in generalization error. Conceptually, boosting methods generate more compact and interpretable leaf views but preserve rich structural information in higher-level views. In contrast, Random Forests tend to produce denser and more redundant concept lattices. These trade-offs highlight how boosting methods, when interpreted through FCA, can strike a balance between performance and transparency. Overall, this work contributes to explainable AI by demonstrating how lattice-based conceptual views can be systematically extended to complex boosting models, offering interpretable insights without sacrificing predictive power.}
}
@article{BLOKHUIS2024107193,
title = {Transitioning towards more plant-based diets: sharing expert knowledge through a system lens},
journal = {Appetite},
volume = {195},
pages = {107193},
year = {2024},
issn = {0195-6663},
doi = {https://doi.org/10.1016/j.appet.2023.107193},
url = {https://www.sciencedirect.com/science/article/pii/S0195666323026557},
author = {Christa Blokhuis and Gert Jan Hofstede and Marga Ocké and Emely {de Vet}},
keywords = {Protein transition, Consumption, Food environment, Systems thinking, Causal loop diagram},
abstract = {Transitioning towards more plant-based protein diets is essential for public and planetary health. Current research about consumption practices of protein sources provides limited insight in the multidisciplinary nature and interconnectivity of the food environment. This study aimed to collect mental models of review authors by synthesizing both their implicit and explicit system views into an overarching system view. Published reviews were used to select participants and identify variables that explain the protein transition in relation to the food environment. To overcome differences in disciplines and scale levels (e.g. individual, interpersonal, environmental), variables were organized according to the Determinants of Nutrition and Eating Framework. Eight review authors shared their mental models in an interview. Participants were asked to construct a causal loop diagram (CLD), a tool proven valuable in making one's ontology explicit to others. Implicit system views in narrative were converted into CLDs using a coding framework. The overarching system view suggests that a multitude of feedback loops sustain current consumption patterns of protein sources, for example by reinforcement through habit, availability and peer support. Several aspects require further research, such as variable relationships that were subject to disagreement and the lack of reciprocity between the physical and social elements of the food environment. In addition, knowledge gaps were exposed, including long-term behaviour and interaction of multiple variables. As a boundary object, the overarching system view can facilitate the direction of future research. The findings underscore the interconnected nature of many disparate elements within the food environment, stressing the need for holistic methods like systems thinking. These are essential in developing a systemic understanding and facilitating the transition towards more plant-based diets.}
}
@article{AN2023102108,
title = {Sandpile-simulation-based graph data model for MVD generative design of shield tunnel lining using information entropy},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102108},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102108},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002367},
author = {Yi An and Xuhui Lin and Haijiang Li and Yitong Wang},
abstract = {BIM standard development is central to the performance and behavior of BIM model application across transmission, visualization, and information management perspectives. Tremendous effort has been made to ease the implementation of IFC data model in practice. Yet, the complexity of IFC data model hurdles the implementation of the import and export functionality by software vendors. To overcome this, buildingSMART introduced the concept of Model View Definitions to define which parts of an IFC data model need to be implemented for a specific data exchange scenario. With such, the certification of compatibility for software products with the IFC standard is formed. The Model View Definition is use case orientated to determine whether the specific information should be included in an IFC partial model. With the creation of ad-hoc, project-specific Exchange Requirements increasing, associated MVD development requires much more work to incorporate standard development. To resolve this issue, this paper attempts to exploit the potential of information entropy which has proven itself extremely crucial in many other industries in terms of information management, and then integrates it with sandpile simulation to propose a Top-down hierarchy to structure as well as interpret IFC partial model via Model View Definition. The proposed information entropy shifted MVD development approach would manage to unify the MVD development process that enables the reduction on confusion for various end users, specific organization, or project needs. Moreover, to better translate the BIM standard topology into sandpile simulations, a new notion system is proposed. Sandpile simulations are further implemented to prove their applicability, during the simulation, self-organized criticality is identified, and the existence of chaos is observed.}
}
@article{COENEN2025111949,
title = {A perspective on the use of personal data stores in data spaces to support multiparty data access as required by the data Act},
journal = {Data in Brief},
volume = {62},
pages = {111949},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2025.111949},
url = {https://www.sciencedirect.com/science/article/pii/S2352340925006730},
author = {Tanguy Coenen and Michiel Fierens and Beatriz Esteves and Esther {De Loof}},
keywords = {Data Act, Personal data store, Data space},
abstract = {This perspective article explores the requirements arising from the EU’s Data Act for multiparty data access in connected products, a key element of the EU’s Data Strategy. We categorize these requirements into six dimensions: Data accessibility for users, Information transparency for users, User rights regarding data, Fair and transparent data sharing, Interoperability, and Protection of legitimate interests. For each category, we provide detailed explanations and propose a conceptual model to address them. Central to this model are technological concepts such as identity management, consent management, data access policies, data usage policies, data catalogues, confidential compute, and linked data. We analyze how these technologies can support the Data Act’s requirements and integrate them into a cohesive architecture. To ground these insights, we present an “agentic home” use case, illustrating the deployment of personal data stores in edge environments to enable generative AI applications. This application exemplifies the transformative potential of combining personal data in data spaces with generative AI, showcasing a pathway for the EU to maintain a competitive position in AI while upholding ethical norms and values. We argue that by leveraging the data space paradigm in relation to personal data and within the provisions of the Data Act, the EU can capitalize on generative AI as the “killer app” for data spaces, driving innovation and ethical data practices in the digital economy.}
}
@article{VANDONGEN2020112,
title = {Emergence and correspondence for string theory black holes},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {69},
pages = {112-127},
year = {2020},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2019.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1355219819300565},
author = {Jeroen {van Dongen} and Sebastian {De Haro} and Manus Visser and Jeremy Butterfield},
abstract = {This is one of a pair of papers that give a historical-cum-philosophical analysis of the endeavour to understand black hole entropy as a statistical mechanical entropy obtained by counting string-theoretic microstates. Both papers focus on Andrew Strominger and Cumrun Vafa's ground-breaking 1996 calculation, which analysed the black hole in terms of D-branes. The first paper gives a conceptual analysis of the Strominger-Vafa argument, and of several research efforts that it engendered. In this paper, we assess whether the black hole should be considered as emergent from the d-brane system, particularly in light of the role that duality plays in the argument. We further identify uses of the quantum-to-classical correspondence principle in string theory discussions of black holes, and compare these to the heuristics of earlier efforts in theory construction, in particular those of the old quantum theory.}
}
@article{ZENG2021104894,
title = {Systematic analysis of the mechanism of Xiaochaihu decoction in hepatitis B treatment via network pharmacology and molecular docking},
journal = {Computers in Biology and Medicine},
volume = {138},
pages = {104894},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104894},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521006880},
author = {Ya Zeng and Shen Xiao and Luna Yang and Kai Ma and Hanxiao Shang and Yinli Gao and Yuan Wang and Fei Zhai and Rongwu Xiang},
keywords = {Xiaochaihu decoction, Hepatitis B, Network pharmacology, Molecular docking},
abstract = {Hepatitis B (HB) is a globally prevalent infectious disease caused by the HB virus. Xiaochaihu decoction (XCHD) is a classic herbal formula with a long history of clinical application in treating HB. Although the anti-HB activity of XCHD has been reported, systematic research on the exact mechanism of action is lacking. Here, a network pharmacology-based approach was used to predict the active components, important targets, and potential mechanism of XCHD in HB treatment. Investigation included drug-likeness evaluation; absorption, distribution, metabolism, and elimination (ADME) screening; protein-protein interaction (PPI) network construction and cluster analysis; Gene Ontology (GO) analysis; and Kyoto Encyclopedia of Genes and Genomes (KEGG) annotation. Molecular docking was adopted to investigate the interaction between important target proteins and active components. Eighty-seven active components of XCHD and 155 anti-HB targets were selected for further analysis. The GO enrichment and similarity analysis results indicated that XCHD might perform similar or the same GO functions. Glycyrrhizae Radix (GR), one of the seven XCHD herbs, likely exerts some unique GO functions such as the regulation of interleukin-12 production, positive regulation of interleukin-1 beta secretion, and regulation of the I-kappaB/NF-kappaB complex. The PPI network and KEGG pathway analysis results showed that XCHD affects HB mainly through modulating pathways related to viral infection, immunity, cancer, signal transduction, and metabolism. Additionally, molecular docking verified that the active compounds (quercetin, chrysin, and capsaicin) could bind with the key targets. This work systematically explored the anti-HB mechanism of XCHD and provides a novel perspective for future pharmacological research.}
}
@article{HENDAWI2022,
title = {A Smart Mobile App to Simplify Medical Documents and Improve Health Literacy: System Design and Feasibility Validation},
journal = {JMIR Formative Research},
volume = {6},
number = {4},
year = {2022},
issn = {2561-326X},
doi = {https://doi.org/10.2196/35069},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X22003936},
author = {Rasha Hendawi and Shadi Alian and Juan Li},
keywords = {health literacy, knowledge graph, natural language processing, machine learning, medical entity recognition},
abstract = {Background
People with low health literacy experience more challenges in understanding instructions given by their health providers, following prescriptions, and understanding their health care system sufficiently to obtain the maximum benefits. People with insufficient health literacy have high risk of making medical mistakes, more chances of experiencing adverse drug effects, and inferior control of chronic diseases.
Objective
This study aims to design, develop, and evaluate a mobile health app, MediReader, to help individuals better understand complex medical materials and improve their health literacy.
Methods
MediReader is designed and implemented through several steps, which are as follows: measure and understand an individual’s health literacy level; identify medical terminologies that the individual may not understand based on their health literacy; annotate and interpret the identified medical terminologies tailored to the individual’s reading skill levels, with meanings defined in the appropriate external knowledge sources; evaluate MediReader using task-based user study and satisfaction surveys.
Results
On the basis of the comparison with a control group, user study results demonstrate that MediReader can improve users’ understanding of medical documents. This improvement is particularly significant for users with low health literacy levels. The satisfaction survey showed that users are satisfied with the tool in general.
Conclusions
MediReader provides an easy-to-use interface for users to read and understand medical documents. It can effectively identify medical terms that a user may not understand, and then, annotate and interpret them with appropriate meanings using languages that the user can understand. Experimental results demonstrate the feasibility of using this tool to improve an individual’s understanding of medical materials.}
}
@article{LIU2024320,
title = {A multi-hierarchical aggregation-based graph convolutional network for industrial knowledge graph embedding towards cognitive intelligent manufacturing},
journal = {Journal of Manufacturing Systems},
volume = {76},
pages = {320-332},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001766},
author = {Bufan Liu and Chun-Hsien Chen and Zuoxu Wang},
keywords = {Multi-hierarchical aggregation, Graph convolutional network, Knowledge graph, Cognitive intelligent manufacturing, Link prediction},
abstract = {The rapid development and widespread applications of cognitive computing technologies have led to a paradigm shift towards cognitive intelligent development in manufacturing, where knowledge plays an increasingly important role in enabling higher levels of cognition. Knowledge graph (KG) has emerged as one of the essential tools in cognitive intelligent manufacturing and its completion would significantly impact the quality of knowledge. To facilitate effective knowledge management, KG embedding has proven to be an effective approach for KG completion. However, existing models have deficiencies in achieving relation-specific transformations, differentiating the neighbor nodes, and exploiting the intermediate information generated during the KG embedding learning process, which is prone to limit model performance and hinder successful applications. To address these limitations, this paper proposes a new multi-hierarchical aggregation-based graph convolutional network (GCN), consisting of relation-aware, entity-aware, and across-block aggregation. A parallel relation and entity-aware aggregation (PREA) block is established to simultaneously perform relation-specific transformations and entity-differentiated learning. Additionally, an across-block aggregation is constructed to efficiently integrate extracted information from the intermediate stacked block. To demonstrate the effectiveness and superiority of the proposed approach, 3D printing KG is constructed, which is a representative knowledge-intensive industry linking to a variety of aspects like raw materials, adhesion, usages, etc. Extensive experiments are conducted based on the link prediction task. Illustrative examples are provided to reveal the practical implementation of the proposed method, along with technical details and insightful opinions, exhibiting its promising applications.}
}
@article{CENGEL2023e13603,
title = {Eighteen distinctive characteristics of life},
journal = {Heliyon},
volume = {9},
number = {3},
pages = {e13603},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e13603},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023008101},
author = {Yunus A. Çengel},
keywords = {Life, Nature of life, Characteristics of life, Features of life, Traits of life, Ontology of life, Agency of life, Animate beings},
abstract = {A practical approach in the inquiry of life is to contrast living beings with nonliving ones from different perspectives and extract the distinctive features of living beings. We can identify features and mechanisms that truthfully account for the differences between living and nonliving beings by making rigorous logic-based inferences. The set of these differences constitutes the traits or characteristics of life. When the living beings are carefully examined, the apparent characteristics of life are ascertained to be existence, subjectivity, agency, purposiveness and mission orientation, primacy and supremacy, naturality, field phenomenon, locality, transience, transcendence, simplicity, unicity, initiation, information processing, traits, code of conduct, hierarchy and nesting, and the aptitude to vanish. Each feature is described, justified, and explained in detail in this observation-based philosophical article. Among them, an agency with purpose, knowledge, and power is the key feature of life without which the behavior of living beings cannot be explained. These eighteen characteristics constitute a reasonably comprehensive set of features to distinguish living beings from nonliving ones. However, the enigma of life remains.}
}
@article{LUKERSMITH202541,
title = {Determining the process components of impact assessment in health and social program implementation: A scoping review of theories, models and frameworks},
journal = {Public Health},
volume = {240},
pages = {41-47},
year = {2025},
issn = {0033-3506},
doi = {https://doi.org/10.1016/j.puhe.2024.12.056},
url = {https://www.sciencedirect.com/science/article/pii/S0033350625000174},
author = {S. Lukersmith and C. Woods and H. Sarma and C. {de Miquel} and L. Salvador-Carulla},
keywords = {Framework, Impact analysis, Implementation, Process, Throughputs, Impact assessment},
abstract = {Objectives
Health and social service research impact analysis play a pivotal role in demonstrating research value. Impact analysis of programs, interventions, or policies in real-world settings is complex. There are many implementation evaluation theories, models, and frameworks (TMF) and researchers find choosing one challenging. Our objective was to systematically scope TMFs, review and chart key components of the process of implementation impact analysis to identify gaps.
Study design
A scoping review was undertaken and reported using PRISMA-ScR guidelines.
Methods
Systematic literature searches were conducted for impact analysis and impact assessment TMFs in MEDLINE, SCOPUS databases, hand searches, and expert directed search (2010–2024). Peer-reviewed articles were eligible for inclusion if they described an implementation evaluation TMF in English and used in the real world. Data extracted by the study team was charted in an Excel spreadsheet.
Results
The review identified 71 relevant papers which included a theory (n = 6), model (n = 14), or framework (n = 51). Most considered resources and/or results, whereas only 25 % considered implementation process components. Ten frameworks were deemed comprehensive and covered at least two phases of implementation and five components. Most frameworks had not developed or tested practical tools to facilitate use of the framework.
Conclusions
No frameworks were identified that incorporated all phases of implementation, nor key components of the process in each phase of implementation research. The findings highlight the need to identify key components and develop a taxonomy, glossary and tools to assess the process components of implementation in real world settings.}
}
@article{MANCEBO2021100558,
title = {FEETINGS: Framework for Energy Efficiency Testing to Improve Environmental Goal of the Software},
journal = {Sustainable Computing: Informatics and Systems},
volume = {30},
pages = {100558},
year = {2021},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100558},
url = {https://www.sciencedirect.com/science/article/pii/S2210537921000494},
author = {Javier Mancebo and Coral Calero and Felix Garcia and Mª Angeles Moraga and Ignacio {Garcia-Rodriguez de Guzman}},
keywords = {Software sustainability, Green software, Energy efficiency, Energy consumption},
abstract = {Software is a fundamental part of today's society. However, both users and software professionals need to be aware that its use impacts on the environment, due to the high energy consumption it entails. One of the main gaps to be faced is the difficulty of analyzing software energy consumption in the endeavor to know whether a particular software product is as much energetically efficient as possible, or at least more efficient than another, and to improve the environmental objectives of the software. For this reason, a Framework for Energy Efficiency Testing to Improve eNviromental Goals of the Software (FEETINGS) is presented in this paper. FEETINGS is composed of three main components: an ontology to provide precise definitions and harmonize the terminology related to software energy measurement; a process to guide researchers in carrying out the energy consumption measurements of the software, and a technological environment, which allows the capture, analysis and interpretation of software energy consumption data. This paper also presents an example of the application of the FEETINGS, which aims to raise awareness of the energy consumed by the software in activities that we perform daily, such as writing a tweet or a Facebook post. As a result, we have been able to verify that FEETINGS allows us to carry out an analysis and measurement of software energy consumption to provide users with good practices, as using an emoji or a picture rather than a GIF. © 2001 Elsevier Science. All rights reserved.}
}
@article{YU2025126356,
title = {A non-autoregressive Chinese-Braille translation approach with CTC loss optimization},
journal = {Expert Systems with Applications},
volume = {269},
pages = {126356},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126356},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424032238},
author = {Hailong Yu and Wei Su and Yi Yang and Lei liu and Yongna Yuan and Yingchun Xie and Tianyuan Huang},
keywords = {Chinese-Braille translation, Non-autoregressive translation, CTC loss},
abstract = {The rise of Neural Machine Translation (NMT) models opens doors for translating Chinese text into Braille, improving information access for visually impaired individuals. However, current NMT models, often based on encoder–decoder architectures, utilize sequential rather than parallel processing in the decoder. This autoregressive decoding hinders architectures like the Transformer from fully leveraging their training speed advantages during inference. While the Transformer excels in parallel training, its inference time complexity remains O(T2), where T represents sequence length. This bottleneck becomes particularly significant when translating Braille, known for its long character sequences. We propose a non-autoregressive Chinese-to-Braille translation model that solely employs the encoder architecture along with Connectionist Temporal Classification (CTC) loss to generate complete Braille sequences simultaneously. This approach significantly improves inference speed, achieving a substantial acceleration compared to autoregressive models during inference with a time complexity of O(1). Remarkably, alongside increased inference speed, translation accuracy also improves. By incorporating a pre-training technique, our method achieves a remarkable BLEU Score of 95.10% with a limited dataset of only 2k Chinese-Braille training pairs.}
}
@article{JOUXTEL201893,
title = {Rituals and routines},
journal = {Society and Business Review},
volume = {14},
number = {1},
pages = {93-111},
year = {2018},
issn = {1746-5680},
doi = {https://doi.org/10.1108/SBR-03-2018-0029},
url = {https://www.sciencedirect.com/science/article/pii/S1746568018000263},
author = {Pascal Jouxtel},
keywords = {Change},
abstract = {Purpose
The terms rituals and routines are often conflated in everyday speech about teams, which betrays a common ontology. Yet these concepts have long been researched in two segregated currents of thought: one stemming from sociology and anthropology, focused on the quality of togetherness and the other from evolutionary economics, focused on market performance. The common ontology is nevertheless present in the processual nature of rituals and routines, the underlying shared reference to the “structure-action-artifact” triad and the statement that both are sources of change as well as stability. This paper aims to assess the pertinence of a joint approach.
Design/methodology/approach
The paper presents a historical and contrasted view on the two concepts. A comprehensive field observation of two teams in mid-term organizational change contexts, focused on collective “doings”, is reported. The tentative “binocular lens” was made of two chosen sets of variables, drawn from the theoretical fields of rituals and organizational routines.
Findings
The distinction between rituals and routines in people’s perception, though largely confused, nonetheless reveals the tension between variable and opposing demands for both change and stability from the team side and from the organization side. Their joint action is effective in enhancing the team’s feelings of confidence and control over its own performance and its future within the organization.
Research limitations/implications
This paper is supported by a comparison of only two teams, leaving room for further empirical research about the effects of endogenous rituality and localized routines on autonomy, efficiency and pride.
Originality/value
This paper offers a new theoretical joint view on the two concepts and explores an endogenous potential for organizational change feeding on emotional and symbolic aspects of team work.}
}
@article{RASTELLI2019102709,
title = {The imperfective paradox in a second language: A dynamic completion-entailment test},
journal = {Lingua},
volume = {231},
pages = {102709},
year = {2019},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2019.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0024384118306752},
author = {Stefano Rastelli},
keywords = {Imperfective Paradox, LA, Grammatical Aspect, Second Language Acquisition, Interval Semantics, LA Hypothesis},
abstract = {The imperfective paradox (IP) refers to the fact that the imperfective-progressive yields completion entailment with atelic predicates (e.g., Livia was pushing the chair → Livia pushed the chair=true) but not with telic predicates (Livia was peeling the tangerine → Livia peeled the tangerine=not necessarily true). The paper questions whether L2 learners too – like adult native speakers – are sensitive to the IP. Ninety-nine adult L2 Italian learners with different L1s and proficiency levels underwent a novel version of the completion judgment task based on event sub-intervals. In this task, learners did not assess if imperfective and perfective events in a video clip was completed but when. Analysis of reaction times and of interruptive-clicks showed that beginner and intermediate L2 learners – unlike native speakers – did not differentiate the patterns of completion between perfective and imperfective predicates and between telic and atelic predicates. It is possible that – at initial stages of acquisition – such aspectual oppositions are still underspecified. These results challenge the predictions of the Lexical Aspect Hypothesis.}
}
@article{WANG2021135,
title = {Differences between common endothelial cell models (primary human aortic endothelial cells and EA.hy926 cells) revealed through transcriptomics, bioinformatics, and functional analysis},
journal = {Current Research in Biotechnology},
volume = {3},
pages = {135-145},
year = {2021},
issn = {2590-2628},
doi = {https://doi.org/10.1016/j.crbiot.2021.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2590262821000150},
author = {Dongdong Wang and Zhu Chen and Andy {Wai Kan Yeung} and Atanas G. Atanasov},
keywords = {Endothelial cells, Bioinformatics analysis, Rap1 signaling pathway, Ras signaling pathway, HDL cellular association},
abstract = {Endothelial cells (ECs) are involved in various physiological process. Both primary human ECs and immortal endothelial cells are used in various studies. Available genomic or transcriptomic information for difference in ECs is deficient. Therefore, in this study we aim to reveal the difference between primary human aortic ECs (HAECs) and immortal EA.hy926 cells. We identified 529 differentially expressed genes (DEGs) between HAECs and EA.hy926 cells. Gene Ontology (GO), KEGG Pathway and GSEA enrichment analysis suggest that DEGs highly expressed in HAECs are distributed in Rap1 signaling pathway and Ras signaling pathway, which are contributing to the endothelial barrier function and endocytosis, among other functions. We also established long non-coding (lncRNA)-miRNA-mRNA ceRNA network, and further set up protein–protein interaction (PPI) network. High-density lipoprotein (HDL) cellular association experiments were verified that HAECs have stronger response to HDL cellular binding and endocytosis compared to EA.hy926 cells. This study identified DEGs between HAECs and EA.hy926 cells, and found enrichment of the Ras signaling pathway and Rap1 signaling pathway in HAECs, established ceRNA network and suggested that HAECs may have a stronger response to endothelial binding and endocytosis compared to EA.hy926 cells. This work provides a genomic basis to choose suitable EC model to reach respective research goals.}
}
@article{JOSYULA202319,
title = {Analysis of gene expression profile for identification of novel gene signatures during dengue infection},
journal = {Infectious Medicine},
volume = {2},
number = {1},
pages = {19-30},
year = {2023},
issn = {2772-431X},
doi = {https://doi.org/10.1016/j.imj.2023.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2772431X23000072},
author = {Jhansi Venkata Nagamani Josyula and Prathima Talari and Agiesh Kumar Balakrishna Pillai and Srinivasa Rao Mutheneni},
keywords = {Dengue fever, Severe dengue, Microarray, Gene expression, Data analysis, Gene signatures},
abstract = {Background
Dengue is a major arthropod-borne viral disease spreading rapidly across the globe. The absence of vaccines and inadequate vector control measures leads to further expansion of dengue in many regions globally. Hence, the identification of genes involved in the pathogenesis of dengue will help to understand the molecular basis of the disease and the genes responsible for the disease progression.
Methods
In the present study, a meta-analysis was carried out using dengue gene expression data obtained from Gene Expression Omnibus repository. The differentially expressed genes such as CCNB1 and CCNB2 (G2/mitotic-specific cyclin-B2 and B1) were upregulated in dengue fever to control (DF-CO) and severe dengue (dengue hemorrhagic fever [DHF]) to control (DHF-CO) were identified as key genes for controlling the major pathways (cell cycle, oocyte meiosis, p53 signaling pathway, cellular senescence and progesterone-mediated oocyte maturation). Similarly, interferon alpha-inducible (IFI27) genes, type-I and type-III interferon (IFN) signaling genes (STAT1 and STAT2), B cell activation and survival genes (TNFSF13B, TNFRSF17) and toll like receptor (TLR7) genes were differentially up activated during DF-CO and DHF-CO. Followed by, Cytoscape was used to identify the immune system process and topological analysis.
Results
The results showed that the top differentially expressed genes under the statistical significance p <0.001, which is majorly involved in Kyoto Encyclopedia of Genes and Genomes orthology K05868 and K21770 with gene names CCNB1 and CCNB2. In addition to this, the immune system profile showed up-regulation of IL12A, CXCR3, TNFSF13B, IFI27, TNFRSF17, STAT, STAT2, and TLR7 genes in DF-CO and DHF-CO act as immunological signatures for inducing the immune response towards dengue infection.
Conclusions
The current study could aid in understanding of molecular pathogenesis, genes and corresponding pathway upon dengue infection, and could facilitate for identification of novel drug targets and prognostic markers.}
}
@article{LORKIEWICZ20203163,
title = {Grounding of Modal Responses in Question Answering System Equipped with Hierarchical Categorisation},
journal = {Procedia Computer Science},
volume = {176},
pages = {3163-3172},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.172},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320743},
author = {Wojciech Lorkiewicz and Grzegorz Popek},
keywords = {question answering systems, language grounding, concept hierarchy, cognitive agent},
abstract = {Intelligent and dialogue systems highly utilise natural language interfaces. Such systems do not only process linguistic questions, but also formulate proper linguistic responses. Often neglected and important aspect of such responses lies in the ability to express and communicate systems internal beliefs. The proposed model fills in the current research gap in the grounding theory by enriching empirical experiences with hierarchical semantic structures in establishing agents internal belief stance. Such an extension significantly influences the process of grounding and allows for a dedicated computational mechanism of moving the conversations focus, which is incorporated into grounding mechanisms and fully presented.}
}
@article{MAKAROV2024108632,
title = {Good machine learning practices: Learnings from the modern pharmaceutical discovery enterprise},
journal = {Computers in Biology and Medicine},
volume = {177},
pages = {108632},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108632},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524007170},
author = {Vladimir Makarov and Christophe Chabbert and Elina Koletou and Fotis Psomopoulos and Natalja Kurbatova and Samuel Ramirez and Chas Nelson and Prashant Natarajan and Bikalpa Neupane},
keywords = {Artificial intelligence, Machine learning, Pharmaceutical, Drug discovery, Best practice, Life sciences},
abstract = {Machine Learning (ML) and Artificial Intelligence (AI) have become an integral part of the drug discovery and development value chain. Many teams in the pharmaceutical industry nevertheless report the challenges associated with the timely, cost effective and meaningful delivery of ML and AI powered solutions for their scientists. We sought to better understand what these challenges were and how to overcome them by performing an industry wide assessment of the practices in AI and Machine Learning. Here we report results of the systematic business analysis of the personas in the modern pharmaceutical discovery enterprise in relation to their work with the AI and ML technologies. We identify 23 common business problems that individuals in these roles face when they encounter AI and ML technologies at work, and describe best practices (Good Machine Learning Practices) that address these issues.}
}
@article{STRASS201955,
title = {EMIL: Extracting Meaning from Inconsistent Language: Towards argumentation using a controlled natural language interface},
journal = {International Journal of Approximate Reasoning},
volume = {112},
pages = {55-84},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X18300793},
author = {Hannes Strass and Adam Wyner and Martin Diller},
keywords = {Argumentation, Non-monotonic reasoning, Controlled natural language, Defeasible reasoning},
abstract = {There are well-developed formal and computational theories of argumentation to reason in the face of inconsistency, some with implementations; there are recent efforts to extract arguments from large textual corpora. Both developments are leading towards automated processing and reasoning with inconsistent, linguistically expressed knowledge in order to provide explanations and justifications in a form accessible to humans. However, there remains a gap between the knowledge-bases of computational theories of argumentation, which are generally coarse-grained and semi-structured (e.g. propositional logic), and inferences from knowledge-bases derived from natural language, which are fine-grained and highly structured (e.g. predicate logic). Arguments that occur in textual corpora are very rich, highly various, and incompletely understood. We identify several subproblems which must be addressed in order to bridge the gap, requiring the development of a computational foundation for argumentation coupled with natural language processing. For the computational foundation, we provide a direct semantics, a formal approach for argumentation, which is implemented and suitable to represent and reason with an associated natural language expression for defeasibility. It has attractive properties with respect to expressivity and complexity; we can reason by cases; we can structure higher level argumentation components such as cases and debates. With the implementation, we output experimental results which emphasise the importance of our efficient approach. To motivate our formal approach, we identify a range of issues found in other approaches. For the natural language processing, we adopt and adapt an existing controlled natural language (CNL) to interface with our computational theory of argumentation; the tool takes natural language input and automatically outputs expressions suitable for automated inference engines. A CNL, as a constrained fragment of natural language, helps to control variables, highlights key problems, and provides a framework to engineer solutions. The key adaptation incorporates the expression ‘it is usual that’, which is a plausibly ‘natural’ linguistic expression of defeasibility. This is an important, albeit incremental, step towards the incorporation of linguistic expressions of defeasibility; yet, by engineering such specific solutions, a range of other, relevant issues arise to be addressed. Overall, we can input arguments expressed in a controlled natural language, translate them to a formal knowledge base, represent the knowledge in a rule language, reason with the rules, generate argument extensions, and finally convert the arguments in the extensions into natural language. Our approach makes for fine-grained, highly structure, accessible, and linguistically represented argumentation evaluation. The overall novel contribution of the paper is an integrated, end-to-end argumentation system which bridges a gap between automated defeasible reasoning and a natural language interface. The component novel contributions are the computational theory of ‘direct semantics’, the motivation for our theory, the results with respect to the direct semantics, the implementation, the experimental results, the tie between the formalisation and the CNL, the adaptation of a CNL defeasibility, and an ‘engineering’ approach to fine-grained argument analysis.}
}
@article{LALITHA2020583,
title = {Personalised Self-Directed Learning Recommendation System},
journal = {Procedia Computer Science},
volume = {171},
pages = {583-592},
year = {2020},
note = {Third International Conference on Computing and Network Communications (CoCoNet'19)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.04.063},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920310309},
author = {T B Lalitha and P S Sreeja},
keywords = {e-Learning, PSDLR, Recommendation System, SDL, Self-Directed Learning},
abstract = {Modern educational systems have changed drastically bringing in knowledge anywhere as needed by the learner with the evolution of Internet. Availability of knowledge in public domain, capability of exchanging large amount of information and filtering relevant information quickly has enabled disruption to conventional educational system. Thus, future trends are looking towards E-Learning (Electronic Learning) and M-Learning (Mobile Learning) technologies over the Internet for their vast knowledge acquisition. In this paper, the work gives an elaborate context of learning strategies prevailing and emerging with the classification of e-learning Techniques. It majorly focuses on the features and variety of aspects with the e-learning and the choice of learning method involved and facilitate the adoption of new ways for personalized selection on learning resources for SDL (Self-Directed Learning) from the unstructured, large web-based environment. Thereby, proposes a Personalised Self-Directed Learning Recommendation System (PSDLR) based on the personal specifications of the SDL learner. The result offers insight into the perspectives and challenges of Self-Directed Learning based on cognitive and constructive characteristics which majorly incorporates web-based learning and gives path in finding appropriate solutions using machine learning techniques and ontology for the open problems in the respective fields with personalised recommendations and guidelines for future research.}
}
@article{JI2020102365,
title = {A deep neural network model for speakers coreference resolution in legal texts},
journal = {Information Processing & Management},
volume = {57},
number = {6},
pages = {102365},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102365},
url = {https://www.sciencedirect.com/science/article/pii/S0306457320308608},
author = {Donghong Ji and Jun Gao and Hao Fei and Chong Teng and Yafeng Ren},
keywords = {Legal text mining, Coreference resolution, Court record document, Neural networks, Attention mechanism},
abstract = {Coreference resolution is one of the fundamental tasks in natural language processing (NLP), and is of great significance to understand the semantics of texts. Meanwhile, resolving coreference is essential for many NLP downstream applications. Existing methods largely focus on pronouns, possessives and noun phrases resolution in the general domain, while little work is proposed for professional domains such as the legal field. Different from general texts, how to code legal texts and capture the relationship between entities in the text, and then resolve coreference is a challenging problem. For better understanding the legal text, and facilitating a series of downstream tasks in legal text mining, we propose a deep neural network model for coreference resolution in court record documents. Specifically, the pre-trained language model and bi-directional long short-term memory networks are first utilized to encode legal texts. Second, graph neural networks are applied to incorporate reference relations between entities. Finally, two distinct classifiers are used to score the candidate pairs. Results on the dataset show that our model achieves 87.53% F1 score on court record documents, outperforming neural baseline models by a large margin. Further analysis shows that the proposed method can effectively identify the reference relations between entities and model the entity dependencies.}
}
@article{ITO2020602,
title = {Process mining approach to formal business process modelling and verification: a case study},
journal = {Journal of Modelling in Management},
volume = {16},
number = {2},
pages = {602-622},
year = {2020},
issn = {1746-5664},
doi = {https://doi.org/10.1108/JM2-03-2020-0077},
url = {https://www.sciencedirect.com/science/article/pii/S1746566420000517},
author = {Sohei Ito and Dominik Vymětal and Roman Šperka},
keywords = {Modelling, Computing, Business process modelling, Formal method, Process mining, Timed automaton},
abstract = {Purpose
The need for assuring correctness of business processes in enterprises is widely recognised in terms of business process re-engineering and improvement. Formal methods are a promising approach to this issue. The challenge in business process verification is to create a formal model that is well-aligned to the reality. Process mining is a well-known technique to discover a model of a process based on facts. However, no studies exist that apply it to formal verification. This study aims to propose a methodology for formal business process verification by means of process mining, and attempts to clarify the challenges and necessary technologies in this approach using a case study.
Design/methodology/approach
A trading company simulation model is used as a case study. A workflow model is discovered from an event log produced by a simulation tool and manually complemented to a formal model. Correctness requirements of both domain-dependent and domain-independent types of the model are checked by means of model-checking.
Findings
For business process verification with both domain-dependent and domain-independent correctness requirements, more advanced process mining techniques that discover data-related aspects of processes are desirable. The choice of a formal modelling language is also crucial. It depends on the correctness requirements and the characteristics of the business process.
Originality/value
Formal verification of business processes starting with creating its formal model is quite new. Furthermore, domain-dependent and domain-independent correctness properties are considered in the same framework, which is also new. This study revealed necessary technologies for this approach with process mining.}
}
@incollection{MONTGOMERY2021347,
title = {2 - Ethical systems},
editor = {Erwin B. Montgomery},
booktitle = {The Ethics of Everyday Medicine},
publisher = {Academic Press},
pages = {347-387},
year = {2021},
isbn = {978-0-12-822829-6},
doi = {https://doi.org/10.1016/B978-0-12-822829-6.00027-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128228296000278},
author = {Erwin B. Montgomery},
keywords = {Ethical systems, Principlism, Anti-Principlism, Phenomenological ethics, Virtue ethics, Care ethics, Feminist ethics, Ethical ontology, Ethical epistemology},
abstract = {The fundamental problem of ethical reality (ontology) is the potentially infinite variety of ethical experiences. The challenging question is how to make sense of it (epistemology). The choice is to proceed from the premise that variety is diversity where each experience is unique and de novo. Alternatively, variety is variation where every experience is a combination of some economical set of fundamental principles. The results in ethics have been two different approaches characterized as Principlist and Anti-Principlist. Among the Anti-Principlist school are versions of phenomenological, narrative, value, care, and feminist ethics. Typically, these approaches have been held as separate and antagonistic. The result is that nether approach is totally satisfactory. Explored here is the notion that each approach is a different side of the epistemic coin and thus, cannot be separated just as the two sides of a coin cannot be separated. Better ethics may evolve from iteration involving both.}
}
@article{ZHAO2024100864,
title = {Energy insufficiency induced by high purine diet: Catalysts for renal impairment in hyperuricemia nephropathy rat model},
journal = {Current Research in Food Science},
volume = {9},
pages = {100864},
year = {2024},
issn = {2665-9271},
doi = {https://doi.org/10.1016/j.crfs.2024.100864},
url = {https://www.sciencedirect.com/science/article/pii/S2665927124001904},
author = {Zhenxiong Zhao and Zhikun Li and Yubin Xu and Shiqi Zhao and Qing Fan and Zhencang Zheng},
keywords = {Hyperuricemia nephropathy, High purine diet, Metabolomic, Proteomic, Fatty acid β-oxidation},
abstract = {A high purine diet emerges as a significant risk factor for hyperuricemia, and this diet may potentiate hyperuricemia nephropathy. Despite this, the mechanistic underpinnings of kidney damage precipitated by a high purine diet warrant further research. In the current investigation, a hyperuricemia nephropathy rat model was developed through induction via a high purine diet. Subsequently, metabolomic and proteomic analyses were employed to explore the metabolic characteristics of the kidney and shed light on the corresponding mechanistic pathway. Finally, fluorescence imaging and 18F-fluorodeoxyglucose positron emission tomography computed tomography (18F-FDG-PET/CT) were utilized to validate the overarching energy metabolism state. The results revealed extensive damage to the kidneys of hyperuricemia nephropathy rats following eight weeks of induction via a high purine diet. We used metabolomic to found that acyl carnitines and L-carnitine reduced in high purine diet group, indicated abnormal fatty acid metabolism. Irregularities were discerned in metabolites and enzymes associated with fatty acid β-oxidation, glycolysis, and oxidative phosphorylation within the kidneys of hyperuricemia nephropathy rats by proteomic and co-expression network analysis. The application of fluorescence imaging and 18F-FDG-PET/CT substantiated the inhibition of fatty acid β-oxidation and glycolysis within the kidneys of hyperuricemia nephropathy rats. On the contrary, a compensatory enhancement in the function of oxidative phosphorylation was observed. Given that the primary energy supply for renal function was derived from the metabolic pathway of fatty acids β-oxidation, any disruption within this pathway could contribute to a deficit in the energy provision to the kidneys. Such an energy insufficiency potentially laid the groundwork for eventual renal impairment. In addition, inhibition of the peroxisome proliferator-activated receptors signaling pathway was noted in the present findings, which could further exacerbate the impediment in the β-oxidation function. In conclusion, it was discerned that a deficiency in energy supply plays a critical role in the kidney injury in hyperuricemia nephropathy rats, thereby endorsing paying more attention to renal energy supply in the therapy of hyperuricemia nephropathy.}
}
@article{LADEIRA2021102089,
title = {RoBMEX: ROS-based modelling framework for end-users and experts},
journal = {Journal of Systems Architecture},
volume = {117},
pages = {102089},
year = {2021},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2021.102089},
url = {https://www.sciencedirect.com/science/article/pii/S1383762121000746},
author = {Matheus Ladeira and Yassine Ouhammou and Emmanuel Grolleau},
keywords = {ROS, Model-driven engineering, Autopilot, Domain specific language},
abstract = {Autonomous vehicles, such as drones, are gaining great popularity due to their usability and versatility. Nowadays, a significant number of them operate using open source software, such as Robot Operating System (ROS) and the de facto standard MAVLink communication protocol, as they are free and many of them are reusable enough that they can be deployed in various different vehicles. Although these technologies offer a wide variety of resources, using them requires a reasonable background of programming and system engineering. Often, this is not achievable by common drone end-users in the short-term, as they would need to acquire a considerably large amount of know-how before working on specific domains. However, a graphical Domain Specific Modelling Language (DSML) might provide a shortcut to design drone missions using already known concepts to the end-users (or, at least, ones easier to learn). Pursuing this shortcut, RoBMEX is presented as a top-down methodology based on a set of domain specific languages able to enhance the autonomy of ROS-based systems, by allowing the creation of missions graphically, and then generating automatically executable source codes conforming to the designed missions.}
}
@article{MACIAS2019167,
title = {Multilevel coupled model transformations for precise and reusable definition of model behaviour},
journal = {Journal of Logical and Algebraic Methods in Programming},
volume = {106},
pages = {167-195},
year = {2019},
issn = {2352-2208},
doi = {https://doi.org/10.1016/j.jlamp.2018.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352220817300585},
author = {Fernando Macías and Uwe Wolter and Adrian Rutle and Francisco Durán and Roberto Rodriguez-Echeverria},
keywords = {Model-driven engineering, Graph transformation, Multilevel modelling, Multilevel coupled model transformation, Behavioural modelling},
abstract = {The use of Domain-Specific Languages (DSLs) is a promising field for the development of tools tailored to specific problem spaces, effectively diminishing the complexity of hand-made software. With the goal of making models as precise, simple and reusable as possible, we augment DSLs with concepts from multilevel modelling, where the number of abstraction levels are not limited. This is particularly useful for DSL definitions with behaviour, whose concepts inherently belong to different levels of abstraction. Here, models can represent the state of the modelled system and evolve using model transformations. These transformations can benefit from a multilevel setting, becoming a precise and reusable definition of the semantics for behavioural modelling languages. We present in this paper the concept of Multilevel Coupled Model Transformations, together with examples, formal definitions and tools to assess their conceptual soundness and practical value.}
}
@article{LAI2023101488,
title = {The DEAD-box RNA helicase, DDX60, Suppresses immunotherapy and promotes malignant progression of pancreatic cancer},
journal = {Biochemistry and Biophysics Reports},
volume = {34},
pages = {101488},
year = {2023},
issn = {2405-5808},
doi = {https://doi.org/10.1016/j.bbrep.2023.101488},
url = {https://www.sciencedirect.com/science/article/pii/S2405580823000699},
author = {Tiantian Lai and Xiaowen Su and Enhong Chen and Yue Tao and Shuo Zhang and Leisheng Wang and Yong Mao and Hao Hu},
keywords = {DDX60, Pancreatic cancer, Prognosis, immune response, Immunotherapy},
abstract = {Excessive proliferation, invasion, metastasis, and immune resistance in pancreatic cancer (PC) makes it one of the most lethal malignant tumors. Recently, DDX60 was found to be involved in the development of various tumors and in immunotherapy. Therefore, we aimed to investigate whether DDX60 is a new factor involved in PC immunotherapy. The DDX60 mRNA was screened using transcriptome sequencing (RNA-seq). The Cox and survival analysis of DDX60 was performed using the Gene Expression Omnibus (GEO) and The Cancer Genome Atlas (TCGA) databases. In addition, clinical and immune infiltration data in the databases were analyzed and plotted using the R language. Clinical samples and in vitro experiments were used to determine the molecular evolution of DDX60 during PC progression. We found that DDX60 was upregulated in PC tissues (P value = 0.0083) and was associated with poor prognosis and short survival time of patients with PC. Results of Gene Ontology, Kyoto Encyclopedia of Genes and Genomes, and gene set variation analyses showed that viral defense, tumor, and immune-related pathways were significantly enriched in samples with high DDX60 expression. The Pearson correlation test demonstrated that DDX60 expression correlated strongly with immune checkpoint and immune system-related metagene clusters. Our results indicated that DDX60 promoted cell proliferation, migration, and invasion and was related to poor prognosis and immune resistance. Therefore, DDX60 may be a promising novel target for PC immunotherapy.}
}
@article{GE20241117,
title = {Simulated Microgravity can Promote the Apoptosis and Change Inflammatory State of Kupffer Cells},
journal = {Biomedical and Environmental Sciences},
volume = {37},
number = {10},
pages = {1117-1127},
year = {2024},
issn = {0895-3988},
doi = {https://doi.org/10.3967/bes2024.141},
url = {https://www.sciencedirect.com/science/article/pii/S0895398824001557},
author = {Jun Ge and Fei Liu and Hongyun Nie and Yuan Yue and Kaige Liu and Haiguan Lin and Hao Li and Tao Zhang and Hongfeng Yan and Bingxin Xu and Hongwei Sun and Jianwu Yang and Shaoyan Si and Jinlian Zhou and Yan Cui},
keywords = {Microgravity, Apoptosis, Kupffer cell, Polarization},
abstract = {Objective
In this study, we analyzed the transcriptome sequences of Kupffer cells exposed to simulated microgravity for 3 d and conducted biological experiments to determine how microgravity initiates apoptosis in Kupffer cells.
Methods
Rotary cell culture system was used to construct a simulated microgravity model. GO and KEGG analyses were conducted using the DAVID database. GSEA was performed using the R language. The STRING database was used to conduct PPI analysis. qPCR was used to measure the IL1B, TNFA, CASP3, CASP9, and BCL2L11 mRNA expressions. Western Blotting was performed to detect the level of proteins CASP3 and CASP 9. Flow cytometry was used to detect apoptosis and mitochondrial membrane cells. Transmission electron microscopy was used to detect changes in the ultrastructure of Kupffer cells.
Results
Transcriptome Sequencing indicated that simulated microgravity affected apoptosis and the inflammatory state of Kupffer cells. Simulated microgravity improved the CASP3, CASP9, and BCL2L11 expressions in Kupffer cells. Annexin-V/ PI and JC-1 assays showed that simulated microgravity promoted apoptosis in Kupffer cells. Simulated microgravity causes M1 polarization in Kupffer cells.
Conclusion
Our study found that simulated microgravity facilitated the apoptosis of Kupffer cells through the mitochondrial pathway and activated Kupffer cells into M1 polarization, which can secrete TNFA to promote apoptosis.}
}
@article{FELDMANN2019105,
title = {Managing inter-model inconsistencies in model-based systems engineering: Application in automated production systems engineering},
journal = {Journal of Systems and Software},
volume = {153},
pages = {105-134},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.03.060},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300639},
author = {S. Feldmann and K. Kernschmidt and M. Wimmer and B. Vogel-Heuser},
keywords = {Automated production systems, Model-based systems engineering, Inconsistency management},
abstract = {To cope with the challenge of managing the complexity of automated production systems, model-based approaches are applied increasingly. However, due to the multitude of different disciplines involved in automated production systems engineering, e.g., mechanical, electrical, and software engineering, several modeling languages are used within a project to describe the system from different perspectives. To ensure that the resulting system models are not contradictory, the necessity to continuously diagnose and handle inconsistencies within and in between models arises. This article proposes a comprehensive approach that allows stakeholders to specify, diagnose, and handle inconsistencies in model-based systems engineering. In particular, to explicitly capture the dependencies and consistency rules that must hold between the disparate engineering models, a dedicated graphical modeling language is proposed. By means of this language, stakeholders can specify, diagnose, and handle inconsistencies in the accompanying inconsistency management framework. The approach is implemented based on the Eclipse Modeling Framework (EMF) and evaluated based on a demonstrator project as well as a small user experiment. First findings indicate that the approach is expressive enough to capture typical dependencies and consistency rules in the automated production system domain and that it requires less effort compared to manually developing inter-model inconsistency management solutions.}
}
@article{JAMY2025,
title = {Towards a trait-based framework for protist ecology and evolution},
journal = {Trends in Microbiology},
year = {2025},
issn = {0966-842X},
doi = {https://doi.org/10.1016/j.tim.2025.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0966842X25002513},
author = {Mahwash Jamy and Pierre Ramond and David Bass and Javier {del Campo} and Micah Dunthorn and Enrique Lara and Aditee Mitra and Daniel Vaulot and Luciana Santoferrara},
keywords = {protist biodiversity, ecosystem functioning, functional ecology, trait database},
abstract = {Protists comprise the vast majority of eukaryotic genetic and functional diversity. While they have traditionally been difficult to study due to their small size and varied phenotypes, environmental sequencing studies have revealed the stunning diversity and abundance of protists in all ecosystems. Protists are key primary and secondary producers across many biomes, with ecological specializations that range from mutualism to parasitism, complex predation behaviors, mixotrophy, detritivory, and saprotrophy. Current genomic and transcriptomic approaches provide valuable insights into protist diversity at the genetic level, but they fall short in capturing the morphological and behavioral traits critical for understanding the functional roles of protists in ecosystems. This knowledge gap hinders our ability to answer important questions about protist functional diversity, including how protist functional groups will respond to environmental change. In this opinion article, we advocate adopting a traits-based approach for studying protist diversity and developing a trait database for protists to support this goal. By integrating examples of recent work characterizing protist functional diversity using a range of approaches, we emphasize the opportunities that trait databases offer and propose strategies for moving towards a trait-based framework to guide future research in protist ecology and evolution.}
}
@article{FANTECHI2023111540,
title = {VIBE: Looking for Variability In amBiguous rEquirements},
journal = {Journal of Systems and Software},
volume = {195},
pages = {111540},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111540},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002163},
author = {Alessandro Fantechi and Stefania Gnesi and Laura Semini},
keywords = {Natural language requirements documents, Ambiguity, Natural language processing tools, Software product lines, Variability detection},
abstract = {Variability is a characteristic of a software project and describes the fact that a system can be configured in different ways, obtaining different products (variants) from a common code base, accordingly to the software product line paradigm. This paradigm can be conveniently applied in all phases of the software process, starting from the definition and analysis of the requirements. We observe that often requirements contain ambiguities which can reveal an unintentional and implicit source of variability, that has to be detected. To this end we define VIBE, a tool supported process to identify variability aspects in requirements documents. VIBE is defined on the basis of a study of the different sources of ambiguity in natural language requirements documents that are useful to recognize potential variability, and is characterized by the use of a NLP tool customized to detect variability indicators. The tool to be used in VIBE is selected from a number of ambiguity detection tools, after a comparison of their customization features. The validation of VIBE is conducted using real-world requirements documents.}
}
@article{BOSSENKO2024101839,
title = {TermX: The semantic interoperability, knowledge management and sharing platform},
journal = {SoftwareX},
volume = {27},
pages = {101839},
year = {2024},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2024.101839},
url = {https://www.sciencedirect.com/science/article/pii/S2352711024002103},
author = {Igor Bossenko and Gunnar Piho and Marina Ivanova and Peeter Ross},
keywords = {TermX, Terminology server, Fast Healthcare Interoperability Resources (FHIR), Model designer, FHIR Mapping Language (FML) editor},
abstract = {TermX is an open-source knowledge management and sharing platform, including a terminology server, a Wiki, a model designer, a transformation editor, and tools for authoring and publishing. The core development goals of TermX were to enhance the semantic interoperability of software and systems, particularly within the healthcare sector, by improving access to terminology, simplifying the design of data models, optimising the efficiency of data transformations across various data models, and developing implementation guides through an intuitive web interface. TermX aims to guarantee open and standardised access to published terminology, data models and schemas, ensuring semantic interoperability following the FHIR standard or other standards or agreements.}
}
@article{DEAN2025104403,
title = {Algebras of actions in an agent's representations of the world},
journal = {Artificial Intelligence},
volume = {348},
pages = {104403},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104403},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225001225},
author = {Alexander Dean and Eduardo Alonso and Esther Mondragón},
keywords = {Representation learning, Agents, Disentanglement, Symmetries, Algebra},
abstract = {Learning efficient representations allows robust processing of data, data that can then be generalised across different tasks and domains, and it is thus paramount in various areas of Artificial Intelligence, including computer vision, natural language processing and reinforcement learning, among others. Within the context of reinforcement learning, we propose in this paper a mathematical framework to learn representations by extracting the algebra of the transformations of worlds from the perspective of an agent. As a starting point, we use our framework to reproduce representations from the symmetry-based disentangled representation learning (SBDRL) formalism proposed by [1] and prove that, although useful, they are restricted to transformations that respond to the properties of algebraic groups. We then generalise two important results of SBDRL –the equivariance condition and the disentangling definition– from only working with group-based symmetry representations to working with representations capturing the transformation properties of worlds for any algebra, using examples common in reinforcement learning and generated by an algorithm that computes their corresponding Cayley tables. Finally, we combine our generalised equivariance condition and our generalised disentangling definition to show that disentangled sub-algebras can each have their own individual equivariance conditions, which can be treated independently, using category theory. In so doing, our framework offers a rich formal tool to represent different types of symmetry transformations in reinforcement learning, extending the scope of previous proposals and providing Artificial Intelligence developers with a sound foundation to implement efficient applications.}
}
@article{YVARS202185,
title = {A Model-based Synthesis approach to system design correct by construction under environmental impact requirements},
journal = {Procedia CIRP},
volume = {103},
pages = {85-90},
year = {2021},
note = {9th CIRP Global Web Conference – Sustainable, resilient, and agile manufacturing and service operations : Lessons from COVID-19},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121008544},
author = {Pierre-Alain Yvars and Laurent Zimmer},
keywords = {Model-based approach, System Synthesis, constraint programming, environmental impact, requirements},
abstract = {This paper presents an integrated approach for the preliminary design of complex systems and for the generation of correct by construction system architectures with respect to the requirements to be satisfied. These requirements can be functional and/or non-functional (safety, sustainability ...). The approach presented is complementary to the usual approaches based on system analysis, performance evaluation and optimization. It focuses on the simultaneous consideration of functional and non-functional requirements such as environmental impact. The originality of the approach consists in modeling the design problem rather than the system and then solving the problem in such a way as to obtain necessarily admissible solutions. The DEPS (Design Problem Specification) language is used for modeling the problem. DEPS models are generic and reusable. The DEPS Studio environment is used to edit, compile, debug and solve problems expressed in DEPS. It integrates a mixed constraint programming solver. A case-study of a design of a mechanical power transmission system under functional and environmental requirements illustrates the approach. The DEPS modeling, the problem solving and the results obtained are detailed.}
}
@article{JAMES2020143,
title = {Artificial Intelligence in the Genetic Diagnosis of Rare Disease},
journal = {Advances in Molecular Pathology},
volume = {3},
pages = {143-155},
year = {2020},
note = {Advances in Molecular Pathology},
issn = {2589-4080},
doi = {https://doi.org/10.1016/j.yamp.2020.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S2589408020300144},
author = {Kiely N. James and Sujal Phadke and Terence C. Wong and Shimul Chowdhury},
keywords = {Genomics, Precision medicine, Natural language processing, Artificial intelligence}
}
@article{QIAN2025104332,
title = {Enhancing clinical trial outcome prediction with artificial intelligence: a systematic review},
journal = {Drug Discovery Today},
volume = {30},
number = {4},
pages = {104332},
year = {2025},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2025.104332},
url = {https://www.sciencedirect.com/science/article/pii/S1359644625000455},
author = {Long Qian and Xin Lu and Parvez Haris and Jianyong Zhu and Shuo Li and Yingjie Yang},
keywords = {Artificial intelligence (AI), Clinical trials, Trial outcome prediction, Drug development},
abstract = {Clinical trials are pivotal in drug development yet fraught with uncertainties and resource-intensive demands. The application of AI models to forecast trial outcomes could mitigate failures and expedite the drug discovery process. This review discusses AI methodologies that impact clinical trial outcomes, focusing on clinical text embedding, trial multimodal learning, and prediction techniques, while addressing practical challenges and opportunities.}
}
@article{TANG2024,
title = {Chinese Clinical Named Entity Recognition With Segmentation Synonym Sentence Synthesis Mechanism: Algorithm Development and Validation},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/60334},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001716},
author = {Jian Tang and Zikun Huang and Hongzhen Xu and Hao Zhang and Hailing Huang and Minqiong Tang and Pengsheng Luo and Dong Qin},
keywords = {clinical named entity recognition, word embedding, Chinese electronic medical records, RoBERTa, entity recognition, segmentation, natural language processing, AI, artificial intelligence, dataset, dataset augmentation, algorithm, entity, EMR},
abstract = {Background
Clinical named entity recognition (CNER) is a fundamental task in natural language processing used to extract named entities from electronic medical record texts. In recent years, with the continuous development of machine learning, deep learning models have replaced traditional machine learning and template-based methods, becoming widely applied in the CNER field. However, due to the complexity of clinical texts, the diversity and large quantity of named entity types, and the unclear boundaries between different entities, existing advanced methods rely to some extent on annotated databases and the scale of embedded dictionaries.
Objective
This study aims to address the issues of data scarcity and labeling difficulties in CNER tasks by proposing a dataset augmentation algorithm based on proximity word calculation.
Methods
We propose a Segmentation Synonym Sentence Synthesis (SSSS) algorithm based on neighboring vocabulary, which leverages existing public knowledge without the need for manual expansion of specialized domain dictionaries. Through lexical segmentation, the algorithm replaces new synonymous vocabulary by recombining from vast natural language data, achieving nearby expansion expressions of the dataset. We applied the SSSS algorithm to the Robustly Optimized Bidirectional Encoder Representations from Transformers Pretraining Approach (RoBERTa) + conditional random field (CRF) and RoBERTa + Bidirectional Long Short-Term Memory (BiLSTM) + CRF models and evaluated our models (SSSS + RoBERTa + CRF; SSSS + RoBERTa + BiLSTM + CRF) on the China Conference on Knowledge Graph and Semantic Computing (CCKS) 2017 and 2019 datasets.
Results
Our experiments demonstrated that the models SSSS + RoBERTa + CRF and SSSS + RoBERTa + BiLSTM + CRF achieved F1-scores of 91.30% and 91.35% on the CCKS-2017 dataset, respectively. They also achieved F1-scores of 83.21% and 83.01% on the CCKS-2019 dataset, respectively.
Conclusions
The experimental results indicated that our proposed method successfully expanded the dataset and remarkably improved the performance of the model, effectively addressing the challenges of data acquisition, annotation difficulties, and insufficient model generalization performance.}
}
@article{WAN2025106077,
title = {An emotion-behavior perspective of understanding public and government responses to rainstorm disasters: A case study of Zhengzhou Rainstorm in China},
journal = {Cities},
volume = {164},
pages = {106077},
year = {2025},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2025.106077},
url = {https://www.sciencedirect.com/science/article/pii/S0264275125003774},
author = {Xin Wan and Xinyu Ding and Sijia Liu and Yan Zhang and Xinyi Luo and Jingfeng Yuan and Changzheng Zhang},
keywords = {Rainstorm disasters, Social media, Public emotions, Public behaviors, Government strategies, Evolution patterns},
abstract = {Understanding public emotional and behavioral response is critical for adaptive disaster management. Integrating natural language processing (NLP), econometric, and social psychological models, this study establishes an emotion-behavior framework to analyze multidimensional interactions between public and government responses. Using social media data from “7.20” Zhengzhou Rainstorm, we reveal distinct emotional drivers: social support offering (SSO) thrived on positive emotions, yet help seeking (HSK) correlated with fear, deviance (DEV) driven by anger, and avoidance & venting (A&V) sustained by multiple negative emotions. Public behavior patterns shifted from pre-disaster instrumental aid to fear-driven avoidance coupled with emotional aid arising during crises, eventually evolving into intensified post-disaster resource competition. Government strategies show asymmetric impacts on these behaviors. Most strategies yielded instantly positive effects on SSO, while all strategies responded passively to HSK and some had time-lag or limited effects in mitigating A&V and DEV. The findings advocate integrating psychosocial factors into emergency strategies, with emphases on prioritizing proactive community engagement to sustain social cohesion, embedding psychological support mechanisms, and enforcing transparent resource governance to redirect emotions like fear, anger, and sadness. This approach advances urban resilience by highlighting that adaptive climate defenses requires aligning policy interventions with community-driven collaboration and emotion-driven public behavioral dynamics.}
}
@article{RADICH20231567,
title = {Molecular response in newly diagnosed chronic-phase chronic myeloid leukemia: prediction modeling and pathway analysis},
journal = {Haematologica},
volume = {108},
number = {6},
pages = {1567-1578},
year = {2023},
issn = {1592-8721},
doi = {https://doi.org/10.3324/haematol.2022.281878},
url = {https://www.sciencedirect.com/science/article/pii/S1592872123003246},
author = {Jerald P. Radich and Matthew Wall and Susan Branford and Catarina D. Campbell and Shalini Chaturvedi and Daniel J. DeAngelo and Michael W. Deininger and Justin Guinney and Andreas Hochhaus and Timothy P. Hughes and Hagop M. Kantarjian and Richard A. Larson and Sai Li and Rodrigo Maegawa and Kaushal Mishra and Vanessa Obourn and Javier Pinilla-Ibarz and Das Purkayastha and Islam Sadek and Giuseppe Saglio and Alok Shrestha and Brian S. White and Brian J. Druker},
abstract = {Tyrosine kinase inhibitor therapy revolutionized chronic myeloid leukemia treatment and showed how targeted therapy and molecular monitoring could be used to substantially improve survival outcomes. We used chronic myeloid leukemia as a model to understand a critical question: why do some patients have an excellent response to therapy, while others have a poor response? We studied gene expression in whole blood samples from 112 patients from a large phase III randomized trial (clinicaltrials gov. Identifier: NCT00471497), dichotomizing cases into good responders (BCR::ABL1 ≤10% on the International Scale by 3 and 6 months and ≤0.1% by 12 months) and poor responders (failure to meet these criteria). Predictive models based on gene expression demonstrated the best performance (area under the curve =0.76, standard deviation =0.07). All of the top 20 pathways overexpressed in good responders involved immune regulation, a finding validated in an independent data set. This study emphasizes the importance of pretreatment adaptive immune response in treatment efficacy and suggests biological pathways that can be targeted to improve response.}
}
@article{MILOSEVIC2023100756,
title = {Comparison of biomedical relationship extraction methods and models for knowledge graph creation},
journal = {Journal of Web Semantics},
volume = {75},
pages = {100756},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100756},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000403},
author = {Nikola Milošević and Wolfgang Thielemann},
keywords = {Knowledge graphs, Information extraction, Machine learning, Natural language processing, Text mining, Text-to-text model, Linked data, Transformers, PubMedBERT, T5, SciFive},
abstract = {Biomedical research is growing at such an exponential pace that scientists, researchers, and practitioners are no more able to cope with the amount of published literature in the domain. The knowledge presented in the literature needs to be systematized in such a way that claims and hypotheses can be easily found, accessed, and validated. Knowledge graphs can provide such a framework for semantic knowledge representation from literature. However, in order to build a knowledge graph, it is necessary to extract knowledge as relationships between biomedical entities and normalize both entities and relationship types. In this paper, we present and compare a few rule-based and machine learning-based (Naive Bayes, Random Forests as examples of traditional machine learning methods and DistilBERT, PubMedBERT, T5, and SciFive-based models as examples of modern deep learning transformers) methods for scalable relationship extraction from biomedical literature, and for the integration into the knowledge graphs. We examine how resilient are these various methods to unbalanced and fairly small datasets. Our experiments show that transformer-based models handle well both small (due to pre-training on a large dataset) and unbalanced datasets. The best performing model was the PubMedBERT-based model fine-tuned on balanced data, with a reported F1-score of 0.92. The distilBERT-based model followed with an F1-score of 0.89, performing faster and with lower resource requirements. BERT-based models performed better than T5-based generative models.}
}
@article{CENTOBELLI2018107,
title = {Aligning enterprise knowledge and knowledge management systems to improve efficiency and effectiveness performance: A three-dimensional Fuzzy-based decision support system},
journal = {Expert Systems with Applications},
volume = {91},
pages = {107-126},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.08.032},
url = {https://www.sciencedirect.com/science/article/pii/S0957417417305699},
author = {Piera Centobelli and Roberto Cerchione and Emilio Esposito},
keywords = {Knowledge management (KM), Decision support system (DSS), Knowledge management systems (KMSs), 3D Fuzzy Logic, Small and medium enterprises (SMEs)},
abstract = {The purpose of this paper is to propose a three-dimensional fuzzy logic approach to evaluate the level of alignment between the knowledge an enterprise possesses and the knowledge management systems (KMSs) it adopts. The study also aims to propose the KMSs best suited to reducing misalignment and improving operational performance in terms of efficiency and effectiveness, analysing the level of alignment between an enterprise's knowledge and its KMSs from both the ontological and epistemological points of view. The authors have used the proposed methodology to develop a software-based Knowledge Management Decision Support System (KM-DSS), which was tested on a small and medium enterprise (SME) operating in the high-tech industry. The results highlight that the proposed DSS allows managers to evaluate knowledge management processes and identify which KMSs to adopt to improve alignment with the nature of the knowledge their enterprise possesses as well as to increase their level of efficiency and effectiveness.}
}
@article{MOHAMMADI2025101673,
title = {Cross-linking clinical practice guidelines for multimorbidity},
journal = {Informatics in Medicine Unlocked},
volume = {58},
pages = {101673},
year = {2025},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2025.101673},
url = {https://www.sciencedirect.com/science/article/pii/S2352914825000619},
author = {Majid Mohammadi and Annette ten Teije and Tim Christen and Janke {de Groot} and Marlies Verhoeff},
keywords = {Clinical practice guideline, Multimorbidity, Entity linking, Computer-interpretable guideline},
abstract = {Clinical practice guidelines (CPGs) play a pivotal role in elevating healthcare quality. However, their traditional single-disease focus falls short in addressing the complexities of multimorbidity, a condition increasingly prevalent in an aging population. This discrepancy necessitates that healthcare providers juggle multiple guidelines to formulate comprehensive care plans for such patients. Our paper introduces an innovative methodology designed to streamline this process by cross-linking different CPGs, thereby facilitating more efficient navigation across various guidelines. This approach is grounded in language-agnostic principles and leverages Semantic Web technologies to connect guideline terms to biomedical knowledge sources like SNOMED. Our methodology has undergone validation by medical practitioners and guideline developers, particularly within the context of Dutch CPGs. The results of these experiments underscore the effectiveness of our approach, demonstrating its potential to contribute to key issues associated with CPGs in multimorbidity scenarios, such as computer-interpretable clinical guidelines and the interaction of multiple CPGs. Furthermore, the outcomes of this method extend beyond immediate practical applications, offering valuable insights for the enhancement of guideline databases and medical knowledge bases like SNOMED. By bridging gaps between separate guidelines, our method represents a step forward in the integrated management of multimorbidity in CPGs.}
}
@article{ROSSINI2024,
title = {Towards computable taxonomic knowledge: Leveraging nanopublications for sharing new synonyms in the Madagascan genus Helictopleurus (Coleoptera, Scarabaeinae)},
journal = {Biodiversity Data Journal},
volume = {12},
year = {2024},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.12.e120304},
url = {https://www.sciencedirect.com/science/article/pii/S1314283624001696},
author = {Michele Rossini and Giulio Montanaro and Olivier Montreuil and Sergei Tarasov},
keywords = {dung beetles, taxonomy, nomenclature, machine-readable data, SPARQL, ontology, Madagascar, extinction},
abstract = {Background
Numerous taxonomic studies have focused on the dung beetle genus Helictopleurus d’Orbigny, 1915, endemic to Madagascar. However, this genus stilll needs a thorough revision. Semantic technologies, such as nanopublications, hold the potential to enhance taxonomy by transforming how data are published and analysed. This paper evaluates the effectiveness of nanopublications in establishing synonyms within the genus Helictopleurus.
New information
In this study, we identify four new synonyms within Helictopleurus: H. rudicollis (Fairmaire, 1898) = H. hypocrita Balthasar, 1941 syn. nov.; H. vadoni Lebis, 1960 = H. perpunctatus Balthasar, 1963 syn. nov.; H. halffteri Balthasar, 1964 = H. dorbignyi Montreuil, 2005 syn. nov.; H. clouei (Harold, 1869) = H. gibbicollis (Fairmaire, 1895) syn. nov. Helictopleurus may have a significantly larger number of synonyms than currently known, indicating potentially inaccurate estimates about its recent extinction. We also publish the newly-established synonyms as nanopublications, which are machine-readable data snippets accessible online. Additionally, we explore the utility of nanopublications in taxonomy and demonstrate their practical use with an example query for data extraction.}
}
@article{LEBLOND2021103623,
title = {Operative list of genes associated with autism and neurodevelopmental disorders based on database review},
journal = {Molecular and Cellular Neuroscience},
volume = {113},
pages = {103623},
year = {2021},
issn = {1044-7431},
doi = {https://doi.org/10.1016/j.mcn.2021.103623},
url = {https://www.sciencedirect.com/science/article/pii/S1044743121000361},
author = {Claire S. Leblond and Thuy-Linh Le and Simon Malesys and Freddy Cliquet and Anne-Claude Tabet and Richard Delorme and Thomas Rolland and Thomas Bourgeron},
keywords = {Autism, Neurodevelopmental disorders, Databases, High-confidence neurodevelopmental disorder genes, Genetic diagnostic, Fetal brain expression & synaptic function},
abstract = {The genetics of neurodevelopmental disorders (NDD) has made tremendous progress during the last few decades with the identification of more than 1,500 genes associated with conditions such as intellectual disability and autism. The functional roles of these genes are currently studied to uncover the biological mechanisms influencing the clinical outcome of the mutation carriers. To integrate the data, several databases and curated gene lists have been generated. Here, we provide an overview of the main databases focusing on the genetics of NDD, that are widely used by the medical and scientific communities, and extract a list of high confidence NDD genes (HC-NDD). This gene set can be used as a first filter for interpreting large scale omics dataset or for diagnostic purposes. Overall HC-NDD genes (N = 1,586) are expressed at very early stages of fetal brain development and enriched in several biological pathways such as chromosome organization, cell cycle, metabolism and synaptic function. Among those HC-NDD genes, 204 (12,9%) are listed in the synaptic gene ontology SynGO and are enriched in genes expressed after birth in the cerebellum and the cortex of the human brain. Finally, we point at several limitations regarding the relatively poor standardized information available, especially on the carriers of the mutations. Progress on the phenotypic characterization and genetic profiling of the carriers will be crucial to improve our knowledge on the biological mechanisms and on risk and protective factors for NDD.}
}
@article{GROSSETTI20192078,
title = {Interface and requirements analysis on the DEMO Heating and Current Drive system using Systems Engineering methodologies},
journal = {Fusion Engineering and Design},
volume = {146},
pages = {2078-2082},
year = {2019},
note = {SI:SOFT-30},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2019.03.107},
url = {https://www.sciencedirect.com/science/article/pii/S0920379619304405},
author = {Giovanni Grossetti and Christophe Baylard and Thomas Franke and Jürgen Gafert and Ian Jenkins and Mattia Siccinio and Dirk Strauß and Minh Quang Tran and Hartmut Zohm},
keywords = {Nuclear fusion, DEMO, Heating and Current Drive, Systems Engineering, MBSE},
abstract = {In this paper we present the methodology implemented for analyzing System Requirements and Interfaces of the Heating and Current Drive (HCD) system of the European Demonstration Fusion Power Reactor DEMO. The work consisted in updating the preliminary framework of the Model-Based Systems Engineering model of the HCD System Architecture. This is now containing an ontology, a set of 6 perspectives and a defined set of viewpoints for each Perspective, for refining the HCD System Architecture. The scope of the work is to manage the interdependencies of HCD system elements and their integration into DEMO, for a given set of system functions. On the one hand, this means to address the identification and definition of the interfaces occurring, both internally in the HCD system, and between the HCD system and neighboring systems. On the other hand, this implies studying the impact of requirements coming from the ongoing physics studies. The rationale is to provide the technical foreground for supporting the decision-making processes related to the HCD system which is planned to be carried out during the forthcoming Conceptual Design Phase. The results we show in this paper are part of the design and integration activities consisting of both systems engineering methodologies and design analysis, all aiming at ensuring consistency in the overall EU DEMO plant design. In this framework the DEMO Heating and Current Drive system has been selected as pilot project for the application of Systems Engineering methodologies.}
}
@article{VISWESWARAN2024104713,
title = {Fairness and inclusion methods for biomedical informatics research},
journal = {Journal of Biomedical Informatics},
volume = {158},
pages = {104713},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104713},
url = {https://www.sciencedirect.com/science/article/pii/S153204642400131X},
author = {Shyam Visweswaran and Yuan Luo and Mor Peleg}
}
@article{PASKALEVA2021103689,
title = {Leveraging integration facades for model-based tool interoperability},
journal = {Automation in Construction},
volume = {128},
pages = {103689},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103689},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521001400},
author = {Galina Paskaleva and Alexandra Mazak-Huemer and Manuel Wimmer and Thomas Bednar},
keywords = {MDE, Data exchange, Big Open BIM, Semantic integration, Pragmatic integration, Heterogeneity},
abstract = {Data exchange and management methods are of paramount importance in areas as complex as the Architecture, Engineering and Construction industries and Facility Management. For example, Big Open BIM requires seamless information flow among an arbitrary number of applications. The backbone of such information flow is a robust integration, whose tasks include overcoming technological as well as semantic and pragmatic gaps and conflicts both within and between data models. In this work, we introduce a method for integrating the pragmatics at design-time and the semantics of independent applications at run-time into so-called “integration facades”. We utilize Model-driven Engineering for the automatic discovery of functionalities and data models, and for finding a user-guided consensus. We present a case study involving the domains of architecture, building physics and structural engineering for evaluating our approach in object-oriented as well as data-oriented programming environments. The results produce, for each scenario, a single integration facade that acts as a single source of truth in the data exchange process.}
}
@article{NING2025118895,
title = {Development of a multi-indicator risk prediction model for cervical cancer associated with benzo[a]pyrene and nicotine exposure: A multi-omics study integrating toxicological analyses and molecular docking},
journal = {Ecotoxicology and Environmental Safety},
volume = {303},
pages = {118895},
year = {2025},
issn = {0147-6513},
doi = {https://doi.org/10.1016/j.ecoenv.2025.118895},
url = {https://www.sciencedirect.com/science/article/pii/S0147651325012400},
author = {Li Ning and Xiu Li and Yating Xu and Yihan Zhang and Yu Si and Hongting Zhao and Qingling Ren},
keywords = {Cervix, Benzo[]pyrene, Nicotine, Molecular docking, Prognostic target, Prognosis, Immune relevance},
abstract = {Background
Exposure to the tobacco-related compounds Benzo[a]pyrene and Nicotine has been associated with the development of several diseases. The aim of this study was to investigate the common genes associated with cervical cancer, construct a risk prediction model to reveal their biological functions, and evaluate the prognostic significance of the model to identify its potential value in the treatment of cervical cancer.
Methods
In this study, genes associated with Benzo[a]pyrene and Nicotine and cervical cancer-related genes were screened by multiple databases. Target genes were analysed using a multi-omics machine learning algorithm to construct a risk-prognostic model, and nine key target genes were identified. The risk prediction models were evaluated by univariate and multivariate Cox regression analyses, and model validation was performed using the TCGA and GSE44001 datasets. In addition, clinical relevance, biofunctional enrichment, immune infiltration, and drug sensitivity analyses were performed, and the binding affinities of the two compounds to the target genes were investigated by combining molecular docking and kinetic analyses, and the Mendelian randomisation method was applied to analyse the causal association between the target genes and cervical cancer.
Results
A total of 682 genes associated with the two compounds were screened by ChEMBL, STITCH and SwissTargetPrediction databases, while 1451 genes associated with cervical cancer were identified by using GeneCards and OMIM databases, among which 109 genes were associated with both the two compounds and cervical cancer. The degree of interaction between different genes was determined by protein interaction network analysis. Based on various machine learning algorithms, a risk prediction model associated with Benzo[a]pyrene and Nicotine exposure and cervical cancer was constructed, and the good prediction performance of the model was verified in TCGA and GSE44001 datasets. In addition, a column-line diagram associated with the risk prediction model was constructed to provide a clinical tool for predicting prognosis. Further analyses revealed significant differences in the enrichment of biological processes, immune-infiltrating cells and immunomodulatory factors between the high-risk and low-risk groups, and the risk prediction model was strongly correlated with drug susceptibility, showing significant associations especially in tipifarnib-P1, AZD3463, docetaxel and AT-7519. Molecular docking and molecular dynamics simulations revealed a strong binding affinity between Benzo[a]pyrene and SLAMF6. Furthermore, Mendelian randomisation analysis revealed a significant causal relationship between SLAMF6 and AIG1.
Conclusions
Risk prediction models based on multi-omics data and machine learning algorithms provide potential reference targets for prognosis prediction and personalised treatment of cervical cancer patients. The results of this study provide important insights into the understanding of the health risks of cervical cancer associated with Benzo[a]pyrene and Nicotine exposures and the development of preventive and therapeutic strategies for cervical cancer, which may contribute to the development of precision medicine for cervical cancer.}
}
@article{PARKER2019193,
title = {Language in pursuit of professional branding: The case of scientific costing},
journal = {The British Accounting Review},
volume = {51},
number = {2},
pages = {193-210},
year = {2019},
issn = {0890-8389},
doi = {https://doi.org/10.1016/j.bar.2018.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0890838918300635},
author = {Lee D. Parker and Trevor Boyns},
keywords = {Branding strategy, Professionalization, Scientific costing, Budgetary discourse, Hermeneutic analysis, Institute of Cost and Works Accountants},
abstract = {The period between the 1880s and 1930 witnessed the development of concepts and discourses associated with costing as a science. Against this background, and in the context of the professionalization campaign pursued by the newly established Institute of Cost and Works Accountants (formed in 1919), we employ insights from hermeneutic analysis to examine the ascendancy and subsequent demise of 'scientific costing' as a branding strategy. Building on the earlier work of Loft (1986, 1990), we place these developments within both the internal machinations of the Institute in its early years and the wider context of the business, professional and regulatory environment of the period. We find that the rise and fall from favour of 'scientific costing' was conditioned by a number of contextual factors, not least the changing environment of the early decades of the twentieth century surrounding the emergence of scientism, its links to the efficiency gospel, and a changing rhetoric which shifted towards a business budgeting discourse. These, together with difficulties in finding a common specification of 'scientific costing' limited its usefulness as an organisational branding strategy. Implications are drawn from our study for contemporary attempts to develop branding strategies by professional accounting bodies.}
}
@article{BHUIYAN2024e25191,
title = {System biology approaches to identify hub genes linked with ECM organization and inflammatory signaling pathways in schizophrenia pathogenesis},
journal = {Heliyon},
volume = {10},
number = {3},
pages = {e25191},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e25191},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024012222},
author = {Piplu Bhuiyan and Zhaochu Sun and Md Arif Khan and Md Arju Hossain and Md Habibur Rahman and Yanning Qian},
keywords = {Schizophrenia (SZ), Bioinformatics, System biology, Extracellular matrix organization (EMC), Inflammatory signaling pathway},
abstract = {Schizophrenia (SZ) is a chronic and devastating mental illness that affects around 20 million individuals worldwide. Cognitive deficits and structural and functional changes of the brain, abnormalities of brain ECM components, chronic neuroinflammation, and devastating clinical manifestation during SZ are likely etiological factors shown by affected individuals. However, the pathophysiological events associated with multiple regulatory pathways involved in the brain of this complex disorder are still unclear. This study aimed to develop a pipeline based on bioinformatics and systems biology approaches for identifying potential therapeutic targets involving possible biological mechanisms from SZ patients and healthy volunteers. About 420 overlapping differentially expressed genes (DEGs) from three RNA-seq datasets were identified. Gene ontology (GO), and pathways analysis showed several biological mechanisms enriched by the commonly shared DEGs, including extracellular matrix organization (ECM) organization, collagen fibril organization, integrin signaling pathway, inflammation mediated by chemokines and cytokines signaling pathway, and GABA-B receptor II and IL4 mediated signaling. Besides, 15 hub genes (FN1, COL1A1, COL3A1, COL1A2, COL5A1, COL2A1, COL6A2, COL6A3, MMP2, THBS1, DCN, LUM, HLA-A, HLA-C, and FBN1) were discovered by comprehensive analysis, which was mainly involved in the ECM organization and inflammatory signaling pathway. Furthermore, the miRNA target of the hub genes was analyzed with the random-forest-based approach software miRTarBase. In addition, the transcriptional factors and protein kinases regulating overlapping DEGs in SZ, namely, SUZ12, EZH2, TRIM28, TP53, EGR1, CSNK2A1, GSK3B, CDK1, and MAPK14, were also identified. The results point to a new understanding that the hub genes (fibronectin 1, collagen, matrix metalloproteinase-2, and lumican) in the ECM organization and inflammatory signaling pathways may be involved in the SZ occurrence and pathogenesis.}
}
@article{NORTHOFF2025173,
title = {Brain dynamics shape cognition–Spatiotemporal Neuroscience},
journal = {Physics of Life Reviews},
volume = {54},
pages = {173-201},
year = {2025},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2025.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S157106452500106X},
author = {Georg Northoff and Angelika Wolman and Jianfeng Zhang},
keywords = {Cognition, Neural dynamics, Intrinsic neural timescales, Variability, Background-foreground},
abstract = {Current neuroscience faces a divide between cognitive function and neural dynamics. Cognitive function is typically studied during task-related activity, while neural dynamics are a key feature of the brain’s spontaneous activity, as measured in the resting state. How are dynamics and cognition connected? Although neural dynamics themselves are well understood, their relationship to—and influence on—cognitive functions remain yet unclear. Addressing this gap is the goal of our paper. For that purpose, we first review recent findings on how dynamic features like neural variability and intrinsic neural timescales (INT) shape various cognitive functions. We then expand our view beyond task-specific foreground activity to the deeper background layers of the brain’s neural activity—its task-unspecific and spontaneous activity. This leads us to propose a Dynamic Layer Model of the Brain (DLB). Drawing on empirical and computational evidence, we demonstrate how neural variability, INT, and other dynamic features (such as scale-free dynamics) connect these three layers of neural activity. Next, we show how these three layers from spontaneous over task-unspecific to task-specific activity mediate four temporal mechanisms through which brain dynamics shape cognition: these range from temporal encoding and integration of input dynamics to temporal scaffolding and segmentation of cognitive output. We conclude that the brain’s neural dynamics operate in the background, shaping the cognitive functions and their contents in the neural foreground in a temporal-dynamic manner. This perspective is at the core of Spatiotemporal Neuroscience, which provides a wider framework than Cognitive Neuroscience by revealing how the brain’s intrinsic dynamics shape our cognition.}
}
@article{AFTAB2022100242,
title = {ImShot: An Open-Source Software for Probabilistic Identification of Proteins In Situ and Visualization of Proteomics Data},
journal = {Molecular & Cellular Proteomics},
volume = {21},
number = {6},
pages = {100242},
year = {2022},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2022.100242},
url = {https://www.sciencedirect.com/science/article/pii/S1535947622000500},
author = {Wasim Aftab and Shibojyoti Lahiri and Axel Imhof},
keywords = {imaging mass spectrometry, peptides, de-isotopoing, shotgun proteomics, data integration, peptide ranking, open source, software, desktop application, graphic user interface},
abstract = {Imaging mass spectrometry (IMS) has developed into a powerful tool allowing label-free detection of numerous biomolecules in situ. In contrast to shotgun proteomics, proteins/peptides can be detected directly from biological tissues and correlated to its morphology leading to a gain of crucial clinical information. However, direct identification of the detected molecules is currently challenging for MALDI–IMS, thereby compelling researchers to use complementary techniques and resource intensive experimental setups. Despite these strategies, sufficient information could not be extracted because of lack of an optimum data combination strategy/software. Here, we introduce a new open-source software ImShot that aims at identifying peptides obtained in MALDI–IMS. This is achieved by combining information from IMS and shotgun proteomics (LC–MS) measurements of serial sections of the same tissue. The software takes advantage of a two-group comparison to determine the search space of IMS masses after deisotoping the corresponding spectra. Ambiguity in annotations of IMS peptides is eliminated by introduction of a novel scoring system that identifies the most likely parent protein of a detected peptide in the corresponding IMS dataset. Thanks to its modular structure, the software can also handle LC–MS data separately and display interactive enrichment plots and enriched Gene Ontology terms or cellular pathways. The software has been built as a desktop application with a conveniently designed graphic user interface to provide users with a seamless experience in data analysis. ImShot can run on all the three major desktop operating systems and is freely available under Massachusetts Institute of Technology license.}
}
@article{RAHHAL2024124101,
title = {Data science for job market analysis: A survey on applications and techniques},
journal = {Expert Systems with Applications},
volume = {251},
pages = {124101},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124101},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424009679},
author = {Ibrahim Rahhal and Ismail Kassou and Mounir Ghogho},
keywords = {Labor market analytics, Job market needs, Data science, Job title classification, Skill identification, Natural language processing},
abstract = {The job market is evolving continuously due to changes in economic landscapes, technological improvements, and skill requirements. In the era of digitalization, a wealth of data is becoming available, opening up new opportunities for labor market analysis. Many stakeholders can make informed decisions if they benefit from accurate and timely insights about the job market. However, traditional data sources and methods used for labor market analysis often fall short of capturing the diversity and trends of the evolving job market. Recently, researchers started exploring various data sources by leveraging data science techniques, which makes information extraction achievable. This survey reviews recent research published between 2015 and 2022 on labor market analytics through data science techniques and discusses future research directions. 101 primary studies were classified and evaluated to identify the data sources utilized for job market analysis; the skill extraction methods and their type; the occupation and sector identification methods; and the application of the study conducted. Finally, we explore potential avenues for future research in this area.}
}
@article{SARKER2024935,
title = {Explainable AI for cybersecurity automation, intelligence and trustworthiness in digital twin: Methods, taxonomy, challenges and prospects},
journal = {ICT Express},
volume = {10},
number = {4},
pages = {935-958},
year = {2024},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2024.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S2405959524000572},
author = {Iqbal H. Sarker and Helge Janicke and Ahmad Mohsin and Asif Gill and Leandros Maglaras},
keywords = {Cybersecurity, Explainable AI, Machine learning, Data-driven, Automation, Intelligent decision-making, Trustworthiness, Digital twin},
abstract = {Digital twins (DTs) are an emerging digitalization technology with a huge impact on today’s innovations in both industry and research. DTs can significantly enhance our society and quality of life through the virtualization of a real-world physical system, providing greater insights about their operations and assets, as well as enhancing their resilience through real-time monitoring and proactive maintenance. DTs also pose significant security risks, as intellectual property is encoded and more accessible, as well as their continued synchronization to their physical counterparts. The rapid proliferation and dynamism of cyber threats in today’s digital environments motivate the development of automated and intelligent cyber solutions. Today’s industrial transformation relies heavily on artificial intelligence (AI), including machine learning (ML) and data-driven technologies that allow machines to perform tasks such as self-monitoring, investigation, diagnosis, future prediction, and decision-making intelligently. However, to effectively employ AI-based models in the context of cybersecurity, human-understandable explanations, and their trustworthiness, are significant factors when making decisions in real-world scenarios. This article provides an extensive study of explainable AI (XAI) based cybersecurity modeling through a taxonomy of AI and XAI methods that can assist security analysts and professionals in comprehending system functions, identifying potential threats and anomalies, and ultimately addressing them in DT environments in an intelligent manner. We discuss how these methods can play a key role in solving contemporary cybersecurity issues in various real-world applications. We conclude this paper by identifying crucial challenges and avenues for further research, as well as directions on how professionals and researchers might approach and model future-generation cybersecurity in this emerging field.}
}
@article{JACKSON2021102221,
title = {Perceptions of disaster temporalities in two Indigenous societies from the Southwest Pacific},
journal = {International Journal of Disaster Risk Reduction},
volume = {57},
pages = {102221},
year = {2021},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2021.102221},
url = {https://www.sciencedirect.com/science/article/pii/S2212420921001874},
author = {Guy Jackson},
keywords = {Disaster temporality, Ontology, Religion, Vulnerability, Indigenous, Pacific islands},
abstract = {Disasters are typically conceptualised as extreme events that disrupt the “normal” functioning of a society. Dominant framings imply temporal boundedness and nature as primarily responsible for environmental disasters. Yet, many critical investigations into the root causes of disasters have taken aim at their eventfulness and naturalness through convincing analysis of the historical construction of vulnerability. Relatively underexplored, however, is the examination of the lived experiences and perceptions of disaster temporalities, which are mediated through culture and institutions. This paper explores the perceptions of disaster temporalities within two Indigenous societies in the Southwest Pacific: the Bedamuni of Western Province, Papua New Guinea and Emae Island, Vanuatu. Additionally, disaster management actors’ perceptions are explored within Vanuatu. Based on two separate research projects utilising ethnographic methods to understand disaster vulnerability, the findings suggest different perceptions of disaster temporalities. The Bedamuni have historically normalised periods of food insecurity, and even though hazards are of great concern they were, and to some extent still are, considered a cyclical feature of life caused by spirits and human conjuration. They also tied major recent disasters to their eschatological beliefs, which pre-date introduced Christian teachings. Emae islanders, with far longer sustained contact with Western societies (including aid and development activities), appear to now experience and perceive large-scale disasters (e.g., cyclone Pam) as abnormal, natural, and discrete events that are tied to climate change. Both cultures understand many aspects of their disaster vulnerability but are more likely to focus on response, similar to disaster management actors in Vanuatu.}
}
@article{HE2025169357,
title = {KDBI-RP: Kinetic Data of RNA-Protein Interactions Database},
journal = {Journal of Molecular Biology},
volume = {437},
number = {21},
pages = {169357},
year = {2025},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2025.169357},
url = {https://www.sciencedirect.com/science/article/pii/S0022283625004231},
author = {Yunpeng He and Dongyue Hou and Yuzong Chen and Xian Zeng},
keywords = {RNA-protein interactions, kinetics, binding affinity},
abstract = {Biomolecular interaction kinetics underpin essential cellular mechanisms, yet quantitative databases remain scarce for RNA-protein interactions (RPIs)–a critical regulatory axis in post-transcriptional control, synthetic biology, and therapeutic development. We previously established KDBI (Kinetic Data of Bio-molecular Interactions database) to catalog quantitative kinetics data across diverse biomolecular interaction types. Here, we present KDBI-RP, a dedicated extension focused on RPI kinetics, addressing the growing demand for RNA-centric kinetic research. KDBI-RP systematically integrates binding data for RNA–protein interactions, including kinetic rate constants–association (kon, 3657 entries) and dissociation (koff, 3761 entries)–supplemented by equilibrium dissociation constants (Kd, 175,932 entries). The database offers well-curated information on kinetic constants, assay conditions, literature sources, and comprehensive sequence, structural, and functional annotations for proteins, RNAs, and their complexes. KDBI-RP is freely accessible at http://www.kdbirp.aiddlab.com. We anticipate that KDBI-RP will serve as a valuable resource for the RNA biology and RNA-based medicine research communities.}
}
@article{CASTELLANZA2022106032,
title = {Discipline, abjection, and poverty alleviation through entrepreneurship: A constitutive perspective},
journal = {Journal of Business Venturing},
volume = {37},
number = {1},
pages = {106032},
year = {2022},
issn = {0883-9026},
doi = {https://doi.org/10.1016/j.jbusvent.2020.106032},
url = {https://www.sciencedirect.com/science/article/pii/S0883902619301375},
author = {Luca Castellanza},
keywords = {Constitutive ontology, Poverty, Opportunities, Emancipation, Grounded theory},
abstract = {Collective entrepreneurship has been found to alleviate extreme poverty by helping poor individuals integrate into their societies and overcome their multiple intertwined liabilities. We complement this line of inquiry by exploring the conditions under which group structures may instead reinforce economic and gendered poverty constraints. We conducted grounded-theoretical interviews with 104 women entrepreneurs operating in farming cooperatives and non-farm groups in war-torn South-West Cameroon. Analysing our data through a constitutive lens, we found that discipline, the extent to which rules determine and control individual behaviours, helps poor women overcome extreme economic constraints but prevents them from attaining prosperity and emancipation.}
}
@article{WANG2025105459,
title = {Comprehensive review of segment anything model across multiple domains},
journal = {Digital Signal Processing},
volume = {167},
pages = {105459},
year = {2025},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2025.105459},
url = {https://www.sciencedirect.com/science/article/pii/S1051200425004816},
author = {Xin Wang and Taisen Duan and Ganxin Ouyang and Weifeng Hao and Lu Mu and Xuejun Zhang},
keywords = {Computational efficiency, Image segmentation, Real-time processing, Visual foundation model, Zero-shot generalization},
abstract = {The Segment Anything Model (SAM) has demonstrated exceptional zero-shot and few-shot generalization capabilities, enabling its effective transfer to novel image processing tasks and supporting diverse downstream applications. As a foundational component in large-scale systems or when integrated with complementary techniques for co-optimization, SAM has significantly advanced the development of image segmentation across multiple domains. However, inherent complexities within domain-specific datasets present critical challenges in boundary refinement, real-time processing, and computational efficiency. This review systematically summarizes current SAM applications in diverse scenarios, evaluates its strengths and limitations, and identifies recurrent challenges in representative datasets. Key optimization strategies for enhancing SAM's performance, generalizability, and efficiency are highlighted. The insights provided aim to guide future dataset construction and interdisciplinary applications, facilitating technological advancements in image segmentation.}
}
@article{MEHDIZAVAREH20252898,
title = {Enhancing glucose level prediction of ICU patients through hierarchical modeling of irregular time-series},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {2898-2914},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.06.039},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025002545},
author = {Hadi Mehdizavareh and Arijit Khan and Simon Lebech Cichosz},
keywords = {Electronic health record, Multi-source learning, Irregular time series, Next glucose level prediction, ICU patients},
abstract = {Accurately predicting blood glucose (BG) levels of ICU patients is critical, as both hypoglycemia (BG < 70 mg/dL) and hyperglycemia (BG > 180 mg/dL) are associated with increased morbidity and mortality. This study presents a proof-of-concept machine learning framework, the Multi-source Irregular Time-Series Transformer (MITST), designed to predict BG levels in ICU patients. In contrast to existing methods that rely heavily on manual feature engineering or utilize limited Electronic Health Record (EHR) data sources, MITST integrates diverse clinical data—including laboratory results, medications, and vital signs—without predefined aggregation. The model leverages a hierarchical Transformer architecture, designed to capture interactions among features within individual timestamps, temporal dependencies across different timestamps, and semantic relationships across multiple data sources. Evaluated using the extensive eICU database (200,859 ICU stays across 208 hospitals), MITST achieves a statistically significant (p<0.001) average improvement of 1.7 percentage points (pp) in AUROC and 1.8 pp in AUPRC over a state-of-the-art random forest baseline. Crucially, for hypoglycemia—a rare but life-threatening condition—MITST increases sensitivity by 7.2 pp, potentially enabling hundreds of earlier interventions across ICU populations. The flexible architecture of MITST allows seamless integration of new data sources without retraining the entire model, enhancing its adaptability for clinical decision support. While this study focuses on predicting BG levels, we also demonstrate MITST's ability to generalize to a distinct clinical task (in-hospital mortality prediction), highlighting its potential for broader applicability in ICU settings. MITST thus offers a robust and extensible solution for analyzing complex, multi-source, irregular time-series data.}
}
@article{FAN2025103848,
title = {A history-based parametric CAD sketch dataset with advanced engineering commands},
journal = {Computer-Aided Design},
volume = {182},
pages = {103848},
year = {2025},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2025.103848},
url = {https://www.sciencedirect.com/science/article/pii/S0010448525000107},
author = {Rubin Fan and Fazhi He and Yuxin Liu and Jing Lin},
keywords = {Sketch, Dataset, CAD, History-based, Parametric},
abstract = {Modern computer-aided design (CAD) adopts history-based parametric modeling paradigm, in which the sketches play a crucial role to represent the design history, design intent, and design semantics of human engineers. Recent academic works include simple features, which are not suitable for the real-world engineering tasks. How to understand, generate and reconstruct CAD sketches with advanced engineering commands (primitives, operations and constraints) are still open challenges. To address these challenges, this paper is the first work to propose a history-based parametric CAD sketch dataset to support advanced engineering commands, named as HPSketch. Firstly, unlike existing simple datasets which are mainly composed of simple sketch primitives (typical line, arc, circle), HPSketch devises advanced primitives (such as parabola, hyperbola), operations (such as chamfer, fillet, rotation), and fruitful constraints. Secondly, HPSketch creatively propose a primitive selection command for advanced sketch operations, which can fully express the design intent and design knowledge for engineering design. This selection method is only available in our HPSketch dataset. Furthermore, unlike the simple fixed interactive constraints in previous sketch datasets, HPSketch presents a complicated, flexible constraint mechanism, which supports complicated engineering constraints. At present, HPSketch consists of 151,984 parametric sketches and 377,623 loops with 29 command types. The experiments show that the generated 2D sketches and 3D CAD models based on HPSketch are more complicated and more diverse than those based on existing sketch datasets. What is more encouraging is that sketches generated by HPSketch can be edited and re-designed by human engineers on mainstream industrial CAD softwares.}
}
@article{VASILIEV20211041,
title = {Evaluation of Data Integration Plans based on Graph Data},
journal = {Procedia Computer Science},
volume = {192},
pages = {1041-1050},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.107},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015957},
author = {Diana Anca Vasiliev and Ana-Maria Ghiran and Robert Andrei Buchmann},
keywords = {Graph Databases, Knowledge graphs, RDF, GraphQL;Performance evaluation, Data Integration},
abstract = {In data management, new data storage models and technologies are gradually adopted, including graph-based data models that have been enabled by technologies such as RDF, GraphQL and LPG. These are particularly fit when dealing with the coexistence of multitude and heterogeneous data sources that require an integration architecture in a data-centric organization due to intrinsic data connectedness. Two prominent approaches are distinguished in our study: (1) embracing a unifying graph format, where, following a series of transformations, all data sources are lifted to a common format in a consolidated graph repository, (2) creating a mediator tier where legacy data sources hold their data but are accessed through a virtual graph integration layer. Each approach comes with a specific way to query data and a plethora of query languages were created for this purpose. In the context of a data integration project we investigated two approaches to data retrieval and data modeling that have gained momentum during recent years: RDF graphs holding data lifted from non-graph data sources and GraphQL acting as a proxy for retrieving virtually connected data from heterogeneous sources. The paper reports on comparative experiments with technological instantiations of the two approaches, which can inform further IT strategies in an institutional project advocating a migration from app-centricity towards data connectedness.}
}
@article{SEPULVEDAGALEAS2025104942,
title = {Monstrous substance: ‘Tuci’, pharmacopolitical assemblages and spectral materialities},
journal = {International Journal of Drug Policy},
volume = {145},
pages = {104942},
year = {2025},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2025.104942},
url = {https://www.sciencedirect.com/science/article/pii/S0955395925002385},
author = {Mauricio {Sepúlveda Galeas} and Ernesto Escobar and Sebastían {Ubiergo Scheel} and Camilo {Obregón Fernández}},
keywords = {Tusi, Tuci, Monstrous substance, Decoloniality},
abstract = {This article critically examines the ‘Tuci’ (or "pink cocaine") phenomenon as an epistemic, cultural, and pharmacopolitical object, proposing its conceptualization as a monstrous substance. Two common assumptions in institutional and media discourses are problematized: that ‘Tuci’ is a counterfeit of 2C-B and that its identity is defined by the presence of ketamine. Using an approach that articulates Foucauldian poststructuralism, neomaterialisms, and decolonial studies, the authors dismantle these premises and propose a dense performative, speculative, and ontopolitical reading of the phenomenon. The article proposes a reading that describes how ‘Tuci’ not only acts on the body, but also produces it as an effect of material and discursive assemblages. It shows how this substance does not refer to an original nor can it be fixed in a stable composition, but is defined by its affective operativity, its contextual modulation and its performative adoption in liminal youth niches. Through two key concepts, monstrosity and decolonial critique, the essay proposes new grammar for understanding what ‘Tuci’ is and does. It concludes that this substance demands an epistemological shift towards a politics of drugs that recognises the mutability, relationality and power of the unclassifiable.}
}
@article{HUANG2023167,
title = {Combining Deep Learning with Knowledge Graph for Design Knowledge Acquisition in Conceptual Product Design},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {138},
number = {1},
pages = {167-200},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.028268},
url = {https://www.sciencedirect.com/science/article/pii/S1526149223000073},
author = {Yuexin Huang and Suihuai Yu and Jianjie Chu and Zhaojing Su and Yangfan Cong and Hanyu Wang and Hao Fan},
keywords = {Conceptual product design, design knowledge acquisition, knowledge graph, entity extraction, relation extraction},
abstract = {The acquisition of valuable design knowledge from massive fragmentary data is challenging for designers in conceptual product design. This study proposes a novel method for acquiring design knowledge by combining deep learning with knowledge graph. Specifically, the design knowledge acquisition method utilises the knowledge extraction model to extract design-related entities and relations from fragmentary data, and further constructs the knowledge graph to support design knowledge acquisition for conceptual product design. Moreover, the knowledge extraction model introduces ALBERT to solve memory limitation and communication overhead in the entity extraction module, and uses multi-granularity information to overcome segmentation errors and polysemy ambiguity in the relation extraction module. Experimental comparison verified the effectiveness and accuracy of the proposed knowledge extraction model. The case study demonstrated the feasibility of the knowledge graph construction with real fragmentary porcelain data and showed the capability to provide designers with interconnected and visualised design knowledge.}
}
@article{ANDERSEN2020627,
title = {Re-appraising interaction and process for industrial network research: The future plunging mirror hall metaphor},
journal = {Industrial Marketing Management},
volume = {91},
pages = {627-638},
year = {2020},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2019.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0019850118304711},
author = {Poul Houman Andersen and Christopher John Medlin and Jan-Åke Törnroos},
keywords = {Interactivity, Mechanisms, Process research, Sensemaking, Social constructivism, Withness},
abstract = {Constructivist and realist research is undertaken in the business relationship and network approach of the Industrial Marketing and Purchasing Group. These two divergent research perspectives seek a different form of contextual understanding vs. general knowledge. They are not incommensurable as one can gain insights from the other. But a researcher must know and understand both perspectives and sometimes be able to see when a writer is playing almost a middle ground to make a specific point. To provide a broader understanding of the ontological distinctions and their ramifications for researching and translating meanings concerning business networks we introduce the temporal mirror hall metaphor. We propose that: (i) researchers should avoid reading and understanding only a single research perspective, (ii) research is a social temporal process embedded in a research community, (iii) understanding different researcher perspectives is necessary for a constructivist scholar working in a world of realist education, and (iv) realist researchers need constructivist research to change and develop systematized theories. The paper extends specific advice to constructivist researchers undertaking longitudinal studies of interaction in business networks. Realist researchers will find intriguing the comparisons and refractions, as well as illusions, in the temporalities of the mirror hall.}
}
@article{HOBBS2024102420,
title = {Agonism in the arena: Analyzing cancel culture using a rhetorical model of deviance and reputational repair},
journal = {Public Relations Review},
volume = {50},
number = {1},
pages = {102420},
year = {2024},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2023.102420},
url = {https://www.sciencedirect.com/science/article/pii/S0363811123001352},
author = {Mitchell John Hobbs and Sarah O’Keefe},
keywords = {Cancel culture, De-platforming movements, Rhetoric, Crisis communication, Image, Repair, Sentiment analysis, Celebrities, Identity politics},
abstract = {Cancel culture is a socio-political movement that aims to financially punish or ostracize a person from the public sphere due to a transgression. Such offenses range from criminal acts to the public expression of controversial opinions. In response, digital activism coalesces into a socio-political force that seeks to shame, silence, or punish the offending individual. This study seeks to understand the agonistic reputational wrangle facilitated by cancel culture using an original theoretical framework that combines the rhetorical paradigm from public relations with deviance theory from sociology and media studies. Specifically, this study analyzes the cancellation and reputational repair strategies of four celebrities—Louis C.K., Logan Paul, Jussie Smollett, and J.K. Rowling. It utilizes large-scale social media sentiment analysis to reveal varying degrees of reputational decay and recovery. The study shows that factors impacting the severity of cancellation and the likelihood of reputational recovery include distinguishing between criminal acts and controversial opinions, as well as social variables and the strength of a parasocial relationship between a celebrity and their fandom.}
}
@article{LI2021101333,
title = {Context-aware sequence labeling for condition information extraction from historical bridge inspection reports},
journal = {Advanced Engineering Informatics},
volume = {49},
pages = {101333},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101333},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000860},
author = {Tianshu Li and Mohamad Alipour and Devin K. Harris},
keywords = {Bridge inspection report, Information extraction, Context-aware, Deep learning, Data-driven bridge management},
abstract = {Effective upkeep of aging infrastructure systems with limited funding and resources calls for efficient bridge management systems. Although data-driven models have been extensively studied in the last decade for extracting knowledge from past experience to guide future maintenance decision making, their performance and usefulness have been limited by the level of detail and accuracy of currently available bridge condition databases. This paper leverages an untapped resource for bridge condition data and proposes a new method to extract condition information from it at a high level of detail. To that end, a natural language processing approach was developed to formalize structural condition knowledge by formulating a sequence labeling task and modeling inspection narratives as a combination of words representing defects, their severity and location, while accounting for the context of each word. The proposed framework employs a deep-learning-based approach and incorporates context-aware components including a bi-directional Long Short Term Memory (LSTM) neural network architecture and a Conditional Random Field (CRF) classifier to account for the context of words when assigning labels. A dependency-based word embedding model was also used to represent the raw text while incorporating both semantic and contextual information. The sequence labeling model was trained using bridge inspection reports collected from the Virginia Department of Transportation bridge inspection database and achieved an F1 score of 94.12% during testing. The proposed model also demonstrated improvements compared with baseline sequence labeling models, and was further used to demonstrate the capability of detecting condition changes with respect to previous inspection records. Results of this study show that the proposed method can be used to extract and create a condition information database that can further assist in developing data-driven bridge management and condition forecasting models, as well as automated bridge inspection systems.}
}
@article{WU2024e34524,
title = {Integrative analyses of genes associated with oxidative stress and cellular senescence in triple-negative breast cancer},
journal = {Heliyon},
volume = {10},
number = {14},
pages = {e34524},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e34524},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024105555},
author = {Lihua Wu and Hongyan Zheng and Xiaorong Guo and Nan Li and Luyao Qin and Xiaoqing Li and Ge Lou},
keywords = {Triple-negative breast cancer, Bioinformatics, Oxidative stress, Cellular senescence, Tumor immune microenvironment},
abstract = {Background
Oxidative stress and cellular senescence (OSCS) have great impacts on the occurrence and progression of triple-negative breast cancer (TNBC). This study was intended to construct a prognostic model based on oxidative stress and cellular senescence related difference expression genes (OSCSRDEGs) for TNBC.
Methods
The Cancer Genome Atlas (TCGA) databases and two Gene Expression Omnibus (GEO) databases were used to identify OSCSRDEGs. The relationship between OSCSRDEGs and immune infiltration was examined using single-sample gene-set enrichment analysis (ssGSEA), ESTIMATE, and the CIBERSORT algorithm. Least absolute shrinkage and selection operator (LASSO) regression analyses, Cox regression and Kaplan-Meier analysis were employed to construct a prognostic model. Receiver operating characteristic (ROC) curves, nomograms, and decision curve analysis (DCA) were used to evaluate the prognostic efficacy. Gene Set Enrichment Analysis (GSEA) Gene Ontology (GO), and Kyoto Encyclopedia of Genes and Genomes (KEGG) were utilized to explore the potential functions and mechanism.
Results
A comprehensive analysis identified a total of 27 OSCSRDEGs, out of which 15 genes selected for development of a prognostic model. A high degree of statistical significance was observed for the riskscores derived from this model to accurately predict TNBC Overall survival. The decision curve analysis (DCA) and ROC curve analysis further confirmed the superior accuracy of the OSCSRDEGs prognostic model in predicting efficacy. Notably, the nomogram analysis highlighted that DMD exhibited the highest utility within the model. In comparison between high and low OSCScore groups, the infiltration abundance of immune cells was statistically different in the TCGA-TNBC dataset.
Conclusion
These studies have effectively identified four essential OSCSRDEGs (CFI, DMD, NDRG2, and NRP1) and meticulously developed an OSCS-associated prognostic model for individuals diagnosed with TNBC. These discoveries have the potential to significantly contribute to the comprehension of the involvement of OSCS in TNBC.}
}
@article{FERRUZ2023238,
title = {From sequence to function through structure: Deep learning for protein design},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {238-250},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2022.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S2001037022005086},
author = {Noelia Ferruz and Michael Heinzinger and Mehmet Akdel and Alexander Goncearenco and Luca Naef and Christian Dallago},
keywords = {Protein design, Protein prediction, Drug discovery, Deep learning, Protein language models},
abstract = {The process of designing biomolecules, in particular proteins, is witnessing a rapid change in available tooling and approaches, moving from design through physicochemical force fields, to producing plausible, complex sequences fast via end-to-end differentiable statistical models. To achieve conditional and controllable protein design, researchers at the interface of artificial intelligence and biology leverage advances in natural language processing (NLP) and computer vision techniques, coupled with advances in computing hardware to learn patterns from growing biological databases, curated annotations thereof, or both. Once learned, these patterns can be used to provide novel insights into mechanistic biology and the design of biomolecules. However, navigating and understanding the practical applications for the many recent protein design tools is complex. To facilitate this, we 1) document recent advances in deep learning (DL) assisted protein design from the last three years, 2) present a practical pipeline that allows to go from de novo-generated sequences to their predicted properties and web-powered visualization within minutes, and 3) leverage it to suggest a generated protein sequence which might be used to engineer a biosynthetic gene cluster to produce a molecular glue-like compound. Lastly, we discuss challenges and highlight opportunities for the protein design field.}
}
@article{LI2025110361,
title = {Few-shot machine reading comprehension for bridge inspection via domain-specific and task-aware pre-tuning approach},
journal = {Engineering Applications of Artificial Intelligence},
volume = {147},
pages = {110361},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110361},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625003616},
author = {Ren Li and Luyi Zhang and Qiao Xiao and Jianxi Yang and Yu Chen and Shixin Jiang and Di Wang},
keywords = {Machine reading comprehension, Bridge inspection, Few-shot, Pre-tuning strategy, Data augmentation},
abstract = {With the wide application of information technologies in the field of bridge engineering, many electronic bridge inspection reports have been generated. However, due to insufficient research on machine reading comprehension (MRC) in this field, a lot of bridge inspection information, e.g., structural basic data, inspected defects, and maintenance suggestions, has not been fully used. Especially, it is time-consuming and labor-intensive to pre-train a domain-specific language model from scratch or annotate large-scale question answering corpora, which also brings challenges to the MRC research in this field. To tackle the problems, this paper proposes a novel few-shot MRC approach for bridge inspection based on the idea of data augmentation. The proposed model uses a pre-trained model as backbone, along with introducing a pre-tuning stage to bridge the gaps between general-purpose pre-training and domain-specific MRC tasks. In order to reduce the workload of manual annotation, we present a novel pre-tuning data generation algorithm which is based on the domain-specific question classification and answer prediction neural models. After pre-tuning and fine-tuning, the proposed model achieves efficient bridge inspection MRC. The experimental results show that the proposed model outperforms the mainstream fine-tuning-based approaches and few-shot MRC baseline models in various settings. With 1024 fine-tuning samples, the F1 value and Exact Match (EM) value are 86.42%, 74.65%, respectively. Our research work can serve as a foundation for the construction of automatic question answering systems for intelligent bridge management and maintenance.}
}
@article{DEMENDIZABAL2020102249,
title = {SDRS: A new lossless dimensionality reduction for text corpora},
journal = {Information Processing & Management},
volume = {57},
number = {4},
pages = {102249},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102249},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319314694},
author = {Iñaki Velez {de Mendizabal} and Vitor Basto-Fernandes and Enaitz Ezpeleta and José R. Méndez and Urko Zurutuza},
keywords = {Spam filtering, Token-based representation, Synset-based representation, Semantic-based feature reduction, Multi-objective evolutionary algorithms},
abstract = {In recent years, most content-based spam filters have been implemented using Machine Learning (ML) approaches by means of token-based representations of textual contents. After introducing multiple performance enhancements, the impact has been virtually irrelevant. Recent studies have introduced synset-based content representations as a reliable way to improve classification, as well as different forms to take advantage of semantic information to address problems, such as dimensionality reduction. These preliminary solutions present some limitations and enforce simplifications that must be gradually redefined in order to obtain significant improvements in spam content filtering. This study addresses the problem of feature reduction by introducing a new semantic-based proposal (SDRS) that avoids losing knowledge (lossless). Synset-features can be semantically grouped by taking advantage of taxonomic relations (mainly hypernyms) provided by BabelNet ontological dictionary (e.g. “Viagra” and “Cialis” can be summarized into the single features “anti-impotence drug”, “drug” or “chemical substance” depending on the generalization of 1, 2 or 3 levels). In order to decide how many levels should be used to generalize each synset of a dataset, our proposal takes advantage of Multi-Objective Evolutionary Algorithms (MOEA) and particularly, of the Non-dominated Sorting Genetic Algorithm (NSGA-II). We have compared the performance achieved by a Naïve Bayes classifier, using both token-based and synset-based dataset representations, with and without executing dimensional reductions. As a result, our lossless semantic reduction strategy was able to find optimal semantic-based feature grouping strategies for the input texts, leading to a better performance of Naïve Bayes classifiers.}
}
@article{POLHILL2023103121,
title = {Cognition and hypocognition: Discursive and simulation-supported decision-making within complex systems},
journal = {Futures},
volume = {148},
pages = {103121},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103121},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723000253},
author = {J. Gareth Polhill and Bruce Edmonds},
keywords = {Simulation, Cognition, Hypocognition, Divination, Ecocyborgs, Blasphemy},
abstract = {Homo sapiens is currently believed to have evolved in the African savannah several hundreds of thousands of years ago. Since then, human societies have become, through technological innovation and application, powerful influencers of the planet’s ecological, hydrological and meteorological systems – for good and ill. They have experimented with many different systems of governance, in order to manage their societies and the environments they inhabit – using computer simulations as a tool to help make decisions concerning highly complex systems, is only the most recent of these. In questioning whether, when and how computer simulations should play a role in determining decision-making in these systems of governance, it is also worth reflecting on whether, when and how humans, or groups of humans, have the capability to make such decisions without the aid of such technology. This paper looks at and compares the characteristics of natural language-based and simulation-based decision-making. We argue that computational tools for decision-making can and should be complementary to natural language discourse approaches, but that this requires that both systems are used with their limitations in mind. All tools and approaches – physical, social and mental – have dangers when used inappropriately, but it seems unlikely humankind can survive without them. The challenge is how to do so.}
}
@article{REN2025103525,
title = {Automated disassembly-oriented knowledge graph construction for retired battery packs using a candidate entity-based relational triple joint extraction method},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103525},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103525},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004185},
author = {Yaping Ren and Junying Wu and Cunbo Zhuang and Xiaoguang Sun and Hongfei Guo and Jianzhao Wu and Yang Chen and Jianhua Liu},
keywords = {Retired battery packs, Automated disassembly, Knowledge graph, Relational triple joint extraction, Knowledge recommendation},
abstract = {Currently, the disassembly of retired electric vehicle battery packs relies on manpower and results in high cost, low efficiency, and poor stability. With the development of artificial intelligence, automated disassembly is an efficient method to largely reduce even completely replace human disassembly. However, the various kinds of battery packs and the uncertainty on their retired numbers and types lead to frequent changes of their disassembly processes. It is necessary to provide a method that can integrate valuable disassembly knowledge to enable automated disassembly. Thus, this study proposes an automated disassembly-oriented knowledge graph for retired battery packs which considers the properties of subassemblies (entities) and explicit physical connections/implicit associations among subassemblies (relations). A large amount of unstructured data exists regarding battery packs, such as product manuals and maintenance records, whereas the knowledge that can be available to guide the disassembly process is dispersed and sparse. To solve this, a candidate entity-based relational triple joint extraction method is developed to efficiently extract the disassembly knowledge, which consists of semantic feature learning, candidate entity recognition, and explicit/implicit relational triple identification. Finally, more than 10,000 sentences collected from multi-source unstructured texts are adopted to verify the proposed method. The experimental results demonstrate that our proposed method achieves an F1-score of 93.99% in candidate entity recognition and an F1-score of 95.6% in triple extraction. Also, the information of disassembly operations, disassembly tools, and subassembly properties can be recommended by the automated disassembly-oriented knowledge graph for retired battery packs.}
}
@article{GONG2025112754,
title = {Multi-domain dialogue state tracking via dual dynamic graph with hierarchical slot selector},
journal = {Knowledge-Based Systems},
volume = {308},
pages = {112754},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112754},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013881},
author = {Yeseul Gong and Heeseon Kim and Seokju Hwang and Donghyun Kim and Kyong-Ho Lee},
keywords = {Dialogue state tracking (DST), Hierarchical slot selection, Multi-domain, Dual dynamic graph, Co-reference},
abstract = {Dialogue state tracking aims to maintain user intent as a consistent state across multi-domains to accomplish natural dialogue systems. However, previous researches often fall short in capturing the difference of multiple slot types and fail to adequately consider the selection of discerning information. The increase in unnecessary information correlates with a decrease in predictive performance. Therefore, the careful selection of high-quality information is imperative. Moreover, considering that the types of essential and available information vary for each slot, the process of selecting appropriate information may also differ. To address these issues, we propose HS2DG-DST, a Hierarchical Slot Selector and Dual Dynamic Graph-based DST. Our model is designed to provide maximum information for optimal value prediction by clearly exploiting the need for differentiated information for each slot. First, we hierarchically classify slot types based on the multiple properties. Then, two dynamic graphs provide highly relevant information to each slot. Experimental results on MultiWOZ datasets demonstrate that our model outperforms state-of-the-art models.}
}
@article{LIU2024509,
title = {Integration of data science with product design towards data-driven design},
journal = {CIRP Annals},
volume = {73},
number = {2},
pages = {509-532},
year = {2024},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2024.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0007850624001252},
author = {Ang Liu and Stephen Lu and Fei Tao and Nabil Anwer},
keywords = {Product design, Data science, Data-driven design},
abstract = {This paper aims to investigate the scientific integration of data science with product design towards data-driven design (D3). Data science has potential to facilitate design decision-making through insight extraction, predictive analytics, and automatic decisions. A systematic scoping review is conduced to converge various D3 applications in four dimensions: the design dimension about design operations, the data dimension about popular data sources and common data-related challenges, the method dimension about the methodological foundations, and the social/ethical dimension about social/ethical considerations and implications. Based on the state-of-the-art, this paper also highlights potential future research avenues in this dynamic field.}
}
@article{LAWLER2025106230,
title = {Evaluation of the SpatioTemporal Asset Catalog for management and discovery of FAIR flood hazard models},
journal = {Environmental Modelling & Software},
volume = {183},
pages = {106230},
year = {2025},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106230},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224002913},
author = {Seth Lawler and Thomas Williams and William Lehman and Christina Lindemer and David Rosa and Celso Ferreira and Chen Zhang},
keywords = {Flood modeling, SpatioTemporal Asset Catalog, FAIR principles, Hydroinformatics},
abstract = {Approaches for performing flood hazards modeling and risk assessment at federal, state, and local agencies are undergoing emergent challenge for consistent metadata and cataloging systems to ensure the sharing of flood risk data in a Findable, Accessible, Interoperable, and Reusable (FAIR) manner. This paper explores the suitability of a suite of software and specifications developed by the Earth observation community for environmental modeling, which adhere to the FAIR principles not only for managing published or authoritative data but throughout the model development and flood hazard analysis phases. Specifically, we evaluate the SpatioTemporal Asset Catalog (STAC) in a pilot study undertaken as part of the Future of Flood Risk Data (FFRD) initiative of FEMA. The experimental results indicate the STAC ecosystem offers a flexible cloud native approach for linking data, managing metadata, and cataloging collections of models. Further, the STAC framework shows favorable results in a probabilistic and other use cases.}
}
@article{FERNANDEZALVAREZ2022107975,
title = {Automatic extraction of shapes using sheXer},
journal = {Knowledge-Based Systems},
volume = {238},
pages = {107975},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107975},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121010972},
author = {Daniel Fernandez-Álvarez and Jose Emilio Labra-Gayo and Daniel Gayo-Avello},
keywords = {Knowledge Graph, RDF, ShEx, SHACL, Automatic extraction},
abstract = {There is an increasing number of projects based on Knowledge Graphs and SPARQL endpoints. These SPARQL endpoints are later queried by final users or used to feed many different kinds of applications. Shape languages, such as ShEx and SHACL, have emerged to guide the evolution of these graphs and to validate their expected topology. However, authoring shapes for an existing knowledge graph is a time-consuming task. The task gets more challenging when dealing with sources, possibly maintained by heterogeneous agents. In this paper, we present sheXer, a system that extracts shapes by mining the graph structure. We offer sheXer as a free Python library capable of producing both ShEx and SHACL content. Compared to other automatic shape extractors, sheXer includes some novel features such as shape inter-linkage and computation of big real-world datasets. We analyze the features and limitations w.r.t. performance with different experiments using the English chapter of DBpedia.}
}
@article{SAKEEF2025100146,
title = {Detecting cognitive engagement in online course forums: A review of frameworks and methodologies},
journal = {Natural Language Processing Journal},
volume = {11},
pages = {100146},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100146},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000226},
author = {Nazmus Sakeef and M. Ali Akber Dewan and Fuhua Lin and Dharamjit Parmar},
keywords = {Online learning, Cognitive engagement detection, ICAP, CoI, KCSA, Course forum analysis},
abstract = {A key aspect of online learning in higher education involves the utilization of course discussion forums. Assessing the quality of posts, such as cognitive engagement, within online course discussion forums, and determining students’ interest and participation is challenging yet beneficial. This research investigates existing literature on identifying the cognitive engagement of online learners through the analysis of course discussion forums. Essentially, this review examines three educational frameworks - Van Der Meijden’s Knowledge Construction in Synchronous and Asynchronous Discussion Posts (KCSA), Community of Inquiry (CoI), and Interactive, Constructive, Active, and Passive (ICAP), which have been widely used for students’ cognitive engagement detection analyzing their posts in course discussion forums. This study also examines the natural language processing and deep learning approaches employed and integrated with the above three educational frameworks in the existing literature concerning the detection of cognitive engagement in the context of online learning. The article provides recommendations for enhancing instructional design and fostering student engagement by leveraging cognitive engagement detection. This research underscores the significance of automating the identification of cognitive engagement in online learning and puts forth suggestions for future research directions.}
}
@article{SHEN2025103136,
title = {CATI: A medical context-enhanced framework for diagnosis code assignment in the UK Biobank study},
journal = {Artificial Intelligence in Medicine},
volume = {166},
pages = {103136},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103136},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725000715},
author = {Yue Shen and Jie Wang and Zhe Wang and Zhihao Shi and Hanzhu Chen and Zheng Wang and Yukang Jiang and Xiaopu Wang and Chuandong Cheng and Xueqin Wang and Hongtu Zhu and Jieping Ye},
keywords = {Diagnosis code assignment, Disease hierarchy, Medical context, UK biobank},
abstract = {Diagnosis codes are standard code format of diseases or medical conditions. This study is aimed at assigning diagnosis codes to patients in large-scale biobanks, particularly addressing the issue of missing codes for some patients. This is crucial for downstream disease-related tasks. While recent methods primarily rely on structured biobank data for code assignment, they often overlook the valuable medical context provided by textual information in the biobanks and hierarchical structure of the disease coding system. To address this gap, we have developed CATI, a medical context-enhanced framework for diagnosis Code Assignment by integrating Textual details derived from key features and disease hIerarchy. The study is based on the UK Biobank data and considers Phecodes and ICD-10 codes as standard disease formats. We start by representing ten informative codified features using their formal names and then integrate them into CATI as text embeddings, achieved through prompt tuning on the pre-trained language model BioBERT. Recognizing the hierarchical structure of diagnosis codes, we have developed a novel convolution layer in our method that effectively propagates logits between adjacent diagnosis codes. Evaluation results demonstrate that CATI outperforms existing state-of-the-art methods in terms of both Phecodes and ICD-10 codes, boasting at least a 5.16% improvement in average AUROC for unseen disease codes and an 8.68% rise in average AUPRC for disease codes with training instances ranging in (1000,10000]. This framework contributes to the formation of well-defined cohorts for downstream studies and offers a unique perspective for addressing complex healthcare tasks by incorporating vital medical context.}
}
@article{PROSDOCIMI2021104445,
title = {Life and living beings under the perspective of organic macrocodes},
journal = {Biosystems},
volume = {206},
pages = {104445},
year = {2021},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2021.104445},
url = {https://www.sciencedirect.com/science/article/pii/S0303264721001003},
author = {Francisco Prosdocimi and Sávio Torres {de Farias}},
keywords = {Code biology, Origin of life, Macrocode, Progenote.},
abstract = {A powerful and concise concept of life is crucial for studies aiming to understand the characteristics that emerged from an inorganic world. Among biologists, the most accepted argument define life under a top-down strategy by looking into the shared characteristics observed in all cellular organisms. This is often made highlighting (i) autonomy and (ii) evolutionary capacity as fundamental characteristics observed in all cellular organisms. Along the present work, we assume the framework of code biology considering that biology started with the emergence of the first organic code by self-organization. We reinforces that the conceptual structure of life should be reallocated from the ontology class of Matter to its sister class of Process. Along the emergence and early evolution of biological systems, biological codes changed from open systems of “naked” molecules (at the progenote era), to close, encapsulated systems (at the organismic era). Living beings appeared at the very moment when nucleic acids with coding properties became encapsulated. This led to the origin of viruses and, then, to the origin of cells. In this context, we propose that the single character that makes a clear distinction between the abiotic and the biotic world is the capacity to process organic codes. Thus, life appears with the self-assembly of a genetic code and evolves by the emergence of other overlapping codes. Once life has been clearly conceptualized, we go further to conceptualize organisms, parents, lineages, and species in terms of code biology.}
}
@article{CHUA2024100120,
title = {A concise guide to essential R packages for analyses of DNA, RNA, and proteins},
journal = {Molecules and Cells},
volume = {47},
number = {11},
pages = {100120},
year = {2024},
issn = {1016-8478},
doi = {https://doi.org/10.1016/j.mocell.2024.100120},
url = {https://www.sciencedirect.com/science/article/pii/S1016847824001456},
author = {Eng Wee Chua and Der Jiun Ooi and Nor Azlan {Nor Muhammad}},
keywords = {Genomics, Proteomics, R package, Transcriptomics},
abstract = {ABSTRACT
R is widely regarded as unrivaled by other high-level programming languages for its statistical functions. The popularity of R as a statistical language has led many to overlook its applications outside the statistical realm. In this brief review, we present a list of R packages for supporting projects that entail analyses of DNA, RNA, and proteins. These R packages span the gamut of important molecular techniques, from routine quantitative polymerase chain reaction (qPCR) and Western blotting to high-throughput sequencing and proteomics generating very large datasets. The text-mining power of R can also be harnessed to facilitate literature reviews and predict future research trends and avenues. We encourage researchers to make full use of R in their work, given the versatility of the language, as well as its straightforward syntax which eases the initial learning curve.}
}
@article{BERTOSSI2023102156,
title = {Extending sticky-Datalog± via finite-position selection functions: Tractability, algorithms, and optimization},
journal = {Information Systems},
volume = {114},
pages = {102156},
year = {2023},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2022.102156},
url = {https://www.sciencedirect.com/science/article/pii/S030643792200134X},
author = {Leopoldo Bertossi and Mostafa Milani},
keywords = {Datalog, Query answering and reasoning, Ontology-based data access},
abstract = {Weakly-Sticky (WS) Datalog± is an expressive member of the family of Datalog± program classes that is defined on the basis of the conditions of stickiness and weak-acyclicity. Conjunctive query answering (QA) over the WS programs has been investigated, and its tractability in data complexity has been established. However, the design and implementation of practical QA algorithms and their optimizations have been open. In order to fill this gap, we first study Sticky and WS programs from the point of view of the behavior of the chase procedure. We extend the stickiness property of the chase to that of generalized stickiness of the chase (GSCh) modulo an oracle that selects (and provides) the predicate positions where finitely many values appear during the chase. Stickiness modulo a selection function S that provides only a subset of those positions defines sch(S), a semantic subclass of GSCh. Program classes with selection functions include Sticky and WS, and another syntactic class that we introduce and characterize, namely JWS, of jointly-weakly-sticky programs, which contains WS. The selection functions for these three classes are computable, and no external, possibly non-computable oracle is needed. We propose a bottom-up QA algorithm for programs in the class sch(S), for a general selection function S. As a particular case, we obtain a polynomial-time QA algorithm for JWS and weakly-sticky programs (in data complexity). Unlike WS, JWS turns out to be closed under magic-sets query optimization. As a consequence, both the generic polynomial-time QA algorithm and its magic-set optimization can be particularized and applied to WS.}
}
@article{AMIRI2023,
title = {Personalized Flexible Meal Planning for Individuals With Diet-Related Health Concerns: System Design and Feasibility Validation Study},
journal = {JMIR Formative Research},
volume = {7},
year = {2023},
issn = {2561-326X},
doi = {https://doi.org/10.2196/46434},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X23003244},
author = {Maryam Amiri and Juan Li and Wordh Hasan},
keywords = {diabetes, fuzzy logic, meal planning, multicriteria decision-making, optimization},
abstract = {Background
Chronic diseases such as heart disease, stroke, diabetes, and hypertension are major global health challenges. Healthy eating can help people with chronic diseases manage their condition and prevent complications. However, making healthy meal plans is not easy, as it requires the consideration of various factors such as health concerns, nutritional requirements, tastes, economic status, and time limits. Therefore, there is a need for effective, affordable, and personalized meal planning that can assist people in choosing food that suits their individual needs and preferences.
Objective
This study aimed to design an artificial intelligence (AI)–powered meal planner that can generate personalized healthy meal plans based on the user’s specific health conditions, personal preferences, and status.
Methods
We proposed a system that integrates semantic reasoning, fuzzy logic, heuristic search, and multicriteria analysis to produce flexible, optimized meal plans based on the user’s health concerns, nutrition needs, as well as food restrictions or constraints, along with other personal preferences. Specifically, we constructed an ontology-based knowledge base to model knowledge about food and nutrition. We defined semantic rules to represent dietary guidelines for different health concerns and built a fuzzy membership of food nutrition based on the experience of experts to handle vague and uncertain nutritional data. We applied a semantic rule-based filtering mechanism to filter out food that violate mandatory health guidelines and constraints, such as allergies and religion. We designed a novel, heuristic search method that identifies the best meals among several candidates and evaluates them based on their fuzzy nutritional score. To select nutritious meals that also satisfy the user’s other preferences, we proposed a multicriteria decision-making approach.
Results
We implemented a mobile app prototype system and evaluated its effectiveness through a use case study and user study. The results showed that the system generated healthy and personalized meal plans that considered the user’s health concerns, optimized nutrition values, respected dietary restrictions and constraints, and met the user’s preferences. The users were generally satisfied with the system and its features.
Conclusions
We designed an AI-powered meal planner that helps people create healthy and personalized meal plans based on their health conditions, preferences, and status. Our system uses multiple techniques to create optimized meal plans that consider multiple factors that affect food choice. Our evaluation tests confirmed the usability and feasibility of the proposed system. However, some limitations such as the lack of dynamic and real-time updates should be addressed in future studies. This study contributes to the development of AI-powered personalized meal planning systems that can support people’s health and nutrition goals.}
}
@article{DHINGRA2023136,
title = {Cardiovascular Care Innovation through Data-Driven Discoveries in the Electronic Health Record},
journal = {The American Journal of Cardiology},
volume = {203},
pages = {136-148},
year = {2023},
issn = {0002-9149},
doi = {https://doi.org/10.1016/j.amjcard.2023.06.104},
url = {https://www.sciencedirect.com/science/article/pii/S0002914923005131},
author = {Lovedeep Singh Dhingra and Miles Shen and Anjali Mangla and Rohan Khera},
abstract = {The electronic health record (EHR) represents a rich source of patient information, increasingly being leveraged for cardiovascular research. Although its primary use remains the seamless delivery of health care, the various longitudinally aggregated structured and unstructured data elements for each patient within the EHR can define the computational phenotypes of disease and care signatures and their association with outcomes. Although structured data elements, such as demographic characteristics, laboratory measurements, problem lists, and medications, are easily extracted, unstructured data are underused. The latter include free text in clinical narratives, documentation of procedures, and reports of imaging and pathology. Rapid scaling up of data storage and rapid innovation in natural language processing and computer vision can power insights from unstructured data streams. However, despite an array of opportunities for research using the EHR, specific expertise is necessary to adequately address confidentiality, accuracy, completeness, and heterogeneity challenges in EHR-based research. These often require methodological innovation and best practices to design and conduct successful research studies. Our review discusses these challenges and their proposed solutions. In addition, we highlight the ongoing innovations in federated learning in the EHR through a greater focus on common data models and discuss ongoing work that defines such an approach to large-scale, multicenter, federated studies. Such parallel improvements in technology and research methods enable innovative care and optimization of patient outcomes.}
}
@article{MU2025,
title = {Semantic AI-Driven Knowledge Networks for Enhancing Linguistic Competence in Educational Management},
journal = {International Journal on Semantic Web and Information Systems},
volume = {21},
number = {1},
year = {2025},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.382223},
url = {https://www.sciencedirect.com/science/article/pii/S1552628325000328},
author = {Yuanyuan Mu and Shuangshuang Zheng and Lizhen Du and Youqing Wang},
keywords = {Knowledge Network, Education Management, Linguistic Education},
abstract = {ABSTRACT
Rapid information growth and digital transformation demand advanced educational management systems to foster more effective learning. This study explores the role of semantic artificial intelligence (AI)-driven knowledge networks in enhancing linguistic competence within educational management. By leveraging AI’s semantic capabilities, such networks organize, analyze, and visualize linguistic data to support deeper knowledge sharing and language development. This interdisciplinary research empirically investigates the approach. A semester-long intervention involved 69 Chinese junior students majoring in linguistics, who engaged with an AI-assisted education management system equipped with tools, such as Gephi, WORDij, and mathematical software. Triangulated data from linguistic assessments, structured interviews, and semantic network analysis demonstrates notable improvements in students’ language proficiency and conceptual understanding. The findings suggest that semantic AI-driven knowledge networks hold significant potential in advancing linguistic competence and optimizing educational management practices.}
}
@article{2025S15,
title = {0084 WITHDRAWN},
journal = {Journal of Investigative Dermatology},
volume = {145},
number = {8, Supplement },
pages = {S15},
year = {2025},
note = {Society for Investigative Dermatology (SID) 2025 Meeting Abstract Supplement},
issn = {0022-202X},
doi = {https://doi.org/10.1016/j.jid.2025.06.086},
url = {https://www.sciencedirect.com/science/article/pii/S0022202X25006475}
}
@article{CHEN2025104085,
title = {Improving cross-document event coreference resolution by discourse coherence and structure},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104085},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104085},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000275},
author = {Xinyu Chen and Peifeng Li and Qiaoming Zhu},
keywords = {Event coreference resolution, Discourse coherence, Discourse structure},
abstract = {Cross-Document Event Coreference Resolution (CD-ECR) is to identify and cluster together event mentions that occur across multiple documents. Existing methods exhibit two limitations: (1) In contrast to within-document event mentions, which are linked by rich, coherent contexts, cross-document event mentions lack such contexts, posing a challenging for the model to understand the relation between two event mentions in different documents. (2) The lack of coherent textual information between cross-document event mentions lead to the inability to capture their global information, which is important to mine long-distance interactions between them. To tackle these issues, we propose a novel discourse coherence enhancement mechanism and introduce discourse structure to improve cross-document event coreference resolution. Specifically, we first introduce a new task: Event-oriented cross-document coherence enhancement (ECD-CoE), which selects coherent sentences that form a coherent text for two cross-document event mentions. Second, we represent the coherent text as a tree structure with rhetorical relation information between textual units. We then obtain the global interaction information of event mentions from the tree structures and finally resolve coreferent events. Experimental results on both the ECB+ and GVC datasets indicate that our proposed method outperforms several state-of-the-art baselines.}
}
@article{MENORFLORES2024108188,
title = {A protein-protein interaction network aligner study in the multi-objective domain},
journal = {Computer Methods and Programs in Biomedicine},
volume = {250},
pages = {108188},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108188},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724001846},
author = {Manuel Menor-Flores and Miguel A. Vega-Rodríguez},
keywords = {Protein-protein interaction, Network alignment, Multi-objective optimization, Gene ontology consistency, Symmetric substructure score},
abstract = {Background and Objective
The protein-protein interaction (PPI) network alignment has proven to be an efficient technique in the diagnosis and prevention of certain diseases. However, the difficulty in maximizing, at the same time, the two qualities that measure the goodness of alignments (topological and biological quality) has led aligners to produce very different alignments. Thus making a comparative study among alignments of such different qualities a big challenge. Multi-objective optimization is a computer method, which is very powerful in this kind of contexts because both conflicting qualities are considered together. Analysing the alignments of each PPI network aligner with multi-objective methodologies allows you to visualize a bigger picture of the alignments and their qualities, obtaining very interesting conclusions. This paper proposes a comprehensive PPI network aligner study in the multi-objective domain.
Methods
Alignments from each aligner and all aligners together were studied and compared to each other via Pareto dominance methodologies. The best alignments produced by each aligner and all aligners together for five different alignment scenarios were displayed in Pareto front graphs. Later, the aligners were ranked according to the topological, biological, and combined quality of their alignments. Finally, the aligners were also ranked based on their average runtimes.
Results
Regarding aligners constructing the best overall alignments, we found that SAlign, BEAMS, SANA, and HubAlign are the best options. Additionally, the alignments of best topological quality are produced by: SANA, SAlign, and HubAlign aligners. On the contrary, the aligners returning the alignments of best biological quality are: BEAMS, TAME, and WAVE. However, if there are time constraints, it is recommended to select SAlign to obtain high topological quality alignments and PISwap or SAlign aligners for high biological quality alignments.
Conclusions
The use of the SANA aligner is recommended for obtaining the best alignments of topological quality, BEAMS for alignments of the best biological quality, and SAlign for alignments of the best combined topological and biological quality. Simultaneously, SANA and BEAMS have above-average runtimes. Therefore, it is suggested, if necessary due to time restrictions, to choose other, faster aligners like SAlign or PISwap whose alignments are also of high quality.}
}
@article{MIHAI2024104122,
title = {Far-right Ecology and Geopolitical Resentment at Europe’s Periphery: The Case of Romania’s “Conservative Revolution”},
journal = {Geoforum},
volume = {156},
pages = {104122},
year = {2024},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2024.104122},
url = {https://www.sciencedirect.com/science/article/pii/S0016718524001830},
author = {Mihaela Mihai and Camil Ungureanu},
keywords = {Far-right ecologism, Far-right populism, Anti-capitalism, Anticolonialism, Geopolitical resentment, The Alliance for the Union of Romanians},
abstract = {Building on insights from political geography and the social sciences, this paper illuminates the diversity of European far-right politics in general and far-right ecologism in particular by contextually examining a party at Europe’s margins—the Alliance for the Union of Romanians (AUR). Based on a discursive thematic analysis, our objective is to show how AUR challenges existing theoretical accounts, predominantly tethered to the Western and Central European experiences. While most influential scholars emphasize far right’s culturalized view of religion and the fixation on immigration, AUR outlines a theological vision of politics and perceives emigration as a critical problem. Moreover, it co-opts the language of anticolonialism to articulate a socio-ecological critique of global extractive capitalism in a semi-peripheral context. These specificities are essential for understanding the party’s outlier position within far-right ecologism: AUR places the environment at the very centre of its programme—and not merely as a strategic add-on to attract voters or respond to domestic or external pressures. To substantiate our claims, we reconstruct three dimensions of its hyper-nationalist, Orthodox geographical imaginary: AUR’s complex, human, and natural resource nationalism; its focus on food sovereignty and the Romanian peasant as an exemplar of sustainable agriculture; and the protection of “the last virgin forests in Europe” as central to Romania’s national identity and prosperity. We conclude that AUR effectively mobilizes historical geopolitical resentment at Europe’s margins and addresses it with a promise of recovered plenitude that endangers democratic politics.}
}
@article{ZHANG2022104168,
title = {AdaDiag: Adversarial Domain Adaptation of Diagnostic Prediction with Clinical Event Sequences},
journal = {Journal of Biomedical Informatics},
volume = {134},
pages = {104168},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104168},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422001794},
author = {Tianran Zhang and Muhao Chen and Alex A.T. Bui},
keywords = {Domain adaptation, Heart failure, Transformers, Clinical event sequence modeling},
abstract = {Early detection of heart failure (HF) can provide patients with the opportunity for more timely intervention and better disease management, as well as efficient use of healthcare resources. Recent machine learning (ML) methods have shown promising performance on diagnostic prediction using temporal sequences from electronic health records (EHRs). In practice, however, these models may not generalize to other populations due to dataset shift. Shifts in datasets can be attributed to a range of factors such as variations in demographics, data management methods, and healthcare delivery patterns. In this paper, we use unsupervised adversarial domain adaptation methods to adaptively reduce the impact of dataset shift on cross-institutional transfer performance. The proposed framework is validated on a next-visit HF onset prediction task using a BERT-style Transformer-based language model pre-trained with a masked language modeling (MLM) task. Our model empirically demonstrates superior prediction performance relative to non-adversarial baselines in both transfer directions on two different clinical event sequence data sources.}
}
@article{LU2024102380,
title = {Privacy-preserving data integration and sharing in multi-party IoT environments: An entity embedding perspective},
journal = {Information Fusion},
volume = {108},
pages = {102380},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102380},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524001581},
author = {Junyu Lu and Henry Leung and Nan Xie},
keywords = {Data integration & sharing, Privacy preservation, Entity embedding},
abstract = {The increasing prevalence of IoT applications highlights the urgency for insightful data fusion and information acquisition, boosting data integration and sharing needs. However, challenges arise in multi-party data sharing due to inherent data heterogeneity and privacy concerns. To address these issues, this paper discusses the feasibility of using embedding vectors as the semantic representation, aiming to enhance interoperability across diverse data sources and lay the foundation for natural language-based data querying. At the specific method level, this paper proposes an improved entity tree embedding algorithm to reduce information loss and ameliorate the representation of entity semantics. Additionally, a privacy preservation mechanism based on the entity embedding approach is introduced to provide privacy protection for text-based data. Experimental results on address data demonstrate the mechanism’s efficacy in achieving privacy protection comparable to the widely adopted 2D Laplace plane noise method. Furthermore, incorporating the entity tree embedding into the privacy mechanism could yield more robust and reasonable results regarding location privacy and service quality, signifying the validity of the entity embedding results.}
}
@article{DONHAUSER201911,
title = {Knowledge transfer in theoretical ecology: Implications for incommensurability, voluntarism, and pluralism},
journal = {Studies in History and Philosophy of Science Part A},
volume = {77},
pages = {11-20},
year = {2019},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2018.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0039368118301730},
author = {Justin Donhauser and Jamie Shaw},
keywords = {Model transfer, Voluntarism, Incommensurability, Pluralism, Models, Epistemology},
abstract = {Well-known epistemologies of science have implications for how best to understand knowledge transfer (KT). Yet, to date, no serious attempt has been made to explicate these particular implications. This paper infers views about KT from two popular epistemologies; what we characterize as incommensurabilitist views (after Devitt, 2001; Bird, 2002, 2008; Sankey and Hoyningen-Huene 2013) and voluntarist views (after Van Fraassen, 1984; Dupré, 2001; Chakravartty, 2015). We argue views of the former sort define the methodological, ontological, and social conditions under which research operates within ‘different worlds’ (to use Kuhn's expression), and entail that genuine KTs under those conditions should be difficult or even impossible. By contrast, more liberal voluntarist views recognize epistemological processes that allow for transfers across different sciences even under such conditions. After outlining these antithetical positions, we identify two kinds of KTs present in well-known episodes in the history of ecology—specifically, successful model transfers from chemical kinetics and thermodynamics into areas of ecological research—which reveal significant limitations of incommensurabilitist views. We conclude by discussing how the selected examples support a pluralistic voluntarism regarding KT.}
}
@article{MARTINSCOSTA2024866,
title = {ARID1B controls transcriptional programs of axon projection in an organoid model of the human corpus callosum},
journal = {Cell Stem Cell},
volume = {31},
number = {6},
pages = {866-885.e14},
year = {2024},
issn = {1934-5909},
doi = {https://doi.org/10.1016/j.stem.2024.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S1934590924001413},
author = {Catarina Martins-Costa and Andrea Wiegers and Vincent A. Pham and Jaydeep Sidhaye and Balint Doleschall and Maria Novatchkova and Thomas Lendl and Marielle Piber and Angela Peer and Paul Möseneder and Marlene Stuempflen and Siu Yu A. Chow and Rainer Seidl and Daniela Prayer and Romana Höftberger and Gregor Kasprian and Yoshiho Ikeuchi and Nina S. Corsini and Jürgen A. Knoblich},
keywords = {mSWI/SNF, ARID1B, SATB2, corpus callosum agenesis, axonogenesis, cerebral cortex, neural organoids},
abstract = {Summary
Mutations in ARID1B, a member of the mSWI/SNF complex, cause severe neurodevelopmental phenotypes with elusive mechanisms in humans. The most common structural abnormality in the brain of ARID1B patients is agenesis of the corpus callosum (ACC), characterized by the absence of an interhemispheric white matter tract that connects distant cortical regions. Here, we find that neurons expressing SATB2, a determinant of callosal projection neuron (CPN) identity, show impaired maturation in ARID1B+/− neural organoids. Molecularly, a reduction in chromatin accessibility of genomic regions targeted by TCF-like, NFI-like, and ARID-like transcription factors drives the differential expression of genes required for corpus callosum (CC) development. Through an in vitro model of the CC tract, we demonstrate that this transcriptional dysregulation impairs the formation of long-range axonal projections, causing structural underconnectivity. Our study uncovers new functions of the mSWI/SNF during human corticogenesis, identifying cell-autonomous axonogenesis defects in SATB2+ neurons as a cause of ACC in ARID1B patients.}
}
@article{ILIEVSKI2021107347,
title = {Dimensions of commonsense knowledge},
journal = {Knowledge-Based Systems},
volume = {229},
pages = {107347},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107347},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121006092},
author = {Filip Ilievski and Alessandro Oltramari and Kaixin Ma and Bin Zhang and Deborah L. McGuinness and Pedro Szekely},
keywords = {Commonsense knowledge, Semantics, Knowledge graphs, Reasoning},
abstract = {Commonsense knowledge is essential for many AI applications, including those in natural language processing, visual processing, and planning. Consequently, many sources that include commonsense knowledge have been designed and constructed over the past decades. Recently, the focus has been on large text-based sources, which facilitate easier integration with neural (language) models and application to textual tasks, typically at the expense of the semantics of the sources and their harmonization. Efforts to consolidate commonsense knowledge have yielded partial success, with no clear path towards a comprehensive solution. We aim to organize these sources around a common set of dimensions of commonsense knowledge. We survey a wide range of popular commonsense sources with a special focus on their relations. We consolidate these relations into 13 knowledge dimensions. This consolidation allows us to unify the separate sources and to compute indications of their coverage, overlap, and gaps with respect to the knowledge dimensions. Moreover, we analyze the impact of each dimension on downstream reasoning tasks that require commonsense knowledge, observing that the temporal and desire/goal dimensions are very beneficial for reasoning on current downstream tasks, while distinctness and lexical knowledge have little impact. These results reveal preferences for some dimensions in current evaluation, and potential neglect of others.}
}
@article{WACHTER2018148,
title = {Integrating multi-purpose natural language understanding, robot’s memory, and symbolic planning for task execution in humanoid robots},
journal = {Robotics and Autonomous Systems},
volume = {99},
pages = {148-165},
year = {2018},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2017.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S092188901730074X},
author = {Mirko Wächter and Ekaterina Ovchinnikova and Valerij Wittenbeck and Peter Kaiser and Sandor Szedmak and Wail Mustafa and Dirk Kraft and Norbert Krüger and Justus Piater and Tamim Asfour},
keywords = {Structural bootstrapping, Natural language understanding, Planning, Task execution, Object replacement, Humanoid robotics},
abstract = {We propose an approach for instructing a robot using natural language to solve complex tasks in a dynamic environment. In this study, we elaborate on a framework that allows a humanoid robot to understand natural language, derive symbolic representations of its sensorimotor experience, generate complex plans according to the current world state, and monitor plan execution. The presented development supports replacing missing objects and suggesting possible object locations. It is a realization of the concept of structural bootstrapping developed in the context of the European project Xperience. The framework is implemented within the robot development environment ArmarX. We evaluate the framework on the humanoid robot ARMAR-III in the context of two experiments: a demonstration of the real execution of a complex task in the kitchen environment on ARMAR-III and an experiment with untrained users in a simulation environment.}
}
@article{DASGUPTA2021,
title = {Adverse Drug Event Prediction Using Noisy Literature-Derived Knowledge Graphs: Algorithm Development and Validation},
journal = {JMIR Medical Informatics},
volume = {9},
number = {10},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/32730},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421000624},
author = {Soham Dasgupta and Aishwarya Jayagopal and Abel Lim {Jun Hong} and Ragunathan Mariappan and Vaibhav Rajan},
keywords = {adverse drug event, knowledge graph, Embedding of Semantic Predications, biomedical literature},
abstract = {Background
Adverse drug events (ADEs) are unintended side effects of drugs that cause substantial clinical and economic burdens globally. Not all ADEs are discovered during clinical trials; therefore, postmarketing surveillance, called pharmacovigilance, is routinely conducted to find unknown ADEs. A wealth of information, which facilitates ADE discovery, lies in the growing body of biomedical literature. Knowledge graphs (KGs) encode information from the literature, where the vertices and the edges represent clinical concepts and their relations, respectively. The scale and unstructured form of the literature necessitates the use of natural language processing (NLP) to automatically create such KGs. Previous studies have demonstrated the utility of such literature-derived KGs in ADE prediction. Through unsupervised learning of the representations (features) of clinical concepts from the KG, which are used in machine learning models, state-of-the-art results for ADE prediction were obtained on benchmark data sets.
Objective
Due to the use of NLP to infer literature-derived KGs, there is noise in the form of false positive (erroneous) and false negative (absent) nodes and edges. Previous representation learning methods do not account for such inaccuracies in the graph. NLP algorithms can quantify the confidence in their inference of extracted concepts and relations from the literature. Our hypothesis, which motivates this work, is that by using such confidence scores during representation learning, the learned embeddings would yield better features for ADE prediction models.
Methods
We developed methods to use these confidence scores on two well-known representation learning methods—DeepWalk and Translating Embeddings for Modeling Multi-relational Data (TransE)—to develop their weighted versions: Weighted DeepWalk and Weighted TransE. These methods were used to learn representations from a large literature-derived KG, the Semantic MEDLINE Database, which contains more than 93 million clinical relations. They were compared with Embedding of Semantic Predications, which, to our knowledge, is the best reported representation learning method using the Semantic MEDLINE Database with state-of-the-art results for ADE prediction. Representations learned from different methods were used (separately) as features of drugs and diseases to build classification models for ADE prediction using benchmark data sets. The methods were compared rigorously over multiple cross-validation settings.
Results
The weighted versions we designed were able to learn representations that yielded more accurate predictive models than the corresponding unweighted versions of both DeepWalk and TransE, as well as Embedding of Semantic Predications, in our experiments. There were performance improvements of up to 5.75% in the F1-score and 8.4% in the area under the receiver operating characteristic curve value, thus advancing the state of the art in ADE prediction from literature-derived KGs.
Conclusions
Our classification models can be used to aid pharmacovigilance teams in detecting potentially new ADEs. Our experiments demonstrate the importance of modeling inaccuracies in the inferred KGs for representation learning.}
}
@article{CAO2025104762,
title = {Novel machine learning model for predicting cancer drugs’ susceptibilities and discovering novel treatments},
journal = {Journal of Biomedical Informatics},
volume = {161},
pages = {104762},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104762},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424001801},
author = {Xiaowen Cao and Li Xing and Hao Ding and He Li and Yushan Hu and Yao Dong and Hua He and Junhua Gu and Xuekui Zhang},
keywords = {Drug susceptibility, Cancer treatment, Machine learning, Functional enrichment, Multi-task prediction, PI3K-Akt pathway, Combination therapy},
abstract = {Background and Objective
Timely treatment is crucial for cancer patients, so it’s important to administer the appropriate treatment as soon as possible. Because individuals can respond differently to a given drug due to their unique genomic profiles, we aim to use their genomic information to predict how various drugs will affect them and determine the best course of treatment.
Methods
We present Kernelized Residual Stacking (KRS), a new multi-task learning approach, and use it to predict the responses to anti-cancer drugs based on genomic data. We demonstrate the superior predictive performance of KRS, outperforming popular competitors, by utilizing the Genomics of Drug Sensitivity in Cancer (GDSC) study and the Cancer Cell Line Encyclopedia (CCLE) study. Downstream analysis of feature genes selected by KRS is conducted to discover novel therapies.
Results
We used two genomic studies to show that KRS outperforms a few popular competitors in predicting drugs’ susceptibilities. Through downstream analysis of feature genes selected by KRS, we found that the PI3K-Akt pathway could alter drugs’ susceptibilities, and its expression correlated positively with the hub gene ERBB2. We discovered eight novel small molecules based on these feature genes, which could be developed into novel combination therapies with anti-cancer drugs.
Conclusions
KRS outperforms competitors in prediction performance and selects feature genes highly correlated with drugs’ susceptibilities. Novel biological results are found by investigating KRS’s feature genes.}
}
@article{GALDO2024195,
title = {Artificial intelligence in paediatrics: Current events and challenges},
journal = {Anales de Pediatría (English Edition)},
volume = {100},
number = {3},
pages = {195-201},
year = {2024},
issn = {2341-2879},
doi = {https://doi.org/10.1016/j.anpede.2024.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S2341287924000383},
author = {Brais Galdo and Carla Pazos and Jerónimo Pardo and Alfonso Solar and Daniel Llamas and Enrique Fernández-Blanco and Alejandro Pazos},
keywords = {Artificial intelligence, 7P medicine, Machine learning, Paediatrics, Personalized medicine, Inteligencia artificial, Medicina de las 7P, Aprendizaje máquina, Pediatría, Medicina personalizada},
abstract = {This article examines the use of artificial intelligence (AI) in the field of paediatric care within the framework of the 7P medicine model (Predictive, Preventive, Personalized, Precise, Participatory, Peripheral and Polyprofessional). It highlights various applications of AI in the diagnosis, treatment and management of paediatric diseases as well as the role of AI in prevention and in the efficient management of health care resources and the resulting impact on the sustainability of public health systems. Successful cases of the application of AI in the paediatric care setting are presented, placing emphasis on the need to move towards a 7P health care model. Artificial intelligence is revolutionizing society at large and has a great potential for significantly improving paediatric care.
Resumen
Se examina el uso de la inteligencia artificial (IA) en el campo de la atención a la salud pediátrica dentro del marco de la "Medicina de las 7P" (Predictiva, Preventiva, Personalizada, Precisa, Participativa, Periférica y Poliprofesional). Se destacan diversas aplicaciones de la IA en el diagnóstico, el tratamiento y el control de enfermedades pediátricas, así como su papel en la prevención y en la gestión eficiente de los recursos médicos con su repercusión en la sostenibilidad de los sistemas públicos de salud. Se presentan casos de éxito de la aplicación de la IA en el ámbito pediátrico y se hace un gran énfasis en la necesidad de caminar hacia la Medicina de las 7P. La IA está revolucionando la sociedad en general ofreciendo un gran potencial para mejorar significativamente el cuidado de la salud en pediatría.}
}
@article{RIMASSA2025S-1780,
title = {Su1751: FIVE-YEAR OVERALL SURVIVAL (OS) AND OS BY TUMOR RESPONSE MEASURES FROM THE PHASE 3 HIMALAYA STUDY OF TREMELIMUMAB PLUS DURVALUMAB IN UNRESECTABLE HEPATOCELLULAR CARCINOMA (UHCC)},
journal = {Gastroenterology},
volume = {169},
number = {1, Supplement },
pages = {S-1780},
year = {2025},
issn = {0016-5085},
doi = {https://doi.org/10.1016/S0016-5085(25)04958-3},
url = {https://www.sciencedirect.com/science/article/pii/S0016508525049583},
author = {Lorenza Rimassa and Stephen L. Chan and Bruno Sangro and George Lau and Masatoshi Kudo and Valeriy V. Breder and Maria Varela and Oxana V. Crysler and Mohamed Bouattour and Tu {Van Dao} and Adilson Faccio and Junji Furuse and Long-Bin Jeng and Yoon Koo Kang and R. Kate Kelley and Michael J. Paskow and Mallory Makowsky and Di Ran and Alejandra Negro and Ghassan K. Abou-Alfa}
}
@article{BEHR20245699,
title = {Generating knowledge graphs through text mining of catalysis research related literature††Electronic supplementary information (ESI) available: https://github.com/AleSteB/CatalysisIE_Knowledge_Graph_Generator. See DOI: https://doi.org/10.1039/d4cy00369a},
journal = {Catalysis Science & Technology},
volume = {14},
number = {19},
pages = {5699-5713},
year = {2024},
issn = {2044-4753},
doi = {https://doi.org/10.1039/d4cy00369a},
url = {https://www.sciencedirect.com/science/article/pii/S2044475324004696},
author = {Alexander S. Behr and Diana Chernenko and Dominik Koßmann and Arjun Neyyathala and Schirin Hanf and Stephan A. Schunk and Norbert Kockmann},
abstract = {Structured research data management in catalysis is crucial, especially for large amounts of data, and should be guided by FAIR principles for easy access and compatibility of data. Ontologies help to organize knowledge in a structured and FAIR way. The increasing numbers of scientific publications call for automated methods to preselect and access the desired knowledge while minimizing the effort to search for relevant publications. While ontology learning can be used to create structured knowledge graphs, named entity recognition allows detection and categorization of important information in text. This work combines ontology learning and named entity recognition for automated extraction of key data from publications and organization of the implicit knowledge in a machine- and user-readable knowledge graph and data. CatalysisIE is a pre-trained model for such information extraction for catalysis research. This model is used and extended in this work based on a new data set, increasing the precision and recall of the model with regard to the data set. Validation of the presented workflow is presented on two datasets regarding catalysis research. Preformulated SPARQL-queries are provided to show the usability and applicability of the resulting knowledge graph for researchers.}
}
@article{MOLLAHASSANI2025630,
title = {Generating additional Engineering Knowledge in Smart Product Value Creation Networks},
journal = {Procedia CIRP},
volume = {136},
pages = {630-635},
year = {2025},
note = {35th CIRP Design 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.08.108},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125008613},
author = {Damun Mollahassani and Martin Becker and Andreas Emrich and Peter Fettke and Jens C. Göbel},
keywords = {Knowledge Base, Knowledge Development, Innovation Knowledge, Smart Products},
abstract = {Collaboration processes to enable model-based engineering of smart industrial product systems require the integration of heterogeneous engineering domains in different companies of dynamic value creation networks. To improve and accelerate engineering processes, a collaborative and compliant use of existing and developed system knowledge is crucial. This can be ensured by using a hybrid AI approach that reuses explicit design knowledge and Deep Learning-generated insights. The approach aims to map and cluster product- and process-related engineering knowledge by creating and linking heterogeneous engineering ontologies utilizing different nomenclatures. The engineering of the innovative function wireless charging point for mobile phones illustrates the approach.}
}
@article{LI2024100612,
title = {Building a knowledge graph to enrich ChatGPT responses in manufacturing service discovery},
journal = {Journal of Industrial Information Integration},
volume = {40},
pages = {100612},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100612},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000566},
author = {Yunqing Li and Binil Starly},
keywords = {Digital supply chain, Knowledge graph, ChatGPT, Manufacturing service discovery},
abstract = {Sourcing and identification of new manufacturing partners is crucial for manufacturing system integrators to enhance agility and reduce risk through supply chain diversification in the global economy. The advent of advanced large language models has captured significant interest, due to their ability to generate comprehensive and articulate responses across a wide range of knowledge domains. However, the system often falls short in accuracy and completeness when responding to domain-specific inquiries, particularly in areas like manufacturing service discovery. This research explores the potential of leveraging Knowledge Graphs in conjunction with ChatGPT to streamline the process for prospective clients in identifying small manufacturing enterprises. In this study, we propose a method that integrates bottom-up ontology with advanced machine learning models to develop a Manufacturing Service Knowledge Graph from an array of structured and unstructured data sources, including the digital footprints of small-scale manufacturers throughout North America. The Knowledge Graph and the learned graph embedding vectors are leveraged to tackle intricate queries within the digital supply chain network, responding with enhanced reliability and greater interpretability. The approach highlighted is scalable to millions of entities that can be distributed to form a global Manufacturing Service Knowledge Network Graph that can potentially interconnect multiple types of Knowledge Graphs that span industry sectors, geopolitical boundaries, and business domains. The dataset developed for this study, now publicly accessible, encompasses more than 13,000 manufacturers’ weblinks, manufacturing services, certifications, and location entity types.}
}
@article{WALDEMARIN201814,
title = {OBO to UML: Support for the development of conceptual models in the biomedical domain},
journal = {Journal of Biomedical Informatics},
volume = {80},
pages = {14-25},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S1532046418300340},
author = {Ricardo C. Waldemarin and Cléver R.G. {de Farias}},
keywords = {Conceptual modeling, Unified Modeling Language, UML Profile for the OBO Core Relations Ontology, OBO Foundry ontologies reuse},
abstract = {A conceptual model abstractly defines a number of concepts and their relationships for the purposes of understanding and communication. Once a conceptual model is available, it can also be used as a starting point for the development of a software system. The development of conceptual models using the Unified Modeling Language (UML) facilitates the representation of modeled concepts and allows software developers to directly reuse these concepts in the design of a software system. The OBO Foundry represents the most relevant collaborative effort towards the development of ontologies in the biomedical domain. The development of UML conceptual models in the biomedical domain may benefit from the use of domain-specific semantics and notation. Further, the development of these models may also benefit from the reuse of knowledge contained in OBO ontologies. This paper investigates the support for the development of conceptual models in the biomedical domain using UML as a conceptual modeling language and using the support provided by the OBO Foundry for the development of biomedical ontologies, namely entity kind and relationship types definitions provided by the Basic Formal Ontology (BFO) and the OBO Core Relations Ontology (OBO Core), respectively. Further, the paper investigates the support for the reuse of biomedical knowledge currently available in OBOFFF ontologies in the development these conceptual models. The paper describes a UML profile for the OBO Core Relations Ontology, which basically defines a number of stereotypes to represent BFO entity kinds and OBO Core relationship types definitions. The paper also presents a support toolset consisting of a graphical editor named OBO-RO Editor, which directly supports the development of UML models using the extensions defined by our profile, and a command-line tool named OBO2UML, which directly converts an OBOFFF ontology into a UML model.}
}
@article{BLOKZIJL2025104004,
title = {Sámi perspectives on energy justice and wind energy developments in Northern Norway},
journal = {Energy Research & Social Science},
volume = {122},
pages = {104004},
year = {2025},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2025.104004},
url = {https://www.sciencedirect.com/science/article/pii/S2214629625000854},
author = {Aniek Blokzijl and Elisabet Dueholm Rasch},
keywords = {Energy justice, Sámi, Reindeer, Wind energy, Ontology, Reindeer husbandry, Energy transition},
abstract = {This article uses a relational ontology lens to analyze how Sámi reindeer herders in Northern Norway experience wind energy developments and energy justice. While most research on just energy transitions tends to focus on distribution, procedural and recognition justice, decolonial environmental justice scholars have argued that this approach fails to fully capture how energy (in)justice is perceived by Indigenous peoples. Our study builds on these insights and explores how Sámi relational ontologies shape their perceptions of energy justice. Building on ethnographic fieldwork - including participant observation and interviews conducted in Guovdageaidnu - we seek to answer the question: How does relational ontology shape Sámi reindeer herders' perceptions of justice in the transition towards renewable energy? We conclude that for Sámi reindeer herders, a just energy transition not only depends on distributional, procedural and recognition justice, but that their perceptions of what is “just” in energy transitions also revolve around: 1) other-than-humans 2) multiple ways of being in the world 3) diverse ways of knowing 4) temporality and 5) historical processes of dispossession. By unravelling why Sámi perceive wind developments as unjust, this article shows how the transition towards renewable energy can deepen already existing injustices and that embracing alternative ontologies could pave the way for an energy transition that is also considered just by Indigenous peoples. In so doing, the article contributes to the emerging literature that analyses energy justice through an ontological lens.}
}
@article{KHARLAMOV201911,
title = {Semantically-enhanced rule-based diagnostics for industrial Internet of Things: The SDRL language and case study for Siemens trains and turbines},
journal = {Journal of Web Semantics},
volume = {56},
pages = {11-29},
year = {2019},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826818300520},
author = {Evgeny Kharlamov and Gulnar Mehdi and Ognjen Savković and Guohui Xiao and Elem Güzel Kalaycı and Mikhail Roshchin},
keywords = {Internet of Things, Ontology based data access, Trains, Turbines, Diagnostics, Signal processing rules},
abstract = {An Industrial Internet of Things (IoT) is a network of intelligent industrial equipment such as trains and power generating turbines that collect and share large amounts of data. These data are either generated by various sensors deployed in the equipment or captures equipment specific information such as configurations, history of use, and manufacturer. Diagnostics of the industrial IoT is critical to minimise the maintenance cost and downtime of its equipment. It is common that industry today employs rule-based diagnostic systems for this purpose. Rules are typically used to process signals from sensors installed in equipment by filtering, aggregating, and combining sequences of time-stamped measurements recorded by the sensors. Such rules are often data-dependent in the sense that they rely on specific characteristics of individual sensors and equipment. This dependence poses significant challenges in rule authoring, reuse, and maintenance by engineers especially when the rules are applied in industrial IoT scenarios. In this work we propose an approach to address these problems by relying on the well-known Ontology-Based Data Access approach: we propose to use ontologies to mediate the sensor signals and the rules. To this end, we propose a semantic rule language, SDRL, where signals are first class citizens. Our language offers a balance of expressive power, usability, and efficiency: it captures most of Siemens data-driven diagnostic rules, significantly simplifies authoring of diagnostic tasks, and allows to efficiently rewrite semantic rules from ontologies to data and execute over data. We implemented our approach in a semantic diagnostic system and evaluated it. For evaluation, we developed a use case of rail systems as well as power generating turbines at Siemens and conducted experiments to demonstrate both usability and efficiency of our solution.}
}
@article{ZHOU2025106206,
title = {Integrating domain-specific knowledge and fine-tuned general-purpose large language models for question-answering in construction engineering management},
journal = {Automation in Construction},
volume = {175},
pages = {106206},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106206},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002468},
author = {Shenghua Zhou and Xuefan Liu and Dezhi Li and Tiantian Gu and Keyan Liu and Yifan Yang and Mun On Wong},
keywords = {Question-answering, Construction engineering management, Knowledge base, Fine-tuning, Large language model},
abstract = {General-purpose Large Language Models (GLLMs) for Question-Answering (QA) of Construction Engineering Management (CEM) usually lack CEM knowledge and fine-tuning datasets, leading to unsatisfactory performance. Hence, this paper integrates the CEM External Knowledge Base (CEM-EKB) with out-of-domain fine-tuned GLLMs for CEM-QA. It encompasses (i) devising a process to develop the CEM-EKB with 235 documents, (ii) conducting out-of-domain fine-tuning to enhance GLLMs' abilities, (iii) integrating CEM-EKB with fine-tuned GLLMs, (iv) building CEM-QA test datasets with 5050 Multiple-Choice Questions (MCQs) and 100 Case-Based Questions (CBQs), and (v) comparing GLLMs' performance. The results indicate that CEM knowledge-incorporated fine-tuned GLLMs surpass original GLLMs by an average of 27.1 % in professional examinations, with an average improvement of 27.5 % across 7 CEM subdomains and 22.05 % for CBQs. This paper contributes to devising an effective, reusable, and updatable CEM-EKB; revealing the feasibility of out-of-domain datasets for fine-tuning; and sharing a large-scale CEM-QA test dataset.}
}
@article{ZHANG2025,
title = {A Knowledge-Enhanced Platform (MetaSepsisKnowHub) for Retrieval Augmented Generation–Based Sepsis Heterogeneity and Personalized Management: Development Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/67201},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125007873},
author = {Chi Zhang and Hao Yang and Xingyun Liu and Rongrong Wu and Hui Zong and Erman Wu and Yi Zhou and Jiakun Li and Bairong Shen},
keywords = {human sepsis, knowledge-enhanced, personalized application, retrieval augmented generation, precision medicine},
abstract = {Background
Sepsis is a severe syndrome of organ dysfunction caused by infection; it has high heterogeneity and high in-hospital mortality, representing a grim clinical challenge for precision medicine in critical care.
Objective
We aimed to extract reported sepsis biomarkers to provide users with comprehensive biomedical information and integrate retrieval augmented generation (RAG) and prompt engineering to enhance the accuracy, stability, and interpretability of clinical decisions recommended by large language models (LLMs).
Methods
To address the challenge, we established and updated the first knowledge-enhanced platform, MetaSepsisKnowHub, comprising 427 sepsis biomarkers and 423 studies, aiming to systematically collect and annotate sepsis biomarkers to guide personalized clinical decision-making in the diagnosis and treatment of human sepsis. We curated a tailored LLM framework incorporating RAG and prompt engineering and incorporated 2 performance evaluation scales: the System Usability Scale and the Net Promoter Score.
Results
The overall quantitative ratings of expert-reviewed clinical recommendations based on RAG surpassed baseline responses generated by 4 LLMs and showed a statistically significant improvement in textual questions (GPT-4: mean 75.79, SD 7.11 vs mean 81.59, SD 9.87; P=.02; GPT-4o: mean 70.36, SD 7.63 vs mean 77.98, SD 13.26; P=.02; Qwen2.5-instruct: mean 77.08 SD 3.75 vs mean 85.46, SD 7.27; P<.001; and DeepSeek-R1: mean 77.67, SD 3.66 vs mean 86.42, SD 8.56; P<.001), but no significant statistical differences could be measured in clinical scenarios. The RAG assessment score comparing RAG-based responses and expert-provided benchmark answers illustrated prominent factual correctness, accuracy, and knowledge recall compared to the baseline responses. After use, the average the System Usability Scale score was 82.20 (SD 14.17) and the Net Promoter Score was 72, demonstrating high user satisfaction and loyalty.
Conclusions
We highlight the pioneering MetaSepsisKnowHub platform, and we show that combining MetaSepsisKnowHub with RAG can minimize limitations on precision and maximize the breadth of LLMs to shorten the bench-to-bedside distance, serving as a knowledge-enhanced paradigm for future application of artificial intelligence in critical care medicine.}
}
@article{AKOKA2025102492,
title = {Data and knowledge engineering: Insights from forty years of publication},
journal = {Data & Knowledge Engineering},
volume = {160},
pages = {102492},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102492},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000874},
author = {Jacky Akoka and Isabelle Comyn-Wattiau and Nicolas Prat and Veda C. Storey},
keywords = {Conceptual modeling, Bibliometrics, Data and Knowledge Engineering journal, Intellectual structure, Pathways, Topic modeling, Research topics and themes, Data and process modeling, Query optimization and processing, Ontologies and knowledge representation, Machine learning, Specialized databases, Data privacy and security, Data integration and concurrency, Reasoning and expert systems, Information extraction and retrieval},
abstract = {The journal, Data and Knowledge Engineering (DKE), first published by Elsevier in 1985, has now been in existence for forty years. This journal has evolved and matured to play an important role in establishing and progressing research on conceptual modeling and related areas. To accurately characterize the history and current state of the research contributions and their impact, we analyze its publications in three phases, by employing bibliometric techniques of co-citation, bibliographic coupling, main path analysis, and topic modeling. Using descriptive bibliometrics, the results from the first phase provide an overview of the articles that have been published in the journal. It analyzes the dynamics and trend patterns of publications, specifically, their main topics and contributions. Using bibliometric mapping, the second phase identifies the journal's intellectual structure, its primary research themes, and the pathways through which knowledge is disseminated between the most influential articles. The third phase entails a comparison of DKE with other scientific journals that share at least some of its scope. In addition to delineating the strengths of DKE, we provide insights into how DKE might continue to evolve and progress the contributions to the field.}
}
@article{DROSTE2025108734,
title = {Evaluating transformative policies in complex land-use systems},
journal = {Ecological Economics},
volume = {238},
pages = {108734},
year = {2025},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2025.108734},
url = {https://www.sciencedirect.com/science/article/pii/S0921800925002174},
author = {Nils Droste and Huntley Brownell and Dalia D'Amato and Hanna Ekström and Alexia Fridén and Teemu Harrinkari and Bogomil Iliev and Wilhelm May and Ayonghe Nebasifu and Marianne Thomsen},
keywords = {Socio-ecological systems, Methodological pluralism, Integrated policy assessment, Land use change, Transformation},
abstract = {Policies that facilitate sustainability transformations require knowledge about the dynamics of complex socio-ecological systems, including biophysical mechanisms and diverse human-nature relationships. Such a comprehensive evidence base can only be built by integrating multiple types of knowledges. Ontology, epistemology, and semantics are a well-established terminology to structure and facilitate such knowledge integration. Co-creation with societal knowledge-holders can furthermore generate a more robust understanding of societal processes. Here, we present an approach that we call integrated policy assessment and use the case of Nordic forest policies to illustrate how such an integration can look in practice. We present three guiding principles to coordinate such transdisciplinary socio-ecological modelling: 1) a theory of change as a shared ontological ground about the structure of the system and causal mechanisms therein, 2) a modular architecture that integrates epistemologically distinct approaches and operationalizes data flows between various models, methods and scales, 3) a co-creative procedure that can create a shared problem understanding to semantically integrate knowledges from multiple stakeholders and address societal challenges in a relevant and legitimate way. The general idea of such co-creative modular architecture for integrated policy assessments can in principle be applied to any land use policy nexus.}
}
@article{CARLIN2024100639,
title = {An interactive framework to support decision-making for Digital Twin design},
journal = {Journal of Industrial Information Integration},
volume = {41},
pages = {100639},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100639},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24000839},
author = {H M Carlin and P A Goodall and R I M Young and A A West},
keywords = {Digital twin, Decision-support, Design framework, Ontology},
abstract = {Producing a Digital Twin (DT) involves many inter-linking decisions. Existing research tends to describe the parts of a DT and how they work, but not the decision-making that goes into building a DT nor the consideration of alternative design options. There is therefore a need for decision support to guide developers to create DTs efficiently while meeting functional requirements such as accuracy and interoperability. This paper presents an ontology-based decision support framework to achieve this need. Firstly an analysis of the decisions required to create a predictive maintenance DT for an automotive manufacturer is performed. The analysis found that each decision point produces an output by consideration of various influencing factors, such as time constraints, computation limits and the required fidelity of the model. The network of decisions is complex, with the outcomes of earlier decisions influencing later ones. An IDEF0 diagram was found to be a useful way to represent decisions, their dependencies and their cross-linking. This knowledge was used to populate an ontology of DT components for a predictive maintenance DT. The ability of an ontology to describe concepts explicitly using standardised vocabulary ensures the integrity of the decision-making guidance. A demonstration of the functionality of the ontology-based decision support framework was made before an evaluation of the concept. The research is a fundamental component in producing decision support for DT creators so that manufacturers can realise the benefits of a connected, responsive and flexible facility.}
}
@article{SUN2025103227,
title = {The role of natural language processing in improving cancer care: A scoping review with narrative synthesis},
journal = {Artificial Intelligence in Medicine},
volume = {168},
pages = {103227},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103227},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725001629},
author = {Mengxuan Sun and Ehud Reiter and Lisa Duncan and Rosalind Adam},
keywords = {Natural language processing, Cancer care, Patient education, Summarise report, Record keeping, Evidence-based decision making},
abstract = {Objectives
To review studies of Natural Language Processing (NLP) systems that assist in cancer care, explore use cases and summarise current research progress.
Methods
A scoping review, searching six databases (1) MEDLINE, (2) Embase, (3) IEEE Xplore, (4) ACM Digital Library, (5) Web of Science, and (6) ACL Anthology. Studies were included that reported NLP systems that had been used to improve cancer management by patients or clinicians. Studies were synthesised descriptively and using content analysis.
Results
Twenty-nine studies were included. Studies mainly applied NLP in mixed cancer types (n = 10, 34.48 %) and breast cancer (n = 8, 27.59 %). NLP was used in four main ways: (1) to support patient education and self-management; (2) to improve efficiency in clinical care by summarising, extracting, and categorising data, and supporting record-keeping; (3) to support prevention and early detection of patient problems or cancer recurrence; and (4) to improve cancer treatment by supporting clinicians to make evidence-based treatment decisions. Studies highlighted a wide variety of use cases for NLP technologies in cancer care. However, few technologies have been evaluated within clinical settings, none have been evaluated against clinical outcomes, and none have been implemented into clinical care.
Conclusion
NLP has the potential to improve cancer care via several mechanisms, including information extraction and classification, which could enable automation and personalization of care processes. Additionally, NLP tools such as chatbots show promise in improving patient communication and support. However, there are deficiencies in the evaluation and clinical integration challenges. Interdisciplinary collaboration between computer scientists and clinicians will be essential if NLP technologies are to fulfil their potential to improve patient experience and outcomes. Registered Protocol: https://doi.org/10.17605/OSF.IO/G9DSR}
}
@article{PANCHENDRARAJAN2024124097,
title = {Synergizing machine learning & symbolic methods: A survey on hybrid approaches to natural language processing},
journal = {Expert Systems with Applications},
volume = {251},
pages = {124097},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124097},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424009631},
author = {Rrubaa Panchendrarajan and Arkaitz Zubiaga},
keywords = {Hybrid NLP, Machine learning, Symbolic methods, Hybrid approaches, Natural language processing},
abstract = {The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP). While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks. Meanwhile, the symbolic methods excel in representing knowledge-rich data. However, they struggle to adapt dynamic data and generalize the knowledge. Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths. Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks. In this paper, we present an overview of hybrid approaches used for NLP. Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generation, and reasoning. Furthermore, we discuss the existing resources available for hybrid approaches for NLP along with the challenges and future directions, offering a roadmap for future research avenues.}
}
@article{ALSUDIAS2021,
title = {Social Media Monitoring of the COVID-19 Pandemic and Influenza Epidemic With Adaptation for Informal Language in Arabic Twitter Data: Qualitative Study},
journal = {JMIR Medical Informatics},
volume = {9},
number = {9},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/27670},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421002829},
author = {Lama Alsudias and Paul Rayson},
keywords = {Arabic, COVID-19, infectious disease, influenza, infodemiology, infoveillance, social listening, informal language, multilabel classification, natural language processing, named entity recognition, Twitter},
abstract = {Background
Twitter is a real-time messaging platform widely used by people and organizations to share information on many topics. Systematic monitoring of social media posts (infodemiology or infoveillance) could be useful to detect misinformation outbreaks as well as to reduce reporting lag time and to provide an independent complementary source of data compared with traditional surveillance approaches. However, such an analysis is currently not possible in the Arabic-speaking world owing to a lack of basic building blocks for research and dialectal variation.
Objective
We collected around 4000 Arabic tweets related to COVID-19 and influenza. We cleaned and labeled the tweets relative to the Arabic Infectious Diseases Ontology, which includes nonstandard terminology, as well as 11 core concepts and 21 relations. The aim of this study was to analyze Arabic tweets to estimate their usefulness for health surveillance, understand the impact of the informal terms in the analysis, show the effect of deep learning methods in the classification process, and identify the locations where the infection is spreading.
Methods
We applied the following multilabel classification techniques: binary relevance, classifier chains, label power set, adapted algorithm (multilabel adapted k-nearest neighbors [MLKNN]), support vector machine with naive Bayes features (NBSVM), bidirectional encoder representations from transformers (BERT), and AraBERT (transformer-based model for Arabic language understanding) to identify tweets appearing to be from infected individuals. We also used named entity recognition to predict the place names mentioned in the tweets.
Results
We achieved an F1 score of up to 88% in the influenza case study and 94% in the COVID-19 one. Adapting for nonstandard terminology and informal language helped to improve accuracy by as much as 15%, with an average improvement of 8%. Deep learning methods achieved an F1 score of up to 94% during the classifying process. Our geolocation detection algorithm had an average accuracy of 54% for predicting the location of users according to tweet content.
Conclusions
This study identified two Arabic social media data sets for monitoring tweets related to influenza and COVID-19. It demonstrated the importance of including informal terms, which are regularly used by social media users, in the analysis. It also proved that BERT achieves good results when used with new terms in COVID-19 tweets. Finally, the tweet content may contain useful information to determine the location of disease spread.}
}
@article{RAZAVIPOUR2023100760,
title = {Classroom writing assessment and feedback practices: A new materialist encounter},
journal = {Assessing Writing},
volume = {57},
pages = {100760},
year = {2023},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2023.100760},
url = {https://www.sciencedirect.com/science/article/pii/S1075293523000685},
author = {Kioumars Razavipour},
keywords = {Writing assessment literacy, New materialisms, Assemblage, Feedback practices, Flat ontology},
abstract = {The mainstream approach to teacher assessment literacy seems to be founded on a (post)positivist paradigm leading to an autonomous model of literacy comprised of generic knowledge and skills. This paradigm obscures the non-cognitive, embodied, and affective dimensions of assessment practices. In this conceptual inquiry, I use the New Materialist philosophy to make sense of writing assessment literacy and feedback practices. In New Materialisms, the materiality of everything is emphasized, ontology is flat, reality is becoming, agency is relational, knowledge is entangled practice, and language is a resource in communicative assemblage. Using the noted conceptual tools, I try to provide a materialized conceptualization of writing assessment and feedback practice arguing that from a New Materialist perspective, feedback practices are an assemblage of rhetoric, IELTS, institution, materiality, art, cross-lingual resources, social relations, affect, and embodiment.}
}
@article{ZARE2025106944,
title = {Gene co-expression patterns shared between chemobrain and neurodegenerative disease models in rodents},
journal = {Neurobiology of Disease},
volume = {211},
pages = {106944},
year = {2025},
issn = {0969-9961},
doi = {https://doi.org/10.1016/j.nbd.2025.106944},
url = {https://www.sciencedirect.com/science/article/pii/S0969996125001603},
author = {Mohammad-Sajad Zare and Navid Abedpoor and Fatemeh Hajibabaie and Adam K. Walker},
keywords = {Cancer-related cognitive impairment, Neuroinflammation, Parkinson’s disease, Alzheimer’s disease, Synaptic transmission, Neuroprotection, Animal models},
abstract = {Chemotherapy-related cognitive impairment (CRCI), is a well-recognized phenomenon in cancer patients who have undergone chemotherapy but the exact molecular mechanisms underpinning CRCI remain elusive. Symptoms reported by people with CRCI resemble those experienced by people with age-related neurodegenerative disorders (ARNDDs), yet no clear connection between CRCI and ARNDDs has been reported to date. The existence of shared mechanisms between these conditions offers opportunities for repurposing drugs already approved for the treatment of ARNDDs to improve symptoms of CRCI. Given that there is no available microarray or RNA-Seq data from the brains of people who have experienced CRCI, we investigated to what extent brain gene expression perturbations from validated rodent models of CRCI induced by chemotherapy compared with validated rodent models of Alzheimer’s disease and Parkinson’s disease. We utilized multiple bioinformatic analyses, including functional enrichment, protein-protein interaction network analyses, gene ontology analyses and identification of hub genes to reveal connections between comparable gene expression perturbations observed in these conditions. Collectively 165 genes overlapped between CRCI and Parkinson’s disease and/or Alzheimer’s disease, and 15 overlapped between all three conditions. The joint genes between Alzheimer’s disease, Parkinson’s disease and CRCI demonstrate an average of 83.65% nucleotide sequence similarity to human orthologues. Gene ontology and pathway enrichment analyses suggest mechanisms involved in neural activity and inflammatory response as the key components of the studied neuropathological conditions. Accordingly, genes in which expression was comparably affected in all three condition models could be attributed to neuroinflammation, cell cycle arrest, and changes in physiological neural activity.}
}
@article{ZHANG2025103214,
title = {Logic Augmented Multi-Decision Fusion Framework for Stance Detection on Social Media},
journal = {Information Fusion},
volume = {122},
pages = {103214},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103214},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525002878},
author = {Bowen Zhang and Jun Ma and Xianghua Fu and Genan Dai},
keywords = {Stance detection, First-order logic, Chain-of-thought},
abstract = {Stance detection in social media has become increasingly crucial for understanding public opinions on controversial issues. While large language models (LLMs) have shown promising results in stance detection, existing methods face challenges in reconciling inconsistent predictions and logical reasoning processes across different LLMs. To address these limitations, we propose LogiMDF, a Logic Augmented Multi-Decision Fusion framework that effectively integrates multiple LLMs’ decision processes through a unified logical framework. Our approach first employs zero-shot prompting to extract first-order logic (FOL) rules representing each LLM’s prediction rationale, then constructs a Logical Fusion Schema (LFS) to bridge different LLMs’ knowledge representations. We further develop a Multi-view Hypergraph Convolutional Network (MvHGCN) that effectively models and encodes the integrated logical knowledge. Extensive experiments on benchmark datasets demonstrate that LogiMDF significantly outperforms existing methods, achieving state-of-the-art performance in stance detection tasks. The results confirm that our framework effectively leverages the complementary strengths of multiple LLMs while maintaining consistent logical reasoning across different targets.}
}
@article{ZOU2025129589,
title = {A novel large language model enhanced joint learning framework for fine-grained sentiment analysis on drug reviews},
journal = {Neurocomputing},
volume = {626},
pages = {129589},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129589},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225002619},
author = {Haochen Zou and Yongli Wang},
keywords = {Fine-grained sentiment analysis, Aspect-based sentiment analysis, Large language model, Health informatics},
abstract = {Patient feedback on drug reviews extracted from social media platforms and online forums provides genuine sentiment information regarding post-medication usage. The insights are invaluable for prospective patients seeking appropriate medical guidance and stakeholders within the biomedical industry aiming to improve products. This paper presents a novel joint learning framework for fine-grained aspect-based sentiment analysis on drug reviews. By leveraging prior biomedical knowledge from the domain-specific pre-trained large language model, we address the challenge of fine-grained aspect-based sentiment analysis by collaboratively integrating both coarse and fine-grained contextual features within the text content, capturing precise biomedical aspect terms and the corresponding sentiment polarities. To the best of our knowledge, this work pioneers the initial introduction of a joint learning framework based on the fine-tuned biomedical large language model for fine-grained aspect-based sentiment analysis within drug reviews. By conducting extensive experiments on publicly available drug review datasets and comparing the constructed architecture with state-of-the-art techniques, the joint learning framework outperforms baseline competitors across evaluation metrics.}
}
@article{PERNECKY2025100901,
title = {Postleisure: Disrupting the Disciplinary Fixity of Leisure Thinking},
journal = {Journal of Outdoor Recreation and Tourism},
volume = {51},
pages = {100901},
year = {2025},
issn = {2213-0780},
doi = {https://doi.org/10.1016/j.jort.2025.100901},
url = {https://www.sciencedirect.com/science/article/pii/S2213078025000477},
author = {Tomas Pernecky},
keywords = {Constructionism, Being, Becoming, Empirical ontology, Postleisure, Ontology, Postdisciplinary},
abstract = {This paper is a postdisciplinary exploration of leisure and the conceptual corollaries of recreation and adventure. It seeks to broaden the ontological discourse in the field and demonstrate that alternative approaches to theorising about and studying leisure, recreation, and tourism are possible – if not necessary – amid concerns and critiques stemming from posthumanism, climate change, decoloniality, and the mobilities of hope and despair. It is argued that leisure as an object of inquiry has been largely possible due to the fragmentation of being, namely the creation of dichotomies that juxtapose different states of being. By dismantling the disciplinary confines of leisure, it is shown that leisure and recreation can be reconsidered vis-à-vis empirical ontology as deeper engagement with questions of being and becoming in lived contexts and in relation to other entities and things. The suggested pathway of thinking beyond leisure might be valued particularly by emerging conceptual and ethical pioneers keen to reexamine and reimagine how we are in and become with the world.}
}
@article{KOROLEVA2019100058,
title = {Measuring semantic similarity of clinical trial outcomes using deep pre-trained language representations},
journal = {Journal of Biomedical Informatics},
volume = {100},
pages = {100058},
year = {2019},
note = {Articles initially published in Journal of Biomedical Informatics: X 1-4, 2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.yjbinx.2019.100058},
url = {https://www.sciencedirect.com/science/article/pii/S2590177X19300575},
author = {Anna Koroleva and Sanjay Kamath and Patrick Paroubek},
keywords = {Trial outcomes, Semantic similarity, Natural Language Processing, Deep learning, Pre-trained language representations, Spin detection},
abstract = {Background
Outcomes are variables monitored during a clinical trial to assess the impact of an intervention on humans’ health.Automatic assessment of semantic similarity of trial outcomes is required for a number of tasks, such as detection of outcome switching (unjustified changes of pre-defined outcomes of a trial) and implementation of Core Outcome Sets (minimal sets of outcomes that should be reported in a particular medical domain).
Objective
We aimed at building an algorithm for assessing semantic similarity of pairs of primary and reported outcomes.We focused on approaches that do not require manually curated domain-specific resources such as ontologies and thesauri.
Methods
We tested several approaches, including single measures of similarity (based on strings, stems and lemmas, paths and distances in an ontology, and vector representations of phrases), classifiers using a combination of single measures as features, and a deep learning approach that consists in fine-tuning pre-trained deep language representations.We tested language models provided by BERT (trained on general-domain texts), BioBERT and SciBERT (trained on biomedical and scientific texts, respectively).We explored the possibility of improving the results by taking into account the variants for referring to an outcome (e.g.the use of a measurement tool name instead on the outcome name; the use of abbreviations).We release an open corpus with annotation for similarity of pairs of outcomes.
Results
Classifiers using a combination of single measures as features outperformed the single measures, while deep learning algorithms using BioBERT and SciBERT models outperformed the classifiers.BioBERT reached the best F-measure of 89.75%.The addition of variants of outcomes did not improve the results for the best-performing single measures nor for the classifiers, but it improved the performance of deep learning algorithms: BioBERT achieved an F-measure of93.38%.
Conclusions
Deep learning approaches using pre-trained language representations outperformed other approaches for similarity assessment of trial outcomes, without relying on any manually curated domain-specific resources (ontologies and other lexical resources). Addition of variants of outcomes further improved the performance of deep learning algorithms.}
}
@article{QI2025497,
title = {Mixed-model product disassembly sequence optimization based on cognitive digital twin},
journal = {Journal of Manufacturing Systems},
volume = {82},
pages = {497-508},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525001803},
author = {Lei Qi and Wenjun Xu and Kaipu Wang and Jiayi Liu and Xun Ye and Hang Yang and Yi Zhong},
keywords = {Cognitive digital twins, Disassembly sequence optimization, Mixed-model product disassembly process, Deep reinforcement learning},
abstract = {As a core step in remanufacturing, the disassembly process for multiple product structures in mixed-model products can improve disassembly efficiency and reduce costs. There are structure uncertainties in the mixed-model product disassembly process, which must be considered and used to optimize the disassembly strategy and improve the disassembly efficiency. This paper proposes a framework of a cognitive digital twin for mixed-model product disassembly sequence optimization. The cognitive model can reason, predict, and complete missing disassembly information due to uncertainty in the mixed-model product structure. Its cognitive capability is achieved by a knowledge graph and a TransD-based method. To provide a basis for semantic inference and relate different knowledge types, an ontology is designed based on the digital twin, and a knowledge graph is developed. Finally, a cognitive digital twin model is built. Upon that, the Soft Actor-Critic algorithm is utilized to optimize the mixed-model product sequence. The proposed model and algorithm are applied to transmissions disassembly case. The results show that the proposed method is effective in optimizing the disassembly sequences of three different products that make up the mixed-model products. It not only realizes the disassembly sequence optimization under the uncertain product structures, but also reduces the disassembly time of individual products and all products.}
}
@article{K20252996,
title = {AI-Driven Multi-Modal Information Synthesis: Integrating PDF Querying, Speech Summarization, and Cross-Language Text Summarization},
journal = {Procedia Computer Science},
volume = {258},
pages = {2996-3018},
year = {2025},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.04.559},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925016631},
author = {Suresh Manic K and Ahmed-Al Balushi and Al-Bemani A.S. and Saleh {Al Araimi} and Balaji G and Uma Suresh and Asiya Najeeb},
keywords = {Artifical Intelligence (AI), PDF Question Answering System, Speech-to-Text Summarization, Multisource Text Summarization, Natural Language Processing (NLP)},
abstract = {This research presents an innovative AI-powered system designed to revolutionize information retrieval and summarization by integrating multiple data modalities. The system is composed of three key components: a PDF Question Answering System, Speech-to-Text Summarization, and Multi-Source Text Summarization with Cross-Language Translation capabilities. The PDF Question Answering System processes documents by segmenting them into 5000-word chunks and generating embeddings using the all-MiniLM-L6-v2 model. These embeddings are then stored in ChromaDB, a specialized vector database, enabling precise querying through similarity searches. The Speech-to-Text Summarization feature converts audio files or live streams into text using the OpenAI Whisper model. It then creates a summary of the text, which can be translated into different languages, making it easier for users to access the information. The Multi-Source Text Summarization and Translation module further broadens the system’s capabilities by processing content from various sources, including PDFs, YouTube videos, and websites. This content is summarized using the Gemma-7b-It model, with additional translation options available to the user. Test results showed that the system accurately converts speech to text, even in difficult audio conditions, with very few mistakes. It creates clear and concise summaries that keep the important details and processes tasks quickly. For example, it converts 5 minutes of audio into text in about 2 seconds and answers questions from PDF documents in less than 12 seconds. These results demonstrate the system’s ability to handle large amounts of data and different types of content efficiently, making it a flexible tool for users who need to collect and summarize information from multiple sources. It also supports multiple languages through its built-in translation feature.}
}
@article{ZHAO2025112385,
title = {Behavioral decision-making and safety verification approaches for autonomous driving system in extreme scenarios},
journal = {Journal of Systems and Software},
volume = {226},
pages = {112385},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112385},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225000536},
author = {Ying Zhao and Yi Zhu and Li Zhao and Junge Huang and Qiang Zhi},
keywords = {Automatic drive, Bayesian network, Behavior decision, Model verification, UPPAAL-SMC},
abstract = {Autonomous vehicles are crucial for improving traffic efficiency and reducing accidents, yet the complexity of driving scenarios and behavioral uncertainty pose challenges for decision-making. Recent research integrates virtual simulation with decision algorithms to enhance system intelligence and performance. Nonetheless, the potential hazards associated with extreme weather conditions are often overlooked. To mitigate this issue, this paper proposes a Bayesian network decision-making model based on hazard probability inference. The model enables the driver assistance system to take over the control of the vehicle in extreme scenarios and dynamically adjust decision strategies based on the potential hazard values under multivariate data. First, safety elements of sporadic hazardous scenarios are extracted using the Accidental and Catastrophic Automatic Driving Scenario Modeling Language and used as nodes to construct a Bayesian network for inferring potential driving hazards. Second, a Bayesian decision-making model is designed based on the semantic hierarchy of the autonomous driving system domain ontology, aiming to derive the optimal driving behavior for the current vehicle in extreme scenarios. The safety of these decisions is verified using the UPPAAL-SMC statistical model checker. Finally, the model’s validity is confirmed through a real-world autonomous vehicle accident, with results indicating more rational decisions and improved safety performance.}
}
@article{TAUQEER2023121049,
title = {Smell and Taste Disorders Knowledge Graph: Answering Questions Using Health Data},
journal = {Expert Systems with Applications},
volume = {234},
pages = {121049},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121049},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423015518},
author = {Amar Tauqeer and Ismaheel Hammid and Sareh Aghaei and Parvaneh Parvin and Elbrich M. Postma and Anna Fensel},
keywords = {Chemosensory dysfunction, Semantic modeling, Health, Data sharing, Knowledge graph, Ontology, Question answering user interface},
abstract = {Smell and taste disorders have become a more prominent issue due to their association with Covid-19, and their impact on quality of life and health outcomes. However, pertinent information regarding these disorders is often inaccessible and poorly organized, with the majority of data stored solely in clinical data repositories. To rectify this, a technological solution capable of digitizing, semantically modeling, and integrating health data is necessary. The knowledge graph, an emerging technology capable of organizing inconsistent and heterogeneous health data and inferring implicit knowledge, presents a viable solution to this problem. In pursuit of the aforementioned goal, an existing ontology pertaining to smell and taste disorders was enriched by introducing additional relevant concepts and relationships. Subsequently, a knowledge graph was constructed based on the defined ontology and patients’ data. The resultant knowledge graph was subjected to a rigorous evaluation, encompassing dimensions such as completeness, coherency, coverage, and succinctness. The evaluation established the effectiveness and usability of the knowledge graph, with only minor issues detected through the OOPS! pitfall scanner. Furthermore, as a proof-of-concept for clinical application, a user interface was created, enabling users to access pertinent information concerning smell and taste disorders, including causative factors, medications, and etiology, among others. The interface generates a graph-based structure based on the selected question from a drop-down menu. The end-user can modify the query by merely clicking on the generated graph to ask related questions. This study showcases the potential of knowledge graphs centered on smell and taste disorders to organize and provide accessible health data to end-users.}
}
@article{LUO2024102904,
title = {Pre-trained language models in medicine: A survey},
journal = {Artificial Intelligence in Medicine},
volume = {154},
pages = {102904},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102904},
url = {https://www.sciencedirect.com/science/article/pii/S0933365724001465},
author = {Xudong Luo and Zhiqi Deng and Binxia Yang and Michael Y. Luo},
keywords = {Natural language processing, Medical science, Healthcare, Pre-trained language model, BERT, GPT},
abstract = {With the rapid progress in Natural Language Processing (NLP), Pre-trained Language Models (PLM) such as BERT, BioBERT, and ChatGPT have shown great potential in various medical NLP tasks. This paper surveys the cutting-edge achievements in applying PLMs to various medical NLP tasks. Specifically, we first brief PLMS and outline the research of PLMs in medicine. Next, we categorise and discuss the types of tasks in medical NLP, covering text summarisation, question-answering, machine translation, sentiment analysis, named entity recognition, information extraction, medical education, relation extraction, and text mining. For each type of task, we first provide an overview of the basic concepts, the main methodologies, the advantages of applying PLMs, the basic steps of applying PLMs application, the datasets for training and testing, and the metrics for task evaluation. Subsequently, a summary of recent important research findings is presented, analysing their motivations, strengths vs weaknesses, similarities vs differences, and discussing potential limitations. Also, we assess the quality and influence of the research reviewed in this paper by comparing the citation count of the papers reviewed and the reputation and impact of the conferences and journals where they are published. Through these indicators, we further identify the most concerned research topics currently. Finally, we look forward to future research directions, including enhancing models’ reliability, explainability, and fairness, to promote the application of PLMs in clinical practice. In addition, this survey also collect some download links of some model codes and the relevant datasets, which are valuable references for researchers applying NLP techniques in medicine and medical professionals seeking to enhance their expertise and healthcare service through AI technology.}
}
@article{KRUTIKHINA2022656,
title = {The fuzzy objects recognition in scientific and technical papers by means of natural languages processing technologies},
journal = {Procedia Computer Science},
volume = {213},
pages = {656-665},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922018178},
author = {T.A. Krutikhina and E.V. Antonov and K.V. Ionkina},
keywords = {NLP, physical parameters, ontology, search index, web service},
abstract = {The article describes an approach to build a search index for fuzzy objects recognition in scientific and technical papers. A fuzzy object is explained as an entity described by some physical parameters. A physical parameter consists of two parts: a value, represented in various forms of digits, and a physical unit, represented as text with some digits. The approach presented in the article allows users to search not by ordinary substrings, but by physical parameters with range, less than or greater than operators. The current realisation of the approach is a web service that has functionality to perform all stages for extraction of physical parameters of a fuzzy object. These stages are the extraction of sentences, the extraction of physical parameters and the normalisation of physical parameters values. The web service can be exploited in the synchronous and asynchronous modes. The synchronous mode helps to troubleshoot the software. The asynchronous mode is designed to process papers using queues in the distributed computer systems. The current work is aimed to develop a search engine which will address the challenge of structuring information of a continuously growing number of scientific and technical papers. The search index will help to perform robust navigation among a huge number of documents for scientific workers and engineers.}
}
@article{DIKMEN2025104251,
title = {Automated construction contract analysis for risk and responsibility assessment using natural language processing and machine learning},
journal = {Computers in Industry},
volume = {166},
pages = {104251},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2025.104251},
url = {https://www.sciencedirect.com/science/article/pii/S0166361525000168},
author = {Irem Dikmen and Gorkem Eken and Huseyin Erol and M. Talat Birgonul},
keywords = {Automated contract review, Natural Language Processing (NLP), Machine Learning (ML), Artificial Intelligence (AI), Text classification, Construction risk management},
abstract = {Construction contracts contain critical risk-related information that requires in-depth examination, yet tight schedules for bidding limit the possibility of comprehensive review of extensive documents manually. This research aims to develop models for automating the review of construction contracts to extract information on risk and responsibility that will provide inputs for risk management plans. Models were trained on 2268 sentences from International Federation of Consulting Engineers templates and tested on an actual construction project contract containing 1217 sentences. A taxonomy classified sentences into Heading, Definition, Obligation, Risk, and Right categories with related parties of Contractor, Employer, and Shared. Twelve models employing diverse Natural Language Processing vectorization techniques and Machine Learning algorithms were implemented and benchmarked based on accuracy and F1 score. Binary classification of sentence types and an ensemble method integrating top models were further applied to improve performance. The best model achieved 89 % accuracy for sentence types and 83 % for related parties, demonstrating the capabilities of automated contract review for identification of risk and responsibilities. Adopting the proposed approach can significantly expedite contract reviews to support risk management activities, bid preparation processes and prevent disputes caused by overlooking risks and responsibilities.}
}
@article{SIDDHARTH2024112410,
title = {Retrieval augmented generation using engineering design knowledge},
journal = {Knowledge-Based Systems},
volume = {303},
pages = {112410},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112410},
url = {https://www.sciencedirect.com/science/article/pii/S095070512401044X},
author = {L. Siddharth and Jianxi Luo},
keywords = {Knowledge graphs, Retrieval-augmented generation, Large-language models, Engineering design knowledge, Patent documents, Graph neural networks},
abstract = {Aiming to support Retrieval Augmented Generation (RAG) in the design process, we present a method to identify explicit, engineering design facts – {head entity:: relationship:: tail entity} from patented artefact descriptions. Given a sentence with a pair of entities (selected from noun phrases) marked in a unique manner, our method extracts their relationship that is explicitly communicated in the sentence. For this task, we create a dataset of 375,084 examples and fine-tune language models for relation identification (token classification task) and relation elicitation (sequence-to-sequence task). The token classification approach achieves up to 99.7% accuracy. Upon applying the method to a domain of 4,870 fan system patents, we populate a knowledge base of over 2.93 million facts. Using this knowledge base, we demonstrate how Large Language Models (LLMs) are guided by explicit facts to synthesise knowledge and generate technical and cohesive responses when sought out for knowledge retrieval tasks in the design process.}
}
@article{PAEK2024S18,
title = {CO14 Profiling Adverse Events in Multiple Myeloma: Insights from Clinical Trials Via Large Language Models},
journal = {Value in Health},
volume = {27},
number = {6, Supplement },
pages = {S18},
year = {2024},
note = {ISPOR Abstracts 2024},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2024.03.103},
url = {https://www.sciencedirect.com/science/article/pii/S1098301524002183},
author = {H. Paek and K. Lee and S. Datta and LC. Huang and J. Higashi and N. Ofoegbu and L. He and B. Lin and J. Wang and X. Wang}
}
@article{PATEL2024240,
title = {User Centered Non-Functional Requirements Specification – An Extended Use-Case Diagram},
journal = {Procedia Computer Science},
volume = {235},
pages = {240-249},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.026},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924007026},
author = {Krupa Patel and Tanvi Trivedi and Unnati Shah},
keywords = {Non-functional Requirements, NFRs Classification, Ontology, Extended Use-Case Diagram},
abstract = {Requirements Engineering (RE) activity is often initiated with vaguely specified software requirements. The ambiguities must be overcome to the extent possible when the requirements are specified. Both functional (FRs) and non-functional (NFRs) requirements should be correctly and unambiguously specified. Users communicate about the NFRs in informal discussions. Requirements engineers use formal or semi-formal language notations to manually convey the NFRs. A laborious and ineffective manual approach, however, is unable to identify all likely NFRs and resolve ambiguities in those NFRs. This paper discusses an approach that attempts to overcome ambiguities from natural language requirements, recognize NFRs, and generate NFR specification through the extended Unified Modelling Language (UML) viz. use-case diagram. The empirical evaluation of the proposed approach on the PROMISE dataset achieves an average result of 79.76% recall, 90.05% precision, and 84.59% F-measure. The proposed approach allows the requirements analyst to semi-automatically recognize NFRs and visualize them using an extended use-case diagram at an early stage of RE.}
}
@article{VOLPERT20242994,
title = {Compatibility Assessment for Interfaces in Drivetrains of Robot-Like Systems},
journal = {Procedia Computer Science},
volume = {232},
pages = {2994-3002},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.115},
url = {https://www.sciencedirect.com/science/article/pii/S187705092400293X},
author = {Marcus Volpert and Birgit Vogel-Heuser and Dominik Hujo and Karsten Stahl and Markus Zimmermann},
keywords = {robot-like systems, interface modeling, ontology, model-based systems engineering, drivetrains},
abstract = {Due to the complexity of robot-like systems (RLS), new developments are often avoided, and most variants of the systems are designed. The primary determinant of an RLS is its drivetrain, which comprises purchased components, namely the controller, motor driver, motor, and gearbox. Each of these purchased parts has different interfaces that are not standardized. If the interfaces are sufficiently complex, the compatibility of the parts can no longer be guaranteed manually. Therefore, this paper presents a draft ontology that verifies the compatibility of purchased parts in RLS drivetrains. Classes and properties are obtained from expert knowledge, norms, data standards, and data sheets to build an ontology. The ontology design is evaluated using an application example for a motor-gearbox interface.}
}
@article{HUSSAIN2022100717,
title = {An MDE-based methodology for closed-world integrity constraint checking in the semantic web},
journal = {Journal of Web Semantics},
volume = {74},
pages = {100717},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100717},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000129},
author = {Ambreen Hussain and Wenyan Wu and Zhaozhao Tang},
keywords = {Data validation, SWRL, SPARQL, Model-driven engineering, Water supply and distribution systems},
abstract = {Ontology-based data-centric systems support open-world reasoning. Therefore, for these systems, Web Ontology Language (OWL) and Semantic Web Rule Language (SWRL) are not suitable for expressing integrity constraints based on the closed-world assumption. Thus, the requirement of integrating the open-world assumption of OWL/SWRL with closed-world integrity constraint checking is inevitable. SPARQL, recommended by World Wide Web (W3C), is a query language for RDF graphs, and many research studies have shown that it is a perfect candidate for closed-world constraint checking for ontology-based data-centric applications. In this regard, many research studies have been performed to transform integrity constraints into SPARQL queries where some studies have shown the limitations of partial expressivity of knowledge bases while performing the indirect transformations, whereas others are limited to a platform-specific implementation. To address these issues, this paper presents a flexible and formal methodology that employs Model-Driven Engineering (MDE) to model closed-world integrity constraints for open-world reasoning. The proposed approach offers semantic validation of data by expressing integrity constraints at both the model level and the code level. Moreover, straightforward transformations from OWL/SWRL to SPARQL can be performed. Finally, the methodology is demonstrated via a real-world case study of water observations data.}
}
@article{GOTTLOB2023103936,
title = {Polynomial combined first-order rewritings for linear and guarded existential rules},
journal = {Artificial Intelligence},
volume = {321},
pages = {103936},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2023.103936},
url = {https://www.sciencedirect.com/science/article/pii/S0004370223000826},
author = {Georg Gottlob and Marco Manna and Andreas Pieris},
keywords = {Ontologies, Existential rules, Tuple-generating dependencies, Guardedness, Conjunctive queries, Query answering, Query rewriting, Combined approach},
abstract = {We consider the problem of ontological query answering, that is, the problem of answering a database query (typically a conjunctive query) in the presence of an ontology. This means that during the query answering process we also need to take into account the knowledge that can be inferred from the given database and ontology. Building, however, ontology-aware database systems from scratch, with sophisticated optimization techniques, is a highly non-trivial task that requires a great engineering effort. Therefore, exploiting conventional database systems is an important route towards efficient ontological query answering. Nevertheless, standard database systems are unaware of ontologies. An approach to ontological query answering that enables the use of standard database systems is the so-called polynomial combined query rewriting, originally introduced in the context of description logics: the conjunctive query q and the ontology Σ are rewritten in polynomial time into a first-order query qΣ (in a database-independent way), while the database D and the ontology Σ are rewritten in polynomial time into a new database DΣ (in a query-independent way), such that the answer to q in the presence of Σ over D coincides with the answer to qΣ over DΣ. The latter can then be computed by exploiting a conventional database system. In this work, we focus on linear and guarded existential rules, which form robust rule-based languages for modeling ontologies, and investigate the limits of polynomial combined query rewriting. In particular, we show that this type of rewriting can be successfully applied to (i) linear existential rules when the rewritten query can use the full power of first-order queries, (ii) linear existential rules when the arity of the underlying schema is fixed and the rewritten query is positive existential, namely it uses only existential quantification, conjunction, and disjunction, and (iii) guarded existential rules when the underlying schema is fixed and the rewritten query is positive existential. We can show that the above results reach the limits (under standard complexity-theoretic assumptions such as Image 1) of polynomial combined query rewriting in the case of linear and guarded existential rules.}
}
@article{MLAMBO2025100257,
title = {A Standardized Temporal Segmentation Framework and Annotation Resource Library in Robotic Surgery},
journal = {Mayo Clinic Proceedings: Digital Health},
pages = {100257},
year = {2025},
issn = {2949-7612},
doi = {https://doi.org/10.1016/j.mcpdig.2025.100257},
url = {https://www.sciencedirect.com/science/article/pii/S2949761225000641},
author = {Busisiwe Mlambo and Mallory Shields and Simon Bach and Armin Bauer and Andrew Hung and Omar Yusef Kudsi and Felix Neis and John Lazar and Daniel Oh and Robert Perez and Seth Rosen and Naeem Soomro and Michael Stany and Mark Tousignant and Christian Wagner and Ken Whaler and Lilia Purvis and Benjamin Mueller and Sadia Yousaf and Casey Troxler and Alfred Song and Emily Summers and Kiran Bhattacharyya and Anthony Jarc},
keywords = {standardized temporal ontology of surgery, standardization of surgical annotation},
abstract = {ABSTRACT
Objective
To develop and share the first clinical temporal annotation guide library for ten robotic procedures accompanied with a standardized, computer-readable ontology framework for surgical video annotation.
Patients and Methods
A standardized temporal annotation framework of surgical videos paired with consistent, procedure-specific annotation guides are critical to enable comparisons of surgical insights and facilitate large-scale insights for exceptional surgical practice. Existing ontologies and guidance provide foundational frameworks but provide limited scalability in clinical settings. Building on these, we developed a temporal annotation framework with nested surgical phases, steps, tasks, and subtasks. Procedure-specific annotation resource guides consistent with this framework that define each surgical segment with formulaic start and stop parameters and surgical objectives were iteratively created across seven years (January 1, 2018-January 1, 2025) through global research collaborations with surgeon researchers and industry scientists.
Results
We provide the first resource library of annotation guides for ten common robotic procedures consistent with our proposed temporal annotation framework, enabling consistent annotations for clinicians and large-scale data comparisons with computer-readable examples. These have been utilized in over 13000 annotated surgical cases globally, demonstrating reproducibility and broad applicability.
Conclusion
This resource library and accompanying computer-readable ontology framework provides critical structure for standardized temporal segmentation in robotic surgery. This framework has been applied globally in private studies examining surgical objective performance metrics, surgical education, workflow characterization, outcome prediction, algorithms for surgical activity recognition, and more. Adoption of these resources will unify clinical, academic, and industry efforts, ultimately catalyzing transformational advancements in surgical practice.}
}
@article{POULS2025283,
title = {Weaving a Net of Knowledge: Exploring Graph-Based Approaches for the Applied Modeling of Battery Cell Production Data and Information},
journal = {Procedia CIRP},
volume = {134},
pages = {283-288},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.03.035},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125004822},
author = {Kevin Pouls and Tom-Hendrik Hülsmann},
keywords = {battery cell manufacturing, graphs, data engineering, ontologies, knowledge-graph, GraphQL},
abstract = {Battery technology plays a significant role in the global energy transition and the transformation of the transport sector. However, the industry continues to be challenged by long ramp-up times, low yields and high scrap rates, while generating large amounts of data that remain largely unused. Reasons for this include the complex and interlinked process chain of battery cell production, the lack of public knowledge (e.g. regarding cause-efect relationships) in the industry, and complex data architectures, in which relevant data is stored in numerous systems and databases. Graph-based approaches for data modeling, storage and access can help to address these challenges by linking and storing data and information (e.g. known cause-efect relationships) in a machine and human-readable way. In this paper, two proofs of concept were developed to demonstrate the application of graph-based systems and data structures in battery cell manufacturing. Initial results indicate that these approaches are well suited to model the types of data and relationships commonly found in battery cell production.}
}
@article{SILEGA2021761,
title = {Applying an MDA-based approach for enhancing the validation of business process models},
journal = {Procedia Computer Science},
volume = {184},
pages = {761-766},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.094},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921007341},
author = {Nemury Silega and Manuel Noguera},
keywords = {MDA, ontologies, business process modeling},
abstract = {Business process modeling is a key activity during the development of complex and large information systems, such as enterprise management systems. These systems deal with a wide number of business processes; thus, the modeling and validation of processes becomes a challenging task. This entails dealing with issues such as the precedence between tasks and activities within a process, as well as resources, roles and enterprise assets involved. Moreover, undetected mistakes in this phase will be propagated to the system design phase and consequently will have a negative effect in the final system quality. On the other hand, the scientific literature advocates the suitability of formal models to address some issues during the process modeling. However, the adoption of formal models leads to new problems because formal languages are difficult to understand and process stakeholders usually lack of knowledge about them. In that direction, the Model-Driven Architecture (MDA) paradigm includes specifications that may alleviate some difficulties in the adoption of formal languages. Hence, in this paper we introduce an approach which combines MDA-specifications and ontologies to support process modeling. These technologies have great acceptance between both software researchers and developers. The use of ontologies permits to semantically validate the models. Furthermore, the application of MDA-guidelines could facilitate the integration of BPMN, a graphical notation for describing business process models widely accepted among business analysts, with a formal language to automate the analysis of business process models.}
}
@article{GAO2025104210,
title = {Improving device access efficiency using a device protocol matching model},
journal = {Computers in Industry},
volume = {164},
pages = {104210},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104210},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524001386},
author = {Zheng Gao and Danfeng Sun and Kai Wang and Huifeng Wu},
keywords = {Device access, Ontology, Protocol matching, Industrial Internet of Things (IIoT)},
abstract = {The connectivity of devices and systems in the Industrial Internet of Things (IIoT) enables interoperability and collaboration between industrial systems. Device access is the pathway to achieve connectivity, while protocol matching is the basis for device access. Protocol matching is a complex task due to the diverse range of device types, numerous protocols, the issues related to protocol privatization, and the reliance on domain knowledge. These complexities result in inefficient device access. To improve the device access efficiency, a Device Protocol Matching Model (DPMM) is proposed in this paper, which uses only the basic device information to find the best-matched protocol, including protocol identification and basic data. The DPMM adopts a two-stage strategy, consisting of an ontology creation stage and a protocol matching stage. In the ontology creation stage, a simplified device ontology is built to enable the uniform expression of device information and the representation of domain knowledge. In the protocol matching stage, a protocol matcher based on the Two-layer Cooperative Iteration (TCI) algorithm is designed to find the best-matched protocol. In the TCI, to achieve the global optimization of protocol matching efficiency, a penalty mechanism-based weight update method and learning-based matcher evolution are designed. Experiments in two scenarios: a communication base station and a copper smelting production line, are conducted to validate the effectiveness of the DPMM. The experimental results demonstrate that the DPMM can achieve automatic protocol matching with an average matching index of 80.3% and an average hit rate of 35.1%. Moreover, it significantly reduces network resource consumption by up to 96.7%, and increases the hit rate by up to 12.1 times compared with the existing methods.}
}
@article{GIORDANO2025104186,
title = {Decomposing maintenance actions into sub-tasks using natural language processing: A case study in an Italian automotive company},
journal = {Computers in Industry},
volume = {164},
pages = {104186},
year = {2025},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104186},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524001143},
author = {Vito Giordano and Gualtiero Fantoni},
keywords = {Natural language processing, Text mining, Maintenance work order, Industrial applications, Association rule mining, Large language model},
abstract = {Industry 4.0 has led to a huge increase in data coming from machine maintenance. At the same time, advances in Natural Language Processing (NLP) and Large Language Models provide new ways to analyse this data. In our research, we use NLP to analyse maintenance work orders, and specifically the descriptions of failures and the corresponding repair actions. Many NLP studies have focused on failure descriptions for categorising them, extracting specific information about failure, or supporting failure analysis methodologies (such as FMEA). Whereas, the analysis of repair actions and its relationship with failure remains underexplored. Addressing this gap, our study makes three significant contributions. Firstly, we focused on the Italian language, which presents additional challenges due to the dominance of NLP systems that are mainly designed for English. Secondly, it proposes a method for automatically subdividing a repair action into a set of sub-tasks. Lastly, it introduces an approach that employs association rule mining to recommend sub-tasks to maintainers when addressing failures. We tested our approach with a case study from an automotive company in Italy. The case study provides insights into the current barriers faced by NLP applications in maintenance, offering a glimpse into the future opportunities for smart maintenance systems.}
}
@article{PANESAR202320,
title = {Natural language processing-driven framework for the early detection of language and cognitive decline},
journal = {Language and Health},
volume = {1},
number = {2},
pages = {20-35},
year = {2023},
issn = {2949-9038},
doi = {https://doi.org/10.1016/j.laheal.2023.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2949903823000337},
author = {Kulvinder Panesar and María Beatriz {Pérez Cabello de Alba}},
keywords = {Language production, Memory concerns, Pre-screening model, Role and reference grammar, Speech assessment, Natural language processing},
abstract = {Natural Language Processing (NLP) technology has the potential to provide a non-invasive, cost-effective method using a timely intervention for detecting early-stage language and cognitive decline in individuals concerned about their memory. The proposed pre-screening language and cognition assessment model (PST-LCAM) is based on the functional linguistic model Role and Reference Grammar (RRG) to analyse and represent the structure and meaning of utterances, via a set of language production and cognition parameters. The model is trained on a DementiaBank dataset with markers of cognitive decline aligned to the global deterioration scale (GDS). A hybrid approach of qualitative linguistic analysis and assessment is applied, which includes the mapping of participants´ tasks of speech utterances and words to RRG phenomena. It uses a metric-based scoring with resulting quantitative scores and qualitative indicators as pre-screening results. This model is to be deployed in a user-centred conversational assessment platform.}
}
@article{DAI2024292,
title = {Facilitating Students’ Adaptive Help-seeking and Peer Interactions through an Analytics-enhanced Forum in Engineering Design Education},
journal = {Procedia CIRP},
volume = {128},
pages = {292-297},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124006735},
author = {Yun Dai and Ziyan Lin and Ang Liu},
keywords = {help-seeking, peer support, learning analytics, design thinking, engineering education},
abstract = {Design often takes place in collective and collaborative settings, and interactions and mutual support among peers have been a critical component of design education. However, in most of the existing design courses, students often work in small groups and peer interactions are limited to group members, which limits the range and depth of knowledge exchange. To complement the group-based activities, this study designs and assesses an analytics-enhanced discussion forum for whole-class interactions. The forum adopts ontology-based recommender systems and anomaly detection techniques to tailor the threads and contents for individual students in a personalized way. This analytics-enhanced forum was implemented in a large-size undergraduate design course (n = 313), and data about student responses to this forum was compared with data from the previous year’s course that adopted a conventional forum (n = 280). From the statistical analysis, students learning with the analytics-enhanced forum demonstrated significantly higher degrees of design practices (specifically, empathize, define, ideate, and test), collaborative learning, and course satisfaction. Qualitative analysis of students’ focus-group interviews shows their perceived benefits and concerns of the analytics-enhanced forum. The study also suggests integrating generative artificial intelligence and large language models to support students’ design thinking and collaborative design.}
}
@article{KEBEDE2022104630,
title = {Integration of manufacturers' product data in BIM platforms using semantic web technologies},
journal = {Automation in Construction},
volume = {144},
pages = {104630},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104630},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522005003},
author = {Rahel Kebede and Annika Moscati and He Tan and Peter Johansson},
keywords = {BIM, Information exchange, Product Data, Semantic Web technologies, Linked Data, Dynamo, Visual programming language, Standards},
abstract = {As building information modeling (BIM) gains popularity in the architecture, engineering, and construction (AEC) industry, manufacturers are required to distribute their product specifications in digital product models. Currently, manufacturers mainly employ proprietary formats, such as BIM objects supplemented by PDF documents to represent their product data descriptions. However, these formats do not support flexible automated product search and data integration. This paper describes the use of Semantic Web technologies in combination with BIM-based visual programming language (VPL) to automatically integrate product data from external databases. To facilitate data integration, we introduced a method to semantically represent product data linked with the CEN/TS 17623:2021 standard using ontologies in web ontology language (OWL). The study has focused on the use case of a manufacturer of lighting products. Results show that building designers are able to execute a more efficient product search that satisfies their query requirements and returns suitable products of their choice from the manufacturer's database based on their requests. This approach eliminates the time-consuming and error-prone process of manually entering product data into BIM software.}
}
@article{LIU2025102023,
title = {FyTok: An integrated modeling simulator for magnetic confinement fusion Tokamak},
journal = {SoftwareX},
volume = {29},
pages = {102023},
year = {2025},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2024.102023},
url = {https://www.sciencedirect.com/science/article/pii/S2352711024003935},
author = {Xiaojuan Liu and Zhi Yu},
keywords = {Tokamak, Integrated modeling, IMAS Data Dictionary, Ontology, Data integration},
abstract = {The tokamak is a highly complex system consisting of a large number of subsystems. It involves a wide range of non-linear physical processes on a variety of spatial and temporal scales. These physical processes are independent but closely interrelated. Therefore, the description and comprehension of the whole system is a very complicated endeavour. Existing single-function simulation codes are highly accuracy at certain specific spatial and temporal scales. However, separate analytical methods are insufficient to describe the multi-scale and non-linear characteristics of complex systems. It is therefore essential to adopt an appropriate “ integration” strategy to improve the understanding of complex systems. This paper describes the development of FyTok, a comprehensive simulator for the magnetic confinement fusion tokamak. The simulator has been designed using a top-down approach with ontology-based modeling, which translates the tokamak ontology description based on IMAS DD into a manipulable programming framework. The modeling objects in FyTok are divided into subsystems and plasma states in FyTok. FyTok handles each of these separately using data bindings, formula bindings and function bindings. FyTok can be easily configured using configuration files to organise the different module classes, generate the required physical workflows, and track the evolution of physical quantities in the workflows for modeling purposes. The ontology-based modeling approach in FyTok improves data consistency and provides the user with a generic and flexible modeling platform.}
}
@article{HASSAN2024200458,
title = {Design and implementation of EventsKG for situational monitoring and security intelligence in India: An open-source intelligence gathering approach},
journal = {Intelligent Systems with Applications},
volume = {24},
pages = {200458},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200458},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324001327},
author = {Hashmy Hassan and Sudheep Elayidom and M.R. Irshad and Christophe Chesneau},
keywords = {Knowledge graphs, Ontology, Domain specific KGs, EventsKG, Open-source intelligence gathering},
abstract = {This paper presents a method to construct and implement an Events Knowledge Graph (EventsKG) for security-related open-source intelligence gathering, focusing on event exploration for situation monitoring in India. The EventsKG is designed to process news articles, extract events of national security significance, and represent them in a consistent and intuitive manner. This method utilizes state-of-the-art natural language understanding techniques and the capabilities of graph databases to extract and organize events. A domain-specific ontology is created for effective storage and retrieval. In addition, we provide a user-friendly dashboard for querying and a complete visualization of events across India. The effectiveness of the EventsKG is assessed through a human evaluation of the information retrieval quality. Our approach contributes to rapid data availability and decision-making through a comprehensive understanding of events, including local events, from every part of India in just a few clicks. The system is evaluated against a manually annotated dataset and by involving human evaluators through a feedback survey, and it has shown good retrieval accuracy. The EventsKG can also be used for other applications such as threat intelligence, incident response, and situational awareness.}
}
@article{RODRIGUEZGARCIA2024125141,
title = {Smart recommender for the configuration of software project development teams},
journal = {Expert Systems with Applications},
volume = {258},
pages = {125141},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125141},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424020086},
author = {Miguel Ángel Rodríguez-García and Francisco García-Sánchez and Rafael Valencia-García},
keywords = {Semantic annotation, Information extraction, Knowledge management, Ontology, Semantic web},
abstract = {The development of Social Media has caused an incredible change in the way people communicate and share information. It provides a set of platforms, web-based applications and services that facilitate the collaborative creation of content and the sharing of ideas and interests. Since its inception, Social Media technologies have been increasingly used in different fields that have integrated them into their daily lives. In Software Engineering, for example, it has caused a disruptive change in the software development model, changing the way that the projects are approached by promoting collaborative environments. This effect has led to the proliferation of the software development communities where huge amounts of information are published every day. Therefore, when a project is started and a development team needs to be assembled, it is difficult to select and identify the most suitable developer profiles for such a project by considering all the disseminated information. To solve this problem, we have proposed an ontology-based system to help find a suitable group of developers to develop a project. The system uses web services to extract user profiles from GitHub, and semantic technologies to represent and annotate the features of the extracted data. Then, when the system receives the natural language description of the project to be developed, it identifies and extracts relevant concepts such as technologies, platforms, tools, among others. As a result, it analyzes the extracted information and lists the most suitable developers to assemble a team of developers with the right technical skills to tackle the software project. For evaluation purposes, we generated a random list of GitHub profiles, and collected a corpus of documents describing research projects and patents. The system produced very promising results, achieving a MAP@5 and F-Measure of 0.68.}
}
@article{NUNDLOLL2022e10710,
title = {Automating the extraction of information from a historical text and building a linked data model for the domain of ecology and conservation science},
journal = {Heliyon},
volume = {8},
number = {10},
pages = {e10710},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e10710},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022019983},
author = {Vatsala Nundloll and Robert Smail and Carly Stevens and Gordon Blair},
keywords = {Data extraction, Unstructured data, Semantic integration, Natural language processing, Machine learning, Ontologies},
abstract = {Data heterogeneity is a pressing issue and is further compounded if we have to deal with data from textual documents. The unstructured nature of such documents implies that collating, comparing and analysing the information contained therein can be a challenging task. Automating these processes can help to unleash insightful knowledge that otherwise remains buried in them. Moreover, integrating the extracted information from the documents with other related information can help to make more information-rich queries. In this context, the paper presents a comprehensive review of text extraction and data integration techniques to enable this automation process in an ecological context. The paper investigates into extracting valuable floristic information from a historical Botany journal. The purpose behind this extraction is to bring to light relevant pieces of information contained within the document. In addition, the paper also explores the need to integrate the extracted information together with other related information from disparate sources. All the information is then rendered into a query-able form in order to make unified queries. Hence, the paper makes use of a combination of Machine Learning, Natural Language Processing and Semantic Web techniques to achieve this. The proposed approach is demonstrated through the information extracted from the journal and the information-rich queries made through the integration process. The paper shows that the approach has a merit in extracting relevant information from the journal, discusses how the machine learning models have been designed to classify complex information and also gives a measure of their performance. The paper also shows that the approach has a merit in query time in regard to querying floristic information from a multi-source linked data model.}
}
@article{VALDERAS2022111139,
title = {Modelling and executing IoT-enhanced business processes through BPMN and microservices},
journal = {Journal of Systems and Software},
volume = {184},
pages = {111139},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111139},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221002363},
author = {Pedro Valderas and Victoria Torres and Estefanía Serral},
keywords = {IoT, BPMN, Microservices},
abstract = {The Internet of Things enables to connect the physical world to digital business processes (BP) and allows a BP to (1) consider real-world data to take more informed business decisions, (2) automate and/or improve BP tasks, and (3) adapt itself to the physical execution environment. We refer to these processes as IoT-enhanced BPs. Although numerous researchers have studied this subject, there are still some challenges to be faced. For instance, the need of a modelling solution that does not increase the notation complexity to facilitate further analysis and engineering decision making, or an execution approach that provides a high degree of independence between the process and the underlying IoT device technology. The objective of this work is defining an approach that (1) considers important intrinsic characteristics of IoT-enhanced BPs at modelling level without growing the complexity of the modelling language, and (2) facilitates the execution of the IoT-enhanced BPs represented in models independently from IoT devices’ technology. To do so, we present a modelling approach that uses standard BPMN concepts to model IoT-enhanced BPs without modifying its metamodel. It applies the Separation of Concern (SoC) design principle: BPMN is used to describe IoT-enhanced BPs while low-level real-world data is captured in an ontology. Finally, a microservice architecture is proposed to execute BPMN models and facilitate its integration with the physical world. This architecture provides high flexibility to support multiples IoT device technologies as well as their evolution and maintenance. The evaluation done allows us to conclude that the application of the SoC principle using BPMN and ontologies facilitates the definition of intrinsic characteristics of IoT-enhanced BPs without increasing the complexity of the BPMN metamodel. Besides, the proposed microservice architecture provides a high degree of decoupling between the created models and the underlying IoT technology.}
}
@article{WU2025103446,
title = {Customer requirement-oriented personalized product configuration method with knowledge graphs},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103446},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103446},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003398},
author = {Yi Wu and Lina He and Mark Goh and Na Li and Zhenyong Wu},
keywords = {Personalized product configuration, Knowledge representation, Knowledge Graph (KG), Recommendation mechanism},
abstract = {A unified framework for representing customer requirements and product knowledge, as well as efficient configuration methods, is lacking. This study proposes a personalized product configuration method based on Knowledge Graphs (KGs) to satisfy the diverse requirements of customers who often demand personalization. This method constructs a schema layer within the KG, so as to organize both precise and fuzzy requirements in a structured manner. It also integrates knowledge on the quality characteristics and formulates a generic normative requirement template, thus establishing appropriate knowledge representation models of customer requirement and configuration scheme domains. By linking the knowledge between these two domains, a personalized product configuration KG is formed, and the KG-based configuration generation and recommendation method oriented to customer requirements are developed. The proposed method is validated using an automobile configuration case study. The case results demonstrate that the proposed method effectively integrates customer requirements and recommends personalized product configuration schemes, offering a novel approach to addressing the personalized product configuration challenge and providing practical value.}
}
@article{DASILVEIRA2024102286,
title = {A knowledge-sharing platform for space resources},
journal = {Data & Knowledge Engineering},
volume = {151},
pages = {102286},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102286},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000107},
author = {Marcos {Da Silveira} and Louis Deladiennee and Emmanuel Scolan and Cedric Pruski},
keywords = {Knowledge engineering, Knowledge graph, Ontology, Space resources},
abstract = {The ever-increasing interest of academia, industry, and government institutions in space resource information highlights the difficulty of finding, accessing, integrating, and reusing this information. Although information is regularly published on the internet, it is disseminated on many different websites and in different formats, including scientific publications, patents, news, and reports. We are currently developing a knowledge management and sharing platform for space resources. This tool, which relies on the combined use of knowledge graphs and ontologies, formalises the domain knowledge contained in the above-mentioned documents and makes it more readily available to the community. In this article, we describe the concepts and techniques of knowledge extraction and management adopted during the design and implementation of the platform.}
}
@article{LUCIFORA2025105243,
title = {Empirical exploration of the 4P theory of creativity using virtual reality},
journal = {Acta Psychologica},
volume = {258},
pages = {105243},
year = {2025},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2025.105243},
url = {https://www.sciencedirect.com/science/article/pii/S0001691825005566},
author = {Chiara Lucifora and Claudia Scorolli and Aldo Gangemi},
keywords = {Creativity, 4P theory, Virtual reality, Creativity ontology, Empirical exploration},
abstract = {In the 4P theory of creativity, Rhodes in 1961 emphasises the importance of different components in human creativity, which arises from the dynamic interplay of four key components: Person, Process, Product, and Press. Following previous studies that focused on relations among the 4P components, we conducted an exploratory empirical study with a sample of 60 participants using virtual reality. In our study we investigated: Person through self-report questionnaires on participants' personality and creativity traits; Process through the registrations of users' interactions during a VR task in which they create artworks; Product through a semi-automated ontological analysis of the artworks' features; Press by altering the virtual environment with contextual auditory stimuli. Our research contributes to the study of creativity, using Virtual Reality as a platform for an empirical investigation of the relations between the 4P components of creativity. Our findings suggest an intricate network of components, with significant correlations for 5 out of 6 relations. Our findings are in line with previous research about the relation between Person and Process, between Person and Press, and between Process and Product, and show novel insights about the relation between Person and Product and between Press and Process. We have applied our novel method to the original 4P foundational domains, demonstrating its utility to lay a research ground to formally (via ontological distinctions) and empirically (based on explicit and implicit measures) compare creativity theories that extend 4P components, possibly addressing cognitive abilities, expertise and impact levels, error functionalization, creativity dynamics, etc.}
}
@article{CHEN2025100123,
title = {Bibliometric analysis of natural language processing using CiteSpace and VOSviewer},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100123},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000712},
author = {Xiuming Chen and Wenjie Tian and Haoyun Fang},
keywords = {NLP, CiteSpace, VOSviewer, Bibliometrics analysis},
abstract = {Natural Language Processing (NLP) holds a pivotal position in the domains of computer science and artificial intelligence (AI). Its focus is on exploring and developing theories and methodologies that facilitate seamless and effective communication between humans and computers through the use of natural language. First of all, In this paper, we employ the bibliometric analysis tools, namely CiteSpace and VOSviewer (Visualization of Similarities viewer) are used as the bibliometric analysis software in this paper to summarize the domain of NLP research and gain insights into its core research priorities. What is more, the Web of Science(WoS) Core Collection database serves as the primary source for data acquisition in this study. The data includes 4803 articles on NLP published from 2011 to May 15, 2024. The trends and types of articles reveal the developmental trajectory and current hotspots in NLP. Finally, the analysis covers eight aspects: volume of published articles, classification, countries, institutional collaboration, author collaboration network, cited author network, co-cited journals, and co-cited references. The applications of NLP are vast, spanning areas such as AI, electronic health records, risk, task analysis, data mining, computational modeling. The findings suggest that the emphasis of future research ought to focus on areas like AI, risk, task analysis, and computational modeling. This paper provides learners and practitioners with a comprehensive insight into the current status and emerging trends in NLP.}
}
@article{YU2025104332,
title = {Incorporating knowledge graph and multi-model stacking ensemble learning for prediction of fines for illegal fishing},
journal = {Regional Studies in Marine Science},
volume = {89},
pages = {104332},
year = {2025},
issn = {2352-4855},
doi = {https://doi.org/10.1016/j.rsma.2025.104332},
url = {https://www.sciencedirect.com/science/article/pii/S2352485525003238},
author = {Hongchu Yu and Yuhao Xiao and Chen Chen and Junhua Zhou and Lei Xu},
keywords = {Illegal fishing, Knowledge graph, Fine prediction, Stacked generalization, Integrated method},
abstract = {Illegal fishing activities have a significant threat to global marine resource management, inflicting severe damage on marine ecosystems, disrupting legal fisheries economies, and hindering biodiversity conservation efforts. Research on Illegal, Unreported, and Unregulated (IUU) fishing activities have gained substantial attention globally, focusing on identification, monitoring, prevention, and policy formulation. However, limited efforts have been directed toward the prediction of fines and the assessment of legal repercussions. This gap hampers the timely accurate evaluation of the economic impacts of IUU fishing behaviors and undermines the full deterrent potential of legal penalties. Therefore, this paper proposes an integrated method combining knowledge graph and multi-model stacked generalization, aiming to enhance the accuracy of fines prediction for illegal fishing activities. Experimental results demonstrate that the proposed model significantly enhances prediction accuracy, interpretability, and stability compared with basic machine learning models, including eXtreme Gradient Boosting (XGBoost), Gradient Boosting Machines (GBM), K-Nearest Neighbor(K-NN), Categorical Boosting (CatBoost), and Random Forest(RF). This study provides a new technical guidance for the prediction of fines for illegal fishing activities, contributing significantly to enhancing the efficiency and effectiveness of fishery law enforcement.}
}
@article{LIU2025111625,
title = {Knowledge graph reasoning: Mainstream methods, applications and prospects},
journal = {Engineering Applications of Artificial Intelligence},
volume = {159},
pages = {111625},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.111625},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625016276},
author = {Han Liu and Shaojie Yang and Guokai Shi and Zongcheng Miao},
keywords = {Knowledge graph, Knowledge graph reasoning, Deep learning, Reinforcement learning},
abstract = {Knowledge graphs have emerged as a leading paradigm for knowledge representation due to their robust capacity to mine, organize, and manage massive datasets. Their versatility has led to extensive research and application in various advanced fields. Knowledge graph reasoning plays a critical role in this context by reasoning new facts from existing ones, thereby completing and refining the knowledge base. In this paper, we provide a comprehensive review of knowledge graph reasoning by researching its definition, foundational concepts, and methodological approaches. We systematically categorize reasoning methods into five groups: reasoning based on ontology, reasoning based on rules, neural rule reasoning based on distributed representations, neural rule reasoning based on deep learning, and hybrid reasoning. Furthermore, we analyze the applications of knowledge graph reasoning, discuss their current limitations, and outline promising directions for future research to enhance both performance and scalability in this rapidly evolving field.}
}
@article{LEE2023,
title = {Detecting Ground Glass Opacity Features in Patients With Lung Cancer: Automated Extraction and Longitudinal Analysis via Deep Learning–Based Natural Language Processing},
journal = {JMIR AI},
volume = {2},
year = {2023},
issn = {2817-1705},
doi = {https://doi.org/10.2196/44537},
url = {https://www.sciencedirect.com/science/article/pii/S2817170523000182},
author = {Kyeryoung Lee and Zongzhi Liu and Urmila Chandran and Iftekhar Kalsekar and Balaji Laxmanan and Mitchell K Higashi and Tomi Jun and Meng Ma and Minghao Li and Yun Mai and Christopher Gilman and Tongyu Wang and Lei Ai and Parag Aggarwal and Qi Pan and William Oh and Gustavo Stolovitzky and Eric Schadt and Xiaoyan Wang},
keywords = {natural language processing, ground glass opacity, real world data, radiology notes, longitudinal analysis, deep learning, bidirectional long short-term memory (Bi-LSTM), conditional random fields (CRF)},
abstract = {Background
Ground-glass opacities (GGOs) appearing in computed tomography (CT) scans may indicate potential lung malignancy. Proper management of GGOs based on their features can prevent the development of lung cancer. Electronic health records are rich sources of information on GGO nodules and their granular features, but most of the valuable information is embedded in unstructured clinical notes.
Objective
We aimed to develop, test, and validate a deep learning–based natural language processing (NLP) tool that automatically extracts GGO features to inform the longitudinal trajectory of GGO status from large-scale radiology notes.
Methods
We developed a bidirectional long short-term memory with a conditional random field–based deep-learning NLP pipeline to extract GGO and granular features of GGO retrospectively from radiology notes of 13,216 lung cancer patients. We evaluated the pipeline with quality assessments and analyzed cohort characterization of the distribution of nodule features longitudinally to assess changes in size and solidity over time.
Results
Our NLP pipeline built on the GGO ontology we developed achieved between 95% and 100% precision, 89% and 100% recall, and 92% and 100% F1-scores on different GGO features. We deployed this GGO NLP model to extract and structure comprehensive characteristics of GGOs from 29,496 radiology notes of 4521 lung cancer patients. Longitudinal analysis revealed that size increased in 16.8% (240/1424) of patients, decreased in 14.6% (208/1424), and remained unchanged in 68.5% (976/1424) in their last note compared to the first note. Among 1127 patients who had longitudinal radiology notes of GGO status, 815 (72.3%) were reported to have stable status, and 259 (23%) had increased/progressed status in the subsequent notes.
Conclusions
Our deep learning–based NLP pipeline can automatically extract granular GGO features at scale from electronic health records when this information is documented in radiology notes and help inform the natural history of GGO. This will open the way for a new paradigm in lung cancer prevention and early detection.}
}
@article{ZHONG2025103786,
title = {Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for pulmonary embolism diagnosis and report generation from CTPA},
journal = {Medical Image Analysis},
pages = {103786},
year = {2025},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2025.103786},
url = {https://www.sciencedirect.com/science/article/pii/S1361841525003329},
author = {Zhusi Zhong and Yuli Wang and Lulu Bi and Zhuoqi Ma and Sun Ho Ahn and Christopher J. Mullin and Colin F. Greineder and Michael K. Atalay and Scott Collins and Grayson L. Baird and Cheng Ting Lin and Webster Stayman and Todd M. Kolb and Ihab Kamel and Harrison X. Bai and Zhicheng Jiao},
keywords = {Radiology report generation, Contrastive learning, 3D medical image, Pulmonary embolism},
abstract = {Medical imaging plays a pivotal role in modern healthcare, with computed tomography pulmonary angiography (CTPA) being a critical tool for diagnosing pulmonary embolism and other thoracic conditions. However, the complexity of interpreting CTPA scans and generating accurate radiology reports remains a significant challenge. This paper introduces Abn-BLIP (Abnormality-aligned Bootstrapping Language-Image Pretraining), an advanced diagnosis model designed to align abnormal findings to generate the accuracy and comprehensiveness of radiology reports. By leveraging learnable queries and cross-modal attention mechanisms, our model demonstrates superior performance in detecting abnormalities, reducing missed findings, and generating structured reports compared to existing methods. Our experiments show that Abn-BLIP outperforms state-of-the-art medical vision-language models and 3D report generation methods in both accuracy and clinical relevance. These results highlight the potential of integrating multimodal learning strategies for improving radiology reporting. The source code is available at https://github.com/zzs95/abn-blip.}
}
@article{SHANAVAS2020172,
title = {Ontology-based enriched concept graphs for medical document classification},
journal = {Information Sciences},
volume = {525},
pages = {172-181},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S002002552030178X},
author = {Niloofer Shanavas and Hui Wang and Zhiwei Lin and Glenn Hawe},
keywords = {Medical text classification, SVM, Graph-based text representation, UMLS, Similarity measure, Graph kernel},
abstract = {The rapidly increasing volume of medical text data, including biomedical literature and clinical records, presents difficulties to biomedical researchers and clinical practitioners. Automatic text classification is an important means for managing medical text data. The main challenge in medical text classification is the complex terminology used in these documents. Therefore, it is critical to handle synonymy, polysemy, and multi-word concepts so that classification is based on the meaning of these documents. The solution to this problem of complex terminology helps in building systems with better access to relevant data, resulting in more effective utilisation of the existing information. In this paper, we present a simple and effective approach to address this challenge. A concept graph is automatically constructed and enriched for each medical text document with the help of a domain-specific similarity matrix that is built using Unified Medical Language System (UMLS) concepts in the training documents. Medical text documents are compared based on their enriched concept graphs using a graph kernel. Classification is then done based on the comparison result. The benefit of this approach is that it allows the incorporation of domain knowledge into the classification framework. The experiments on biomedical abstracts and clinical reports classification show the effectiveness of the proposed approach. Based on evaluation metrics of precision, recall and F1-scores, our method achieves a significantly higher classification performance than other widely used similarity measures for similarity-based text classification.}
}
@article{CATREN201825,
title = {Klein-Weyl's program and the ontology of gauge and quantum systems},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {61},
pages = {25-40},
year = {2018},
note = {Hermann Weyl and the Philosophy of the `New Physics'},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2017.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1355219816300351},
author = {Gabriel Catren},
keywords = {Hermann Weyl, Klein's Erlangen program, General relativity, Gauge theories, Quantum mechanics, Symmetries},
abstract = {We distinguish two orientations in Weyl's analysis of the fundamental role played by the notion of symmetry in physics, namely an orientation inspired by Klein's Erlangen program and a phenomenological-transcendental orientation. By privileging the former to the detriment of the latter, we sketch a group(oid)-theoretical program—that we call the Klein-Weyl program—for the interpretation of both gauge theories and quantum mechanics in a single conceptual framework. This program is based on Weyl's notion of a “structure-endowed entity” equipped with a “group of automorphisms”. First, we analyze what Weyl calls the “problem of relativity” in the frameworks provided by special relativity, general relativity, and Yang-Mills theories. We argue that both general relativity and Yang-Mills theories can be understood in terms of a localization of Klein's Erlangen program: while the latter describes the group-theoretical automorphisms of a single structure (such as homogenous geometries), local gauge symmetries and the corresponding gauge fields (Ehresmann connections) can be naturally understood in terms of the groupoid-theoretical isomorphisms in a family of identical structures. Second, we argue that quantum mechanics can be understood in terms of a linearization of Klein's Erlangen program. This stance leads us to an interpretation of the fact that quantum numbers are “indices characterizing representations of groups” ((Weyl, 1931a), p.xxi) in terms of a correspondence between the ontological categories of identity and determinateness.}
}
@article{ZHANG2024104220,
title = {An event logic graph for geographic environment observation planning in disaster chain monitoring},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {134},
pages = {104220},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.104220},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224005764},
author = {Yunbo Zhang and Wenjie Chen and Bingshu Huang and Zongran Zhang and Jie Li and Ruishan Gao and Ke Wang and Chuli Hu},
keywords = {Event logic graph, Disaster chain, Geographic environment, Observation planning, Knowledge ontology, Event reasoning},
abstract = {Effective geographic environment observation planning is the key to obtain disaster monitoring and warning information. The previous researches can only make observation plans for a single disaster at some specific stages. They are difficult to apply to the dynamic evolution of the disaster chain. Timely and comprehensive geographic environment observation planning is urgently needed to provide high-value monitoring data for the identification and response of secondary disaster chains. Event logic graph (ELG) shows great potential in evolutionary law expression and chain event reasoning. Therefore, this study proposed an observation ELG (OELG), in which events and their logical relationships are modeled as nodes and edges to express the occurrence and development motivation of observation events. The disaster chain observation planning can be transformed into the reasoning of potential continuous observation events. Subsequently, an OELG-based geographic environment observation planning framework was proposed, which realizes the construction, instantiation, and plan reasoning of OELG. The observation planning experiment was carried out taking the flood disaster chain that occurred in Beijing, China and Nordrhein-Westfalen, Germany as examples. The results show that OELG can generate disaster chain observation plan more timely, comprehensively, and continuously than other models, thus providing support for disaster chain risk monitoring and emergency response.}
}
@article{LIU2024119280,
title = {MAKG: A maritime accident knowledge graph for intelligent accident analysis and management},
journal = {Ocean Engineering},
volume = {312},
pages = {119280},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119280},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824026180},
author = {Dongge Liu and Liang Cheng},
keywords = {Maritime accident, Knowledge graph, Named entity recognition, BERT, Prompt learning},
abstract = {With the increasing frequency of human activities at sea, maritime accidents are occurring more often. Analyzing and mining maritime accident cases can help uncover the causal mechanisms behind these incidents, thereby enhancing maritime safety. As an emerging technology for knowledge management and mining, knowledge graphs offer significant support for the storage, reasoning, and decision-making processes related to maritime accidents. In this study, we established a knowledge graph construction and application framework for maritime accidents to facilitates the extraction and management of maritime knowledge from unstructured texts. First, 581 accident reports released by the China Maritime Safety Administration over the past decade (2014–2023) were used as the data basis for analysis and construction of the maritime accident ontology structure using the seven-step method, which comprises 8 entity types, 8 relationship types, and 18 attribute entity types. Second, We proposed MBERT-BiLSTM-CRF-SF, a named entity recognition model based on domain pretraining and self-training, to reduce graph construction costs. This model achieved state-of-the-art performance in the maritime domain, with an F1 score of 0.910 ± 0.006, which is about 5% higher than the mainstream model. In addition, we proposed an entity alignment method based on font and semantics to refine knowledge further. On the basis of the proposed method, we constructed a large, high-quality maritime accident knowledge graph (MAKG) system that contains 16,099 entities and 20,809 relationship instances. Finally, we reduced the complexity of applying knowledge graphs by integrating the CRISPE prompt learning framework of the large language model, and experiments on graph traversal, pattern recognition, and aggregation analysis were conducted to assess the quality of MAKG. Results demonstrate that MAKG can effectively enhance the efficiency of querying and reasoning about maritime accident information, thus providing significant support for the prevention and management of maritime accidents.}
}
@article{VERHELST2025242,
title = {A Digital Colleague as intuitive operator support system in a HMLV Production Environment},
journal = {Procedia CIRP},
volume = {136},
pages = {242-247},
year = {2025},
note = {35th CIRP Design 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125007954},
author = {Ewoud Verhelst and Nicholas Harley and Bart {Van Doninck} and Abdellatif Bey-Temsamani},
keywords = {HMLV, LLM, CHAKRA, AI, industry 5.0},
abstract = {High-Mix Low-Volume (HMLV) manufacturing, driven by the demand for customized and flexible products, has increased the complexity and variability of tasks for operators. To maintain efficiency, support systems must provide operators with precise, relevant information and actions without overwhelming them. We introduce a ‘Digital Colleague,’ a modular and intuitive human-centric architecture designed to streamline design and production information for operators upon request. Built on large language models (LLMs), a skill-based robot framework and a hierarchical knowledge base, this research highlights the potential of AI to enhance operator support, paving the way for advancements in smart manufacturing technologies.}
}
@article{VO2025104376,
title = {The McDonaldization of democracy? Globalization and space-making in practices of innovating mini-publics},
journal = {Geoforum},
volume = {165},
pages = {104376},
year = {2025},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2025.104376},
url = {https://www.sciencedirect.com/science/article/pii/S0016718525001769},
author = {Jan-Peter Voß and Jannik Schritt},
keywords = {Ontography, Empirical ontology, Space-making, Spatiality, Democratic innovation, Translocal, Globalization, Modern, Postmodern, Reflexive modern, Region, Fluid, Network, Science and technology studies (STS)},
abstract = {The paper proposes an “ontographic approach” to the study of globalization. It focuses on different ontologies of globalization enacted in practices of translocal innovation. We distinguish: (1) modern globalization in practices of regionally expanding functional systems, (2) postmodern globalization in practices of fluidly dispersing heterogeneous assemblages, and (3) reflexive modern globalization in practices of infrastructuring translocal networks. We illustrate these different ways of doing globalization with accounts of democratic innovation practices that seek to spread “mini-publics” as a new form of deliberative democracy. At first glance, this may appear to be the McDonaldization of democracy, referring to Ritzer’s model of globalization as a worldwide expansion of Western functionalist management and standardization. On closer inspection, however, we find that three different ways of doing globalization coexist, each associated with different ways of doing social order and space. We propose the ontographic approach to account for this diversity. We then suggest it as a more general approach to turn the confrontation of modern, postmodern, and reflexive modern theories of globalization into a heuristic repertoire for studying how different forms of globalization are done in practice.}
}
@article{EGUIA2024,
title = {Clinical Decision Support and Natural Language Processing in Medicine: Systematic Literature Review},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/55315},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124006149},
author = {Hans Eguia and Carlos Luis Sánchez-Bocanegra and Franco Vinciarelli and Fernando Alvarez-Lopez and Francesc Saigí-Rubió},
keywords = {artificial intelligence, AI, natural language processing, clinical decision support system, CDSS, health recommender system, clinical information extraction, electronic health record, systematic literature review, patient, treatment, diagnosis, health workers},
abstract = {Background
Ensuring access to accurate and verified information is essential for effective patient treatment and diagnosis. Although health workers rely on the internet for clinical data, there is a need for a more streamlined approach.
Objective
This systematic review aims to assess the current state of artificial intelligence (AI) and natural language processing (NLP) techniques in health care to identify their potential use in electronic health records and automated information searches.
Methods
A search was conducted in the PubMed, Embase, ScienceDirect, Scopus, and Web of Science online databases for articles published between January 2000 and April 2023. The only inclusion criteria were (1) original research articles and studies on the application of AI-based medical clinical decision support using NLP techniques and (2) publications in English. A Critical Appraisal Skills Programme tool was used to assess the quality of the studies.
Results
The search yielded 707 articles, from which 26 studies were included (24 original articles and 2 systematic reviews). Of the evaluated articles, 21 (81%) explained the use of NLP as a source of data collection, 18 (69%) used electronic health records as a data source, and a further 8 (31%) were based on clinical data. Only 5 (19%) of the articles showed the use of combined strategies for NLP to obtain clinical data. In total, 16 (62%) articles presented stand-alone data review algorithms. Other studies (n=9, 35%) showed that the clinical decision support system alternative was also a way of displaying the information obtained for immediate clinical use.
Conclusions
The use of NLP engines can effectively improve clinical decision systems’ accuracy, while biphasic tools combining AI algorithms and human criteria may optimize clinical diagnosis and treatment flows.
Trial Registration
PROSPERO CRD42022373386; https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=373386}
}
@article{CLAY2025109808,
title = {Natural language processing techniques applied to the electronic health record in clinical research and practice - an introduction to methodologies},
journal = {Computers in Biology and Medicine},
volume = {188},
pages = {109808},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.109808},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525001581},
author = {Benjamin Clay and Henry I. Bergman and Safa Salim and Gabriele Pergola and Joseph Shalhoub and Alun H. Davies},
keywords = {natural language processing, NLP, methodology, electronic health record, EHR},
abstract = {Natural Language Processing (NLP) has the potential to revolutionise clinical research utilising Electronic Health Records (EHR) through the automated analysis of unstructured free text. Despite this potential, relatively few applications have entered real-world clinical practice. This paper aims to introduce the whole pipeline of NLP methodologies for EHR analysis to the clinical researcher, with case studies to demonstrate the application of these methods in the existing literature. Essential pre-processing steps are introduced, followed by the two major classes of analytical frameworks: statistical methods and Artificial Neural Networks (ANNs). Case studies which apply statistical and ANN-based methods are then provided and discussed, illustrating information extraction tasks for objective and subjective information, and classification/prediction tasks using supervised and unsupervised approaches. State-of-the-art large language models and future directions for research are then discussed. This educational article aims to bridge the gap between the clinical researcher and the NLP expert, providing clinicians with a background understanding of the NLP techniques relevant to EHR analysis, allowing engagement with this rapidly evolving area of research, which is likely to have a major impact on clinical practice in coming years.}
}
@article{LAUB2025109384,
title = {Automated Generation of Mechanistic Models for Chemical Process Digital Twins using Reinforcement Learning - Part II: Compartmentalization and Learning-Based Recalibration},
journal = {Computers & Chemical Engineering},
pages = {109384},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2025.109384},
url = {https://www.sciencedirect.com/science/article/pii/S0098135425003874},
author = {Jan-Frederic Laub and Jiyizhe Zhang and Mathis Heyer and Alexei A. Lapkin},
keywords = {Automated modeling, Reinforcement learning, Mechanistic models, Transfer learning, Digital twins},
abstract = {Developing predictive models is central to building digital twins for chemical processes, which have a variety of applications in their development and operation. Mechanistic models are highly interpretable and have a larger domain of validity compared to data-driven models, but require significant time and expert knowledge to construct. In this contribution, a workflow for automated mechanistic model generation is extended to handle systems comprised of interdependent, spatially distributed phenomena. The search for accurate models is performed by hierarchically connected reinforcement learning agents. Different ways to incorporate human expertise in model generation are explored, and an ontology is introduced to manage expert and modeling knowledge. The extended workflow is shown to reliably find accurate models of chemical systems, exemplified on a phase transfer catalysis reaction and a Taylor-Couette reactor. For the latter, its non-ideal flow patterns were predicted within a deviation of 5%, and automatically generated compartmentalization results were found to have comparable physical interpretations to bespoke models from literature. Additionally, the reinforcement learning agents were able to accurately recalibrate models up to twice as fast when drawing upon pre-training under a different operation condition. By generalizing all parts of the automated modeling procedures, we enable the efficient (re-)use of knowledge previously confined to the human modeler. We envision that in the future, the role of experts can be shifted from actively constructing each model iteration to curating knowledge and working collaboratively with autonomous agents.}
}
@article{BADENESOLMEDO2023104382,
title = {Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature},
journal = {Journal of Biomedical Informatics},
volume = {142},
pages = {104382},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104382},
url = {https://www.sciencedirect.com/science/article/pii/S153204642300103X},
author = {Carlos Badenes-Olmedo and Oscar Corcho},
keywords = {Question-answering, Knowledge graphs, Ontology, Evidences},
abstract = {The article presents a workflow to create a question-answering system whose knowledge base combines knowledge graphs and scientific publications on coronaviruses. It is based on the experience gained in modeling evidence from research articles to provide answers to questions in natural language. The work contains best practices for acquiring scientific publications, tuning language models to identify and normalize relevant entities, creating representational models based on probabilistic topics, and formalizing an ontology that describes the associations between domain concepts supported by the scientific literature. All the resources generated in the domain of coronavirus are available openly as part of the Drugs4COVID initiative, and can be (re)-used independently or as a whole. They can be exploited by scientific communities conducting research related to SARS-CoV-2/COVID-19 and also by therapeutic communities, laboratories, etc., wishing to find and understand relationships between symptoms, drugs, active ingredients and their documentary evidence.}
}
@article{MOUICHE2025104120,
title = {Entity and relation extractions for threat intelligence knowledge graphs},
journal = {Computers & Security},
volume = {148},
pages = {104120},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104120},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004255},
author = {Inoussa Mouiche and Sherif Saad},
keywords = {Threat intelligence knowledge graphs (TiKG), Cyber threat intelligence (CTI), Cyber knowledge graphs (CKGs), Pipeline extraction, Joint extraction, Entity-relation extraction, Knowledge Ontology, NER Datasets},
abstract = {Advanced persistent threats (APTs) represent a complex challenge in cybersecurity as they infiltrate networks stealthily to conduct espionage, steal data, and maintain a long-term presence. To combat these threats, security professionals increasingly rely on cyber knowledge graphs (CKGs), which provide scalable solutions to analyze and structure vast amounts of cyber threat intelligence (CTI) from diverse sources in real-time, enabling the automation of proactive security measures. Developing CKGs requires extracting entity and their relationships from unstructured CTI reports. However, existing approaches face significant limitations, such as difficulties with the nuances of cybersecurity language, diverse threat terminologies, and high rates of error propagation, resulting in low accuracy and poor generalizability. This paper introduces a novel Threat Intelligence Knowledge Graph (TiKG) pipeline designed to address these challenges. The TiKG framework leverages SecureBERT, a domain-specific transformer-based model optimized for cybersecurity, and integrates it with an attention-based BiLSTM to capture the context and nuances of security texts, reducing error propagation and improving extraction accuracy. Additionally, the pipeline incorporates a domain-specific ontology and inference model to ensure precise relation mapping in relation extraction. Using three large-scale TI open-source datasets (DNRTI, STUCCO, and CYNER) and a curated CTI dataset, extensive evaluations demonstrate the effectiveness of our framework, showing significant improvements over existing methods in detecting and linking cyber threats. These contributions provide a robust platform for security professionals to analyze and predict potential attacks, develop effective defenses, and enhance the strategic capabilities of cybersecurity operations.}
}
@article{LUKUMBUZYA2024104099,
title = {Datalog rewritability and data complexity of ALCHOIQ with closed predicates},
journal = {Artificial Intelligence},
volume = {330},
pages = {104099},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2024.104099},
url = {https://www.sciencedirect.com/science/article/pii/S0004370224000353},
author = {Sanja Lukumbuzya and Magdalena Ortiz and Mantas Šimkus},
keywords = {Description logics, Closed predicates, Datalog, Query rewriting},
abstract = {We study the relative expressiveness of ontology-mediated queries (OMQs) formulated in the expressive Description Logic ALCHOIQ extended with closed predicates. In particular, we present a polynomial time translation from OMQs into Datalog with negation under the stable model semantics, the formalism that underlies Answer Set Programming. This is a novel and non-trivial result: the considered OMQs are not only non-monotonic, but also feature a tricky combination of nominals, inverse roles, and counting. We start with atomic queries and then lift our approach to a large class of first-order queries where quantification is “guarded” by closed predicates. Our translation is based on a characterization of the query answering problem via integer programming, and a specially crafted program in Datalog with negation that finds solutions to dynamically generated systems of integer inequalities. As an important by-product of our translation we get that the query answering problem is co-NP-complete in data complexity for the considered class of OMQs. Thus, answering these OMQs in the presence of closed predicates is not harder than answering them in the standard setting. This is not obvious as closed predicates are known to increase data complexity for some existing ontology languages.}
}
@article{HONG2022101645,
title = {Improving the accuracy of schedule information communication between humans and data},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101645},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101645},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001094},
author = {Ying Hong and Haiyan Xie and Gary Bhumbra and Ioannis Brilakis},
keywords = {Construction schedules, Information exchange, Semi-structured data, Ontology},
abstract = {Construction schedules are written instructions of construction execution shared between stakeholders for essential project information exchange. However, construction schedules are semi-structured data that lack semantic details and coherence within and across projects. This study proposes an ontology-based Recurrent Neural Network approach to bi-directionally translate between human written language and machinery ontological language. The proposed approach is assessed in three areas: text generation accuracy, machine readability, and human understandability. This study collected 30 project schedules with 19,589 activities (sample size = 19,589) from a Tier-1 contractor in the UK. The experimental results indicate that: (1) precision and recall of text generation LSTM-RNN model is 0.991 and 0.874, respectively; (2) schedule readability improved by increasing the semantic distinctiveness, measured using the cosine similarity which was reduced from 0.995 to 0.990 (p < 0.01); (3) schedule understandability improved from 75.90% to 85.55%. The proposed approach formalises text descriptions in construction schedules and other construction documents with less labour investment. It supports contractors to establish knowledge management systems to learn from historic data and make more informed decisions in future similar scenarios.}
}
@article{GAO2025106146,
title = {Lifecycle framework for AI-driven parametric generative design in industrialized construction},
journal = {Automation in Construction},
volume = {174},
pages = {106146},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106146},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525001864},
author = {Maggie Y. Gao and Chao Li and Frank Petzold and Robert L.K. Tiong and Yaowen Yang},
keywords = {Industrialized construction, Generative design, Design integration, Building information modelling (BIM), Design optimization, Knowledge graphs},
abstract = {In the Architecture, Engineering, and Construction (AEC) industry, design processes remain fragmented across architectural, structural, and mechanical domains, limiting integration and optimization opportunities throughout building lifecycles. This paper investigates how artificial intelligence can be leveraged to create a comprehensive framework for parametric generative design in industrialized construction that integrates multiple design disciplines and optimization criteria. The methodology employs knowledge graph question answering (KGQA) enabled by large language models (LLMs) to acquire design requirements and constraints, implements multi-objective optimization algorithms to balance competing criteria, and establishes a three-tier priority hierarchy to resolve conflicts in cross-domain design processes. The framework demonstrates significant improvements in a real-world case study, achieving 15.8 % reduction in lifecycle costs, 21.2 % decrease in energy consumption, and significantly reducing preliminary design modelling time. These findings provide valuable insights for AEC practitioners seeking to implement human-AI collaborative design workflows and illustrate pathways for integrating domain-specific knowledge with advanced AI systems.}
}
@article{CHO2024,
title = {Task-Specific Transformer-Based Language Models in Health Care: Scoping Review},
journal = {JMIR Medical Informatics},
volume = {12},
year = {2024},
issn = {2291-9694},
doi = {https://doi.org/10.2196/49724},
url = {https://www.sciencedirect.com/science/article/pii/S2291969424001650},
author = {Ha Na Cho and Tae Joon Jun and Young-Hak Kim and Heejun Kang and Imjin Ahn and Hansle Gwon and Yunha Kim and Jiahn Seo and Heejung Choi and Minkyoung Kim and Jiye Han and Gaeun Kee and Seohyun Park and Soyoung Ko},
keywords = {transformer-based language models, medicine, health care, medical language model},
abstract = {Background
Transformer-based language models have shown great potential to revolutionize health care by advancing clinical decision support, patient interaction, and disease prediction. However, despite their rapid development, the implementation of transformer-based language models in health care settings remains limited. This is partly due to the lack of a comprehensive review, which hinders a systematic understanding of their applications and limitations. Without clear guidelines and consolidated information, both researchers and physicians face difficulties in using these models effectively, resulting in inefficient research efforts and slow integration into clinical workflows.
Objective
This scoping review addresses this gap by examining studies on medical transformer-based language models and categorizing them into 6 tasks: dialogue generation, question answering, summarization, text classification, sentiment analysis, and named entity recognition.
Methods
We conducted a scoping review following the Cochrane scoping review protocol. A comprehensive literature search was performed across databases, including Google Scholar and PubMed, covering publications from January 2017 to September 2024. Studies involving transformer-derived models in medical tasks were included. Data were categorized into 6 key tasks.
Results
Our key findings revealed both advancements and critical challenges in applying transformer-based models to health care tasks. For example, models like MedPIR involving dialogue generation show promise but face privacy and ethical concerns, while question-answering models like BioBERT improve accuracy but struggle with the complexity of medical terminology. The BioBERTSum summarization model aids clinicians by condensing medical texts but needs better handling of long sequences.
Conclusions
This review attempted to provide a consolidated understanding of the role of transformer-based language models in health care and to guide future research directions. By addressing current challenges and exploring the potential for real-world applications, we envision significant improvements in health care informatics. Addressing the identified challenges and implementing proposed solutions can enable transformer-based language models to significantly improve health care delivery and patient outcomes. Our review provides valuable insights for future research and practical applications, setting the stage for transformative advancements in medical informatics.}
}
@article{WANG2025102633,
title = {Multi-omics integrative analysis reveals novel genetic loci and candidate genes for ischemic stroke},
journal = {Molecular Therapy Nucleic Acids},
volume = {36},
number = {3},
pages = {102633},
year = {2025},
issn = {2162-2531},
doi = {https://doi.org/10.1016/j.omtn.2025.102633},
url = {https://www.sciencedirect.com/science/article/pii/S2162253125001878},
author = {Min Wang and Chong Xu and Xiaoshan Du and Tian Zhu and Xitong Yang and Fuhui Duan and Guangyan Wang and Yongchun Zuo and Huaqiu Chen and Guangming Wang},
keywords = {MT: Bioinformatics, ischemic stroke, IS, genetic loci, Mendelian randomization, LLM-based knowledge base},
abstract = {Ischemic stroke (IS) is a major cause of disability and mortality, but its genetic basis remains poorly understood. This study integrates data from three large-scale genome-wide association studies (GWASs), the GWAS Catalog, MEGASTROKE, and Open GWAS, to identify novel genetic loci linked to IS. Our meta-analysis revealed 124 new IS-associated loci, with enrichment in genes involved in cerebrovascular function, inflammation, and metabolism. Candidate genes like CPNE1, HSD17B12, and SFXN4 are linked to lipid metabolism, immune response, and iron metabolism, indicating diverse pathogenic mechanisms in IS. Further analyses, including expression quantitative trait locus (eQTL) and protein quantitative trait locus (pQTL), confirmed the relevance of these genes in the brain. Mendelian randomization and colocalization analyses highlighted seven genes with potential causal relationships to IS. Single-cell RNA sequencing identified differential gene expression in endothelial cells, implicating these genes in vascular dysfunction. Functional validation in knockout mouse models showed HSD17B12’s role in fatty acid metabolism, linking it to cerebrovascular diseases. We also developed StrokeGene, an intelligent assistant based on large language models (LLMs) to aid IS genetic research. StrokeGene offers insights into IS pathophysiology. Collectively, these findings substantially advance the understanding of IS genetics and provide a foundation for precision medicine strategies in stroke prevention and treatment.}
}
@article{WANG2023105243,
title = {Named entity annotation schema for geological literature mining in the domain of porphyry copper deposits},
journal = {Ore Geology Reviews},
volume = {152},
pages = {105243},
year = {2023},
issn = {0169-1368},
doi = {https://doi.org/10.1016/j.oregeorev.2022.105243},
url = {https://www.sciencedirect.com/science/article/pii/S0169136822005510},
author = {Chengbin Wang and Yuanjun Li and Jianguo Chen and Xiaogang Ma},
keywords = {Entity corpus, Porphyry copper deposit, Entity annotation schema, Text mining, Ontology model},
abstract = {Owing to the development of natural language processing and deep learning models, geological text data have become a vital resource for knowledge discovery and have attracted the attention of publishers, academic organizations, and domain scientists. However, the extraction of information from unstructured literature still remains a challenge, in which a fundamental issue is the categories and the type of discipline-specific information. This paper presents an effective workflow of building and applying ontologies in geoscience text mining, which includes a use case-driven method for building an ontology model of porphyry copper deposits, an entity annotation schema for text mining, and implementation of them to tackle real-world data. First, the Dexing porphyry copper deposit was selected as a case study to guide the construction of the ontology model. Text data in this study provided a series of entity instances. By analyzing both domain knowledge of mineral deposit models and the instance data, we built classes in the ontology. Second, with the established ontology, a named entity annotation schema comprising 21 entity tokens was designed to scale up the text mining tasks. Third, based on the annotation schema, a draft corpus with more than 200,000 words and a finely corrected corpus of 53,339 words were built for training a geological entity recognizer for porphyry copper deposits. The performance of the geological entity recognizer and the statistical distribution of entities in the corpus prove that the workflow presented in this study is effective for designing entity annotation schemas and facilitating large-scale text data mining in geoscience.}
}
@article{BORNET2025103108,
title = {Comparing neural language models for medical concept representation and patient trajectory prediction},
journal = {Artificial Intelligence in Medicine},
volume = {163},
pages = {103108},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103108},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725000430},
author = {Alban Bornet and Dimitrios Proios and Anthony Yazdani and Fernando Jaume-Santero and Guy Haller and Edward Choi and Douglas Teodoro},
keywords = {Neural language models, Medical concept embeddings, Electronic health records, Patient trajectory prediction, Clinical outcome prediction, Biomedical terminologies, Hierarchical clustering},
abstract = {Effective representation of medical concepts is crucial for secondary analyses of electronic health records. Neural language models have shown promise in automatically deriving medical concept representations from clinical data. However, the comparative performance of different language models for creating these empirical representations, and the extent to which they encode medical semantics, has not been extensively studied. This study aims to address this gap by evaluating the effectiveness of three popular language models - word2vec, fastText, and GloVe - in creating medical concept embeddings that capture their semantic meaning. By using a large dataset of digital health records, we created patient trajectories and used them to train the language models. We then assessed the ability of the learned embeddings to encode semantics through an explicit comparison with biomedical terminologies, and implicitly by predicting patient outcomes and trajectories with different levels of available information. Our qualitative analysis shows that empirical clusters of embeddings learned by fastText exhibit the highest similarity with theoretical clustering patterns obtained from biomedical terminologies, with a similarity score between empirical and theoretical clusters of 0.88, 0.80, and 0.92 for diagnosis, procedure, and medication codes, respectively. Conversely, for outcome prediction, word2vec and GloVe tend to outperform fastText, with the former achieving AUROC as high as 0.78, 0.62, and 0.85 for length-of-stay, readmission, and mortality prediction, respectively. In predicting medical codes in patient trajectories, GloVe achieves the highest performance for diagnosis and medication codes (AUPRC of 0.45 and of 0.81, respectively) at the highest level of the semantic hierarchy, while fastText outperforms the other models for procedure codes (AUPRC of 0.66). Our study demonstrates that subword information is crucial for learning medical concept representations, but global embedding vectors are better suited for more high-level downstream tasks, such as trajectory prediction. Thus, these models can be harnessed to learn representations that convey clinical meaning, and our insights highlight the potential of using machine learning techniques to semantically encode medical data.}
}
@article{LEE2023105122,
title = {Visual Brick model authoring tool for building metadata standardization},
journal = {Automation in Construction},
volume = {156},
pages = {105122},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105122},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003825},
author = {Sangkeun Lee and Borui Cui and Mahabir S. Bhandari and Piljae Im},
keywords = {Building data, Standardization, Ontology, Interoperability, Automation, Smart building},
abstract = {The Brick ontology is a unified semantic metadata standard for building assets and their relationships, serving as a key enabler for effective interoperability and automation of building systems and analytics. However, creating a Brick model, in other words, standard semantic metadata based on the Brick ontology for a building dataset, can be a complex task. This paper presents two case studies of the creation of Brick models for real-world residential and commercial building datasets, highlighting the challenges during the Brick model creation process. Additionally, the paper introduces VizBrick, an interactive authoring tool for creating semantic building metadata. VizBrick facilitates the creation of Brick models by providing an intuitive visual interface and interactive capabilities, such as keyword search, automatic mapping suggestions, and recommendations. The use of VizBrick is shown to significantly reduce the time and effort required during the Brick model creation process.}
}
@article{REITZ2021552,
title = {Using the Unified Medical Language System to Expand the Operative Stress Score – First Use Case},
journal = {Journal of Surgical Research},
volume = {268},
pages = {552-561},
year = {2021},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2021.07.030},
url = {https://www.sciencedirect.com/science/article/pii/S0022480421004972},
author = {Katherine M. Reitz and Daniel E. Hall and Myrick C. Shinall and Paula K. Shireman and Jonathan C. Silverstein},
keywords = {Unified medical language system, Surgical procedures, Medical information computing, Medical informatics, Ontologies},
abstract = {ABSTRACT
Background
The Unified Medical Language System (UMLS) maps relationships between and within >100 biomedical vocabularies, including Current Procedural Terminology (CPT) codes, creating a powerful knowledge resource which can accelerate clinical research.
Methods
We used synonymy and concepts relating hierarchical structure of CPT codes within the UMLS, (1) guiding surgical experts in expanding the Operative Stress Score (OSS) from 565 originally rated CPT codes to additional, 1,853 related procedures; (2) establishing validity of the association between the added OSS ratings and 30-day outcomes in VASQIP (2015-2018).
Results
The UMLS Metathesaurus and Semantic Network was converted into an interactive graph database (https://github.com/dbmi-pitt/UMLS-Graph) delineating ontology relatedness. From this UMLS-graph, the CPT hierarchy was queried obtaining all paths from each code to the hierarchical apex. Of 1,853 added ratings, 43% and 76% were siblings and cousins of original OSS CPT codes. Of 857,577 VASQIP cases (mean age, 64±11years; 91% male; 75% white), 786,122 (92%) and 71,455 (8%) were rated in the original and added OSS. Compared to original, added OSS cases included more females (14% versus 9%) and frail patients (25% versus 19%) undergoing high stress procedures (11% versus 8%; all P <.001). Postoperative mortality consistently increased with OSS. Very low stress procedures had <0.5% (original, 0.4% [95%CI, 0.4%–0.5%] versus added, 0.9% [95%CI, 0.6%–1.2%]) and very high 3.8% (original, 3.5% [95%CI, 3.0%–4.0%] versus added, 5.8% [95%CI, 4.6–7.3%]) mortality rates.
Conclusions
The synonymy and concepts relating biomedical data within the UMLS can be abstracted and efficiently used to expand the utility of existing clinical research tools.}
}
@article{ZHONG2024105316,
title = {Domain-specific language models pre-trained on construction management systems corpora},
journal = {Automation in Construction},
volume = {160},
pages = {105316},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105316},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524000529},
author = {Yunshun Zhong and Sebastian D. Goodfellow},
keywords = {Construction management, Domain-specific large language models, Pre-training, Natural language processing (NLP), Transfer learning, Text classification (TC), Named entity recognition (NER), Corpus development},
abstract = {The rising demand for automated methods in the Construction Management Systems (CMS) sector highlights opportunities for the Transformer architecture, which enables pre-training Deep Learning models on large, unlabeled datasets for Natural Language Processing (NLP) tasks, outperforming traditional Recurrent Neural Network models. However, their potential in the CMS domain remains underexplored. Therefore, this research produced the first CMS domain corpora from academic papers and introduced an end-to-end pipeline for pre-training and fine-tuning domain-specific Pre-trained Language Models. Four corpora were constructed and transfer learning was employed to pre-train BERT and RoBERTa using the corpora. The best-performing models were then fine-tuned and outperformed models pre-trained on general corpora. In two key NLP tasks, text classification using an infrastructure condition prediction dataset and named entity recognition using an automatic construction control dataset, domain-specific pre-training improved F1 scores by 5.9% and 8.5%, respectively. These promising results demonstrate extended applicability beyond CMS to the Architecture, Engineering, and Construction sectors.}
}
@article{FRANKE2024781,
title = {Operational AI analysis flows: A framework for the efficient integration and execution within test processes},
journal = {Procedia CIRP},
volume = {128},
pages = {781-786},
year = {2024},
note = {34th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.06.037},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124007698},
author = {Marco Franke and Karl A. Hribernik and Klaus-Dieter Thoben},
keywords = {Artificial intelligence, design, Optimisation in design, Knowledge sharing, communication, Ontologies in design},
abstract = {The European Commission’s “Flightpath 2050” highlights the critical role of the design phase in complex aircraft development. Ensuring the aircraft’s functionality, particularly for certification, represents a significant proportion (around 50%) of the total development cost. The increasing complexity of the testing process presents challenges. For example, the creation of simulation models for HIL testing and the performance of exploratory tests for certification are becoming increasingly complex due to the growing complexity of aircraft design. To address this challenge, this research aims to integrate AI modelling and data analysis into aircraft test processes as part of the design process. The aim is to develop a user-friendly modelling tool for test engineers without the need for data analyst expertise. The approach focuses on achieving seamless interoperability and easy integration of test-related data sources and AI models, eliminating the need for complex scripting in scripting languages such as Python or Matlab. To achieve this goal, a domain ontology and generic wrappers have been developed to map generic AI models to the TensorFlow and Matlab tools. At the same time, data sources are connected via a data integration tool. The evaluation successfully demonstrates the ability to analyse the correlation between sensors in an experimental cooling system without understanding the data format of the data sources and without the scripting of the AI model to model and execute the AI models.}
}
@article{ZAMIRALOV202132,
title = {Knowledge graph mining for realty domain using dependency parsing and QAT models},
journal = {Procedia Computer Science},
volume = {193},
pages = {32-41},
year = {2021},
note = {10th International Young Scientists Conference in Computational Science, YSC2021, 28 June – 2 July, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020457},
author = {Alexander Zamiralov and Timur Sohin and Nikolay Butakov},
keywords = {ontology, knowledge-graph, QAT, neural network, dependency parsing, real estates},
abstract = {The real estate business has a lot of risks, and in order to minimize them, you need a lot of information from different sources. Systems based on natural language processing can help customers find this information more easily: question answering, information retrieval, etc. The existing method of question answering requires data aligned with possible questions, which are not easy to obtain, in contrast, the knowledge-graph provides structured information. In this paper, we propose semi-automated ontology generation for the realty domain and a subsequent method for information retrieval related to the knowledge-graph of this ontology. The first contribution is the method for relation extraction method based on dependency-parsing and semantic similarity evaluation, which allows us to form ontology for a particular domain. The second contribution is knowledge-graph completion method based on question answering over text neural network. Our experimental analysis shows the efficiency of the proposed approaches.}
}
@article{BUREKA20201053,
title = {A Lightweight Approach to the Multi-perspective Modeling of Processes and Objects},
journal = {Procedia Computer Science},
volume = {176},
pages = {1053-1062},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920320019},
author = {Patryk Bureka and Heinrich Herre},
keywords = {Semantic Web, Knowledge Representation, Process Modeling, Conceptual Modeling, Ontology},
abstract = {Process modeling has a broad range of applications, varying from business and system engineering, via artifact design, up to natural process modeling utilized in natural sciences. Over the last decades, various sophisticated languages and frameworks have been developed to support process modeling. The current paper discusses an approach to process modeling, which is, in contrast to many existing solutions, intended for the integrated process and object modeling. Furthermore, it is designed to be a lightweight approach with only a few constructs, which, however, permit the representation of processes from various perspectives. The developed solution provides an abstract language-independent model (ontology), partial formalization in first-order logic as well as a Web Ontology Language (OWL) implementation.}
}
@article{CUZZOCREA2024100363,
title = {Modeling and supporting adaptive Complex Data-Intensive Web Systems via XML and the O-O paradigm: The OO-XAHM model},
journal = {Array},
volume = {23},
pages = {100363},
year = {2024},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2024.100363},
url = {https://www.sciencedirect.com/science/article/pii/S2590005624000298},
author = {A. Cuzzocrea and E. Fadda},
keywords = {Adaptive Web Systems, Object-oriented adaptive web systems, Adaptive data-intensive web systems, Adaptive Complex Data-Intensive Web Systems},
abstract = {The data model is a critical component of an Adaptive Web System (AWS). The major goals of such a data model are describing the application domain of the AWS and capturing data about the user in order to support the “adaptation effect”. There have been many proposals for data models, principally based on knowledge representation, machine learning, logic and reasoning, and, recently, ontologies. These models are focused on the implementation of the core layer of AWS, that is realizing the adaptation of contents and presentations of the system, but sometimes they are poor with respect to the application domain design. In this paper, we present an extension of the state-of-the-art XML Adaptive Hypermedia Model (XAHM), Object-Oriented XAHM (OO-XAHM) that supports the application domain modeling using an object-oriented approach. We also provide the formal definition of the model, its description via Unified Modeling Language (UML), and its implementation using XML Schema. Finally, we provide a complete case study that focuses the attention on the well-known Italian archaeological site Pompeii.}
}
@article{XU201915,
title = {Flexible parametric FEA modeling for product family based on script fragment grammar},
journal = {Computers in Industry},
volume = {111},
pages = {15-25},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519300752},
author = {Xuesong Xu and Gang Xiao and Gonghui Lou and Jiawei Lu and Jun Yang and Zhenbo Cheng},
keywords = {Parametric FEA, Ontology, SFG, FEA script generation engine (SGE)},
abstract = {The flexible finite element analysis (FEA) modeling process is addressed within the framework of scripting programming language such as ANSYS Parametric Design Language(APDL). Resorting to recently proposed upper ontology and specific ontology, the FEA modeling processes are expressed as the entities and relations among entities in an ontology tree. An algorithm to obtain a script fragment grammar (SFG) that describe the combination rules of script fragments by instantiating FEA ontology tree is provided. In addition, a method of automatically generating FEA parametric scripts based on SFG derivations is proposed. Then, the whole procedure is applied to develop an automatically generating FEA script software for an air separator product family. The proposed method can effectively reduce repetitive operations in the parametric FEA modeling process, which thereby improves the overall efficiency of the FEA modeling process.}
}
@article{MARKOVA2024101625,
title = {A dialogical perspective of interaction: the case of people with deaf/blindness},
journal = {Language Sciences},
volume = {103},
pages = {101625},
year = {2024},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101625},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000147},
author = {Ivana Marková},
keywords = {Dialogical strategies, Deaf/blindness, Self-other communicative interdependence, Unique single cases},
abstract = {This article considers dialogicality as a dynamic ontology and epistemology, and as interaction in concrete daily situations. These two features of dialogicality are presented in selected examples involving communication of people with congenital deaf/blindness and their carers. Since people with deaf/blindness cannot use verbal language in their dialogues, they make themselves understood to their partners by using a variety of innovative non-verbal strategies. For example, they improvise, repeat touching gestures, overextend meanings of signs, guess meanings of co-participants, and otherwise. Each dialogical situation is a unique single case, in which participants use simultaneously different modalities of communication and attempt to balance their subjective and intersubjective activities.}
}
@article{CHASSERAY2023324,
title = {Knowledge extraction from textual data and performance evaluation in an unsupervised context},
journal = {Information Sciences},
volume = {629},
pages = {324-343},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.01.150},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523001640},
author = {Yohann Chasseray and Anne-Marie Barthe-Delanoë and Stéphane Négny and Jean-Marc {Le Lann}},
keywords = {Natural Language Processing, Performance measure, Ontological relation extraction, Knowledge base, Automated validation},
abstract = {Among the incoming challenges in monitoring systems, the aggregation, synthesis and management of knowledge through ontological structures hold an essential place. Existing knowledge extraction systems often use a supervised approach that relies on annotated data, inducing implicitly a fastidious annotation process. Current research is towards the definition of unsupervised or semi-supervised systems, allowing a wider range of knowledge extraction. The evaluation of such systems, performing knowledge extraction using natural language processing methods requires performance indicators. The indicators usually used in such evaluations have limitations in the specific context of knowledge extraction for unsupervised ontology population. Thus, the definition of new evaluation methods becomes a need arising from the singularity of the harvested data, especially when these are not annotated. Hence, this article proposes a method for measuring performance in unsupervised context where reference data and extracted data do not overlap optimally. The proposed evaluation method is based on the exploitation of data that serve as a reference but are not specifically linked to the data used for extraction, which makes it an original evaluation method. To apply the performance measure on concrete cases, this paper also presents an unsupervised self-feeding rule-based approach for domain-independent ontology population from textual data.}
}
@article{ZOLFAGHARNASAB2025100525,
title = {A novel rule-based expert system for early diagnosis of bipolar and Major Depressive Disorder},
journal = {Smart Health},
volume = {35},
pages = {100525},
year = {2025},
issn = {2352-6483},
doi = {https://doi.org/10.1016/j.smhl.2024.100525},
url = {https://www.sciencedirect.com/science/article/pii/S2352648324000813},
author = {Mohammad Hossein Zolfagharnasab and Siavash Damari and Madjid Soltani and Artie Ng and Hengameh Karbalaeipour and Amin Haghdadi and Masood Hamed Saghayan and Farzam Matinfar},
keywords = {Clinical Decision Support, Rule-based Reasoning, Healthcare Informatics, Expert system, Bipolar disorder, Computer science},
abstract = {A confident and timely diagnosis of mental illnesses is one of the primary challenges practitioners repeatedly encounter when they start treating new patients. However, diagnosing can quickly become problematic as the subjects expose comparative symptoms among mental illnesses. Due to influencing a broad populace among mental ailments, an adjusted differentiation between Major Depressive Disorder, Mania Bipolar Disorder, Depressive Bipolar Disorder, and ordinary individuals with mild symptoms is one of the critical subjects for community health. This study responded to the described problem by proposing a novel rule-based Expert System, which evaluates the impact of disorder symptoms on the Certainty Factor concerning each mental status. The semantic rules are developed based on the recommendation of experts, and the implementation is carried out using Prolog and C# languages. Furthermore, an easy-to-use user interface is considered to facilitate the system workflow. The consistency of the developed framework is established by performing rigorous tests by expert psychiatrists as well as 120 clinical samples collected from private samples. Based on the results, the current model classifies mental disorder cases with a success rate of 93.33% using only the 17 symptoms specified in the ontology model. Furthermore, a questionnaire that measures user satisfaction after the test also achieves a mean score of 3.56 out of 4, which indicates a high degree of user acceptance. As a result, it is concluded that the current framework is a reliable tool for achieving a solid diagnosis in a shorter period.}
}
@article{COX2020,
title = {Visualization Environment for Federated Knowledge Graphs: Development of an Interactive Biomedical Query Language and Web Application Interface},
journal = {JMIR Medical Informatics},
volume = {8},
number = {11},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/17964},
url = {https://www.sciencedirect.com/science/article/pii/S2291969420000800},
author = {Steven Cox and Stanley C Ahalt and James Balhoff and Chris Bizon and Karamarie Fecho and Yaphet Kebede and Kenneth Morton and Alexander Tropsha and Patrick Wang and Hao Xu},
keywords = {knowledge graphs, clinical data, biomedical data, federation, ontologies, semantic harmonization, visualization, application programming interface, translational science, clinical practice},
abstract = {Background
Efforts are underway to semantically integrate large biomedical knowledge graphs using common upper-level ontologies to federate graph-oriented application programming interfaces (APIs) to the data. However, federation poses several challenges, including query routing to appropriate knowledge sources, generation and evaluation of answer subsets, semantic merger of those answer subsets, and visualization and exploration of results.
Objective
We aimed to develop an interactive environment for query, visualization, and deep exploration of federated knowledge graphs.
Methods
We developed a biomedical query language and web application interphase—termed as Translator Query Language (TranQL)—to query semantically federated knowledge graphs and explore query results. TranQL uses the Biolink data model as an upper-level biomedical ontology and an API standard that has been adopted by the Biomedical Data Translator Consortium to specify a protocol for expressing a query as a graph of Biolink data elements compiled from statements in the TranQL query language. Queries are mapped to federated knowledge sources, and answers are merged into a knowledge graph, with mappings between the knowledge graph and specific elements of the query. The TranQL interactive web application includes a user interface to support user exploration of the federated knowledge graph.
Results
We developed 2 real-world use cases to validate TranQL and address biomedical questions of relevance to translational science. The use cases posed questions that traversed 2 federated Translator API endpoints: Integrated Clinical and Environmental Exposures Service (ICEES) and Reasoning Over Biomedical Objects linked in Knowledge Oriented Pathways (ROBOKOP). ICEES provides open access to observational clinical and environmental data, and ROBOKOP provides access to linked biomedical entities, such as “gene,” “chemical substance,” and “disease,” that are derived largely from curated public data sources. We successfully posed queries to TranQL that traversed these endpoints and retrieved answers that we visualized and evaluated.
Conclusions
TranQL can be used to ask questions of relevance to translational science, rapidly obtain answers that require assertions from a federation of knowledge sources, and provide valuable insights for translational research and clinical practice.}
}
@incollection{PEREZPAREDES2025,
title = {Corpus Linguistics and Computer-Assisted Language Learning},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00489-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041004890},
author = {Pascual Pérez-Paredes},
keywords = {CALL, Corpus linguistics, Data-driven learning, Language learning, Language teaching, Language data, Language patterns},
abstract = {This entry explores the impact of Corpus Linguistics (CL) on Computer-Assisted Language Learning (CALL). CALL studies the use of computers in language learning and teaching. The use of corpora in language learning has facilitated the use of authentic language and language pattern analysis in language classrooms. The entry explores some of the most prominent resources in the field such as the emergence of Data-driven Learning (DDL) and its focus on increasing learners' awareness about language patterns, enabling learners to identify and generalize language patterning regularities that occur in concordance lines.}
}
@article{CECCONI2025100991,
title = {OntoPortal-Astro, a semantic artefact catalogue for astronomy},
journal = {Astronomy and Computing},
volume = {53},
pages = {100991},
year = {2025},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2025.100991},
url = {https://www.sciencedirect.com/science/article/pii/S2213133725000642},
author = {Baptiste Cecconi and Laura Debisschop and Sébastien Derrière and Mireille Louys and Carmen Corre and Nina Grau and Cl’ement Jonquet},
keywords = {Astronomy, Heliophysics, Planetary sciences, Semantic artefact, Vocabularies, Ontologies, Thesaurus, Terminologies, Open science, Semantic artefact catalogue, OntoPortal, Semantic web, Metadata},
abstract = {The astronomy communities are widely recognised as mature communities for their open science practices. However, while their data ecosystems are rather advanced and permit efficient data interoperability, there are still gaps between these ecosystems. Semantic artefacts (SAs) – e.g., ontologies, thesauri, vocabularies or metadata schemas – are a means to bridge that gap as they allow to semantically described the data and map the underlying concepts. The increasing use of SAs in astronomy presents challenges in description, selection, evaluation, trust, and mappings. The landscape remains fragmented, with SAs scattered across various registries in diverse formats and structures – not yet fully developed or encoded with rich semantic web standards like OWL or SKOS – and often with overlapping scopes. Enhancing data semantic interoperability requires common platforms to catalogue, align, and facilitate the sharing of FAIR (Findable, Accessible, Interoperable and Reusable) SAs. In the frame of the FAIR-IMPACT project, we prototyped a SA catalogue for astronomy, heliophysics and planetary sciences. This exercise resulted in improved vocabulary and ontology management in the communities, and is now paving the way for better interdisciplinary data discovery and reuse. This article presents current practices in our discipline, reviews candidate SAs for such a catalogue, presents driving use cases and the perspective of a real production service for the astronomy community based on the OntoPortal technology, that will be called OntoPortal-Astro.}
}
@article{CHEBBA2018183,
title = {Attributed and n-ary relations in OWL for knowledge modeling},
journal = {Computer Languages, Systems & Structures},
volume = {54},
pages = {183-198},
year = {2018},
issn = {1477-8424},
doi = {https://doi.org/10.1016/j.cl.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1477842417302026},
author = {Asmaa Chebba and Thouraya Bouabana-Tebibel and Stuart H. Rubin},
keywords = {Knowledge modeling, Attributed relation, N-ary relation, Ontology, OWL, Protégé, Randomization, Reuse},
abstract = {Knowledge modeling is the basis for the whole process of conceptual design. We aim, in this paper, at representing knowledge through an efficient model that enables efficient reuse. The knowledge model must be both expressive, so as not to omit any pertinent knowledge, and computable to allow for reasoning and learning. The ontology model, along with its well-known description language OWL (Web Ontology Language), is commonly used for this purpose. However, despite the strong expressiveness of OWL, this ontology language still shows limitations with regard to some modeling needs. We aim at promoting the representation of such aspects. We especially deal with the lack of an adequate representation for attributed relations and n-ary relations. The attributed relation, which is a relation that possesses its own attributes, is introduced into OWL, while preserving the basic semantics. The n-ary relation is also added to OWL by introducing a supplementary representational layer that is defined with a sound semantics, inspired from the DLR logic.}
}
@article{BARROWS2022107702,
title = {Markup language for chemical process control and simulation},
journal = {Computers & Chemical Engineering},
volume = {160},
pages = {107702},
year = {2022},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2022.107702},
url = {https://www.sciencedirect.com/science/article/pii/S009813542200045X},
author = {Elina Barrows and Katherine Martin and Thérèse Smith},
keywords = {Synthesis automation, Temperature profile, Safety, Simulation, Semantic labels, XML, Process composition},
abstract = {Exothermic reactions can yield different products, depending upon the available heat energy, a consequence of the reaction rate. To control the yield of desired products, control of the rate of temperature rise is used. The acceptable range of temperature vs. time can be expressed in various ways, e. g., functions, individual temperature values. To afford humans this variety, and to facilitate the transfer of data among computational tools from multiple vendors, such as calorimetry and analysis tools, use of semantic labels, compatible with existing tools, has been implemented. An extension of XML to support expression of the range of acceptable temperature behavior vs. time as a reaction proceeds is described herein. A markup language organized to express the rich ontology of concepts used in process safety seems warranted, and though chemistry schema exist, none was found that specified the reactor temperature boundaries concept. We propose schema components for this concept.}
}
@article{ZHU2025106270,
title = {Component-based BIM-semantic web integration for enhanced robotic visual perception},
journal = {Automation in Construction},
volume = {177},
pages = {106270},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106270},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525003103},
author = {Aiyu Zhu and Zimu Shao and Xini Chai and Encheng Ma and Qingyang Li and Hongyu Ye and Hong Zhang},
keywords = {Construction robot visual perception, Multi-source BIM integration, Building semantic web, Image recognition, Ontology-based modeling},
abstract = {Visual perception plays a crucial role in enhancing construction robots’ real-time performance by enabling accurate object recognition, localization, and scene understanding in complex construction environments. While current research focuses on visual recognition for tasks like spatial localization and navigation, it falls short of addressing the more advanced functions necessary for comprehensive construction planning. This paper proposes an integrated framework that combines building semantic web technologies with image recognition techniques to significantly improve robotic perception. By merging real-time image recognition with BIM-based semantic mapping, robots can gather critical information on component types, spatial relationships, and construction requirements. Case studies illustrate the framework’s ability to improve adaptability, precision, and efficiency in both static and dynamic construction environments, thereby enabling more intelligent, automated, and efficient robotic construction processes, with the potential for broader applications in the future of construction technology.}
}
@article{YOUSAF20213355,
title = {Modelling and Verification of Context-Aware Intelligent Assistive Formalism},
journal = {Computers, Materials and Continua},
volume = {71},
number = {2},
pages = {3355-3373},
year = {2021},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.023019},
url = {https://www.sciencedirect.com/science/article/pii/S1546221821002186},
author = {Shahid Yousaf and Hafiz {Mahfooz Ul Haque} and Abbas Khalid and Muhammad Adnan Hashmi and Eraj Khan},
keywords = {Context-awareness, multi-agents, colored petri net, ontology},
abstract = {Recent years have witnessed the expeditious evolution of intelligent smart devices and autonomous software technologies with the expanded domains of computing from workplaces to smart computing in everyday routine life activities. This trend has been rapidly advancing towards the new generation of systems where smart devices play vital roles in acting intelligently on behalf of the users. Context-awareness has emerged from the pervasive computing paradigm. Context-aware systems have the ability to acquire contextual information from the surrounding environment autonomously, perform reasoning on it, and then adapt their behaviors accordingly. With the proliferation of context-aware systems and smart sensors, real-time monitoring of environmental situations (context) has become quite trivial. However, it is often challenging because the imperfect nature of context can cause the inconsistent behavior of the system. In this paper, we propose a context-aware intelligent decision support formalism to assist cognitively impaired people in managing their routine life activities. For this, we present a semantic knowledge-based framework to contextualize the information from the environment using the protégé ontology editor and Semantic Web Rule Language (SWRL) rules. The set of contextualized information and the set of rules acquired from the ontology can be used to model Context-aware Multi-Agent Systems (CMAS) in order to autonomously plan all activities of the users and notify users to act accordingly. To illustrate the use of the proposed formalism, we model a case study of Mild Cognitive Impaired (MCI) patients using Colored Petri Nets (CPN) to show the reasoning process on how the context-aware agents collaboratively plan activities on the user's behalf and validate the correctness properties of the system.}
}
@article{MORENO2024916,
title = {Toward Clinical-Grade Evaluation of Large Language Models},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {118},
number = {4},
pages = {916-920},
year = {2024},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2023.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0360301623081348},
author = {Amy C. Moreno and Danielle S. Bitterman}
}
@article{POGODA20233203,
title = {Open versus Closed: A Comparative Empirical Assessment of Automated News Article Tagging Strategies},
journal = {Procedia Computer Science},
volume = {225},
pages = {3203-3212},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.314},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923014722},
author = {Michał Pogoda and Marcin Oleksy and Konrad Wojtasik and Tomasz Walkowiak and Bartosz Bojanowski},
keywords = {natural language processing, topic modeling, multicategorical classification, News article tagging},
abstract = {Automatically tagging news articles is a fundamental task for indexing and analyzing news streams. Most news portals a form of article tagging ontology, which is a predefined set of tags or keywords that are assigned to news articles to improve their discoverability and relevance. However, manually categorizing and tagging news articles using an ontology is a time-consuming and labor-intensive task, especially for news portals that publish a large volume of articles each day. The automation of the news article tagging process using machine learning techniques has emerged as a promising solution to address the challenge of manually categorizing and tagging a large volume of articles each day. In this paper, we explore the effectiveness of different machine learning approaches for automatically tagging news articles. Specifically, we train and compare several models using both closed-ontology and open-ontology approaches. We evaluate the performance of these models based on their ability to accurately categorize and tag news articles using comparative human evaluation. Our findings provide insights into the advantages and limitations of each approach and highlight the potential applications of automatic news article tagging.}
}
@article{EREIRASVEDOR2025105586,
title = {Dreams as a Code Language},
journal = {BioSystems},
pages = {105586},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2025.105586},
url = {https://www.sciencedirect.com/science/article/pii/S0303264725001960},
author = {João {Ereiras Vedor}},
keywords = {dreams, Code Biology, archetypes, neuroscience, psychoanalysis, Large Language Models},
abstract = {Since Freud, psychologists have sought to decipher the language of dreams, but no universal interpretive manual exists. Advances in dream neuroscience and the emergence of Code Biology have brought us closer to understanding the rules and functions underlying dream formation. Code Biology, which studies coding processes in living systems, offers a revolutionary framework for how neural and symbolic patterns generate dream narratives. Dreams are not epiphenomena; they serve crucial adaptive functions—emotional regulation, memory consolidation, and ego development. Despite energetic costs, their evolutionary conservation across species highlights their biological necessity. Cartwright’s studies confirm dreams’ essential regulatory role in survival. This article argues that dreams are coded artifacts produced by multiple organic codes—biological, neuronal, symbolic, and cultural—mediated by archetypes. Drawing on Barbieri’s, Major’s and Goodwyn work, archetypes are reframed as artifacts of embodied organic codes regulating dream formation and symbolic imagery. Integrating affective neuroscience findings and Jung’s theory, this perspective proposes emotional affects as archetypal neurodynamic patterns. A key methodological gap remains: the separation of neuroscience and clinical practice. Through clinical dream analysis, the article shows therapists act as “adaptors,” translating dream codes into meaningful narratives—a process termed psychic codepoiesis. Finally, it discusses how Large Language Models, trained on life-coding principles, may support but not replace embodied human adaptors, offering a new paradigm for understanding dreams as the foundational language of subjective life.}
}
@article{GAO2023104363,
title = {A hybrid system to understand the relations between assessments and plans in progress notes},
journal = {Journal of Biomedical Informatics},
volume = {141},
pages = {104363},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104363},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000849},
author = {Jifan Gao and Shilu He and Junjie Hu and Guanhua Chen},
keywords = {Progress note, Transformers, Medical ontology, Order information, Combiner},
abstract = {Objective:
The paper presents a novel solution to the 2022 National NLP Clinical Challenges (n2c2) Track 3, which aims to predict the relations between assessment and plan subsections in progress notes.
Methods:
Our approach goes beyond standard transformer models and incorporates external information such as medical ontology and order information to comprehend the semantics of progress notes. We fine-tuned transformers to understand the textual data and incorporated medical ontology concepts and their relationships to enhance the model’s accuracy. We also captured order information that regular transformers cannot by taking into account the position of the assessment and plan subsections in progress notes.
Results:
Our submission earned third place in the challenge phase with a macro-F1 score of 0.811. After refining our pipeline further, we achieved a macro-F1 of 0.826, outperforming the top-performing system during the challenge phase.
Conclusion:
Our approach, which combines fine-tuned transformers, medical ontology, and order information, outperformed other systems in predicting the relationships between assessment and plan subsections in progress notes. This highlights the importance of incorporating external information beyond textual data in natural language processing (NLP) tasks related to medical documentation. Our work could potentially improve the efficiency and accuracy of progress note analysis.}
}
@article{SIMONE2023103849,
title = {Industrial safety management in the digital era: Constructing a knowledge graph from near misses},
journal = {Computers in Industry},
volume = {146},
pages = {103849},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103849},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522002457},
author = {Francesco Simone and Silvia Maria Ansaldi and Patrizia Agnello and Riccardo Patriarca},
keywords = {Safety ontology, Knowledge graphs, Industrial plants, Seveso Legislation, Refineries, Risk management},
abstract = {Learning from incidents is instrumental for modern safety management. Over recent years, a positive safety trend proved a steady decrease in the number of high-consequences adverse events. However, accidents still happen, with various degrees of consequences. Learning should then be enlarged to include near misses, i.e. all those reported events that could have been resulted in an incident or accident but they did not. These events carry large sets of data, which should be investigated systematically to improve the current and future safety management era. Even though near misses are mandatory to be reported in the industrial sectors regulated by the Seveso regulation, the reports themselves are usually written in natural language. In this paper we propose a methodological solution to extract knowledge from near miss reports, relying on the development of a knowledge graph. The knowledge graph is grounded on a custom ontology developed to model information flows as contained in near miss reports. Out of the knowledge graph, an ontological explorative analysis is presented to get information of interest from large set of reports, otherwise difficult to analyze. The exploratory results provide evidence of the benefits of the proposed modelling solution to support safety monitoring and instructing safety interventions.}
}
@article{ZHAO2023103513,
title = {Construction of an aspect-level sentiment analysis model for online medical reviews},
journal = {Information Processing & Management},
volume = {60},
number = {6},
pages = {103513},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103513},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323002509},
author = {Yuehua Zhao and Linyi Zhang and Chenxi Zeng and Wenrui Lu and Yidan Chen and Tao Fan},
keywords = {Online medical review, Fine-grained sentiment analysis, Aspect-level sentiment analysis, Ontology, Knowledge graph},
abstract = {Online medical services have become increasingly popular, and patient feedback can significantly influence other patients’ medical decision-making. This study utilizes a double-layer domain ontology for conducting aspect-level sentiment analysis of reviews from online medical platforms. A double-layer aspect recognition model (OMR-ARM), aggregating the knowledge of the domain ontology, is built to identify the aspects of online medical reviews. The proposed model outperforms baseline models by up to 23.12%. Incorporating this model into a series of state-of-the-art models, the resultant OMR-ALSA model achieves a F1-score of 93.53% for aspect-level sentiment analysis of online medical reviews. Additionally, this study develops an object-aspect-sentiment knowledge graph of online medical reviews (OMR-KG) that can classify patients’ sentimental polarities towards the different aspects of online medical reviews. The proposed model and constructed KG have the potential to provide reference and guidance to sentiment analysis research in the online medical review domain, thus contributing to more informed and personalized healthcare decision-making.}
}
@article{LAAMECH20231881,
title = {Translating Usage Control Policies to Semantic Rules: A Model using OrBAC and SWRL},
journal = {Procedia Computer Science},
volume = {225},
pages = {1881-1890},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.178},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923013364},
author = {Nouha Laamech and Manuel Munier and Congduc Pham},
keywords = {Usage Control, Semantic Web Rule Language, Security},
abstract = {The increasing volume of data in various environments such as IoT and the need to maintain data privacy and security have led to the development of usage control models. Usage control policies are models that enable fine-grained access control over data by enforcing restrictions on how users can use the data. Semantic mechanisms, on the other hand, use context and meaning to identify potential security threats and prevent them from accessing sensitive information. Although not widely explored, merging these two techniques could create an efficient mechanism to help ensure the confidentiality, integrity, and availability of critical data and resources. This paper aims to encourage this research path by proposing a translation model that converts usage control rules into SWRL. In particular, we consider during our approach the notions of contexţ permission and prohibition. The proposition is validated by constructing a multi-layer proof of concept that use ontologies and OWL for implementing the translation model. Furthermore, to ascertain the practicality of our approach, a time processing evaluation is conducted, and the results are found to be satisfactory.}
}
@incollection{GALITSKY2025411,
title = {Chapter 11 - Enabling retrieval-augmented generation and knowledge graphs with discourse analysis},
editor = {Boris Galitsky},
booktitle = {Healthcare Applications of Neuro-Symbolic Artificial Intelligence},
publisher = {Academic Press},
pages = {411-461},
year = {2025},
isbn = {978-0-443-30046-2},
doi = {https://doi.org/10.1016/B978-0-443-30046-2.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044330046200003X},
author = {Boris Galitsky},
keywords = {Retrieval-augmented generation (RAG), conformal prediction, knowledge graph (KG), discourse analysis},
abstract = {We consider a number of retrieval-augmented generation (RAG) architectures to address a lack of specific information and hallucination issues of large language model (LLM)-based question answering (Q&A). We start with conformal prediction, which acts on top of LLM and maintains a set of generations instead of a single one and attempts to find the best element of this set assumed to be the “most average” one. We then proceed to LLM self-reflection series of RAG architectures predicting the multihop Q&A session before actual search for an answer. After that, we propose a mechanism for LLM to filter out answers inappropriate with respect to style. All these components need discourse-level analysis for more robust functioning. Knowledge graph (KG) and abstract meaning representation-based KG construction follow. We evaluate the contribution of all of these components to overall answer relevance and also zoom in on the role of discourse-based subsystem in each of these components. There is a substantial improvement of performance due to the five-component architecture introduced in this chapter; the contribution of discourse-based subsystems is fairly modest.}
}
@article{HASSEN2024594,
title = {Sensitive Business Process Modeling Aspects},
journal = {Procedia Computer Science},
volume = {237},
pages = {594-601},
year = {2024},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.05.144},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924011608},
author = {Mariam Ben Hassen and Faïez Gargouri},
keywords = {Knowledge management, Business process modeling, Sensitive business process, Modeling approaches and languages},
abstract = {In order to improve their performance, modern organizations are increasingly aware of the need to identify, preserve and manage the crucial knowledge (individual and collective) that are mobilized by their sensitive business processes (SBPs). Thus, it will be necessary to characterize, identify, specify, model and analyze these processes, in order to optimize the activities of identification, localization and management of the knowledge on which it is necessary to capitalize. This paper introduces the problematic of the SBP modeling. Our objective is to provide a conceptual analysis related to the concept of SBP. First of all, we propose a rigorous characterization of SBP (which distinguishes it from classic, structured and conventional BPs). Secondly, we propose a multidimensional classification of SBP modeling aspects and requirements to develop expressive, com-prehensive and rigorous models. Besides, we present an in-depth study of the different modeling approaches and languages, in order to analyze their expressiveness and their ability to perfectly and explicitly represent the new specific requirements of SBP modeling. In this study, we choose the better one positioned nowadays, BPMN 2.0, as the best suited standard for SBP representation. Finally, we propose a semantically rich conceptualization of a SBP organized in a core ontology.}
}
@article{BANIA2024104592,
title = {FiReS: A semantic model for advanced querying and prediction analysis for first responders in post-disaster response plans},
journal = {International Journal of Disaster Risk Reduction},
volume = {109},
pages = {104592},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104592},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924003546},
author = {A. Bania and O. Iatrellis and N. Samaras and T. Panagiotakopoulos},
keywords = {Natural disasters, First responders, Post-disaster response, Web semantic, Ontology, Machine learning, Data analysis},
abstract = {Natural disasters have emerged as a recurring and severe menace to the sustainability of countries, resulting in compromised environmental and infrastructural integrity, human fatalities, and significant economic repercussions. Prompt and effective response by first responders is essential in Disaster Risk Management (DRM) to reduce individuals' vulnerability and minimize environmental and infrastructural damages. However, disaster-related information is often generated by heterogeneous sources, making the first responders’ decision-making process complex and time-consuming. To address these challenges, a common conceptual model is imperative to improve interoperability among diverse organizations and software systems, enabling effective collaboration. Semantic Web technologies offer a promising solution for integrating heterogeneous data and providing well-defined meaning in the representation and exchange of DRM-related knowledge. In this context, this study introduces FiReS (First Responders System), an ontological model designed to enhance data interoperability among first responders in post-disaster response plans for advanced data analysis and machine learning prediction. The validation of FiReS is conducted through a series of case studies exploring various aspects of disaster response, such as the response time of emergency services and the volume and classification of emergency calls. This approach facilitates streamlined access, thorough analysis, and seamless exchange of information, empowering stakeholders to strengthen their disaster response strategies and foster resilience within communities.}
}
@article{KARABIYIK2025112556,
title = {Fault analysis in additive manufacturing: Identifying causes of three-dimensional printer faults using machine learning and large language models},
journal = {Journal of Systems and Software},
volume = {230},
pages = {112556},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112556},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225002250},
author = {Muhammed Abdulhamid Karabiyik},
keywords = {Machine learning, Image classification, Prompt engineering, Three dimensional-printer faults},
abstract = {Fault detection in additive manufacturing, particularly within 3D printing systems, is a critical issue that impacts the quality and reliability of the final products. Solving these challenges is essential for ensuring high standards and consistent performance in manufacturing processes. We have developed a sophisticated system that combines traditional machine learning classifiers with advanced convolutional neural networks (CNNs) and large language models (LLMs) to enhance fault detection and diagnostic capabilities. This system employs diverse machine learning models to achieve robust image-based fault classification, supported by CNNs and cutting-edge prompt engineering techniques. Central to our approach is the Prompt Evaluation Framework (PEF), which leverages strategies such as zero-shot prompting, chain-of-thought, and directional stimulus prompting to refine interactions with LLMs. This framework enables the dynamic generation of personalized explanations and resolution strategies for detected faults, thereby enhancing accessibility and usability for users across different technical backgrounds. Our experimental results indicate that this integrated methodology not only improves the accuracy of fault detection across various fault types but also significantly enhances the interpretability and usability of the outputs. These findings have considerable practical implications for quality control in additive manufacturing, highlighting the potential for broader applications in intelligent, interactive fault diagnosis systems. By leveraging the power of machine learning and LLMs, our work represents a significant advancement in methodologies for 3D print fault detection and resolution.}
}
@article{SHI2025103523,
title = {A stepwise intelligence generative method for structured maintenance guidance documents based on knowledge graph augmented LLM},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103523},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103523},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004161},
author = {Fangcheng Shi and Liang Chen and Moshi Zhou and Yue Zhao and Yu Zheng},
keywords = {Graph retrieval augmented generation, Chain of thought, Large language model, Maintenance guidance documents, Knowledge graph},
abstract = {Maintenance guidance documents (MGDs) are the basis for engineering maintenance process. At present, application of large language models (LLMs) in the generation of industrial documents have issues with inaccurate content and structure that does not match professional requirements. Therefore, this paper proposes an enhanced method that integrates professional knowledge graph retrieval augmented generation (GraphRAG) and chain-of-thought (CoT) prompts to guide LLMs to intelligently generate structured MGDs step by step. First, the LLM prompt enhancement methods are used to assist in the construction of the professional knowledge graph. Second, a CoT prompt is constructed corresponding to the stepwise characteristics of MGDs. Finally, based on the CoT prompt, the corresponding graph entity content is retrieved step by step to construct a stepwise prompt, enhancing the generation. This method has been experimentally verified in the automatic generation task of Baosteel continuous casting equipment MGDs. Compared with methods that rely solely on prompts and examples, this method significantly improves the structural controllability and content accuracy of the generated results.}
}
@article{LI2020110566,
title = {An ontology-based learning approach for automatically classifying security requirements},
journal = {Journal of Systems and Software},
volume = {165},
pages = {110566},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110566},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220300479},
author = {Tong Li and Zhishuai Chen},
keywords = {security requirements ontology, security requirements classification, linguistic pattern, natural language processing, machine learning},
abstract = {Although academia has recognized the importance of explicitly specifying security requirements in early stages of system developments for years, in reality, many projects mix security requirements with other types of requirements. Thus, there is a strong need for precisely and efficiently classifying such security requirements from other requirements in requirement specifications. Existing studies leverage lexical evidence to build probabilistic classifiers, which are domain-dependent by design and cannot effectively classify security requirements from different application domains. In this paper, we propose an ontology-driven learning approach to automatically classify security requirements. Our approach consists of a conceptual layer and a linguistic layer, which understands security requirements based on not only lexical evidence but also conceptual domain knowledge. In particular, we apply a systematic approach to identify linguistic features of security requirements based on an extended security requirements ontology and linguistic knowledge, connecting the conceptual layer with the linguistic layer. Such linguistic features are then used to train domain-independent security requirements classifiers by using machine learning techniques. We have carried out a series of experiments to evaluate the performance and generalization ability of our proposal against existing approaches. The results of the experiments show that the proposed approach outperforms existing approaches with a significant increase of F1 score (0.63 VS. 0.44) when the training dataset and the testing dataset come from different application domains, i.e., the classifiers trained by our approach can be generalized to classify security requirements from different domains.}
}
@article{DENICOLA20251943,
title = {A NLP Approach to Quantify Resilience in Cyber-Socio-Technical Systems with LLM Agents},
journal = {Procedia Computer Science},
volume = {253},
pages = {1943-1950},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.256},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925002649},
author = {Antonio {De Nicola} and Maria Guariglia Migliore and Ida Mele and Maria Luisa Villani},
keywords = {Cyber-Socio-Technical System, Large Language Model, Natural Language Processing, Resilience, Safety},
abstract = {Incorporating cyber artifacts into Cyber-Socio-Technical Systems (CSTSs) poses new challenges to human safety due to their increasingly unpredictable behavior. Resilience is the ability to adapt, recover, and bounce back from challenges, setbacks, or adversity. Quantifying the resilience of CSTSs requires the identification of leading or lagging indicators. Among the leading indicators, allostatic load measures the level of systemic tension accumulated from misalignments in the perspectives of system actors regarding how work should be performed. In this paper, we propose a novel approach based on Natural Language Processing (NLP) to measure allostatic load. This approach involves lightweight modelling of process perspectives, extraction of token vectors from process function descriptions, and computing vector similarity by using the Dice similarity algorithm. Then, allostatic load is defined as the complement to one of the similarity value. An example application concerning a chemical spill in a hospital laboratory demonstrates the method’s practical use.}
}
@article{WANG2025103527,
title = {MASC: Large language model-based multi-agent scheduling chain for flexible job shop scheduling problem},
journal = {Advanced Engineering Informatics},
volume = {67},
pages = {103527},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103527},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625004203},
author = {Zelong Wang and Chenhui Wan and Jie Liu and Xi Zhang and Haifeng Wang and Youmin Hu and Zhongxu Hu},
keywords = {Intelligent manufacturing, Flexible job shop scheduling, Large language model, Multi-agent systems},
abstract = {The flexible job shop scheduling problem (FJSP) presents significant challenges in intelligent manufacturing due to the need for efficient resource allocation and dynamic rescheduling. Traditional solutions often require manual intervention and lack real-time adaptability. This paper introduces the Multi-Agent Scheduling Chain (MASC) framework, which leverages large language models (LLMs) for enhanced decision-making and automated equipment control. MASC integrates four agents to manage dynamic scheduling and real-time rescheduling effectively. SchedAgent (Scheduling Agent) uses an improved ReAct method, combining FJSP-specific scheduling indicators and fine-tuned decision algorithms to optimize results. The DialBag (Dialogue Bagging) method is also introduced, constructing a specialized dataset to prevent knowledge loss and enhance decision-making. This method allows the agent to retain knowledge across diverse scheduling contexts while improving performance in specific tasks. MASC was validated through simulations and real-world robotic experiments, handling both machine malfunctions and urgent job additions. These tests demonstrated MASC’s robust performance, with significant improvements in scheduling efficiency and rescheduling accuracy. Quantitative results showed that SchedAgent consistently achieved high ranking percentages, with an average ranking percentage of 84% at a 10-second solving time and 90% at a 30-second solving time. MASC provides a scalable and adaptable solution for intelligent manufacturing, demonstrating the potential of LLMs to automate and optimize production workflows in both static and dynamic environments.}
}
@article{WU2024101030,
title = {Exploring the reversal curse and other deductive logical reasoning in BERT and GPT-based large language models},
journal = {Patterns},
volume = {5},
number = {9},
pages = {101030},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101030},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924001636},
author = {Da Wu and Jingye Yang and Kai Wang},
keywords = {BERT, GPT, large language model, LLM, reversal curse, auto-regressive model, bidirectional encoder, deductive logical reasoning},
abstract = {Summary
The “Reversal Curse” describes the inability of autoregressive decoder large language models (LLMs) to deduce “B is A” from “A is B,” assuming that B and A are distinct and can be uniquely identified from each other. This logical failure suggests limitations in using generative pretrained transformer (GPT) models for tasks like constructing knowledge graphs. Our study revealed that a bidirectional LLM, bidirectional encoder representations from transformers (BERT), does not suffer from this issue. To investigate further, we focused on more complex deductive reasoning by training encoder and decoder LLMs to perform union and intersection operations on sets. While both types of models managed tasks involving two sets, they struggled with operations involving three sets. Our findings underscore the differences between encoder and decoder models in handling logical reasoning. Thus, selecting BERT or GPT should depend on the task’s specific needs, utilizing BERT’s bidirectional context comprehension or GPT’s sequence prediction strengths.}
}
@article{GROEN2025112533,
title = {Classification of quality characteristics in online user feedback using linguistic analysis, crowdsourcing and LLMs},
journal = {Journal of Systems and Software},
volume = {230},
pages = {112533},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2025.112533},
url = {https://www.sciencedirect.com/science/article/pii/S0164121225002018},
author = {Eduard C. Groen and Fabiano Dalpiaz and Martijn {van Vliet} and Boris Winter and Joerg Doerr and Sjaak Brinkkemper},
keywords = {Crowd-based RE, Crowdsourcing, Large language models, Online user reviews, Quality requirements, Requirements engineering, User feedback analysis},
abstract = {Software qualities such as usability or reliability are among the strongest determinants of mobile app user satisfaction and constitute a significant portion of online user feedback on software products, making it a valuable source of quality-related feedback to guide the development process. The abundance of online user feedback warrants the automated identification of quality characteristics, but the online user feedback’s heterogeneity and the lack of appropriate training corpora limit the applicability of supervised machine learning. We therefore investigate the viability of three approaches that could be effective in low-data settings: language patterns (LPs) based on quality-related keywords, instructions for crowdsourced micro-tasks, and large language model (LLM) prompts. We determined the feasibility of each approach and then compared their accuracy. For the complex multiclass classification of quality characteristics, the LP-based approach achieved a varied precision (0.38–0.92) depending on the quality characteristic, and low recall; crowdsourcing achieved the best average accuracy in two consecutive phases (0.63, 0.72), which could be matched by the best-performing LLM condition (0.66) and a prediction based on the LLMs’ majority vote (0.68). Our findings show that in this low-data setting, the two approaches that use crowdsourcing or LLMs instead of involving experts achieved accurate classifications, while the LP-based approach had only limited potential. The promise of crowdsourcing and LLMs in this context might even extend to building training corpora.}
}
@article{XUE2021107343,
title = {Matching large-scale biomedical ontologies with central concept based partitioning algorithm and Adaptive Compact Evolutionary Algorithm},
journal = {Applied Soft Computing},
volume = {106},
pages = {107343},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107343},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621002660},
author = {Xingsi Xue and Jie Zhang},
keywords = {Biomedical ontology matching, Ontology partition, Adaptive Compact Evolutionary Algorithm},
abstract = {As a unified model for describing biomedical knowledge, a biomedical ontology is of help to solve the issues of data heterogeneity in different biomedical databases. However, these ontologies might model same biomedical knowledge differently, yielding the heterogeneity problem. To address the biomedical ontology heterogeneity problem, it is necessary to match the heterogeneous concept pairs between two ontologies. How to reduce the computational complexity is a challenging problem when matching large-scale biomedical ontologies, which directly affects the matching efficiency and the alignment’s quality. To face this challenge, this work proposes a large-scale biomedical ontology partitioning and matching framework. In our proposal, a central concepts based ontology partitioning algorithm is first used to divide the ontology into several disjoint segments, which borrows the idea from the social network and Firefly Algorithm (FA). The proposed algorithm is able to partition the ontologies with low computation complexity, and at the same time, ensure the semantic completeness and the decent scale of each segment. Then, an Adaptive Compact Evolutionary Algorithm (ACEA) based matching technique is utilized to determine the ontology segment alignments, which can efficiently match the similar ontology segments. The experiment utilizes the biomedical testing cases provided by Ontology Alignment Evaluation Initiative (OAEI) to test our approach’s effectiveness, and the experimental results show that the alignments obtained by our method significantly outperforms the state-of-the-art biomedical ontology matching techniques.}
}
@article{VERDONCK201992,
title = {Comparing traditional conceptual modeling with ontology-driven conceptual modeling: An empirical study},
journal = {Information Systems},
volume = {81},
pages = {92-103},
year = {2019},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0306437918303727},
author = {Michaël Verdonck and Frederik Gailly and Robert Pergl and Giancarlo Guizzardi and Beatriz Martins and Oscar Pastor},
keywords = {Conceptual modeling, Empirical study, Ontology-driven conceptual modeling, Experiment, OntoUML, Entity-relationship modeling},
abstract = {This paper conducts an empirical study that explores the differences between adopting a traditional conceptual modeling (TCM) technique and an ontology-driven conceptual modeling (ODCM) technique with the objective to understand and identify in which modeling situations an ODCM technique can prove beneficial compared to a TCM technique. More specifically, we asked ourselves if there exist any meaningful differences in the resulting conceptual model and the effort spent to create such model between novice modelers trained in an ontology-driven conceptual modeling technique and novice modelers trained in a traditional conceptual modeling technique. To answer this question, we discuss previous empirical research efforts and distill these efforts into two hypotheses. Next, these hypotheses are tested in a rigorously developed experiment, where a total of 100 students from two different Universities participated. The findings of our empirical study confirm that there do exist meaningful differences between adopting the two techniques. We observed that novice modelers applying the ODCM technique arrived at higher quality models compared to novice modelers applying the TCM technique. More specifically, the results of the empirical study demonstrated that it is advantageous to apply an ODCM technique over an TCM when having to model the more challenging and advanced facets of a certain domain or scenario. Moreover, we also did not find any significant difference in effort between applying these two techniques. Finally, we specified our results in three findings that aim to clarify the obtained results.}
}
@article{VOIGT2021129836,
title = {Materials graph ontology},
journal = {Materials Letters},
volume = {295},
pages = {129836},
year = {2021},
issn = {0167-577X},
doi = {https://doi.org/10.1016/j.matlet.2021.129836},
url = {https://www.sciencedirect.com/science/article/pii/S0167577X21005322},
author = {Sven P. Voigt and Surya R. Kalidindi},
keywords = {Artificial intelligence},
abstract = {To maximize the use of the materials data being generated by various researchers and organizations, it is necessary to store the data such that it is findable, accessible, interoperable, and reusable (FAIR). Although current materials data repositories and databases partly address the FAIR principles, they do not adequately capture the critical metadata that represents the contextual information (e.g., relationship between materials data and terms typically used by materials scientists such as process, structure, and property). The collection and organization of this metadata along with the original data would allow advanced queries that implicitly improve FAIR characteristics. Recent work has attempted to define this necessary metadata through the development of materials ontologies. This paper introduces a new materials graph and develops the associated materials graph ontology needed to address shortcomings of the current materials ontologies. This novel ontology can be combined with existing ontologies to standardize the inter-relationships between materials data elements and related materials concepts. This paper demonstrates how the proposed materials graph ontology enables the conceptual description of a broad variety of materials data, improves the findability and usability of the different graph-connected material concepts and data, and formalizes a materials data ingest framework that is amenable for the extraction of process-structure–property relationships.}
}
@article{LUTZ2022103709,
title = {A complete classification of the complexity and rewritability of ontology-mediated queries based on the description logic EL},
journal = {Artificial Intelligence},
volume = {308},
pages = {103709},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103709},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000492},
author = {Carsten Lutz and Leif Sabellek},
keywords = {Description logic, Ontology-mediated querying, Complexity classification, Rewritability, Linear datalog},
abstract = {We provide a fine-grained analysis of the data complexity and rewritability of ontology-mediated queries (OMQs) based on an EL ontology and a conjunctive query (CQ). Our main results are that every such OMQ is in Image 1, Image 2-complete, or Image 3-complete and that containment in Image 2 coincides with rewritability into linear Datalog (whereas containment in Image 1 coincides with rewritability into first-order logic). We establish natural characterizations of the three cases in terms of bounded depth and (un)bounded pathwidth of certain minimal ABoxes on which the OMQ yields an answer. We also show that each of the associated meta problems such as deciding whether a given OMQ is rewritable into linear Datalog is ExpTime-complete. We also give a way to construct linear Datalog rewritings when they exist and prove that there is no constant bound on the arity of IDB relations in linear Datalog rewritings.}
}
@article{MURTAZINA2021595,
title = {The constructing of cognitive functions ontology},
journal = {Procedia Computer Science},
volume = {186},
pages = {595-602},
year = {2021},
note = {14th International Symposium "Intelligent Systems},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.04.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921010188},
author = {M.Sh. Murtazina and T.V. Avdeenko},
keywords = {Ontology, OWL, cognitive functions, assessing cognitive functions},
abstract = {In present paper we consider ontology-driven approach applied to neurocognitive science. At the beginning, a brief overview of the works revealing the prospects and possibilities of constructing ontological models in this field of knowledge is given. Based on the conducted analysis we have concluded that the scientific community states the urgency of creating an ontology intended to accumulate interdisciplinary knowledge necessary for assessing cognitive functions being the most complex functions of the brain, through which the process of rational cognition of the world is carried out. So we have analyzed the key features of the knowledge representation about cognitive functions in the field of psychology, cognitive science and neurobiology. As a result we propose conceptual structure, basic classes and relations for the cognitive functions ontology intended to the clearer understanding of the relationships between the brain activity patterns and the human cognitive abilities. The ontology was implemented using OWL in the Protégé 5.2 ontology editor environment. It accumulates knowledge about cognitive functions and methods for assessing them with use of neuropsychological tests and EEG methods. A certain set of axioms also have been implemented and approved.}
}
@article{ESPINOZAARIAS2021100655,
title = {Crossing the chasm between ontology engineering and application development: A survey},
journal = {Journal of Web Semantics},
volume = {70},
pages = {100655},
year = {2021},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100655},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000305},
author = {Paola Espinoza-Arias and Daniel Garijo and Oscar Corcho},
keywords = {Ontology, OWL, Ontology engineering, Web API, Application development, Knowledge graph},
abstract = {The adoption of Knowledge Graphs (KGs) by public and private organizations to integrate and publish data has increased in recent years. Ontologies play a crucial role in providing the structure for KGs, but are usually disregarded when designing Application Programming Interfaces (APIs) to enable browsing KGs in a developer-friendly manner. In this paper we provide a systematic review of the state of the art on existing approaches to ease access to ontology-based KG data by application developers. We propose two comparison frameworks to understand specifications, technologies and tools responsible for providing APIs for KGs. Our results reveal several limitations on existing API-based specifications, technologies and tools for KG consumption, which outline exciting research challenges including automatic API generation, API resource path prediction, ontology-based API versioning, and API validation and testing.}
}
@incollection{BLOUIN20219,
title = {Chapter 2 - An ontological foundation for multi-paradigm modelling for cyber-physical systems},
editor = {Bedir Tekinerdogan and Dominique Blouin and Hans Vangheluwe and Miguel Goulão and Paulo Carreira and Vasco Amaral},
booktitle = {Multi-Paradigm Modelling Approaches for Cyber-Physical Systems},
publisher = {Academic Press},
pages = {9-43},
year = {2021},
isbn = {978-0-12-819105-7},
doi = {https://doi.org/10.1016/B978-0-12-819105-7.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191057000076},
author = {Dominique Blouin and Rima Al-Ali and Mauro Iacono and Bedir Tekinerdogan and Holger Giese},
keywords = {Cyber-physical systems, multi-paradigm modelling, ontology, OWL, feature modelling, Protégé},
abstract = {This chapter introduces the different background material needed to understand the next three Chapters respectively presenting the Cyber-Physical Systems (CPS), Multi-Paradigm Modelling (MPM) and Multi-Paradigm Modelling for Cyber-Physical Systems (MPM4CPS) ontologies. It first introduces the approach and tools that have been used to define the ontologies. It then provides an overview of the ontological framework and a description of the shared ontology, which provides concepts defined in order to frame in a more general context some of the more specific notions of the other ontologies. Then, the chapter presents a brief introduction of the two CPS engineering environments employing MPM that were used to explore the domains covered by the ontologies and to guide readers through our exploration path.}
}
@article{BENABDALLAH2020953,
title = {Personalized cloud service review analysis based on modularized ontology},
journal = {Online Information Review},
volume = {44},
number = {5},
pages = {953-975},
year = {2020},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-06-2019-0207},
url = {https://www.sciencedirect.com/science/article/pii/S1468452720000414},
author = {Emna Ben-Abdallah and Khouloud Boukadi and Mohamed Hammami and Mohamed Hedi Karray},
keywords = {Cloud service, Context, Opinion analysis, Modular ontologies},
abstract = {Purpose
The purpose of this paper is to analyze cloud reviews according to the end-user context and requirements.
Design/methodology/approach
propose a comprehensive knowledge base composed of interconnected Web Ontology Language, namely, modular ontology for cloud service opinion analysis (SOPA). The SOPA knowledge base will be the basis of context-aware cloud service analysis using consumers' reviews. Moreover, the authors provide a framework to evaluate cloud services based on consumers' reviews opinions.
Findings
The findings show that there is a positive impact of personalizing the cloud service analysis by considering the reviewers' contexts in the performance of the framework. The authors also proved that the SOPA-based framework outperforms the available cloud review sites in term of precision, recall and F-measure.
Research limitations/implications
Limited information has been provided in the semantic web literature about the relationships between the different domains and the details on how that can be used to evaluate cloud service through consumer reviews and latent opinions. Furthermore, existing approaches are lacking lightweight and modular mechanisms which can be utilized to effectively exploit information existing in social media.
Practical implications
The SOPA-based framework facilitates the opinion based service evaluation through a large number of consumer's reviews and assists the end-users in analyzing services as per their requirements and their own context.
Originality/value
The SOPA ontology is capable of representing the content of a product/service as well as its related opinions, which are extracted from the customer's reviews written in a specific context. Furthermore, the SOPA-based framework facilitates the opinion based service evaluation through a large number of consumer's reviews and assists the end-users in analyzing services as per their requirements and their own context.}
}
@article{RITCHIE2022,
title = {Automated Clinical Practice Guideline Recommendations for Hereditary Cancer Risk Using Chatbots and Ontologies: System Description},
journal = {JMIR Cancer},
volume = {8},
number = {1},
year = {2022},
issn = {2369-1999},
doi = {https://doi.org/10.2196/29289},
url = {https://www.sciencedirect.com/science/article/pii/S2369199922000076},
author = {Jordon B Ritchie and Lewis J Frey and Jean-Baptiste Lamy and Cecelia Bellcross and Heath Morrison and Joshua D Schiffman and Brandon M Welch},
keywords = {service-oriented architecture, restful API, hereditary cancer, risk assessment, clinical practice guidelines, consumer health informatics},
abstract = {Background
Identifying patients at risk of hereditary cancer based on their family health history is a highly nuanced task. Frequently, patients at risk are not referred for genetic counseling as providers lack the time and training to collect and assess their family health history. Consequently, patients at risk do not receive genetic counseling and testing that they need to determine the preventive steps they should take to mitigate their risk.
Objective
This study aims to automate clinical practice guideline recommendations for hereditary cancer risk based on patient family health history.
Methods
We combined chatbots, web application programming interfaces, clinical practice guidelines, and ontologies into a web service–oriented system that can automate family health history collection and assessment. We used Owlready2 and Protégé to develop a lightweight, patient-centric clinical practice guideline domain ontology using hereditary cancer criteria from the American College of Medical Genetics and Genomics and the National Cancer Comprehensive Network.
Results
The domain ontology has 758 classes, 20 object properties, 23 datatype properties, and 42 individuals and encompasses 44 cancers, 144 genes, and 113 clinical practice guideline criteria. So far, it has been used to assess >5000 family health history cases. We created 192 test cases to ensure concordance with clinical practice guidelines. The average test case completes in 4.5 (SD 1.9) seconds, the longest in 19.6 seconds, and the shortest in 2.9 seconds.
Conclusions
Web service–enabled, chatbot-oriented family health history collection and ontology-driven clinical practice guideline criteria risk assessment is a simple and effective method for automating hereditary cancer risk screening.}
}
@article{GREITZER2019361,
title = {Design and Implementation of a Comprehensive Insider Threat Ontology},
journal = {Procedia Computer Science},
volume = {153},
pages = {361-369},
year = {2019},
note = {17th Annual Conference on Systems Engineering Research (CSER)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.090},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307495},
author = {Frank L. Greitzer and James D. Lee and Justin Purl and Abbas K. Zaidi},
keywords = {Type your keywords here, separated by semicolons},
abstract = {We describe the development and envisioned use case applications of a comprehensive insider threat ontology—“Sociotechnical and Organizational Factors for Insider Threat” (SOFIT)—that comprises more than 300 indicators of technical, behavioral, and organizational factors. Requirements, design, and engineering development for the Web Ontology Language (OWL) implementation of the SOFIT knowledge base are reviewed; additional relationships among constructs are defined that extend the representation beyond a simple taxonomic hierarchy and that enable inferences for qualitative and quantitative threat assessment. We show how queries may be constructed to support these inferences and threat assessments. Several major application concepts are reviewed to show how the ontology may be used by the insider threat research and operational communities. To this end, the SOFIT knowledge base may be shared with stakeholders to advance research and practice for proactive insider threat mitigation.}
}
@article{ZHOU2020360,
title = {Knowledge management practice of medical cloud logistics industry: transportation resource semantic discovery based on ontology modelling},
journal = {Journal of Intellectual Capital},
volume = {22},
number = {2},
pages = {360-383},
year = {2020},
issn = {1469-1930},
doi = {https://doi.org/10.1108/JIC-03-2020-0072},
url = {https://www.sciencedirect.com/science/article/pii/S1469193020000346},
author = {Fuli Zhou and Yandong He and Panpan Ma and Raj V. Mahto},
keywords = {Cloud medical logistics, Ontology modelling, Transportation resource semantic discovery, Rule reasoning, Resource-task matching},
abstract = {Purpose
The booming of the Internet of things (IoT) and artificial intelligence (AI) techniques contributes to knowledge adoption and management innovation for the healthcare industry. It is of great significance to transport the medical resources to required places in an efficient way. However, it is difficult to exactly discover matched transportation resources and deliver to its destination due to the heterogeneity. This paper studies the medical transportation resource discovery mechanism, leading to efficiency improvement and operational innovation.
Design/methodology/approach
To solve the transportation resource semantic discovery problem under the novel cloud environment, the ontology modelling approach is used for both transportation resources and tasks information modes. Besides, medical transportation resource discovery mechanism is proposed, and resource matching rules are designed including three stages: filtering reasoning, QoS-based matching and user preferences-based rank to satisfy personalized demands of users. Furthermore, description logic rules are built to express the developed matching rules.
Findings
An organizational transportation case is taken as an example to describe the medical transportation logistics resource semantic discovery process under cloud medical service scenario. Results derived from the proposed semantic discovery mechanism could assist operators to find the most suitable resources.
Research limitations/implications
The case study validates the effectiveness of the developed transportation resource semantic discovery mechanism, contributing to knowledge management innovation for the medical logistics industry.
Originality/value
To improve task-resource matching accuracy under cloud scenario, this study develops a transportation resource semantic discovery procedure from the viewpoint of knowledge management. The novel knowledge management practice contributes to operational management of the cloud medical logistics service by introducing ontology modelling and creative management.}
}
@article{ATTIGERI2018369,
title = {Knowledge Base Ontology Building For Fraud Detection Using Topic Modeling},
journal = {Procedia Computer Science},
volume = {135},
pages = {369-376},
year = {2018},
note = {The 3rd International Conference on Computer Science and Computational Intelligence (ICCSCI 2018) : Empowering Smart Technology in Digital Era for a Better Life},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.186},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918314753},
author = {Girija Attigeri and Manohara Pai {M M} and Radhika M Pai and Rahul Kulkarni},
keywords = {TF-IDF, Topic modeling, Fraud detection, Ontology},
abstract = {Moving towards the digitization and cashless economy tests the existing IT infrastructure for security and fraud controls substantially. Transition from traditional to cashless economy requires to banks to have more secure system to fight fraud. To understand and transform the needs for more secure banking system it is necessary to understand the domain of fraud and create knowledge base for fraud. It helps bridge the gap between business level and IT levels of banking. So that anti-fraud regulations could be automatically imbibed in the system. Hence the paper focuses on analyzing existing fraud case documentations and understand the significant terms involved in the fraud. For this TF-IDF weighting, topic modeling with LDA is used for identifying the group of words (topic) representing particular type of fraud. Using these knowledge base ontology is extracted which can be used for building fraud detection system. Experiment is performed on extracted fraud documents and ontology is built using the latent topics identified.}
}
@article{JOSE2024124603,
title = {Advancing multimodal diagnostics: Integrating industrial textual data and domain knowledge with large language models},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124603},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124603},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424014702},
author = {Sagar Jose and Khanh T.P Nguyen and Kamal Medjaher and Ryad Zemouri and Mélanie Lévesque and Antoine Tahan},
keywords = {Fault detection and diagnostics, Large language model, Expert knowledge, Technical language processing, Multimodal data},
abstract = {The rapid advancement and application of large language models (LLMs) in various domains prompt an investigation into their potential in the field of prognostics and health management (PHM), particularly for enhancing data-driven model capabilities. This study explores the integration of domain knowledge accumulated in unstructured text data such as technical documents and maintenance logs into diagnostics models using LLMs. The study demonstrates the new possibilities to exploit data that are traditionally underutilized due to their complexity and the presence of domain-specific jargon. By leveraging LLMs for contextual understanding and information extraction from such texts, this study proposes a novel approach that combines textual data with existing condition monitoring systems to improve the accuracy of diagnostics models. A case study on hydrogenerators illustrates the feasibility and value of integrating LLMs into PHM systems. The findings suggest that the incorporation of LLMs can lead to more informed, accurate diagnostics, ultimately enhancing operational efficiency and safety within industrial environments.}
}
@article{WEI2025103268,
title = {User needs insights from UGC based on large language model},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103268},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103268},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625001612},
author = {Wei Wei and Chenliang Hao and Zixin Wang},
keywords = {user needs, User-generated content(UGC), Large language model (LLM), IPA-Kano},
abstract = {With limited resources, it is critical for companies to understand and address user needs to gain a competitive edge.The methods that utilize large-scale user-generated content (UGC) produced by the internet can analyze user needs efficiently and accurately. However, these methods have not been extensively studied.This paper proposes a framework based on large language model (LLM) to extract user’s insights into the priority of product attributes. First, product attributes are extracted from user reviews using LLM. Then, the mapping network between user reviews and satisfaction is established through sentiment analysis based on the LLM and Multi-layer Perceptron (MLP) classification. Finally, a comprehensive analysis of product importance is conducted using a proposed quantified IPA-Kano model. An empirical study on smart wearable bands is conducted to offer an intuitive and quantifiable analysis of user attention and satisfaction for each product attribute. The strengths and weaknesses of the products are highlighted, providing valuable insights that can inspire companies to adopt user-centric optimization strategies.}
}
@article{BABAIHA2024100095,
title = {Rationalism in the face of GPT hypes: Benchmarking the output of large language models against human expert-curated biomedical knowledge graphs},
journal = {Artificial Intelligence in the Life Sciences},
volume = {5},
pages = {100095},
year = {2024},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2024.100095},
url = {https://www.sciencedirect.com/science/article/pii/S2667318524000023},
author = {Negin Sadat Babaiha and Sathvik Guru Rao and Jürgen Klein and Bruce Schultz and Marc Jacobs and Martin Hofmann-Apitius},
keywords = {Large language models (LLMs), Natural language processing (NLP), Biomedical text mining, Biomedical knowledge graphs, Biological expression language (BEL)},
abstract = {Biomedical knowledge graphs (KGs) hold valuable information regarding biomedical entities such as genes, diseases, biological processes, and drugs. KGs have been successfully employed in challenging biomedical areas such as the identification of pathophysiology mechanisms or drug repurposing. The creation of high-quality KGs typically requires labor-intensive multi-database integration or substantial human expert curation, both of which take time and contribute to the workload of data processing and annotation. Therefore, the use of automatic systems for KG building and maintenance is a prerequisite for the wide uptake and utilization of KGs. Technologies supporting the automated generation and updating of KGs typically make use of Natural Language Processing (NLP), which is optimized for extracting implicit triples described in relevant biomedical text sources. At the core of this challenge is how to improve the accuracy and coverage of the information extraction module by utilizing different models and tools. The emergence of pre-trained large language models (LLMs), such as ChatGPT which has grown in popularity dramatically, has revolutionized the field of NLP, making them a potential candidate to be used in text-based graph creation as well. So far, no previous work has investigated the power of LLMs on the generation of cause-and-effect networks and KGs encoded in Biological Expression Language (BEL). In this paper, we present initial studies towards one-shot BEL relation extraction using two different versions of the Generative Pre-trained Transformer (GPT) models and evaluate its performance by comparing the extracted results to a highly accurate, manually curated BEL KG curated by domain experts.}
}
@article{SATHIYAPRASAD202355,
title = {Ontology-based video retrieval using modified classification technique by learning in smart surveillance applications},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {4},
pages = {55-64},
year = {2023},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2023.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666307423000062},
author = {B. Sathiyaprasad},
keywords = {Video retrieval, Pre-processing, Feature extraction, Ontology, Classification, Key frame segmentation},
abstract = {With the rapid advancements in communication and multimedia computing technology, multimedia information, particularly video data, has recently become disproportionately accessible. Video is widely used in a variety of applications, making efficient management and retrieval of the expanding volume of video data critical. To manage such tasks, include automated recognition of the image queries in video retrieval with a reduced degree of system memory usage, spot detection, maintenance time, identification of exact duplicate videos, and so on. This research determination demonstrates the Modified R-Ratio with Viola-Jones Classification Method (MRVJCM) with the relevance of developing techniques and algorithms for automatic recognition of image queries. The R-Ratio Viola–Jones framework has two distinguishable feature maintenance processes, such as Feature Selection and Integration and Feature Cascading. These two distinct features are applied to the motion features (texture, emotions, elements, and shape) within the three features. The proposed techniques and their relevant algorithms are used to retrieve the most accurate videos and to assure the mathematical operation of video retrieval in the operation of the comparable protected system. As a result, the proposed MRVJCM achieves 98% of accuracy, 93% of precision, 92% of recall, and 42% of RMSE.}
}
@article{HUANG2025104216,
title = {A survey on biomedical automatic text summarization with large language models},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104216},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104216},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001578},
author = {Zhenyu Huang and Xianlai Chen and Yunbo Wang and Jincai Huang and Xing Zhao},
keywords = {Biomedical, Automatic text summarization, Large language models, Neural networks, Natural language processing},
abstract = {Automatic text summarization in the biomedical field can support efficient literature screening, medical knowledge management, and innovative medical research. In recent years, Large Language Models (LLMs), as a disruptive technology in natural language processing, have shown great potential for Biomedical Automatic Text Summarization (BATS). This technology helps to better understand the terminology of biomedical texts, track medical hotspots, and generate personalized diagnoses and treatment plans. This paper provides an in-depth discussion on the development of BATS, and the opportunities as well as challenges brought by applying LLMs to biomedical automatic text summarization. Firstly, the development of BATS is reviewed, where traditional text summarization, neural network-based summarization, and LLMs-based summarization are analyzed systematically. Meanwhile, the applications of various LLMs (e.g., BERT and GPT series) in three types of BATS are presented in detail, including extractive summarization, abstractive summarization, and hybrid summarization. Next, the relevant datasets are introduced, such as PubMed, COVID-19 and MIMIC-Ⅲ. Then, traditional, emerging, and auxiliary metrics for evaluating the performance of BATS are shown, and the performance evaluation of different models is elaborated. Finally, the opportunities brought by applying LLMs to BATS are described, and the potential challenges along with the corresponding solutions are discussed.}
}
@article{HUANG2021222,
title = {Core-Concept-Seeded LDA for Ontology Learning},
journal = {Procedia Computer Science},
volume = {192},
pages = {222-231},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015106},
author = {Hao Huang and Mounira Harzallah and Fabrice Guillet and Ziwei Xu},
keywords = {Ontology Learning, Core Ontology, LDA, Term Clustering, Seed Knowledge, Prior Knowledge, Semantic Coherence, Word2vec},
abstract = {Ontologies are powerful semantic models applied for various purposes such as improving system interoperability, information retrieval, question answering, etc. However, building domain ontologies remains a challenging task for humans, especially when the concepts and properties are large or evolving, and also when they are built from large-scale textual data. Machine learning allows to automate the building of ontologies from texts. In particular, clustering techniques have a promising ability on the concept formation task by identifying the cluster of semantically closed terms as a concept. However, current works encounter issues in learning relevant domain-specific clusters or in identifying the relevant concept labels for each cluster. To solve these issues, we propose both to use core concepts from a domain ontology as prior knowledge, and to adapt term clustering with seed knowledge-based LDA models in order to take these core concepts into account. First, each topic is associated with a set of seed terms of a single core concept, then the learning is guided by these seeds to gather in the same topic the terms that refer to its core concept. We evaluate our proposal on two textual corpora and compare it to the baselines (LDA, K-means, and SMBM). The results show that our approach performs significantly better than other methods on the class-balanced dataset and works well on the class-imbalanced dataset with a proper number of topics for each core concept.}
}
@article{DEVITO2025114184,
title = {HELIOT: LLM-Based CDSS for adverse drug reaction management},
journal = {Knowledge-Based Systems},
volume = {328},
pages = {114184},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114184},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125012250},
author = {Gabriele {De Vito} and Filomena Ferrucci and Athanasios Angelakis},
keywords = {Clinical decision support systems, Large language models, Drug administration, Adverse drug reaction management},
abstract = {Medication errors significantly threaten patient safety, leading to adverse drug events and substantial economic burdens on healthcare systems. Clinical Decision Support Systems (CDSSs) aimed at mitigating these errors often face limitations when processing unstructured clinical data, including reliance on static databases and rule-based algorithms, frequently generating excessive alerts that lead to alert fatigue among healthcare providers. This paper introduces HELIOT, an innovative CDSS for adverse drug reaction management that processes free-text clinical information using Large Language Models (LLMs) integrated with a comprehensive pharmaceutical data repository. HELIOT leverages advanced natural language processing capabilities to interpret medical narratives, extract relevant drug reaction information from unstructured clinical notes, and learn from past patient-specific medication tolerances to reduce false alerts, enabling more nuanced and contextual adverse drug event warnings across primary care, specialist consultations, and hospital settings. Evaluation using three state-of-the-art LLMs on synthetic and real-world datasets demonstrates classification accuracy ranging from 98.77 % to 99.80 % with zero false negatives for life-threatening reactions. This high accuracy enabled HELIOT to achieve a 50–53 % reduction in interruptive alerts compared to traditional CDSSs while maintaining perfect safety profiles. To support clinical deployment, the system incorporates a confidence-based risk stratification framework that enables automated decisions for high-certainty cases while ensuring appropriate clinical oversight for uncertain classifications. Clinical usability evaluation with healthcare professionals validated these achievements, revealing strong acceptance and unanimous preference for HELIOT’s contextual approach over traditional systems. These findings show promise; however, broader clinical trials remain essential to confirm effectiveness across diverse healthcare environments.}
}
@article{TIWARI2020475,
title = {Semantic assessment of smart healthcare ontology},
journal = {International Journal of Web Information Systems},
volume = {16},
number = {4},
pages = {475-491},
year = {2020},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-05-2020-0027},
url = {https://www.sciencedirect.com/science/article/pii/S1744008420000105},
author = {Sanju Tiwari and Ajith Abraham},
keywords = {Health care, IoT, Ontology assessment, Knowledge modeling, Linked data, Test cases},
abstract = {Purpose
Health-care ontologies and their terminologies play a vital role in knowledge representation and data integration for health information. In health-care systems, Internet of Technology (IoT) technologies provide data exchange among various entities and ontologies offer a formal description to present the knowledge of health-care domains. These ontologies are advised to assure the quality of their adoption and applicability in the real world.
Design/methodology/approach
Ontology assessment is an integral part of ontology construction and maintenance. It is always performed to identify inconsistencies and modeling errors by the experts during the ontology development. A smart health-care ontology (SHCO) has been designed to deal with health-care information and IoT devices. In this paper, an integrated approach has been proposed to assess the SHCO on different assessment tools such as Themis, Test-Driven Development (TDD)onto, Protégé and OOPs! Several test cases are framed to assess the ontology on these tools, in this research, Themis and TDDonto tools provide the verification for the test cases while Protégé and OOPs! provides validation of modeled knowledge in the ontology.
Findings
As of the best knowledge, no other study has been presented earlier to conduct the integrated assessment on different tools. All test cases are successfully analyzed on these tools and results are drawn and compared with other ontologies.
Originality/value
The developed ontology is analyzed on different verification and validation tools to assure the quality of ontologies.}
}
@article{ALI2018138,
title = {Type-2 fuzzy ontology–aided recommendation systems for IoT–based healthcare},
journal = {Computer Communications},
volume = {119},
pages = {138-155},
year = {2018},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0140366417310587},
author = {Farman Ali and S.M. Riazul Islam and Daehan Kwak and Pervez Khan and Niamat Ullah and Sang-jo Yoo and K.S. Kwak},
keywords = {Semantic knowledge, Remotely monitoring, Type-2 fuzzy ontology, Iot-based healthcare, Recommendation system},
abstract = {The number of people with a chronic disease is rapidly increasing, giving the healthcare industry more challenging problems. To date, there exist several ontology and IoT-based healthcare systems to intelligently supervise the chronic patients for long-term care. The central purposes of these systems are to reduce the volume of manual work in recommendation systems. However, due to the increase of risk and uncertain factors of the diabetes patients, these healthcare systems cannot be utilized to extract precise physiological information about patient. Further, the existing ontology-based approaches cannot extract optimal membership value of risk factors; thus, it provides poor results. In this regards, this paper presents a type-2 fuzzy ontology–aided recommendation systems for IoT-based healthcare to efficiently monitor the patient's body while recommending diets with specific foods and drugs. The proposed system extracts the values of patient risk factors, determines the patient's health condition via wearable sensors, and then recommends diabetes-specific prescriptions for a smart medicine box and food for a smart refrigerator. The combination of type-2 Fuzzy Logic (T2FL) and the fuzzy ontology significantly increases the prediction accuracy of a patient's condition and the precision rate for drug and food recommendations. Information about the patient's disease history, foods consumed, and drugs prescribed is designed in the ontology to deliver decision-making knowledge using Protégé Web Ontology Language (OWL)-2 tools. Semantic Web Rule Language (SWRL) rules and fuzzy logic are employed to automate the recommendation process. Moreover, Description Logic (DL) and Simple Protocol and RDF Query Language (SPARQL) queries are used to evaluate the ontology. The experimental results show that the proposed system is efficient for patient risk factors extraction and diabetes prescriptions.}
}
@article{ALDANAMARTIN2025102290,
title = {eidos: A modular approach to external function integration in LLMs},
journal = {SoftwareX},
volume = {31},
pages = {102290},
year = {2025},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2025.102290},
url = {https://www.sciencedirect.com/science/article/pii/S2352711025002560},
author = {José F. Aldana-Martín and Antonio Benítez-Hidalgo and José F. Aldana-Montes},
keywords = {Large Language Models, Function calling, Validation, Retrieval-Augmented Generation},
abstract = {Function calling allows Large Language Models (LLMs) to execute a wide range of tasks, from data analysis and mathematical computations to interacting with web services and other software systems. By harnessing the power of external tooling, LLMs can provide more dynamic, context-aware responses. However, errors in the model’s understanding of the request can lead to misinterpretations of the intended actions, resulting in function calls that are either irrelevant or incorrect for the task at hand. Without proper validation and control mechanisms, the parameters expected by the function may not align with those provided by the model, leading to incorrect operations or failures in task execution. In this paper, we present eidos, a software tool designed to streamline the integration of functions within LLMs. eidos acts as an intermediary, enabling both the seamless execution and validation of functions by LLMs. By leveraging its modular architecture, function definitions can be injected into the LLM context and invoked as if they were native functions via an API.}
}
@article{WEIGAND2021101878,
title = {An artifact ontology for design science research},
journal = {Data & Knowledge Engineering},
volume = {133},
pages = {101878},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101878},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000057},
author = {Hans Weigand and Paul Johannesson and Birger Andersson},
keywords = {Design science, IT artifact, Ontology, UFO},
abstract = {From a design science perspective, information systems and their components are viewed as artifacts. However, not much has been written yet on the ontological status of artifacts or their structure. After March & Smith’s (1995) initial classification of artifacts in terms of models, constructs, methods and instantiations, there have been only a few attempts to come up with a more systematic approach. This conceptual paper provides an ontological analysis of the notion of artifact grounded in the foundational ontology UFO. Its core is an ontological characterization of artifacts, and technical objects in general from a Design Science Research perspective, developed in conversation with other approaches. This general artifact ontology is applied in a systematic classification of IS artifacts. We include practical implications for Design Science Research.}
}
@article{REN2025103301,
title = {Large language model for patent concept generation},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103301},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103301},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625001946},
author = {Runtao Ren and Jian Ma and Jianxi Luo},
keywords = {Generative AI, Large language model, Finetuning, Patent},
abstract = {In traditional innovation practices, concept and IP generation are often iteratively integrated. Both processes demand an intricate understanding of advanced technical domain knowledge. Existing large language models (LLMs), while possessing massive pre-trained knowledge, often fall short in the innovative concept generation due to a lack of specialized knowledge necessary for the generation. To bridge this critical gap, we propose a novel knowledge finetuning (KFT) framework to endow LLM-based AI with the ability to autonomously mine, understand, and apply domain-specific knowledge and concepts for invention generation, i.e., concept and patent generation together. Our proposed PatentGPT integrates knowledge injection pre-training (KPT), domain-specific supervised finetuning (SFT), and reinforcement learning from human feedback (RLHF). Extensive evaluation shows that PatentGPT significantly outperforms the state-of-the-art models on patent-related benchmark tests. Our method not only provides new insights into data-driven innovation but also paves a new path to fine-tune LLMs for applications in the context of technology. We also discuss the managerial and policy implications of AI-generating inventions in the future.}
}
@article{TAO20181040,
title = {Multi-layer cloud architectural model and ontology-based security service framework for IoT-based smart homes},
journal = {Future Generation Computer Systems},
volume = {78},
pages = {1040-1051},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16305775},
author = {Ming Tao and Jinglong Zuo and Zhusong Liu and Aniello Castiglione and Francesco Palmieri},
keywords = {Smart home, Heterogeneity, IoT, Cloud, Ontology, Security},
abstract = {The Smart Home concept, associated with the pervasiveness of network coverage and embedded computing technologies is assuming an ever-growing significance for people living in the highly developed areas. However, the heterogeneity of devices, services, communication protocols, standards and data formats involved in most of the available solutions developed by different vendors, is adversely affecting its widespread application. In this paper, promoted by several promising opportunities provided by the advances in Internet of Things (IoT) and Cloud Computing technologies for facing these challenges, a novel multi-layer cloud architectural model is developed to enable effective and seamless interactions/interoperations on heterogeneous devices/services provided by different vendors in IoT-based smart home. In addition, to better solve the heterogeneity issues in the presented layered cloud platform, ontology has been used as a promising way to address data representation, knowledge, and application heterogeneity, and an ontology-based security service framework is designed for supporting security and privacy preservation in the process of interactions/interoperations. Challenges and directions for future work on smart home management have been also discussed at the end of this paper.}
}
@article{GHARIB2021101888,
title = {COPri v.2 — A core ontology for privacy requirements},
journal = {Data & Knowledge Engineering},
volume = {133},
pages = {101888},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101888},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2100015X},
author = {Mohamad Gharib and Paolo Giorgini and John Mylopoulos},
keywords = {Privacy ontology, Privacy requirements, Privacy by Design, PbD, Requirements engineering, Conceptual modeling},
abstract = {Nowadays, most enterprises collect, store, and manage personal information of customers to deliver their services. In such a setting, privacy has emerged as a key concern since companies often neglect or even misuse personal data. In response to multiple massive breaches of personal data, governments around the world have enacted laws and regulations for privacy protection. These laws dictate privacy requirements for any system that acquires and manages personal data. Unfortunately, these requirements are often incomplete and/or inaccurate as many RE practitioners are insufficiently versed with privacy requirements and how are they different from other requirements, such as security. To tackle this problem, we developed a comprehensive ontology for privacy requirements. In particular, the contributions of this work include the derivation of an ontology from a previously conducted systematic literature review, an implementation using an ontology definition tool (Protégé), a demonstration of its coverage through an extensive example on Ambient Assisted Living, and a validation through competency questions. Also, we evaluate the ontology against the common pitfalls for ontologies with the help of some software tools, lexical semantics experts, and privacy and security researchers. The ontology presented herein (COPri v.2) has been enhanced with extensions motivated by the feedback received from privacy and security experts.}
}
@article{XU2025107580,
title = {Learning protein language contrastive models with multi-knowledge representation},
journal = {Future Generation Computer Systems},
volume = {164},
pages = {107580},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.107580},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24005442},
author = {Wenjun Xu and Yingchun Xia and Bifan Sun and Zihao Zhao and Lianggui Tang and Xiaobo Zhou and Qingyong Wang and Lichuan Gu},
keywords = {Protein representation learning, Contrastive learning, Multi-knowledge embeddings, Convex approximation},
abstract = {Protein representation learning plays a crucial role in obtaining a comprehensive understanding of biological regulatory mechanisms and in developing proteins and drugs for therapeutic purposes. However, labeled proteins, such as sequenced and functionally annotated data, are incomplete and few. Thus, contrastive learning has emerged as the preferred technique for learning meaningful representations from unlabeled data samples. In addition, at present, natural proteins cannot be fully described by extracting protein knowledge from a single domain. Therefore, Pro-CoRL, a protein contrastive models framework based on multi-knowledge representation learning, was proposed in this study. In particular, Pro-CoRL smooths the objective function using convex approximation, thereby improving the stability of training. Extensive experiments on predicting protein–protein interaction types and clustering protein families have confirmed the high accuracy and robustness of Pro-CoRL.}
}
@article{ZHENG2023771,
title = {A quality evaluation method for the unstructured defect record of relay protection devices based on ontology and knowledge graph},
journal = {Energy Reports},
volume = {9},
pages = {771-781},
year = {2023},
note = {Selected papers from 2022 International Conference on Frontiers of Energy and Environment Engineering},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.04.157},
url = {https://www.sciencedirect.com/science/article/pii/S235248472300519X},
author = {Shaoming Zheng and Zhongshuo Liu and Yiting Yu and Peng Dong and Xinping Yang and Shuhong Wang and Chang Tao and Ancheng Xue},
keywords = {Relay protection device, Defect record, Knowledge graph, AHP-entropy weight method, Text quality evaluation},
abstract = {The original unstructured record data for the defect of the relay protection devices (RPDs) may contain problems influencing the data mining, and it is lack of quantitative evaluation. So the purpose of this paper is to evaluate the quality of these defective texts of relay protection devices, and it proposes a text quality evaluation method for the unstructured defect record of RPDs based on the ontology and knowledge graph for the defects text of relay protection devices, combined with the analytic hierarchy process (AHP) and entropy weighting method. In details, first, the problems existing in the unstructured defect records of RPDs are presented. Secondly, the corresponding evaluation indicators are proposed, and the quantitative calculation method for each indicator is given in combination with the knowledge graph for the defects of relay protection devices. Thirdly, combined with the subjective AHP and the objective entropy weight method, a comprehensive evaluation method for the unstructured defect record for RPDs is proposed, and its effectiveness is verified with examples. Finally, with the 10-year actual defects record data, the data quality of the unstructured defect record for the RPDs in a regional power grid is comprehensively evaluated, and characteristic of the data quality with respected to text length, defect level, and dispatch region are revealed.}
}
@article{SALEM20225552,
title = {LODQuMa: A Free-ontology process for Linked (Open) Data quality management},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part A},
pages = {5552-5563},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821001348},
author = {Samah Salem and Fouzia Benchikha},
keywords = {Linked Open Data, Quality assessment, Quality improvement, Synonym predicates, Profiling statistics, DBpedia},
abstract = {For many years, data quality is among the most commonly discussed issue in Linked Open Data (LOD) due to the huge volume of integrated datasets that are usually heterogeneous. Several ontology-based approaches dealing with quality problems have been proposed. However, when datasets lack a well-defined schema, these approaches become ineffective because of the lack of metadata. Moreover, the detection of quality problems based on an analysis between RDF (Resource Description Framework) triples without requiring ontology statistical and semantical information is not addressed. Keeping in mind that ontologies are not always available and they may be incomplete or misused. In this paper, a novel free-ontology process called LODQuMa is proposed to assess and improve the quality of LOD. It is mainly based on profiling statistics, synonym relationships between predicates, QVCs (Quality Verification Cases), and SPARQL (SPARQL Protocol and RDF Query Language) query templates. Experiments on the DBpedia dataset demonstrate that the proposed process is effective for increasing the intrinsic quality dimensions, resulting in correct and compact datasets.}
}
@article{ZHOU2025106151,
title = {Named entity recognition for construction documents based on fine-tuning of large language models with low-quality datasets},
journal = {Automation in Construction},
volume = {174},
pages = {106151},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106151},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525001918},
author = {Junyu Zhou and Zhiliang Ma},
keywords = {Construction documents, Large language model, Named entity recognition, Low-quality datasets},
abstract = {Named Entity Recognition (NER) is a fundamental task for automatically processing and reusing documents. In traditional methods, machine learning has been used relying on costly high-quality datasets. This paper proposed an NER method based on fine-tuning Large Language Models (LLMs) with low-quality datasets for construction documents. Firstly, low-quality datasets were semi-automatically generated from national standards, qualification textbooks, and lexicons, including datasets of generation-type, tagging-type and question-answering type. Then, they were used to fine-tune an LLM for NER of structural elements to obtain optimal parametric fine-tuning conditions. Next, the results of optimally fine-tuned LLM were used to iterate the low-quality dataset to improve the performance. The F1 finally reached 0.756. Similar results were obtained on two other types of named entities, illustrating the generalizability. This paper provided a more effective and efficient method for the construction documents reuse. Future research should explore how to achieve better results by using other methods.}
}
@article{OLIVA201937,
title = {A computational system based on ontologies to automate the mapping process of medical reports into structured databases},
journal = {Expert Systems with Applications},
volume = {115},
pages = {37-56},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418305049},
author = {Jefferson Tales Oliva and Huei Diana Lee and Newton Spolaôr and Weber Shoity Resende Takaki and Claudio Saddy Rodrigues Coy and João José Fagundes and Feng Chung Wu},
keywords = {Text mining, Natural language processing, Data mining, Software engineering, Ontologies, Medical reports},
abstract = {We have developed, in collaboration with medical and computer experts, the ontology-based Medical Report Mapping Process to support the transformation of unstructured reports into a structured representation. Nevertheless, the techniques employed in this two-phase process must be performed individually and manually by computer instructions, which hinder their use by users not familiar with such language. Thereby, this work proposes a tool to automate and optimize this process by integrating its techniques in a computational system, which was built using a software engineering prototyping approach. This system was experimentally evaluated by applying it to a set of 100 textual reports. The first phase decreased the total number of phrases (853) and words (2520) by 82.25% (48) and 92.70% (184), respectively. In the second phase, 100% of the relevant pieces of information (previously established) present in the reports were transcribed. Also, the second phase was applied, using the same configuration as the first study, in another set with 250 textual reports, resulting in a mapping rate of 99.74%. The unprocessed and unmapped words, regarding both experimental evaluations, were recorded for later inclusion into the ontology. By using this system, efficient and scalable investigations can be performed from medical reports, contributing to generate new knowledge. Also, the system facilitates the definition of these structures due to the feasibility to analyze different sentences in unique phrase sets.}
}
@incollection{DESOUSA20221333,
title = {Ontology for Enhanced Industrial Process Control},
editor = {Ludovic Montastruc and Stephane Negny},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {51},
pages = {1333-1338},
year = {2022},
booktitle = {32nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-95879-0.50223-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032395879050223X},
author = {Renata Samara Rodrigues {de Sousa} and Song Won Park},
keywords = {Cyber-Physical Integration, Industry 4.0, Ontology, Process Control},
abstract = {The cyber-physical integration in the sense of industry 4.0, applied to industrial process control, needs to develop new methodologies. Both top-down hierarchies of commands for Enterprise Resource Planning, Optimization, Advanced Control, Local Control, and bottom-up flux of information from the plant floor to control system, optimization level, and planning strategy are dissolved, aiming to enhance the vertical and horizontal integration and flexible operability. Even though this is a requirement for Cyber-Physical Systems, it would be merely a coexistence of advanced optimization IoT technologies states of art. Therefore, there is a need for a new architecture of a functional system. Since it works essentially as an event-based system, ontology plays a key concept in the practical working of the new process control. First, an overview of the general ontologies framework applied in Industry 4.0, and on the other hand, some ontologies published for chemical process design are discussed extensively here. Then the practical ontology and taxonomy needed for industrial process control are discussed. Here is presented how to recover the functional layers to each specific application of former traditional hierarchy top-down and bottom-up, now named as semantic layers. A description’s language development for process control using this ontology is another challenging task, also proposed. A case based on the transition of the traditional batch reactor process to modern industry 4.0 application illustrates the change of the operation mode. Finally, the potential gains and technology limitations are analyzed as a critique as the enabling technologies (or pillars) of Industry 4.0 to show what concepts apply to this enhanced process control. It also compares and analyzes this modern approach in the manufacturing process and the difference with the industrial process control. The construction of a complete system based on ontology and description language ready for application is an impressive task to be developed further. The main objective of the work, for now, is to clarify the concepts and show the methodology with its practical application, discussing limitations.}
}
@article{LEE2021,
title = {A Determinants-of-Fertility Ontology for Detecting Future Signals of Fertility Issues From Social Media Data: Development of an Ontology},
journal = {Journal of Medical Internet Research},
volume = {23},
number = {6},
year = {2021},
issn = {1438-8871},
doi = {https://doi.org/10.2196/25028},
url = {https://www.sciencedirect.com/science/article/pii/S1438887121006208},
author = {Ji-Hyun Lee and Hyeoun-Ae Park and Tae-Min Song},
keywords = {ontology, fertility, public policy, South Korea, social media, future, infodemiology, infoveillance},
abstract = {Background
South Korea has the lowest fertility rate in the world despite considerable governmental efforts to boost it. Increasing the fertility rate and achieving the desired outcomes of any implemented policies requires reliable data on the ongoing trends in fertility and preparations for the future based on these trends.
Objective
The aims of this study were to (1) develop a determinants-of-fertility ontology with terminology for collecting and analyzing social media data; (2) determine the description logics, content coverage, and structural and representational layers of the ontology; and (3) use the ontology to detect future signals of fertility issues.
Methods
An ontology was developed using the Ontology Development 101 methodology. The domain and scope of the ontology were defined by compiling a list of competency questions. The terms were collected from Korean government reports, Korea’s Basic Plan for Low Fertility and Aging Society, a national survey about marriage and childbirth, and social media postings on fertility issues. The classes and their hierarchy were defined using a top-down approach based on an ecological model. The internal structure of classes was defined using the entity-attribute-value model. The description logics of the ontology were evaluated using Protégé (version 5.5.0), and the content coverage was evaluated by comparing concepts extracted from social media posts with the list of ontology classes. The structural and representational layers of the ontology were evaluated by experts. Social media data were collected from 183 online channels between January 1, 2011, and June 30, 2015. To detect future signals of fertility issues, 2 classes of the ontology, the socioeconomic and cultural environment, and public policy, were identified as keywords. A keyword issue map was constructed, and the defined keywords were mapped to identify future signals. R software (version 3.5.2) was used to mine for future signals.
Results
A determinants-of-fertility ontology comprised 236 classes and terminology comprised 1464 synonyms of the 236 classes. Concept classes in the ontology were found to be coherently and consistently defined. The ontology included more than 90% of the concepts that appeared in social media posts on fertility policies. Average scores for all of the criteria for structural and representations layers exceeded 4 on a 5-point scale. Violence and abuse (socioeconomic and cultural factor) and flexible working arrangement (fertility policy) were weak signals, suggesting that they could increase rapidly in the future.
Conclusions
The determinants-of-fertility ontology developed in this study can be used as a framework for collecting and analyzing social media data on fertility issues and detecting future signals of fertility issues. The future signals identified in this study will be useful for policy makers who are developing policy responses to low fertility.}
}
@article{BHUSHAN2018605,
title = {Analyzing inconsistencies in software product lines using an ontological rule-based approach},
journal = {Journal of Systems and Software},
volume = {137},
pages = {605-617},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217301097},
author = {Megha Bhushan and Shivani Goel and Karamjit Kaur},
keywords = {Feature model, Software product line, Rule-based approach, Ontology, Inconsistency, Defects},
abstract = {Software product line engineering (SPLE) is an evolving technical paradigm for generating software products. Feature model (FM) represents commonality and variability of a group of software products that appears within a specific domain. The quality of FMs is one of the factors that impacts the correctness of software product line (SPL). Developing FMs might also incorporate inaccurate relationships among features which cause numerous defects in models. Inconsistency is one of such defect that decreases the benefits of SPL. Existing approaches have focused in identifying inconsistencies in FMs however, only a few of these approaches are able to provide their causes. In this paper FM is formalized from an ontological view by converting model into a predicate-based ontology and defining a set of first-order logic based rules for identifying FM inconsistencies along with their causes in natural language in order to assist developers with solutions to fix defects. A FM available in software product lines online tools repository has been used to explain the presented approach and validated using 24 FMs of varied sizes up to 22,035 features. Evaluation results demonstrate that our approach is effective and accurate for the FMs scalable up to thousands of features and thus, improves SPL.}
}
@article{ZHONG2025,
title = {Performance of ChatGPT-4o and Four Open-Source Large Language Models in Generating Diagnoses Based on China’s Rare Disease Catalog: Comparative Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/69929},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125008374},
author = {Wei Zhong and YiFan Liu and Yan Liu and Kai Yang and HuiMin Gao and HuiHui Yan and WenJing Hao and YouSheng Yan and ChengHong Yin},
keywords = {large language models, ChatGPT, rare diseases, Llama, open-source LLMs, retrieval augmented generation, chain-of-thought, Deepseek},
abstract = {Background
Diagnosing rare diseases remains challenging due to their inherent complexity and limited physician knowledge. Large language models (LLMs) offer new potential to enhance diagnostic workflows.
Objective
This study aimed to evaluate the diagnostic accuracy of ChatGPT-4o and 4 open-source LLMs (qwen2.5:7b, Llama3.1:8b, qwen2.5:72b, and Llama3.1:70b) for rare diseases, assesses the language effect on diagnostic performance, and explore retrieval augmented generation (RAG) and chain-of-thought (CoT) reasoning.
Methods
We extracted clinical manifestations of 121 rare diseases from China’s inaugural rare disease catalog. ChatGPT-4o generated a primary and 5 differential diagnoses, while 4 LLMs were assessed in both English and Chinese contexts. The lowest-performing model underwent RAG and CoT re-evaluation. Diagnostic accuracy was compared via the McNemar test. A survey evaluated 11 clinicians’ familiarity with rare diseases.
Results
ChatGPT-4o demonstrated the highest diagnostic accuracy with 90.1%. Language effects varied across models: qwen2.5:7b showed comparable performance in Chinese (51.2%) and English (47.9%; χ²1=0.32, P=.57), whereas Llama3.1:8b exhibited significantly higher English accuracy (67.8% vs 31.4%; χ²1=40.20, P<.001). Among larger models, qwen2.5:72b maintained cross-lingual consistency considering the odds ratio (OR; Chinese: 82.6% vs English: 83.5%; OR 0.88, 95% CI 0.27-2.76,P=1.000), contrasting with Llama3.1:70b’s language-dependent variation (Chinese: 80.2% vs English: 90.1%; OR 0.29,95% CI 0.08-0.83, P=.02). Cross-model comparisons revealed Llama3.1:8b underperformed qwen2.5:7b in Chinese (χ²1=13.22,P<.001) but surpassed it in English (χ²1=13.92,P<.001). No significant differences were observed between qwen2.5:72b and Llama3.1:70b (English: OR 0.33, P=.08; Chinese: OR 1.5, 95% CI 0.48-5.12,P=.07); qwen2.5:72b matched ChatGPT-4o’s performance in both languages (English: OR 0.33, P=.08; Chinese: OR 0.44, P=.09); Llama3.1:70b mirrored ChatGPT-4o’s English accuracy (OR 1, P=1.000) but lagged in Chinese (OR 0.33; P=.02). RAG implementation enhanced qwen2.5:7b’s accuracy to 79.3% (χ²1=31.11, P<.001) with 85.9% retrieval precision. The distilled model Deepseek-R1:7b markedly underperformed (9.9% vs qwen2.5:7b; χ²1=42.19, P<.001). Clinician surveys revealed significant knowledge gaps in rare disease management.
Conclusions
ChatGPT-4o demonstrated superior diagnostic performance for rare diseases. While Llama3.1:8b demonstrates viability for localized deployment in resource-constrained English diagnostic workflows, Chinese applications require larger models to achieve comparable diagnostic accuracy. This urgency is heightened by the release of open-source models like DeepSeek-R1, which may see rapid adoption without thorough validation. Successful clinical implementation of LLMs requires 3 core elements: model parameterization, user language, and pretraining data. The integration of RAG significantly enhanced open-source LLM accuracy for rare disease diagnosis, although caution remains warranted for low-parameter reasoning models showing substantial performance limitations. We recommend hospital IT departments and policymakers prioritize language relevance in model selection and consider integrating RAG with curated knowledge bases to enhance diagnostic utility in constrained settings, while exercising caution with low-parameter models.}
}
@article{HURTADO2025108567,
title = {Leveraging Transformers-based models and linked data for deep phenotyping in radiology},
journal = {Computer Methods and Programs in Biomedicine},
volume = {260},
pages = {108567},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108567},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724005601},
author = {Lluís-F. Hurtado and Luis Marco-Ruiz and Encarna Segarra and Maria Jose Castro-Bleda and Aurelia Bustos-Moreno and Maria de la Iglesia-Vayá and Juan Francisco Vallalta-Rueda},
keywords = {Natural language processing, Deep learning, Transformers, Linked data, Deep phenotyping, Radiology, Electronic Health records},
abstract = {Background and Objective:
Despite significant investments in the normalization and the standardization of Electronic Health Records (EHRs), free text is still the rule rather than the exception in clinical notes. The use of free text has implications in data reuse methods used for supporting clinical research since the query mechanisms used in cohort definition and patient matching are mainly based on structured data and clinical terminologies. This study aims to develop a method for the secondary use of clinical text by: (a) using Natural Language Processing (NLP) for tagging clinical notes with biomedical terminology; and (b) designing an ontology that maps and classifies all the identified tags to various terminologies and allows for running phenotyping queries.
Methods and Results:
Transformers-based NLP Models, concretely pre-trained RoBERTa language models, were used to process radiology reports and annotate them identifying elements matching UMLS Concept Unique Identifiers (CUIs) definitions. CUIs were mapped into several biomedical ontologies useful for phenotyping (e.g., SNOMED-CT, HPO, ICD-10, FMA, LOINC, and ICPC2, among others) and represented as a lightweight ontology using OWL (Web Ontology Language) constructs. This process resulted in a Linked Knowledge Base (LKB), which allows running expressive queries to retrieve reports that comply with specific criteria using automatic reasoning.
Conclusion:
Although phenotyping tools mostly rely on relational databases, the combination of NLP and Linked Data technologies allows us to build scalable knowledge bases using standard ontologies from the Web of data. Our approach enables us to execute a pipeline which input is free text and automatically maps identified entities to a LKB that allows answering phenotyping queries. In this work, we have only used Spanish radiology reports, although it is extensible to other languages for which suitable corpora are available. This is particularly valuable in regional and national systems dealing with large research databases from different registries and cohorts and plays an essential role in the scalability of large data reuse infrastructures that require indexing and governing distributed data sources.}
}
@article{HAMED2025112492,
title = {From knowledge generation to knowledge verification: examining the biomedical generative capabilities of ChatGPT},
journal = {iScience},
volume = {28},
number = {6},
pages = {112492},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.112492},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225007539},
author = {Ahmed Abdeen Hamed and Alessandro Crimi and Magdalena M. Misiak and Byung Suk Lee},
keywords = {Health sciences, Medicine, Health informatics, Health technology},
abstract = {Summary
The generative capabilities of LLM models offer opportunities for accelerating tasks but raise concerns about the authenticity of the knowledge they produce. We present a computational approach that evaluates the factual accuracy of biomedical knowledge generated by an LLM. Our approach consists of generating disease-centric associations and verifying them using biomedical ontologies. Using ChatGPT, we designed prompt-engineering processes to establish linkages between diseases and related drugs, symptoms, and genes, and assessed consistency across multiple ChatGPT models (e.g., GPT-4, GPT-4o, and GPT-4o-mini). Results demonstrate high accuracy in identifying disease terms (88%–97%), drug names (90%–91%), and genetic information (88%–98%). Symptom term identification was lower (49%–61%) due to informal symptom descriptions. Verification reveals coverage of 89%–91% for disease-drug and disease-gene pairs; symptom-related associations show lower coverage (49%–62%). Despite high term accuracy, generated IDs were often invalid or redundant. GenAI tools can be reliable if used with care. Retrieval Augmented Generation (RAG) may enhance reliability.}
}
@article{HANNAH2025100843,
title = {On the legal implications of Large Language Model answers: A prompt engineering approach and a view beyond by exploiting Knowledge Graphs},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100843},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100843},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000295},
author = {George Hannah and Rita T. Sousa and Ioannis Dasoulas and Claudia d’Amato},
keywords = {Knowledge Graph, Large Language Models, Prompt engineering, Legislative texts},
abstract = {With the recent surge in popularity of Large Language Models (LLMs), there is the rising risk of users blindly trusting the information in the response. Nevertheless, there are cases where the LLM recommends actions that have potential legal implications and this may put the user in danger. We provide an empirical analysis on multiple existing LLMs showing the urgency of the problem. Hence, we propose a first short-term solution, consisting in an approach for isolating these legal issues through prompt engineering. We prove that this solution is able to stem some risks related to legal implications, nonetheless we also highlight some limitations. Hence, we argue on the need for additional knowledge-intensive resources and specifically Knowledge Graphs for fully solving these limitations. For the purpose, we draw our proposal aiming at designing and developing a solution powered by a legal Knowledge Graph (KG) that, besides capturing and alerting the user on possible legal implications coming from the LLM answers, is also able to provide actual evidence for them by supplying citations of the interested laws. We conclude with a brief discussion on the issues that may be needed to solve for building a comprehensive legal Knowledge Graph}
}
@article{WU202587,
title = {Knowledge-Empowered, Collaborative, and Co-Evolving AI Models: The Post-LLM Roadmap},
journal = {Engineering},
volume = {44},
pages = {87-100},
year = {2025},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924007239},
author = {Fei Wu and Tao Shen and Thomas Bäck and Jingyuan Chen and Gang Huang and Yaochu Jin and Kun Kuang and Mengze Li and Cewu Lu and Jiaxu Miao and Yongwei Wang and Ying Wei and Fan Wu and Junchi Yan and Hongxia Yang and Yi Yang and Shengyu Zhang and Zhou Zhao and Yueting Zhuang and Yunhe Pan},
keywords = {Artificial intelligence, Large language models, Knowledge empowerment, Model collaboration, Model co-evolution},
abstract = {Large language models (LLMs) have significantly advanced artificial intelligence (AI) by excelling in tasks such as understanding, generation, and reasoning across multiple modalities. Despite these achievements, LLMs have inherent limitations including outdated information, hallucinations, inefficiency, lack of interpretability, and challenges in domain-specific accuracy. To address these issues, this survey explores three promising directions in the post-LLM era: knowledge empowerment, model collaboration, and model co-evolution. First, we examine methods of integrating external knowledge into LLMs to enhance factual accuracy, reasoning capabilities, and interpretability, including incorporating knowledge into training objectives, instruction tuning, retrieval-augmented inference, and knowledge prompting. Second, we discuss model collaboration strategies that leverage the complementary strengths of LLMs and smaller models to improve efficiency and domain-specific performance through techniques such as model merging, functional model collaboration, and knowledge injection. Third, we delve into model co-evolution, in which multiple models collaboratively evolve by sharing knowledge, parameters, and learning strategies to adapt to dynamic environments and tasks, thereby enhancing their adaptability and continual learning. We illustrate how the integration of these techniques advances AI capabilities in science, engineering, and society—particularly in hypothesis development, problem formulation, problem-solving, and interpretability across various domains. We conclude by outlining future pathways for further advancement and applications.}
}
@article{HUGHES2019288,
title = {Extracting safety information from multi-lingual accident reports using an ontology-based approach},
journal = {Safety Science},
volume = {118},
pages = {288-297},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518307586},
author = {Peter Hughes and Ryan Robinson and Miguel Figueres-Esteban and Coen {van Gulijk}},
abstract = {This paper describes an approach to extract meaning from multi-lingual free-text safety incident reports. A sample of 5065 safety incident reports from the Swiss Federal Office of Transport were used in the study. Each report was written in either German, French or Italian natural language. An interactive learning approach between a human and computer software was undertaken to identify key terms in the text that are relevant to discovering meaning. A multi-lingual ontology was created to join meaningful semantic patterns and identify specific classes of safety incident on the railway, including injuries occurring whilst passengers were boarding or alighting from vehicles, falling down stairs, struck by closing doors, or struck by objects such as suitcases. A graph database was used to query the text records via the ontology and identify reports of incidents in each class, regardless of the language used in the report. Fluent speakers of each language – German, French and Italian – reviewed the results to confirm true positive results and detect false positives. The performance of the process varied across languages and incident types, however the overall true positive rate was determined by the fluent speakers to be 98.5%.}
}
@article{LI2025104789,
title = {Improving entity recognition using ensembles of deep learning and fine-tuned large language models: A case study on adverse event extraction from VAERS and social media},
journal = {Journal of Biomedical Informatics},
volume = {163},
pages = {104789},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104789},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000188},
author = {Yiming Li and Deepthi Viswaroopan and William He and Jianfu Li and Xu Zuo and Hua Xu and Cui Tao},
keywords = {Named-entity recognition, VAERS, Generative pre-trained transformer (GPT), Large language model (LLM), Social media, Deep learning},
abstract = {Objective
Adverse event (AE) extraction following COVID-19 vaccines from text data is crucial for monitoring and analyzing the safety profiles of immunizations, identifying potential risks and ensuring the safe use of these products. Traditional deep learning models are adept at learning intricate feature representations and dependencies in sequential data, but often require extensive labeled data. In contrast, large language models (LLMs) excel in understanding contextual information, but exhibit unstable performance on named entity recognition (NER) tasks, possibly due to their broad but unspecific training. This study aims to evaluate the effectiveness of LLMs and traditional deep learning models in AE extraction, and to assess the impact of ensembling these models on performance.
Methods
In this study, we utilized reports and posts from the Vaccine Adverse Event Reporting System (VAERS) (n = 230), Twitter (n = 3,383), and Reddit (n = 49) as our corpora. Our goal was to extract three types of entities: vaccine, shot, and adverse event (ae). We explored and fine-tuned (except GPT-4) multiple LLMs, including GPT-2, GPT-3.5, GPT-4, Llama-2 7b, and Llama-2 13b, as well as traditional deep learning models like Recurrent neural network (RNN) and Bidirectional Encoder Representations from Transformers for Biomedical Text Mining (BioBERT). To enhance performance, we created ensembles of the three models with the best performance. For evaluation, we used strict and relaxed F1 scores to evaluate the performance for each entity type, and micro-average F1 was used to assess the overall performance.
Results
The ensemble demonstrated the best performance in identifying the entities “vaccine,” “shot,” and “ae,” achieving strict F1-scores of 0.878, 0.930, and 0.925, respectively, and a micro-average score of 0.903. These results underscore the significance of fine-tuning models for specific tasks and demonstrate the effectiveness of ensemble methods in enhancing performance.
Conclusion
In conclusion, this study demonstrates the effectiveness and robustness of ensembling fine-tuned traditional deep learning models and LLMs, for extracting AE-related information following COVID-19 vaccination. This study contributes to the advancement of natural language processing in the biomedical domain, providing valuable insights into improving AE extraction from text data for pharmacovigilance and public health surveillance.}
}
@article{GILANI2020107099,
title = {A review of ontologies within the domain of smart and ongoing commissioning},
journal = {Building and Environment},
volume = {182},
pages = {107099},
year = {2020},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2020.107099},
url = {https://www.sciencedirect.com/science/article/pii/S0360132320304741},
author = {Sara Gilani and Caroline Quinn and J.J. McArthur},
keywords = {Building management system, Building information model, Data structure, Key performance indicator, Performance improvement, Fault detection and diagnosis},
abstract = {Smart and ongoing commissioning (SOCx) of buildings can result in a significant reduction in the gap between design and operational performance. Ontologies play an important role in SOCx as they facilitate data readability and reasoning by machines. A better understanding of ontologies is required in order to develop and incorporate them in SOCx. This paper critically reviews the state-of-the-art research on building data ontologies since 2014 within the SOCx domain through sorting them based on building data types, general approaches, and applications. The data types of two main domains of building information modeling and building management system have been considered in the majority of existing ontologies. Three main applications are evident from a critical analysis of existing ontologies: (1) key performance indicator calculation, (2) building performance improvement, and (3) fault detection and diagnosis. The key gaps found in the literature review are a holistic ontology for SOCx and insight on how such approaches should be evaluated. Based on these findings, this study provides recommendations for future necessary research including: identification of SOCx-related data types, assessment of ontology performance, and creation of open-source approaches.}
}
@article{GARCIA2020104387,
title = {The GeoCore ontology: A core ontology for general use in Geology},
journal = {Computers & Geosciences},
volume = {135},
pages = {104387},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2019.104387},
url = {https://www.sciencedirect.com/science/article/pii/S0098300419306284},
author = {Luan Fonseca Garcia and Mara Abel and Michel Perrin and Renata {dos Santos Alvarenga}},
keywords = {GeoCore ontology, Core ontology, Geological knowledge, Ontology engineering, BFO top-level ontology, Knowledge modeling},
abstract = {Domain ontologies assume the role of representing, in a formal way, a consensual knowledge of a community over a domain. This task is especially difficult in a wide domain like Geology, which is composed of diversified science resting on a large variety of conceptual models that were developed over time. The meaning of the concepts used by the various professionals often depends on the particular vision that they have of a domain according to their background and working habits. Ontology development in Geology thus necessitates a drastic elucidation of the concepts and vocabulary used by geologists. This article intends to contribute to solving these difficulties by proposing a core ontology named GeoCore Ontology resting on the BFO top ontology, specially designed for describing scientific fields. GeoCore Ontology contains well-founded definitions of a limited set of general concepts within the Geology field that are currently considered by all geologists whatever their skill. It allows modelers to separately consider a geological object, the substance that constitutes it, the boundaries that limit it and the internal arrangement of the matter inside it. The core ontology also allows the description of the existentially dependent qualities attached to a geological object and the geological process that generated it in a particular geological age. This small set of formally defined and described concepts combined with concepts from BFO provides a backbone for deriving by subsumption more specialized geological concepts and also constitutes a baseline for integrating different existent domain ontologies within the Geology domain. The GeoCore ontology and the methodology that we used for building it, provide solutions for unveiling major misunderstanding regarding the concepts that are commonly used for formulating geological interpretations. This will facilitate the communication of this information to external Geology users and its integration in domain applications.}
}
@article{PANAGOULIAS2025113975,
title = {A framework for evaluation and requirement extraction for fine-tuning of Large Language Models in multimodal medical diagnosis},
journal = {Knowledge-Based Systems},
volume = {326},
pages = {113975},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113975},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125010202},
author = {Dimitrios P. Panagoulias and Anastasios Palamidas and Maria Virvou and George A. Tsihrintzis},
keywords = {Artificial intelligence-empowered software engineering, Multimodal large language models, Generative Pre-trained Transformer, Model evaluation and fine-tuning, Explainable AI, Clinical decision support, Telehealth diagnostics},
abstract = {Objective:
Large language models constitute a breakthrough state-of-the-art Artificial Intelligence technology which is rapidly evolving and promises to aid in medical diagnosis. In this study, we propose a novel evaluation framework for extraction of fine-tuning requirements based on the Objective Structured Clinical Examinations (OSCE) that can increase LLM potential and applicability.
Methods:
We developed an OSCE based evaluation meta-framework leveraging IoT-based data retrieval with a two-step approach designed to analyze and guide improvement of LLMs in multimodal medical diagnosis: (1) structured interaction evaluation and (2) domain-specific analysis of extracted data. Using Image-Metadata Analysis (IMA), Named Entity Recognition (NER), and Knowledge Graphs (KG), this framework identifies image domains, extracts relevant entities, and assesses connections in KGs. These methods collectively reveal areas for improvement, guiding fine-tuning to enhance diagnostic accuracy and contextual understanding in medical applications.
Results:
Using this paradigm, (1) we evaluate the correctness and accuracy of generated medical diagnosis with publicly available multimodal-multiple-choice-questions in the vast domain of General Pathology and (2) proceed to the domain-specific analysis. We identify and visualize the model performance across specific organs, diseases, and pathological themes, detecting areas of lower accuracy, such as in cardiovascular conditions like atherosclerosis. This targeted approach enables precision-focused fine-tuning, applying additional data to specific weaknesses rather than a broad, generalized tuning across all pathology.
Contributions:
Our framework’s primary contribution is its OSCE-inspired ability to dynamically identify and target under-performing areas within a broad domain, enhancing fine-tuning efficiency and diagnostic accuracy in a resource-effective and iterative manner removing the dependency on bulk adjustments, making it particularly suitable for sensitive applications where precision and resource efficiency are essential, such as in medical diagnostics.}
}
@article{YADLAPALLI2020123265,
title = {Corporate social responsibility definitions in supply chain research: An ontological analysis},
journal = {Journal of Cleaner Production},
volume = {277},
pages = {123265},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.123265},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620333102},
author = {Aswini Yadlapalli and Shams Rahman and Angappa Gunasekaran},
keywords = {Corporate social responsibility, Definition, Ontological analysis, Supply chain, Systematic literature review},
abstract = {The purpose of this article is to propose an ontological framework of corporate social responsibility (CSR) and to investigate CSR definitions published in supply chain research in reference to the proposed ontological framework. The dimensions of the CSR ontological framework are clustered into five broad categories: approach, benefits, stakeholders, temporality, and action. Through the systematic literature review, a total of 96 definitions from the supply chain research are identified. The chosen definitions are analysed word-to-word and presented as an ontological map of monads and dyads. Further, a mapping software tool Ucinet is used to visualise the network structure of the dimensions and categories of the CSR definitions. The analysis at the monad level demonstrates that secondary and primary stakeholder and social benefits are the most frequently referred dimensions. The dyadic-level analysis highlights that social–economic benefits and primary–secondary stakeholder dimensions co-occurred heavily resulting in bright spots. Centrality measures of the network analysis indicate that there are major differences in how researchers used dimensions/categories while defining CSR in the context of supply chain research. This article is the first attempt to develop a logically constructed natural-language description of CSR in an ontological framework. The proposed framework will help practitioners to develop coherent and congruent CSR knowledge and conceptualise CSR that is suitable to their context and effectively communicate with the employees and external stakeholders.}
}
@article{VARAGNOLO2021158,
title = {A Tool to Explore the Population of a CIDOC-CRM Ontology},
journal = {Procedia Computer Science},
volume = {192},
pages = {158-167},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921015040},
author = {Davide Varagnolo and Dora Melo and Irene Pimenta Rodrigues},
keywords = {Archives, CIDOC-CRM, Ontology Visualization, Knowledge Representation, Semantic Web},
abstract = {This paper presents a visualising tool to explore the population of an Ontology, obtained through the processes of automatic migration and text information extraction. It was developed in the context of EPISA project, a R&D project that aims to represent the Portuguese National Archives records information in CIDOC-CRM, an ontology developed for museums. The tool allows the migration process developers to visualise the instances and their properties, and to debug the migration process and the migration representation model, or to explore the Archives by final users. It uses modeling and reasoners OWL-API with SPARQL-DL queries to obtain the exploration results.}
}
@article{XIE2025103733,
title = {Rapid generation method of process routes based on multi-agent collaboration with LLMs},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103733},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103733},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625006263},
author = {Yanling Xie and Jihong Liu and Ruiwen Wang and Zuoxu Wang and Kai Yu and Ziming Song},
keywords = {Manufacturing process design, Multi-agent, Generative AI, Knowledge graph, Smart manufacturing},
abstract = {In the process of process design for manufacturing, issues such as high reliance on personal experience and knowledge, along with long design cycles, are common. This paper proposes a rapid method for generating machining process routes based on Large Language Models (LLMs) and multi-agent collaboration. The complex task of generating process routes is broken down into four subtasks: machining feature recognition, machining feature sorting, machining feature process chain and resource selection, and process route merging and optimization. Each subtask is assigned to an agent fine-tuned with LLMs, equipped with different specialized tools such as STP file parsing and process knowledge base querying, to endow each agent with distinct expertise. The agents collaborate by exchanging information to achieve the rapid, automated generation of machining process routes, offering heuristic ideas for process designers. The TOPSIS evaluation method integrating quantitative and qualitative indicators based on actual production data and expert scores is used to compare the final generated processing route with typical ones, showing that it achieves a higher closeness degree. This demonstrates the advantages of multi-agent collaboration in complex tasks, providing a new solution for the automation and intelligence of process design in intelligent manufacturing systems.}
}
@article{ZHUANG2020100544,
title = {SOBA: Semi-automated Ontology Builder for Aspect-based sentiment analysis},
journal = {Journal of Web Semantics},
volume = {60},
pages = {100544},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2019.100544},
url = {https://www.sciencedirect.com/science/article/pii/S1570826819300770},
author = {Lisa Zhuang and Kim Schouten and Flavius Frasincar},
keywords = {Domain ontology, Aspect-based sentiment analysis, Ontology learning, Reviews, Semi-automatization},
abstract = {This research explores the possibility of improving knowledge-driven aspect-based sentiment analysis (ABSA) in terms of efficiency and effectiveness. This is done by implementing a Semi-automated Ontology Builder for Aspect-based sentiment analysis (SOBA). Semi-automatization of the ontology building process could produce more extensive ontologies, whilst shortening the building time. Furthermore, SOBA aims to improve the effectiveness of its ontologies in ABSA by attaching to concepts the semantics provided by a semantic lexicon. To evaluate the performance of SOBA, ontologies are created using the ontology builder for the restaurant and laptop domains. The use of these ontologies is then compared with the use of manually constructed ontologies in a state-of-the-art knowledge-driven ABSA model, the Two-Stage Hybrid Model (TSHM). The results show that it is difficult for a machine to beat the quality of a human made ontology, as SOBA does not improve the effectiveness of TSHM, achieving similar results. Including the semantics provided by a semantic lexicon in general increases the performance of TSHM, albeit not significantly. However, SOBA decreases by 50% or more the human time needed to build ontologies, so that it is recommended to use SOBA for knowledge-driven ABSA frameworks, as it leads to greater efficiency.}
}
@article{FERNANDEZIZQUIERDO2021104026,
title = {Conformance testing of ontologies through ontology requirements},
journal = {Engineering Applications of Artificial Intelligence},
volume = {97},
pages = {104026},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.104026},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620303079},
author = {Alba Fernández-Izquierdo and Raúl García-Castro},
keywords = {Ontology conformance, Ontology engineering, Ontology testing, Standard},
abstract = {In recent years, several standard ontologies have been developed to maximise semantic interoperability in different domains; such standard ontologies ensure quality and integrity when describing a domain. Therefore, mechanisms to guarantee that developers build ontologies that conform to such standards are needed. However, while in fields such as Software Engineering or industry, conformance testing plays an essential role during product development, in the Ontology Engineering field there is a lack of techniques for this type of testing. This work introduces an ontology conformance testing method to analyse conformance between an ontology and a standard based on the standard requirements. Grounded on this method, the work also presents a minimum common knowledge identification method for analysing how a group of standards covers a particular domain and for identifying whether there are conflicts between them. This work has been validated by analysing the conformance between an ontology network and a set of standards on the Internet of Things domain, and by analysing the minimum common knowledge between such standards. This analysis shows that the conformance between ontologies and standards is mostly related to definition of classes. Furthermore, the analysis shows that although the analysed standards are related to the same domain, they are created to describe different areas of concern and, thus, there is a minimum overlap between them. Finally, it was concluded that the quality of the conformance analysis depends on the quality of the requirements specification: the more precise the requirements, the more precise the analysis between ontologies and standards.}
}
@article{CAO2019630,
title = {An Ontology-based Approach for Failure Classification in Predictive Maintenance Using Fuzzy C-means and SWRL Rules},
journal = {Procedia Computer Science},
volume = {159},
pages = {630-639},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314024},
author = {Qiushi Cao and Ahmed Samet and Cecilia Zanni-Merk and François de Bertrand {de Beuvron} and Christoph Reich},
keywords = {Industry 4.0, Predictive maintenance, Ontologies, SWRL rules, Fuzzy clustering},
abstract = {Within manufacturing processes, anomalies such as machinery faults and failures may lead to the outage situation of production lines. The outage of production lines is detrimental for the availability of production systems and may cause severe economic loss. To avoid the economic loss that may be caused by the outage situation, the prediction of anomalies on production lines is a crucial concern for manufacturers. Recently, data mining techniques have been applied to the manufacturing domain for predicting occurrence time of anomalies, such as the moment of machinery failure. However, existing predictive maintenance approaches have been limited to the prediction of the time of occurrence of machinery failures, while lacking the capability for identifying the criticality of the failures. This may lead to inappropriate maintenance plans and strategies. In this context, in this paper, we introduce a novel ontology-based approach to facilitate predictive maintenance in industry. The proposed approach is a combination use of fuzzy clustering and semantic technologies, where fuzzy clustering techniques are used to learn the criticality of failures based on machine historical data, and semantic technologies use the results of fuzzy clustering to predict the time of failures and the criticality of them. As results, a domain ontology for modeling predictive maintenance knowledge is developed, and a set of Semantic Web Rule Language (SWRL) predictive rules are proposed to reason about the time and criticality of machinery failures. A case study on a real-world industrial data set is followed to evaluate the usefulness and effectiveness of the proposed approach.}
}
@article{YEHIA2019103276,
title = {Ontology-based clinical information extraction from physician’s free-text notes},
journal = {Journal of Biomedical Informatics},
volume = {98},
pages = {103276},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103276},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419301959},
author = {Engy Yehia and Hussein Boshnak and Sayed AbdelGaber and Amany Abdo and Doaa S. Elzanfaly},
keywords = {Information extraction, Electronic health records, Natural language processing},
abstract = {Documenting clinical notes in electronic health records might affect physician’s workflow. In this paper, an Ontology-based clinical information extraction system, OB-CIE, has been developed. OB-CIE system provides a method for extracting clinical concepts from physician’s free-text notes and converts the unstructured clinical notes to structured information to be accessed in electronic health records. OB-CIE system can help physicians to document visit notes without changing their workflow. For recognizing named entities of clinical concepts, ontology concepts have been used to construct a dictionary of semantic categories, then, exact dictionary matching method has been used to match noun phrases to their semantic categories. A rule-based approach has been used to classify clinical sentences to their predefined categories. The system evaluation results have achieved an F-measure of 94.90% and 97.80% for concepts classification and sentences classification, respectively. The results have showed that OB-CIE system performed well on extracting clinical concepts compared with data mining techniques. The system can be used in another field by adapting its ontology and extraction rule set.}
}
@article{VEGETTI2022100254,
title = {Ontology network to support the integration of planning and scheduling activities in batch process industries},
journal = {Journal of Industrial Information Integration},
volume = {25},
pages = {100254},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100254},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000534},
author = {Marcela Vegetti and Gabriela Henning},
keywords = {Scheduling, Ontologies, Integration, Formal specifications},
abstract = {In the last decades, the integration of information systems supporting planning, scheduling, and control has been a serious concern of the industrial community. Several standards have been developed to tackle this issue by addressing the exchange of data between the scheduling function and its immediate lower and upper levels in the planning pyramid. However, a more comprehensive approach is required to solve these integration problems, since this matter entails much more than data exchange. Along these lines, this article presents a network of ontologies that provides the foundations to reach an effective semantic interoperability among the various applications linked to scheduling activities. The proposed approach reuses and formalizes non-ontological resources, like the ISA-88 and ISA-95 standards, as well as the Resource Task Network (RTN) model. In addition, the application of the ontology network to a case study is also discussed in this article.}
}
@article{SANT2021194745,
title = {Sequence Ontology terminology for gene regulation},
journal = {Biochimica et Biophysica Acta (BBA) - Gene Regulatory Mechanisms},
volume = {1864},
number = {10},
pages = {194745},
year = {2021},
issn = {1874-9399},
doi = {https://doi.org/10.1016/j.bbagrm.2021.194745},
url = {https://www.sciencedirect.com/science/article/pii/S1874939921000638},
author = {David W. Sant and Michael Sinclair and Christopher J. Mungall and Stefan Schulz and Daniel Zerbino and Ruth C. Lovering and Colin Logie and Karen Eilbeck},
keywords = {Gene regulation, Ontology},
abstract = {The Sequence Ontology (SO) is a structured, controlled vocabulary that provides terms and definitions for genomic annotation. The Gene Regulation Ensemble Effort for the Knowledge Commons (GREEKC) initiative has gathered input from many groups of researchers, including the SO, the Gene Ontology (GO), and gene regulation experts, with the goal of curating information about how gene expression is regulated at the molecular level. Here we discuss recent updates to the SO reflecting current knowledge. We have developed more accurate human-readable terms (also known as classes), including new definitions, and relationships related to the expression of genes. New findings continue to give us insight into the biology of gene regulation, including the order of events, and participants in those events. These updates to the SO support logical reasoning with the current understanding of gene expression regulation at the molecular level.}
}
@article{NGUYEN2025106165,
title = {LLMs for legal reasoning: A unified framework and future perspectives},
journal = {Computer Law & Security Review},
volume = {58},
pages = {106165},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106165},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000380},
author = {Ha Thanh Nguyen and Wachara Fungwacharakorn and May Myo Zin and Randy Goebel and Francesca Toni and Kostas Stathis and Ken Satoh},
keywords = {Legal analytics, Legal reasoning, Unified reasoning framework, Abductive reasoning},
abstract = {Large Language Models (LLMs) have recently demonstrated remarkable ease of application to numerous natural language processing tasks, however the question of how well they perform is in serious question. In the case of their use in application domains where precision and accuracy are paramount (e.g., law, medicine), the assessment of their performance is erratic. In particular, the application of these models to legal reasoning presents both unique challenges and substantial opportunities because of the inherently complex and multi-faceted nature of legal decision-making. To begin to harness the potential of LLMs in legal reasoning, we propose a framework for unified legal reasoning that combines rule-based, abductive, and case-based approaches, and then investigate possible methods for their integration with LLMs. The ultimate goal, which we take steps toward, is to provide comprehensive, accurate, and adaptable legal decision analysis. We critically examine this combination of reasoning methods, their formalizations, and their relevance to the legal domain, including the consideration of calibration methods to assess their performance. Moreover, we discuss current research and challenges in applying LLMs to legal reasoning tasks, highlight the importance of reconciling different reasoning paradigms, analyze cultural notions of justice, and address issues of uncertainty, vagueness, and ambiguity. Our study offers insights into the benefits and complexities of integrating LLMs within a proposed unified reasoning framework, with the hope of addressing some of the diverse legal challenges, and to advance the capabilities of AI-driven legal analysis.}
}
@article{CHEN2018,
title = {Representation of Time-Relevant Common Data Elements in the Cancer Data Standards Repository: Statistical Evaluation of an Ontological Approach},
journal = {JMIR Medical Informatics},
volume = {6},
number = {1},
year = {2018},
issn = {2291-9694},
doi = {https://doi.org/10.2196/medinform.8175},
url = {https://www.sciencedirect.com/science/article/pii/S2291969418000091},
author = {Henry W Chen and Jingcheng Du and Hsing-Yi Song and Xiangyu Liu and Guoqian Jiang and Cui Tao},
keywords = {common data elements, database management systems, database, time, biomedical ontology},
abstract = {Background
Today, there is an increasing need to centralize and standardize electronic health data within clinical research as the volume of data continues to balloon. Domain-specific common data elements (CDEs) are emerging as a standard approach to clinical research data capturing and reporting. Recent efforts to standardize clinical study CDEs have been of great benefit in facilitating data integration and data sharing. The importance of the temporal dimension of clinical research studies has been well recognized; however, very few studies have focused on the formal representation of temporal constraints and temporal relationships within clinical research data in the biomedical research community. In particular, temporal information can be extremely powerful to enable high-quality cancer research.
Objective
The objective of the study was to develop and evaluate an ontological approach to represent the temporal aspects of cancer study CDEs.
Methods
We used CDEs recorded in the National Cancer Institute (NCI) Cancer Data Standards Repository (caDSR) and created a CDE parser to extract time-relevant CDEs from the caDSR. Using the Web Ontology Language (OWL)–based Time Event Ontology (TEO), we manually derived representative patterns to semantically model the temporal components of the CDEs using an observing set of randomly selected time-related CDEs (n=600) to create a set of TEO ontological representation patterns. In evaluating TEO’s ability to represent the temporal components of the CDEs, this set of representation patterns was tested against two test sets of randomly selected time-related CDEs (n=425).
Results
It was found that 94.2% (801/850) of the CDEs in the test sets could be represented by the TEO representation patterns.
Conclusions
In conclusion, TEO is a good ontological model for representing the temporal components of the CDEs recorded in caDSR. Our representative model can harness the Semantic Web reasoning and inferencing functionalities and present a means for temporal CDEs to be machine-readable, streamlining meaningful searches.}
}
@article{PAL2018985,
title = {Ontology-Based Web Service Architecture for Retail Supply Chain Management},
journal = {Procedia Computer Science},
volume = {130},
pages = {985-990},
year = {2018},
note = {The 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018) / The 8th International Conference on Sustainable Energy Information Technology (SEIT-2018) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.04.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918304630},
author = {Kamalendu Pal},
keywords = {Retail Supply Chain, Service Oriented Computing, Semantic Web Services, Case-Based Reasoning, Rule-Based Reasoning, Ontology Matchmaking Algorithm},
abstract = {Service-oriented computing (SOC) technologies provide numerous opportunities and value-added service capabilities that global retail business requires to remain competitive in the market. Initiative to semantic web service provision is playing a crucial role to realize the possibility of heterogenous information systems integration in supply chain. The ability to dynamically discover and invoke a web service is an important aspects of semantic web service-based architecture. An essential part of the service discovery process is the ontology-based semantic web service matchmaking algorithm. This paper presents the key features of an improved matchmaking algorithm to calculate the similarity between concepts on ontology for semantic web service. The matchmaking is taking place in the context of OWL-S (Ontology Web Language for Services) based retail sales management. The paper describes the Semantic Web Service Architecture-II (SWSA-II), which uses a hybrid knowledge-based system; and it consists of Structural Case-Based Reasoning (S-CBR), Rule-Based Reasoning (RBR), and an ontological concept similarity assessment algorithm. Finally, a business scenario is used to demonstrate the functionality of the algorithm.}
}
@article{ZHANG2021677,
title = {SSN_SEM: Design and application of a fusion ontology in the field of medical equipment},
journal = {Procedia Computer Science},
volume = {183},
pages = {677-682},
year = {2021},
note = {Proceedings of the 10th International Conference of Information and Communication Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.02.114},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921005901},
author = {Xue-Zhen Zhang and Bi-Hui Yu and Chang Liu},
keywords = {Medical equipment, domain ontology, event ontology},
abstract = {The importance of standardized operation of medical equipment is self-evident. An effective method is to construct the medical equipment domain ontology as the background knowledge base, and make judgments on the legality of the user’s operation sequence based on the ontology, and guide the user to legally operate the equipment. After analysis, medical device data mainly comes from internal sensors. If we want to build an ontology background knowledge base, we can combine data with user operations, map the ontology to events, and build a directed graph model through events to form the final ontology. In this paper, we take the atomized particle preparation device in the field of medical equipment as an example, and selects and constructs the ontology by analyzing the internal structure and use process of the device.The application-level ontology selects the SSN ontology in the sensor field, and the upper ontology selects the general event ontology SEM. Combining the above two ontology and the characteristics of the medical device field, the ontology SSN_SEM is constructed, and the actual data of the device is semantically annotated through the SSN_SEM ontology, which verifies the validity of the ontology.}
}
@article{WANG2024102820,
title = {Knowledge graph of agricultural engineering technology based on large language model},
journal = {Displays},
volume = {85},
pages = {102820},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2024.102820},
url = {https://www.sciencedirect.com/science/article/pii/S0141938224001847},
author = {Haowen Wang and Ruixue Zhao},
keywords = {LLM, Knowledge graph},
abstract = {Agriculture is an industry that has evolved alongside human evolution and has faithfully fulfilled its core mission of food supply. With the reduction of rural labor, the progress of artificial intelligence and the development of Internet of Things technology, it is hoped that the efficiency and productivity of the agricultural industry can be improved. Recently, with the development of information and intelligent technology, agricultural production and management have been significantly enhanced. However, there is still a considerable challenge in effectively integrating the vast amount of fragmented information for downstream applications. An agricultural knowledge graph (AGKG) will serve as the foundation for achieving these goals. Knowledge graphs can be general or domain-specific, and are the basis for many applications, such as search engines, online question-and-answer services, and knowledge inference. Therefore, there are many knowledge graphs, including Wikidata and DBpedia, for accessing structured knowledge. Although some general knowledge graphs contain some entities and relationships related to agriculture, there are no domain-specific knowledge graphs specifically for agricultural applications. Therefore, this paper proposes an agricultural knowledge graph (AGKG) for automatically integrating large amounts of agricultural data from the Internet. By applying natural language processing and deep learning technologies, AGKG can automatically identify agricultural entities from unstructured text and connect them to form a knowledge graph. In addition, we have described the typical scenarios of our AGKG and validated it through real-world applications such as agricultural entity retrieval and agricultural question-answering.}
}
@article{PELDSZUS2021101907,
title = {Ontology-driven evolution of software security},
journal = {Data & Knowledge Engineering},
volume = {134},
pages = {101907},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101907},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000343},
author = {Sven Peldszus and Jens Bürger and Timo Kehrer and Jan Jürjens},
keywords = {Software engineering, Model-based security, Security context knowledge, Ontology evolution, Semantic editing patterns, Security compliance},
abstract = {Ontologies as a means to formally specify the knowledge of a domain of interest have made their way into information and communication technology. Most often, such knowledge is subject to continuous change, which demands for consistent evolution of ontologies and dependent artifacts. In this article, we study ontology evolution in the context of software security, where ontologies may be used to formalize the security context knowledge which is needed to properly implement security requirements. In this application scenario, techniques for detecting ontology changes and determining their semantic impact are required to maintain the security of a software-intensive system in response to changing security context knowledge. Our solution is capable of detecting semantic editing patterns, which may be customly defined using graph transformation rules, but it does not depend on information about editing processes such as persistently managed changelogs. We leverage semantic editing patterns for (i) generating system co-evolution proposals, (ii) adapting the configuration of standard security checks, and (iii) performing incremental security compliance analyses between co-evolved system models and the implementation. We demonstrate the feasibility of the approach using a realistic medical information system known as iTrust.}
}
@article{USHIO2025104359,
title = {RelBERT: Embedding relations with language models},
journal = {Artificial Intelligence},
volume = {347},
pages = {104359},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104359},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225000785},
author = {Asahi Ushio and Jose Camacho-Collados and Steven Schockaert},
abstract = {Many applications need access to background knowledge about how different concepts and entities are related. Although Large Language Models (LLM) can address this need to some extent, LLMs are inefficient and difficult to control. As an alternative, we propose to extract relation embeddings from relatively small language models. In particular, we show that masked language models such as RoBERTa can be straightforwardly fine-tuned for this purpose, using only a small amount of training data. The resulting model, which we call RelBERT, captures relational similarity in a surprisingly fine-grained way, allowing us to set a new state-of-the-art in analogy benchmarks. Crucially, RelBERT is capable of modelling relations that go well beyond what the model has seen during training. For instance, we obtained strong results on relations between named entities with a model that was only trained on lexical relations between concepts, and we observed that RelBERT can recognise morphological analogies despite not being trained on such examples. Overall, we find that RelBERT significantly outperforms strategies based on prompting language models that are several orders of magnitude larger, including recent GPT-based models and open source models.1}
}
@article{YE202564,
title = {Multi-Omics clustering by integrating clinical features from large language model},
journal = {Methods},
volume = {239},
pages = {64-71},
year = {2025},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2025.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1046202325000830},
author = {Xiucai Ye and Tianyi Shi and Dong Huang and Tetsuya Sakurai},
keywords = {Multi-omics clustering, Cancer subtyping, Spectral clustering, Large language model},
abstract = {Multi-omics clustering has emerged as a powerful approach for understanding complex biological systems and enabling cancer subtyping by integrating diverse omics data. Existing methods primarily focus on the integration of different types of omics data, often overlooking the value of clinical context. In this study, we propose a novel framework that incorporates clinical features extracted from large language model (LLM) to enhance multi-omics clustering. Leveraging clinical data extracted from pathology reports using a BERT-based model, our framework converts unstructured medical text into structured clinical features. These features are integrated with omics data through an autoencoder, enriching the information content of each omics layer to improve feature extraction. The extracted features are then projected into a latent subspace using singular value decomposition (SVD), followed by spectral clustering to obtain the final clustering result. We evaluate the proposed framework on six cancer datasets on three omics levels, comparing it with several state-of-the-art methods. The experimental results demonstrate that the proposed framework outperforms existing methods in multi-omics clustering for cancer subtyping. Moreover, the results highlight the efficacy of integrating clinical features derived from LLM, significantly enhancing clustering performance. This work underscores the importance of clinical context in multi-omics analysis and showcases the transformative potential of LLM in advancing precision medicine.}
}
@article{WU2025100036,
title = {Perspectives: LLM agents reshaping the foundation of geotechnical problem-solving},
journal = {Geodata and AI},
volume = {4},
pages = {100036},
year = {2025},
issn = {3050-483X},
doi = {https://doi.org/10.1016/j.geoai.2025.100036},
url = {https://www.sciencedirect.com/science/article/pii/S3050483X25000358},
author = {Stephen Wu and Chao Shi and Yat Fai Leung and Yu Otake and Chisato Konishi and Mingliang Zhou and Yuanqin Tao and Zijun Cao and Tomoka Nakamura},
keywords = {Agentic AI, LLMs, Foundation models},
abstract = {This paper explores the transformative potential of Large Language Model (LLM)-based agentic artificial intelligence (AI) in addressing longstanding challenges in geotechnical engineering. It begins by highlighting the significant growth and increasing interest in applying machine learning (ML) and AI techniques across various geotechnical domains, such as soil classification, slope stability analysis, and foundation design. Emphasizing the Gartner Hype Cycle, the authors reflect on the transition from initial enthusiasm toward realistic appraisal and adoption, highlighting current barriers like limited foundational understanding, skepticism about AI reliability, and a lack of standardized practices. The authors then introduce LLM agents as promising solutions for automating the extraction, interpretation, and quantification of qualitative and semi-quantitative geotechnical data. Drawing insights from the 1st GeoTechathon event, an international collaboration involving engineers, data scientists, and AI practitioners, the paper demonstrates practical applications in geotechnical site planning, landslide investigations, liquefaction analysis, and shield tunnel safety evaluation. Each project leveraged basic techniques, including Retrieval-Augmented Generation (RAG), multimodal data integration, and prompt engineering, achieving improvements in efficiency, accuracy, and decision-making processes. The paper concludes by discussing broader implications for interdisciplinary collaboration, ethical considerations, and future directions, emphasizing the necessity for standardized practices, rigorous validation, and enhanced AI literacy to sustainably integrate LLM technologies within the geotechnical engineering community.}
}
@incollection{KARKERA2025407,
title = {Large Language Models for Pathway Curation: A Preliminary Investigation},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {407-415},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00254-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002542},
author = {Nikitha Karkera and Nikshita Karkera and Mahanash Kumar and Vishnuvardhan P. Srinivasulu and Samik Ghosh and Sucheendra K. Palaniappan},
keywords = {Generative AI, Gpt3.5, Gpt4, LLM, Pathway curation},
abstract = {The pathway curation task involves analyzing scientific literature to identify and represent cellular processes as pathways. This process, often time-consuming and labor-intensive, requires significant curation efforts amidst the rapidly growing biomedical literature. Natural Language Processing (NLP) offers a promising method to automatically extract these interactions from scientific texts. Despite immense progress, there remains room for improvement in these systems. The emergence of Large Language Models (LLMs) provides a promising solution for this challenge. Our study conducts a preliminary investigation into leveraging LLMs for the pathway curation task. This paper first presents a review of the current state-of-the-art algorithms for the pathway curation task. Our objective is to check the feasibility and formulate strategies of using these LLMs to improve the accuracy of pathway curation task. Our experiments demonstrate that our GPT-3.5 based fine-tuned models outperforms existing state-of-the-art methods. Specifically, our model achieved a 10 basis point improvement in overall recall and F1 score compared to the best existing algorithms. These findings highlight the potential of LLMs in pathway curation tasks, warranting further research and substantial efforts in this direction.}
}
@article{CHO2020257,
title = {Ontology for Strategies and Predictive Maintenance models},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {3},
pages = {257-264},
year = {2020},
note = {4th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies - AMEST 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320301889},
author = {Sangje Cho and Marlène Hildebrand-Ehrhardt and Gokan May and Dimitris Kiritsis},
keywords = {Ontology, Semantic technology, Semantic interoperability, Maintenance, Predictive maintenance, Industry 4.0},
abstract = {As of today, to cope with traditional maintenance policies such as reactive and preventive maintenance, the manufacturing companies need the deployment of adaptive and responsive maintenance strategies. Meanwhile, the advent of Industry 4.0 leads the maintenance paradigm shift facilitated by the efficient monitoring of physical assets and forecasting of the potential risks. As the advanced maintenance policies benefit in terms of cost-efficiency, inventory management and reliability management, most of the manufacturing companies are trying to make their own advanced maintenance strategies and to elaborate on the development of an innovative platform for it. However, since advanced enabling technologies collect a huge amount of data from different data sources such as machine, component, document, process and so on, data federation should necessarily be achieved for further discussion, but manufacturing companies are immature to address this issue. H2020 EU project Z-BRE4K, i.e., Strategies and predictive maintenance models wrapped around physical systems for zero-unexpected-breakdowns and increased operating life of factories, deploys semantic technologies to address this issue. This paper deals with the debate on how to efficiently federate various data formats with the support of the semantic technologies in the context of maintenance. In addition, it proposes a maintenance ontology validated and implemented with an actor from European industry.}
}
@article{GUO2025106490,
title = {Enhancing visual-LLM for construction site safety compliance via prompt engineering and Bi-stage retrieval-augmented generation},
journal = {Automation in Construction},
volume = {179},
pages = {106490},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106490},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525005308},
author = {Koi Xiaowen Guo and Peter Kok-Yiu Wong and Jack C.P. Cheng and Chak-Fu Chan and Pak-Him Leung and Xingyu Tao},
keywords = {Construction site safety monitoring, Regulation compliance analysis, Multimodal large language models, Prompt engineering, Retrieval-augmented generation},
abstract = {The escalating frequency of safety incidents on construction sites requires an effective safety management framework. Traditional computer vision systems are constrained by their static nature, limited generalization, and inadequate semantic comprehension. This paper integrates a multi-modal Visual Language Model (VLM) with our proposed Bi-stage Retrieval-Augmented Generation (BiRAG) framework, which enhances safety compliance monitoring based on construction site images, with high scalability and adaptiveness to evolving safety standards without tedious model fine-tuning. A TriPhased prompt (TPP) and a decision-tree-based compliance judgment prompt are designed to enhance the VLM's ability to interpret worker behaviors and safety compliance from site images. A context-aware chunking strategy and hybrid retrieval algorithm are developed to improve the analysis against relevant safety regulations. Experiments with images collected from a real construction site in Hong Kong demonstrated a 7.73 % increase in retrieval accuracy and an 11.66 % improvement in compliance analysis accuracy, offering a holistic construction safety management solution.}
}
@article{BALAJIB2018435,
title = {Fuzzy service conceptual ontology system for cloud service recommendation},
journal = {Computers & Electrical Engineering},
volume = {69},
pages = {435-446},
year = {2018},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0045790616302804},
author = {Saravana {Balaji B} and Karthikeyan {N K} and Raj Kumar {R S}},
keywords = {Fuzzy service ontology, Cloud services, Fuzzy Clustering, OWL, SPARQL},
abstract = {In view of the diverse nature of cloud services, cloud service selection is still an open issue. Unlike web services, which are selected based on WSDL description, cloud services cannot be selected; there is no standard representation of cloud services, since most of these services are described in natural language. In this paper, we propose a cloud service recommendation system which is built upon semantic technologies. This system parses the cloud service description documents using a natural language processing method; the primary concept of the cloud service is identified and the fuzzy service ontology clusters are then updated. The user query is processed using natural language processing methods, and fuzzy connectives are used to refine the query based on first-order Horn clause logic and disjunctive normal form (DNF). A performance evaluation shows that this system provides better results than other methods.}
}
@article{ZHAO2025112380,
title = {LISA: A Lithium-Ion Solid-State Assistant using large language models for knowledge defragmentation in battery science and beyond},
journal = {Materials Today Communications},
volume = {45},
pages = {112380},
year = {2025},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2025.112380},
url = {https://www.sciencedirect.com/science/article/pii/S235249282500892X},
author = {Yinghan Zhao and Anna-Lena Hansen and Anna Dahlhaus and Nico Brandt and Michael Selzer and Arnd Koeppe and Britta Nestler and Michael Knapp and Helmut Ehrenberg},
keywords = {Large language model, Retrieval augmented generation, Solid-state battery, Research assistant},
abstract = {This work presents the development and implementation of a research assistant tool, Lithium-Ion Solid-State Assistant (LISA), based on the Retrieval-Augmented Generation (RAG) architecture. This assistant has been specifically tailored to enhance the retrieval and extraction of information from the domain of solid-state battery research. The system employs sophisticated retrieval techniques to efficiently identify the most pertinent document segments in response to researcher queries. The segments above are subsequently collated into prompts for a Large Language Model (LLM), which generates accurate, contextually enhanced responses to queries about solid-state battery-related subjects. This approach has the potential to markedly improve the accessibility and usability of a range of documentation, from project reports to complex scientific literature. The system provides researchers with a powerful tool to bridge disciplinary gaps, facilitate cross-disciplinary communication, accelerate knowledge discovery, and drive innovation in the field of solid-state batteries. A comprehensive evaluation was conducted to assess the system’s performance, with results indicating its potential to transform scientific research workflows. The system offers a robust open-source framework for future advancements in automated knowledge retrieval, understanding, and management, particularly in supporting the development of new materials.}
}
@article{JARVENPAA20181094,
title = {Product Model ontology and its use in capability-based matchmaking},
journal = {Procedia CIRP},
volume = {72},
pages = {1094-1099},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.211},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118303718},
author = {Eeva Järvenpää and Niko Siltala and Otto Hylli and Minna Lanz},
keywords = {Product Model, Ontology, Information Model, Capability-based matchmaking, Production system design, Reconfiguration},
abstract = {Capability-based matchmaking aims to support rapid design and reconfiguration of modular plug-and-produce type production systems. It relies on formal ontological descriptions of product requirements and resource capabilities. This paper introduces the structure and content of the developed Product Model ontology, and explains its role as a part of the capability matchmaking procedure. A case product is modelled in order to visualize a matchmaking scenario. We expect that such matchmaking will reduce the workload of system designers and reconfiguration planners as it can automatically suggest potential resources for a certain need from large resource catalogues.}
}
@article{SARKAR20191889,
title = {Ontology Model for Process Level Capabilities of Manufacturing Resources},
journal = {Procedia Manufacturing},
volume = {39},
pages = {1889-1898},
year = {2019},
note = {25th International Conference on Production Research Manufacturing Innovation: Cyber Physical Manufacturing August 9-14, 2019 | Chicago, Illinois (USA)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.244},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920303012},
author = {Arkopaul Sarkar and Dušan Šormaz},
keywords = {Process Capability, Manufacturing Resource, Ontology, OWL},
abstract = {The promise of distributed cloud manufacturing (CM) is to manufacture products using shared resources, both asset-full (machines, tools, vehicles) and asset-light (design, analysis, inspection, management, maintenance), which can be provisioned flexibly and rapidly with minimal management and service provider interaction. One of the key enabler of CM are virtual enterprises (VE), which offer manufacturing resources as virtual services (SaaS, HaaS, PaaS) in a cloud based marketplace. Currently, virtualization and provisioning of diverse array of manufacturing resources face challenges from the heterogeneity in representation and communication protocols, as well as lack of integration with legacy practices in the organizations. Aiming to increase interoperability, a number of formal ontologies were developed by researchers in the past, to leverage on semantic data integration and validation. In spite of their success in providing axiomatic description and common taxonomy to classify manufacturing resources from different domains, models of representing the capabilities of the resources (i.e. expected quality of services they offer) were often overlooked. In this research, we present an ontology model to represent capabilities of manufacturing machine-tools at the process level, often called process boundaries (measured by process capability index) in industries. The definition of the capability is derived based on the foundational ontology called ‘Basic Formal Ontology’ (BFO). The primary contribution of this extension is a set of OWL axioms which can be used to assert facts about modal future (possibilia) – ultimately enabling us to associate process specific performance metrics to the semantic models of virtual machine-tools.}
}
@article{NOH2023,
title = {Identification of Emotional Spectrums of Patients Taking an Erectile Dysfunction Medication: Ontology-Based Emotion Analysis of Patient Medication Reviews on Social Media},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/50152},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123009172},
author = {Youran Noh and Maryanne Kim and Song Hee Hong},
keywords = {erectile dysfunction, PDE5 inhibitor, social media, emotion analysis, sentiment analysis, emotions, patient medication experience, tailored patient medication, patient-centered care, men's health, medications, drugs},
abstract = {Background
Patient medication reviews on social networking sites provide valuable insights into the experiences and sentiments of individuals taking specific medications. Understanding the emotional spectrum expressed by patients can shed light on their overall satisfaction with medication treatment. This study aims to explore the emotions expressed by patients taking phosphodiesterase type 5 (PDE5) inhibitors and their impact on sentiment.
Objective
This study aimed to (1) identify the distribution of 6 Parrot emotions in patient medication reviews across different patient characteristics and PDE5 inhibitors, (2) determine the relative impact of each emotion on the overall sentiment derived from the language expressed in each patient medication review while controlling for different patient characteristics and PDE5 inhibitors, and (3) assess the predictive power of the overall sentiment in explaining patient satisfaction with medication treatment.
Methods
A data set of patient medication reviews for sildenafil, vardenafil, and tadalafil was collected from 3 popular social networking sites such as WebMD, Ask-a-Patient, and Drugs.com. The Parrot emotion model, which categorizes emotions into 6 primary classes (surprise, anger, love, joy, sadness, and fear), was used to analyze the emotional content of the reviews. Logistic regression and sentiment analysis techniques were used to examine the distribution of emotions across different patient characteristics and PDE5 inhibitors and to quantify their contribution to sentiment.
Results
The analysis included 3070 patient medication reviews. The most prevalent emotions expressed were joy and sadness, with joy being the most prevalent among positive emotions and sadness being the most prevalent among negative emotions. Emotion distributions varied across patient characteristics and PDE5 inhibitors. Regression analysis revealed that joy had the strongest positive impact on sentiment, while sadness had the most negative impact. The sentiment score derived from patient reviews significantly predicted patient satisfaction with medication treatment, explaining 19% of the variance (increase in R2) when controlling for patient characteristics and PDE5 inhibitors.
Conclusions
This study provides valuable insights into the emotional experiences of patients taking PDE5 inhibitors. The findings highlight the importance of emotions in shaping patient sentiment and satisfaction with medication treatment. Understanding these emotional dynamics can aid health care providers in better addressing patient needs and improving overall patient care.}
}
@article{OYELADE2025127455,
title = {SMAR + NIE IdeaGen: A knowledge graph based node importance estimation with analogical reasoning on large language model for idea generation},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127455},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127455},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010772},
author = {Olaide N. Oyelade and Hui Wang and Karen Rafferty},
keywords = {Knowledge graphs (KGs), Large language model (LLMs), Idea generation, Novelty, Analogical reasoning, Node importance estimation, Natural language processing (NLP), Isomorphic subgraphs},
abstract = {Idea generation describes a creative process involving reasoning over some knowledge to derive new information. Traditional approaches such as mind-map and brainstorming are limited and often fail due to lack of quality ideas and ineffective methods. The reasoning capability of large language models (LLMs) have been investigated for ideation tasks and have reported interesting performance. However, these models suffer from limited logical reasoning capability which hinders the use of structural and factual real-world knowledge in discovery of latent insight and predict possible outcome when applied to ideation. In addition, the possibility of LLMs regurgitating knowledge learnt from datasets might adversely impact the degree of novel ideas the models can generate. In this paper, a two-stage logical reasoning approach is applied to initiate the search for candidate idea pathways based on the knowledge graphs (KGs) to address the problem of reasoning, domain-specificity and novelty. The divergence stage this reasoning explores utilizes a new node importance estimation (NIE) technique over KGs to discover latent connections supporting idea generation. In the convergence stage of this reasoning, subgraph matching using analogical reasoning (SMAR) is applied to find matching patterns to describe a new idea. The use of SMAR + NIE and KGs helps to achieve an improvement in reasoning over KGs before transferring such reasoning to LLMs for translation of idea into natural language. To evaluate the degree of novelty of ideas generated, a relevance-to-novelty scoring metrics is proposed based on multiple premise entailment (MPE). We combined this metric with other popular metrics to evaluate the performance of SMAR + NIE on benchmark datasets, and as well on the quality of ideas generated. Findings from the study showed that this approach demonstrates competitive performance with mainstream LLMs in idea generation tasks.}
}
@article{AYADI2020360,
title = {Ontology-based NLP information extraction to enrich nanomaterial environmental exposure database},
journal = {Procedia Computer Science},
volume = {176},
pages = {360-369},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.037},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318615},
author = {Ali Ayadi and Mélanie Auffan and Jérôme Rose},
keywords = {Database enrichment, Information extraction, NLP techniques, Domain ontology, Engineered nanomaterials, Mesocosms},
abstract = {In recent years, nanotechnologies have led to undeniable progress in any domains, such as electronics, materials and medicine. Despite the benefits of such a technology, a careful assessment of the potential risks for Human and Environmental health have to be studied. Assessing exposure and hazard to nanomaterials is a major challenge in the field of environmental sciences. This task requires to gather a large amount of meaningful experimental data usually generated by laboratory experiments. A first database of environmental exposure to nanomaterials (EXPOSED database) has been developed to gather data generated during mesocosm experiments. The challenge is now to enrich this database with more data from scientific articles in related fields. Herein, we present an ontology-based Natural Language Processing (NLP) approach to automatically extract and transfer data from text sources to database. This approach combines the use of NLP techniques and a domain ontology to automatically extract environmental exposure and hazards information. This approach was tested to enrich the EXPOSED database and indicators of quality highlight that this approach is effective and promising.}
}
@article{XIA2024102728,
title = {Leveraging error-assisted fine-tuning large language models for manufacturing excellence},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {88},
pages = {102728},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2024.102728},
url = {https://www.sciencedirect.com/science/article/pii/S0736584524000140},
author = {Liqiao Xia and Chengxi Li and Canbin Zhang and Shimin Liu and Pai Zheng},
keywords = {Large language model, Smart manufacturing, Industry 4.0, Knowledge management, Generative AI},
abstract = {The emergence of large language models (LLM), like GPT, is revolutionizing the field of information retrieval, finding applications across a wide range of domains. However, the intricate domain knowledge and the unique software paradigms inherent to the manufacturing sector have posed significant barriers to the effective utilization of LLM. To address this divide, an error-assisted fine-tuning approach is proposed to adapt LLM specifically for the manufacturing domain. Initially, the LLM is fine-tuned using a manufacturing-domain corpus, allowing it to learn and adapt to the nuances of the manufacturing field. Additionally, the injection of a labeled dataset into a pre-configured LLM enhances its ability to identify key elements within the domain. To ensure the generation of syntactically valid programs in domain-specific languages, and to accommodate environmental constraints, an error-assisted iterative prompting procedure is introduced, which facilitates the generation of reliable and expected code. Experimental results demonstrate the model’s proficiency in accurately responding to manufacturing-related queries and its effectiveness in generating reliable code, where the accuracy of judgment querying can experience an improvement of approximately 4.1%. By expanding the applicability of LLM to the manufacturing industry, it is hoped that this research will pave the way for a broad array of new LLM-based applications within manufacturing.}
}
@article{BENABDELLAH2023,
title = {Smart Product Design Ontology Development for Managing Digital Agility},
journal = {Journal of Global Information Management},
volume = {31},
number = {8},
year = {2023},
issn = {1062-7375},
doi = {https://doi.org/10.4018/JGIM.333599},
url = {https://www.sciencedirect.com/science/article/pii/S1062737523001191},
author = {Abla Chaouni Benabdellah and Kamar Zekhnini and Surajit Bag and Shivam Gupta and Sarbjit Singh Oberoi},
keywords = {Circular Economy, Collaborative Product Development Process, Design for X Techniques, Digital Agility, Ontology Development, Semantic Web},
abstract = {ABSTRACT
Digital agility is a critical dynamic capability that is becoming increasingly important in the context of collaborative product development processes (PDPs). This paper aims to address the complexity of today's PDPs by considering various quality aspects including safety, environment, and the entire lifecycle, along with diverse dynamic capabilities such as digital agility and circular economy. The authors employed a semantic web methodology and created an ontology-based knowledge model. The proposed ontology uses Design for X techniques, circular economy, digital agility, and the semantic web under the PDP perspective to increase performance and cooperation between designers and the project team. To validate the ontology, measures for domain ontology evaluation have been used. The paper presents a detailed guide for ontology engineering and evaluation for collaborative smart PDP, which incorporates digital agility as a critical dynamic capability. The proposed ontology can help boost PDP performance and increase customer satisfaction.}
}
@article{KIM2019,
title = {Developing a Physical Activity Ontology to Support the Interoperability of Physical Activity Data},
journal = {Journal of Medical Internet Research},
volume = {21},
number = {4},
year = {2019},
issn = {1438-8871},
doi = {https://doi.org/10.2196/12776},
url = {https://www.sciencedirect.com/science/article/pii/S1438887119002012},
author = {Hyeoneui Kim and Jessica Mentzer and Ricky Taira},
keywords = {exercise, leisure activities, health information interoperability, terminology as topic},
abstract = {Background
Physical activity data provides important information on disease onset, progression, and treatment outcomes. Although analyzing physical activity data in conjunction with other clinical and microbiological data will lead to new insights crucial for improving human health, it has been hampered partly because of the large variations in the way the data are collected and presented.
Objective
The aim of this study was to develop a Physical Activity Ontology (PACO) to support structuring and standardizing heterogeneous descriptions of physical activities.
Methods
We prepared a corpus of 1140 unique sentences collected from various physical activity questionnaires and scales as well as existing standardized terminologies and ontologies. We extracted concepts relevant to physical activity from the corpus using a natural language processing toolkit called Multipurpose Text Processing Tool. The target concepts were formalized into an ontology using Protégé (version 4). Evaluation of PACO was performed to ensure logical and structural consistency as well as adherence to the best practice principles of building an ontology. A use case application of PACO was demonstrated by structuring and standardizing 36 exercise habit statements and then automatically classifying them to a defined class of either sufficiently active or insufficiently active using FaCT++, an ontology reasoner available in Protégé.
Results
PACO was constructed using 268 unique concepts extracted from the questionnaires and assessment scales. PACO contains 225 classes including 9 defined classes, 20 object properties, 1 data property, and 23 instances (excluding 36 exercise statements). The maximum depth of classes is 4, and the maximum number of siblings is 38. The evaluations with ontology auditing tools confirmed that PACO is structurally and logically consistent and satisfies the majority of the best practice rules of ontology authoring. We showed in a small sample of 36 exercise habit statements that we could formally represent them using PACO concepts and object properties. The formal representation was used to infer a patient activity status category of sufficiently active or insufficiently active using the FaCT++ reasoner.
Conclusions
As a first step toward standardizing and structuring heterogeneous descriptions of physical activities for integrative data analyses, PACO was constructed based on the concepts collected from physical activity questionnaires and assessment scales. PACO was evaluated to be structurally consistent and compliant to ontology authoring principles. PACO was also demonstrated to be potentially useful in standardizing heterogeneous physical activity descriptions and classifying them into clinically meaningful categories that reflect adequacy of exercise.}
}
@article{BOUYERBOU2019232,
title = {Geographic ontology for major disasters: Methodology and implementation},
journal = {International Journal of Disaster Risk Reduction},
volume = {34},
pages = {232-242},
year = {2019},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2018.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S221242091830476X},
author = {Hafidha Bouyerbou and Kamal Bechkoum and Richard Lepage},
keywords = {Information retrieval, Major disasters, Ontology, Ontology web language (OWL), Reasoning, Semantics},
abstract = {During a catastrophic event, the International Charter11http://www.disasterscharter.org/. "Space and Major Disasters" is regularly activated and provides the rescue teams damage maps prepared by a photo-interpreter team basing on pre and post-disaster satellite images. A satellite image manual processing must be accomplished in most cases to build these maps, a complex and demanding process. Given the importance of time in such critical situations, automatic or semiautomatic tools are highly recommended. Despite the quick treatment presented by automatic processing, it usually presents a semantic gap issue. Our aim is to express expert knowledge using a well-defined knowledge representation method: ontologies and make semantics explicit in geographic and remote sensing applications by taking the ontology advantages in knowledge representation, expression, and knowledge discovery. This research focuses on the design and implementation of a comprehensive geographic ontology in the case of major disasters, that we named GEO-MD, and illustrates its application in the case of Haiti 2010 earthquake. Results show how the ontology integration reduces the semantic gap and improves the automatic classification accuracy.}
}
@article{OTMANI2018359,
title = {Ontology-based approach to enhance medical web information extraction},
journal = {International Journal of Web Information Systems},
volume = {15},
number = {3},
pages = {359-382},
year = {2018},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-03-2018-0017},
url = {https://www.sciencedirect.com/science/article/pii/S1744008418000162},
author = {Nassim Abdeldjallal Otmani and Malik Si-Mohammed and Catherine Comparot and Pierre-Jean Charrel},
keywords = {Web search and information extraction, Metadata and ontologies, Knowledge engineering, Online patient-doctor conversation},
abstract = {Purpose
The purpose of this study is to propose a framework for extracting medical information from the Web using domain ontologies. Patient–Doctor conversations have become prevalent on the Web. For instance, solutions like HealthTap or AskTheDoctors allow patients to ask doctors health-related questions. However, most online health-care consumers still struggle to express their questions efficiently due mainly to the expert/layman language and knowledge discrepancy. Extracting information from these layman descriptions, which typically lack expert terminology, is challenging. This hinders the efficiency of the underlying applications such as information retrieval. Herein, an ontology-driven approach is proposed, which aims at extracting information from such sparse descriptions using a meta-model.
Design/methodology/approach
A meta-model is designed to bridge the gap between the vocabulary of the medical experts and the consumers of the health services. The meta-model is mapped with SNOMED-CT to access the comprehensive medical vocabulary, as well as with WordNet to improve the coverage of layman terms during information extraction. To assess the potential of the approach, an information extraction prototype based on syntactical patterns is implemented.
Findings
The evaluation of the approach on the gold standard corpus defined in Task1 of ShARe CLEF 2013 showed promising results, an F-score of 0.79 for recognizing medical concepts in real-life medical documents.
Originality/value
The originality of the proposed approach lies in the way information is extracted. The context defined through a meta-model proved to be efficient for the task of information extraction, especially from layman descriptions.}
}
@article{ZHONG2018127,
title = {Ontology-based framework for building environmental monitoring and compliance checking under BIM environment},
journal = {Building and Environment},
volume = {141},
pages = {127-142},
year = {2018},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2018.05.046},
url = {https://www.sciencedirect.com/science/article/pii/S0360132318303123},
author = {Botao Zhong and Chen Gan and Hanbin Luo and Xuejiao Xing},
keywords = {Building environmental monitoring, BIM, Ontology, Compliance checking, Semantic web},
abstract = {Building environmental monitoring and compliance checking are important in ensuring environmental performance. The information required for monitoring and checking is obtained from different data sources in different information systems. In this context, information sharing between stakeholders and the semantic interoperability that prevails with varying information systems are necessary. However, the implementation of information sharing and semantic interoperability can be a challenge. This paper proposes an ontology-based framework to support environmental monitoring and compliance checking under building information modeling (BIM) environment among different information systems. The framework integrates building information from BIM, environmental information provided by sensors, and regulatory information based on building regulations and design requirements. In this framework, four specific ontologies are developed to represent relevant knowledge. Building information is extracted from BIM and then converted, together with environmental information provided by sensors, into resource description framework format as ontology instances. The regulation clauses are transformed into SPARQL (SPARQL Protocol and RDF Query Language) rules. A case study is performed to apply the framework, and environmental monitoring and automated compliance checking are implemented in the context of a real distributed energy station project. The testing results validate the feasibility and effectiveness of the proposed framework.}
}
@article{REN2021103565,
title = {Aligning BIM and ontology for information retrieve and reasoning in value for money assessment},
journal = {Automation in Construction},
volume = {124},
pages = {103565},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103565},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521000169},
author = {Guoqian Ren and Haijiang Li and Song Liu and Jaliya Goonetillake and Ali Khudhair and Steven Arthur},
keywords = {Building information modeling, Ontology, Value for money, Cost estimate, Information retrieval, Alignment approach},
abstract = {Value for money (VfM) assessments often lack effective automatic processes and reasoning support in public-private partnership (PPP) projects. To automate these assessments, this paper proposes a comprehensive approach that aligns with the goals of building information modeling (BIM) as the necessary information support and ontology for the knowledge process. The main contribution of this work is the retrieval of information from the BIM environment with the ontological knowledge base to enable more efficient and persuasive methods for project and finance management that facilitate decision-making rather than an experience-based approach. The constructed ontology can also be reused and further expanded to include project needs from end-user perspectives. This work is expected to further the research on expanding semantic BIM-based decision making in different infrastructure procurement projects.}
}
@article{LEE20193285,
title = {A Cloud Model-based Knowledge Mapping Method for Historic Building Maintenance based on Building Information Modelling and Ontology},
journal = {KSCE Journal of Civil Engineering},
volume = {23},
number = {8},
pages = {3285-3296},
year = {2019},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-019-2457-0},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824032380},
author = {Pin-Chan Lee and Wei Xie and Tzu-Ping Lo and Danbing Long and Xiaofei Tang},
keywords = {BIM, cloud model, FMEA, historic building, knowledge mapping, ontology},
abstract = {The maintenance of historic buildings requires a systematic approach to construct and reuse maintenance knowledge. Maintenance knowledge of historic buildings has extensive and specific knowledge, but a small number of experts provide most of experience. With the rapid development of building information modelling (BIM), it can facilitate maintenance management of historic buildings. Recently, research on connecting ontology and BIM was discussed to promote building knowledge management (BKM). However, BKM is less applied in historic buildings and connection strength is also less discussed. Connection strength of knowledge mapping can increase the performance of knowledge retrieval. Therefore, this study aims to build connection between maintenance ontology of historic buildings and BIM, and also proposes a cloud model-based knowledge mapping method to evaluate the connection strength. This study uses FMEA (failure mode and effects analysis) to connect ontology and BIM, and the grey relational analysis to evaluate the connection strength. Meanwhile, cloud model is integrated into FMEA to better deal with uncertain information to increase the reliability. This study adopts a real case to valid the practicability. The results show the proposed method can evaluate the connection strengths with uncertain information and obtain the maintenance knowledge more efficiently.}
}
@article{CHEN2025104812,
title = {Enhancing data quality in medical concept normalization through large language models},
journal = {Journal of Biomedical Informatics},
volume = {165},
pages = {104812},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104812},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000413},
author = {Haihua Chen and Ruochi Li and Ana Cleveland and Junhua Ding},
keywords = {Medical concept normalization, Machine learning, Data quality, Data augmentation, Large language model, ChatGPT},
abstract = {Objective:
Medical concept normalization (MCN) aims to map informal medical terms to formal medical concepts, a critical task in building machine learning systems for medical applications. However, most existing studies on MCN primarily focus on models and algorithms, often overlooking the vital role of data quality. This research evaluates MCN performance across varying data quality scenarios and investigates how to leverage these evaluation results to enhance data quality, ultimately improving MCN performance through the use of large language models (LLMs). The effectiveness of the proposed approach is demonstrated through a case study.
Methods:
We begin by conducting a data quality evaluation of a dataset used for MCN. Based on these findings, we employ ChatGPT-based zero-shot prompting for data augmentation. The quality of the generated data is then assessed across the dimensions of correctness and comprehensiveness. A series of experiments is performed to analyze the impact of data quality on MCN model performance. These results guide us in implementing LLM-based few-shot prompting to further enhance data quality and improve model performance.
Results:
Duplication of data items within a dataset can lead to inaccurate evaluation results. Data augmentation techniques such as zero-shot and few-shot learning with ChatGPT can introduce duplicated data items, particularly those in the mean region of a dataset’s distribution. As such, data augmentation strategies must be carefully designed, incorporating context information and training data to avoid these issues. Additionally, we found that including augmented data in the testing set is necessary to fairly evaluate the effectiveness of data augmentation strategies.
Conclusion:
While LLMs can generate high-quality data for MCN, the success of data augmentation depends heavily on the strategy employed. Our study found that few-shot learning, with prompts that incorporate appropriate context and a small, representative set of original data, is an effective approach. The methods developed in this research, including the data quality evaluation framework, LLM-based data augmentation strategies, and procedures for data quality enhancement, provide valuable insights for data augmentation and evaluation in similar deep learning applications.
Availability:
https://github.com/RichardLRC/mcn-data-quality-llm/tree/main/evaluation}
}
@article{SONG2025130979,
title = {MKE-PLLM: A benchmark for multilingual knowledge editing on pretrained large language model},
journal = {Neurocomputing},
volume = {651},
pages = {130979},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130979},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225016510},
author = {Ran Song and Shengxiang Gao and Xiaofei Gao and Cunli Mao and Zhengtao Yu},
keywords = {Natural language processing, Multilingual knowledge editing, Large language model},
abstract = {Multilingual large language models have demonstrated remarkable performance across various downstream tasks but are still plagued by factuality errors. Knowledge editing aims to correct these errors by modifying the internal knowledge of pre-trained models. However, current knowledge editing methods primarily focus on monolingual settings, neglecting the complexities and interdependencies within multilingual scenarios. Furthermore, benchmarks specifically designed for multilingual knowledge editing are relatively scarce. Addressing this gap, this paper constructs a novel multilingual knowledge editing benchmark. This benchmark comprehensively evaluates methods for mLLMs based on accuracy, reliability, generalization, and consistency. To ensure the robustness and usability of the benchmark, we conducted detailed analysis and validation. Concurrently, we propose a baseline method that adapts existing monolingual knowledge editing techniques to the multilingual environment. Extensive experimental results demonstrate the effectiveness of our constructed benchmark in evaluating multilingual knowledge editing.}
}
@article{ABBASI202155,
title = {An ontology model to support the automated design of aquaponic grow beds},
journal = {Procedia CIRP},
volume = {100},
pages = {55-60},
year = {2021},
note = {31st CIRP Design Conference 2021 (CIRP Design 2021)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121004674},
author = {Rabiya Abbasi and Pablo Martinez and Rafiq Ahmad},
keywords = {knowledge modeling, aquaponics, precision farming, parametric design, design automation},
abstract = {Aquaponics is a promising sustainable farming method that combines aquaculture and hydroponics. It allows the growth of crops without soil, pesticides, or fertilizers, and with a minimum amount of water. In aquaponic systems, the design of the growing area is directly linked to the type of crop about to be planted. The type of crop directly determines, for example, the spacing between plants and between channels, which is critical to determine the footprint required and estimate the system productivity. This paper proposes a knowledge modeling approach to support the design of aquaponic systems by automatically determining the required characteristics of the aquaponic system based on crop selection. The knowledge modeling is outlined as an ontology model that formally describes the existent links between the aquaponic grow bed characteristics and its design parameters. This study gives practitioners the capacity to visualize the impact of the desired crop selection on the aquaponic system design, as well as supporting clearer decision-making regarding production facility layout and system design in aquaponic farms.}
}
@article{HASNAIN20201051,
title = {An Ontology Based Test Case Prioritization Approach in Regression Testing},
journal = {Computers, Materials and Continua},
volume = {67},
number = {1},
pages = {1051-1068},
year = {2020},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2021.014686},
url = {https://www.sciencedirect.com/science/article/pii/S1546221820001897},
author = {Muhammad Hasnain and Seung Ryul Jeong and Muhammad Fermi Pasha and Imran Ghani},
keywords = {Software code metric, machine learning, faults detection, testing},
abstract = {Regression testing is a widely studied research area, with the aim of meeting the quality challenges of software systems. To achieve a software system of good quality, we face high consumption of resources during testing. To overcome this challenge, test case prioritization (TCP) as a sub-type of regression testing is continuously investigated to achieve the testing objectives. This study provides an insight into proposing the ontology-based TCP (OTCP) approach, aimed at reducing the consumption of resources for the quality improvement and maintenance of software systems. The proposed approach uses software metrics to examine the behavior of classes of software systems. It uses Binary Logistic Regression (BLR) and AdaBoostM1 classifiers to verify correct predictions of the faulty and non-faulty classes of software systems. Reference ontology is used to match the code metrics and class attributes. We investigated five Java programs for the evaluation of the proposed approach, which was used to achieve code metrics. This study has resulted in an average percentage of fault detected (APFD) value of 94.80%, which is higher when compared to other TCP approaches. In future works, large sized programs in different languages can be used to evaluate the scalability of the proposed OTCP approach.}
}
@incollection{FERREIRA2021382,
title = {Biomedical Ontologies: Coverage, Access and Use},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {382-395},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11664-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383116642},
author = {João D. Ferreira and David C. Teixeira and Catia Pesquita},
keywords = {Biomedical ontologies, Clinical ontologies, Ontology applications},
abstract = {Biomedical research and healthcare practices generate a vast amount of digital content. Managing, curating and analyzing this data presents unique challenges arising from data heterogeneity, ambiguity, complexity and size. Ontologies provide a solution to these issues by serving as a conceptual framework that represents the entities of a domain, in a way that can be understood both by humans and machines. In recent years, ontologies have been adopted by the systems biology and systems medicine communities to generate knowledge-based representations of simulation models, by means of describing systems and restricting them to the most dynamic components of their behavior. We introduce the basic concepts of ontologies in the life sciences and the challenges and opportunities that their use presents. This is complemented by a survey of life sciences ontologies covering different aspects, from molecules and cells to organisms and the environment, and different domains, ranging from the metabolism, to phenotypes and diseases. Finally, we present an overview of popular tools in four areas of applications: ontology modeling and exploration, annotation and data integration, reasoning, and mining and analytics.}
}
@article{JANI2020,
title = {Using an Ontology to Facilitate More Accurate Coding of Social Prescriptions Addressing Social Determinants of Health: Feasibility Study},
journal = {Journal of Medical Internet Research},
volume = {22},
number = {12},
year = {2020},
issn = {1438-8871},
doi = {https://doi.org/10.2196/23721},
url = {https://www.sciencedirect.com/science/article/pii/S1438887120011310},
author = {Anant Jani and Harshana Liyanage and Cecilia Okusi and Julian Sherlock and Uy Hoang and Filipa Ferreira and Ivelina Yonova and Simon {de Lusignan}},
keywords = {social prescribing, clinical informatics, ontology, social determinants of health},
abstract = {Background
National Health Service (NHS) England supports social prescribing in order to address social determinants of health, which account for approximately 80% of all health outcomes. Nevertheless, data on ongoing social prescribing activities are lacking. Although NHS England has attempted to overcome this problem by recommending 3 standardized primary care codes, these codes do not capture the social prescribing activity to a level of granularity that would allow for fair attribution of outcomes to social prescribing.
Objective
In this study, we explored whether an alternative approach to coding social prescribing activity, specifically through a social prescribing ontology, can be used to capture the social prescriptions used in primary care in greater detail.
Methods
The social prescribing ontology, implemented according to the Web Ontology Language, was designed to cover several key concepts encompassing social determinants of health. Readv2 and Clinical Terms Version 3 codes were identified using the NHS Terms Browser. The Royal College of General Practitioners Research Surveillance Centre, a sentinel network of over 1000 primary care practices across England covering a population of more than 4,000,000 registered patients, was used for data analyses for a defined period (ie, January 2011 to December 2019).
Results
In all, 668 codes capturing social prescriptions addressing different social determinants of health were identified for the social prescribing ontology. For the study period, social prescribing ontology codes were used 5,504,037 times by primary care practices of the Royal College of General Practitioners Research Surveillance Centre as compared to 29,606 instances of use of social prescribing codes, including NHS England’s recommended codes.
Conclusions
A social prescribing ontology provides a powerful alternative to the codes currently recommended by NHS England to capture detailed social prescribing activity in England. The more detailed information thus obtained will allow for explorations about whether outputs or outcomes of care delivery can be attributed to social prescriptions, which is essential for demonstrating the overall value that social prescribing can deliver to the NHS and health care systems.}
}
@article{JEON2025125242,
title = {Grouping research proposals with funding agency requirements: A contextualized language model and constrained K-means clustering approach},
journal = {Expert Systems with Applications},
volume = {259},
pages = {125242},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125242},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424021092},
author = {Daeseong Jeon and Changyong Lee},
keywords = {Research proposal grouping, Funding agency requirements, Contextualized language model, constrained K-means clustering},
abstract = {As the volume of research proposals increases and the interdisciplinary nature of research fields exacerbates the complexity of manual proposal grouping, the grouping of research proposals becomes increasingly vital in funding agencies’ evaluation procedures. Although previous ontology- and word embedding-based approaches have made valuable contributions to the advancement of research proposal grouping, their practical utility has been limited due to a lack of consideration of funding agencies’ requirements. This study proposes a systematic approach for grouping research proposals that aligns with three common requirements: size, cannot-link, and must-link constraints. The proposed approach utilizes KLUE-RoBERTa for proposal vectorization and constrained K-means clustering for proposal grouping with size constraints. We introduce a proposal pre-partitioning and a proposal vector centralization to simultaneously consider the cannot- and must-link constraints in grouping proposals. An empirical analysis of 3,665 proposals submitted to the National Research Foundation of Korea demonstrates the effectiveness and practicality of the proposed approach. Additionally, we conduct a comparative analysis of various combinations of methodological components to optimize this approach. The proposed approach is considered a valuable complementary tool for grouping proposals, enhancing the overall efficiency and effectiveness of the proposal evaluation system.}
}
@article{GIUSTOZZI2018675,
title = {Context Modeling for Industry 4.0: an Ontology-Based Proposal},
journal = {Procedia Computer Science},
volume = {126},
pages = {675-684},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312791},
author = {Franco Giustozzi and Julien Saunier and Cecilia Zanni-Merk},
keywords = {Industry 4.0, Context Modeling, Reasoning, Ontology},
abstract = {Industry 4.0 is an initiative combining a set of technologies that help to achieve more efficient manufacturing processes. An important characteristic for industrial production in Industry 4.0 is that physical items such as sensors, devices and enterprise assets are connected to each other and to the Internet. In this environment, devices and sensors generate increasing amount of data. A key point to consider is that the execution of industrial processes should depend not only on their internal state and on user interactions but also on the context of their execution, in order to become context-aware and provide added-value information to improve the monitoring of operations and their performance. Ontologies emerge as a relevant method for representing manufacturing knowledge in a machine-interpretable way. Therefore, an ontology-based context model for industry is introduced in this paper. The model facilitates context representation and reasoning by providing structures for context-related concepts, rules and their semantics.}
}
@article{CARDINALE2021333,
title = {Application of a methodological approach to compare ontologies},
journal = {International Journal of Web Information Systems},
volume = {17},
number = {4},
pages = {333-376},
year = {2021},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-03-2021-0036},
url = {https://www.sciencedirect.com/science/article/pii/S1744008421000239},
author = {Yudith Cardinale and Maria Alejandra Cornejo-Lupa and Alexander {Pinto-De la Gala} and Regina Ticona-Herrera},
keywords = {Ontologies, SLAM, Semantic web, Cultural heritage, Evaluation of ontologies},
abstract = {Purpose
This study aims to the OQuaRE quality model to the developed methodology.
Design/methodology/approach
Ontologies are formal, well-defined and flexible representations of knowledge related to a specific domain. They provide the base to develop efficient and interoperable solutions. Hence, a proliferation of ontologies in many domains is unleashed. Then, it is necessary to define how to compare such ontologies to decide which one is the most suitable for the specific needs of users/developers. As the emerging development of ontologies, several studies have proposed criteria to evaluate them.
Findings
In a previous study, the authors propose a methodological process to qualitatively and quantitatively compare ontologies at Lexical, Structural and Domain Knowledge levels, considering correctness and quality perspectives. As the evaluation methods of the proposal are based on a golden-standard, it can be customized to compare ontologies in any domain.
Practical implications
To show the suitability of the proposal, the authors apply the methodological approach to conduct comparative studies of ontologies in two different domains, one in the robotic area, in particular for the simultaneous localization and mapping (SLAM) problem; and the other one, in the cultural heritage domain. With these cases of study, the authors demonstrate that with this methodological comparative process, we are able to identify the strengths and weaknesses of ontologies, as well as the gaps still needed to fill in the target domains.
Originality/value
Using these metrics and the quality model from OQuaRE, the authors are incorporating a standard of software engineering at the quality validation into the Semantic Web.}
}
@article{WANG2020100780,
title = {A stratified system of knowledge and knowledge icebergs in cross-cultural business models: Synthesising ontological and epistemological views},
journal = {Journal of International Management},
volume = {26},
number = {4},
pages = {100780},
year = {2020},
issn = {1075-4253},
doi = {https://doi.org/10.1016/j.intman.2020.100780},
url = {https://www.sciencedirect.com/science/article/pii/S1075425320302209},
author = {Shouyang Wang and Tachia Chin},
keywords = {Business model, Cross-cultural knowledge, Ecosystem, Ontology, Epistemology, Knowledge iceberg},
abstract = {Because the lack of consensus on defining knowledge, coupled with its associated knowledge iceberg phenomenon, is a key barrier to effectively managing dispersed knowledge in the Internet-driven cross-cultural business model (CBM), we synthesised an ontological and epistemological understanding with the view of existential phenomenology, proposing a “dynamic hierarchical system of knowledge” and three primary knowledge iceberg archetypes as metaphors of cognitive variances in this context. Theoretically, this integrative perspective enriches the philosophical grounds of knowledge by transcending individual subjectivity to achieve a universal understanding of the objectivity of knowledge, thus contributing to the literature at the intersection of international business and knowledge management domains. It also responds to the calls for addressing larger, urgent problems by associating the social phenomena of reality to theoretical development. From a practical standpoint, this research is instrumental in enabling international leaders and managers to identify the cultural impediments to fulfilling their knowledge management objectives in CBMs.}
}
@article{SHI2025114307,
title = {LLM-powered explanations: Unraveling recommendations through subgraph reasoning},
journal = {Knowledge-Based Systems},
volume = {329},
pages = {114307},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114307},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125013486},
author = {Guangsi Shi and Xiaofeng Deng and Linhao Luo and Lijuan Xia and Lei Bao and Bei Ye and Fei Du and Shirui Pan and Yuxiao Li},
keywords = {Explainable recommendation, Large language model, Knowledge graph},
abstract = {Recommendation systems (RecSys) are pivotal in enhancing user experiences across various web applications by analyzing the complicated relationships between users and items. An explainable RecSys is crucial for the product development and subsequent decision-making. Knowledge graphs (KGs) have been widely used to enhance the performance of RecSys. However, KGs are known to be noisy and incomplete, making it hard to provide reliable explanations for recommendation results. We introduce a novel recommender that synergies Large Language Models (LLMs) and KGs to enhance the recommendation and provide interpretable results. We first harness the power of LLMs to augment KG reconstruction, where LLMs analyze and extract information from user reviews to generate new triples. In this way, we can enrich KGs with explainable paths that express user preferences. In addition, we introduce a novel subgraph reasoning module that effectively measures the importance of nodes and discovers reasoning for recommendation. Finally, these reasoning paths are fed into the LLMs to generate interpretable explanations of the recommendation results. Our approach significantly enhances both the effectiveness and interpretability of RecSys, especially in cross-selling scenarios where traditional methods falter. The effectiveness of our approach has been rigorously tested on four open real-world datasets, with our methods demonstrating a superior performance over contemporary state-of-the-art techniques by an average improvement of 12 %. The application of our model in a cross-selling RecSys for a multinational engineering and technology company further underscores its practical utility and potential to redefine recommendation practices through improved accuracy and user trust.}
}
@article{BUNNELL2021113843,
title = {Development of a consumer financial goals ontology for use with FinTech applications for improving financial capability},
journal = {Expert Systems with Applications},
volume = {165},
pages = {113843},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113843},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420306527},
author = {Lawrence Bunnell and Kweku-Muata Osei-Bryson and Victoria Y. Yoon},
keywords = {Financial goals ontology, Ontology engineering, Ontology acquisition, Ontology evaluation, Consumer financial goals, Financial capability},
abstract = {In this research, we communicate the design and evaluation of a consumer financial goals ontology to be utilized as a knowledgebase within recommender systems applications designed to provide decision support in the domain of financial planning. The goal of this research is to provide a domain conceptualization and knowledge classification of a comprehensive set of financial capability enhancing objectives contextually appropriate for a wide range of socio-economically situated consumers. Currently, to the best of our knowledge, within the domain of consumer financial planning no formal conceptual model or knowledge classification of financial goals exists which might be utilized as a knowledgebase for applications such as a FinTech recommender system. Achieving financial goals is a key behavior associated with consumer financial capability, a topic of national economic importance. A holistic representation of domain concepts is critical for advancement of solutions to problems pertinent to a domain. One primary reason for the dearth of applications designed to assist consumers with financial goal setting is the absence of a common domain ontology of financial goals. This study addresses a gap in the literature by contributing to the research knowledgebase an ontology for a domain of consumer financial goals. In doing so, it advances scholarly research through novel domain knowledge classification while providing researchers and practitioners with an ontological knowledgebase for indexing and retrieval within applications designed to improve consumer financial capability through identification and recommendation of specific, context-aware financial goals. The ontology could be used, for example, as a knowledgebase for a Personal Financial Recommender System (PFRS), or other financial technology (FinTech) application, designed to assist users with identification, setting, and tracking of financial goals.}
}
@article{GOTTS2019100728,
title = {Agent-based modelling of socio-ecological systems: Models, projects and ontologies},
journal = {Ecological Complexity},
volume = {40},
pages = {100728},
year = {2019},
note = {Agent-based modelling to study resilience in socio-ecological systems},
issn = {1476-945X},
doi = {https://doi.org/10.1016/j.ecocom.2018.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1476945X18301272},
author = {Nicholas M. Gotts and George A.K. {van Voorn} and J. Gareth Polhill and Eline de Jong and Bruce Edmonds and Gert Jan Hofstede and Ruth Meyer},
keywords = {Socio-ecological system, Agent-based model, Complexity, Ontology},
abstract = {Socio-Ecological Systems (SESs) are the systems in which our everyday lives are embedded, so understanding them is important. The complex properties of such systems make modelling an indispensable tool for their description and analysis. Human actors play a pivotal role in SESs, but their interactions with each other and their environment are often underrepresented in SES modelling. We argue that more attention should be given to social aspects in models of SESs, but this entails additional kinds of complexity. Modelling choices need to be as transparent as possible, and to be based on analysis of the purposes and limitations of modelling. We recommend thinking in terms of modelling projects rather than single models. Such a project may involve multiple models adopting different modelling methods. We argue that agent-based models (ABMs) are an essential tool in an SES modelling project, but their expressivity, which is their major advantage, also produces problems with model transparency and validation. We propose the use of formal ontologies to make the structure and meaning of models as explicit as possible, facilitating model design, implementation, assessment, comparison and extension.}
}
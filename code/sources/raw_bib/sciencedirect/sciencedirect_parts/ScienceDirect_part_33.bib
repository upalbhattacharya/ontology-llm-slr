@article{ZHU2023108330,
title = {Automated extraction of domain knowledge in the dairy industry},
journal = {Computers and Electronics in Agriculture},
volume = {214},
pages = {108330},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.108330},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923007184},
author = {Junsheng Zhu and René Lacroix and Kevin M. Wade},
keywords = {Knowledge graph, Deep learning, Knowledge base, Transition cows, Question answering},
abstract = {Three weeks prior to calving to three weeks after calving, the transition period poses challenges for dairy cattle and farmers. Vast changes in housing, feeding, and reproduction might result in milk drop, metabolic and reproductive diseases. Moreover, most of the metabolic processes are intricately linked as many conditions can coexist. This challenge means that dairy producers and their advisors have difficulty drawing concise conclusions because of all aspects and relationships in transition cow management. Herein, machine-learning techniques and knowledge-graph theory were explored with a view to creating a decision-support system that could provide producers and their advisors with knowledge from domain literature. Specifically, knowledge is modelled as entities and relationships in knowledge graph theory, and natural language models were developed to extract information as knowledge graphs. A dataset comprising 1152 sentences from 20 papers was created and split into 922 sentences for training and 230 sentences for testing. Sequentially, two deep learning models were trained to extract entities and relationships respectively. For training results, a Bi-directional Long-Short-Term Memory model was applied for the entity extraction task and obtained an F1 score of 80 %. As for relationship extraction, a Transformer-based model was deployed but yielded a low F1 of 23 %, thus another pre-trained Transformer model with 89 % accuracy was deployed into the system. After feeding the domain literature into the deep-learning models, a knowledge graph of 1,576 nodes and 3,456 edges was constructed and stored in the graph database Neo4j. Afterward, a semantic parsing method was used to allow users to conduct question answering through the knowledge graph in natural language. In addition, to determine the quality of answers that the knowledge built from the papers, answers were sampled and evaluated based on human judgment. On average, answers scored 7.5 out of 10 and proved informative with respect to the original literature. Although the final interactive results demonstrated a high degree of visualization and scalability, this study primarily sought to demonstrate its feasibility. For tailored commercial applications, further improvements could be implemented in knowledge graph expansion and reasoning.}
}
@incollection{TEKINERDOGAN20211,
title = {Chapter 1 - Introduction},
editor = {Bedir Tekinerdogan and Dominique Blouin and Hans Vangheluwe and Miguel Goulão and Paulo Carreira and Vasco Amaral},
booktitle = {Multi-Paradigm Modelling Approaches for Cyber-Physical Systems},
publisher = {Academic Press},
pages = {1-6},
year = {2021},
isbn = {978-0-12-819105-7},
doi = {https://doi.org/10.1016/B978-0-12-819105-7.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128191057000064},
author = {Bedir Tekinerdogan and Dominique Blouin and Hans Vangheluwe and Miguel Goulão and Paulo Carreira and Vasco Amaral},
keywords = {Multi-paradigm modelling, cyber-physical systems, ontology, case studies},
abstract = {This introductory chapter provides the context and the motivation for multi-paradigm modeling for cyber-physical systems (MPM4CPS). Three basic parts are presented, including an ontological framework for MPM4CPS, methods and tools, and case studies.}
}
@incollection{CHIARA202519,
title = {Text Mining Basics in Bioinformatics},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {19-32},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00219-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002190},
author = {Martinis Maria Chiara and Chiara Zucco},
keywords = {Bioinformatics, Information extraction (IE), Machine learning (ML), Named entity recognition (NER), Natural language processing (NLP), Text mining (TM)},
abstract = {Over the last decade, the application of Text Mining techniques has fundamentally transformed the analysis and interpretation of extensive volumes of unstructured textual data within the domain of bioinformatics. The present chapter introduces the core techniques and tools employed in Text Mining, including Named Entity Recognition, Information Extraction, Machine Learning applications, and Natural Language Processing technologies. The principal challenges and limitations of Text Mining, as for instance data heterogeneity and the disambiguation of biological entities. The discussion extends to contemporary applications of Text Mining in drug discovery, genomics, proteomics, and clinical research while identifying emerging trends and potential future advancements. As Text Mining progresses, its synergy with other omics disciplines and advancements in artificial intelligence are anticipated to further refine our understanding of complex biological systems and enhance research efficacy.}
}
@article{CHEN20206,
title = {Essential Elements of Natural Language Processing: What the Radiologist Should Know},
journal = {Academic Radiology},
volume = {27},
number = {1},
pages = {6-12},
year = {2020},
note = {Special Issue: Artificial Intelligence},
issn = {1076-6332},
doi = {https://doi.org/10.1016/j.acra.2019.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S1076633219304179},
author = {Po-Hao Chen},
keywords = {Natural language processing, Artificial intelligence, Text mining, Deep learning, Structured reporting},
abstract = {Natural language is ubiquitous in the workflow of medical imaging. Radiologists create and consume free text in their daily work, some of which can be amenable to enhancements through automatic processing. Recent advancements in deep learning and “artificial intelligence” have had a significant positive impact on natural language processing (NLP). This article discusses the history of how researchers have extracted data and encoded natural language information for analytical processing, starting from NLP's humble origins in hand-curated, linguistic rules. The evolution of medical NLP including vectorization, word embedding, classification, as well as its use in automated speech recognition, are also explored. Finally, the article will discuss the role of machine learning and neural networks in the context of significant, if incremental, improvements in NLP.}
}
@article{YOUNG2019103971,
title = {A systematic review of natural language processing for classification tasks in the field of incident reporting and adverse event analysis},
journal = {International Journal of Medical Informatics},
volume = {132},
pages = {103971},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2019.103971},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619302370},
author = {Ian James Bruce Young and Saturnino Luz and Nazir Lone},
keywords = {Natural language processing, Machine learning, Text classification, Incident reporting, Adverse event analysis, Patient safety},
abstract = {Context
Adverse events in healthcare are often collated in incident reports which contain unstructured free text. Learning from these events may improve patient safety. Natural language processing (NLP) uses computational techniques to interrogate free text, reducing the human workload associated with its analysis. There is growing interest in applying NLP to patient safety, but the evidence in the field has not been summarised and evaluated to date.
Objective
To perform a systematic literature review and narrative synthesis to describe and evaluate NLP methods for classification of incident reports and adverse events in healthcare.
Methods
Data sources included Medline, Embase, The Cochrane Library, CINAHL, MIDIRS, ISI Web of Science, SciELO, Google Scholar, PROSPERO, hand searching of key articles, and OpenGrey. Data items were manually abstracted to a standardised extraction form.
Results
From 428 articles screened for eligibility, 35 met the inclusion criteria of using NLP to perform a classification task on incident reports, or with the aim of detecting adverse events. The majority of studies used free text from incident reporting systems or electronic health records. Models were typically designed to classify by type of incident, type of medication error, or harm severity. A broad range of NLP techniques are demonstrated to perform these classification tasks with favourable performance outcomes. There are methodological challenges in how these results can be interpreted in a broader context.
Conclusion
NLP can generate meaningful information from unstructured data in the specific domain of the classification of incident reports and adverse events. Understanding what or why incidents are occurring is important in adverse event analysis. If NLP enables these insights to be drawn from larger datasets it may improve the learning from adverse events in healthcare.}
}
@article{RAHROOH2024104551,
title = {Towards a framework for interoperability and reproducibility of predictive models},
journal = {Journal of Biomedical Informatics},
volume = {149},
pages = {104551},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104551},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423002721},
author = {Al Rahrooh and Anders O. Garlid and Kelly Bartlett and Warren Coons and Panayiotis Petousis and William Hsu and Alex A.T. Bui},
abstract = {The development and deployment of machine learning (ML) models for biomedical research and healthcare currently lacks standard methodologies. Although tools for model replication are numerous, without a unifying blueprint it remains difficult to scientifically reproduce predictive ML models for any number of reasons (e.g., assumptions regarding data distributions and preprocessing, unclear test metrics, etc.) and ultimately, questions around generalizability and transportability are not readily answered. To facilitate scientific reproducibility, we built upon the Predictive Model Markup Language (PMML) to capture essential information. As a key component of the PREdictive Model Index and Exchange REpository (PREMIERE) platform, we present the Automated Metadata Pipeline (AMP) for conversion of a given predictive ML model into an extended PMML file that autocompletes an ML-based checklist, assessing model elements for interoperability and reproducibility. We demonstrate this pipeline on multiple test cases with three different ML algorithms and health-related datasets, providing a foundation for future predictive model reproducibility, sharing, and comparison.}
}
@article{MKHIZE2023101573,
title = {Reconceptualising the notion of cross-linguistic transfer in multilingual spaces: A Global South perspective from South Africa},
journal = {Language Sciences},
volume = {100},
pages = {101573},
year = {2023},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2023.101573},
url = {https://www.sciencedirect.com/science/article/pii/S0388000123000384},
author = {Dumisile N. Mkhize},
keywords = {Cross-linguistic transfer, Multilingualism, Reconceptualization, Translanguaging},
abstract = {There is a growing consensus among applied linguists from the Global South that orthodox linguistic and applied linguistic paradigms, theoretical frameworks and methodologies do not adequately serve these contexts. As a result, there has been an increase in interest in challenging Global North paradigms, theoretical perspectives and methodologies. In line with these concerns, in this critical reflective paper, I interrogate the notion of cross-linguistic transfer, which remains popular in some studies on language and literacy learning and teaching in linguistically and culturally complex Global South contexts, including the South African context. Using two studies drawn from two complex multilingual South African universities as illustrative cases (Dyers and Antia, 2019; Makalela, 2014), and framing these studies from a decolonial lens and a translanguaging perspective, I show that the concept of cross-linguistic transfer is problematic because it fails to capture a range of the communicative repertoires, both linguistic and non-linguistic, of multilingual students in these universities, and by extension, in similar contexts. I also contend that the notion of “transfer” in cross-linguistic transfer undermines the multidimensional interdependence of communicative resources of multilingual users. Following this critical analysis, I call for the reconceptualization of cross-linguistic transfer, arguing that this conceptual vocabulary is not consistent with the ontological realities and epistemological perspectives of multilingual students in a complex multilingual South African context. I conclude the paper by briefly discussing the implications of the reconceptualization of cross-linguistic transfer for multilingual educational settings and language and literacy research in the geographic African context, in general, and the South African context, in particular.}
}
@article{MAGALDILINHARES2022378,
title = {FeedEfficiencyService: An architecture for the comparison of data from multiple studies related to dairy cattle feed efficiency indices},
journal = {Information Processing in Agriculture},
volume = {9},
number = {3},
pages = {378-396},
year = {2022},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214317321000585},
author = {Heitor {Magaldi Linhares} and Regina Braga and Wagner {Antônio Arbex} and Mariana {Magalhães Campos} and Fernanda Campos and José Maria N. David and Victor Stroele},
keywords = {Dairy Cattle},
abstract = {The increased demand for food worldwide, the reduced land availability for livestock production, the increasing cost of animal feed and the need for mitigating livestock-related greenhouse gas emissions have driven the search for animal feeding systems that proves more efficient. To tackle this problem, we propose the use of computational support to help researchers compare data on feed efficiency, therefore improving economic and environmental gains. As a solution, we present an integrative architecture capable of combining heterogeneous data from multiple experiments related to dairy cattle feed efficiency indices. The proposed architecture, called FeedEfficiencyService, classifies animals according to feed efficiency indices and allows visualizations through ontologies and inference engines. The results obtained from a case study with researchers from the Brazilian Agricultural Research Corporation – Dairy Cattle (EMBRAPA) demonstrate that this architecture is a supporting tool in their daily work routine. The researchers highlighted the importance of the proposed architecture as it allows analyzing animal data, comparing experiments, having reliable data analyses, and standardizing and organizing data from experiments. The novelty of our approach is the use of ontologies and inference engines to enable the discovery of new knowledge and new relationships between data from feed efficiency-related experiments. We store such data, relationships, and analyses of results in an integrated repository. This solution ensures unified access to the processing history and data from diverse experiments, including those conducted at external research centers.}
}
@article{KAGIYAMA2025,
title = {PRIME 2.0: An Update to The Proposed Requirements for Cardiovascular Imaging-Related Machine Learning Evaluation Checklist},
journal = {JACC: Cardiovascular Imaging},
year = {2025},
issn = {1936-878X},
doi = {https://doi.org/10.1016/j.jcmg.2025.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1936878X25004668},
author = {Nobuyuki Kagiyama and Márton Tokodi and Quincy A. Hathaway and Rima Arnaout and Rhodri Davies and Damini Dey and Nicolas Duchateau and Alan G. Fraser and Shinichi Goto and Ankush D. Jamthikar and Carolyn S.P. Lam and Evangelos K. Oikonomou and David Ouyang and Ambarish Pandey and Timothy J. Poterucha and Zahra Raisi-Estabragh and Jordan B. Strom and Qiang Zhang and Naveena Yanamala and Partho P. Sengupta},
keywords = {PRIME 2.0 Checklist, Cardiovascular Imaging, Artificial Intelligence, Model Development, Deep Learning, Large Language Models, Multimodal Generative AI, Clinical Validation, Transparency and Reproducibility},
abstract = {The PRIME 2.0 checklist is an updated, domain-specific framework designed to standardize the development, evaluation, and reporting of artificial intelligence (AI) applications in cardiovascular imaging. This update specifically responds to rapid advances from traditional machine learning to deep learning, large language models, and multimodal generative AI. The updated checklist was developed through a modified Delphi process by an international panel of clinical and technical experts. In contrast to general AI reporting guidelines, it delivers detailed, practical recommendations on all critical aspects of AI research, and builds upon the original seven-domain framework by incorporating cardiovascular imaging-specific complexities such as cardiac motion, imaging artifacts, and inter-observer variability. By promoting transparency and rigor, PRIME 2.0 can serve as a vital resource for researchers, clinicians, peer reviewers, and journal editors working at the forefront of AI in cardiovascular imaging.}
}
@article{SANTAMARIAGARCIA2024194,
title = {Deductive Care Methodology: Describing and testing modes of care research},
journal = {Enfermería Clínica (English Edition)},
volume = {34},
number = {3},
pages = {194-206},
year = {2024},
issn = {2445-1479},
doi = {https://doi.org/10.1016/j.enfcle.2024.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2445147924000250},
author = {José María Santamaría-García and Alexandra González-Aguña and Marta Fernández-Batalla and Sara Herrero-Jaén and María Lourdes Jiménez-Rodríguez and León Atilano González-Sotos},
keywords = {Nursing, Health Information Management, Models, Nursing, Models, Theoretical, Nursing Theory, Enfermería, Gestión de la Información en Salud, Modelos de Enfermería, Modelos Teóricos, Teoría de Enfermería},
abstract = {Objective
Define the modes of procedure of the Deductive Care Methodology (DCM) in the generation of knowledge about person's health care.
Methodology
Design and test of the DCM modes based on three phases: mapping of the DCM, generation of models from this methodology and testing of the models through studies in a clinical context.
Results
The DCM presents five levels of abstraction with three modes broken down to 16 types. The modes are: Philosophical Mode to conceptualize and obtain generalities about reality, Mathematical Mode to operate with generalities, and Physical Mode to operationally verify, validating the results and the predictive capacity of the model. This MDC allows the creation of three models: Knowledge Model about Person Care, an ontology of care, Vulnerability Model about the person and Taxonomic Triangulation Model for knowledge management. All models generate products for computational knowledge management. In addition, the models are applied in teaching and generate research with more than a hundred participations in conferences and journals, of which five impact publications (from 2008 to 2022) classified in the categories of Nursing and Informatics are analysed.
Conclusions
The DCM collects prior knowledge to work with certainties, evidence and applying inferences that do not depend on the number of cases or inductive designs. This research presents a formal structure of the DCM with an interdisciplinary orientation between Health Sciences and Computer Sciences.
Resumen
Objetivo
Definir los modos de procedimiento de la Metodología Deductiva del Cuidado (MDC) en la generación de conocimiento acerca del cuidado de la salud de las personas.
Metodología
Diseño y prueba de los modos de la MDC basado en tres fases: mapeado de la MDC, generación de modelos desde esta metodología y prueba de los modelos a través de estudios en contexto clínico.
Resultados
La MDC presenta cinco niveles de abstracción con tres modos desglosados hasta 16 tipos. Los modos son: Modo Filosófico para conceptualizar y obtener generalidades acerca de la realidad, Modo Matemático para operar con las generalidades y Modo Físico para verificar operacionalmente, validando los resultados y la capacidad predictiva del modelo. Esta MDC permite crear tres modelos: Modelo de Conocimiento sobre el Cuidado de la Persona, una ontología de cuidado, Modelo de Vulnerabilidad sobre la persona y Modelo de Triangulación Taxonómica para gestión de conocimiento. Todos los modelos generan productos para gestión computacional del conocimiento. Además, los modelos se aplican en docencia y generan investigaciones con más de un centenar de participaciones en congresos y revistas, de las cuales se analizan cinco publicaciones de impacto (desde 2008 a 2022) clasificadas en las categorías de Enfermería e Informática.
Conclusiones
La MDC recoge conocimiento previo para trabajar con certezas, evidencias y aplicando inferencias que no dependen del número de casos o diseños inductivos. Esta investigación presenta una estructura formal de la MDC con orientación interdisciplinar entre Ciencias de la Salud y Ciencias de la Computación.}
}
@article{BABAALLA2024452,
title = {Extraction of UML class diagrams using deep learning: Comparative study and critical analysis},
journal = {Procedia Computer Science},
volume = {236},
pages = {452-459},
year = {2024},
note = {International Symposium on Green Technologies and Applications (ISGTA’2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S187705092401069X},
author = {Zakaria Babaalla and Hamza Abdelmalek and Abdeslam Jakimi and Mohamed Oualla},
keywords = {Natural Language Processing, UML, class diagram, machine learning, deep learning},
abstract = {Several approaches and tools have been proposed to facilitate the automatic generation of Unified Modeling Language (UML) class diagrams from natural language specifications, based on advances in Natural Language Processing (NLP). However, these tools suffer from difficulties due to the inherent imprecision and ambiguity commonly found in natural language expressions. In this article, we present an overview and a study of approaches and tools designed to extract UML diagrams from textual requirements using NLP and computational linguistics techniques. Furthermore, we introduce approaches employing deep learning techniques. Next, we provide a descriptive study and comparative analysis of the limitations and contributions of these automatic and semiautomatic tools, as well as solutions for improving the existing state of the art.}
}
@article{VENUGOPAL2021100290,
title = {Looking through glass: Knowledge discovery from materials science literature using natural language processing},
journal = {Patterns},
volume = {2},
number = {7},
pages = {100290},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100290},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921001239},
author = {Vineeth Venugopal and Sourav Sahoo and Mohd Zaki and Manish Agarwal and Nitya Nand Gosvami and N. M. Anoop Krishnan},
keywords = {natural language processing, artificial intelligence, glass science, materials science, knowledge discovery},
abstract = {Summary
Most of the knowledge in materials science literature is in the form of unstructured data such as text and images. Here, we present a framework employing natural language processing, which automates text and image comprehension and precision knowledge extraction from inorganic glasses’ literature. The abstracts are automatically categorized using latent Dirichlet allocation (LDA) to classify and search semantically linked publications. Similarly, a comprehensive summary of images and plots is presented using the caption cluster plot (CCP), providing direct access to images buried in the papers. Finally, we combine the LDA and CCP with chemical elements to present an elemental map, a topical and image-wise distribution of elements occurring in the literature. Overall, the framework presented here can be a generic and powerful tool to extract and disseminate material-specific information on composition–structure–processing–property dataspaces, allowing insights into fundamental problems relevant to the materials science community and accelerated materials discovery.}
}
@article{HEMALATHA202540,
title = {Unveiling Data Fairness Functional Requirements in Big Data Analytics Through Data Mapping and Classification Analysis},
journal = {International Journal of Sensors Wireless Communications and Control},
volume = {15},
number = {1},
pages = {40-57},
year = {2025},
issn = {2210-3279},
doi = {https://doi.org/10.2174/0122103279312138240625052021},
url = {https://www.sciencedirect.com/science/article/pii/S221032792500012X},
author = {Palanimanickam Hemalatha and Jayaraman Lavanya},
keywords = {Big data analytics, entity-relationship mapping, group-based clustering, LEFI, xAI, classification},
abstract = {Aims
In the realm of Big Data Analytics, ensuring the fairness of data-driven decision making processes is imperative. This abstract introduces the Learning Embedded Fairness Interpretation (LEFI) Model, a novel approach designed to uncover and address data fairness functional requirements with an exceptional accuracy rate of 97%. The model harnesses advanced data mapping and classification analysis techniques, employing Explainable-AI (xAI) for transparent insights into fairness within large datasets.
Methods
The LEFI Model excels in navigating diverse datasets by mapping data elements to discern patterns contributing to biases. Through systematic classification analysis, LEFI identifies potential sources of unfairness, achieving an accuracy rate of 97% in discerning and addressing these issues. This high accuracy empowers data analysts and stakeholders with confidence in the model's assessments, facilitating informed and reliable decision-making. Crucially, the LEFI Model's implementation in Python leverages the power of this versatile programming language. The Python implementation seamlessly integrates advanced mapping, classification analysis, and xAI to provide a robust and efficient solution for achieving data fairness in Big Data Analytics.
Results
This implementation ensures accessibility and ease of adoption for organizations aiming to embed fairness into their data-driven processes. The LEFI Model, with its 97% accuracy, exemplifies a comprehensive solution for data fairness in Big Data Analytics. Moreover, by combining advanced technologies and implementing them in Python, LEFI stands as a reliable framework for organizations committed to ethical data usage.
Conclusion
The model not only contributes to the ongoing dialogue on fairness but also sets a new standard for accuracy and transparency in the analytics pipeline, advocating for a more equitable future in the realm of Big Data Analytics.}
}
@article{PSAROMMATIS2022100263,
title = {A hybrid Decision Support System for automating decision making in the event of defects in the era of Zero Defect Manufacturing},
journal = {Journal of Industrial Information Integration},
volume = {26},
pages = {100263},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100263},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000613},
author = {Foivos Psarommatis and Dimitris Kiritsis},
keywords = {Zero Defect Manufacturing, Decision Support, Quality management, Data driven, Knowledge base, Ontology},
abstract = {Defects are unavoidable during manufacturing processes, and a tremendous amount of research aimed at improving defect prevention has been conducted by scholars. Zero Defect Manufacturing (ZDM) seeks to eliminate defects in production. In addition, technological advancements now allow the repair of defective products. This creates the need to re-schedule productions more frequently in order to take into account the actions necessary to fix defective parts. This study focuses on detection and repair-based ZDM strategies. It implements a newly developed, hybrid Decision Support System (DSS) that uses data-driven and knowledge-based approaches to detect defects and then automate the necessary decision-making processes. The system uses an ontology based on the MASON ontology in order to describe the production domain and enrich the available data with contextual information. Real time production data and past knowledge are utilized to analyse defects, identify their type and severity, and suggest alternative repair plans. Possible repair plans are evaluated using a dynamic multi-criteria approach that determines the plan most suited to production conditions at the time of defect detection. To test the efficacy of the DSS developed for this study, it was integrated with a dynamic scheduling tool and was also used in an industrial application in the semiconductor domain. The simulations and the real-world implementation both show that the proposed DSS system can efficiently detect defects and automate the post-detection decision-making process. The multi-criteria approach adopted by this study proves that the DSS can make well-adapted decisions, handle the dynamic nature of a production system, and help manufacturers move closer to Zero Defect Manufacturing.}
}
@article{WANG2024128580,
title = {Zero-shot text classification with knowledge resources under label-fully-unseen setting},
journal = {Neurocomputing},
volume = {610},
pages = {128580},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128580},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224013511},
author = {Yuqi Wang and Wei Wang and Qi Chen and Kaizhu Huang and Anh Nguyen and Suparna De},
keywords = {Zero-shot learning, Knowledge graph embedding, Natural language processing, Textual analysis, Multi-class classification},
abstract = {Classification techniques are at the heart of many real-world applications, e.g. sentiment analysis, recommender systems and automatic text annotation, to process and analyse large-scale textual data in multiple fields. However, the effectiveness of natural language processing models can only be confirmed when a large amount of up-to-date training data is available. An unprecedented amount of data is continuously created, and new topics are introduced, making it less likely or even infeasible to collect labelled samples covering all topics for training models. We attempt to study the extreme case: there is no labelled data for model training, and the model, without being adapted to any specific dataset, will be directly applied to the testing samples. We propose a transformer-based framework to encode sentences in a contextualised way and leverage the existing knowledge resources, i.e. ConceptNet and WordNet, to integrate both descriptive and structural knowledge for better performance. To enhance the robustness of the model, we design an adversarial example generator based on relations from external knowledge bases. The framework is evaluated on both general and specific domain text classification datasets. Results show that the proposed framework can outperform the existing competitive state-of-the-art baselines, delivering new benchmark results.}
}
@article{TRUJILLO2021101911,
title = {Conceptual modeling in the era of Big Data and Artificial Intelligence: Research topics and introduction to the special issue},
journal = {Data & Knowledge Engineering},
volume = {135},
pages = {101911},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101911},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000380},
author = {Juan Trujillo and Karen C. Davis and Xiaoyong Du and Ernesto Damiani and Veda C. Storey},
keywords = {Conceptual modeling, Big Data, Machine learning, Artificial Intelligence},
abstract = {Since the first version of the Entity–Relationship (ER) model proposed by Peter Chen over forty years ago, both the ER model and conceptual modeling activities have been key success factors for modeling computer-based systems. During the last decade, conceptual modeling has been recognized as an important research topic in academia, as well as a necessity for practitioners. However, there are many research challenges for conceptual modeling in contemporary applications such as Big Data, data-intensive applications, decision support systems, e-health applications, and ontologies. In addition, there remain challenges related to the traditional efforts associated with methodologies, tools, and theory development. Recently, novel research is uniting contributions from both the conceptual modeling area and the Artificial Intelligence discipline in two directions. The first one is efforts related to how conceptual modeling can aid in the design of Artificial Intelligence (AI) and Machine Learning (ML) algorithms. The second one is how Artificial Intelligence and Machine Learning can be applied in model-based solutions, such as model-based engineering, to infer and improve the generated models. For the first time in the history of Conceptual Modeling (ER) conferences, we encouraged the submission of papers based on AI and ML solutions in an attempt to highlight research from both communities. In this paper, we present some of important topics in current research in conceptual modeling. We introduce the selected best papers from the 37th International Conference on Conceptual Modeling (ER’18) held in Xi’an, China and summarize some of the valuable contributions made based on the discussions of these papers. We conclude with suggestions for continued research.}
}
@article{AVILAGARZON2022,
title = {An Agent-Based Social Simulation for Citizenship Competences and Conflict Resolution Styles},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.306749},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000357},
author = {Cecilia Avila-Garzon and Manuel Balaguera and Valentina Tabares-Morales},
keywords = {Agent-Based Social Simulation, Citizenship Competences, Conflict Resolution Styles, NetLogo, Ontology},
abstract = {ABSTRACT
The development of citizenship competences plays an important role in a complex system like society. Thus, to analyze how such competences impact other contexts is a great challenge because this kind of study involves the work with people and the use of variables that depend on human behaviors. In this sense, many studies have highlighted the advantage of using simulation systems and tools. In particular, the agent-based social simulation field relies upon the Semantic Web to manage knowledge representation in social scenarios. This study focuses on how citizenship competences impact conflict resolution. Moreover, a simulation model in which citizens interact to resolve conflicts by considering citizenship competences and conflict resolution styles is also introduced. It was developed in NetLogo together with an extension that connects it with the ontology of competences. Results show that the higher interactions of citizens-conflicts, the higher level of citizenship competences, and the number of conflicts solved is higher when using citizenship competences.}
}
@article{ZHANG2025115315,
title = {Typography meets question type: Unveiling their matching effect on willingness to pay for AI products},
journal = {Journal of Business Research},
volume = {192},
pages = {115315},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2025.115315},
url = {https://www.sciencedirect.com/science/article/pii/S0148296325001389},
author = {Yangting Zhang and Jiaming Fang and Miyan Liao and Lintong Han and Chao Wen and Addo Prince Clement},
keywords = {Typography effect, Typeface, AI LLM product, Perceived authority, Perceived friendliness, consumer AI knowledge},
abstract = {This study examines the matching effect of a critical visual element—typeface—and question type in AI large language model products, and their impact on consumers’ willingness to pay (WTP). Drawing on six experiments with 3,634 participants from three culturally diverse countries, the results demonstrate that a machine-like typeface increases WTP for non-conversational questions, while a handwritten-like typeface enhances WTP for conversational questions. For non-conversational questions, the effect is mediated by the perceived authority of the AI, whereas for conversational questions, the effect is mediated by perceived friendliness. Additionally, the study investigates consumers’ AI knowledge as a boundary condition, revealing that the congruence between typeface and question type has a more substantial influence on WTP among users with lower AI knowledge. These findings provide key insights into the interplay between typeface, question type, user perceptions, and WTP in AI product contexts.}
}
@article{TOKMAK2024103794,
title = {Web service discovery: Rationale, challenges, and solution directions},
journal = {Computer Standards & Interfaces},
volume = {88},
pages = {103794},
year = {2024},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2023.103794},
url = {https://www.sciencedirect.com/science/article/pii/S0920548923000752},
author = {Ahmet Vedat Tokmak and Akhan Akbulut and Cagatay Catal},
keywords = {Microservices, Microservice architecture, Service discovery, SLR},
abstract = {Service Oriented Architecture (SOA) is a methodology that promotes cooperation between services with diverse, but connected functions. Web Service technology paved the way for microservice architecture as it is a feature of modern web applications that resulted from the rise of SOA. With the proliferation of self-contained services, the ease of finding has emerged as a critical concern. Due to the increasing number of services that perform identical tasks, it has become difficult for users to select the most feasible service. Providing the most relevant service for the customer quickly is a crucial infrastructure task, and undiscovered services increase ecosystem expenses. Syntactic, semantic-conscious, and ontology-based studies have been presented as ways to improve the effectiveness and quality of service discovery techniques. While there are many approaches that have been proposed and validated for service discovery in literature, these studies are fragmented and there is a lack of overview of the techniques of web service discovery. As such, we conduct a Systematic Literature Review (SLR) study to review the existing body of knowledge surrounding service discovery and discuss the state-of-the-art. We present an overview of the techniques and empirical evidence by identifying, analyzing, and classifying the papers. Among the 764 papers we retrieved, 54 papers were included. We provide a comprehensive analysis of methodologies and tools for discovering web services.}
}
@article{PAPASTRATIS2024112291,
title = {Can ChatGPT provide appropriate meal plans for NCD patients?},
journal = {Nutrition},
volume = {121},
pages = {112291},
year = {2024},
issn = {0899-9007},
doi = {https://doi.org/10.1016/j.nut.2023.112291},
url = {https://www.sciencedirect.com/science/article/pii/S0899900723003192},
author = {Ilias Papastratis and Andreas Stergioulas and Dimitrios Konstantinidis and Petros Daras and Kosmas Dimitropoulos},
keywords = {ChatGPT, Nutrition, Artificial intelligence, Recommendation systems},
abstract = {Objectives
Dietary habits significantly affect health conditions and are closely related to the onset and progression of non-communicable diseases (NCDs). Consequently, a well-balanced diet plays an important role in lessening the effects of various disorders, including NCDs. Several artificial intelligence recommendation systems have been developed to propose healthy and nutritious diets. Most of these systems use expert knowledge and guidelines to provide tailored diets and encourage healthier eating habits. However, new advances in large language models such as ChatGPT, with their ability to produce human-like responses, have led individuals to search for advice in several tasks, including diet recommendations. This study aimed to determine the ability of ChatGPT models to generate appropriate personalized meal plans for patients with obesity, cardiovascular diseases, and type 2 diabetes.
Methods
Using a state-of-the-art knowledge-based recommendation system as a reference, we assessed the meal plans generated by two large language models in terms of energy intake, nutrient accuracy, and meal variability.
Results
Experimental results with different user profiles revealed the potential of ChatGPT models to provide personalized nutritional advice.
Conclusion
Additional supervision and guidance by nutrition experts or knowledge-based systems are required to ensure meal appropriateness for users with NCDs.}
}
@article{GONCALVES201812,
title = {Semantic Guided Interactive Image Retrieval for plant identification},
journal = {Expert Systems with Applications},
volume = {91},
pages = {12-26},
year = {2018},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2017.08.035},
url = {https://www.sciencedirect.com/science/article/pii/S095741741730581X},
author = {Filipe Marcel Fernandes Gonçalves and Ivan Rizzo Guilherme and Daniel Carlos Guimarães Pedronette},
keywords = {Interactive image retrieval, Unsupervised learning, Semantic gap, Ontology},
abstract = {A lot of images are currently generated in many domains, requiring specialized knowledge of identification and analysis. From one standpoint, many advances have been accomplished in the development of image retrieval techniques based on visual image properties. However, the semantic gap between low-level features and high-level concepts still represents a challenging scenario. On another standpoint, knowledge has also been structured in many fields by ontologies. A promising solution for bridging the semantic gap consists in combining the information from low-level features with semantic knowledge. This work proposes a novel graph-based approach denominated Semantic Interactive Image Retrieval (SIIR) capable of combining Content Based Image Retrieval (CBIR), unsupervised learning, ontology techniques and interactive retrieval. To the best of our knowledge, there is no approach in the literature that combines those diverse techniques like SIIR. The proposed approach supports expert identification tasks, such as the biologist’s role in plant identification of Angiosperm families. Since the system exploits information from different sources as visual content, ontology, and user interactions, the user efforts required are drastically reduced. For the semantic model, we developed a domain ontology which represents the plant properties and structures, relating features from Angiosperm families. A novel graph-based approach is proposed for combining the semantic information and the visual retrieval results. A bipartite and a discriminative attribute graph allow a semantic selection of the most discriminative attributes for plant identification tasks. The selected attributes are used for formulating a question to the user. The system updates similarity information among images based on the user’s answer, thus improving the retrieval effectiveness and reducing the user’s efforts required for identification tasks. The proposed method was evaluated on the popular Oxford Flowers 17 and 102 Classes datasets, yielding highly effective results in both datasets when compared to other approaches. For example, the first five retrieved images for 17 classes achieve a retrieval precision of 97.07% and for 102 classes, 91.33%.}
}
@article{KWOK2020101286,
title = {Postcolonial translation theories and the language myth: an integrationist perspective},
journal = {Language Sciences},
volume = {80},
pages = {101286},
year = {2020},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2020.101286},
url = {https://www.sciencedirect.com/science/article/pii/S0388000120300188},
author = {Sinead Kwok},
keywords = {Integrationism, Translation studies, Postcolonialism, Poststructuralism, Literalism, },
abstract = {These few decades have witnessed a surge of postcolonial translation movements which put the Western transference ideal, one that sees translation as transference of meaning between two languages, to the test. Two of such movements are postcolonial literalism and Antropofagia, which both redirect significance from the meaning to the form and unsettle the colonial notion of ‘originality’ in translation. This paper introduces a semiological viewpoint on these two movements, evaluating the similarities and incongruities between a postcolonial re-theorization of translation and an integrationist theoretical account of translation. It is argued that while postcolonial translation theories show some tendencies to break away from ‘the language myth’ inherent in colonial translation i.e. a supposition of pre-given language codes for translation to take place between (Harris, 2011a), demythologization is never the postcolonialists' primary concern nor achievement, who still ultimately rely on the language myth and its entailments to serve political ends.}
}
@article{SWIFT2024106076,
title = {A theory building critical realist evaluation of an integrated cognitive-behavioural fluency enhancing stuttering treatment for school-age children. Part 1: Development of a preliminary program theory from expert speech-language pathologist data.},
journal = {Journal of Fluency Disorders},
volume = {82},
pages = {106076},
year = {2024},
issn = {0094-730X},
doi = {https://doi.org/10.1016/j.jfludis.2024.106076},
url = {https://www.sciencedirect.com/science/article/pii/S0094730X24000408},
author = {Michelle C. Swift and Marilyn Langevin},
keywords = {Critical realist evaluation, Evidence-based practice, Stuttering, Integrated stuttering treatment, School-aged children},
abstract = {Purpose
This study initiated a program of research that aims to develop a program theory underlying integrated cognitive-behavioural fluency enhancing stuttering treatments for school-age children. This research asks, what in the treatment program works (or does not work), for whom, in what contexts, and why.
Methods
Using a critical realist evaluation approach, seven speech-language pathologists (SLPs) with extensive experience in treating children who stutter were asked about barriers and facilitators of optimal treatment outcomes within the context of the Comprehensive Stuttering Program - School-aged Children (CSP-SC). From these data discrete resource mechanisms, contexts, within child reasoning mechanisms, and outcomes were derived and a preliminary program theory was proposed.
Results
Facilitating and impeding child physiology, treatment and SLP resource mechanisms, family and school contexts, and within-child mechanisms were identified. Facilitating mechanisms included motivation, personality/psychological characteristics, understanding and trust of the treatment process, experience of speaking with less effort, and self-efficacy. Impeding mechanisms included reduced motivation, impeding personality/psychological characteristics, lack of buy-in, and, for some children, a prohibitive cost of effort in using learned strategies.
Conclusion
A preliminary program theory was hypothesized which will be further developed in future analysis of data obtained from children and parents who participated in the CSP-SC at the same centre from which the SLPs came. Subsequent research with new cohorts of SLPs, children, and parents from other treatment programs and centres will be needed to establish the generalizability of the program theory generated in this program of research}
}
@article{TERKAJ201988,
title = {A digital factory platform for the design of roll shop plants},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {26},
pages = {88-93},
year = {2019},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1755581719300264},
author = {W. Terkaj and P. Gaboardi and C. Trevisan and T. Tolio and M. Urgo},
keywords = {Digital factory, Virtual factory, Discrete event simulation, Ontology},
abstract = {Replying to requests for quotation in a fast and effective way is a key need for technology providers, in particular machine tool builders and system integrators. Digital factory technologies provide the opportunity of speeding up the generation of technical offers through the development of a digital twin of the system under study, thus enabling the assessment of different candidate configurations and the associated performance. In this paper we present a set of integrated digital tools to support the design of roll shop plants, i.e. plants dedicated to grinding cylinders for rolling mills. These digital tools are aimed to engineers and provide a configuration workflow, a 3D environment and performance evaluation tools. The interoperability among the software modules and the reuse of knowledge is enhanced by semantic web technologies and the definition of a common data model as an ontology relying on technical standards.}
}
@article{MARTINS2025105841,
title = {SUDEP risk is influenced by longevity genomics: a polygenic risk score study},
journal = {eBioMedicine},
volume = {118},
pages = {105841},
year = {2025},
issn = {2352-3964},
doi = {https://doi.org/10.1016/j.ebiom.2025.105841},
url = {https://www.sciencedirect.com/science/article/pii/S2352396425002853},
author = {Helena Martins and James D. Mills and Susanna Pagni and Medine I. Gulcebi and Angeliki Vakrinou and Patrick B. Moloney and Lisa M. Clayton and Ravishankara Bellampalli and Hannah Stamberger and Sarah Weckhuysen and Pasquale Striano and Federico Zara and Richard D. Bagnall and Rebekah V. Harris and Kate M. Lawrence and Lynette G. Sadleir and Douglas E. Crompton and Daniel Friedman and Juliana Laze and Ling Li and Samuel F. Berkovic and Christopher Semsarian and Ingrid E. Scheffer and Orrin Devinsky and Karoline Kuchenbaecker and Simona Balestrini and Sanjay M. Sisodiya},
keywords = {Epilepsy, Death, Risk, Longevity, Intelligence},
abstract = {Summary
Background
Sudden Unexpected Death in Epilepsy (SUDEP) is a rare and tragic outcome in epilepsy, identified by those with the condition as their most serious concern. Although several clinical factors are associated with elevated SUDEP risk, mechanisms underlying SUDEP are poorly understood, making individual risk prediction challenging, especially early in the disease course. We hypothesised that common genetic variation contributes to SUDEP risk.
Methods
Genetic data from people who had succumbed to SUDEP was compared to data from people with epilepsy who had not succumbed to SUDEP and from healthy controls. Polygenic risk scores (PRSs) for longevity, intelligence and epilepsy were compared across cohorts. Reactome pathways and gene ontology terms implicated by the contributing single nucleotide polymorphisms (SNPs) were explored. In the subset of SUDEP cases with the necessary data available, a risk score was calculated using an existing risk prediction tool (SUDEP-3); the added value to this prediction of SNP-based genomic information was evaluated.
Findings
Only European-ancestry participants were included. 161 SUDEP cases were compared to 768 cases with epilepsy and 1153 healthy controls. PRS for longevity was significantly reduced in SUDEP cases compared to disease (P = 0·0096) and healthy controls (P = 0·0016), as was PRS for intelligence (SUDEP cases compared to disease (P = 0·0073) and healthy controls (P = 0·00024)). The PRS for epilepsy did not differ between SUDEP cases and disease controls (P = 0·76). SNP-determined pathway and gene ontology analysis highlighted those related to inter-neuronal communication as amongst the most enriched in SUDEP. Addition of PRS for longevity and intelligence to SUDEP-3 scores improved risk prediction in a subset of cases (38) and controls (703), raising the area-under-the-curve in a receiver-operator characteristic from 0·699 using SUDEP-3 alone to 0·913 when PRSs were added.
Interpretation
Common genetic variation contributes to SUDEP risk, offering new approaches to improve risk prediction and to understand underlying mechanisms.
Funding
The Amelia Roberts Fund; CURE Epilepsy; Epilepsy Society, UK; Finding A Cure for Epilepsy and Seizures (FACES).}
}
@article{ZHOU2019,
title = {Adapting State-of-the-Art Deep Language Models to Clinical Information Extraction Systems: Potentials, Challenges, and Solutions},
journal = {JMIR Medical Informatics},
volume = {7},
number = {2},
year = {2019},
issn = {2291-9694},
doi = {https://doi.org/10.2196/11499},
url = {https://www.sciencedirect.com/science/article/pii/S2291969419000528},
author = {Liyuan Zhou and Hanna Suominen and Tom Gedeon},
keywords = {computer systems, artificial intelligence, deep learning, information storage and retrieval, medical informatics, nursing records, patient handoff},
abstract = {Background
Deep learning (DL) has been widely used to solve problems with success in speech recognition, visual object recognition, and object detection for drug discovery and genomics. Natural language processing has achieved noticeable progress in artificial intelligence. This gives an opportunity to improve on the accuracy and human-computer interaction of clinical informatics. However, due to difference of vocabularies and context between a clinical environment and generic English, transplanting language models directly from up-to-date methods to real-world health care settings is not always satisfactory. Moreover, the legal restriction on using privacy-sensitive patient records hinders the progress in applying machine learning (ML) to clinical language processing.
Objective
The aim of this study was to investigate 2 ways to adapt state-of-the-art language models to extracting patient information from free-form clinical narratives to populate a handover form at a nursing shift change automatically for proofing and revising by hand: first, by using domain-specific word representations and second, by using transfer learning models to adapt knowledge from general to clinical English. We have described the practical problem, composed it as an ML task known as information extraction, proposed methods for solving the task, and evaluated their performance.
Methods
First, word representations trained from different domains served as the input of a DL system for information extraction. Second, the transfer learning model was applied as a way to adapt the knowledge learned from general text sources to the task domain. The goal was to gain improvements in the extraction performance, especially for the classes that were topically related but did not have a sufficient amount of model solutions available for ML directly from the target domain. A total of 3 independent datasets were generated for this task, and they were used as the training (101 patient reports), validation (100 patient reports), and test (100 patient reports) sets in our experiments.
Results
Our system is now the state-of-the-art in this task. Domain-specific word representations improved the macroaveraged F1 by 3.4%. Transferring the knowledge from general English corpora to the task-specific domain contributed a further 7.1% improvement. The best performance in populating the handover form with 37 headings was the macroaveraged F1 of 41.6% and F1 of 81.1% for filtering out irrelevant information. Performance differences between this system and its baseline were statistically significant (P<.001; Wilcoxon test).
Conclusions
To our knowledge, our study is the first attempt to transfer models from general deep models to specific tasks in health care and gain a significant improvement. As transfer learning shows its advantage over other methods, especially on classes with a limited amount of training data, less experts’ time is needed to annotate data for ML, which may enable good results even in resource-poor domains.}
}
@article{MEJIAGARCIA2024e24382,
title = {RNA-seq analysis reveals modulation of inflammatory pathways by an enriched-triterpene natural extract in mouse and human macrophage cell lines},
journal = {Heliyon},
volume = {10},
number = {2},
pages = {e24382},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e24382},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024004134},
author = {Alejandro Mejia-Garcia and Geysson Javier Fernandez and Luis Fernando Echeverri and Norman Balcazar and Sergio Acin},
keywords = {Natural extract, Triterpenes, Immune system, Transcriptome, Inflammation, Macrophage},
abstract = {Chronic inflammation is crucial in developing insulin resistance and type 2 diabetes. Previous studies have shown that a leaf extract of Eucalyptus tereticornis, with ursolic acid (UA), oleanolic acid (OA), and ursolic acid lactone (UAL) as the main molecules (78 %) mixed with unknown minor metabolites (22 %), provided superior anti-inflammatory, hypoglycemic, and hypolipidemic effects than reconstituted triterpenoid mixtures in macrophage cell lines and a pre-diabetic mouse model. Further identification of the molecular mechanisms of action of this mixture of triterpenes is required. This study aims to analyse the RNA expression profiles of mouse and human macrophage cell lines treated with the natural extract and its components. Activated macrophage cell lines were treated with the natural extract, UA, OA, UAL or a triterpene mixture (M1). RNA was extracted and sequenced using the DNBseq platform and the EnrichR software to perform gene enrichment analysis using the Gene Ontology database, Kyoto Encyclopedia of Genes and Genomes, and Reactome. To conduct clustering analysis, we standardised the normalised counts of each gene and applied k-means clustering. The combination of molecules in the natural extract has an additive or synergic effect that affects the expression of up-regulated genes by macrophage activation. Triterpenes (M1) regulated 76 % of human and 68 % of mouse genes, while uncharacterised minority molecules could regulate 24 % of human and 32 % of mouse genes. The extract inhibited the expression of many cytokines (IL6, IL1, OSM), chemokines (CXCL3), inflammatory mediators (MMP8 and MMP13) and the JAK-STAT signalling pathway in both models. The natural extract has a more powerful immunomodulatory effect than the triterpene mixture, increasing the number of genes regulated in mouse and human models. Our study shows that Eucalyptus tereticornis extract is a promising option for breaking the link between inflammation and insulin resistance.}
}
@article{RUBI2022108200,
title = {Forestry 4.0 and Industry 4.0: Use case on wildfire behavior predictions},
journal = {Computers and Electrical Engineering},
volume = {102},
pages = {108200},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108200},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622004414},
author = {Jesús N.S. Rubí and Paulo H.P. {de Carvalho} and Paulo R.L. Gondim},
keywords = {Industry 4.0, Forestry 4.0, Semantic, Platform, IoFT, Ontology, Wildfires, Machine Learning},
abstract = {Forest industries deserve special attention due to relations between environmental impact and social and economic development. The increase of forest fires caused by the untenable exploitation has motivated the application of concepts such as Industry/Forestry 4.0 and Internet of Forest Things (IoFT) towards improving the performance of current supply chains and assuming an environmental responsibility. This research focuses on the application of IoFT for the prediction of wildfires behavior and proposes a semantic platform for heterogeneous IoFT data aggregation that grants interoperability through semantic technologies. The dataset considered climatic- and vegetation-related data gathered by Brazilian government sensors and satellite information on fires, and Machine Learning predicted the areas affected after a fire event. Both platform and predictions were validated and Random Forest predicted the area with 89% accuracy, showing better performance than Deep Neural Network, with 79%.}
}
@article{XU2022107595,
title = {Extend auction description language to represent and reason knowledge in auctions},
journal = {Computers & Electrical Engineering},
volume = {97},
pages = {107595},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107595},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621005310},
author = {Zhenlei Xu and Junwu Zhu},
keywords = {Game description language, Auction, Strategic reasoning, General game playing},
abstract = {Current auction description only can represent and reason basic auction rules in one specific scene, while player’s strategies are usually dismissed, besides, most of description languages lack of general and reasoning ability to describe the auction process. Facing these problems, the Auction Description Language with Extension (ADLE) is proposed by using epistemic strategy to represent and reason more domain knowledge in various auctions, and extend description language with epistemic operators and state based semantics, thus players’ dynamic strategies are used to describe auction process. Besides, ADLE is proved to meet important economic properties, and more domain knowledge of different auctions, various strategy description can be well described through experiment analysis and setting examples. Finally, through the conducted experiment, auction process time under ADLE is no less than other auction description methods, shows that ADLE used for describing various auctions is effective and feasible.}
}
@article{MAHBUB2022100888,
title = {Identifying molecular signatures and pathways shared between Alzheimer's and Huntington's disorders: A bioinformatics and systems biology approach},
journal = {Informatics in Medicine Unlocked},
volume = {30},
pages = {100888},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.100888},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822000405},
author = {Nosin Ibna Mahbub and Md. Imran Hasan and Md Habibur Rahman and Feroza Naznin and Md Zahidul Islam and Mohammad Ali Moni},
keywords = {Alzheimer's disease, Huntington's disease, Biomarker signatures, Differentially expressed genes, Protein-protein interaction, Protein-drug interactions},
abstract = {Alzheimer's disease and Huntington's disease are considered to be the most lethal illnesses that result in common human disorders. Alzheimer's disease (AD) is a progressive neurological illness distinguished by age-related dementia, mental abnormalities, and poor memory, among other symptoms. However, Huntington's disease (HD) is influenced by genetics as well as a generalized dysfunction of the motor system. Despite the fact that many similar genetic elements have been found in the literature as being interrelated between these two diseases, it is still unclear how people acquire infected with these two neurological disorders. Detecting biomarkers for Alzheimer's and Huntington's disease in brain tissue might help in drug development and treatment. The purpose of this research was to find brain cell transcripts that show levels of gene expression linked to the progression of Alzheimer's and Huntington's disease. A bioinformatics pipeline was used to study one RNA-Seq transcriptomic dataset and one microarray dataset, and 24 significant differentially expressed genes (DEGs) were discovered that were shared by two brain cell datasets. We uncovered disease-gene association networks and signaling pathways, as well as gene ontology (GO) investigations and hub protein identification, to determine the roles of these DEGs. The discovery of significant gene ontologies and molecular pathways increased our understanding of the pathophysiology of these two disorders, and the hub proteins B2M, HLA-A, HLA-E, HLA-B, HLA-C, HLA-F, CANX, HLA-DQA1, HLA-DRA, and HLA-DRB1 might be exploited to design therapeutic interventions. In neurological disorder subjects, we uncovered efficient hypothetical linkages between pathogenic processes in brain cells, implying that brain cells may be exploited to detect and monitor illness origin and development, as well as design pharmacological therapies.}
}
@incollection{INBAMANI202219,
title = {Chapter 2 - Role of IoT and semantics in e-Health},
editor = {Sanju Tiwari and Fernando {Ortiz Rodriguez} and M.A. Jabbar},
booktitle = {Semantic Models in IoT and eHealth Applications},
publisher = {Academic Press},
pages = {19-37},
year = {2022},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-323-91773-5},
doi = {https://doi.org/10.1016/B978-0-32-391773-5.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032391773500008X},
author = {Abinaya Inbamani and A. Siva Sakthi and R.R. Rubia Gandhi and M. Preethi and R. Rajalakshmi and Veerapandi Veerasamy and Thirumeni Mariammal},
keywords = {IoT, Middleware, MOM, SOA},
abstract = {IoT in healthcare is a boon to human beings wherein prevention of disease and maintenance of health is ensured. The quality of decision-making has improved nowadays, by bringing AI and big data in e-Health along with the Internet of Things (IoT). The heterogeneity of the things needs to ensure to incorporate an interoperable environment. The healthcare solutions given should be acceptable globally and standardized, too. The middleware helps in providing such kind of infrastructure environment. The IoT sensor network is connected to the middleware, which is then used for the real time applications like smart home, e-Health, healthcare, etc. The advantages of service oriented architecture are numerous and help in providing a scalable and user-friendly environment. As the number of sensors are increasing, the data rate has also become huge. Hence, efficient modeling and semantic observations sent to the network should be considered. There are a lot of data models available with respect to applications like Web Service Definition Languages. Various features like abstraction, smart reasoning, and security needs to be considered to ensure a sustainable environment. Descriptive features and design principles are playing a major role in IoT ontology. The chapter gives an overview of the IoT, various middleware techniques, and role of semantics in e-Health.}
}
@article{LONGO2024102301,
title = {Explainable Artificial Intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions},
journal = {Information Fusion},
volume = {106},
pages = {102301},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102301},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524000794},
author = {Luca Longo and Mario Brcic and Federico Cabitza and Jaesik Choi and Roberto Confalonieri and Javier Del Ser and Riccardo Guidotti and Yoichi Hayashi and Francisco Herrera and Andreas Holzinger and Richard Jiang and Hassan Khosravi and Freddy Lecue and Gianclaudio Malgieri and Andrés Páez and Wojciech Samek and Johannes Schneider and Timo Speith and Simone Stumpf},
keywords = {Explainable artificial intelligence, XAI, Interpretability, Manifesto, Open challenges, Interdisciplinarity, Ethical AI, Large language models, Trustworthy AI, Responsible AI, Generative AI, Multi-faceted explanations, Concept-based explanations, Causality, Actionable XAI, Falsifiability},
abstract = {Understanding black box models has become paramount as systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper highlights the advancements in XAI and its application in real-world scenarios and addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. We aim to develop a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 28 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.}
}
@article{HURTADO2024111230,
title = {e-Science workflow: A semantic approach for airborne pollen prediction},
journal = {Knowledge-Based Systems},
volume = {284},
pages = {111230},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.111230},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123009802},
author = {Sandro Hurtado and María Luisa Antequera-Gómez and Cristóbal Barba-González and Antonio Picornell and Ismael Navas-Delgado},
keywords = {Big data analytics, Semantics, e-Science, Pollen prediction},
abstract = {Allergic rhinitis has become a global health problem in recent decades because airborne pollen is a primary trigger of this respiratory disorder. Moreover, pollinosis can exacerbate the symptoms of asthma and favour respiratory infections. Seasonal pollen trends and climatic circumstances (such as temperature, precipitation, relative humidity, wind speed and direction, and other variables) can impact daily airborne pollen concentrations, influencing local pollen emission and dispersion. Because of that, pollen monitoring and prediction are becoming more relevant to the urban population and scientific interest is put into them. Due to such tasks’ high volume of data, scientists are starting to use computational tools like workflows to automate and speed up the process. Furthermore, using the expert scientific domain is critical for improving the analysis, allowing, among others, a better workflow configuration and data provenance. As semantic web technologies have been revealed as an essential means for knowledge representation, we implemented this workflow information as an ontology using formats like RDF(S) and OWL. Consequently, this paper provides a semantic-enhanced e-Science workflow based on the TITAN framework for pollen forecasting analysis using meteorological data. Furthermore, a catalogue of components is developed on the TITAN framework, which allows the creation of different workflow versions. Two case studies of pollen prediction were developed to test the implementation of the aforementioned methodologies. Both were elaborated with airborne pollen data obtained in the city of Málaga (Spain). Still, one was elaborated for Platanus pollen type (narrow annual main pollination period), while the other was done for Amaranthaceae pollen type (extensive annual main pollination period). The predictions have been conducted using machine and deep learning algorithms like SARIMA or CNN-LSTM that intend to optimise the pollen prediction procedure depending on its stational and seasonal profile.}
}
@incollection{YADEN2023xix,
title = {Volume 10: Literacies and Languages Education},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {xix-xxiv},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.02010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305020108},
author = {David Yaden and Theresa Rogers}
}
@article{SIKSTROM2024105140,
title = {Pedagogical agents communicating and scaffolding students' learning: High school teachers' and students' perspectives},
journal = {Computers & Education},
volume = {222},
pages = {105140},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105140},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524001544},
author = {Pieta Sikström and Chiara Valentini and Anu Sivunen and Tommi Kärkkäinen},
keywords = {Pedagogical agent, Secondary education, User-centered design, Human–machine communication (HMC), Human-to-human communication script},
abstract = {Pedagogical agents (PAs) communicate verbally and non-verbally with students in digital and virtual reality/augmented reality learning environments. PAs have been shown to be beneficial for learning, and generative artificial intelligence, such as large language models, can improve PAs' communication abilities significantly. K-12 education is underrepresented in learning technology research and teachers' and students' insights have not been considered when developing PA communication. The current study addresses this research gap by conducting and analyzing semi-structured, in-depth interviews with eleven high school teachers and sixteen high school students about their expectations for PAs' communication capabilities. The interviewees identified relational and task-related communication capabilities that a PA should perform to communicate effectively with students and scaffold their learning. PA communication that is simultaneously affirmative and relational can induce immediacy, foster the relationship and engagement with a PA, and support students' learning management. Additionally, the teachers and students described the activities and technological aspects that should be considered when designing conversational PAs. The study showed that teachers and students applied human-to-human communication scripts when outlining their desired PA communication characteristics. The study offers novel insights and recommendations to researchers and developers on the communicational, pedagogical, and technological aspects that must be considered when designing communicative PAs that scaffold students’ learning, and discusses the contributions on human–machine communication in education.}
}
@article{HASAN2022,
title = {Monitoring COVID-19 on Social Media: Development of an End-to-End Natural Language Processing Pipeline Using a Novel Triage and Diagnosis Approach},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {2},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/30397},
url = {https://www.sciencedirect.com/science/article/pii/S1438887122001881},
author = {Abul Hasan and Mark Levene and David Weston and Renate Fromson and Nicolas Koslover and Tamara Levene},
keywords = {COVID-19, conditional random fields, disease detection and surveillance, medical social media, natural language processing, severity and prevalence, support vector machines, triage and diagnosis},
abstract = {Background
The COVID-19 pandemic has created a pressing need for integrating information from disparate sources in order to assist decision makers. Social media is important in this respect; however, to make sense of the textual information it provides and be able to automate the processing of large amounts of data, natural language processing methods are needed. Social media posts are often noisy, yet they may provide valuable insights regarding the severity and prevalence of the disease in the population. Here, we adopt a triage and diagnosis approach to analyzing social media posts using machine learning techniques for the purpose of disease detection and surveillance. We thus obtain useful prevalence and incidence statistics to identify disease symptoms and their severities, motivated by public health concerns.
Objective
This study aims to develop an end-to-end natural language processing pipeline for triage and diagnosis of COVID-19 from patient-authored social media posts in order to provide researchers and public health practitioners with additional information on the symptoms, severity, and prevalence of the disease rather than to provide an actionable decision at the individual level.
Methods
The text processing pipeline first extracted COVID-19 symptoms and related concepts, such as severity, duration, negations, and body parts, from patients’ posts using conditional random fields. An unsupervised rule-based algorithm was then applied to establish relations between concepts in the next step of the pipeline. The extracted concepts and relations were subsequently used to construct 2 different vector representations of each post. These vectors were separately applied to build support vector machine learning models to triage patients into 3 categories and diagnose them for COVID-19.
Results
We reported macro- and microaveraged F1 scores in the range of 71%-96% and 61%-87%, respectively, for the triage and diagnosis of COVID-19 when the models were trained on human-labeled data. Our experimental results indicated that similar performance can be achieved when the models are trained using predicted labels from concept extraction and rule-based classifiers, thus yielding end-to-end machine learning. In addition, we highlighted important features uncovered by our diagnostic machine learning models and compared them with the most frequent symptoms revealed in another COVID-19 data set. In particular, we found that the most important features are not always the most frequent ones.
Conclusions
Our preliminary results show that it is possible to automatically triage and diagnose patients for COVID-19 from social media natural language narratives, using a machine learning pipeline in order to provide information on the severity and prevalence of the disease for use within health surveillance systems.}
}
@article{ARORA2018680,
title = {Why the Common Model of the mind needs holographic a-priori categories},
journal = {Procedia Computer Science},
volume = {145},
pages = {680-690},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.060},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918323469},
author = {Nipun Arora and Robert West and Andrew Brook and Mary Alexandria Kelly},
keywords = {Common Model of Cognition, Kant, Reason, Holographic Memory, Knowledge Level},
abstract = {The enterprise of developing a common model of the mind aims to create a foundational architecture for rational behavior in humans. Philosopher Immanuel Kant attempted something similar in 1781. The principles laid out by Kant for pursuing this goal can shed important light on the common model project. Unfortunately, Kant’s program has become hopelessly mired in philosophical hair-splitting. In this paper, we first use Kant’s approach to isolate the founding conditions of rationality in humans. His philosophy lends support to Newell’s knowledge level hypothesis, and together with it directs the common model enterprise to take knowledge, and not just memory, seriously as a component of the common model of the mind. We then map Kant’s cognitive mechanics to the operations which are used in the current models of cognitive architecture. Finally, we argue that this mapping can pave the way to develop the ontology of the knowledge level for general intelligence. We further show how they can be actualized in a memory system using high dimensional vectors to achieve specific cognitive abilities.}
}
@article{WANG20232957,
title = {Critical Relation Path Aggregation-Based Industrial Control Component Exploitable Vulnerability Reasoning},
journal = {Computers, Materials and Continua},
volume = {75},
number = {2},
pages = {2957-2979},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.035694},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823002886},
author = {Zibo Wang and Chaobin Huo and Yaofang Zhang and Shengtao Cheng and Yilu Chen and Xiaojie Wei and Chao Li and Bailing Wang},
keywords = {Path-based reasoning, representation learning, attention mechanism, vulnerability knowledge graph, industrial control component},
abstract = {With the growing discovery of exposed vulnerabilities in the Industrial Control Components (ICCs), identification of the exploitable ones is urgent for Industrial Control System (ICS) administrators to proactively forecast potential threats. However, it is not a trivial task due to the complexity of the multi-source heterogeneous data and the lack of automatic analysis methods. To address these challenges, we propose an exploitability reasoning method based on the ICC-Vulnerability Knowledge Graph (KG) in which relation paths contain abundant potential evidence to support the reasoning. The reasoning task in this work refers to determining whether a specific relation is valid between an attacker entity and a possible exploitable vulnerability entity with the help of a collective of the critical paths. The proposed method consists of three primary building blocks: KG construction, relation path representation, and query relation reasoning. A security-oriented ontology combines exploit modeling, which provides a guideline for the integration of the scattered knowledge while constructing the KG. We emphasize the role of the aggregation of the attention mechanism in representation learning and ultimate reasoning. In order to acquire a high-quality representation, the entity and relation embeddings take advantage of their local structure and related semantics. Some critical paths are assigned corresponding attentive weights and then they are aggregated for the determination of the query relation validity. In particular, similarity calculation is introduced into a critical path selection algorithm, which improves search and reasoning performance. Meanwhile, the proposed algorithm avoids redundant paths between the given pairs of entities. Experimental results show that the proposed method outperforms the state-of-the-art ones in the aspects of embedding quality and query relation reasoning accuracy.}
}
@article{ALLA2021100024,
title = {Cohort selection for construction of a clinical natural language processing corpus},
journal = {Computer Methods and Programs in Biomedicine Update},
volume = {1},
pages = {100024},
year = {2021},
issn = {2666-9900},
doi = {https://doi.org/10.1016/j.cmpbup.2021.100024},
url = {https://www.sciencedirect.com/science/article/pii/S2666990021000239},
author = {Naga Lalitha Valli ALLA and Aipeng CHEN and Sean BATONGBACAL and Chandini NEKKANTTI and Hong-Jie Dai and Jitendra JONNAGADDALA},
keywords = {Electronic health records, Clinical natural language processing, Surgical pathology reports, HL7 messages, Corpus construction},
abstract = {In Electronic Health Record (EHR) systems, key patient information is often captured in the form of unstructured clinical notes. The information from these notes can be extracted using Clinical Natural Language Processing (NLP). Training corpus is a key factor in development of efficient clinical NLP models. Clinical NLP corpus construction is complex and multifaceted. There are several challenges in corpus construction, but one challenge often not researched well is cohort selection aspect. In this study, we present methods employed and challenges encountered in cohort selection for construction of a clinical NLP corpus. In specific we present our methods in selection of cancer pathology reports to construct a corpus for automatic deidentification. 2100 pathology reports were extracted from 1833 (518 male and 1313 female) cancer patients using Health Level-7 (HL7) message standard. In terms of the age group distribution, the age group 60–70 years was highest with 872 patients. Our findings suggest deciphering the segment information from HL7 messages that are collected from different hospitals is a challenging task. The quality of HL7 messages also varied significantly with inconsistent tags making it difficult to identify reports that meet criteria set a priori. One of key lessons learned is linking the HL7 reports data with additional EMR data such as admissions, would help in identifying high quality reports and resolve duplicates. Also, our findings suggest that, in general the EHR data quality is poor with varying clinical coding and metadata standards between different hospitals. It is vital to identify and address these challenges for development of a high-quality corpus.}
}
@article{ZAIDELMAN202160,
title = {Russian-language neurosemantics: Clustering of word meaning and sense from oral narratives},
journal = {Cognitive Systems Research},
volume = {67},
pages = {60-65},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000012},
author = {Liudmila Zaidelman and Zakhar Nosovets and Artemiy Kotov and Vadim Ushakov and Vera Zabotkina and Boris M. Velichkovsky},
keywords = {Brain mapping, Semantic space, Text comprehension},
abstract = {This article is a part of a large-scale brain mapping project aimed at finding the relations among semantic categories in oral Russian-language texts and brain activity as measured using functional magnetic resonance imaging (fMRI). The goal of present study in particular is to examine the nature of lexical semantic relations and find an appropriate lexical space, homeomorphic to the activation patterns in the brain. Participants were presented with oral narratives, which described significant social issues from the first-person perspective. Stimuli were annotated using a dictionary and a vector approach. Results show that fMRI signal and clusters of related words have similar patterns of brain activation across participants. Results also show that annotation by a list of features more strongly contributes to prediction of the observed activation patterns. Findings confirm the hypothesis of situational semantic representation in the brain.}
}
@article{SUNG2021337,
title = {Applications of Semantic Web in integrating open data and bibliographic records: a development example of an infomediary of Taiwanese indigenous people},
journal = {The Electronic Library},
volume = {39},
number = {2},
pages = {337-353},
year = {2021},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-09-2020-0258},
url = {https://www.sciencedirect.com/science/article/pii/S0264047321000278},
author = {Han-Yu Sung and Yu-Liang Chi},
keywords = {Open data, Semantic Web, Linked data, Indigenous people},
abstract = {Purpose
This study aims to develop a Web-based application system called Infomediary of Taiwanese Indigenous Peoples (ITIP) that can help individuals comprehend the society and culture of indigenous people. The ITIP is based on the use of Semantic Web technologies to integrate a number of data sources, particularly including the bibliographic records of a museum. Moreover, an ontology model was developed to help users search cultural collections by topic concepts.
Design/methodology/approach
Two issues were identified that needed to be addressed: the integration of heterogeneous data sources and semantic-based information retrieval. Two corresponding methods were proposed: SPARQL federated queries were designed for data integration across the Web and ontology-driven queries were designed to semantically search by knowledge inference. Furthermore, to help users perform searches easily, three searching interfaces, namely, ethnicity, region and topic, were developed to take full advantage of the content available on the Web.
Findings
Most open government data provides structured but non-resource description framework data, Semantic Web consumers, therefore, require additional data conversion before the data can be used. On the other hand, although the library, archive and museum (LAM) community has produced some emerging linked data, very few data sets are released to the general public as open data. The Semantic Web’s vision of “web of data” remains challenging.
Originality/value
This study developed data integration from various institutions, including those of the LAM community. The development was conducted based on the mode of non-institution members (i.e. institutional outsiders). The challenges encountered included uncertain data quality and the absence of institutional participation.}
}
@article{ADALAT2025111367,
title = {Model-based generation of manufacturing process plans through on-the-fly topology formation},
journal = {Computers & Industrial Engineering},
volume = {208},
pages = {111367},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111367},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225005133},
author = {Omar Adalat and Daniele Scrimieri and Shukri Afazov and Svetan Ratchev},
keywords = {Automated process planning, Controller synthesis, Realizability, Labelled transition system, Assembly, Robotics},
abstract = {In advanced manufacturing systems, the production of complex and highly customised products requires the preparation of many different product specifications and associated manufacturing process plans. The creation of these plans involves the search for the production resources (e.g. robots, machine tools, sensors, end effectors) that are needed to implement the product specifications, and how to orchestrate them. This paper presents a model-based approach to the automatic generation of manufacturing process plans from the models of the target products and available resources. The modelling language is based on labelled transition systems, which are useful to represent and manipulate efficiently sequences of high-level tasks that can be executed on the production resources. The proposed solution improves on existing approaches by forming the system topology on the fly while generating a process plan, creating only the topology states that are actually needed and, therefore, drastically reducing computational space and time. Experimental results demonstrate the greater computational performance and scalability of the presented techniques with an increasing number of resources, as well as the efficiency of the generated (optimal) plans. In summary, the benefits of this work are the following: minimal human intervention, as process planners only have to create the models; shorter process planning time, generation of optimal plans and, consequently, reduced manufacturing costs and shorter lead time for customised products.}
}
@article{KIM2023100770,
title = {Teaching argumentation with a dialogic stance: A case of an 11th-grade English language arts classroom},
journal = {Learning, Culture and Social Interaction},
volume = {43},
pages = {100770},
year = {2023},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2023.100770},
url = {https://www.sciencedirect.com/science/article/pii/S2210656123000867},
author = {Min-Young Kim and Eileen Shanahan},
keywords = {Argumentation, Dialogic stance, Epistemic openness, Relationships, Classroom discourse, Discourse analysis},
abstract = {In this article, we present a case of teaching argumentation with a dialogic stance to explore how such teaching shapes what counts of argumentation and engages students in a sophisticated and nuanced understanding of argumentation. Adopting a microethnographic approach to discourse analysis, we closely examine the interactions in one argumentation lesson in a high school English class. The findings demonstrate that the teacher's enacted dialogic stance facilitates the construction of argumentation components as organically related and foregrounds argument as inherently connected to and among arguers. This study holds the potential to expand the landscape of the field of argumentation education.}
}
@article{SALZMANNERIKSON2024586,
title = {A scoping review of autoethnography in nursing},
journal = {International Journal of Nursing Sciences},
volume = {11},
number = {5},
pages = {586-594},
year = {2024},
issn = {2352-0132},
doi = {https://doi.org/10.1016/j.ijnss.2024.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352013224001017},
author = {Martin Salzmann-Erikson},
keywords = {Anthropology, Cultural, Nursing research, Psychological, Qualitative research, Social theory},
abstract = {Objective
Autoethnography combines personal experiences with cultural analysis, emerging as a response to the limitations of traditional ethnography. This review aimed to explore, describe, and delineate the utilization of autoethnography by nurses published in peer-reviewed journals.
Methods
A scoping review was conducted according to the Arksey and O’Malley framework. On October 12, 2023, autoethnographic studies in nursing were identified through searches of CINAHL, PubMed, PsycINFO, and Scopus. Peer-reviewed articles published in English language were retrieved. We applied no date restriction. Data were extracted on nursing, epiphany, results, style of writing, implications for nursing, and ethical considerations.
Results
Twenty-six articles met the inclusion criteria. Mental health nursing, covered by nine articles, elucidated experiences of stigma, ethical dilemmas, and professional identity. Nursing education, represented by seven articles, highlighted identity struggles, systemic biases, and evolving pedagogies. Palliative care, addressed by three articles, provided insights into communication challenges and emotional complexities in end-of-life care. The remaining articles explored rehabilitation, cultural competence, and chronic pain management. A conceptual framework integrating ontological, epistemological, ethical, and practical dimensions was developed, emphasizing the interplay between personal and professional roles.
Conclusions
This review underscores autoethnography’s value in uncovering the cultural and ethical dimensions of nursing. This framework advocates for a reflective, culturally attuned approach to healthcare, fostering transformative changes in nursing. Further research should explore underrepresented nursing specialties to harness autoethnography’s potential fully.}
}
@article{QIAO2025104877,
title = {Food recommendation towards personalized wellbeing},
journal = {Trends in Food Science & Technology},
volume = {156},
pages = {104877},
year = {2025},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2025.104877},
url = {https://www.sciencedirect.com/science/article/pii/S0924224425000135},
author = {Guanhua Qiao and Dachuan Zhang and Nana Zhang and Xiaotao Shen and Xidong Jiao and Wenwei Lu and Daming Fan and Jianxin Zhao and Hao Zhang and Wei Chen and Jinlin Zhu},
keywords = {Food recommendation system, Machine learning, Personalized diet, Recommendation algorithm, Precision nutrition},
abstract = {Background
The intersection of nutrition and technology gave birth to the research of food recommendation system (FRS), which marked the transformation of traditional diet to a more personalized and healthy direction. The FRS uses advanced data analysis and machine learning technology to provide customized dietary advice according to users' personal preferences, and nutritional needs, which plays a vital role in promoting public health and reducing disease risks.
Scope and approach
This review presents the architecture of FRS and deeply discusses various recommendation algorithms, including the content-based method, collaborative filtering method, knowledge graph-based method, and hybrid methods. The review further introduces existing data resources and evaluation metrics, and highlights key technologies in user profiling and food analysis. In addition, the wide application of personalized FRS is summarized, and the importance of these systems in satisfying users' dietary preferences and maintaining balanced nutrition is emphasized. Finally, the key challenges and development trends of FRS are deeply analyzed from data level, model level and user experience level.
Key findings and conclusions
Personalized FRS shows great potential in helping users make healthier dietary decisions. Although there are still many challenges, such as dealing with heterogeneous data and interpretability. But with the progress of technology, there will be broader development in the future. For example, the powerful data processing ability of deep learning will effectively improve the accuracy of the system. In addition, the application of interactive recommendation system and large language model will also provide strong support for satisfying user experience and improving acceptance.}
}
@article{ZHANG2025103170,
title = {ATSIU: A large-scale dataset for spoken instruction understanding in air traffic control},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103170},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103170},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000631},
author = {Minghua Zhang and Yang Yang and Shengsheng Qian and Qihan Deng and Jing Fang and Kaiquan Cai},
keywords = {Air traffic operation, Spoken instruction understanding, Dataset, Intent detection, Slot filling},
abstract = {Spoken instruction communication between air traffic controllers and pilots is a crucial and fundamental process of air traffic control (ATC). Automated understanding of these instructions can significantly enhance the safety and efficiency of air traffic, making this field a prominent area of current research. However, thorough scrutiny of instruction semantics and associated benchmarks for instruction understanding in the ATC domain have remained unexplored. In this paper, we aim to build a large-scale specialized air traffic spoken instruction understanding (ATSIU) dataset to bridge this gap. The proposed dataset features a tailored hierarchical intent taxonomy, encompassing 9 coarse-grained intents and 26 fine-grained intents, together with 78 customized slots. It was developed by transcribing over 200 hours of raw ATC audio into 19.8k texts, each meticulously annotated with golden intent and slot labels by industry professionals. Moreover, we present an air traffic spoken instruction understanding network (ATSIU-Net) as a baseline method for ATC spoken instruction understanding, which employs a pre-trained language model and a joint learning mechanism to facilitate collaborative ATC intent detection and slot filling. Extensive experiment results demonstrate that ATSIU-Net establishes promising performance benchmarks while revealing key challenges in intent granularity, flight phases, multi-task learning, and low-data scenarios. It is believed that this work not only showcases the potential of advanced algorithms in ATC-specific domain, but also provides diverse research topics for the common natural language processing community.}
}
@article{QIU2025100588,
title = {Understanding the comorbidities among psychiatric disorders, chronic low-back pain, and spinal degenerative disease using observational and genetically informed analyses},
journal = {Biological Psychiatry Global Open Science},
pages = {100588},
year = {2025},
issn = {2667-1743},
doi = {https://doi.org/10.1016/j.bpsgos.2025.100588},
url = {https://www.sciencedirect.com/science/article/pii/S2667174325001429},
author = {Dan Qiu and Eleni Friligkou and Jun He and Brenda Cabrera-Mendoza and Mihaela Aslan and Mihir Gupta and Renato Polimanti},
keywords = {Mental Health, Polygenic Risk, Shared Pathogenesis, Musculoskeletal Disorders},
abstract = {Background
Psychiatric disorders and symptoms are associated with differences in pain perception and sensitivity. These differences can have important implications in treating spinal degenerative disease (SDD) and chronic low-back pain (CLBP).
Methods
Leveraging UK Biobank (UKB, N=402,072) and All of Us Research Program (AoU, N=157,415), we investigated the effects linking psychiatric disorders to SDD and CLBP. We applied multi-nominal regression models, polygenic risk scoring (PRS), and one-sample Mendelian randomization (MR) to triangulate the effects underlying the associations observed. We also performed gene ontology and drug-repurposing analyses to dissect the biology shared among mental illnesses, SDD, and CLBP.
Results
Comparing individuals affected only by SDD, those affected only by CLBP, and those affected by both conditions to controls, observational and genetically informed analyses highlighted that the strongest effects across the three case groups were observed for alcohol use disorder, anxiety, depression, and posttraumatic stress disorder. Additionally, schizophrenia and its PRS appeared to have an inverse relationship with CLBP, SDD, and their comorbidity. One-sample MR highlighted a potential direct effect of internalizing disorders on the outcomes investigated that was particularly strong on SDD. Our drug-repurposing analyses identified histone deacetylase inhibitors as targeting molecular pathways shared among psychiatric disorders, SDD, and CLBP.
Conclusions
These findings support that the comorbidity among psychiatric disorders, SDD, and CLBP is due to the contribution of direct effects and shared biology linking these health outcomes. These pleiotropic mechanisms, together with sociocultural factors, play a key role in shaping the SDD-CLBP comorbidity patterns observed across the psychopathology spectrum.}
}
@incollection{DANU2024,
title = {Idiolect},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00122-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041001228},
author = {Julija Danu and Krzysztof Kredens and Tim Grant},
keywords = {Idiolect, Linguistic individual, Style, Authorship, Authorship analysis, Forensic linguistics},
abstract = {We examine the development of the idea of the linguistic individual and the concept of idiolect from antiquity to contemporary theories and provide a brief survey of international approaches to these discussions, and empirical attempts to establish the evidence of existence of idiolects before looking at applications, primarily in forensic linguistic casework. The future indicates that interest in Large Language Models is likely to influence development of both theory and empirical evidence that idiolects exist.}
}
@article{WANG2020105478,
title = {Semantic characterization of adverse outcome pathways},
journal = {Aquatic Toxicology},
volume = {222},
pages = {105478},
year = {2020},
issn = {0166-445X},
doi = {https://doi.org/10.1016/j.aquatox.2020.105478},
url = {https://www.sciencedirect.com/science/article/pii/S0166445X19311063},
author = {Rong-Lin Wang},
keywords = {Adverse outcome pathway, Ontology, Semantic analysis, Toxicity event network},
abstract = {This study was undertaken to systematically assess the utilities and performance of ontology-based semantic analysis in adverse outcome pathway (AOP) research. With an increasing number of AOPs developed by scientific domain experts to organize toxicity information and facilitate chemical risk assessment, there is a pressing need for objective approaches to evaluate the biological coherence and quality of these AOPs. Powered by ontologies covering a wide range of biological domains, abundant phenotypic data annotated ontologically, and some sophisticated knowledge computing tools, semantic analysis has great potential in this area of application. With the events in the AOP-Wiki first annotated into logical definitions and then grouped into phenotypic profiles by individual AOPs, the coherence and quality of AOPs were assessed at several levels: paired key event relationships (KER), all possible event pair combinations within AOPs, and the phenotypic profiles of AOPs, genes, biological pathways, human diseases, and selected chemicals. The semantic similarities were assessed at all these levels based on a unified cross-species vertebrate phenotype ontology encompassing the logical definitions of AOP events as well as many other domain ontologies. A substantial number of KERs and AOPs in the AOP-Wiki were found to be semantically coherent. These same coherent AOPs also mapped to many more genes, pathways, and diseases biologically aligned with the intended chain of events therein leading to their respective adverse outcomes. Significantly, these findings imply that semantic analysis should also have utilities in developing future AOPs by selecting candidate events from either the existing AOP-Wiki events or a broader collection of ontology terms semantically similar to the molecular initiating events or adverse outcomes of interest. In addition, semantic analysis enabled AOP networks to be constructed at the level of phenotypic profiles based on similarities, complementing those based on event sharing by bringing genes, pathways, diseases, and chemicals into the networks too—thus greatly expanding the biological scope and our understanding of AOPs.}
}
@article{LI2024100164,
title = {A hybrid knowledge graph for efficient exploration of lithostratigraphic information in open text data},
journal = {Applied Computing and Geosciences},
volume = {22},
pages = {100164},
year = {2024},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2024.100164},
url = {https://www.sciencedirect.com/science/article/pii/S2590197424000119},
author = {Wenjia Li and Xiaogang Ma and Xinqing Wang and Liang Wu and Sanaz Salati and Zhong Xie},
keywords = {Knowledge graph, Stratigraphy, Natural language processing, Relationship extraction, Bidirectional encoder representation from transformers (BERT), Data mining},
abstract = {Rocks formed during different geologic time record the diverse evolution of the geosphere and biosphere. In the past decades, substantial geoscience data have been made open access, providing invaluable resources for studying the stratigraphy in different regions and at different scales. However, many open datasets have information recorded in natural language with heterogeneous terminologies, short of efficient approaches to analyze them. In this research, we constructed a hybrid Stratigraphic Knowledge Graph (StraKG) to help address this challenge. StraKG has two layers, a simple schema layer and a rich instance layer. For the schemas, we used a short but functional list of classes and relationships, and then incorporated community-recognized terminologies from geological dictionaries. For the instances, we used natural language processing techniques to analyze open text data and obtained massive records, such as rocks and spatial locations. The nodes in the two layers were associated to establish a consistent structure of stratigraphic knowledge. To verify the functionality of StraKG, we applied it to the Baidu encyclopedia, the largest online Chinese encyclopedia. Three experiments were implemented on the topics of stratigraphic correlation, spatial distribution of ophiolite in China, and spatio-temporal distribution of open lithostratigraphic data. The results show that StraKG can provide strong knowledge reference for stratigraphic studies. Used together with data exploration and data mining methods, StraKG illustrates a new approach to analyze the open and big text data in geoscience.}
}
@article{LI2022100511,
title = {Neural Natural Language Processing for unstructured data in electronic health records: A review},
journal = {Computer Science Review},
volume = {46},
pages = {100511},
year = {2022},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2022.100511},
url = {https://www.sciencedirect.com/science/article/pii/S1574013722000454},
author = {Irene Li and Jessica Pan and Jeremy Goldwasser and Neha Verma and Wai Pan Wong and Muhammed Yavuz Nuzumlalı and Benjamin Rosand and Yixin Li and Matthew Zhang and David Chang and R. Andrew Taylor and Harlan M. Krumholz and Dragomir Radev},
keywords = {Natural language processing, Electronic health records, Deep learning},
abstract = {Electronic health records (EHRs), digital collections of patient healthcare events and observations, are ubiquitous in medicine and critical to healthcare delivery, operations, and research. Despite this central role, EHRs are notoriously difficult to process automatically. Well over half of the information stored within EHRs is in the form of unstructured text (e.g., provider notes, operation reports) and remains largely untapped for secondary use. Recently, however, newer neural network and deep learning approaches to Natural Language Processing (NLP) have made considerable advances, outperforming traditional statistical and rule-based systems on a variety of tasks. In this survey paper, we summarize current neural NLP methods for EHR applications. We focus on a broad scope of tasks, namely, classification and prediction, word embeddings, extraction, generation, and other topics such as question answering, phenotyping, knowledge graphs, medical dialogue, multilinguality, interpretability, etc.}
}
@article{HUI2025113524,
title = {Knowledge-augmented encoder for few-shot deep intent recognition in air traffic control},
journal = {Knowledge-Based Systems},
volume = {320},
pages = {113524},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113524},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125005702},
author = {Yi Hui and Yang Yang and Shengsheng Qian and Kaiquan Cai},
keywords = {Air traffic control, Open-world circumstances, AI system and human hybrid, Few-shot intent recognition, Knowledge augmentation},
abstract = {To reduce the negative effects of human operators in the open-world air traffic control (ATC), the artificial intelligence (AI) system and human hybrid architecture have drawn considerable attention. As the core of the computer interaction model, the few-shot intent recognition to capture the scarce and implicit purpose and intentions of human operators (“gestalt”) from controller-pilot instructions is still a non-trivial issue. This study presents an end-to-end Knowledge-augmented Few-shot Intent Recognition (KaFIR) scheme to address the intrinsic challenges in achieving unified and effective feature representation in the few-shot setting. On the strength of the fine-tuned language model as a basic encoder, the task-oriented module is further designed to incorporate two unsupervised models for a better knowledge-augmented instruction representation to avoid overfitting. Specifically, a duality-augmented contrastive learning model and a dictionary-based masked language model are proposed, exploiting the global and the local prior semantics from the read-back sentence pattern and the highly related keywords, respectively. This study performs extensive experiments on two real-world ATC instruction datasets collected in China and verifies the superior performance of the proposed KaFIR scheme compared with the state-of-the-art baselines, including fine-tuned language models and feature-augmented models. Results demonstrate the promising prospects of integrating the AI system as the auxiliary tool into various human-in-the-loop ATC tasks.}
}
@article{LI2020101851,
title = {A survey of feature modeling methods: Historical evolution and new development},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {61},
pages = {101851},
year = {2020},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.101851},
url = {https://www.sciencedirect.com/science/article/pii/S0736584519300857},
author = {Lei Li and Yufan Zheng and Maolin Yang and Jiewu Leng and Zhengrong Cheng and Yanan Xie and Pingyu Jiang and Yongsheng Ma},
keywords = {Feature modeling, Feature ontology, Feature interoperability, Engineering informatics, Socio-cyber-physical system},
abstract = {Initially developed for geometric representation, feature modeling has been applied in product design and manufacturing with great success. With the growth of computer-aided engineering (CAE), computer-aided process planning (CAPP), computer-aided manufacturing (CAM), and other applications for product engineering, the definitions of features have been mostly application-driven. This survey briefly reviews feature modeling historical evolution first. Subsequently, various approaches to resolving the interoperability issues during product lifecycle management are reviewed. In view of the recent progress of emerging technologies, such as Internet of Things (IoT), big data, social manufacturing, and additive manufacturing (AM), the focus of this survey is on the state of the art application of features in the emerging research fields. The interactions among these trending techniques constitute the socio-cyber-physical system (SCPS)-based manufacturing which demands for feature interoperability across heterogeneous domains. Future efforts required to extend feature capability in SCPS-based manufacturing system modeling are discussed at the end of this survey.}
}
@article{IAKSCH2019100942,
title = {Method for digital evaluation of existing production systems adequacy to changes in product engineering in the context of the automotive industry},
journal = {Advanced Engineering Informatics},
volume = {42},
pages = {100942},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.100942},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618305305},
author = {Jaqueline Sebastiany Iaksch and Milton Borsato},
keywords = {Model-based engineering, Ontology, Product development process, Production systems},
abstract = {Current industry practices during the Product Development Process (PDP) still points to the isolation of knowledge domains even with the increase of digitalization. Considering manufacturing process constrains from the beginning of the PDP avoids problems in later stages during the whole product life cycle. Through the application of concepts of the Digital Thread approach, the opportunity to intelligently integrate knowledge into product development is presented, creating a “digital fabric” capable of directing and supporting all stages of the product life cycle. Through these concepts, this research proposes the elaboration of an ontological model and application method capable of evaluating, in real time, the adequacy of the existing production systems, integrating the project and process information. The methodological framework used for the development of this method was Design Science Research. In this way, six steps were performed: (i) problem identification and motivation; (ii) definition of the objectives of the solution; (iii) artifact design and development; (iv) demonstration; (v) evaluation; and, (vi) results report. Through the description of manufacturing systems, the solution contributes to the digital evaluation and facilitates the decision making regarding productive systems, as well as data recovery, reutilization and management. In order to do an initial framework validation, it was performed the application of adequacy principles of a specific production line in automotive sector. However, this choice of a complex industry sector translates a clear possibility of framework adaptation to another industrial segment.}
}
@article{LEE2023100174,
title = {High-level implementable methods for automated building code compliance checking},
journal = {Developments in the Built Environment},
volume = {15},
pages = {100174},
year = {2023},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2023.100174},
url = {https://www.sciencedirect.com/science/article/pii/S266616592300056X},
author = {Jin-Kook Lee and Kyunghyun Cho and Hyeokjin Choi and Soohyung Choi and Sumin Kim and Seung Hyun Cha},
keywords = {High-level method, Implementable method, Automated building code compliance checking, Building permit, Building information modeling (BIM)},
abstract = {This paper presents an approach for defining high-level implementable methods to improve their low-level rule-checking procedures. This is part of an effort to develop challenging building information modeling (BIM)-enabled applications for building permits through automated code compliance checking. The main approach described here aims to alleviate the time-consuming and error-prone tasks of translating natural language into explicitly defined rules. An explicit expression of design requirements is key to building projects involving collaboration between architects, code experts, and developers. To maximize the generalization, neutralization, and reusability of the given rules in natural (written) language, we propose a series of high-level implementable computer programming methods (operators); these can be beneficial for translating verb phrases in building act sentences with minimal ambiguity as well as representing the peculiar properties of building objects without conflicts. Lastly, we demonstrate its application in code compliance checking by employing KBimCode and the developed rule-checking software.}
}
@article{HARVEY2022,
title = {Natural Language Processing Methods and Bipolar Disorder: Scoping Review},
journal = {JMIR Mental Health},
volume = {9},
number = {4},
year = {2022},
issn = {2368-7959},
doi = {https://doi.org/10.2196/35928},
url = {https://www.sciencedirect.com/science/article/pii/S2368795922000907},
author = {Daisy Harvey and Fiona Lobban and Paul Rayson and Aaron Warner and Steven Jones},
keywords = {bipolar disorder, mental health, mental illness, natural language processing, computational linguistics},
abstract = {Background
Health researchers are increasingly using natural language processing (NLP) to study various mental health conditions using both social media and electronic health records (EHRs). There is currently no published synthesis that relates specifically to the use of NLP methods for bipolar disorder, and this scoping review was conducted to synthesize valuable insights that have been presented in the literature.
Objective
This scoping review explored how NLP methods have been used in research to better understand bipolar disorder and identify opportunities for further use of these methods.
Methods
A systematic, computerized search of index and free-text terms related to bipolar disorder and NLP was conducted using 5 databases and 1 anthology: MEDLINE, PsycINFO, Academic Search Ultimate, Scopus, Web of Science Core Collection, and the ACL Anthology.
Results
Of 507 identified studies, a total of 35 (6.9%) studies met the inclusion criteria. A narrative synthesis was used to describe the data, and the studies were grouped into four objectives: prediction and classification (n=25), characterization of the language of bipolar disorder (n=13), use of EHRs to measure health outcomes (n=3), and use of EHRs for phenotyping (n=2). Ethical considerations were reported in 60% (21/35) of the studies.
Conclusions
The current literature demonstrates how language analysis can be used to assist in and improve the provision of care for people living with bipolar disorder. Individuals with bipolar disorder and the medical community could benefit from research that uses NLP to investigate risk-taking, web-based services, social and occupational functioning, and the representation of gender in bipolar disorder populations on the web. Future research that implements NLP methods to study bipolar disorder should be governed by ethical principles, and any decisions regarding the collection and sharing of data sets should ultimately be made on a case-by-case basis, considering the risk to the data participants and whether their privacy can be ensured.}
}
@article{GHANNAD201914,
title = {Automated BIM data validation integrating open-standard schema with visual programming language},
journal = {Advanced Engineering Informatics},
volume = {40},
pages = {14-28},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618304610},
author = {Pedram Ghannad and Yong-Cheol Lee and Johannes Dimyadi and Wawan Solihin},
keywords = {Building Information Modeling, BIM Data Checking, Visual programming language, LegalRuleML (LRML)},
abstract = {A building design must comply with a wide spectrum of requirements stipulated by building codes, normative standards, owner’s specifications, industry’s guidelines, and project requirements. The current rule-based compliance checking practice is a costly bottleneck in a building project, and thus, there is a demand for a design evaluation process that incorporates automated checking capabilities to address the inefficiency and the error-prone nature of the current manual checking practice. The inherent complexity of building design rules and impracticability of existing automated checking approaches are two key challenges that must be addressed to enable practical compliance checking automation. This research study proposes a new modularized framework that integrates the emerging open standard, LegalRuleML, with a Visual Programming Language. The framework allows a standardized method of defining design rules in a machine-readable and executable format. The proposed approach encompasses the entire compliance checking process from the interpretation of natural language-based requirements to machine-readable rules, rule categorization, rule parameterization, and the execution of the rules on the ISO-standard building information model. This modularized BIM-based design validation framework is expected to help automatically and iteratively evaluate the level of quality and defects of information conveyed in a given building model as an essential part of the early design process.}
}
@article{DEWAELE2021102660,
title = {Predicting the emotional labor strategies of Chinese English Foreign Language teachers},
journal = {System},
volume = {103},
pages = {102660},
year = {2021},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2021.102660},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X21002141},
author = {Jean-Marc Dewaele and Aihui Wu},
keywords = {Emotional labor, Expression of naturally felt emotions, Surface acting, EFL Teachers, Trait emotional intelligence},
abstract = {Teachers routinely make a conscious effort to manage their emotions in front of students. The present study focused on emotional labor strategies from 594 Chinese English Foreign Language (EFL) teachers, seeking to discover which ones are preferred, how they are related to each other and how they are linked to teachers’ sociobiographical, institutional, attitudinal, linguistic and psychological characteristics. Statistical analyses revealed that the strategy Expression of naturally felt emotions was preferred to Surface acting and that both dimensions were moderately negatively correlated. Correlation analyses revealed that sociobiographical variables were unrelated to both dependent variables. Multiple regression analyses revealed that the strongest predictors of Expression of naturally felt emotions were attitudes towards students, Emotionality (a factor of Trait Emotional Intelligence) and two institutional variables. Sociability (another factor of Trait Emotional Intelligence) was the only predictor of Surface acting. The findings are linked to previous research and some pedagogical implications are presented.}
}
@article{LI201810,
title = {Semantic multi-agent system to assist business integration: An application on supplier selection for shipbuilding yards},
journal = {Computers in Industry},
volume = {96},
pages = {10-26},
year = {2018},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517300635},
author = {Jinghua Li and Miaomiao Sun and Duanfeng Han and Xiaoyuan Wu and Boxin Yang and Xuezhang Mao and Qinghua Zhou},
keywords = {Shipbuilding industry, Service-oriented architecture, Multi-agent system, Domain ontology, Supplier selection},
abstract = {To compete in the fierce market, a key issue for shipbuilding companies is to effectively integrate outer resources with the inner business. However, multi-agent systems (MAS) developed so far have proven to be inapt to support the whole process of making a coordinated supply plan. Meanwhile, huge amount of data emerging from various kinds of enterprise infrastructures have not been semantically consolidated for smooth and efficient workflows. To overcome these barriers, this paper proposes a hybrid semantic-augmented MAS (OMAS), consisting of a Service Management Agent, a Configuration Agent, a Member Management Agent, a Process-Driven Module and a Semantic-Enhanced Module. These components collaborate to integrate heterogeneous data devoid of semantic ambiguities, thereby improving workflows’ coherence and efficiency. A scalable ontology grid is initialized as the operational foundation after analyzing the related works. In addition, OMAS innovatively adds semantic assistance into the supplier selection process wherein a two-phase negotiation algorithm is suggested. Finally, an illustrative example from a marine company is conducted to demonstrate the benefits of OMAS, through which the effectiveness of the proposed algorithm is proved to be much better than that of a contemporary contribution.}
}
@article{GAVRILKINA2018209,
title = {Building a well-formalized conceptual semantic network based on scientific and technical texts},
journal = {Procedia Computer Science},
volume = {145},
pages = {209-213},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918323287},
author = {A S Gavrilkina and O L Golitsyna and N V Maksimov},
keywords = {semantic network, text processing, information retrieval, taxonomy of relations},
abstract = {This paper presents a method for automatic constructing a conceptual semantic network (Task ontology) based on texts of documents. Ontology is considered as metagraph that is reflecting immanent and situational relations. Verbose constructs, which corresponds to concepts and relationships, are extracted from texts by analyzing the linguistic characteristics of words and the templates of typical constructions. Linguistic constructions of situational links are identified by relations taxonomy.}
}
@article{GOUDAMOHAMED2020103209,
title = {BIM and semantic web-based maintenance information for existing buildings},
journal = {Automation in Construction},
volume = {116},
pages = {103209},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103209},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519304029},
author = {Ahmed {Gouda Mohamed} and Mohamed Reda Abdallah and Mohamed Marzouk},
keywords = {As-is information, BIM ontology, Semantic web technology, As-is record-COBie, Point cloud},
abstract = {In the existing building facilities context, the BIM substantial intervention concerning semantics and as-is information based portrayal for maintenance requirements is very confined. This confinement stresses the challenges associated with the lack and absence of reusing existing building facilities, as is domain knowledge. Within this scope, this paper proposes a novel approach for existing building facilities semantic-enrichment from indoor digital scanning portrayal phase to semantics depiction and management phase for renovation, rehabilitation, or restoration functions. In this paper, we present an ontological system, which relies on integrating the as-is information BIM and semantic web technology. The developed approach improves existing building facilities, as-is information formalization, and management for maintenance purposes. The paper outcome is a semantically integrated knowledge-based framework that can potentially: 1) constitute a semantic database for building facilities components as-is information using BIM; 2) overcome the lack and absence of retrieving and reusing building facilities as-is domain information; and (3) formalize the building facilities as-is knowledge in a computable way.}
}
@article{MEIJER2025654,
title = {Empowering natural product science with AI: leveraging multimodal data and knowledge graphs††In memory of Dr Othman Skiredj},
journal = {Natural Product Reports},
volume = {42},
number = {4},
pages = {654-662},
year = {2025},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d4np00008k},
url = {https://www.sciencedirect.com/science/article/pii/S0265056824000862},
author = {David Meijer and Mehdi A. Beniddir and Connor W. Coley and Yassine M. Mejri and Meltem Öztürk and Justin J. J. {van der Hooft} and Marnix H. Medema and Adam Skiredj},
abstract = {Artificial intelligence (AI) is accelerating how we conduct science, from folding proteins with AlphaFold and summarizing literature findings with large language models, to annotating genomes and prioritizing newly generated molecules for screening using specialized software. However, the application of AI to emulate human cognition in natural product research and its subsequent impact has so far been limited. One reason for this limited impact is that available natural product data is multimodal, unbalanced, unstandardized, and scattered across many data repositories. This makes natural product data challenging to use with existing deep learning architectures that consume fairly standardized, often non-relational, data. It also prevents models from learning overarching patterns in natural product science. In this Viewpoint, we address this challenge and support ongoing initiatives aimed at democratizing natural product data by collating our collective knowledge into a knowledge graph. By doing so, we believe there will be an opportunity to use such a knowledge graph to develop AI models that can truly mimic natural product scientists' decision-making.}
}
@incollection{ALI20191124,
title = {Challenges in Creating Online Biodiversity Repositories With Taxonomic Classification},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {1124-1130},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20199-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338201994},
author = {Mohd N.M. Ali and Amy Y. Then Hui and Sarinder K. Dhillon},
keywords = {Database, Fish ontology, Ontology, Taxonomy classification},
abstract = {Taxonomically accurate and well documented biological information are essential for scientific identification and management of organisms. Incorrect identifications which are based on low quality and incorrect literature can significantly impede ecological studies. In this article, we discuss the challenges faced in creating repositories for taxonomic classification, focusing on the differences in classification in several fish-related databases, and propose ways to overcome taxonomic problems. Taxonomic classification for some known ontologies is also discussed i comparison to the Fish Ontology scheme, an ontology for automated recognition of fish species using taxonomic classification.}
}
@article{VIKTOROVIC201931,
title = {Semantic web technologies as enablers for truly connected mobility within smart cities},
journal = {Procedia Computer Science},
volume = {151},
pages = {31-36},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919304697},
author = {Miloš Viktorović and Dujuan Yang and Bauke de Vries and Nico Baken},
keywords = {Linked data, Authonomous vehicles, Semantic web, Mobility, Smart Cities, Ontologies},
abstract = {Most car manufacturers predict that in the first half of the next decade there will be fully autonomous vehicles on our roads. Such vehicles would have to communicate in order to mitigate problems caused by single-viewpoint approach. So there are a lot of researches and developments when it comes to communication layer of V2X (Vehicle-to-Everything), but there is still a lot to be done when it comes to data layer of this communication. This is why we propose using Semantic Web Technologies (SWT) to fill in gaps within data layer of V2X communication. By using SWT (Semantic Web Technologies) and Linked data, we plan to interconnect various data sources, in order to provide homogeneous way for connected autonomous vehicles (CAV) to access relevant information. Such information is currently contained in three distinctive type of sources. These are: Geo-stationary Static data sources (Maps, City models), Geostationary Dynamic data sources (IoT devices) and Non-geostationary Dynamic sources (Vehicles). Using SWT, our goal is to develop ontology(s), in such a way that in-vehicle algorithms can extract and process information about environment they are in, while taking into account available network bandwidth.}
}
@article{SCHNEIDER2019127,
title = {Virtual engineering of cyber-physical automation systems: The case of control logic},
journal = {Advanced Engineering Informatics},
volume = {39},
pages = {127-143},
year = {2019},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1474034618300740},
author = {Georg Ferdinand Schneider and Hendro Wicaksono and Jivka Ovtcharova},
keywords = {Control logic, Cyber-physical systems, Ontology, Industrial automation, Virtual engineering},
abstract = {Mastering the fusion of information and communication technologies with physical systems to cyber-physical automation systems is of main concern to engineers in the industrial automation domain. The engineering of these systems is challenging as their distributed nature and the heterogeneity of stakeholders and tools involved in their engineering contradict the need for the simultaneous engineering of their cyber and physical parts over their life cycle. This paper presents a novel approach based on the virtual engineering method, which provides support for the simultaneous engineering of the cyber and physical parts of automation systems. The approach extends and integrates the life cycle centered view mandated by current conceptual architectures and the digital twin paradigm with an integrated, iterative engineering method. The benefits of the approach are highlighted in a case study related to the engineering of the control logic of a cyber physical automation system originating from the process engineering domain. We describe for the first time a modular domain ontology, which formally describes the cyber and physical part of the system. We present cyber services built on top of the ontology layer, which allow to automatically verify different control logic types and simultaneously verify cyber and physical parts of the system in an incremental manner.}
}
@article{JANG2025110984,
title = {An explainable artificial intelligence – human collaborative model for investigating patent novelty},
journal = {Engineering Applications of Artificial Intelligence},
volume = {154},
pages = {110984},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110984},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625009844},
author = {Hyejin Jang and Byungun Yoon},
keywords = {Technology intelligence, Patent mining, Patent novelty analysis, Explainable artificial intelligence (XAI), Human–machine collaboration, Natural language processing (NLP)},
abstract = {With the accumulation of technology-related big data, including the patent database, existing studies have proposed a framework for patent analysis using natural language processing models. Artificial intelligence (AI) applications require human experience and insight based on the understanding of complex environments and uncertainties and model predictive performance. However, existing research has focused on applying big data and developing automated processes. Actual user understanding and the consideration of model usability are insufficient. Studies must consider the human–machine cooperation-based approach in developing the AI model. This study proposes a collaborative approach through which the explainable AI (XAI) model, a self-explaining deep neural network for text classification, communicates with users. The proposed XAI model provides users with an explanation for the model prediction along with the prediction results for patent evaluation. Users provide feedback based on the model predictions and their explanations. The source XAI model is refined via relearning by reflecting on user feedback. This study experiments to assess model improvement using the human collaboration method. As for the human collaborative method, this study considers the process of human intervention independent of the XAI model's results as well as the method of human participation based on the explanation presented by the XAI model. The experimental results verified the XAI model performance, showing the highest accuracy (0.890) and F1 score (0.916), such that the model can be applied efficiently to patent evaluation. The XAI–human collaboration model presented in this study can also be expanded and applied to technology intelligence tasks. However, the collaborative approach in this study has complete trust in human advice from technical experts; thus, subsequent collaborative XAI models could be improved by communicating bidirectionally with human resources as a complementary relationship.}
}
@article{ROMERO2022100297,
title = {A framework for assessing capability in organisations using enterprise models},
journal = {Journal of Industrial Information Integration},
volume = {27},
pages = {100297},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100297},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000935},
author = {Marcelo Romero and Wided Guédria and Hervé Panetto and Béatrix Barafort},
keywords = {Enterprise modelling, Capability Assessment, Requirements engineering, Business Process Model and Notation, ArchiMate},
abstract = {Organisational assessments provide a view of the state of different aspects of an enterprise so as to understand its strengths, weaknesses and possible improvement paths. A relevant element used to perform assessments is the assessment framework, which contains the requirements that the entity must fulfil as well as other aspects. On the other hand, another essential element during an assessment is the Evidence, which is any type of data reflecting the state of the assessed entity. Among several possible assessment evidences, such as documents, e-mails, interview extracts, and questionnaires, Enterprise Models represented through a specific Modelling Language can serve as such, since they allow to describe diverse organisational aspects. Thus, they can provide useful information to be considered when performing an assessment. Notwithstanding, the analysis of the models is demanding in terms of time and resources, and reducing the effort to perform such analysis could improve the assessment process. Thus, a possible manner of improving the analysis process is to understand beforehand which type of information can be found within an enterprise model taking into consideration the modelling language that has been used to represent it and the list of requirements defined by the assessment framework used to perform the assessment. In this sense, this work introduces a method to identify the requirements, devised by an assessment framework, that can be answered through the analysis of modelling elements defined using a specific modelling language. We ground our approach on a Requirement Decomposition task following the Pseudo-Requirement Graph method, and the matching between requirements and modelling language elements following the Goal Question Metric paradigm. To validate our method, we perform separate evaluations of the BPMN and ArchiMate languages, to identify the set of requirements from the ISO/IEC 33020 international standard that can be answered through models that are defined using each language. We also introduce a case study based on the analysis of a concrete business process, following the proposed approach.}
}
@article{BOJE2018103,
title = {Crowd simulation-based knowledge mining supporting building evacuation design},
journal = {Advanced Engineering Informatics},
volume = {37},
pages = {103-118},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617305803},
author = {Calin Boje and Haijiang Li},
keywords = {Knowledge mining, Crowd simulation (CS), Ontology, Evacuation design, Building Information Modelling (BIM), Industry Foundation Classes (IFC)},
abstract = {Assessing building evacuation performance designs in emergency situations requires complex scenarios which need to be prepared and analysed using crowd simulation tools, requiring significant manual input. With current procedures, every design iteration requires several simulation scenarios, leading to a complicated and time-consuming process. This study aims to investigate the level of integration between digital building models and crowd simulation, within the scope of design automation. A methodology is presented in which existing ontology tools facilitate knowledge representation and mining throughout the process. Several information models are used to integrate, automate and provide feedback to the design decision-making processes. The proposed concept thus reduces the effort required to create valid simulation scenarios by applying represented knowledge, and provides feedback based on results and design objectives. To apply and test the methodology a system was developed, which is introduced here. The context of building performance during evacuation scenarios is considered, but additional design perspectives can be included. The system development section expands on the essential theoretical concepts required and the case study section shows a practical implementation of the system.}
}
@article{BENITEZHIDALGO2021107489,
title = {TITAN: A knowledge-based platform for Big Data workflow management},
journal = {Knowledge-Based Systems},
volume = {232},
pages = {107489},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107489},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121007516},
author = {Antonio Benítez-Hidalgo and Cristóbal Barba-González and José García-Nieto and Pedro Gutiérrez-Moncayo and Manuel Paneque and Antonio J. Nebro and María del Mar Roldán-García and José F. Aldana-Montes and Ismael Navas-Delgado},
keywords = {Big Data analytics, Semantics, Knowledge extraction},
abstract = {Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.}
}
@article{KRISHNAMOORTHY202444,
title = {A novel and secured email classification and emotion detection using hybrid deep neural network},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {5},
pages = {44-57},
year = {2024},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666307424000019},
author = {Parthiban Krishnamoorthy and Mithileysh Sathiyanarayanan and Hugo Pedro Proença},
keywords = {Email classification, DNN-BiLSTM, AES algorithm, Rabit algorithm, Random forests (RF)},
abstract = {Compared to other social media data, email data differs from it in various topic-specific ways, including extensive replies, formal language, significant length disparities, high levels of anomalies, and indirect linkages. In this paper, the creation of a potent and computationally effective classifier to categorize spam and ham email documents is proposed. To assess and validate spam texts, this paper employs a variety of data mining-based classification approaches. On the benchmark Enron dataset, which is open to the public, tests were run. The final 7 Enron datasets were created by combining the six different types of Enron datasets that we had acquired. We preprocess the dataset at an early stage to exclude any useless phrases. This method falls under several categories, including Logistic Regression (LR), Convolutional Neural Networks (CNN), Random Forests (RF), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and suggested Deep Neural Networks (DNN). Using Bidirectional Long Short-Term Memory (BiLSTM), email documents may be screened for spam and labeled as such. In performance comparisons, DNN-BiLSTM outperforms other classifiers in terms of accuracy on all seven Enron datasets. In comparison to other machine learning classifiers, the findings demonstrate that DNN-BiLSTM and Convolutional Neural Networks can categorize spam with 96.39 % and 98.69 % accuracy, respectively. The report also covers the dangers of managing cloud data and the security problems that might occur. To safeguard data in the cloud while maintaining privacy, hybrid encryption is examined in this white paper. In the AES-Rabit hybrid encryption system, the symmetric session key exchange-based Rabit technique is combined with the benefits of the AES algorithm for faster data encryption.}
}
@article{BOUCHAAYA20191096,
title = {Context-aware System for Dynamic Privacy Risk Inference: Application to smart IoT environments},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {1096-1111},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19301311},
author = {Karam {Bou Chaaya} and Mahmoud Barhamgi and Richard Chbeir and Philippe Arnould and Djamal Benslimane},
keywords = {Privacy engineering, Privacy risk, Context-aware computing, Semantic reasoning, Ontology, Internet of Things},
abstract = {With the rapid expansion of smart cyber–physical systems and environments, users become more and more concerned about their privacy, and ask for more involvement in the protection of their data. However, users may not be necessarily aware of the direct and indirect privacy risks they take to properly protect their privacy. In this paper, we propose a context-aware semantic reasoning system, denoted as the Privacy Oracle, capable of providing users with a dynamic overview of the privacy risks taken as their context evolves. To do so, the system continuously models, according to a proposed Semantic User Environment Modeling (SUEM) ontology, the knowledge (received by the system) about the user of interest and his surrounding cyber–physical environment. In parallel, it performs continuous reasoning over modeled information, by relying on set of privacy rules, in order to dynamically infer the privacy risks taken by the user. To validate our approach, we developed a prototype based on the semantic web tools such as OWL API, SWRL API and the inference engine Pellet. We evaluated the system performance by considering multiple use cases. Our experimental results show that the Privacy Oracle can assist users by dynamically detecting their incurred privacy risks, and by tracking, in real-time, the evolution of those risks as user context changes.}
}
@article{CHIA2025100468,
title = {A design-based approach to analysing student engagement with a GenAI-Enabled brainstorming app},
journal = {Computers and Education: Artificial Intelligence},
volume = {9},
pages = {100468},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100468},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25001080},
author = {Joanne Chia and Angela Frattarola},
keywords = {GenAI for education, Writing assistant apps, Customised apps, Personalization in technology, Student engagement, Human-AI interactions, Design thinking, Data storytelling, Technological use and innovation in institutes of higher learning, Faculty and student collaborations},
abstract = {While there are several “writing buddy” Generative Artificial Intelligence (GenAI) apps that check grammar and language usage, not many focus exclusively on enhancing the brainstorming process for writing across disciplines. To fill this gap, a team of staff and student assistants with programming and User Interface (UI) and User Experience (UX) expertise designed and prototyped a web app named “Waai,” which rhymes with ‘why,’ that could assist students throughout the writing process for a first-year general writing module for all undergraduate students at a Singaporean university. Utilising surveys, focus group discussions, and app data that shows the nature and type of student engagement with the Waai app, this paper studies the impact of one aspect of the Waai app, an uni-directional chatbot named “Nudgy,” as a first step to optimising interactions with an AI chatbot for writing purposes. Overall, we found that students were able to benefit from the GenAI chatbot Nudgy in 5 distinct ways: 1) its pre-engineered prompts, which were tailored to the course assignment rubrics; 2) its tendency to recommend topics to research rather than give students answers; 3) its suggested research topics, which helped students to consider different perspectives on their topics; 4) how it modelled ways to ideate new insights; and 5) its constant availability. Students, however, expressed reservations about the Nudgy, particularly in terms of: 1) the limitations of pre-engineered prompts within the app; 2) difficulty in discerning the most relevant of the Nudgy feedback; 3) mistrust in GenAI and Aigiarism; and 4) a recognition of the limitations of GenAI in supporting argumentative writing. “Waai” essentially presents a decision-making framework for brainstorming based on cognitive socialisation, a method of learning that emphasises inductive as opposed to deductive experience that could be applied to online environments, as an ideology of learning that considers, among other aspects, the development of selfhood, where learning is both guided and mediated (Kesebir & Gardner, 2010). In the context of asynchronous learning, meaning is not intrinsic but rather picked up through interactions on online platforms. Interacting with a chatbot with pre-designed prompts result in a ritual that both define and explore the limits of knowledge building. Symbolic interactionism (Aksan et al., 2009) through the medium of technology is a key objective of 21st century education, where ‘learning’ is internalised as an individual experience. Waai offers educatros additional understanding of the effects of personalization (Mygland et al., 2021) as proposed by this design-based study of the role of instruction in the creation of online ‘learning’ experiences. The centrality of instruction and standards of reasoning through the process of brainstorming suggests that the developmental stages of ‘learning’ concepts could empower a process for self-regulation (Zimmerman, 1989) that goes beyond immediate causes and effects to inspire a spiral of reflection and change essential to ideation.}
}
@article{WANG2025132533,
title = {Risk Evolution Analysis of Cross-Regional Water Diversion Projects Based on Spatio-Temporal Knowledge Graphs},
journal = {Journal of Hydrology},
volume = {650},
pages = {132533},
year = {2025},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2024.132533},
url = {https://www.sciencedirect.com/science/article/pii/S0022169424019292},
author = {Lihu Wang and Xuemei Liu and Yi Dong and Dongxiao Zhao and Zhenfan Wang and Xiaonan Chen},
keywords = {Cross-regional water diversion projects, Spatio-temporal knowledge graphs, Risk evolution analysis, Complex networks, Pre-trained models},
abstract = {The safe operation environment of cross-regional water diversion projects is complex and variable, with risks showing diverse, networked, and dynamic characteristics, posing significant challenges for risk prevention and control. This study attempts to explore the spatio-temporal evolution patterns of coupled risks from a knowledge perspective based on spatio-temporal knowledge graph technology. Incorporating the temporal and spatial dimensions of risk, a risk knowledge extraction method based on pre-trained language models and semantic matching is designed to construct a spatio-temporal knowledge graph. By setting a sliding time window, aggregating identical risks within the window, the spatio-temporal knowledge graph is mapped into a risk evolution network. Investigate the dynamic topological structure of the risk evolution network, and analyze the causal correlations of risks and their evolution patterns in the spatio-temporal dimension. The results indicate that the proposed method achieves high accuracy in risk knowledge extraction (with an average F1 score of 96.65%), the causal correlations of risks significantly influence their spatio-temporal evolution patterns, and the level of causal correlation is positively correlated with the propagation and diffusion of risks. Relevant research can effectively enhance the reliability of engineering management and reduce the impact of potential risks on engineering safety.}
}
@article{LIU2024108289,
title = {MulStack: An ensemble learning prediction model of multilabel mRNA subcellular localization},
journal = {Computers in Biology and Medicine},
volume = {175},
pages = {108289},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108289},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524003731},
author = {Ziqi Liu and Tao Bai and Bin Liu and Liang Yu},
keywords = {Multilabel mRNA subcellular localization, Ensemble learning predictor, mRNA features at two levels, Position encoding, Deep learning},
abstract = {Subcellular localization of mRNA is related to protein synthesis, cell polarity, cell movement and other biological regulation mechanisms. The distribution of mRNAs in subcellulars is similar to that of proteins, and most mRNAs are distributed in multiple subcellulars. Recently, some computational methods have been designed to predict the subcellular localization of mRNA. However, these methods only employed a sin-gle level of mRNA features and did not employ the position encoding of nucleotides in mRNA. In this paper, an ensemble learning prediction model is proposed, named MulStack, which is based on random forest and deep learning for multilabel mRNA subcellular localization. The proposed method employs two levels of mRNA features, including sequence-level and residue-level features, and position encoding is employed for the first time in the field of subcellular localization of mRNA. Random forest is employed to learn mRNA sequence-level feature, deep learning is employed to learn mRNA sequence-level feature and mRNA residue-level combined with position encoding. And the outputs of random forest and deep learning model will be weighted sum as the prediction probability. Compared with existing methods, the results show that MulStack is the best in the localization of the nucleus, cytosol and exosome. In addition, position weight matrices (PWMs) are extracted by convolutional neural networks (CNNs) that can be matched with known RNA binding protein motifs. Gene ontology (GO) enrichment analysis shows biological processes, molecular functions and cellular components of mRNA genes. The prediction web server of MulStack is freely accessible at http://bliulab.net/MulStack.}
}
@article{STAPLETON2019562,
title = {A Semi-Automated Systems Architecture for Cultural Heritage: Sustainable Solutions for Digitising Cultural Heritage},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {25},
pages = {562-567},
year = {2019},
note = {19th IFAC Conference on Technology, Culture and International Stability TECIS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.606},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319325273},
author = {L. Stapleton and Brenda O’Neill and Kieran Cronin and Patrick McInerney and Matthew Hendrick and Eoin Dalton},
keywords = {Developing countries, systems development, culture},
abstract = {The loss of cultural diversity impacts international stability by undermining sustainable development goals. UNESCO is charged with promoting cultural diversity, guided by important, globally recognised, conventions. IFAC researchers have recently directed attention to the application of automation systems to the preservation of cultural heritage. This paper reports developments in digital cultural heritage and media automation. Using a human-centred approach, the paper presents a systems project which places culture at the centre of technical development. Leading metadata standards are surveyed and machine-readable ontological models proposed which can describe intangible and tangible cultural heritage as envisaged by UNESCO. This study proposes a digitisation process which encodes artefact properties in XML to be linked into the ontologies. It also synthesises an architecture to guide the work of a new research laboratory. This study embodies a new trans-disciplinary research agenda at the interface of systems engineering and the humanities.}
}
@article{ARSHAD2022558,
title = {Semantic Attribute-Based Encryption: A framework for combining ABE schemes with semantic technologies},
journal = {Information Sciences},
volume = {616},
pages = {558-576},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.10.132},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522012610},
author = {Hamed Arshad and Christian Johansen and Olaf Owe and Pablo Picazo-Sanchez and Gerardo Schneider},
keywords = {Attribute-based encryption, Semantic technologies, Security, Interoperability, Privacy, Access control, Ontology},
abstract = {Attribute-Based Encryption (ABE) is a cryptographic solution to protect resources in a fine-grained manner based on a set of public attributes. This is similar to attribute-based access control schemes in the sense that both rely on public attributes and access control policies to grant access to resources. However, ABE schemes do not consider the semantics of attributes provided by users or required by access structures. Such semantics not only improve the functionality by making proper access decisions but also enable cross-domain interoperability by making users from one domain able to access and use resources of other domains. This paper proposes a Semantic ABE (SABE) framework by augmenting a classical Ciphertext-Policy ABE (CP-ABE) scheme with semantic technologies using a generic procedure by which any CP-ABE scheme can be extended to an SABE. The proposed SABE framework is implemented in Java and the source code is publicly available. The experiment results confirm that the performance of the proposed framework is promising.}
}
@article{HORTA2018584,
title = {Analyzing scientific context of researchers and communities by using complex network and semantic technologies},
journal = {Future Generation Computer Systems},
volume = {89},
pages = {584-605},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17328431},
author = {Vitor Horta and Victor Ströele and Regina Braga and José Maria N. David and Fernanda Campos},
keywords = {Scientific social network analysis, Semantic analysis, Overlapping community detection, Clustering algorithm, Ontology, Scientific context},
abstract = {Social network communities are composed of people with common interests who influence or are influenced by themselves. In the scientific context, Scientific Social Networks are characterized as social networks that represent the social relations established by researchers. Identifying and exploring these relationships are fundamental activities to support scientific experiments. In this study, we aim to discuss the use of complex networks combined with semantic analysis in a network of scientific publications called DBLP. DBLP can be classified as big data, and its use for the analysis of connections and influences among researchers can be considered a context-aware approach. Therefore, in the present study, concepts of complex network analysis are applied to verify the level of influence among researchers, by analyzing the structure of the scientific social network under study and its communities. A bidirectional graph-based model was proposed in order to evaluate the influence of researchers, in addition to algorithms to analyze the network structure and identify scientific communities, using ontological terms and rules, considering the scientific context, and identifying new connections to promote scientific collaboration. For the identification of scientific communities, we proposed an overlapping community detection algorithm, named NetSCAN. A large scientific database (DBLP) together with digital libraries were used to evaluate the model and the algorithms in a historical research experiment. The results point to the viability and effectiveness of the proposed solution.}
}
@article{MARTINEZHUERTAS2021115621,
title = {Enhancing topic-detection in computerized assessments of constructed responses with distributional models of language},
journal = {Expert Systems with Applications},
volume = {185},
pages = {115621},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115621},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421010150},
author = {José Á. Martínez-Huertas and Ricardo Olmos and José A. León},
keywords = {Inbuilt rubric, Constructed responses, Summaries, Topic detection, Latent semantic analysis, Automated summary evaluation},
abstract = {Usually, computerized assessments of constructed responses use a predictive-centered approach instead of a validity-centered one. Here, we compared the convergent and discriminant validity of two computerized assessment methods designed to detect semantic topics in constructed responses: Inbuilt Rubric (IR) and Partial Contents Similarity (PCS). While both methods are distributional models of language and use the same Latent Semantic Analysis (LSA) prior knowledge, they produce different semantic representations. PCS evaluates constructed responses using non-meaningful semantic dimensions (this method is the standard LSA assessment of constructed responses), but IR endows original LSA semantic space coordinates with meaning. In the present study, 255 undergraduate and high school students were allocated one of three texts and were tasked to make a summary. A topic-detection task was conducted comparing IR and PCS methods. Evidence from convergent and discriminant validity was found in favor of the IR method for topic-detection in computerized constructed response assessments. In this line, the multicollinearity of PCS method was larger than the one of IR method, which means that the former is less capable of discriminating between related concepts or meanings. Moreover, the semantic representations of both methods were qualitatively different, that is, they evaluated different concepts or meanings. The implications of these automated assessment methods are also discussed. First, the meaningful coordinates of the Inbuilt Rubric method can accommodate expert rubrics for computerized assessments of constructed responses improving computer-assisted language learning. Second, they can provide high-quality computerized feedback accurately detecting topics in other educational constructed response assessments. Thus, it is concluded that: (1) IR method can represent different concepts and contents of a text, simultaneously mapping a considerable variability of contents in constructed responses; (2) IR method semantic representations have a qualitatively different meaning than the LSA ones and present a desirable multicollinearity that promotes the discriminant validity of the scores of distributional models of language; and (3) IR method can extend the performance and the applications of current LSA semantic representations by endowing the dimensions of the semantic space with semantic meanings.}
}
@article{LALANDER2025101508,
title = {Comparative reflections on contested hydro-territorial rights in Indigenous communities of Bolivia, India and Tanzania},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101508},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101508},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125002360},
author = {Rickard Lalander and Nandita Singh and J. Fernando Galindo and Faustin Maganga and Sara Sjöling and Kari Lehtilä},
keywords = {Hydro-territorial rights, Rural indigenous communities, Critical institutionalism- political ecology, Water and land rights, Bolivia, India, Tanzania},
abstract = {In Indigenous and rural communities of the Global South, relationships between humans, water, and life are understood and organized in various ways, with water often viewed as intrinsically linked to land. These resources not only serve the tangible purpose of supporting livelihoods but also form a fundamental basis for intangible aspects such as culture, identity, and epistemic-ontological foundations. In this article, the interconnected rights to both water and land for these communities are conceptualized as "hydro-territorial rights" (HTRs). This concept encompasses the formal and/or customary norms and practices related to the ownership, access, control, and use of both land and water, which are regarded as interrelated entities. Theoretically, this article draws on rights-based critical institutionalism and political ecology approaches to natural resource governance, including the legal-pluralist distinction between de jure rights on paper and de facto rights in practice. The aim is to identify and comparatively analyze contentious situations and conflicts surrounding water and land rights in rural Indigenous contexts across three postcolonial settings in the Global South. Methodologically, we employ a comparative strategy based on theory and literature reviews to examine conflictual hydro-territorial rights situations within selected Indigenous localities in Bolivia, India, and Tanzania. This analysis is complemented by interviews with local actors and observations in these three settings. Among our findings, we highlight both conflicts and temporary alliances between local and external interests, as well as practices and mechanisms related to the colonial legacy. We also explore how contemporary capitalist developmental interventions in these areas have impacted communities' access to and rights over local water and land resources, resulting in significant consequences for local livelihoods and ethno-cultural-territorial identities.}
}
@article{YENG2021,
title = {Mapping the Psychosocialcultural Aspects of Healthcare Professionals’ Information Security Practices: Systematic Mapping Study},
journal = {JMIR Human Factors},
volume = {8},
number = {2},
year = {2021},
issn = {2292-9495},
doi = {https://doi.org/10.2196/17604},
url = {https://www.sciencedirect.com/science/article/pii/S2292949521000328},
author = {Prosper Kandabongee Yeng and Adam Szekeres and Bian Yang and Einar Arthur Snekkenes},
keywords = {information security, psychological, sociocultural, health care professionals},
abstract = {Background
Data breaches in health care are on the rise, emphasizing the need for a holistic approach to mitigation efforts.
Objective
The purpose of this study was to develop a comprehensive framework for modeling and analyzing health care professionals’ information security practices related to their individual characteristics, such as their psychological, social, and cultural traits.
Methods
The study area was a hospital setting under an ongoing project called the Healthcare Security Practice Analysis, Modeling, and Incentivization (HSPAMI) project. A literature review was conducted for relevant theories and information security practices. The theories and security practices were used to develop an ontology and a comprehensive framework consisting of psychological, social, cultural, and demographic variables.
Results
In the review, a number of psychological, social, and cultural theories were identified, including the health belief model, protection motivation theory, theory of planned behavior, and social control theory, in addition to some social demographic variables, to form a comprehensive set of health care professionals’ characteristics. Furthermore, an ontology was developed from these theories to systematically organize the concepts. The framework, called the psychosociocultural (PSC) framework, was then developed from the various combined psychological and sociocultural attributes of the ontology. The Human Aspect of Information Security Questionnaire was adopted as a comprehensive tool for gathering staff security practices as mediating variables in the framework.
Conclusions
Data breaches occur often in health care today. This frequency has been attributed to the lack of experience of health care professionals in information security, the lack of development of conscious care security practices, and the lack of motivation to incentivize health care professionals. The frequent data breaches in health care threaten the mutual trust between health care professionals and patients, which implicitly impacts the quality of the health care service. The modeling and analysis of health care professionals’ security practices can be conducted with the PSC framework by combining methods of statistical survey, observations, and interviews in relation to PSC variables, such as perceptions (perceived benefits, perceived threats, and perceived barriers) or psychological traits, social factors, cultural factors, and social demographics.}
}
@article{BASSIER2020103180,
title = {Processing existing building geometry for reuse as Linked Data},
journal = {Automation in Construction},
volume = {115},
pages = {103180},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103180},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519308234},
author = {Maarten Bassier and Mathias Bonduel and Jens Derdaele and Maarten Vergauwen},
keywords = {Existing data, 3D geometry, Linked Building Data, Classification, Structure-from-Motion},
abstract = {The Web currently hosts a vast amount of 2D images and 3D building models. Each repository has its own data structure and a limited set of semantics according to their own needs. With the advent of Semantic Web Technologies, the opportunity arises to combine these heterogeneous data sets and publish them as Linked Data. It is within the scope of this research to investigate whether online 2D and 3D content can be enriched, published and reused as RDF. The emphasis of this work is on extracting building component information from online building geometry and publishing it as Linked Data. An interpretation framework is presented that takes as input any building mesh and computes its building components through machine learning techniques. Additionally, a Structure-from-Motion pipeline is proposed that provides similar outputs and links the 2D imagery to the reconstructed 3D building geometry. The experiments show that, even though the building content originates from different sources and was not modeled according to any standards, building geometry in online repositories and photogrammetric reconstructions can be semantically enriched with component information using terminology from Linked Building Data ontologies such as BOT, PRODUCT and OMG/FOG/GOM. This is an important step towards making structureless geometric information retrievable, linkable and thus reusable over the Web.}
}
@article{SPIVEYFAULKNER2021102882,
title = {Juggling sand: Ethics, identity, and archaeological geophysics in the Mississippian world},
journal = {Journal of Archaeological Science: Reports},
volume = {36},
pages = {102882},
year = {2021},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2021.102882},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X21000948},
author = {S. Margaret Spivey-Faulkner},
keywords = {Indigeneity, History of science, Identity, Geophysics, Ethics},
abstract = {This review piece evaluates the suitability of geophysical data collection methods for anthropological research on cultural identity in the Mississippian interaction sphere, which lies predominantly in the American Southeast. An understanding of an Indigenous Muskogean ontology described by Creek and Seminole philosophers, in conversation with other Indigenous theorists, is developed with emphasis on the role of all beings, including the archaeologist, within a relational web of responsibility. We then explore how this ontology suggests that the cultural lives of Mississippian peoples would be particularly visible to the methods of archaeological geophysics. The value of geophysical methods in ethical, collaborative work with descendant communities is highlighted, with special care taken to outline the dissonant bodies of concern found in Indigenous and scientific communities. Finally, the research articles presented in this special issue are reviewed, with an eye to the three concerns detailed within the previous discussion.}
}
@article{D2020571,
title = {A Novel Semantic Approach for Intelligent Response Generation using Emotion Detection Incorporating NPMI Measure},
journal = {Procedia Computer Science},
volume = {167},
pages = {571-579},
year = {2020},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.320},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920307869},
author = {Naresh Kumar D and Gerard Deepak and A Santhanavijayan},
keywords = {Chunking, Folksonomies, Role-based Ontology, Semantic Similarity, Knowledge Graph},
abstract = {Expressions and emotions are the most common way of communication in day-to-day life. In the age of Artificial Intelligence and technological advancements, the entire human race finds itself amidst many software driven voice-assistants. The only reason AI cannot excel and spread its limits is that humans can interpret, understand and express in the form of emotions and these AI-driven systems cannot. Hence, there is a need for a proper methodology for the interpretation of emotions based on both text and speech. In order to accomplish this task, a light weight computational linguistic semantic approach has been proposed for detecting emotions and generating response incorporating NPMI and NAVA words, bridging the gap between Semantics and Natural Language Processing. Experimentations are conducted for the real-word TDIL dataset for emotions such as joy, sorrow, anger, disgust, and fear. The proposed approach yields an accuracy of 96.155% for the emotion joy and 82.44 % for fear which definitely is the best-in-class accuracy for such systems.}
}
@article{DIMITROVA2021,
title = {Infrastructure and Population of the OpenBiodiv Biodiversity Knowledge Graph},
journal = {Biodiversity Data Journal},
volume = {9},
year = {2021},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.9.e67671},
url = {https://www.sciencedirect.com/science/article/pii/S1314283621000622},
author = {Mariya Dimitrova and Viktor E Senderov and Teodor Georgiev and Georgi Zhelezov and Lyubomir Penev},
abstract = {Background
OpenBiodiv is a biodiversity knowledge graph containing a synthetic linked open dataset, OpenBiodiv-LOD, which combines knowledge extracted from academic literature with the taxonomic backbone used by the Global Biodiversity Information Facility. The linked open data is modelled according to the OpenBiodiv-O ontology integrating semantic resource types from recognised biodiversity and publishing ontologies with OpenBiodiv-O resource types, introduced to capture the semantics of resources not modelled before.
New information
We introduce the new release of the OpenBiodiv-LOD attained through information extraction and modelling of additional biodiversity entities. It was achieved by further developments to OpenBiodiv-O, the data storage infrastructure and the workflow and accompanying R software packages used for transformation of academic literature into Resource Description Framework (RDF). We discuss how to utilise the LOD in biodiversity informatics and give examples by providing solutions to several competency questions. We investigate performance issues that arise due to the large amount of inferred statements in the graph and conclude that OWL-full inference is impractical for the project and that unnecessary inference should be avoided.}
}
@incollection{PREISIG2018241,
title = {Graph-Based Modelling with Distributed Systems},
editor = {Anton Friedl and Jiří J. Klemeš and Stefan Radl and Petar S. Varbanov and Thomas Wallek},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {43},
pages = {241-246},
year = {2018},
booktitle = {28th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64235-6.50043-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642356500437},
author = {Heinz A. Preisig and Arne Tobias Elve},
keywords = {Ontology, computer-aided modelling, multi-scale modelling, modelling framework},
abstract = {The importance of explicitly discussing the effects of assumptions when modelling is being put on the stage. Starting with a completely distributed system asking for simplifications puts the assumptions right in top of the table. Every assumption on transfer rates, capacity effects and what is considered and what not comes very visible with visual modelling starting with distributed systems.}
}
@article{ARDIZZONE2018326,
title = {A knowledge based architecture for the virtual restoration of ancient photos},
journal = {Pattern Recognition},
volume = {74},
pages = {326-339},
year = {2018},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2017.09.031},
url = {https://www.sciencedirect.com/science/article/pii/S0031320317303837},
author = {E. Ardizzone and H. Dindo and G. Mazzola},
keywords = {Image restoration, Historical photos, Digitization, Ontology, Knowledge base},
abstract = {Historical images are essential documents of the recent past. Nevertheless, time and bad preservation corrupt their physical supports. Digitization can be the solution to extend their “lives”, and digital techniques can be used to recover lost information. This task is often difficult and time-consuming, if commercial restoration tools are used for the purpose. A new solution is proposed to help non-expert users in restoring their damaged photos. First, we defined a dual taxonomy for the defects in printed and digitized photos. We represented our restoration domain with an ontology and we created some rules to suggest actions to perform in case of some specific events. Classes and properties of the ontology are included into a knowledge base, that grows dynamically with its use. A prototypal tool and a web application version have been implemented as an interface to the database, and to support non-expert users in the restoration process.}
}
@article{PINEDA2020102214,
title = {Practical non-monotonic knowledge-base system for un-regimented domains: A Case-study in digital humanities},
journal = {Information Processing & Management},
volume = {57},
number = {3},
pages = {102214},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102214},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319309240},
author = {Luis A. Pineda and Noé Hernández and Iván Torres and Gibrán Fuentes and Nydia {Pineda De Avila}},
keywords = {Non-monotonic knowledge-base, Un-regimented knowledge domains, Information systems for un-regimented Knowledge Domains, Digital humanities},
abstract = {Information systems for un-regimented domains such as museums, art and book collections, face representational and usability challenges that surpass the demands of traditional information systems for regimented domains. While the former require complex conceptual models supporting a set of dynamic and evolving qualitative properties of a small number of objects, the latter focus on the quantitative aspects of a possibly very large number of objects but with a relatively small and stable set of properties. In this paper we study the use of a non-monotonic knowledge-base system for the development of information systems for un-regimented domains. We discuss the ontological assumptions of the formalism, its structure and its inferential mechanisms through a simple example. Then we present an information system for a highly un-regimented domain in the digital humanities with promising results. The present study shows that the so-called extensible, flexible, dynamic or evolving information systems need the expressive power of non-monotonic knowledge-base systems, and that such phenomena should be addressed explicitly.}
}
@article{BOUGZIME2025113737,
title = {Neuro-symbolic artificial intelligence in accelerated design for 4D printing: Status, challenges, and perspectives},
journal = {Materials & Design},
volume = {252},
pages = {113737},
year = {2025},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2025.113737},
url = {https://www.sciencedirect.com/science/article/pii/S0264127525001571},
author = {Oualid Bougzime and Christophe Cruz and Jean-Claude André and Kun Zhou and H. {Jerry Qi} and Frédéric Demoly},
keywords = {Neuro-symbolic AI, Neural network, Machine learning, Symbolic AI, Inverse design, Additive manufacturing, 4D printing},
abstract = {4D printing enables the creation of adaptive and reconfigurable devices by combining additive manufacturing with smart materials. This integration introduces challenges in designing printable, responsive materials and structures. Current research focuses on improving the responsiveness and mechanical performance of smart materials, but incremental advances often lack sufficient feedback for achieving specific properties, shapes, and performance targets. Inverse design has emerged as a strategy for determining material compositions and structural configurations to meet desired outputs, but its application remains limited to simple structures. Accelerating material and structural discovery is crucial for advancing 4D printing. Artificial intelligence (AI), especially machine learning (ML), offers promising solutions to address the complexity of 4D printing design. However, conventional AI approaches often lack logical reasoning, explainability, and interpretability. This review paper highlights recent achievements and challenges in 4D printing design and introduces neuro-symbolic AI as a promising approach. By combining ML's learning capabilities with the logical reasoning and semantic understanding of symbolic AI, this approach can enhance the exploration of advanced active materials and structures. The insights provided aim to guide future research toward optimizing 4D printing for broader applications and enhanced performance.}
}
@article{REGAL2019863,
title = {Towards a conceptual model of structural and behavioral elements in cyber-physical production systems},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {863-868},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.238},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319311899},
author = {Thiago Regal and Carlos Eduardo Pereira},
keywords = {industry 4.0, conceptual model, ontology},
abstract = {In a more competitive business environment, where technology is driving major transformations, being capable of keeping up with the pace of change is crucial to remain a relevant business player. Since its introduction, Industry 4.0 (I4.0) has been aiming at solving some of these challenges by encompassing multiple concepts, which leads to a considerable challenge to integrate very distinct fields. Growing integration between Internet of Everything (IoE) and the industrial value chain, as well as the complexities involved in the integration between Cyber-Physical Systems (CPS) and Cyber-Physical Production Systems (CPPS) require a systematization of knowledge that allows the description of these elements, their integration, interoperability and realization of benefits expected from I4.0. Moreover, I4.0 elements have also behavioral aspects that lack a proper representation in conceptual models. Current semantic tools have gaps in their ability to represent these behavioral elements. It is important that this systematization also enables the precise description of the elements and their characteristics, including flexibility and change capability so the full potential of the 4th industrial revolution can be unleashed. To do so, addressing the gaps of semantic description tools, as well as proposing a methodology to include behavioral elements in conceptual description are necessary steps. This paper analyses current gaps towards a model that allows representing both structural and behavioral characteristics of I4.0 elements and suggests a few steps to address these gaps.}
}
@article{PATRIGNANI2025967,
title = {SpaceKG: Towards exploiting Knowledge Graphs in space systems},
journal = {Acta Astronautica},
volume = {236},
pages = {967-981},
year = {2025},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2025.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0094576525004278},
author = {Luca Patrignani and Eleonora Laurenza and Emanuel Sallinger and Adriano Vlad and Paolo Gaudenzi},
keywords = {Knowledge Graphs, Space systems, Model Based System Engineering, Artificial intelligence, Vadalog, Reasoning},
abstract = {Space systems consist of a vast quantity of deeply interconnected and time-evolving elements. This complexity has been steadily increasing in recent years, prompting the development of intelligent tools capable of managing information in such a scenario and transforming it into actionable knowledge. This paper aims to combine deductive artificial intelligence with space engineering, integrating space project activities with Knowledge Graphs’ semantics and operational dynamics. We propose SpaceKG, a real-time, data-driven, dynamically evolving cognitive digital twin that enables digital continuity throughout the life cycle and across the disciplines of space systems. While traditional Model Based Systems Engineering approaches focus on Knowledge Representation, they struggle with rapid adaptation to change. In contrast, our solution offers a dynamic framework to deal with faster interactions with domain experts and evolving requirements. We showcase and validate the effectiveness of the proposed approach in a specific case study for detecting failure events in the Space Shuttle Main Engine. To this aim, we leverage Vadalog, a high-performance deductive reasoning language that provides full transparency and explainability over the portion of the space system more comprehensively and flexibly than existing approaches.}
}
@article{DADZIE201851,
title = {Structuring visual exploratory analysis of skill demand},
journal = {Journal of Web Semantics},
volume = {49},
pages = {51-70},
year = {2018},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2017.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570826817300690},
author = {A.-S. Dadzie and E.M. Sibarani and I. Novalija and S. Scerri},
keywords = {Domain modeling, Knowledge discovery, Visual exploration, Ontology-guided visual analytics, Trend identification, Demand analysis},
abstract = {The analysis of increasingly large and diverse data for meaningful interpretation and question answering is handicapped by human cognitive limitations. Consequently, semi-automatic abstraction of complex data within structured information spaces becomes increasingly important, if its knowledge content is to support intuitive, exploratory discovery. Exploration of skill demand is an area where regularly updated, multi-dimensional data may be exploited to assess capability within the workforce to manage the demands of the modern, technology- and data-driven economy. The knowledge derived may be employed by skilled practitioners in defining career pathways, to identify where, when and how to update their skillsets in line with advancing technology and changing work demands. This same knowledge may also be used to identify the combination of skills essential in recruiting for new roles. To address the challenges inherent in exploring the complex, heterogeneous, dynamic data that feeds into such applications, we investigate the use of an ontology to guide structuring of the information space, to allow individuals and institutions to interactively explore and interpret the dynamic skill demand landscape for their specific needs. As a test case we consider the relatively new and highly dynamic field of Data Science, where insightful, exploratory data analysis and knowledge discovery are critical. We employ context-driven and task-centred scenarios to explore our research questions and guide iterative design, development and formative evaluation of our ontology-driven, visual exploratory discovery and analysis approach, to measure where it adds value to users’ analytical activity. Our findings reinforce the potential in our approach, and point us to future paths to build on.}
}
@article{RASMUSSEN2019102956,
title = {Managing interrelated project information in AEC Knowledge Graphs},
journal = {Automation in Construction},
volume = {108},
pages = {102956},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2019.102956},
url = {https://www.sciencedirect.com/science/article/pii/S0926580519300378},
author = {Mads Holten Rasmussen and Maxime Lefrançois and Pieter Pauwels and Christian Anker Hviid and Jan Karlshøj},
keywords = {Linked data, Building information modelling, Complex design, Ontology, Inference, Information exchange, BIM, AEC Knowledge Graph, Linked building data},
abstract = {In the architecture, engineering and construction (AEC) industry stakeholders from different companies and backgrounds collaborate in realising a common goal being some physical structure. The exact goal is typically not known from the beginning, and throughout all design stages, new decisions are made - similarly to other design industries [1]. As a result, the design must adapt and subsequent consequences follow. With working methods being predominantly document-centric, highly interrelated and rapidly changing design data in a complex network of decisions, requirements and product specifications is primarily captured in static documents. In this paper, we consider a purely data-driven approach based on semantic web technologies and an earlier proposed Ontology for Property Management (OPM). The main contribution of this work consists of extensions for OPM to account for new competency questions including the description of property reliability and the reasoning logic behind derived properties. The secondary contribution is the specification of a homogeneous way to generate parametric queries for managing an OPM-compliant AEC Knowledge Graph (AEC-KG). A software library for operating an OPM-compliant AEC-KG is further presented in the form of an OPM Query Generator (OPM-QG). The library generates SPARQL 1.1 queries to query and manipulate construction project Knowledge Graphs represented using OPM. The OPM ontology aligns with latest developments in the W3C Community Group on Linked Building Data and suggests an approach to working with design data in a distributed environment using separate graphs for explicit facts and for materialised, deduced data. Finally, we evaluate the suggested approach using an open-source software artefact developed using OPM and OPM-QG, demonstrated online with an actual building Knowledge Graph. The particular design task evaluated is performing heat loss calculations for spaces of a future building using an AEC-KG described using domain- and project specific extensions of the Building Topology Ontology (BOT) in combination with OPM. With this work, we demonstrate how a typical engineering task can be accomplished and managed in an evolving design environment, thereby providing the engineers with insights to support decision making as changes occur. The application uses a strict division between the client viewer and the actual data model holding design logic, and can easily be extended to support other design tasks.}
}
@article{WEISKOPF2020101328,
title = {Representing and coordinating ethnobiological knowledge},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {84},
pages = {101328},
year = {2020},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2020.101328},
url = {https://www.sciencedirect.com/science/article/pii/S1369848620301394},
author = {Daniel A. Weiskopf},
keywords = {Traditional ecological knowledge, Indigenous knowledge, Knowledge integration, Natural kinds, Ethnobiology, Ontology},
abstract = {Indigenous peoples possess enormously rich and articulated knowledge of the natural world. A major goal of research in anthropology and ethnobiology as well as ecology, conservation biology, and development studies is to find ways of integrating this knowledge with that produced by academic and other institutionalized scientific communities. Here I present a challenge to this integration project. I argue, by reference to ethnographic and cross-cultural psychological studies, that the models of the world developed within specialized academic disciplines do not map onto anything existing within traditional beliefs and practices for coping with nature. Traditional ecological knowledge is distributed across a heterogeneous array of overlapping practices within Indigenous cultures, including spiritual and ritual practices that invoke categories, properties, and causal-explanatory models that do not in general converge with those of the academic sciences. In light of this divergence I argue that we should abandon the integration project, and conclude by sketching a notion of knowledge coordination as a possible successor framework.}
}
@article{SCHREFL2022328,
title = {Creating an ATC knowledge graph in support of the artificial situational awareness system},
journal = {Transportation Research Procedia},
volume = {64},
pages = {328-336},
year = {2022},
note = {International Scientific Conference “The Science and Development of Transport - Znanost i razvitak prometa”},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.09.037},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522006524},
author = {Michael Schrefl and Bernd Neumayr and Sebastian Gruber and Marlene Hartmann and Ivan Tukarić and Tomislav Radišić},
keywords = {Knowledge graph, air traffic control, AIXM, FIXM, artificial situational awareness, hybrid system},
abstract = {Automation has been recognized as a possible solution for increasing air traffic controller workload trends. This paper presents a methodology for creating an air traffic control knowledge graph, which is used as part of a hybrid artificial intelligence system for air traffic control operations. The system combines machine learning and symbolic reasoning with the purpose of achieving artificial situational awareness in a narrow domain of en-route air traffic control operations. This approach allows the use of user-defined knowledge alongside existing knowledge repositories. The novel knowledge graph development methodology is universal for any area of air traffic management which relies on the aeronautical information exchange models. In this paper we also present the open-source tools which were developed to make this approach possible and system performance evaluations. Future work should address achieving real-time operation and additional task automation, accompanied by appropriate ontology and graph expansion.}
}
@article{RAHMAN2021104859,
title = {Bioinformatics and system biology approaches to identify pathophysiological impact of COVID-19 to the progression and severity of neurological diseases},
journal = {Computers in Biology and Medicine},
volume = {138},
pages = {104859},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104859},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521006533},
author = {Md Habibur Rahman and Humayan Kabir Rana and Silong Peng and Md Golam Kibria and Md Zahidul Islam and S M Hasan Mahmud and Mohammad Ali Moni},
keywords = {Bioinformatics, Transcriptomic analysis, COVID-19, Neurological diseases, Pathways, Ontology, Proteins, Semantic similarity},
abstract = {The Coronavirus Disease 2019 (COVID-19) still tends to propagate and increase the occurrence of COVID-19 across the globe. The clinical and epidemiological analyses indicate the link between COVID-19 and Neurological Diseases (NDs) that drive the progression and severity of NDs. Elucidating why some patients with COVID-19 influence the progression of NDs and patients with NDs who are diagnosed with COVID-19 are becoming increasingly sick, although others are not is unclear. In this research, we investigated how COVID-19 and ND interact and the impact of COVID-19 on the severity of NDs by performing transcriptomic analyses of COVID-19 and NDs samples by developing the pipeline of bioinformatics and network-based approaches. The transcriptomic study identified the contributing genes which are then filtered with cell signaling pathway, gene ontology, protein-protein interactions, transcription factor, and microRNA analysis. Identifying hub-proteins using protein-protein interactions leads to the identification of a therapeutic strategy. Additionally, the incorporation of comorbidity interactions score enhances the identification beyond simply detecting novel biological mechanisms involved in the pathophysiology of COVID-19 and its NDs comorbidities. By computing the semantic similarity between COVID-19 and each of the ND, we have found gene-based maximum semantic score between COVID-19 and Parkinson's disease, the minimum semantic score between COVID-19 and Multiple sclerosis. Similarly, we have found gene ontology-based maximum semantic score between COVID-19 and Huntington disease, minimum semantic score between COVID-19 and Epilepsy disease. Finally, we validated our findings using gold-standard databases and literature searches to determine which genes and pathways had previously been associated with COVID-19 and NDs.}
}
@article{BENFER2024114637,
title = {Semantic digital twin creation of building systems through time series based metadata inference – A review},
journal = {Energy and Buildings},
volume = {321},
pages = {114637},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114637},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824007539},
author = {Rebekka Benfer and Jochen Müller},
keywords = {Building operation, Building systems, Digital twin, Time series, Information extraction, Metadata inference, Data analytics, Artificial intelligence},
abstract = {Numerous applications are being developed to enhance the energy efficiency of building systems, including fault detection and diagnosis, performance assessment, and intelligent control. For these applications to be effectively utilised, a data connection between the real and virtual worlds must be established. One potential solution to establish this connection and enable semantic enrichment of data with metadata is the semantic digital twin. Semantic digital twins use semantic technologies, such as ontologies, as metadata schemas. However, creating these twins requires substantial manual effort due to the need to examine diverse sources of information about the building systems and normalise this information into a metadata schema. This review investigates whether metadata inference based on time series data from building systems can assist in the automated creation of semantic digital twins. To this end, 53 artificial intelligence-based publications on metadata inference are analyzed for their applicability and efficiency. Three key tasks of metadata inference are examined to create a semantic digital twin: type classification, relation inference, and extraction of operational information. Based on these findings, future research directions are proposed.}
}
@article{TRAN2025128505,
title = {Hybrid contextual and sentiment-based machine learning model for identifying depression risk in social media},
journal = {Expert Systems with Applications},
volume = {291},
pages = {128505},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128505},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425021244},
author = {Nha Tran and Phi Ta and Hung Nguyen and Hien D. Nguyen and Anh-Cuong Le},
keywords = {Depression detection, Mental health, Social media, Natural language processing, Deep learning},
abstract = {Depression is a dangerous and widespread mental disorder globally, often leading to feelings of low self-esteem, hopelessness, and suicide. With the rapid development of social media platforms, they have become spaces for people to share experiences and emotions and relieve stress and fatigue. Consequently, detecting depression on social media has become meaningful and consistent with development trends. However, it faces significant challenges due to the unstructured nature of social media data and the complex interaction of linguistic signals, context, and sentiment. In this paper, a novel model for detecting depressive posts on social media is proposed, called CLSDepDet. This model leverages effective feature extraction techniques, combining context, language, and sentiment features to enhance classification performance. We employ the Long Short-Term Memory (LSTM) architecture to capture linguistic and sentiment characteristics, augmented by the Hierarchical Contextual Attention Network (HCAN) to capture contextual information at both the word and sentence levels. Experimental results on a Reddit dataset demonstrate that CLSDepDet outperforms advanced methods, achieving an accuracy of 93 % and an F1 score of 95 %. The proposed model underscores the importance of integrating diverse features to improve classification accuracy and opens avenues for further research in developing efficient deep learning models for mental health applications. CLSDepDet not only provides a novel approach to detecting depressive posts on social media but also contributes to the development of early detection and diagnosis systems for depression, thereby improving the quality of life for affected individuals.}
}
@article{RIVIERE2024637,
title = {Proceedings from the inaugural Artificial Intelligence in Primary Immune Deficiencies (AIPID) conference},
journal = {Journal of Allergy and Clinical Immunology},
volume = {153},
number = {3},
pages = {637-642},
year = {2024},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0091674924000332},
author = {Jacques G. Rivière and Pere {Soler Palacín} and Manish J. Butte},
keywords = {Artificial intelligence, machine learning, large language models, natural language processing, electronic health records, inborn errors of immunity, diagnosis, ethics},
abstract = {Here, we summarize the proceedings of the inaugural Artificial Intelligence in Primary Immune Deficiencies conference, during which experts and advocates gathered to advance research into the applications of artificial intelligence (AI), machine learning, and other computational tools in the diagnosis and management of inborn errors of immunity (IEIs). The conference focused on the key themes of expediting IEI diagnoses, challenges in data collection, roles of natural language processing and large language models in interpreting electronic health records, and ethical considerations in implementation. Innovative AI-based tools trained on electronic health records and claims databases have discovered new patterns of warning signs for IEIs, facilitating faster diagnoses and enhancing patient outcomes. Challenges in training AIs persist on account of data limitations, especially in cases of rare diseases, overlapping phenotypes, and biases inherent in current data sets. Furthermore, experts highlighted the significance of ethical considerations, data protection, and the necessity for open science principles. The conference delved into regulatory frameworks, equity in access, and the imperative for collaborative efforts to overcome these obstacles and harness the transformative potential of AI. Concerted efforts to successfully integrate AI into daily clinical immunology practice are still needed.}
}
@article{WATTS2023101502,
title = {All our relations: The Grandview site and ancestral Huron-Wendat gathering logics},
journal = {Journal of Anthropological Archaeology},
volume = {70},
pages = {101502},
year = {2023},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2023.101502},
url = {https://www.sciencedirect.com/science/article/pii/S0278416523000181},
author = {Christopher Watts and Ronald F. Williamson and Louis Lesage},
keywords = {Huron-Wendat archaeology, Depositional practices, Bundling, Relational ontologies, Human-nonhuman kinship},
abstract = {Together with the development of ancestral Huron-Wendat village life in what is now southern Ontario, Canada, unusual deposits consisting of animal parts, small stones, and manufactured items such as smoking pipes were occasionally sequestered in sweat lodges, longhouse post holes, and other features. In instances where such deposits have received comment, most turn on notions of ritual behavior that betray commitments to a modernist distinction between sacred and profane acts. Such deposits, it is thought, align with the former category and can be reduced to their veiled symbolic meanings. In this paper, we seek to reframe this understanding by drawing upon Indigenous scholarship, particularly the Huron-Wendat philosophy and theology of Georges Sioui, along with the early documentary record and archaeological evidence. We argue that such deposits are better understood as the material vestiges of gatherings — concerted efforts to reveal latent yet ever-powerful beings or forces by bringing together seemingly disparate things and siting them in important places. Recast as products of an immanent rather than transcendent ontology, Huron-Wendat gatherings work against traditional notions of ritual as applied in archaeological settings. The late fourteenth through mid-fifteenth century ancestral Huron-Wendat village known as Grandview provides the case study for exploring these ideas.}
}
@article{KUTT2023119968,
title = {Loki – the semantic wiki for collaborative knowledge engineering},
journal = {Expert Systems with Applications},
volume = {224},
pages = {119968},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119968},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423004700},
author = {Krzysztof Kutt and Grzegorz J. Nalepa},
keywords = {Knowledge engineering, Semantic wiki, Software engineering, Unit tests, Prolog},
abstract = {We present Loki, a semantic wiki designed to support the collaborative knowledge engineering process with the use of software engineering methods. Designed as a set of DokuWiki plug-ins, it provides a variety of knowledge representation methods, including semantic annotations, Prolog clauses, and business processes and rules oriented to specific tasks. Knowledge stored in Loki can be retrieved via SPARQL queries, in-line Semantic MediaWiki-like queries, or Prolog goals. Loki includes a number of useful features for a group of experts and knowledge engineers developing the wiki, such as knowledge visualization, ontology storage, or code hint and completion mechanism. Reasoning unit tests are also introduced to validate knowledge quality. The paper is complemented by the formulation of the collaborative knowledge engineering process and the description of experiments performed during Loki development to evaluate its functionality. Loki is available as free software at https://loki.re.}
}
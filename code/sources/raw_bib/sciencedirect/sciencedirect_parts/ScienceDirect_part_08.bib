@article{ZOUAOUI202168,
title = {Islamic inheritance calculation system based on Arabic ontology (AraFamOnto)},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {1},
pages = {68-76},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2018.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818309959},
author = {Samia Zouaoui and Khaled Rezeg},
keywords = {Arabic ontology, Inheritance calculation, Islamic inheritance, Family relationships},
abstract = {Recently, a large number of automated applications are developed to improve the retrieval of different types of knowledge. However, there are few automated applications of semantic web technologies (ontology) for the retrieval of Islamic knowledge and in particular for Arabic language, despite the strong demand and need for this knowledge by Muslims and also by non-Muslims. In this paper, we present AraFamOnto, an Arabic ontology-based inheritance calculation system. The use of ontology is becoming increasingly important to store knowledge about the person's family relationships in order to facilitate research, the processing of information about the person and family members, and the calculation of the inheritance of the deceased person's heirs. We present a practical method to limit the time needed to process family data and reduce human effort in the search for family relationships to calculate the Islamic Inheritance correctly.}
}
@article{DAI2021102173,
title = {Ontology-based information modeling method for digital twin creation of as-fabricated machining parts},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {72},
pages = {102173},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102173},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521000570},
author = {Sheng Dai and Gang Zhao and Yong Yu and Pai Zheng and Qiangwei Bao and Wei Wang},
keywords = {Information modeling, As-fabricated data, Digital twin, Ontology, Machining part},
abstract = {The Digital Twin concept, as the cutting edge of digital manufacturing solution for modern industries, plays a significant role in the Industry 4.0 era. One key enabling technology for developing a DT is the information modeling of physical products, so as to combine the physical world with the cyberspace more extensively and closely. Therefore, the modeling approach to managing as-fabricated data of physical products, which faithfully reflects the product's physical status, emerges to be pivotal. This paper addresses the problem of modeling as-fabricated parts in the machining process, which is difficult to accomplish by relevant methods, and hinders the long-term data archiving and reuse of process data. Furthermore, to fill the gap, an ontology-based information modeling method of as-fabricated parts is proposed as the recommendation to create DTs for as-fabricated parts. It provides a simple and standardized process for companies to create DTs of as-fabricated parts by specifying the information classification, the contents to be modeled and the modeling method. To validate the effectiveness of the proposed approach, a case study is undertaken in an aviation manufacturing plant at last. The result shows that the proposed information modeling methodology is readily to DT creation of as-fabricated parts.}
}
@article{LI2022108469,
title = {Combining deep learning and ontology reasoning for remote sensing image semantic segmentation},
journal = {Knowledge-Based Systems},
volume = {243},
pages = {108469},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108469},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122001939},
author = {Yansheng Li and Song Ouyang and Yongjun Zhang},
keywords = {Collaboratively boosting framework (CBF), Deep learning, Ontology reasoning, Deep semantic segmentation network (DSSN), Remote sensing (RS) imagery},
abstract = {Because of its wide potential applications, remote sensing (RS) image semantic segmentation has attracted increasing research interest in recent years. Until now, deep semantic segmentation network (DSSN) has achieved a certain degree of success on semantic segmentation of RS imagery and can obviously outperform the traditional methods based on hand-crafted features. As a classic data-driven technique, DSSN can be trained by an end-to-end mechanism and is competent for employing low-level and mid-level cues (i.e., the discriminative image structure) to understand RS images. However, its interpretability and reliability are poor due to the nature weakness of the data-driven deep learning methods. By contrast, human beings have an excellent inference capacity and can reliably interpret RS imagery with the basic RS domain knowledge. Ontological reasoning is an ideal way to imitate and employ the domain knowledge of human beings. However, it is still rarely explored and adopted in the RS domain. As a solution of the aforementioned critical limitation of DSSN, this study proposes a collaboratively boosting framework (CBF) to combine the data-driven deep learning module and knowledge-guided ontology reasoning module in an iterative manner. The deep learning module adopts the DSSN architecture and takes the integration of the original image and inferred channels as the input of the DSSN. In addition, the ontology reasoning module is composed of intra- and extra-taxonomy reasoning. More specifically, the intra-taxonomy reasoning directly corrects misclassifications of the deep learning module based on the domain knowledge, which is the key to improve the classification performance. The extra-taxonomy reasoning aims to generate the inferred channels beyond the current taxonomy to improve the discriminative performance of DSSN in the original RS image space. On the one hand, benefiting from the referred channels from the ontology reasoning module, the deep learning module using the integration of the original image and referred channels can achieve better classification performance than only using the original image. On the other hand, better classification results from the deep learning module further improve the performance of the ontology reasoning module. As a whole, the deep learning and ontology reasoning modules are mutually boosted in the iterations. Extensive experiments on two publicly open RS datasets such as UCM and ISPRS Potsdam show that our proposed CBF can outperform the competitive baselines with a large margin.}
}
@article{LI2025129643,
title = {A Survey of Large Language Models for Data Challenges in Graphs},
journal = {Expert Systems with Applications},
pages = {129643},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.129643},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425032580},
author = {Mengran Li and Pengyu Zhang and Wenbin Xing and Yijia Zheng and Klim Zaporojets and Junzhou Chen and Ronghui Zhang and Yong Zhang and Siyuan Gong and Jia Hu and Xiaolei Ma and Zhiyuan Liu and Paul Groth and Marcel Worring},
keywords = {Graph Learning, Large Language Models, Graph Incompleteness, Data Imbalance, Cross-domain Graph Heterogeneity, Dynamic Graph Instability},
abstract = {Graphs are a widely used paradigm for representing non-Euclidean data, with applications ranging from social network analysis to biomolecular prediction. While graph learning has achieved remarkable progress, real-world graph data presents a number of challenges that significantly hinder the learning process. In this survey, we focus on four fundamental data-centric challenges: (1) Incompleteness, real-world graphs have missing nodes, edges, or attributes; (2) Imbalance, the distribution of the labels of nodes or edges and their structures for real-world graphs are highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains exhibit incompatible feature spaces or structural patterns; and (4) Dynamic Instability, graphs evolve over time in unpredictable ways. Recently, Large Language Models (LLMs) offer the potential to tackle these challenges by leveraging rich semantic reasoning and external knowledge. This survey focuses on how LLMs can address four fundamental data-centric challenges in graph-structured data, thereby improving the effectiveness of graph learning. For each challenge, we review both traditional solutions and modern LLM-driven approaches, highlighting how LLMs contribute unique advantages. Finally, we discuss open research questions and promising future directions in this emerging interdisciplinary field. To support further exploration, we have curated a repository of recent advances on graph learning challenges: https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.}
}
@article{WAN2025103212,
title = {Empowering LLMs by hybrid retrieval-augmented generation for domain-centric Q&A in smart manufacturing},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103212},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103212},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625001053},
author = {Yuwei Wan and Zheyuan Chen and Ying Liu and Chong Chen and Michael Packianather},
keywords = {Smart manufacturing, Large language model, Retrieval-augmented generation, Q&A, Knowledge graph, Additive manufacturing, Design for additive manufacturing},
abstract = {Large language models (LLMs) have shown remarkable performances in generic question-answering (QA) but often suffer from domain gaps and outdated knowledge in smart manufacturing (SM). Retrieval-augmented generation (RAG) based on LLMs has emerged as a potential approach by incorporating an external knowledge base. However, conventional vector-based RAG delivers rapid responses but often returns contextually vague results, while knowledge graph (KG)-based methods offer structured relational reasoning at the expense of scalability and efficiency. To address these challenges, a hybrid KG-Vector RAG framework that systematically integrates structured KG metadata with unstructured vector retrieval is proposed. Firstly, a metadata-enriched KG was constructed from domain corpora by systematically extracting and indexing structured information to capture essential domain-specific relationships. Secondly, semantic alignment was achieved by injecting domain-specific constraints to refine and enhance the contextual relevance of the knowledge representations. Lastly, a layered hybrid retrieval strategy was employed that combined the explicit reasoning capabilities of the KG with the efficient search power of vector-based similarity methods, and the resulting outputs were integrated via prompt engineering to generate comprehensive, context-aware responses. Evaluated on design for additive manufacturing (DfAM) tasks, the proposed approach achieved 77.8% exact match accuracy and 76.5% context precision. This study establishes a new paradigm for industrial LLM systems, which demonstrates that hybrid symbolic-neural architectures can overcome the precision-scalability trade-off in mission-critical manufacturing applications. Experimental results indicated that integrating structured KG information with vector-based retrieval and prompt engineering can enhance retrieval accuracy, contextual relevance, and efficiency in LLM-based Q&A systems for SM.}
}
@article{MATOS2022599,
title = {Use of Ontologies in Product Information Management: A Proposal for a Multinational Engineering and Technology Company},
journal = {Procedia Computer Science},
volume = {204},
pages = {599-609},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.073},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922008110},
author = {Alexandra Isabel Matos and Fernando Paulo Belfo},
keywords = {Industry 4.0, Industry 5.0, Product Information Management, PIM, ERP, CMS, e-commerce, ontology, action research},
abstract = {The PIM system importance has increased due to technical sophistication of products, their need of internal management or external publishment. ERP and CCMS systems should be integrated with a PIM system that acts as the “backbone” of product information. This project proposed an ontology-centric solution to manage product information for complex modular systems available in the catalog of one of the largest multinational organizations in engineering and technology sector. Based on action-research methodology, existing taxonomies of online product catalog at ERP and CCMS systems were analyzed, an ontology of updated taxonomies was created using Protégé tool and validated by experts. This solution was anchored in some Industry 4.0 and 5.0 pillars, by formalizing smart manufacturing knowledge in an interoperable way between multiple systems and allowing a tailored customer experience, based on interactive products and hyper customization. The organization can now be more efficient in managing individual or integrated products as complex modular systems and in communicating with customers.}
}
@incollection{BERMAN2022113,
title = {3 - Ontologies and semantics},
editor = {Jules J. Berman},
booktitle = {Classification Made Relevant},
publisher = {Academic Press},
pages = {113-154},
year = {2022},
isbn = {978-0-323-91786-5},
doi = {https://doi.org/10.1016/B978-0-323-91786-5.00002-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323917865000021},
author = {Jules J. Berman},
keywords = {Ontologies, Semantics, Semantic languages, Multi-parental inheritance, Multiclass membership, RDF, OWL, Linked data},
abstract = {Despite everything we have learned from classifications, we must recognize that they are minimalist data structures, consisting only of classes, plus the relationships among the different classes. A well-behaved classification can be represented as a simple graph consisting of arrows connecting each class to its parent class. Classifications do not necessarily come with definitions of the class, or annotations of class members. Because data scientists need a way to connect classes and class members with data (e.g., chemical reactions, genome sequences, biological processes, descriptions of subatomic particles), they have developed a language to describe objects and classes, and their properties. These languages, intended to capture the meaning of data (i.e., data semantics), allow us to construct computer programs that parse through annotated data sets, drawing logical inferences along the way. Ontologies are data structures that have been developed to provide some of the descriptive features those traditional classifications lack. In restrictive cases, ontologies have served as classifications that have been expressed in a semantic language. In general practice, ontologies have been used to greatly expand the rigid constraints imposed by traditional classifications. In this chapter, we will look at semantic logic, and how it has been used to create the modern data structures known as ontologies. We will see how ontologies are well-suited to utilize statements of meaning expressed as triples, and how any ontology can be deconstructed as a simple collection of triples. Using numerous examples, we will show that ontologies that violate the constraints of traditional classifications may lead us to draw erroneous inferences if we are not very cautious.}
}
@article{BASSILIADES201881,
title = {PaaSport semantic model: An ontology for a platform-as-a-service semantically interoperable marketplace},
journal = {Data & Knowledge Engineering},
volume = {113},
pages = {81-115},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300551},
author = {Nick Bassiliades and Moisis Symeonidis and Panagiotis Gouvas and Efstratios Kontopoulos and Georgios Meditskos and Ioannis Vlahavas},
keywords = {Cloud computing, Platform-as-a-Service, Cloud Marketplace, Semantic interoperability, Ontologies, Quality and metrics},
abstract = {PaaS is a Cloud computing service that provides a computing platform to develop, run, and manage applications without the complexity of infrastructure maintenance. SMEs are reluctant to enter the growing PaaS market due to the possibility of being locked in to a certain platform, mostly provided by the market's giants. The PaaSport Marketplace aims to avoid the provider lock-in problem by allowing Platform provider SMEs to roll out semantically interoperable PaaS offerings and Software SMEs to deploy or migrate their applications on the best-matching offering, through a thin, non-intrusive Cloud broker. In this paper, we present the PaaSport semantic model, namely an OWL ontology, extension of the DUL ontology. The ontology is used for semantically representing (a) PaaS offering capabilities and (b) requirements of applications to be deployed. The ontology has been designed to optimally support a semantic matchmaking and ranking algorithm that recommends the best-matching PaaS offering to the application developer. The DUL ontology offers seamless extensibility, since both PaaS Characteristics and parameters are defined as classes; therefore, extending the ontology with new characteristics and parameters requires the addition of new specialized subclasses of the already existing classes, which is less complicated than adding ontology properties. The PaaSport ontology is evaluated through verification tools, competency questions, human experts, application tasks and query performance tests.}
}
@article{OS2021102751,
title = {Detection of malicious Android applications using Ontology-based intelligent model in mobile cloud environment},
journal = {Journal of Information Security and Applications},
volume = {58},
pages = {102751},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102751},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621000041},
author = {Jannath Nisha O.S and Mary Saira Bhanu S},
keywords = {Mobile Cloud Computing, Malware and Benign apps, Ontology, Optimization algorithms, Machine Learning classifiers},
abstract = {Mobile Cloud Computing (MCC) is a computing model that makes mobile devices resourceful by executing mobile applications (apps) in the cloud and storing data in cloud servers. MCC faces several security threats in both the Cloud and Mobile environments. Among several threats, malicious apps are the most threatening ones, because they can perform various malicious activities in both environments. The traditional malware detection methods may not detect new types of malware or rapidly changing malware behavior. So, there is a need to develop an accurate model for detecting malicious apps in the MCC environment. Scalability and Knowledge Reusability are challenging issues in existing detection methods. To overcome these issues, the proposed model uses an effective Ontology-based intelligent model based on app permissions to detect malware apps. This model extracts the relationship between the static features from the apps and builds an Apps Feature Ontology (AFO). A concept vector set for apps is created using the items obtained from the AFO. The most discriminant features are selected using optimization algorithms like Particle Swarm Optimization, Social Spider Algorithm (SSA), and Gravitational Search Algorithm to reduce the dimension of the concept vector set. Various classifiers are applied to the reduced set. The efficiency of the proposed approach was evaluated on datasets obtained from the AndroZoo repository and VirusShare. The experimental results reveal that the proposed model can correctly detect malware using the Random Forest (RF) classifier with SSA and achieve higher detection accuracy with the lesser fall-out and less detection speed than existing Android malware detection techniques. Specifically, RF with SSA obtained higher accuracy, F1-score, and reduction in the fall-out of 94.11%, 93%, and 3%, respectively.}
}
@article{KATZER2025112186,
title = {Towards an automated workflow in materials science for combining multi-modal simulation and experimental information using data mining and large language models},
journal = {Materials Today Communications},
volume = {45},
pages = {112186},
year = {2025},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2025.112186},
url = {https://www.sciencedirect.com/science/article/pii/S2352492825006981},
author = {Balduin Katzer and Steffen Klinder and Katrin Schulz},
keywords = {Natural Language Processing (NLP), Large Language Model (LLM), Vision Transformer (ViT) Model, Data Mining, Materials Science, Plasticity, Microstructure},
abstract = {To retrieve and compare scientific data of simulations and experiments in materials science, data needs to be easily accessible and machine readable to qualify and quantify various materials science phenomena. The recent progress in open science leverages the accessibility to data. However, a majority of information is encoded within scientific documents limiting the capability of finding suitable literature as well as material properties. This manuscript showcases an automated workflow, which unravels the encoded information from scientific literature to a machine readable data structure of texts, figures, tables, equations and meta-data, using natural language processing and language as well as vision transformer models to generate a machine-readable database. The machine-readable database can be enriched with local data, as e.g. unpublished or private material data, leading to knowledge synthesis. The study shows that such an automated workflow accelerates information retrieval, proximate context detection and material property extraction from multi-modal input data exemplarily shown for the research field of microstructural analyses of face-centered cubic single crystals. Ultimately, a Retrieval-Augmented Generation (RAG) based Large Language Model (LLM) enables a fast and efficient question answering chat bot.}
}
@article{LENZERINI2020101294,
title = {Metaquerying made practical for OWL2QL ontologies},
journal = {Information Systems},
volume = {88},
pages = {101294},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2018.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0306437917303010},
author = {Maurizio Lenzerini and Lorenzo Lepore and Antonella Poggi},
abstract = {Metamodeling and metaquerying are gaining momentum in the context of both conceptual modeling and semantic web. Indeed it has been largely recognized that metamodeling represents a very useful tool to formalize complex patterns involving elements of the domain of interest, that otherwise are forced to be excluded from the modeling process, and a number of languages equipped with a spectrum of metamodeling features have been studied. However, it has been recognized as well, that the benefit of using metamodeling is greatly limited if one cannot use metaquerying as tool for extracting knowledge deriving from such meta-level patterns. Unfortunately, at the moment, no system exists that correctly manages metamodeling and metaquerying. The goal of this work is precisely to fill this gap by introducing a system, called MQ-Mastro, that allows metaquerying over ontologies expressed in a language of the OWL2 family equipped with a semantics appropriate for metamodeling.}
}
@article{LV2021107239,
title = {A novel periodic learning ontology matching model based on interactive grasshopper optimization algorithm},
journal = {Knowledge-Based Systems},
volume = {228},
pages = {107239},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107239},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121005013},
author = {Zhaoming Lv and Rong Peng},
keywords = {Ontology matching, Periodic learning, User involvement, Roulette wheel selection, Interactive grasshopper optimization algorithm},
abstract = {The ontology matching is a significant task for data integration and semantic interoperability. Although a large number of effective ontology matching methods have been proposed in a fully automated way, user involvement during the matching process is needed for real-world applications. It has been recognized as an effective method for further improving the quality of matching, especially for very precise matching cases. However, involving users during complex matching process suffers from new challenges of how to reduce the burden on users and how to increase effective interaction. In this paper, we propose a novel periodic learning ontology matching model based on interactive grasshopper optimization algorithm to address the above-mentioned issues. This new model takes into account the periodic feedback from users during the optimization process, rather than every generation, and a roulette wheel method is introduced to select the most problematic candidate mappings to present to users, not all, and to reduce the burden on users. To ensure the effectiveness of the interaction, a reward and punishment mechanism is considered for candidate mappings to propagate the feedback of user, and to guide the search direction of the algorithm. The experiments, conducted on two interactive tracks from Ontology Alignment Evaluation Initiative (OAEI), show that the proposed model significantly improve the quality of matching. Compared to other state-of-the-art matching systems, our model outperforms other methods in almost all cases with given different error rate, which makes it one of the most advanced leaders. Finally, a typical case of data integration is studied to present how the proposed approach is able to help enterprises to harmonize product catalogs.}
}
@article{SANFILIPPO2018174,
title = {Ontological foundations for feature-based modeling},
journal = {Procedia CIRP},
volume = {70},
pages = {174-179},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118300994},
author = {Emilio M. Sanfilippo},
keywords = {Ontology, Feature-based modeling, Design, Manufacturing},
abstract = {Feature-based modeling is amongst the leading approaches for Computer Aided (CAx) product modeling. Its core benefit is the use of features to embed design intents into pure geometric product models in order to convey information based on experts’ knowledge and applications requirements. Despite various attempts, the very notion of feature remains ambiguous and no promising approach has been proposed to disambiguate and possibly unify its various meanings under a common framework. As a consequence, feature-based models are tuned on specific applications, are hardly reusable across systems, and are scarcely transparent for human comprehension. The purpose of this paper is to present an ontological characterization of features that can act as backbone conceptual and computational structure to represent the meaning of feature classes in a clear manner. For this goal, the ontology formalizes the most general and fundamental properties that all features are required to satisfy. The ontology is built on previous works and integrates the notion of feature within a broader framework for product knowledge representation.}
}
@article{KAKAD2021115046,
title = {Cross domain-based ontology construction via Jaccard Semantic Similarity with hybrid optimization model},
journal = {Expert Systems with Applications},
volume = {178},
pages = {115046},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115046},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421004875},
author = {Shital Kakad and Sudhir Dhage},
keywords = {Ontology, Semantic web, Data filtering, Data annotation, Cross-domain network, Optimization},
abstract = {Semantic web technology seems to be in the infant stage as only little efforts have been taken on ontology construction with cross-domain application. This paper intends to take an effort on a new workspace, in which the ontology construction model under cross-domain application is performed. The core concern of this work is on two decision-making process namely data filtering and data annotation. Certain process is followed in this work: (i) Preprocessing (ii) Proposed Jaccard Similarity Evaluation (iii) Data filtering and Outlier Detection (iv) Semantic annotation and clustering. More particularly, data filtering is performed based on the evaluated similarity function. The outliers are identified and grouped separately. The data annotation is performed based on the semantics and thereby the clustering process takes place to form the ontology precisely. This clustering process obviously relies to the optimization crisis as the optimal centroid selection becomes the greatest issue. In order to solve this, this paper extends with the introduction of a hybrid algorithm named Circling Insisted-Rider Optimization Algorithm (CI-ROA), which hybrids the concept of Whale Optimization Algorithm (WOA) and Rider Optimization Algorithm (ROA), respectively. Finally, the performance of proposed work is compared and proved over other state-of-the-art models.}
}
@article{JALALI2024109801,
title = {Large language models in electronic laboratory notebooks: Transforming materials science research workflows},
journal = {Materials Today Communications},
volume = {40},
pages = {109801},
year = {2024},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2024.109801},
url = {https://www.sciencedirect.com/science/article/pii/S2352492824017823},
author = {Mehrdad Jalali and Yi Luo and Lachlan Caulfield and Eric Sauter and Alexei Nefedov and Christof Wöll},
keywords = {Materials science research, Natural language processing (NLP), Electronic laboratory notebooks (ELNs), Large language models (LLMs), Knowledge extraction, Scientific data management},
abstract = {In recent years, there has been a surge in research efforts dedicated to harnessing the capabilities of Large Language Models (LLMs) in various domains, particularly in material science. This paper delves into the transformative role of LLMs within Electronic Laboratory Notebooks (ELNs) for scientific research. ELNs represent a pivotal technological advancement, providing a digital platform for researchers to record and manage their experiments, data, and findings. This study explores the potential of LLMs to revolutionize fundamental aspects of science, including experimental methodologies, data analysis, and knowledge extraction within the ELN framework. We present a demonstrative showcase of LLM applications in ELN environments and, furthermore, we conduct a series of empirical evaluations to critically assess the practical impact of LLMs in enhancing research processes within the dynamic field of materials science. Our findings illustrate how LLMs can significantly elevate the quality and efficiency of research outcomes in ELNs, thereby advancing knowledge and innovation in materials science research and beyond.}
}
@article{MAYO2023533,
title = {Operational Ontology for Oncology (O3): A Professional Society-Based, Multistakeholder, Consensus-Driven Informatics Standard Supporting Clinical and Research Use of Real-World Data From Patients Treated for Cancer},
journal = {International Journal of Radiation Oncology*Biology*Physics},
volume = {117},
number = {3},
pages = {533-550},
year = {2023},
issn = {0360-3016},
doi = {https://doi.org/10.1016/j.ijrobp.2023.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0360301623005254},
author = {Charles S. Mayo and Mary U. Feng and Kristy K. Brock and Randi Kudner and Peter Balter and Jeffrey C. Buchsbaum and Amanda Caissie and Elizabeth Covington and Emily C. Daugherty and Andre L. Dekker and Clifton D. Fuller and Anneka L. Hallstrom and David S. Hong and Julian C. Hong and Sophia C. Kamran and Eva Katsoulakis and John Kildea and Andra V. Krauze and Jon J. Kruse and Tod McNutt and Michelle Mierzwa and Amy Moreno and Jatinder R. Palta and Richard Popple and Thomas G. Purdie and Susan Richardson and Gregory C. Sharp and Shiraishi Satomi and Lawrence R. Tarbox and Aradhana M. Venkatesan and Alon Witztum and Kelly E. Woods and Yuan Yao and Keyvan Farahani and Sanjay Aneja and Peter E. Gabriel and Lubomire Hadjiiski and Dan Ruan and Jeffrey H. Siewerdsen and Steven Bratt and Michelle Casagni and Su Chen and John C. Christodouleas and Anthony DiDonato and James Hayman and Rishhab Kapoor and Saul Kravitz and Sharon Sebastian and Martin {Von Siebenthal} and Walter Bosch and Coen Hurkmans and Sue S. Yom and Ying Xiao},
abstract = {Purpose
The ongoing lack of data standardization severely undermines the potential for automated learning from the vast amount of information routinely archived in electronic health records (EHRs), radiation oncology information systems, treatment planning systems, and other cancer care and outcomes databases. We sought to create a standardized ontology for clinical data, social determinants of health, and other radiation oncology concepts and interrelationships.
Methods and Materials
The American Association of Physicists in Medicine's Big Data Science Committee was initiated in July 2019 to explore common ground from the stakeholders’ collective experience of issues that typically compromise the formation of large inter- and intra-institutional databases from EHRs. The Big Data Science Committee adopted an iterative, cyclical approach to engaging stakeholders beyond its membership to optimize the integration of diverse perspectives from the community.
Results
We developed the Operational Ontology for Oncology (O3), which identified 42 key elements, 359 attributes, 144 value sets, and 155 relationships ranked in relative importance of clinical significance, likelihood of availability in EHRs, and the ability to modify routine clinical processes to permit aggregation. Recommendations are provided for best use and development of the O3 to 4 constituencies: device manufacturers, centers of clinical care, researchers, and professional societies.
Conclusions
O3 is designed to extend and interoperate with existing global infrastructure and data science standards. The implementation of these recommendations will lower the barriers for aggregation of information that could be used to create large, representative, findable, accessible, interoperable, and reusable data sets to support the scientific objectives of grant programs. The construction of comprehensive “real-world” data sets and application of advanced analytical techniques, including artificial intelligence, holds the potential to revolutionize patient management and improve outcomes by leveraging increased access to information derived from larger, more representative data sets.}
}
@article{VANDAMME201859,
title = {From lexical regularities to axiomatic patterns for the quality assurance of biomedical terminologies and ontologies},
journal = {Journal of Biomedical Informatics},
volume = {84},
pages = {59-74},
year = {2018},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2018.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S153204641830114X},
author = {Philip {van Damme} and Manuel Quesada-Martínez and Ronald Cornet and Jesualdo Tomás Fernández-Breis},
keywords = {Ontology quality assurance, Lexical regularities, Axiomatic patterns, SNOMED CT},
abstract = {Ontologies and terminologies have been identified as key resources for the achievement of semantic interoperability in biomedical domains. The development of ontologies is performed as a joint work by domain experts and knowledge engineers. The maintenance and auditing of these resources is also the responsibility of such experts, and this is usually a time-consuming, mostly manual task. Manual auditing is impractical and ineffective for most biomedical ontologies, especially for larger ones. An example is SNOMED CT, a key resource in many countries for codifying medical information. SNOMED CT contains more than 300000 concepts. Consequently its auditing requires the support of automatic methods. Many biomedical ontologies contain natural language content for humans and logical axioms for machines. The ‘lexically suggest, logically define’ principle means that there should be a relation between what is expressed in natural language and as logical axioms, and that such a relation should be useful for auditing and quality assurance. Besides, the meaning of this principle is that the natural language content for humans could be used to generate the logical axioms for the machines. In this work, we propose a method that combines lexical analysis and clustering techniques to (1) identify regularities in the natural language content of ontologies; (2) cluster, by similarity, labels exhibiting a regularity; (3) extract relevant information from those clusters; and (4) propose logical axioms for each cluster with the support of axiom templates. These logical axioms can then be evaluated with the existing axioms in the ontology to check their correctness and completeness, which are two fundamental objectives in auditing and quality assurance. In this paper, we describe the application of the method to two SNOMED CT modules, a ‘congenital’ module, obtained using concepts exhibiting the attribute Occurrence - Congenital, and a ‘chronic’ module, using concepts exhibiting the attribute Clinical course - Chronic. We obtained a precision and a recall of respectively 75% and 28% for the ‘congenital’ module, and 64% and 40% for the ‘chronic’ one. We consider these results to be promising, so our method can contribute to the support of content editors by using automatic methods for assuring the quality of biomedical ontologies and terminologies.}
}
@article{NOWROOZI2018750,
title = {Constructing an ontology based on a thesaurus},
journal = {The Electronic Library},
volume = {36},
number = {4},
pages = {750-764},
year = {2018},
issn = {0264-0473},
doi = {https://doi.org/10.1108/EL-02-2017-0037},
url = {https://www.sciencedirect.com/science/article/pii/S0264047318000838},
author = {Maryam Nowroozi and Mahdieh Mirzabeigi and Hajar Sotudeh},
keywords = {Ontologies, Thesauri, ASIS&T Thesaurus, Semantic tools},
abstract = {Purpose
Considering the shortcomings of the ASIS&T Web-based thesaurus in representing concepts and semantic relations, there is a need to use more effective semantic tools, such as ontologies. The purpose of this paper is to build a prototype ontology (ASIS&TOnto) based on a Web-based thesaurus.
Design/methodology/approach
The prototype ontology was built based on the ASIS&T Web-based thesaurus by using the methodology developed by Noy and McGuinness (2001) and with the use of Protégé 4.3.3. With regard to the purpose of the study and the massive amount of concepts represented in the ASIS&T thesaurus, the focus of term selection for creating the core source of vocabulary for the prototype ontology was the “searching” area. The knowledge engineering approach (Na and Neoh, 2008) was used to extract concepts and semantic relations. The criterion for extracting semantic relations from each concept pair was one-sentence statement, one-paragraph statement and one-page statement. Finally, the extracted relations were analysed by subject experts.
Findings
Based on the findings of this research, the possibility of using the methodology developed by Noy and McGuinness (2001) for building an ontology based on a Web-based thesaurus in the field of LIS was investigated and the prototype ontology (ASIS&TOnto) was constructed in the area of “searching”.
Practical implications
The primary implication of ASIS&TOnto is aligned with Web 3.0 research where implications of semantic modelling are a priority for community equity of access to information as a basic human right. ASIS&TOnto ends up with an ontology that is comprehensive, at least with respect to expressivity of the current ASIS&T thesaurus user interface definition languages; that is, it can be universal and can be extendable to future user interfaces that do not exist at the moment.
Originality/value
For the first time, a new enriched semantic tool (ASIS&TOnto) was constructed based on the ASIS&T Web-based thesaurus.}
}
@article{ALI2020103175,
title = {Ontology-based approach to extract product's design features from online customers’ reviews},
journal = {Computers in Industry},
volume = {116},
pages = {103175},
year = {2020},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.103175},
url = {https://www.sciencedirect.com/science/article/pii/S0166361519301836},
author = {Munira Mohd Ali and Mamadou Bilo Doumbouya and Thierry Louge and Rahul Rai and Mohamed Hedi Karray},
keywords = {, Product lifecycle management (PLM), Customers’ reviews, Sentiment analysis, Product design},
abstract = {Online customer reviews provide new potential customers with relevant information about a product or service. It has been empirically shown that the type of reviews (positive or negative) a product receives significantly impacts its future sales. In this paper, the online customers’ reviews analysis for the identification of key product attributes to be used in the conceptual design phase of a product is outlined. Our goal is to bring a value-added link between the Middle of Life phase to the Beginning of Life phase in the closed-loop product lifecycle management (PLM) by developing an ontology-based reasoning system to provide information that represents the customers’ opinions for the product's conceptual design. The main contributions of the proposed approach are the integration between the ontology and the natural language processing system in extracting the customers’ reviews data in the overall framework. The utility of the proposed approach is shown through the application on the digital camera product review dataset from Amazon.}
}
@article{LUQUEAYALA2024100081,
title = {Digital natures: New ontologies, new politics?},
journal = {Digital Geography and Society},
volume = {6},
pages = {100081},
year = {2024},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2024.100081},
url = {https://www.sciencedirect.com/science/article/pii/S2666378324000035},
author = {Andrés Luque-Ayala and Ruth Machen and Eric Nost},
abstract = {Digital tools and practices are transforming societal relationships with non-human worlds—whether through smartphone apps that city dwellers use to navigate urban forests, robotic bees that pollinate crops, or webcams that livestream rare birds' nests. Recent academic and popular interest in the coming together of digital and natural worlds has generated both creative and critical reflections on what the digital means for the very concept of nature, troubling the latter's ontological stability. In this Introduction to the special issue Digital Natures: Reworking Epistemologies, Ontologies and Politics we claim that the digital, when considered beyond an epistemological register, is a productive and political force that is unsettling, rather than reinforcing, the boundaries between society and nature. We review the extensive body of work from across geography and the social sciences that is actively engaging with digital–nature intersections, and historicise current debates through reference to the figures of the cyborg, technonatures, biomimicry and digital organisms. Asking whether digitalized practices of sensing, abstraction and algorithmic recombination simply mirror a pre-existing and external Nature, or whether they advance a reconceptualization of nature, we set out to trace the progressive political potential of a digitally-entangled ontological redefinition of nature. We discuss how, within emerging digital natures, agencies are entangled in a reimagining of what both nature and society are about. Here, we argue, lies the transformative potential of digital natures—precisely in challenging and subverting the ontological place of an external Nature. The introduction finishes by simultaneously outlining a research agenda for digital natures and presenting the six papers that comprise the special issue.}
}
@article{MUNCH2023108950,
title = {CO2 solubility and composition data of food products stored in data warehouse structured by an ontology},
journal = {Data in Brief},
volume = {47},
pages = {108950},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.108950},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923000689},
author = {Melanie Munch and Patrice Buche and Luc Menut and Julien Cufi and Valérie Guillard},
keywords = {CO2 Solubility, Ontology, Food, Compositional parameters, Knowledge graph},
abstract = {This data paper presents the values of CO2 solubility at different temperatures and main compositional parameters (protein, fat, moisture, sugars and salt content) for food products from different categories: dairy products, fishes and meats. It is the result of an extensive meta-analysis gathering the results of different major papers published on the domain on the period of 1980 to 2021, presenting the composition of 81 different food products corresponding to 362 solubility measures. For each food product, the compositional parameters were either extracted directly from the original source, or extracted from open-source databases. This dataset has also been enriched with measurements made on pure water and oil for comparison purposes. In order to ease the comparison between different sources, data have been semantized and structured by an ontology enriched with domain vocabulary. They are stored in a public repository and can be retrieved through the @Web tool, a user-friendly interface allowing to capitalize and query the data.}
}
@article{LALIS2019290,
title = {Functional modeling in safety by means of foundational ontologies},
journal = {Transportation Research Procedia},
volume = {43},
pages = {290-299},
year = {2019},
note = {INAIR 2019 - Global Trends in Aviation},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2019.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S2352146519306118},
author = {Andrej Lališ and Riccardo Patriarca and Jana Ahmad and Giulio Di Gravio and Bogdan Kostov},
keywords = {aviation safety, socio-technical systems, ontology engineering, safety engineering, resilience engineering},
abstract = {Modern theory of safety deals with systemic approach to safety, formalized in form of several systemic prediction models or methods such as FRAM (Functional Resonance Analysis Method) or STAMP (System-Theoretic Accident Model and Processes). The theory of each approach emphasizes different viewpoints to be considered in approaching various industrial safety issues. This paper focuses on FRAM and its functional viewpoint for modern complex sociotechnical systems. The methodology in this paper is based on the utilization of foundational ontologies to conceptualize the core ideas of FRAM, with the focus on the concept of functions as used in theory. The outcomes of the case study in the aviation domain provide for what needs to be determined to properly model functions in FRAM and they allow for better utilization of the method in real-case applications. The results also confirm some previous research, suggesting that modern systemic approach to safety is theoretically grounded on common - or at least complementary - tenets, to be prospectively integrated by means of ontology engineering.}
}
@article{WILDE2022398,
title = {Ontology-based approach to support life cycle engineering: Development of a data and knowledge structure},
journal = {Procedia CIRP},
volume = {105},
pages = {398-403},
year = {2022},
note = {The 29th CIRP Conference on Life Cycle Engineering, April 4 – 6, 2022, Leuven, Belgium.},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.02.066},
url = {https://www.sciencedirect.com/science/article/pii/S221282712200066X},
author = {A.-S. Wilde and F. Wanielik and M. Rolinck and M. Mennenga and T. Abraham and F. Cerdas and C. Herrmann},
keywords = {Life Cycle Engineering, Ontologies, Life Cycle Technologies},
abstract = {The data supported consideration of the whole life cycle of a product or component for an optimization of the life cycle and single life cycle stages is a promising research area in Life Cycle Engineering. Here, the acquisition of data along the life cycle is essential and can be achieved through the use of emerging so-called Life Cycle Technologies. For a value-adding use of the data and a target-oriented data acquisition it is important to derive a standardized data and knowledge structure. In this paper, an ontology-based approach is presented to support Life Cycle Engineering. The focus lies on structuring the necessary data and knowledge for approaches to optimize the whole life cycle or single life cycle stages. The approach supports an efficient, application-based use of the data by connecting the required knowledge and the acquired data and is a first step towards a generalized ontology for Life Cycle Engineering.}
}
@article{LIU2025116815,
title = {Automated extraction of materials system charts using a large language model framework},
journal = {Scripta Materialia},
volume = {267},
pages = {116815},
year = {2025},
issn = {1359-6462},
doi = {https://doi.org/10.1016/j.scriptamat.2025.116815},
url = {https://www.sciencedirect.com/science/article/pii/S1359646225002787},
author = {Quanliang Liu and Maciej P Polak and MD Al Amin Shuvo and Hrishikesh Shridhar Deodhar and Jeongsoo Han and Dane Morgan and Hyunseok Oh},
keywords = {Materials system chart, Metallurgy, Processing-Structure-Property relationships, Large language model, Prompt engineering},
abstract = {A framework leveraging large language models (LLMs) is developed to systematically extract and organize Processing-Mechanism-Structure-Mechanism-Property (P-M-S-M-P) relationships from materials science and engineering literature, with a particular focus on metallurgy. Using multi-stage prompts, our method identifies key properties, microstructures, processing methods, and associated mechanisms, then integrates them to generate comprehensive materials system charts. Additionally, the framework refines the extracted system charts for visualization, enabling the creation of informative diagrams that capture essential insights from each paper. Evaluated across 70 papers spanning multiple alloy systems and research types, the approach achieves 94 % accuracy in mechanism extraction, 87 % in information source labeling, and 97 % in the human-machine readability index for processing, structure, and property entities. The prompts and codes are provided alongside guidelines for researchers unfamiliar with coding. This framework offers an effective methodology for knowledge extraction in materials science using the P-M-S-M-P framework.}
}
@article{GERASIMOVA2022103738,
title = {A tetrachotomy of ontology-mediated queries with a covering axiom},
journal = {Artificial Intelligence},
volume = {309},
pages = {103738},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103738},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222000789},
author = {Olga Gerasimova and Stanislav Kikot and Agi Kurucz and Vladimir Podolskii and Michael Zakharyaschev},
keywords = {Ontology-mediated query, Description logic, Datalog, Disjunctive datalog, First-order rewritability, Data complexity},
abstract = {Our concern is the problem of efficiently determining the data complexity of answering queries mediated by description logic ontologies and constructing their optimal rewritings to standard database queries. Originated in ontology-based data access and datalog optimisation, this problem is known to be computationally very complex in general, with no explicit syntactic characterisations available. In this article, aiming to understand the fundamental roots of this difficulty, we strip the problem to the bare bones and focus on Boolean conjunctive queries mediated by a simple covering axiom stating that one class is covered by the union of two other classes. We show that, on the one hand, these rudimentary ontology-mediated queries, called disjunctive sirups (or d-sirups), capture many features and difficulties of the general case. For example, answering d-sirups is Π2p-complete for combined complexity and can be in Image 1 or L-, NL-, P-, or coNP-complete for data complexity (with the problem of recognising FO-rewritability of d-sirups being 2ExpTime-hard); some d-sirups only have exponential-size resolution proofs, some only double-exponential-size positive existential FO-rewritings and single-exponential-size nonrecursive datalog rewritings. On the other hand, we prove a few partial sufficient and necessary conditions of FO- and (symmetric/linear-) datalog rewritability of d-sirups. Our main technical result is a complete and transparent syntactic Image 1/NL/P/coNP tetrachotomy of d-sirups with disjoint covering classes and a path-shaped Boolean conjunctive query. To obtain this tetrachotomy, we develop new techniques for establishing P- and coNP-hardness of answering non-Horn ontology-mediated queries as well as showing that they can be answered in NL.}
}
@article{SAHADEVAN2025103141,
title = {Knowledge augmented generalizer specializer: A framework for early stage design exploration},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103141},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103141},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000345},
author = {Vijayalaxmi Sahadevan and Rohin Joshi and Kane Borg and Vishal Singh and Abhishek Raj Singh and Bilal Muhammed and Soban Babu Beemaraj and Amol Joshi},
abstract = {In non-routine engineering design projects, the design outcome is determined by how the problem is formulated and represented in the early conceptual stage. The problem representation comprises schemas, ontologies, variables, and parameters relevant to the given problem class. Despite the critical role of early conceptual decisions in shaping the eventual design outcome, most of the computational support and automation are focused on the latter stages of parametric modelling, problem-solving, and optimization. There is inadequate support for aiding and automating problem formulation, variable and parameter identification and representation, and early-stage conceptual decisions. Therefore, this paper presents an innovative, transparent, and explainable method employing semantic reasoning to automate the step-by-step conceptual design generation process, including problem formulation, identification and representation of the variables and parameters and their dependencies. The method is realized through a novel framework called Knowledge Augmented Generalizer Specializer (KAGS). KAGS employs the Function-Behavior-Structure (FBS) ontology and the Graph-of-Thought (GoT) mechanism to enable automated reasoning with a Large Language Model (LLM). The workflow comprises various stages: problem breakdown, design prototype creation, assessment, and prototype merging. The framework is implemented and tested on a Subsea Layout (SSL) planning problem, a special class of infrastructure planning projects in deep-sea oil and gas production systems. The experimentations with KAGS demonstrate its capacity to support problem formulation, hierarchical decomposition, and solution generation. The research also provides new insights into the FBS framework and meta-level reasoning in early design stages.}
}
@article{AGARONNIK2025243,
title = {Large Language Models to Identify Advance Care Planning in Patients With Advanced Cancer},
journal = {Journal of Pain and Symptom Management},
volume = {69},
number = {3},
pages = {243-250.e1},
year = {2025},
issn = {0885-3924},
doi = {https://doi.org/10.1016/j.jpainsymman.2024.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S088539242401128X},
author = {Nicole D. Agaronnik and Joshua Davis and Christopher R. Manz and James A. Tulsky and Charlotta Lindvall},
keywords = {Large language models, Advance care planning, Goals of care, Artificial intelligence},
abstract = {Context
Efficiently tracking Advance Care Planning (ACP) documentation in electronic heath records (EHRs) is essential for quality improvement and research efforts. The use of large language models (LLMs) offers a novel approach to this task.
Objectives
To evaluate the ability of LLMs to identify ACP in EHRs for patients with advanced cancer and compare performance to gold-standard manual chart review and natural language processing (NLP).
Methods
EHRs from patients with advanced cancer followed at seven Dana Farber Cancer Center (DFCI) clinics in June 2024. We utilized GPT-4o-2024-05-13 within DFCI's HIPAA-secure digital infrastructure. We designed LLM prompts to identify ACP domains: goals of care, limitation of life-sustaining treatment, hospice, and palliative care. We developed a novel hallucination index to measure production of factually-incorrect evidence by the LLM. Performance was compared to gold-standard manual chart review and NLP.
Results
60 unique patients associated with 528 notes were used to construct the gold-standard data set. LLM prompts had sensitivity ranging from 0.85 to 1.0, specificity ranging from 0.80 to 0.91, and accuracy ranging from 0.81 to 0.91 across domains. The LLM had better sensitivity than NLP for identifying complex topics such as goals of care. Average hallucination index for notes identified by LLM was less than 0.5, indicating a low probability of hallucination. Despite lower precision compared to NLP, false positive documentation identified by LLMs was clinically-relevant and useful for guiding management.
Conclusion
LLMs can capture ACP domains from EHRs, with sensitivity exceeding NLP methods for complex domains such as goals of care. Future studies should explore approaches for scaling this methodology.}
}
@article{VIANI2018140,
title = {Information extraction from Italian medical reports: An ontology-driven approach},
journal = {International Journal of Medical Informatics},
volume = {111},
pages = {140-148},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2017.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1386505617304586},
author = {Natalia Viani and Cristiana Larizza and Valentina Tibollo and Carlo Napolitano and Silvia G. Priori and Riccardo Bellazzi and Lucia Sacchi},
keywords = {Information extraction, Natural language processing},
abstract = {Objective
In this work, we propose an ontology-driven approach to identify events and their attributes from episodes of care included in medical reports written in Italian. For this language, shared resources for clinical information extraction are not easily accessible.
Materials and methods
The corpus considered in this work includes 5432 non-annotated medical reports belonging to patients with rare arrhythmias. To guide the information extraction process, we built a domain-specific ontology that includes the events and the attributes to be extracted, with related regular expressions. The ontology and the annotation system were constructed on a development set, while the performance was evaluated on an independent test set. As a gold standard, we considered a manually curated hospital database named TRIAD, which stores most of the information written in reports.
Results
The proposed approach performs well on the considered Italian medical corpus, with a percentage of correct annotations above 90% for most considered clinical events. We also assessed the possibility to adapt the system to the analysis of another language (i.e., English), with promising results.
Discussion and conclusion
Our annotation system relies on a domain ontology to extract and link information in clinical text. We developed an ontology that can be easily enriched and translated, and the system performs well on the considered task. In the future, it could be successfully used to automatically populate the TRIAD database.}
}
@article{BENITEZANDRADES2020390,
title = {An ontology-based multi-domain model in social network analysis: Experimental validation and case study},
journal = {Information Sciences},
volume = {540},
pages = {390-413},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520305909},
author = {José Alberto Benítez-Andrades and Isaías García-Rodríguez and Carmen Benavides and Héctor Alaiz-Moretón and José Emilio {Labra Gayo}},
keywords = {Ontology-based systems, Semantic web, Semantic technologies, Social network analysis, Ontology multi-domain, Knowledge-based systems},
abstract = {The use of social network theory and methods of analysis have been applied to different domains in recent years, including public health. The complete procedure for carrying out a social network analysis (SNA) is a time-consuming task that entails a series of steps in which the expert in social network analysis could make mistakes. This research presents a multi-domain knowledge model capable of automatically gathering data and carrying out different social network analyses in different domains, without errors and obtaining the same conclusions that an expert in SNA would obtain. The model is represented in an ontology called OntoSNAQA, which is made up of classes, properties and rules representing the domains of People, Questionnaires and Social Network Analysis. Besides the ontology itself, different rules are represented by SWRL and SPARQL queries. A Knowledge Based System was created using OntoSNAQA and applied to a real case study in order to show the advantages of the approach. Finally, the results of an SNA analysis obtained through the model were compared to those obtained from some of the most widely used SNA applications: UCINET, Pajek, Cytoscape and Gephi, to test and confirm the validity of the model.}
}
@article{LI2025113302,
title = {LLM4Jobs: Unsupervised occupation extraction and standardization leveraging Large Language Models},
journal = {Knowledge-Based Systems},
volume = {316},
pages = {113302},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113302},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125003491},
author = {Nan Li and Bo Kang and Tijl {De Bie}},
keywords = {Automatic occupation coding, Information retrieval, Large language model},
abstract = {Automated occupation extraction and standardization from free-text job postings and resumes are crucial for applications like job recommendation and labor market policy formation. This paper introduces LLM4Jobs, a novel unsupervised methodology that taps into the capabilities of large language models (LLMs) for occupation coding. LLM4Jobs uniquely harnesses both the natural language understanding and generation capacities of LLMs. Evaluated on rigorous experimentation on synthetic and real-world datasets, we demonstrate that LLM4Jobs consistently surpasses unsupervised state-of-the-art benchmarks, demonstrating its versatility across diverse datasets and granularities. As a side result of our work, we present both synthetic and real-world datasets, which may be instrumental for subsequent research in this domain. Overall, this investigation highlights the promise of contemporary LLMs for the intricate task of occupation extraction and standardization, laying the foundation for a robust and adaptable framework relevant to both research and industrial contexts.}
}
@article{PALOMBI201959,
title = {OntoSIDES: Ontology-based student progress monitoring on the national evaluation system of French Medical Schools},
journal = {Artificial Intelligence in Medicine},
volume = {96},
pages = {59-67},
year = {2019},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2019.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0933365718301295},
author = {Olivier Palombi and Fabrice Jouanot and Nafissetou Nziengam and Behrooz Omidvar-Tehrani and Marie-Christine Rousset and Adam Sanchez},
keywords = {Learning Management System (LMS), Ontologies, RDF, Rules, SPARQL, e-Learning, Medicine},
abstract = {We introduce OntoSIDES, the core of an ontology-based learning management system in Medicine, in which the educational content, the traces of students’ activities and the correction of exams are linked and related to items of an official reference program in a unified RDF data model. OntoSIDES is an RDF knowledge base comprised of a lightweight domain ontology that serves as a pivot high-level vocabulary of the query interface with users, and of a dataset made of factual statements relating individual entities to classes and properties of the ontology. Thanks to an automatic mapping-based data materialization and rule-based data saturation, OntoSIDES contains around 8 millions triples to date, and provides an integrated access to useful information for student progress monitoring, using a powerful query language (namely SPARQL) allowing users to express their specific needs of data exploration and analysis. Since we do not expect end-users to master the raw syntax of SPARQL and to express directly complex queries in SPARQL, we have designed a set of parametrized queries that users can instantiate through a user-friendly interface.}
}
@article{MA2025112018,
title = {A Knowledge Graph Dataset for Broiler Farming Automatically Constructed Based on a Large Language Model},
journal = {Data in Brief},
pages = {112018},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2025.112018},
url = {https://www.sciencedirect.com/science/article/pii/S2352340925007401},
author = {Nan Ma and Fantao Kong and Jifang Liu and Chenyang Zhang and Chenxv Zhao and Shanshan Cao and Wei Sun},
keywords = {Broiler farming, Large language model, Extraction of knowledge, Knowledge graph},
abstract = {With the rapid advancement of artificial intelligence, intelligent farming has become a key trend in modern agriculture. In particular, the application of intelligent systems in broiler farming is essential for enhancing production efficiency and optimizing management practices. Broiler farming is a complex process involving multiple interrelated components. However, existing knowledge graphs primarily focus on disease and prevention, making it difficult to capture the intricate interdependencies within the farming process. This limits the effectiveness of knowledge-based support in decision-making. To develop a high-quality broiler farming knowledge system, this study adopts large language modeling technology to integrate a Chinese corpus and construct a comprehensive knowledge graph dataset covering four core dimensions: broiler breeds, farming environment, feeding management, and disease prevention. The construction of the dataset involved three key stages. First, text scanning was used to extract information from farming-related literature, while web crawlers collected data from authoritative online sources. The data were then cleaned and manually validated to ensure accuracy and consistency. Second, the DeepKE knowledge extraction framework is used to automatically extract triples related to broiler farming from the text. These are then used as prompts to guide large-scale pre-trained language models (LLMs) to complete and optimize the knowledge, ultimately constructing a relatively complete knowledge graph of broiler farming. Finally, the structured knowledge was stored in a Neo4j graph database to support efficient querying and reasoning. The dataset not only provides researchers and farms with multidimensional knowledge of the broiler farming domain, but also supports visual management and analysis, enables data-driven inference through large models, and offers new approaches to optimize farming strategies and enhance production efficiency.}
}
@article{HASHEMI201828,
title = {Developing a domain ontology for knowledge management technologies},
journal = {Online Information Review},
volume = {42},
number = {1},
pages = {28-44},
year = {2018},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-07-2016-0177},
url = {https://www.sciencedirect.com/science/article/pii/S1468452718000033},
author = {Parvin Hashemi and Ameneh Khadivar and Mehdi Shamizanjani},
keywords = {Ontology, Knowledge management processes, Knowledge management strategies, Growth stage for knowledge management technology, Knowledge management technologies},
abstract = {Purpose
The purpose of this paper is to develop a new ontology for knowledge management (KM) technologies, determining the relationships between these technologies and classification of them.
Design/methodology/approach
The study applies NOY methodology – named after Natalya F. Noy who initiated this methodology. Protégé software and ontology web language are used for building the ontology. The presented ontology is evaluated with abbreviation and consistency criteria and knowledge retrieval of KM technologies by experts.
Findings
All the main concepts in the scope of KM technologies are extracted from existing literature. There are 241 words, 49 out of them are domain concepts, eight terms are about taxonomic and non-taxonomic relations, one term relates to data property and 183 terms are instances. These terms are used to develop KM technologies’ ontology based on three factors: facilitating KM processes, supporting KM strategies and the position of technology in the KM technology stage model. The presented ontology is created a common understanding in the field of KM technologies.
Research limitations/implications
Lack of specific documentary about logic behind decision making and prioritizing criteria in choosing KM technologies.
Practical implications
Uploading the presented ontology in the web environment provides a platform for knowledge sharing between experts from around the world. In addition, it helps to decide on the choice of KM technologies based on KM processes and KM strategy.
Originality/value
Among the many categories of KM technologies in literature, there is no classifying according to several criteria simultaneously. This paper contributes to filling this gap and considers KM processes, KM strategy and stages of growth for KM technologies simultaneously to choice the KM technologies and also there exists no formal ontology regarding KM technologies. This study has tried to propose a formal KM technologies’ ontology.}
}
@article{JENNER2025100183,
title = {Using large language models for narrative analysis: a novel application of generative AI},
journal = {Methods in Psychology},
volume = {12},
pages = {100183},
year = {2025},
issn = {2590-2601},
doi = {https://doi.org/10.1016/j.metip.2025.100183},
url = {https://www.sciencedirect.com/science/article/pii/S2590260125000098},
author = {Sarah Jenner and Dimitris Raidos and Emma Anderson and Stella Fleetwood and Ben Ainsworth and Kerry Fox and Jana Kreppner and Mary Barker},
keywords = {Artificial intelligence, Large language models, Narrative analysis, Story completion, Adolescent health, Health psychology},
abstract = {This study, a collaboration between the University of Southampton and Ipsos UK, aimed to develop and test a novel method for analysing qualitative data using generative artificial intelligence (AI). It compared large language model (LLM)-conducted analysis with human analysis of the same qualitative data, explored optimisation of LLMs for narrative analysis and evaluated their benefits and drawbacks. Using existing data, 138 short stories written by young people (aged 13–25 years) about social media, identity formation and food choices were analysed separately three times: by human researchers, and by two different LLMs (Claude and GPT-o1). The method was developed iteratively, combining Ipsos' artificial intelligence (AI) expertise and tools with researchers’ qualitative analysis expertise. Claude and GPT-o1 each conducted a narrative analysis of all 138 stories using the same analytic steps as the human researchers. Findings between the humans and both LLMs were then compared. Both LLMs quickly and successfully conducted a narrative analysis of the stories. Their findings were comparable to those of the human researchers and were judged by the researchers to be credible and thorough. Beyond replication, the LLMs provided additional insights into the data that enhanced the human analysis. This study highlights the significant potential benefits of LLMs to the field of qualitative research and proposes that LLMs could one day be seen as valuable tools for strengthening research quality and increasing efficiency. Additionally, this study discusses ethical concerns surrounding responsible AI use in research and proposes a framework for using LLMs in qualitative analysis.}
}
@article{DEVANAND2020100008,
title = {OntoPowSys: A power system ontology for cross domain interactions in an eco industrial park},
journal = {Energy and AI},
volume = {1},
pages = {100008},
year = {2020},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2020.100008},
url = {https://www.sciencedirect.com/science/article/pii/S2666546820300082},
author = {Aravind Devanand and Gourab Karmakar and Nenad Krdzavac and Rémy Rigo-Mariani and Y.S. {Foo Eddy} and Iftekhar A. Karimi and Markus Kraft},
keywords = {Bio-diesel plant, Description logic, Eco-Industrial Park, Industry 4.0, Knowledge base, Knowledge graph, Ontology, Optimal power flow},
abstract = {Knowledge management in multi-domain, heterogeneous industrial networks like an Eco-Industrial Park (EIP) is a challenging task. In this paper, an ontology-based management system has been proposed for addressing this challenge. It focuses on the power systems domain and provides a framework for integrating this knowledge with the other domains of an EIP. The proposed ontology, OntoPowSys is expressed using a Description Logics (DL) syntax and the OWL2 language was used to make it alive. It is then used as a part of the Knowledge Management System (KMS) in a virtual EIP called the J-Park Simulator (JPS). The advantages of the proposed approach are demonstrated by conducting two case studies on the JPS. The first case study illustrates the application of optimal power flow (OPF) in the electrical network of the JPS. The second case study plays an important role in understanding the cross-domain interactions between the chemical and electrical engineering domains in a bio-diesel plant of the JPS. These case studies are available as web services on the JPS website. The results showcase the advantages of using ontologies in the development of decision support tools. These tools are capable of taking into account contextual information on top of data during their decision-making processes. They are also able to exchange knowledge across different domains without the need for a communication interface.}
}
@article{ZHANG2025106244,
title = {Large language model-based agent Schema and library for automated building energy analysis and modeling},
journal = {Automation in Construction},
volume = {176},
pages = {106244},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106244},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002845},
author = {Liang Zhang and Xiaoqin Fu and Yanfei Li and Jianli Chen},
keywords = {Building energy analysis, Building energy modeling, Large language model, Agentic workflow, Agent schema and library},
abstract = {Large language models (LLMs) agents can function as autonomous, interactive, goal-oriented systems, but in the building energy sector, there is currently no structured paradigm that researchers and engineers can follow to create, access, and share effective LLM agents without starting from scratch. This paper introduces a JSON-based agent schema designed to structure the description of LLM agents. Additionally, the paper introduces an open-source library on GitHub that serves as a centralized repository for LLM agents designed for building energy analysis and modeling, all structured according to this schema. This library is publicly accessible, allowing users to utilize and upload agents, thereby enhancing the accessibility of LLM agents. The case studies demonstrate the schema's effectiveness with four example agents developed across different platform. These applications, developed on diverse platforms, successfully execute and seamlessly align with the proposed schema and can be reproduced without additional information beyond the schema.}
}
@article{VU2021104098,
title = {Supporting teacher scripting with an ontological model of task-technique content knowledge},
journal = {Computers & Education},
volume = {163},
pages = {104098},
year = {2021},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.104098},
url = {https://www.sciencedirect.com/science/article/pii/S0360131520302967},
author = {Thi My Hang Vu and Pierre Tchounikine},
keywords = {Learning scenarios, Scripting, Task/technique knowledge, Ontology, Classroom orchestration},
abstract = {Scripting is the phase of classroom management in which teachers build and/or fine-tune learning scenarios. When the learning objectives include task-technique knowledge, teachers face a specific difficulty: defining learning scenarios requires a holistic perspective of the different techniques, the types of tasks they address, and the interrelations between techniques and tasks, which may be highly complex for teachers. To address this issue, we developed a process for the semi-automatic elaboration of a task-technique knowledge model as an ontology, and tested it on a real scale case-study in mathematics education. We also designed interfaces that provide teachers with some access to this knowledge. A test shows that teachers found the proposal useful and useable.}
}
@article{HAN2018110,
title = {The uncertainty mapping of ontologies based on three-dimensional combination weight vector space model},
journal = {Information Discovery and Delivery},
volume = {46},
number = {2},
pages = {110-119},
year = {2018},
issn = {2398-6247},
doi = {https://doi.org/10.1108/IDD-02-2017-0008},
url = {https://www.sciencedirect.com/science/article/pii/S2398624718000110},
author = {Dongmei Han and Wen Wang and Suyuan Luo and Weiguo Fan and Songxin Wang},
keywords = {VSM, Similarity, Ontology, Semantic similarity, Uncertainty mapping, Vector space model},
abstract = {Purpose
This paper aims to apply vector space model (VSM)-PCR model to compute the similarity of Fault zone ontology semantics, which verified the feasibility and effectiveness of the application of VSM-PCR method in uncertainty mapping of ontologies.
Design/methodology/approach
The authors first define the concept of uncertainty ontology and then propose the method of ontology mapping. The proposed method fully considers the properties of ontology in measuring the similarity of concept. It expands the single VSM of concept meaning or instance set to the “meaning, properties, instance” three-dimensional VSM and uses membership degree or correlation to express the level of uncertainty.
Findings
It provides a relatively better accuracy which verified the feasibility and effectiveness of VSM-PCR method in treating the uncertainty mapping of ontology.
Research limitations/implications
The future work will focus on exploring the similarity measure and combinational methods in every dimension.
Originality/value
This paper presents an uncertain mapping method of ontology concept based on three-dimensional combination weighted VSM, namely, VSM-PCR. It expands the single VSM of concept meaning or instance set to the “meaning, properties, instance” three-dimensional VSM. The model uses membership degree or correlation which is used to express the degree of uncertainty; as a result, a three-dimensional VSM is obtained. The authors finally provide an example to verify the feasibility and effectiveness of VSM-PCR method in treating the uncertainty mapping of ontology.}
}
@article{DELMARROLDANGARCIA2021102018,
title = {Ontology-driven approach for KPI meta-modelling, selection and reasoning},
journal = {International Journal of Information Management},
volume = {58},
pages = {102018},
year = {2021},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2019.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S026840121930581X},
author = {María {del Mar Roldán-García} and José García-Nieto and Alejandro Maté and Juan Trujillo and José F. Aldana-Montes},
keywords = {Ontology, KPI Modelling, Semantics, Reasoning, Knowledge extraction, Water management},
abstract = {A key challenge in current Business Analytics (BA) is the selection of suitable indicators for business objectives. This requires the exploration of business data through data-driven approaches, while modelling business strategies together with domain experts in order to represent domain knowledge. In particular, Key Performance Indicators (KPIs) allow human experts to properly model ambiguous enterprise goals by means of quantitative variables with numeric ranges and clear thresholds. Besides business-related domains, the usefulness of KPIs has been shown in multiple domains, such as: Education, Healthcare and Agriculture. However, finding accurate KPIs for a given strategic goal still remains a complex task, specially due to the discrepancy between domain assumptions and data facts. In this regard, the semantic web emerges as a powerful technology for knowledge representation and data modeling through explicit representation formats and standards such as RDF(S) and OWL. By using this technology, the semantic annotation of indicators of business objectives would enrich the strategic model obtained. With this motivation, an ontology-driven approach is proposed to formally conceptualize essential elements of indicators, covering: performance, results, measures, goals and relationships of a given business strategy. In this way, all the data involved in the selection and analysis of KPIs are then integrated and stored in common repositories, hence enabling sophisticated querying and reasoning for semantic validation. The proposed semantic model is evaluated on a real-world case study on water management. A series of data analysis and reasoning tasks are conducted to show how the ontological model is able to detect semantic conflicts in actual correlations of selected indicators.}
}
@article{PRIETOGONZALEZ2020103563,
title = {Automated generation of decision-tree models for the economic assessment of interventions for rare diseases using the RaDiOS ontology},
journal = {Journal of Biomedical Informatics},
volume = {110},
pages = {103563},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103563},
url = {https://www.sciencedirect.com/science/article/pii/S153204642030191X},
author = {David Prieto-González and Iván Castilla-Rodríguez and Evelio González and María L. Couce},
keywords = {Economic assessment, Ontology, Rare diseases, Simulation, Decision tree},
abstract = {Objective:
The development of decision models to assess interventions for rare diseases require huge efforts from research groups, especially regarding collecting and synthesizing the knowledge to parameterize the model. This article presents a method to reuse the knowledge collected in an ontology to automatically generate decision tree models for different contexts and interventions.
Material and methods:
We updated the reference ontology (RaDiOS) to include more knowledge required to generate a model. We implemented a transformation tool (RaDiOS-MTT) that uses the knowledge stored in RaDiOS to automatically generate decision trees for the economic assessment of interventions on rare diseases.
Results:
We used a case study to illustrate the potential of the tool, and automatically generate a decision tree that reproduces an actual study on newborn screening for profound biotinidase deficiency.
Conclusions:
RaDiOS-MTT allows research groups to reuse the evidence collected, and thus speeding up the development of health economics assessments for interventions on rare diseases.}
}
@article{JEON2025103076,
title = {Hybrid large language model approach for prompt and sensitive defect management: A comparative analysis of hybrid, non-hybrid, and GraphRAG approaches},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103076},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103076},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007274},
author = {Kahyun Jeon and Ghang Lee},
keywords = {Housing defect management, Large language model (LLM), Question–answering (QA), Fine-tuning, Graph-retrieval augmented generation (GraphRAG), Synthetic data generation},
abstract = {This study aims to propose a large language model (LLM)-enhanced defect question-answering (QA) method that can secure private and sensitive data while yielding high performance. Prompt responses to residents’ complaints are crucial for preventing recurring defects. However, traditional defect analysis and response methods rely on the expertise of a few skilled workers, making it difficult to ensure timely responses. The rapid advancement of LLMs offers a potential solution for improving defect QA tasks. However, many companies prohibit the use of closed-source LLM services, such as ChatGPT, due to concerns about potential data breaches. One possible solution is to use open-source LLMs like Llama and BERT, which can be locally installed and used. However, open-source LLMs typically perform worse than closed-source LLMs. Although the performance of open-source LLMs can be greatly improved through fine-tuning, the preparation of training datasets requires a significant amount of time and labor. To address these challenges, this study proposes a hybrid defect QA method that deploys an open-source LLM for defect management to secure sensitive information, and a closed-source LLM for generating a training dataset to reduce both the time and labor required. To validate the proposed method, we compare it to the state-of-the-art LLMs, GPT-4o and Llama 3, as well as graph retrieval-augmented generation (GraphRAG)-based QA systems, which have been extensively studied recently. Our results show that the hybrid LLM-based QA method achieved the highest ROUGE score of 81.6%. These findings demonstrate superior practical applicability, enabling cost-effective data generation and reliable domain adaptation within a secure data environment. This approach is beneficial for domain-specific tasks beyond defect management, where the accurate provision of specialized information and integration of historical knowledge are essential.}
}
@article{YU2025104844,
title = {Evaluating large language models for information extraction from gastroscopy and colonoscopy reports through multi-strategy prompting},
journal = {Journal of Biomedical Informatics},
volume = {168},
pages = {104844},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104844},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000735},
author = {Zhengqiu Yu and Lexin Fang and Yueping Ding and Yan Shen and Lei Xu and Yaozheng Cai and Xiangrong Liu},
keywords = {Large language models, Medical information extraction, Endoscopic reports, Prompt engineering, Clinical natural language processing},
abstract = {Objective:
To systematically evaluate large language models (LLMs) for automated information extraction from gastroscopy and colonoscopy reports through prompt engineering, addressing their ability to extract structured information, recognize complex patterns, and support diagnostic reasoning in clinical contexts.
Methods:
We developed an evaluation framework incorporating three hierarchical tasks: basic entity extraction, pattern recognition, and diagnostic assessment. The study utilized a dataset of 162 endoscopic reports with structured annotations from clinical experts. Various language models, including proprietary, emerging, and open-source alternatives, were evaluated under both zero-shot and few-shot learning paradigms. For each task, multiple prompting strategies were implemented, including direct prompting and five Chain-of-Thought (CoT) prompting variants.
Results:
Larger models with specialized architectures achieved better performance in entity extraction tasks but faced notable challenges in capturing spatial relationships and integrating clinical findings. The effectiveness of few-shot learning varied across models and tasks, with larger models showing more consistent improvement patterns.
Conclusion:
These findings provide important insights into the current capabilities and limitations of language models in specialized medical domains, contributing to the development of more effective clinical documentation analysis systems.}
}
@article{SAHA2019417,
title = {Core domain ontology for joining processes to consolidate welding standards},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {59},
pages = {417-430},
year = {2019},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518302126},
author = {S. Saha and Z. Usman and W.D. Li and S. Jones and N. Shah},
keywords = {Ontology, Joining, Welding, Interoperability},
abstract = {Extensive advancement in the field of the joining science has led to the development of a wide range of joining processes and techniques. Welding is one of the most widely used joining processes. Various standards have been developed to regularise welding processes and ensure manufacturing consistency. However, there are many inconsistencies and incoherencies in terms of process categorisation within and across the welding standards. It is imperative to undertake investigations on the limitations and issues of the current welding standards in semantic inconsistency, and then to reveal the inadequacy of text based definitions of welding concepts. To address the issues, in this paper, core domain ontology for joining processes (CDOJP), that can semantically categorise joining processes to reconcile semantic inconsistency, is developed. An ontological approach using Web Ontology Language (OWL) is used to define the ontology. The generic nature of the proposed ontology allows it to be applicable to various joining processes. The proposed ontology is further consolidated to the welding standards so that semantic inconsistencies in the standards can be effectively resolved. This research is validated in an industrial environment via collaboration with one of the largest aero engine manufacturing companies. The recommendations from this research are useful for the international standards committees to improve the joining and welding standards.}
}
@article{YANG2020104437,
title = {Construction of logistics financial security risk ontology model based on risk association and machine learning},
journal = {Safety Science},
volume = {123},
pages = {104437},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0925753519313050},
author = {Bo Yang},
keywords = {Logistics finance risk, Ontology, Semantic parsing, Apriori, Risk association},
abstract = {Previous research on logistics financial risk pre-warning and pre-control focuses on the linear causal relationship between risk and risk events. In fact, risk events in logistics financial field are often caused by multiple risk factors, which are directly or indirectly related to these risk factors. Therefore, it is helpful for the healthy development of logistics finance to find out the related risks of each logistics financial risk event and screen and control them one by one. This paper proposes OntoLFR (Logistics Financial Risk Ontology), and constructs the logistics financial risk ontology model to adapt to the variability, complexity and relevance of risk in early warning and pre-control. Then, based on the risk source association inference rules obtained by knowledge association analysis, Apriori algorithm is adopted to conduct association analysis on the risk hidden danger database, and the acquired association rules are reintroduced into the knowledge ontology database of risk event source to realize self-learning and self-correction of the knowledge ontology database. Taking the risk event (RW_risk) of the financing enterprise to escape, the feasibility of using the logistics financial risk ontology model for risk-related reasoning and analysis is verified.}
}
@article{KUDRYAVTSEV2020500,
title = {Modelling Consumer Knowledge: the Role of Ontology},
journal = {Procedia Computer Science},
volume = {176},
pages = {500-507},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.08.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920318767},
author = {D. Kudryavtsev and T. Gavrilova and M. Smirnova and K. Golovacheva},
keywords = {consumer knowledge, consumer behaviour, knowledge management, innovative products, services, knowledge economy, ontology},
abstract = {Knowledge economy and further development of the information society made knowledge of all market interaction participants a key factor of consumption, adding value, including joint generation of value and innovation. Alongside with general increase in information volumes and decrease in consumer trust to it, the very products and services, as well as consumption technology and culture have become more complex and, thus, demonstrate relevance of managing consumer knowledge. Such complexity requires to teach consumers and to exchange knowledge with them. Consumer knowledge is of paramount importance for innovative products and services, as it is a key factor of innovation-decision process. Consumer knowledge practice needs clear understanding of this concept ("consumer knowledge"), its kinds and features, processes of acquiring and changing this knowledge, its influence on consumer behaviour, as well as company’s capabilities to establish consumer knowledge. Such understanding will be provided by creating ontology of innovative products and services’ consumer knowledge. Such ontology will help to resolve a whole range of enterprise engineering tasks: design of innovative products and services, as well as ecosystem surrounding them; design of an interaction system between a company and a consumer during a whole customer journey. This paper describes main requirements on ontology, discusses some existing ontologies, as well as contains primary results of ontology conceptualisation.}
}
@article{NUNEZ2018746,
title = {OntoProg: An ontology-based model for implementing Prognostics Health Management in mechanical machines},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {746-759},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617306080},
author = {David Lira Nuñez and Milton Borsato},
keywords = {Prognostics Health Management, Failure analysis, Ontology engineering},
abstract = {Trends in Prognostics Health Management (PHM) have been introduced into mechanical items of manufacturing systems to predict Remaining Useful Life (RUL). PHM as an estimate of the RUL allows Condition-based Maintenance (CBM) before a functional failure occurs, avoiding corrective maintenance that generates unnecessary costs on production lines. An important factor for the implementation of PHM is the correct data collection for monitoring a machine’s health, in order to evaluate its reliability. Data collection, besides providing information about the state of degradation of the machine, also assists in the analysis of failures for intelligent interventions. Thus, the present work proposes the construction of an ontological model for future applications such as expert system in the support in the correct decision-making, besides assisting in the implementation of the PHM in several manufacturing scenarios, to be used in the future by web semantics tools focused on intelligent manufacturing, standardizing its concepts, terms, and the form of collection and processing of data. The methodological approach Design Science Research (DSR) is used to guide the development of this study. The model construction is achieved using the ontology development 101 procedure. The main result is the creation of the ontological model called OntoProg, which presents: a generic ontology addressing by international standards, capable of being used in several types of mechanical machines, of different types of manufacturing, the possibility of storing the knowledge contained in events of real activities that allow through consultations in SPARQL for decision-making which enable timely interventions of maintenance in the equipment of a real industry. The limitation of the work is that said model can be implemented only by specialists who have knowledge in ontology.}
}
@article{UMAVATHY2020107691,
title = {Ontology based conceptual models for predicting fundamental organic reactivity},
journal = {Journal of Molecular Graphics and Modelling},
volume = {100},
pages = {107691},
year = {2020},
issn = {1093-3263},
doi = {https://doi.org/10.1016/j.jmgm.2020.107691},
url = {https://www.sciencedirect.com/science/article/pii/S1093326320304800},
author = {K. Umavathy and P. Sankar},
keywords = {Conceptual chemistry, Fundamental organic reactions, Knowledge representation, Ontology, Organic reactivity, Terminal group},
abstract = {Generic knowledge, related to fundamental organic reactivity is conceptualized and organized into model concept network resource. A concept of ‘terminal group’ is conceived to represent and encode reactivity of fundamental organic reactions such as substitution, elimination, addition, oxidation, reduction and condensation reactions. Chemical ontologies namely reactivity ontology, terminal group ontology, reagent ontology and skeletal carbon ontology are created to serve as domain knowledge repository. Graphical user interface is developed to compose various concepts of chemical ontologies to build generic reactivity axiom definitions in XML. Using the extendable and editable reactivity definitions, synthetic and retro-synthetic predictions are demonstrated with model applications.}
}
@article{MOHAMMAD20221201,
title = {New Ontology structure for intelligent controlling of traffic signals},
journal = {Procedia Computer Science},
volume = {207},
pages = {1201-1211},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.176},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922010584},
author = {Mahmud Abdulla Mohammad and Kamaran H. Manguri and Taib Shamsadin Abdulsamad and Abdulbasit K. Faeq Al-Talabani and Akam Aziz Abdulrahman},
keywords = {Ontologies, Traffic Light Control, Transportation, Semantic Segmentatio, Object Detection},
abstract = {This article proposes a novel ontology design for intelligent controlling of traffic signals, considering the investigated factors, crowded factors, road factors, visibility conditions, and emergency situations. Essentially, the proposed method uses video-based knowledge and key feature from a monocular video camera only, capturing footage from either a traffic signals perspective or the top of the road lane. The key factors and entities in the traffic scene are formed into an ontology, which has been evaluated using synthetic datasets to interpret challenging cases. Semantic features related to the key factors in the scene are obtained and fed to the ontology. The experimental results indicate that the proposed method is capable of controlling traffic signals more efficiently than the fixed intervals protocol.}
}
@incollection{GRECO2019785,
title = {Ontology: Introduction},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {785-789},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20392-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338203920},
author = {Gianluigi Greco and Marco Manna and Francesco Ricca},
keywords = {Formal languages, Knowledge representation, Languages, Ontologies, Ontology design, Tools},
abstract = {The concept of ontology plays a crucial role in the design of information systems that are knowledge intensive and that provide advanced capabilities for representing and reasoning on concepts and on data. The article provides a gentle introduction to this wide area of research, by addressing a few natural questions that arise when dealing with ontologies, by overviewing a number of concrete languages for ontology specification, and by discussing the use of ontologies in Bioinformatics and Biology.}
}
@article{DANIALI2023102523,
title = {Enriching representation learning using 53 million patient notes through human phenotype ontology embedding},
journal = {Artificial Intelligence in Medicine},
volume = {139},
pages = {102523},
year = {2023},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2023.102523},
url = {https://www.sciencedirect.com/science/article/pii/S0933365723000374},
author = {Maryam Daniali and Peter D. Galer and David Lewis-Smith and Shridhar Parthasarathy and Edward Kim and Dario D. Salvucci and Jeffrey M. Miller and Scott Haag and Ingo Helbig},
keywords = {Human phenotype ontology, Representation learning, Dimension reduction, Electronic health record, Phenotype embedding, Patient similarity},
abstract = {The Human Phenotype Ontology (HPO) is a dictionary of >15,000 clinical phenotypic terms with defined semantic relationships, developed to standardize phenotypic analysis. Over the last decade, the HPO has been used to accelerate the implementation of precision medicine into clinical practice. In addition, recent research in representation learning, specifically in graph embedding, has led to notable progress in automated prediction via learned features. Here, we present a novel approach to phenotype representation by incorporating phenotypic frequencies based on 53 million full-text health care notes from >1.5 million individuals. We demonstrate the efficacy of our proposed phenotype embedding technique by comparing our work to existing phenotypic similarity-measuring methods. Using phenotype frequencies in our embedding technique, we are able to identify phenotypic similarities that surpass current computational models. Furthermore, our embedding technique exhibits a high degree of agreement with domain experts' judgment. By transforming complex and multidimensional phenotypes from the HPO format into vectors, our proposed method enables efficient representation of these phenotypes for downstream tasks that require deep phenotyping. This is demonstrated in a patient similarity analysis and can further be applied to disease trajectory and risk prediction.}
}
@article{DAPAZ2025100210,
title = {Development of a software architecture for bioprocess modeling},
journal = {Digital Chemical Engineering},
volume = {14},
pages = {100210},
year = {2025},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2024.100210},
url = {https://www.sciencedirect.com/science/article/pii/S2772508124000723},
author = {Priscila Marques {da Paz} and Caroline Satye Martins Nakama and Galo Antonio Carrillo {Le Roux}},
keywords = {Bioprocess, Ontology, Modeling, Unified modeling language},
abstract = {Increasing the productivity of a biotechnological process becomes feasible through the development of Process Systems Engineering tools, which integrate experimental data with mathematical modeling. This work aims to develop a software architecture for modeling bioprocesses that is accessible to a multidisciplinary group. To achieve this aim, the software must be thoroughly designed based on an ontology that describes bioprocesses that can be apprehended by researchers from different fields. The ontological representation is carried out using Unified Modeling Language diagrams, whose use is demonstrated by a parameter estimation case study. It is concluded that good software development practices can be provided through the proposed architecture, since it guides simulations and parameter estimations of biotechnological processes in a structured way.}
}
@article{MUNIR2018116,
title = {The use of ontologies for effective knowledge modelling and information retrieval},
journal = {Applied Computing and Informatics},
volume = {14},
number = {2},
pages = {116-126},
year = {2018},
issn = {2210-8327},
doi = {https://doi.org/10.1016/j.aci.2017.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S2210832717300649},
author = {Kamran Munir and M. {Sheraz Anjum}},
keywords = {Information systems, Ontology, Domain knowledge, Database, Information retrieval, Knowledge management},
abstract = {The dramatic increase in the use of knowledge discovery applications requires end users to write complex database search requests to retrieve information. Such users are not only expected to grasp the structural complexity of complex databases but also the semantic relationships between data stored in databases. In order to overcome such difficulties, researchers have been focusing on knowledge representation and interactive query generation through ontologies, with particular emphasis on improving the interface between data and search requests in order to bring the result sets closer to users research requirements. This paper discusses ontology-based information retrieval approaches and techniques by taking into consideration the aspects of ontology modelling, processing and the translation of ontological knowledge into database search requests. It also extensively compares the existing ontology-to-database transformation and mapping approaches in terms of loss of data and semantics, structural mapping and domain knowledge applicability. The research outcomes, recommendations and future challenges presented in this paper can bridge the gap between ontology and relational models to generate precise search requests using ontologies. Moreover, the comparison presented between various ontology-based information retrieval, database-to-ontology transformations and ontology-to-database mappings approaches provides a reference for enhancing the searching capabilities of massively loaded information management systems.}
}
@article{KAMBLE2022,
title = {Hybrid Method for Semantic Similarity Computation Using Weighted Components in Ontology},
journal = {International Journal of Software Innovation},
volume = {10},
number = {1},
year = {2022},
issn = {2166-7160},
doi = {https://doi.org/10.4018/IJSI.309734},
url = {https://www.sciencedirect.com/science/article/pii/S2166716022001357},
author = {Kanishka N. Kamble and Suresh K. Shirgave},
keywords = {DBpedia, Information Content, Knowledge-Based Methods, Ontology, PLICD, Semantic Similarity, Weighted Shortest Path, WordNet},
abstract = {ABSTRACT
In this paper, the researchers propose an approach to measure the semantic similarity between two concepts in an ontology like WordNet and DBpedia. Some earlier semantic similarity approaches proposed concentrated on the ontology structure between concepts and some concentrated only on the information content of concepts. This paper proposes a semantic similarity approach with path length, information content, and semantic depth (i.e., PLICD) to combine both path length as well as information content-based approaches. This proposed approach uses weighted shortest path length and information content calculated using semantic depth and hyponyms of the concepts to measure semantic similarity between two concepts. Through experimentations performed on WordNet and DBpedia, the researchers note that the PLICD semantic similarity approach has delivered a statistically meaningful enhancement as compared to the other semantic similarity approaches concerning accuracy and F score.}
}
@article{LAAZ2020851,
title = {Combining Domain Ontologies and BPMN Models at the CIM Level to generate IFML Models},
journal = {Procedia Computer Science},
volume = {170},
pages = {851-856},
year = {2020},
note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.03.145},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920306013},
author = {Naziha Laaz and Nassim Kharmoum and Samir Mbarki},
keywords = {IFML, Ontology Modeling, CIM level, PIM, BPMN, Ontology Definition Metamodel (ODM), Model-driven Architecture (MDA)},
abstract = {The use of Business Process Model and Notation (BPMN) has been widely adopted by several research studies as one of the standard metamodels dedicated to the representation and design of business processes at the CIM level of MDA-oriented approaches. However, all of these researches don’t include semantic data that accompanies business processes in the early stages of software development. Thus, the involvement of domain ontologies in this software development phase is key for a complete representation of the CIM level. This article presents a new modeling approach based on a model-driven development process, starting with the representation of BPMN and ODM models as CIM Models, then transforming them into an IFML diagram by applying a transformation engine. We can obtain easily a final code from the abstract model generated. A case study illustrates our approach. It represents a scenario of managing medications of an eHealth information system.}
}
@article{GUILLEMIN2025128464,
title = {Weighted horn clause: Extending SWRL to model antecedents’ importance and handle missing data},
journal = {Expert Systems with Applications},
volume = {293},
pages = {128464},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128464},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425020834},
author = {Sébastien Guillemin and Ana Roxin and Laurence Dujourdy and Ludovic Journaux},
keywords = {Knowledge modelling, Decision-support systems, Semantic web rule language (SWRL), Rule-based reasoning},
abstract = {Ontologies are formal and explicit specifications of shared conceptualisations. Horn clause (HC) rules may enrich an ontology for modelling complex knowledge and enhancing its expressiveness. While the Semantic Web Rule Language (SWRL) offers a human-readable syntax for incorporating HC into an ontology, it is often too rigid for specific applications, lacking the reasoning nuances needed by domain experts. Additionally, SWRL struggles to handle missing data when inferring new axioms. To address these issues, we propose Weighted Horn Clauses (WHC), an extension of SWRL that incorporates weights to model the importance of antecedent atoms, allowing for more flexible reasoning. WHC syntax and model-theoretical semantics are detailed. We also show how WHC handles missing data when inferring new knowledge in backward and forward chaining strategies. Finally, we propose an open-source prototype reasoner for WHC rules, which is evaluated against SWRL through qualitative and quantitative evaluations. These evaluations illustrate the relevance and feasibility of WHC.}
}
@article{NIKOLAOU201991,
title = {Foundations of ontology-based data access under bag semantics},
journal = {Artificial Intelligence},
volume = {274},
pages = {91-132},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370219300426},
author = {Charalampos Nikolaou and Egor V. Kostylev and George Konstantinidis and Mark Kaminski and Bernardo {Cuenca Grau} and Ian Horrocks},
keywords = {Ontology-based data access, Description logics, Bag semantics, Query answering, Query rewriting},
abstract = {Ontology-based data access (OBDA) is a popular approach for integrating and querying multiple data sources by means of a shared ontology. The ontology is linked to the sources using mappings, which assign to ontology predicates views over the data. The conventional semantics of OBDA is set-based—that is, the extension of the views defined by the mappings does not contain duplicate tuples. This treatment is, however, in disagreement with the standard semantics of database views and database management systems in general, which is based on bags and where duplicate tuples are retained by default. The distinction between set and bag semantics in databases is very significant in practice, and it influences the evaluation of aggregate queries. In this article, we propose and study a bag semantics for OBDA which provides a solid foundation for the future study of aggregate and analytic queries. Our semantics is compatible with both the bag semantics of database views and the set-based conventional semantics of OBDA. Furthermore, it is compatible with existing bag-based semantics for data exchange recently proposed in the literature. We show that adopting a bag semantics makes conjunctive query answering in OBDA coNP-hard in data complexity. To regain tractability of query answering, we consider suitable restrictions along three dimensions, namely, the query language, the ontology language, and the adoption of the unique name assumption. Our investigation shows a complete picture of the computational properties of query answering under bag semantics over ontologies in the DL-Lite family.}
}
@article{SCHILLER2022280,
title = {Towards Ontology-based Lifecycle Management in Blisk Manufacturing},
journal = {Procedia CIRP},
volume = {112},
pages = {280-285},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.085},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122012483},
author = {Sven Schiller and Markus Landwehr and Georg Vinogradov and Iraklis Dimitriadis and Haydar Akyürek and Johannes Lipp and Philipp Ganser and Thomas Bergs},
keywords = {Product Lifecycle Management (PLM), Internet of Things (IoT), Computer Aided Manufacturing (CAM), Ontology, Semantic Web Technologies},
abstract = {Product Lifecycle Management (PLM) handles the typical stages of a product's lifespan, and is usually implemented via different methods. This paper addresses the stages product design, process design, process analysis and manufacturing in the product and process development chain of a blade-integrated disk (blisk). Domain ontologies are evolved and incorporated, as well as used with Uniform Resource Identifiers (URI) to implement a comprehensive PLM in the Internet of Production. An architecture based on the Resource Description Framework (RDF) that offers both ingestion and utilization of valuable information along the blisk lifecycle, and therefore enables PLM for all involved participants, is presented.}
}
@article{JIE2018,
title = {Study on Ontology Ranking Models Based on the Ensemble Learning},
journal = {International Journal on Semantic Web and Information Systems},
volume = {14},
number = {2},
year = {2018},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.2018040107},
url = {https://www.sciencedirect.com/science/article/pii/S155262831800008X},
author = {Liu Jie and Yuan Kerou and Zhou Jianshe and Shi Jinsheng},
keywords = {Bagging, Ensemble Learning, Ontology Ranking, Random Forests, Ranking Learning},
abstract = {ABSTRACT
This article describes how more knowledge appears on the Internet than in an ontological form. Displaying results to users precisely when searching is the key issue of the research on ontology retrieval. The considered factors of ontology ranking are not only limited to internal character-matching, but analysis of metadata, including the entities, structures and the relations in ontologies. Currently, existing single feature ranking algorithms focus on the structures, elements and the contents of a certain aspect in ontology, thus, the results are not satisfactory. Combining multiple single-featured models seems to achieve better results, but the objectivity and versatility of models’ weights are debatable. Machine learning effectively solves the problem and putting advantages of ranking learning algorithms together is the pressing issue. So we propose ensemble learning strategies to combine different algorithms in ontology ranking. And the ranking result is more satisfied compared to Swoogle and base algorithms.}
}
@article{DUARTE2021101892,
title = {An ontological analysis of software system anomalies and their associated risks},
journal = {Data & Knowledge Engineering},
volume = {134},
pages = {101892},
year = {2021},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2021.101892},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X21000197},
author = {Bruno Borlini Duarte and Ricardo {de Almeida Falbo} and Giancarlo Guizzardi and Renata Guizzardi and Vítor E. Silva Souza},
keywords = {Software defects, Errors and failures, Ontological foundations of software systems, Conceptual modeling, Methods and methodologies, Software system risk, Unified Foundational Ontology (UFO)},
abstract = {Software systems have an increasing value in our lives, as our society relies on them for the numerous services they provide. However, as our need for larger and more complex software systems grows, the risks involved in their operation also grows, with possible consequences in terms of significant material and social losses. The rational management of software defects and possible failures is a fundamental requirement for a mature software industry. Standards, professional guides and capability models directly emphasize how important it is for an organization to know and to have a well-established history of failures, errors and defects as they occur in software activities. The problem is that each of these reference models employs its own vocabulary to deal with these phenomena, which can lead to a deficiency in the understanding of these notions by software engineers, causing potential interoperability problems between supporting tools, and, consequently, a poorer adoption of these standards and tools in practice. In this paper, we address this problem of the lack of a consensual conceptualization in this area by proposing two reference conceptual models: an Ontology of Software Defects, Errors and Failures (OSDEF), which takes into account an ecosystem of software artifacts, and a Reference Ontology of Software Systems (ROSS), which characterizes software systems and related artifacts at different levels of abstraction. Moreover, we use OSDEF and ROSS to perform an ontological analysis of the impact of defects, errors and failures of software systems from a risk analysis perspective. To do that, we employee an existing core ontology, namely, the Common Ontology of Value and Risk (COVR). The ontologies presented here are grounded on the Unified Foundational Ontology (UFO) and based on well-known and widely-accepted standards, professional and scientific guides and capability models. We demonstrate how this approach can suitably promote conceptual clarification and terminological harmonization in this area.}
}
@article{DI2025114140,
title = {LoRP: LLM-based Logical Reasoning via Prolog},
journal = {Knowledge-Based Systems},
volume = {327},
pages = {114140},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114140},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125011815},
author = {Zhengkun Di and Chaoli Zhang and Hongtao Lv and Lizhen Cui and Lei Liu},
keywords = {Large language model, Logical reasoning enhancement, Prolog},
abstract = {Enhancing the logical reasoning capabilities of large language models (LLMs) is crucial for advancing LLMs’s applications in complex problem-solving contexts. Neurosymbolic programming-based approaches have demonstrated significant advantages in logical reasoning. Prolog is a high-level declarative programming language based on formal logic, well-suited for handling complex deductive reasoning tasks. However, its strict syntactic structure imposes inherent limitations on expressiveness, making it difficult to represent certain common logical constructs found in natural language. Since first-order logic (FOL) is the most fundamental formal language for logical semantic representation, we take it as a reference for analysis and find that even some of its most basic structures cannot be directly expressed in Prolog. To address this, we propose a systematic translation mechanism from FOL to Prolog, thereby extending Prolog’s expressiveness to support richer logical representations. Building on this foundation, we propose LoRP (LLM-based Logical Reasoning via Prolog), a novel framework that utilizes LLMs to convert natural language queries into Prolog code, and delegates reasoning to the external SWI-Prolog interpreter. This hybrid architecture combines the formal rigor of symbolic logic with the flexibility of LLMs, enabling precise, interpretable, and verifiable reasoning. Empirical evaluations demonstrate that LoRP significantly improves LLMs’ reasoning performance, particularly as inference depth increases. It also exhibits strong generalization and stability across various model architectures. These findings highlight the potential of symbolic-neural integration as a promising direction for advancing the logical reasoning capabilities of LLMs.}
}
@article{TRAN20201021,
title = {Ontology-based model generation to support customizable KBE frameworks},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1021-1026},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.143},
url = {https://www.sciencedirect.com/science/article/pii/S235197892032000X},
author = {Tuan Anh Tran and Andrei Lobov},
keywords = {Knowledge Based Engineering, Product Development, Ontology, Model generation},
abstract = {In this paper, we propose a method for generating 3D geometry in CAD tools in a platform-independent manner through the use of ontologies. The aim is to enable knowledge-based engineering (KBE) systems to be developed with the freedom to incorporate development tools regardless of discipline or platform. Such functionality may then be utilized by small and medium-sized enterprises (SME) to cost-effectively manifest holistic development characteristics within a customized framework. An ontology structure is developed for storing knowledge about geometric shapes and functions that are commonly used for CAD software. A translation module and parser is then used to extract relevant information to generate 3D models in Siemens NX through Knowledge Fusion. Finally, an example is presented by generating a hand pump through a developed system.}
}
@article{ALI2018127,
title = {Cross-Lingual Ontology Enrichment Based on Multi-Agent Architecture},
journal = {Procedia Computer Science},
volume = {137},
pages = {127-138},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S187705091831617X},
author = {Mohamed Ali and Said Fathalla and Shimaa Ibrahim and Mohamed Kholief and Yasser Hassan},
keywords = {cross-lingual ontology enrichment, multi-agent, knowledge management, ontology learning.},
abstract = {The proliferation of ontologies and multilingual data available on the Web has motivated many researchers to contribute to multilingual and cross-lingual ontology enrichment. Cross-lingual ontology enrichment greatly facilitates ontology learning from multilingual text/ontologies in order to support collaborative ontology engineering process. This article proposes a cross-lingual ontology enrichment (CLOE) approach based on a multi-agent architecture in order to enrich ontologies from a multilingual text or ontology. This has several advantages: 1) an ontology is used to enrich another one, written in a different natural language, and 2) several ontologies could be enriched at the same time using a single chunk of text (Simultaneous Ontology Enrichment). A prototype for the proposed approach has been implemented in order to enrich several ontologies using English, Arabic and German text. Evaluation results are promising and showing that CLOE performs well in comparison with four state-of-the-art approaches.}
}
@article{HAKANSSON20245458,
title = {Generative AI and Large Language Models - Benefits, Drawbacks, Future and Recommendations},
journal = {Procedia Computer Science},
volume = {246},
pages = {5458-5468},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.689},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924027492},
author = {Anne Håkansson and Gloria Phillips-Wren},
keywords = {Natural Language Processing, Generative AI, Large Language Models},
abstract = {Natural language processing, with parsing and generation, has a long tradition. Parsing has been easier to perform than a generation but with generative artificial intelligence (a.k.a Gen AI) and large language models (abbr. LLMs), this has changed. Generative artificial intelligence is a type of artificial intelligence that uses a large data set to create something in the genre of that data set. It can generate different outputs ranging from texts, audio, objects, pictures, and paintings to videos, but also synthetic data. LLMs use deep learning and deep neural networks to train on large text corpora for recognizing and generating texts. These models are based on massive data sets, collected from databases and the web. They use transformer models to detect how elements in sequences relate to each other. This provides context support. Two well-known large language models are the Generative Pre-trained Transformer, GPT, used in ChatGPT and Bidirectional Encoder Representations from Transformers, BERT. Although LLMs have advantages, they have problems. This paper presents generative artificial intelligence and LLMs with benefits and drawbacks. Results from applying these models have shown that they can work well for accuracy in specificity, user personalization and human-computer communication but they may not provide acceptable, reliable and truthful results. For example, ethics, hallucinations and incorrect information, or misjudgments, are some major problems. The paper ends with future directions, research questions on LLMs, and recommendations.}
}
@article{MISIRLI20191498,
title = {SBOL-OWL: An Ontological Approach for Formal and Semantic Representation of Synthetic Biology Information},
journal = {ACS Synthetic Biology},
volume = {8},
number = {7},
pages = {1498-1514},
year = {2019},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.8b00532},
url = {https://www.sciencedirect.com/science/article/pii/S2161506319001487},
author = {Göksel Mısırlı and Renee Taylor and Angel Goñi-Moreno and James Alastair McLaughlin and Chris Myers and John H. Gennari and Phillip Lord and Anil Wipat},
keywords = {ontology, Synthetic Biology Open Language, standardization, genetic circuits},
abstract = {Standard representation of data is key for the reproducibility of designs in synthetic biology. The Synthetic Biology Open Language (SBOL) has already emerged as a data standard to represent information about genetic circuits, and it is based on capturing data using graphs. The language provides the syntax using a free text document that is accessible to humans only. This paper describes SBOL-OWL, an ontology for a machine understandable definition of SBOL. This ontology acts as a semantic layer for genetic circuit designs. As a result, computational tools can understand the meaning of design entities in addition to parsing structured SBOL data. SBOL-OWL not only describes how genetic circuits can be constructed computationally, it also facilitates the use of several existing Semantic Web tools for synthetic biology. This paper demonstrates some of these features, for example, to validate designs and check for inconsistencies. Through the use of SBOL-OWL, queries can be simplified and become more intuitive. Moreover, existing reasoners can be used to infer information about genetic circuit designs that cannot be directly retrieved using existing querying mechanisms. This ontological representation of the SBOL standard provides a new perspective to the verification, representation, and querying of information about genetic circuits and is important to incorporate complex design information via the integration of biological ontologies.
}
}
@article{CONFALONIERI2021103471,
title = {Using ontologies to enhance human understandability of global post-hoc explanations of black-box models},
journal = {Artificial Intelligence},
volume = {296},
pages = {103471},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103471},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000229},
author = {Roberto Confalonieri and Tillman Weyde and Tarek R. Besold and Fermín {Moscoso del Prado Martín}},
keywords = {Human-understandable explainable AI, Global explanations, Ontologies, Neural-symbolic learning and reasoning, Knowledge extraction, Concept refinement},
abstract = {The interest in explainable artificial intelligence has grown strongly in recent years because of the need to convey safety and trust in the ‘how’ and ‘why’ of automated decision-making to users. While a plethora of approaches has been developed, only a few focus on how to use domain knowledge and how this influences the understanding of explanations by users. In this paper, we show that by using ontologies we can improve the human understandability of global post-hoc explanations, presented in the form of decision trees. In particular, we introduce Trepan Reloaded, which builds on Trepan, an algorithm that extracts surrogate decision trees from black-box models. Trepan Reloaded includes ontologies, that model domain knowledge, in the process of extracting explanations to improve their understandability. We tested the understandability of the extracted explanations by humans in a user study with four different tasks. We evaluate the results in terms of response times and correctness, subjective ease of understanding and confidence, and similarity of free text responses. The results show that decision trees generated with Trepan Reloaded, taking into account domain knowledge, are significantly more understandable throughout than those generated by standard Trepan. The enhanced understandability of post-hoc explanations is achieved with little compromise on the accuracy with which the surrogate decision trees replicate the behaviour of the original neural network models.}
}
@article{GUAN2025102359,
title = {Leveraging large language models for peptide antibiotic design},
journal = {Cell Reports Physical Science},
volume = {6},
number = {1},
pages = {102359},
year = {2025},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2024.102359},
url = {https://www.sciencedirect.com/science/article/pii/S2666386424006738},
author = {Changge Guan and Fabiano C. Fernandes and Octavio L. Franco and Cesar {de la Fuente-Nunez}},
abstract = {Summary
Large language models (LLMs) have significantly impacted various domains of our society, including recent applications in complex fields such as biology and chemistry. These models, built on sophisticated neural network architectures and trained on extensive datasets, are powerful tools for designing, optimizing, and generating molecules. This review explores the role of LLMs in discovering and designing antibiotics, focusing on peptide molecules. We highlight advancements in drug design and outline the challenges of applying LLMs in these areas.}
}
@article{ALMENDROSJIMENEZ2021113772,
title = {Discovery and diagnosis of wrong SPARQL queries with ontology and constraint reasoning},
journal = {Expert Systems with Applications},
volume = {165},
pages = {113772},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.113772},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420305960},
author = {Jesús M. Almendros-Jiménez and Antonio Becerra-Terón},
keywords = {SPARQL, Semantic web, Debugging, Static analysis, Ontology reasoning, Constraint reasoning},
abstract = {The discovery and diagnosis of wrong queries in database query languages have gained more attention in recent years. While for imperative languages well-known and mature debugging tools exist, the case of database query languages has traditionally attracted less attention. SPARQL is a database query language proposed for the retrieval of information in Semantic Web resources. RDF and OWL are standardized formats for representing Semantic Web information, and SPARQL acts on RDF/OWL resources allowing to retrieve answers of user’s queries. In spite of the SPARQL apparent simplicity, the number of mistakes a user can make in queries can be high and their detection, localization, and correction can be difficult to carry out. Wrong queries have as consequence most of the times empty answers, but also wrong and missing (expected but not found) answers. In this paper we present two ontology and constraint reasoning based methods for the discovery and diagnosis of wrong queries in SPARQL. The first method is used for detecting wrongly typed and unsatisfiable queries. The second method is used for detecting mismatching between user intention and queries, reporting incomplete, faulty queries as well as counterexamples. We formally define the above concepts and a list of examples to illustrate the methods is shown. A Web online tool has been developed to analyze SPARQL queries according to the proposed methods.}
}
@article{AGUILAR2018202,
title = {CAMeOnto: Context awareness meta ontology modeling},
journal = {Applied Computing and Informatics},
volume = {14},
number = {2},
pages = {202-213},
year = {2018},
issn = {2210-8327},
doi = {https://doi.org/10.1016/j.aci.2017.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2210832717301643},
author = {Jose Aguilar and Marxjhony Jerez and Taniana Rodríguez},
keywords = {Context-awareness, Ontologies, Meta ontologies, Context modeling},
abstract = {In order to model a context and adapt it to any domain, it is necessary an ontology that captures generic concepts to a higher level. The context model must provide mechanisms to extend the specific information of a context in a hierarchical manner. In this paper, we propose CAMeOnto, an ontology with these characteristics, based on the principles of 5Ws: who, when, what, where and why. CAMeOnto is used by CARMiCLOC, a reflective middleware for context-aware applications, and is instantiated in several case studies, in order to test how CAMeOnto works correctly and can reason to infer information about the context.}
}
@article{YANG20251118,
title = {SeedLLM·Rice: A large language model integrated with rice biological knowledge graph},
journal = {Molecular Plant},
volume = {18},
number = {7},
pages = {1118-1129},
year = {2025},
issn = {1674-2052},
doi = {https://doi.org/10.1016/j.molp.2025.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1674205225001728},
author = {Fan Yang and Huanjun Kong and Jie Ying and Zihong Chen and Tao Luo and Wanli Jiang and Zhonghang Yuan and Zhefan Wang and Zhaona Ma and Shikuan Wang and Wanfeng Ma and Xiaoyi Wang and Xiaoying Li and Zhengyin Hu and Xiaodong Ma and Minguo Liu and Xiqing Wang and Fan Chen and Nanqing Dong},
keywords = {LLM, large language model, knowledge graph, multiomics data integration, GPT, DeepSeek},
abstract = {Rice biology research involves complex decision-making, requiring researchers to navigate a rapidly expanding body of knowledge encompassing extensive literature and multiomics data. The exponential increase in biological data and scientific publications presents significant challenges for efficiently extracting meaningful insights. Although large language models (LLMs) show promise for knowledge retrieval, their application to rice-specific research has been limited by the absence of specialized models and the challenge of synthesizing multimodal data integral to the field. Moreover, the lack of standardized evaluation frameworks for domain-specific tasks impedes the effective assessment of model performance. To address these challenges, we introduce SeedLLM·Rice (SeedLLM), a 7-billion-parameter model trained on 1.4 million rice-related publications, representing nearly 98.24% of global rice research output. Additionally, we present a novel human-centric evaluation framework designed to assess LLM performance in rice biology tasks. Initial evaluations demonstrate that SeedLLM outperforms general-purpose models, including OpenAI GPT-4o1 and DeepSeek-R1, achieving win rates of 57% to 88% on rice-specific tasks. Furthermore, SeedLLM is integrated with the Rice Biological Knowledge Graph (RBKG), which consolidates genome annotations for Nipponbare and large-scale synthesis of transcriptomic and proteomic information from over 1800 studies. This integration enhances the ability of SeedLLM to address complex research questions requiring the fusion of textual and multiomics data. To facilitate global collaboration, we provide free access to SeedLLM and the RBKG via an interactive web portal (https://seedllm.org.cn/). SeedLLM represents a transformative tool for rice biology research, enabling unprecedented discoveries in crop improvement and climate adaptation through advanced reasoning and comprehensive data integration.}
}
@article{BLANKENBERG2022314,
title = {Using a graph database for the ontology-based information integration of business objects from heterogenous Business Information Systems},
journal = {Procedia Computer Science},
volume = {196},
pages = {314-323},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.019},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921022420},
author = {Carolin Blankenberg and Berit Gebel-Sauer and Petra Schubert},
keywords = {IS Integration, Enterprise Knowledge Graph, Ontology, Graph Database},
abstract = {This paper reports on findings from a project on information integration from multiple Business Information Systems with the help of a user-specific Enterprise Knowledge Graph. Most ERP systems currently in use store information objects in relational databases. Research in Web Sciences has shown that graph structures present information in a more intuitive way that is easier to interpret for humans. Following a DSR approach, we developed a concept for storing an ontology in a graph database that allows us to map ERP objects and load them at runtime. This allows the end user to navigate through the graph structure, thus providing an intuitive and quick access to essential job-related information. We evaluated the suggested concept with a prototype following the paradigm of polyglot persistence; the prototype was equipped with a graph database to store the company-specific ontology in its native form. The program code was encapsulated into a separate module following a service-oriented software design.}
}
@article{BEIRADE2021753,
title = {Semantic query for Quranic ontology},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {6},
pages = {753-760},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818309625},
author = {Faiza Beirade and Hamid Azzoune and D. Eddine Zegour},
keywords = {Ontology, Semantic, Search engine, Enrich},
abstract = {The automatic processing of natural languages knew a remarkable growth in terms of techniques, methods and the variety of its potential applications. For instance, to automatize the processing of the Arabic language with its particular features, we selected the holy Quran as a study case. The latter has represented a challenge for the artificial intelligence. The main objective of the present paper is to design the semantic search engine for the text of the Quran using Quranic ontology. To determine the semantic fields of the words of the holy Quran, the Quranic ontology was developed, that presents the meaning of words and their relations. This method is used for each concept in order to enrich the query.}
}
@article{XIAO202543,
title = {Large language model-guided graph convolution network reasoning system for complex human-robot collaboration disassembly operations},
journal = {Procedia CIRP},
volume = {134},
pages = {43-48},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125004561},
author = {Jinhua Xiao and Sergio Terzi},
keywords = {Large Language Model, Graph Convolutional Network, Human-Robot Collaboration, Disassembly System},
abstract = {This paper proposes a systematic reasoning framework combining Large Language Models (LLMs) with Graph Convolutional Networks (GCNs) for Human-Robot Collaboration (HRC) reasoning system for the actual disassembly operations. GCNs with the sensor data can be used to capture spatial-temporal relationships for human operation actions, while LLMs interpret text-prompt data inputs to provide the contextual insights that focus on relevant aspects of human-robot collaboration disassembly. The system enables the real-time reasoning of disassembly tasks and disassembly strategies, thereby enhancing both the safety and efficiency of HRC disassembly operations. This approach is particularly valuable in complex and dynamic environments, where the capability to quickly and accurately understand the complex disassembly tasks based on HRC disassembly environment that is crucial for successful collaboration disassembly between humans and robots. To address key challenges in ensuring the safe, and efficient disassembly operations, it is necessary to provide the way for more advanced human-robot collaboration disassembly with the complex disassembly environments in the disassembly operations.}
}
@article{ELHADJ2021417,
title = {Do-Care: A dynamic ontology reasoning based healthcare monitoring system},
journal = {Future Generation Computer Systems},
volume = {118},
pages = {417-431},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21000017},
author = {Hadda Ben Elhadj and Farag Sallabi and Amira Henaien and Lamia Chaari and Khaled Shuaib and Maryam {Al Thawadi}},
keywords = {Dynamic ontology, Healthcare, IoT, Decision support system, Reasoning},
abstract = {Healthcare remote monitoring applications dominate the market of new technologies due to their valuable aid to patients, families, and medical staff. They provide ubiquitous remote health services for patients with chronic diseases or specific conditions and can provide ubiquitous communication between patients and caregiver(s). This paper presents an ontology reasoning-based healthcare monitoring system called Do-Care. The proposed system supports the supervision and follow-up of outdoor and indoor patients suffering from chronic diseases. Collected data, from wearable11Worn or implanted in the patient’s body., nearable22Ambient and physiological sensors distributed in patient’s environment. or usable33Mobile, tablets and laptops. devices forms the instances for entities from the proposed Do-Care ontology used by the reasoner when applying a set of SWRL44Semantic Web Rule Language. rules to determinate the health situation of a patient as Normal, Abnormal or Wrong. The main contribution in this paper is a modular and dynamic ontology composed of FOAF55Friend of a Friend Ontology., SSN66Semantic Sensors Network./SOSA77Sensor, Observation, Sample and Actuator Ontology. and ICNP88International Classification Nursing Practices Ontology. ontologies with a scalable set of inference rules. The proposed rule based methodology is dynamic and adjustable to meet possible changes in the medication market, medical discoveries, and personal users’ profiles. The presented experimental results show the efficiency of the proposed DO-Care system.}
}
@article{XU2019562,
title = {Modular Ontology Learning with Topic Modelling over Core Ontology},
journal = {Procedia Computer Science},
volume = {159},
pages = {562-571},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.211},
url = {https://www.sciencedirect.com/science/article/pii/S187705091931395X},
author = {Ziwei Xu and Mounira Harzallah and Fabrice Guillet and Ryutaro Ichise},
keywords = {LDA, term partition, modular ontology, ontology learning, core ontology, knowledge supplementation},
abstract = {Nowadays, modular domain ontology, where each module represents a subdomain of the ontology domain, facilitates the reuse of information and provides users with domain-specific knowledge. In this paper, we focus on modular taxonomy learning from text, where each module collects terms with the same topic insights, and in parallel we manage to discover hypernym and 'related' relations among those collected terms.However, it is difficult to automatically fit terms into modules and discover relations.We propose to employ twice trainedLDAto partition termsof each subdomain, and relate subdomains into modules of ontology. Meanwhile, we apply core concept replacement and subdomain knowledge supplementation as supportive information embedding technique over the corpus. This shows that the twice trained LDA strategy can effectively identify topic-relevant terms into subdomains, with nearly two-fold precision comparing to that of normal LDA training. The combination of core concept replacement and subdomain knowledge supplementation contributes to significant improvements in modular taxonomy learning.}
}
@article{WANG2021,
title = {Matching Biomedical Ontologies: Construction of Matching Clues and Systematic Evaluation of Different Combinations of Matchers},
journal = {JMIR Medical Informatics},
volume = {9},
number = {8},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/28212},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421002866},
author = {Peng Wang and Yunyan Hu and Shaochen Bai and Shiyi Zou},
keywords = {biomedical ontology, ontology matching, matching clues, reduction anchors},
abstract = {Background
Ontology matching seeks to find semantic correspondences between ontologies. With an increasing number of biomedical ontologies being developed independently, matching these ontologies to solve the interoperability problem has become a critical task in biomedical applications. However, some challenges remain. First, extracting and constructing matching clues from biomedical ontologies is a nontrivial problem. Second, it is unknown whether there are dominant matchers while matching biomedical ontologies. Finally, ontology matching also suffers from computational complexity owing to the large-scale sizes of biomedical ontologies.
Objective
To investigate the effectiveness of matching clues and composite match approaches, this paper presents a spectrum of matchers with different combination strategies and empirically studies their influence on matching biomedical ontologies. Besides, extended reduction anchors are introduced to effectively decrease the time complexity while matching large biomedical ontologies.
Methods
In this paper, atomic and composite matching clues are first constructed in 4 dimensions: terminology, structure, external knowledge, and representation learning. Then, a spectrum of matchers based on a flexible combination of atomic clues are designed and utilized to comprehensively study the effectiveness. Besides, we carry out a systematic comparative evaluation of different combinations of matchers. Finally, extended reduction anchor is proposed to significantly alleviate the time complexity for matching large-scale biomedical ontologies.
Results
Experimental results show that considering distinguishable matching clues in biomedical ontologies leads to a substantial improvement in all available information. Besides, incorporating different types of matchers with reliability results in a marked improvement, which is comparative to the state-of-the-art methods. The dominant matchers achieve F1 measures of 0.9271, 0.8218, and 0.5 on Anatomy, FMA-NCI (Foundation Model of Anatomy-National Cancer Institute), and FMA-SNOMED data sets, respectively. Extended reduction anchor is able to solve the scalability problem of matching large biomedical ontologies. It achieves a significant reduction in time complexity with little loss of F1 measure at the same time, with a 0.21% decrease on the Anatomy data set and 0.84% decrease on the FMA-NCI data set, but with a 2.65% increase on the FMA-SNOMED data set.
Conclusions
This paper systematically analyzes and compares the effectiveness of different matching clues, matchers, and combination strategies. Multiple empirical studies demonstrate that distinguishing clues have significant implications for matching biomedical ontologies. In contrast to the matchers with single clue, those combining multiple clues exhibit more stable and accurate performance. In addition, our results provide evidence that the approach based on extended reduction anchors performs well for large ontology matching tasks, demonstrating an effective solution for the problem.}
}
@article{KANTARELIS2023100754,
title = {Functional harmony ontology: Musical harmony analysis with Description Logics},
journal = {Journal of Web Semantics},
volume = {75},
pages = {100754},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100754},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000385},
author = {Spyridon Kantarelis and Edmund Dervakos and Natalia Kotsani and Giorgos Stamou},
keywords = {Description Logics, Ontology engineering, OWL, Music harmony, Modal harmony, Tonal harmony},
abstract = {Symbolic representations of music are emerging as an important data domain both for the music industry and for computer science research, aiding in the organization of large collections of music and facilitating the development of creative and interactive AI. An aspect of symbolic representations of music, which differentiates them from audio representations, is their suitability to be linked with notions from music theory that have been developed over the centuries. One core such notion is that of functional harmony, which involves analyzing progressions of chords. This paper proposes a description of the theory of functional harmony within the OWL 2 RL profile and experimentally demonstrates its practical use.}
}
@article{ALBARGHOTHI2018104,
title = {Automatic Construction of E-Government Services Ontology from Arabic Webpages},
journal = {Procedia Computer Science},
volume = {142},
pages = {104-113},
year = {2018},
note = {Arabic Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.10.465},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918321677},
author = {Ali Albarghothi and Walaa Saber and Khaled Shaalan},
keywords = {Ontology Construction, Natural Language Processing, Keyword Extraction, OWL, Government Services, Protégé, Arabic},
abstract = {The digital age has led to a significant increase in the amount of information, and the relationships among different types of information have become more sophisticated. In addition, growing numbers of users are benefiting from this information. The types of these users vary, necessitating techniques for analyzing customers’ experiences in order to meet their needs. Regarding e-government, the number of structured and unstructured webpages of electronic services has increased, making the repositories more complex and difficult to analyze without considering semantic knowledge. In this paper, we present an approach for the automatic extraction of the ontology-based Semantic Web (SW) constructed from Arabic webpages related to Dubai’s e-government services. The proposed methodology consists of four stages. The first stage is concerned with data extraction and validation from the www.dubai.ae portal, including official profiles for more than 500 services. We developed an Automatic Extraction Dataset System (AEDS) tool to extract the dataset that represents all government services profiles. The second stage is data processing and involved using Natural Language Processing (NLP) tasks to process the services’ profiles and extract the ontology keywords. The third stage is the mapping rules process to link the ontology components with the extracted keywords. Lastly, the ontology was constructed using the OWL format and Protégé tool standards. Afterwards, an experimental evaluation was conducted to evaluate the accuracy of the constructed ontology. The results indicate high scores in terms of Precision, with 87% on average, and Recall, with 97% on average. Finally, we present the challenges and future prospects.}
}
@article{MA2025809,
title = {Leveraging large language models in next generation intelligent manufacturing: Retrospect and prospect},
journal = {Journal of Manufacturing Systems},
volume = {82},
pages = {809-840},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525001943},
author = {Yunfei Ma and Shuai Zheng and Zheng Yang and Pai Zheng and Jiewu Leng and Jun Hong},
keywords = {Industry 5.0, Large language model, Human–robot collaboration, Human centered intelligent manufacturing},
abstract = {Industry 5.0, as the guiding ideology of the new generation intelligent manufacturing, points the way for global industrial transformation. It emphasizes the collaborative cooperation between humans, machines and intelligent systems, and places humans at the core of the industrial production process, aiming to create a more flexible, personalized and sustainable production paradigm. Large language model, as an advanced natural language processing technology, has received attention from researchers related to Industry 5.0 due to its ease of use and powerful language processing capability. LLM is considered to be one of the key enabling technologies to drive the development of Industry 5.0 and has great application potential. After a rigorous review of existing approaches, we find there is few existing survey papers that focuses on how LLM will drive the development of Industry 5.0 applications. Therefore, this paper provides a comprehensive review of the application of LLM in the field of Industry 5.0. Firstly, we conduct a literature review to explore the current state of research related to Industry 5.0. Subsequently, we analyze LLM-based technologies, synergizing LLMs with Industry 5.0 enablers and the applications of LLM in various domains of intelligent manufacturing. Finally, we explore the challenges of LLM in real-world scenarios and future research directions in the context of Industry 5.0. It is hoped that this study will contribute to the further development of LLM-based solutions in the context of Industry 5.0 and unite various efforts to achieve the vision of Industry 5.0.}
}
@article{GROZA20251158,
title = {First steps toward building natural history of diseases computationally: Lessons learned from the Noonan syndrome use case},
journal = {The American Journal of Human Genetics},
volume = {112},
number = {5},
pages = {1158-1172},
year = {2025},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2025.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0002929725001351},
author = {Tudor Groza and Warittha Rayabsri and Dylan Gration and Harshini Hariram and Saumya Shekhar Jamuar and Gareth Baynam},
keywords = {natural history of disease, Human Phenotype Ontology, generative AI, Noonan syndrome, rare diseases, Large Language Models},
abstract = {Summary
Rare diseases (RDs) are conditions affecting fewer than 1 in 2,000 people, with over 7,000 identified, primarily genetic in nature, and more than half impacting children. Although each RD affects a small population, collectively, between 3.5% and 5.9% of the global population, or 262.9–446.2 million people, live with an RD. Most RDs lack established treatment protocols, highlighting the need for proper care pathways addressing prognosis, diagnosis, and management. Advances in generative AI and large language models (LLMs) offer new opportunities to document the temporal progression of phenotypic features, addressing gaps in current knowledge bases. This study proposes an LLM-based framework to capture the natural history of diseases, specifically focusing on Noonan syndrome. The framework aims to document phenotypic trajectories, validate against RD knowledge bases, and integrate insights into care coordination using electronic health record (EHR) data from the Undiagnosed Diseases Program Singapore.}
}
@article{S2020267,
title = {A comparative study on the performance of rule engines in automated ontology learning: a case study with erythemato-squamous disease (ESD)},
journal = {International Journal of Intelligent Unmanned Systems},
volume = {8},
number = {4},
pages = {267-280},
year = {2020},
issn = {2049-6427},
doi = {https://doi.org/10.1108/IJIUS-08-2019-0047},
url = {https://www.sciencedirect.com/science/article/pii/S2049642720000093},
author = {Sivasankari S and Dinah Punnoose and Krishnamoorthy D},
keywords = {Ontology, Rule engine, OWL knowledge, Jess, Drools, Semantic Web Rule Language},
abstract = {Purpose
Erythemato-squamous disease (ESD) is one of the complex diseases related to the dermatology field. Due to common morphological features, the diagnosis of ESDs become stringent and leads to inconsistency. Besides, diagnosis has been done on the basis of inculcated visible symptoms pertinent with the expertise of the physician. Hence, ontology construction for ESD is essential to ensure credibility, consistency, to resolve lack of time, labor and competence and to diminish human error.
Design/methodology/approach
This paper presents the design of an automatic ontology framework through data mining techniques and subsequently depicts the diagnosis of ESD using the available knowledge- and rule-based system.
Findings
The rule language (Semantic Web Rule Language) and rule engine (Jess and Drools) have been integrated to explore the severity of the ESD and foresee the most appropriate class to be suggested.
Social implications
In this paper, the authors identify the efficiency of the rule engine and investigate the performance of the computational techniques in predicting ESD using three different measures.
Originality/value
Primarily, the approach assesses transfer time for total number of axioms exported to rule engine (Jess and Drools) while the other approach measures the number of inferred axioms (process time) using the rule engine while the third measure calculates the time to translate the inferred axioms to OWL knowledge (execution time).}
}
@article{ZHANG2024102587,
title = {Semantic Web Rule Language-based approach for implementing Knowledge-Based Engineering systems},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102587},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102587},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002350},
author = {Liang Zhang and Andrei Lobov},
keywords = {Product design, Knowledge-Based Engineering, Black-box, Computer-Aided Design, Ontology, Semantic Web Rule Language},
abstract = {The culture of product design is shifting from case-by-case development to the Knowledge-Based Engineering (KBE) paradigm facilitating knowledge sharing and reusing among different stages and groups. The Semantic Web stack including Web Ontology Language (OWL) and Semantic Web Rule Language (SWRL) offers promising formats to represent data and rules for engineering knowledge sharing and reuse. However, many KBE applications for product design treat ontology-based knowledge bases as graph databases, often neglecting the reasoning abilities provided by the Semantic Web stack. Consequently, design rules, especially those concerning the (re)construction of geometric models, are frequently encapsulated as black-box processes within KBE systems. This type of reuse tends to result in non-cohesive solutions, where fragments of relevant knowledge, especially about the (re)construction of geometric models, are dispersed across various locations. This article demonstrates an approach to realizing the automated product design facilitated by semantically representing engineering knowledge using OWL and SWRL. This approach enables the construction of a cohesive knowledge base, leveraging the reasoning capabilities provided by the Semantic Web stack. Notably, the (re)construction of geometric models can be achieved using KBE language code snippets and the string processing capabilities of SWRL. To demonstrate this approach, a shaft design case, frequently used in research on product design, serves as a demonstrator to provide conceptual proof. The resulting geometric models are generated in KBE languages compatible with Siemens NX and AVEVA design software and can be visualized through interaction with the Computer-aided Design (CAD) kernel. This showcases the potential for seamless integration and knowledge sharing in the realm of product design through the application of the Semantic Web stack and KBE.}
}
@article{XU2025348,
title = {Design of Question-Answering and Reasoning System Combining Large Language Models and Knowledge Graphs},
journal = {Procedia Computer Science},
volume = {262},
pages = {348-357},
year = {2025},
note = {The 5th International Conference on Multi-modal Information Analytics (MMIA)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.05.062},
url = {https://www.sciencedirect.com/science/article/pii/S187705092501909X},
author = {Yanru Xu},
keywords = {LLM, LLaMa, KG, question-answering, reasoning, system design, command fine-tuning},
abstract = {Problem reasoning uses natural language processing technology to analyze natural language questions input by users, and finally generate accurate and suitable answers. At present, the performance of Q&A model has been significantly improved, and the deep semantic understanding of text can be obtained through accurate knowledge reasoning. Based on LLaMa model, this paper systematically studies Transformer architecture, normalization technology, rotary position coding and packet query attention, creatively studies the combination of LLM and KG, and constructs a mathematical model of instruction fine-tuning algorithm. It not only understands the surface meaning of text, but also uses background knowledge such as entity relationships in KG to execute instructions more accurately and intelligently. The LLM studied in this paper is combined with KG to build a question-and-answer reasoning system, which can overcome the "illusion" problem of large models. KG significantly improves the accuracy by constrains the generated results of large models with structured knowledge.}
}
@incollection{MASSEROLI2019813,
title = {Biological and Medical Ontologies: Introduction},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {813-822},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20395-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338203956},
author = {Marco Masseroli},
keywords = {Controlled vocabulary, Interoperability, Knowledge discovery, Ontology, Open Biological and Biomedical Ontologies, Semantic network, Terminology, Unified Medial Language System},
abstract = {Increasing availability of biomedical-molecular data and information pushed the need for their standard descriptions, which allow their automatic processing for knowledge extraction. Towards this goal, many biological and medical terminologies and ontologies were developed, and actively used to describe the features of numerous biomedical-molecular entities. However, bio-terminology and bio-ontology development grew dispersedly and disuniformly, producing not matching ontologies and their not comparable annotations, hampering their interoperability role. These issues have been overcome with the Open Biological and Biomedical Ontologies, a set of orthogonal bio-ontologies, and the Unified Medical Language System, providing mapping among biomedical concepts and relations in multiple sources.}
}
@article{ZHANG2022105159,
title = {Ontological Revision and Quantum Mechanics},
journal = {Results in Physics},
volume = {33},
pages = {105159},
year = {2022},
issn = {2211-3797},
doi = {https://doi.org/10.1016/j.rinp.2021.105159},
url = {https://www.sciencedirect.com/science/article/pii/S2211379721011165},
author = {Hwe Ik Zhang and M.Y. Choi},
keywords = {Ontology, Propensity, Dual space, Discerner, Event, Null-event},
abstract = {We observe that there exists a presumed ontology, on the basis of which a dynamical theory is formulated. Extracting the essence of the ontology underlying classical mechanics, we revise it suitably to accommodate quantum mechanics seamlessly. The revision goes through two stages: First, we elaborate the concept of “state” as the propensity of an object to arouse events on discerners. Second, we integrate the position and momentum spaces into a single entity, related via the Fourier transform. Based on this revised ontology, quantum mechanics is deduced naturally without relying upon any artificial assumptions like the “quantum condition” or mathematical apparatuses like the Hilbert space and self-adjoint operators. In consequence, such thorny issues as the “measurement problem” are shown to either resolve or simply disappear.}
}
@article{LOUGE20181,
title = {ASON: An OWL-S based ontology for astrophysical services},
journal = {Astronomy and Computing},
volume = {24},
pages = {1-16},
year = {2018},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2018.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2213133716301421},
author = {T. Louge and M.H. Karray and B. Archimède and J. Knödlseder},
abstract = {Modern astrophysics heavily relies on Web services to expose most of the data coming from many different instruments and researches worldwide. The virtual observatory (VO) has been designed to allow scientists to locate, retrieve and analyze useful information among those heterogeneous data. The use of ontologies has been studied in the VO context for astrophysical concerns like object types or astrophysical services subjects. On the operative point of view, ontological description of astrophysical services for interoperability and querying still has to be considered. In this paper, we design a global ontology (Astrophysical Services ONtology, ASON) based on web Ontology Language for Services (OWL-S) to enhance existing astrophysical services description. By expressing together VO specific and non-VO specific services design, it will improve the automation of services queries and allow automatic composition of heterogeneous astrophysical services.}
}
@article{YUAN2018776,
title = {Modelling residual value risk through ontology to address vulnerability of PPP project system},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {776-793},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617303063},
author = {Jingfeng Yuan and Xuan Li and Kaiwen Chen and Mirosław J. Skibniewski},
keywords = {Ontology, PPP projects, Residual value risk, Knowledge management},
abstract = {For facilitating the management of Residual Value Risk (RVR) in Public Private Partnership (PPP) projects, an ontology-based model is established to describe the generation process and complex relationships of RVR. An ontology-based approach is proposed to analyze the RVR in PPPs, which is a framework addressing the vulnerability and a knowledge-based modeling for RVR management. The RVR ontology model is composed of class of Project, Risk, and Vulnerability, as well as taxonomy of risk factors for risk sources (RS), risk events (RE), risk consequences (RC), exposure (V1), resilience (V2) and sensitivity (V3). Meanwhile, different relationships among taxonomies, classes and individuals are expressed in model. Moreover, the object properties for class project and the object properties of inherited/non-inherited relationships are defined. Meanwhile, project-based, risk-based, and vulnerability-based datatype property are further described. Then a real individual is established by using the ontology editing software Protégé. The proposed RVR ontology model can be used to visualize and manipulate various representations in RVR management as well as to implement the work of risk reasoning and analyzing. The proposed RVR ontology framework provides a useful framework to systematize different knowledge of RVRs in PPP projects, in which the related knowledge can be described clearly and effectively. Moreover, the proposed framework can enhance data process function and improve the analysis of RVR probability and vulnerability in PPP projects through sharedness and transferability of RVR knowledge provided by ontology-based RVR model for different stakeholders in PPP projects.}
}
@article{CHERGUI2020818,
title = {An approach to the acquisition of tacit knowledge based on an ontological model},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {32},
number = {7},
pages = {818-828},
year = {2020},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2018.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1319157818305184},
author = {Wahid Chergui and Samir Zidat and Farhi Marir},
keywords = {Knowledge model, Tacit knowledge, Explicit knowledge, Know-how, Know-that, Ontology},
abstract = {Our knowledge includes irreducible tacit elements which are related to the individual’s personal nature that go beyond what we can express, which makes it very difficult to formalize, communicate and share. As this tacit knowledge consists of either actions or personal attitudes, we propose an approach to acquisition of tacit knowledge based on an ontological model. The ontology is built top down by changing the actors’ cognitive focus from the focal to the subsidiary, or from the aim of an action to its detailed objectives. We also use explicitation interviews and self-confrontation techniques to identify the tacit elements in actors’ activities, such as the concepts of situation, know-how and know-that, which constitute our ontology for knowledge acquisition.}
}
@article{LECLAIR2022102044,
title = {Architecture for ontology-supported multi-context reasoning systems},
journal = {Data & Knowledge Engineering},
volume = {140},
pages = {102044},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102044},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X22000453},
author = {Andrew LeClair and Jason Jaskolka and Wendy MacCaull and Ridha Khedri},
keywords = {Software architecture, Knowledge-based system, Ontology-based system, Intelligent system},
abstract = {Modern smart systems such as those needed for Industry 4.0 integrate data from various sources and increasingly require that data be contextualized with domain knowledge. The integration and contextualization of data allows for the advanced reasoning needed to generate knowledge grounded in the data under consideration. In this paper, we propose an architecture for an ontology-supported multi-context reasoning system which inherently supports a number of desired system qualities including data transparency, system interactivity, and graceful aging. The architecture is inspired by the Presentation–Abstraction–Control architecture style, which is an interaction-based architecture. Our architecture uses a two level hierarchy with three agents and can incorporate and utilize multiple contexts. It is flexible, supporting an interface between data and users, highly interactive, and easily maintained. The evolution of data is isolated to a single component of the system and therefore does not cascade to several others. A domain of application can be easily determined by the use of archetypes and domain-specification components. Our architecture is demonstrated using a case study involving data from the city of San Francisco.}
}
@article{YAGO201848,
title = {ON-SMMILE: Ontology Network-based Student Model for MultIple Learning Environments},
journal = {Data & Knowledge Engineering},
volume = {115},
pages = {48-67},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17301945},
author = {Hector Yago and Julia Clemente and Daniel Rodriguez and Pedro Fernandez-de-Cordoba},
keywords = {Ontological engineering, Student modeling, Ontology network, Learning supervision, Semantic web},
abstract = {Currently, many educational researchers focus on the extraction of information about the learning progress to properly assist students. We present ON-SMMILE, a student-centered and flexible student model which is represented as an ontology network combining information related to (i) students and their knowledge state, (ii) assessments that rely on rubrics and different types of objectives, (iii) units of learning and (iv) information resources previously employed as support for the student model in intelligent virtual environment for training/instruction and here extended. The aim of this work is to design and build methodologically, throughout ontological engineering, the ON-SMMILE model to be used as support of future works closely linked to supervision of student's learning as competence-based recommender system. For this purpose, our model is designed as a set of ontological resources that have been extended, standardized, interrelated and adapted to be used in multiple learning environments. In this paper, we also analyze the available approaches based on instructional design which can be added to ontology network to build the proposed model. As a case study, a chemical experiment in a virtual environment and its instantiation are described in terms of ON-SMMILE.}
}
@article{BOOSHEHRI2021100074,
title = {Introducing the Open Energy Ontology: Enhancing data interpretation and interfacing in energy systems analysis},
journal = {Energy and AI},
volume = {5},
pages = {100074},
year = {2021},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2021.100074},
url = {https://www.sciencedirect.com/science/article/pii/S2666546821000288},
author = {Meisam Booshehri and Lukas Emele and Simon Flügel and Hannah Förster and Johannes Frey and Ulrich Frey and Martin Glauer and Janna Hastings and Christian Hofmann and Carsten Hoyer-Klick and Ludwig Hülk and Anna Kleinau and Kevin Knosala and Leander Kotzur and Patrick Kuckertz and Till Mossakowski and Christoph Muschner and Fabian Neuhaus and Michaja Pehl and Martin Robinius and Vera Sehn and Mirjam Stappel},
keywords = {Collaborative ontology development, Linked open data, Metadata annotation, Energy systems analysis},
abstract = {Heterogeneous data, different definitions and incompatible models are a huge problem in many domains, with no exception for the field of energy systems analysis. Hence, it is hard to re-use results, compare model results or couple models at all. Ontologies provide a precisely defined vocabulary to build a common and shared conceptualisation of the energy domain. Here, we present the Open Energy Ontology (OEO) developed for the domain of energy systems analysis. Using the OEO provides several benefits for the community. First, it enables consistent annotation of large amounts of data from various research projects. One example is the Open Energy Platform (OEP). Adding such annotations makes data semantically searchable, exchangeable, re-usable and interoperable. Second, computational model coupling becomes much easier. The advantages of using an ontology such as the OEO are demonstrated with three use cases: data representation, data annotation and interface homogenisation. We also describe how the ontology can be used for linked open data (LOD).}
}
@article{LASTRADIAZ2019645,
title = {A reproducible survey on word embeddings and ontology-based methods for word similarity: Linear combinations outperform the state of the art},
journal = {Engineering Applications of Artificial Intelligence},
volume = {85},
pages = {645-665},
year = {2019},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2019.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0952197619301745},
author = {Juan J. Lastra-Díaz and Josu Goikoetxea and Mohamed Ali {Hadj Taieb} and Ana García-Serrano and Mohamed {Ben Aouicha} and Eneko Agirre},
keywords = {Ontology-based semantic similarity measures, Word embedding models, Information Content models, WordNet, Experimental survey, HESML},
abstract = {Human similarity and relatedness judgements between concepts underlie most of cognitive capabilities, such as categorisation, memory, decision-making and reasoning. For this reason, the proposal of methods for the estimation of the degree of similarity and relatedness between words and concepts has been a very active line of research in the fields of artificial intelligence, information retrieval and natural language processing among others. Main approaches proposed in the literature can be categorised in two large families as follows: (1) Ontology-based semantic similarity Measures (OM) and (2) distributional measures whose most recent and successful methods are based on Word Embedding (WE) models. However, the lack of a deep analysis of both families of methods slows down the advance of this line of research and its applications. This work introduces the largest, reproducible and detailed experimental survey of OM measures and WE models reported in the literature which is based on the evaluation of both families of methods on a same software platform, with the aim of elucidating what is the state of the problem. We show that WE models which combine distributional and ontology-based information get the best results, and in addition, we show for the first time that a simple average of two best performing WE models with other ontology-based measures or WE models is able to improve the state of the art by a large margin. In addition, we provide a very detailed reproducibility protocol together with a collection of software tools and datasets as supplementary material to allow the exact replication of our results.}
}
@article{STOYANOVADOYCHEVA2022206,
title = {Event Ontology about Wheat Cultivation},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {32},
pages = {206-210},
year = {2022},
note = {7th IFAC Conference on Sensing, Control and Automation Technologies for Agriculture AGRICONTROL 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.11.140},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322027732},
author = {A. Stoyanova-Doycheva and E. Doychev and V. Ivanova and V. Valkanov and V. Tabakova-Komsalova},
keywords = {wheat event ontology, intelligent agriculture, intelligent agriculture environment, ZEMELA},
abstract = {The article presents the development of the WheatEventOntology that includes events in the cultivation of winter wheat. The ontology divides events into two main types – domain events and emergency events. Each of the domain events leads to a state, in which the wheat is present during cultivation. Emergency events are conditions of wheat or soil, the overlook of which will lead to reduced yields. The WheatEventOntology is created to be integrated into an intelligent agricultural environment called ZEMELA. It is implemented from intelligent components that use the ontology to warn of various wheat-growing events, which facilitate the work of farmers.}
}
@article{YANG2025,
title = {Large Language Model–Driven Knowledge Graph Construction in Sepsis Care Using Multicenter Clinical Databases: Development and Usability Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/65537},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125004534},
author = {Hao Yang and Jiaxi Li and Chi Zhang and Alejandro Pazos Sierra and Bairong Shen},
keywords = {sepsis, knowledge graph, large language models, prompt engineering, real-world, GPT-4.0},
abstract = {Background
Sepsis is a complex, life-threatening condition characterized by significant heterogeneity and vast amounts of unstructured data, posing substantial challenges for traditional knowledge graph construction methods. The integration of large language models (LLMs) with real-world data offers a promising avenue to address these challenges and enhance the understanding and management of sepsis.
Objective
This study aims to develop a comprehensive sepsis knowledge graph by leveraging the capabilities of LLMs, specifically GPT-4.0, in conjunction with multicenter clinical databases. The goal is to improve the understanding of sepsis and provide actionable insights for clinical decision-making. We also established a multicenter sepsis database (MSD) to support this effort.
Methods
We collected clinical guidelines, public databases, and real-world data from 3 major hospitals in Western China, encompassing 10,544 patients diagnosed with sepsis. Using GPT-4.0, we used advanced prompt engineering techniques for entity recognition and relationship extraction, which facilitated the construction of a nuanced sepsis knowledge graph.
Results
We established a sepsis database with 10,544 patient records, including 8497 from West China Hospital, 690 from Shangjin Hospital, and 357 from Tianfu Hospital. The sepsis knowledge graph comprises of 1894 nodes and 2021 distinct relationships, encompassing nine entity concepts (diseases, symptoms, biomarkers, imaging examinations, etc) and 8 semantic relationships (complications, recommended medications, laboratory tests, etc). GPT-4.0 demonstrated superior performance in entity recognition and relationship extraction, achieving an F1-score of 76.76 on a sepsis-specific dataset, outperforming other models such as Qwen2 (43.77) and Llama3 (48.39). On the CMeEE dataset, GPT-4.0 achieved an F1-score of 65.42 using few-shot learning, surpassing traditional models such as BERT-CRF (62.11) and Med-BERT (60.66). Building upon this, we compiled a comprehensive sepsis knowledge graph, comprising of 1894 nodes and 2021 distinct relationships.
Conclusions
This study represents a pioneering effort in using LLMs, particularly GPT-4.0, to construct a comprehensive sepsis knowledge graph. The innovative application of prompt engineering, combined with the integration of multicenter real-world data, has significantly enhanced the efficiency and accuracy of knowledge graph construction. The resulting knowledge graph provides a robust framework for understanding sepsis, supporting clinical decision-making, and facilitating further research. The success of this approach underscores the potential of LLMs in medical research and sets a new benchmark for future studies in sepsis and other complex medical conditions.}
}
@article{BAYOUDHI20214249,
title = {An Overview of Biomedical Ontologies for Pandemics and Infectious Diseases Representation},
journal = {Procedia Computer Science},
volume = {192},
pages = {4249-4258},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.201},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921019402},
author = {Leila Bayoudhi and Najla Sassi and Wassim Jaziri},
keywords = {Pandemics, Infectious diseases, Biomedical, Ontology, Representation},
abstract = {Several infectious diseases and pandemics have so far emerged. Pandemics are by nature rapidly evolving. In this context, COVID-19 cases, seen recently in a growing number of countries around the world, have been increasing exponentially. So, researchers and responsible actors should take quick decisions to mitigate the spread of such diseases. To do so, several computer science solutions, including ontologies, have been proposed to cope with these issues and save humanity. The ontology is the key formalism which allows modelling knowledge along with its semantics in a formal way. Indeed, the ontology provides unambiguous definitions of a discourse’s domain terms in a machine understandable way. Particularly, biomedical ontologies have ever been developed to capture and represent pandemics and infectious diseases. In this context, this paper aims to scrutinize and study these state-of-the-art ontologies.}
}
@article{CHU2025128760,
title = {A large language model-enhanced argument extraction and clustering model for urban hotspot event detection using crowdsourced data},
journal = {Expert Systems with Applications},
volume = {293},
pages = {128760},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128760},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425023784},
author = {Tianyou Chu and Yumin Chen and John P. Wilson and Licheng Liu},
keywords = {Large Language Model, Incremental clustering, Event argument extraction, Graph contrastive learning, Natural Language Processing},
abstract = {Large Language Models (LLMs) have significantly improved performance in tasks such as text generation and question answering. However, their potential in clustering-based tasks, especially event detection, remains to be explored. The challenge of event detection is detecting different event clusters within continuous data streams. Current methods rely on constructing a global event-relationship graph and training Graph Neural Networks for event embedding and clustering, which is difficult to apply to long-term, large-scale detection. Meanwhile, urban hotspot event detection requires the identification of diverse events affecting residents at a specific spatial–temporal range, based on large volumes of crowdsourced citizen reports. Therefore, a Large Language Model-enhanced argument extraction and clustering (LLM-AEC) model is proposed, introducing a novel framework for improved situational awareness of the urban living environment. The model proposes an open event argument extraction approach using LLMs to comprehensively enhance the semantic representation of event graph nodes, particularly their locations, thereby reducing the dependence on global event relationships. Additionally, spatial locations are considered to improve the incremental clustering method for continuously detecting local urban events. Long-term data is segmented into time blocks to perform evaluation experiments. The proposed method shows stable improvements of 3.17 %–33.21 % compared to Word2Vec, Sentence-BERT, QSGNN, and HISEvent. To further validate the effectiveness of LLM-enhanced extraction and explore its potential applications, the relationships between concurrent events are quantified through an event association graph based on the extraction and detection results. This approach facilitates the tracking of specific event clusters and supports improvements in urban environments and resilience.}
}
@article{LABIDI2018292,
title = {Cloud SLA Terms Analysis Based On Ontology},
journal = {Procedia Computer Science},
volume = {126},
pages = {292-301},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.263},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312390},
author = {Taher Labidi and Achraf Mtibaa and Faiez Gargouri},
keywords = {Service Level Agreement, Cloud Computing, Ontology, SLA Analyzing, SLA Termination},
abstract = {Services in cloud computing run under specified constraints defined in Service Level Agreement (SLA) which becomes the basic core that guarantee the Quality of Service (QoS). However, the ambiguity in agreement terms makes the consumer face new challenges especially in understanding and analyzing SLA document. These challenges increasingly rise when the consumer uses services from multiple providers; since each one has its own SLA terminology. In this paper, we have significantly automated the process of managing and analyzing cloud SLA using semantic web technologies like OWL (Ontology Web Language), SWRL (Semantic Web Rule Language) and SQWRL (Semantic Query-Enhanced Web Rule Language). We describe the cloud SLA modeling and analyzing approach that assists consumers automatically analyze terms of various SLA documents. A prototype implementation demonstrates the feasibility and the efficiency of our approach.}
}
@article{ZHANG2024100549,
title = {Automatic bridge inspection database construction through hybrid information extraction and large language models},
journal = {Developments in the Built Environment},
volume = {20},
pages = {100549},
year = {2024},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2024.100549},
url = {https://www.sciencedirect.com/science/article/pii/S2666165924002308},
author = {Chenhong Zhang and Xiaoming Lei and Ye Xia and Limin Sun},
keywords = {Bridge inspection data, Natural language processing, Information extraction, Large languge model, Pseudo label},
abstract = {Regular bridge inspections generate extensive reports that, while critical for maintenance, often remain underutilized due to their unstructured format. Traditional information extraction methods depend on intricate labeling systems that commonly require time-consuming and labor-intensive labeling. This paper presents a novel bridge inspection database construction method leveraging LLM-assisted information extraction. First, we introduce the pseudo-labelling method using a closed-source LLM to generate high-quality data. Then we propose the hybrid extraction pipeline to extract relevant information segments and process them by a generation-based IE model, fine-tuned on pseudo-labeled data. Finally, the extracted data is used to construct the bridge inspection database. The proposed method, validated with real-world data, not only demonstrates higher extraction precision than the closed-source LLM used for pseudo-labeling but also outperforms traditional methods in both data preparation time and extraction accuracy. This approach provides a scalable solution for more proactive and data-driven bridge maintenance strategies.}
}
@article{REGAL20181511,
title = {Ontology for Conceptual Modelling of Intelligent Maintenance Systems and Spare Parts Supply Chain Integration},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {1511-1516},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.285},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314095},
author = {Thiago Regal and Carlos Eduardo Pereira},
abstract = {With an increasing demand for more efficiency and less costs in industry, the integration between Intelligent Maintenance Systems (IMS) and Spare Parts Supply Chain (SPSC) results in better availability of parts and services, avoiding breakdowns and unplanned production interruptions. Better demand planning results in better cost-effectivity as well, as parts and services can be better planned and always available when needed. The proper integration of IMS and SPSC has challenges related to semantic differences between areas with diverse concepts and vocabularies. This work intends to explore these challenges and propose a conceptual model for such integration, using existing ontologies in domains such as supply chain, maintenance and manufacturing to build on top of them an ontology aimed at allowing conceptual integration between IMS and SPSC and becoming a foundation to build future information systems to integrate these two areas. Its purpose is also further explored by using the characteristics of an ontology as tool for semantic description. Artificial intelligence, reasoning and context-aware systems are some of the areas that can benefit from the existence of an ontology to model IMS and SPSC integration. In this paper, we explore such characteristics along with integration and interoperability obtained by using an ontological model in IMS and SPSC integration.}
}
@article{AHAMED2021377,
title = {RML based ontology development approach in internet of things for healthcare domain},
journal = {International Journal of Pervasive Computing and Communications},
volume = {17},
number = {4},
pages = {377-389},
year = {2021},
issn = {1742-7371},
doi = {https://doi.org/10.1108/IJPCC-01-2021-0026},
url = {https://www.sciencedirect.com/science/article/pii/S1742737121000223},
author = {Jameel Ahamed and Roohie Naaz Mir and Mohammad Ahsan Chishti},
keywords = {RDF, Ontology, SPARQL, Internet of things, Semantic interoperability},
abstract = {Purpose
A huge amount of diverse data is generated in the Internet of Things (IoT) because of heterogeneous devices like sensors, actuators, gateways and many more. Due to assorted nature of devices, interoperability remains a major challenge for IoT system developers. The purpose of this study is to use mapping techniques for converting relational database (RDB) to resource directory framework (RDF) for the development of ontology. Ontology helps in achieving semantic interoperability in application areas of IoT which results in shared/common understanding of the heterogeneous data generated by the diverse devices used in health-care domain.
Design/methodology/approach
To overcome the issue of semantic interoperability in healthcare domain, the authors developed an ontology for patients having cardio vascular diseases. Patients located at any place around the world can be diagnosed by Heart Experts located at another place by using this approach. This mechanism deals with the mapping of heterogeneous data into the RDF format in an integrated and interoperable manner. This approach is used to integrate the diverse data of heart patients needed for diagnosis with respect to cardio vascular diseases. This approach is also applicable in other fields where IoT is mostly used.
Findings
Experimental results showed that the RDF works better than the relational database for semantic interoperability in the IoT. This concept-based approach is better than key-based approach and reduces the computation time and storage of the data.
Originality/value
The proposed approach helps in overcoming the demerits of relational database like standardization, expressivity, provenance and supports SPARQL. Therefore, it helps to overcome the heterogeneity, thereby enabling the semantic interoperability in IoT.}
}
@article{GENG202047,
title = {Cross-domain ontology construction and alignment from online customer product reviews},
journal = {Information Sciences},
volume = {531},
pages = {47-67},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.03.058},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520302383},
author = {Qian Geng and Siyu Deng and Danping Jia and Jian Jin},
keywords = {Product review, Ontology construction, Ontology alignment, Product comparison, Purchase decision making},
abstract = {Online reviews often contain detailed sentiment towards different aspects of products and these opinions help consumers to be familiar with products. The introduction of domain ontology from online reviews may help consumers to obtain relevant information about products quickly. Nonetheless, they may compare products in multiple domains for purchase decisions. On this basis, the comparison of products in different domains induces that ontology alignment becomes a fundamental task to form a cross-domain ontology. However, due to large-scale text data and complex alignment mapping relations, many alignment algorithms are far from performing effectively. In this paper, a series of natural language processing approaches are applied to construct domain ontologies from online product reviews. Next, a new ontology alignment method is proposed to make purchase decisions regarding cross-domain product comparisons, in which a semantic-based algorithm and a structure-based algorithm are integrated to form a cross-domain ontology. Categories of experiments were conducted on reviews of smartphone and digital camera. Compared with benchmarked alignment tools, it shows that the proposed method yields to more accurate results. Finally, a case study with a customer friendly website is illustrated to present how the alignment of cross-domain ontology is able to help consumers on purchase decision support.}
}
@article{DOSSANTOS2024108422,
title = {Machine learning applied to digital phenotyping: A systematic literature review and taxonomy},
journal = {Computers in Human Behavior},
volume = {161},
pages = {108422},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108422},
url = {https://www.sciencedirect.com/science/article/pii/S0747563224002905},
author = {Marília Pit {dos Santos} and Wesllei Felipe Heckler and Rodrigo Simon Bavaresco and Jorge Luis Victória Barbosa},
keywords = {Systematic literature review, Digital phenotyping, Digital phenotype, Machine learning},
abstract = {Health conditions, encompassing both physical and mental aspects, hold an influence that extends beyond the individual. These conditions affect personal well-being, relationships, and financial stability. Innovative strategies in healthcare, such as digital phenotyping, are strategic to mitigate these impacts. By merging diverse data sources, digital phenotyping seeks a comprehensive understanding of health, well-being, and behavioral conditions. Machine learning can enhance the analysis of these data, improving the comprehension of health and well-being. Therefore, this paper presents a systematic literature review on machine learning and digital phenotyping, examining the research field by filtering 2,860 articles from eleven databases published up to November 2023. The analysis focused on 124 articles to answer six research questions addressing machine learning techniques, data, devices, ontologies, and research challenges. This work presents a taxonomy for mapping explored areas in digital phenotyping and another for organizing machine learning techniques used in digital phenotyping research. The review found increased publications in 2023, indicating a growing interest in the field. The main challenges arise from the studies’ small participant samples and imbalanced datasets, limiting the generalizability of the results to broader populations and the choice of ML methods. Furthermore, the reliance on self-reported data can introduce potential inaccuracies due to recall and reporting biases. Beyond self-reports, authors explored different data types, including physiological, clinical, contextual, smartphone-based, and multimedia. Despite using video recordings in controlled experiments, studies have yet to investigate this method within intelligent environments. Researchers also analyzed neurophysiological phenotypes, suggesting the potential for interventions based on these characteristics.}
}
@article{AFSHAR2023,
title = {Deployment of Real-time Natural Language Processing and Deep Learning Clinical Decision Support in the Electronic Health Record: Pipeline Implementation for an Opioid Misuse Screener in Hospitalized Adults},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/44977},
url = {https://www.sciencedirect.com/science/article/pii/S2291969423000303},
author = {Majid Afshar and Sabrina Adelaine and Felice Resnik and Marlon P Mundt and John Long and Margaret Leaf and Theodore Ampian and Graham J Wills and Benjamin Schnapp and Michael Chao and Randy Brown and Cara Joyce and Brihat Sharma and Dmitriy Dligach and Elizabeth S Burnside and Jane Mahoney and Matthew M Churpek and Brian W Patterson and Frank Liao},
keywords = {clinical decision support, natural language processing, medical informatics, opioid related disorder, opioid use, electronic health record, clinical note, cloud service, artificial intelligence, AI},
abstract = {Background
The clinical narrative in electronic health records (EHRs) carries valuable information for predictive analytics; however, its free-text form is difficult to mine and analyze for clinical decision support (CDS). Large-scale clinical natural language processing (NLP) pipelines have focused on data warehouse applications for retrospective research efforts. There remains a paucity of evidence for implementing NLP pipelines at the bedside for health care delivery.
Objective
We aimed to detail a hospital-wide, operational pipeline to implement a real-time NLP-driven CDS tool and describe a protocol for an implementation framework with a user-centered design of the CDS tool.
Methods
The pipeline integrated a previously trained open-source convolutional neural network model for screening opioid misuse that leveraged EHR notes mapped to standardized medical vocabularies in the Unified Medical Language System. A sample of 100 adult encounters were reviewed by a physician informaticist for silent testing of the deep learning algorithm before deployment. An end user interview survey was developed to examine the user acceptability of a best practice alert (BPA) to provide the screening results with recommendations. The planned implementation also included a human-centered design with user feedback on the BPA, an implementation framework with cost-effectiveness, and a noninferiority patient outcome analysis plan.
Results
The pipeline was a reproducible workflow with a shared pseudocode for a cloud service to ingest, process, and store clinical notes as Health Level 7 messages from a major EHR vendor in an elastic cloud computing environment. Feature engineering of the notes used an open-source NLP engine, and the features were fed into the deep learning algorithm, with the results returned as a BPA in the EHR. On-site silent testing of the deep learning algorithm demonstrated a sensitivity of 93% (95% CI 66%-99%) and specificity of 92% (95% CI 84%-96%), similar to published validation studies. Before deployment, approvals were received across hospital committees for inpatient operations. Five interviews were conducted; they informed the development of an educational flyer and further modified the BPA to exclude certain patients and allow the refusal of recommendations. The longest delay in pipeline development was because of cybersecurity approvals, especially because of the exchange of protected health information between the Microsoft (Microsoft Corp) and Epic (Epic Systems Corp) cloud vendors. In silent testing, the resultant pipeline provided a BPA to the bedside within minutes of a provider entering a note in the EHR.
Conclusions
The components of the real-time NLP pipeline were detailed with open-source tools and pseudocode for other health systems to benchmark. The deployment of medical artificial intelligence systems in routine clinical care presents an important yet unfulfilled opportunity, and our protocol aimed to close the gap in the implementation of artificial intelligence–driven CDS.
Trial Registration
ClinicalTrials.gov NCT05745480; https://www.clinicaltrials.gov/ct2/show/NCT05745480}
}
@article{RBOUL2024102391,
title = {Alternative knowledges in intercultural education and educators as epistemic subjects},
journal = {International Journal of Educational Research},
volume = {127},
pages = {102391},
year = {2024},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2024.102391},
url = {https://www.sciencedirect.com/science/article/pii/S0883035524000776},
author = {Hamza R'boul},
keywords = {Intercultural education, The peripheries, The global south, Alternative knowledges, Educators as epistemic subjects},
abstract = {The activist underpinnings of intercultural education may be more effective when the theories about the peripheries prioritize self-criticality in conceptualizing and implementing this pedagogy. This warrants examining how epistemic agency is exercised by educators to align the sociologies of intercultural education with the ecologies of their spaces. The aims of this paper are, (a) to unpack the cultural and epistemic formations of teachers’ understandings of the premises and objectives of intercultural education and (b) prompt southern epistemic subjects to produce knowledge about intercultural education that accounts for their epistemological positionalities and their consideration of local intimacies, needs and aspirations. This qualitative study uses email interviews with Moroccan EFL teachers to elicit their epistemic input rather than to situate their responses among mainstream literatures. Findings illuminated the complex epistemic processes that educators are engaged in to construct their situated knowledges informed by the intersection of available scholarships, their ontologies and contextual factors. Findings suggest that educators’ exercising of intercultural education is problematized by the lack of training which leads to ‘improvisation’ and the high-abstract rhetorics in literature which may require educators to sustain more efforts in addition to thorny attempt of doing intercultural education otherwise.}
}
@article{PELLISON2020,
title = {Data Integration in the Brazilian Public Health System for Tuberculosis: Use of the Semantic Web to Establish Interoperability},
journal = {JMIR Medical Informatics},
volume = {8},
number = {7},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/17176},
url = {https://www.sciencedirect.com/science/article/pii/S2291969420002173},
author = {Felipe Carvalho Pellison and Rui Pedro Charters Lopes Rijo and Vinicius Costa Lima and Nathalia Yukie Crepaldi and Filipe Andrade Bernardi and Rafael Mello Galliez and Afrânio Kritski and Kumar Abhishek and Domingos Alves},
keywords = {health information systems, tuberculosis, ontology, interoperability, electronic health records, semantic web},
abstract = {Background
Interoperability of health information systems is a challenge due to the heterogeneity of existing systems at both the technological and semantic levels of their data. The lack of existing data about interoperability disrupts intra-unit and inter-unit medical operations as well as creates challenges in conducting studies on existing data. The goal is to exchange data while providing the same meaning for data from different sources.
Objective
To find ways to solve this challenge, this research paper proposes an interoperability solution for the tuberculosis treatment and follow-up scenario in Brazil using Semantic Web technology supported by an ontology.
Methods
The entities of the ontology were allocated under the definitions of Basic Formal Ontology. Brazilian tuberculosis applications were tagged with entities from the resulting ontology.
Results
An interoperability layer was developed to retrieve data with the same meaning and in a structured way enabling semantic and functional interoperability.
Conclusions
Health professionals could use the data gathered from several data sources to enhance the effectiveness of their actions and decisions, as shown in a practical use case to integrate tuberculosis data in the State of São Paulo.}
}
@article{MEIER2021448,
title = {Knowledge Graph for the Visualisation of CRM Objects in a Social Network of Business Objects (SoNBO): Development of the SoNBO Visualiser},
journal = {Procedia Computer Science},
volume = {181},
pages = {448-456},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002325},
author = {Simon Meier and Berit Gebel-Sauer and Petra Schubert},
keywords = {Ontology, Business Object, Knowledge graph, SoNBO, Social Network of Business Objects},
abstract = {This paper investigates possibilities to visually support the collaborative design process of Enterprise Knowledge Graphs for Business Application Systems. Starting with a description of the concept of the Social Network of Business Objects (SoNBO), we show how the SoNBO Visualiser supports the visualisation of a company-specific ontology and the resulting knowledge graph. Our previous research showed that existing visualisation tools from the Semantic Web are unsuited for the human involvement that is required in the design of an ontology for a SoNBO. In order to address the lack of a tool, the SoNBO Visualiser was developed using a Design Science Research approach. This paper aims to provide a methodological as well as a technical contribution: The method provides support for the visualisation of the ontology (on two levels) and facilitates the involvement of domain experts (employees) without technical knowledge in the design process. The complementary technical solution connects heterogeneous source systems in a simple configuration process and allows to visualise the ontology and the resulting knowledge graph.}
}
@article{JAHANIYEKTA2024100078,
title = {The general intelligence of GPT–4, its knowledge diffusive and societal influences, and its governance},
journal = {Meta-Radiology},
volume = {2},
number = {2},
pages = {100078},
year = {2024},
issn = {2950-1628},
doi = {https://doi.org/10.1016/j.metrad.2024.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2950162824000316},
author = {Mohammad Mahdi {Jahani Yekta}},
keywords = {GPT–4, Artificial general intelligence, Knowledge diffusion, Interpretability and explainability, Societal influences, Governance},
abstract = {Recent breakthroughs in artificial intelligence (AI) research include advancements in natural language processing (NLP) achieved by large language models (LLMs), and; in particular, generative pre–trained transformer (GPT) architectures. The latest GPT developed by OpenAI, GPT–4, has shown remarkable intelligence across various domains and tasks. It exhibits capabilities in abstraction, comprehension, vision, computer coding, mathematics, and more, suggesting it to be a significant step towards artificial general intelligence (AGI), a level of AI that possesses capabilities similar to human intelligence. This paper explores this AGI, its knowledge diffusive and societal influences, and its governance. In addition to coverage of the major associated topics studied in the literature, and making up for their loopholes, we scrutinize how GPT-4 can facilitate the diffusion of knowledge across different areas of science by promoting their interpretability and explainability (IE) to inexperts. Where applicable, the topics are also accompanied by their specific potential implications on medical imaging.}
}
@article{ARISTA20191584,
title = {Framework to support Models for Manufacturing (MfM) methodology},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {13},
pages = {1584-1589},
year = {2019},
note = {9th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.426},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319314077},
author = {R. Arista and F. Mas and M. Oliva and J. Racero and D. Morales-Palma},
keywords = {Models for Manufacturing (MfM), Model Lifecycle Management (MLM), 3-Layer Model (3LM), Ontologies, Manufacturing modelling},
abstract = {Model-Based System Engineering (MBSE) provides a conceptualization of systems and proposes the independence between knowledge conceptualization and tools, reusability and interoperability. In the scope of manufacturing there are several research works oriented to define ontologies, but only a few consider a framework to manage the whole lifecycle of the models and software tools for supporting this entire process. The aim of this work is to present a framework supporting Models for Manufacturing (MfM) methodology, which allows the definition, design and simulation of complex manufacturing systems. The framework will be defined around a collaborative architecture based on a commercial Free and Open-Source Software (FOSS), ARAS Innovator, which supports connecting software tools to develop and define models and its lifecycle. The framework, called Model Lifecycle Management (MLM), has been tested with several aerospace and non-aerospace use cases and is the core software tool supporting the MfM methodology. MfM methodology has been proposed by the authors as a new approach to apply MBSE concepts to Manufacturing, and aims to provide a set of processes, methods and associated tools to support the discipline of manufacturing in a model-based context.}
}
@article{FAVI2022101537,
title = {Engineering knowledge formalization and proposition for informatics development towards a CAD-integrated DfX system for product design},
journal = {Advanced Engineering Informatics},
volume = {51},
pages = {101537},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101537},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622000131},
author = {Claudio Favi and Federico Campi and Michele Germani and Marco Mandolini},
keywords = {DfX, CAD, Design guidelines, Design rules, Feature recognition, Engineering knowledge, Ontology},
abstract = {Target design methodologies (DfX) were developed to cope with specific engineering design issues such as cost-effectiveness, manufacturability, assemblability, maintainability, among others. However, DfX methodologies are undergoing the lack of real integration with 3D CAD systems. Their principles are currently applied downstream of the 3D modelling by following the well-known rules available from the literature and engineers’ know-how (tacit internal knowledge). This paper provides a method to formalize complex DfX engineering knowledge into explicit knowledge that can be reused for Advanced Engineering Informatics to aid designers and engineers in developing mechanical products. This research work wants to define a general method (ontology) able to couple DfX design guidelines (engineering knowledge) with geometrical product features of a product 3D model (engineering parametric data). A common layer for all DfX methods (horizontal) and dedicated layers for each DfX method (vertical) allow creating the suitable ontology for the systematic collection of the DfX rules considering each target. Moreover, the proposed framework is the first step for developing (future work) a software tool to assist engineers and designers during product development (3D CAD modelling). A design for assembly (DfA) case study shows how to collect assembly rules in the given framework. It demonstrates the applicability of the CAD-integrated DfX system in the mechanical design of a jig-crane. Several benefits are recognized: (i) systematic collection of DfA rules for informatics development, (ii) identification of assembly issues in the product development process, and (iii) reduction of effort and time during the design review.}
}
@article{PILLAI2023,
title = {Toward Community-Based Natural Language Processing (CBNLP): Cocreating With Communities},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/48498},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123006003},
author = {Malvika Pillai and Ashley C Griffin and Clair A Kronk and Terika McCall},
keywords = {ChatGPT, natural language processing, community-based participatory research, research design, artificial intelligence, participatory, co-design, machine learning, co-creation, community based, lived experience, lived experiences, collaboration, collaborative},
abstract = {Rapid development and adoption of natural language processing (NLP) techniques has led to a multitude of exciting and innovative societal and health care applications. These advancements have also generated concerns around perpetuation of historical injustices and that these tools lack cultural considerations. While traditional health care NLP techniques typically include clinical subject matter experts to extract health information or aid in interpretation, few NLP tools involve community stakeholders with lived experiences. In this perspective paper, we draw upon the field of community-based participatory research, which gathers input from community members for development of public health interventions, to identify and examine ways to equitably involve communities in developing health care NLP tools. To realize the potential of community-based NLP (CBNLP), research and development teams must thoughtfully consider mechanisms and resources needed to effectively collaborate with community members for maximal societal and ethical impact of NLP-based tools.}
}
@article{ROGER2022119672,
title = {Unraveling the functional attributes of the language connectome: crucial subnetworks, flexibility and variability},
journal = {NeuroImage},
volume = {263},
pages = {119672},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119672},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922007935},
author = {E. Roger and L. {Rodrigues De Almeida} and H. Loevenbruck and M. Perrone-Bertolotti and E. Cousin and J.L. Schwartz and P. Perrier and M. Dohen and A. Vilain and P. Baraduc and S. Achard and M. Baciu},
keywords = {Language, fMRI, Connectome, Functional connectivity, Networks},
abstract = {Language processing is a highly integrative function, intertwining linguistic operations (processing the language code intentionally used for communication) and extra-linguistic processes (e.g., attention monitoring, predictive inference, long-term memory). This synergetic cognitive architecture requires a distributed and specialized neural substrate. Brain systems have mainly been examined at rest. However, task-related functional connectivity provides additional and valuable information about how information is processed when various cognitive states are involved. We gathered thirteen language fMRI tasks in a unique database of one hundred and fifty neurotypical adults (InLang [Interactive networks of Language] database), providing the opportunity to assess language features across a wide range of linguistic processes. Using this database, we applied network theory as a computational tool to model the task-related functional connectome of language (LANG atlas). The organization of this data-driven neurocognitive atlas of language was examined at multiple levels, uncovering its major components (or crucial subnetworks), and its anatomical and functional correlates. In addition, we estimated its reconfiguration as a function of linguistic demand (flexibility) or several factors such as age or gender (variability). We observed that several discrete networks could be specifically shaped to promote key functional features of language: coding-decoding (Net1), control-executive (Net2), abstract-knowledge (Net3), and sensorimotor (Net4) functions. The architecture of these systems and the functional connectivity of the pivotal brain regions varied according to the nature of the linguistic process, gender, or age. By accounting for the multifaceted nature of language and modulating factors, this study can contribute to enriching and refining existing neurocognitive models of language. The LANG atlas can also be considered a reference for comparative or clinical studies involving various patients and conditions.}
}
@article{BAIZAL2020101813,
title = {Computational model for generating interactions in conversational recommender system based on product functional requirements},
journal = {Data & Knowledge Engineering},
volume = {128},
pages = {101813},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101813},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X1830524X},
author = {Z.K.A. Baizal and Dwi H. Widyantoro and Nur Ulfa Maulidevi},
keywords = {Recommender systems, Conversational recommender system, Knowledge-based recommendation, User modeling, Ontology-based knowledge, User interaction},
abstract = {Conversational recommender system is a tool to help customer in deciding products they are going to buy, by conversational mechanism. By this mechanism, the system is able to imitate natural conversation between customer and professional sales support, for eliciting customer preference. However, many customers are not familiar with the technical features of multi-function and multi-feature products. A more natural way to explore customer preferences is by asking what they want to use with the product they are looking for (product functional requirements). Therefore, this paper proposes a computational model incorporating product functional requirements for interaction. The proposed model covers ontology and its structure as well as algorithms for generating interaction that comprises asking question, recommending products and presenting explanation of why a product is recommended. Based on our user studies, both expert users (familiar with product technical features) and novice users (not familiar with product technical feature) prefer our proposed interaction model than that of the flat interaction model (interaction model based on technical features). Meanwhile, functional requirements-based explanation is able to improve user trust in recommended products by 30% for novice users and 17% for expert users.}
}
@article{KIM2018337,
title = {Formal representation of cost and duration estimates for hard rock tunnel excavation},
journal = {Automation in Construction},
volume = {96},
pages = {337-349},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0926580518300451},
author = {Jung In Kim and Martin Fischer and Min Jae Suh},
keywords = {Tunnel, Scheduling, Excavation, Construction method model, Building information modeling, Rock mass properties, Schedule adjustment policy},
abstract = {Due to the inherent uncertainties of rock mass properties, construction planners of hard rock tunnels have difficulty achieving on-time completion within budget. Despite the potential benefits of adapting stochastic programming and feedback control approaches for decision-making for excavation schedules, the lack of formal representations of the planners' rationales required to estimate the costs and durations of excavation schedules makes the implementation of these approaches extremely challenging. To address these limitations, the authors developed an ontology that represents the estimation rationales (e.g., transition costs and durations among excavation methods, multiple sets of rock mass properties, and schedule adjustment policies). This ontology enables planners to explicitly describe more the comprehensive information required to consistently estimate the costs and durations of excavation schedules for both preconstruction and construction compared to the current practices and the existing studies. Further research that accounts for learning effects resulting from transitions among excavation methods would make cost and duration estimations for excavation schedules more realistic.}
}
@article{IGAMBERDIEV2023104983,
title = {Reflexive structure of the conscious subject and the origin of language codes},
journal = {Biosystems},
volume = {231},
pages = {104983},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104983},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001582},
author = {Abir U. Igamberdiev},
keywords = {Aristotle, Code biology, Common language, Consciousness, Musical code, Reflexive psychology, Self-awareness, Social dynamics},
abstract = {The code paradigm in biological and social sciences arises to Aristotle. For conscious activity, Aristotle introduced the notion of reflexive self-awareness in sense perception. This reflexive process generates the codes that signify sensual perceptive events and constrain human behavior. Coding systems grow via the generation of hypertextual statements reflecting new meanings in the process defined by Marcello Barbieri as a codepoiesis. It results in the establishment of higher-level codes (metacodes) forming the semiotic screen that has a nature of the set of perceived objects internalized by the conscious subject in encoding the symbolic actions. The characteristic feature of the semiotic screen consists in its property of being shared between the communicating agents. A sufficient complexity of nervous system, through the appearance of mirror neurons that are fired both when a subject executes certain action and when he observes another subject performing a similar action, represents a prerequisite for the emergence of reflexive codes in evolution. The codes appearing as a result of reflexive awareness and establishing different sociotypes, span from the symbolic systems of art and music through the common language to the formal language of logic and mathematics. Social dynamics is based on the implementation of reflexive coding activity and results in the growth and decay of social systems and civilizations.}
}
@article{BEAL2021103134,
title = {Cognition in moral space: A minimal model},
journal = {Consciousness and Cognition},
volume = {92},
pages = {103134},
year = {2021},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2021.103134},
url = {https://www.sciencedirect.com/science/article/pii/S105381002100060X},
author = {Bree Beal and Guram Gogia},
keywords = {Minimal model, Moral cognition, Moral psychology, Moral relationship, Moral space, Ontological framing},
abstract = {We describe moral cognition as a process occurring in a distinctive cognitive space, wherein moral relationships are defined along several morally relevant dimensions. After identifying candidate dimensions, we show how moral judgments can emerge in this space directly from object perception, without any appeal to moral rules or abstract values. Our reductive “minimal model” (Batterman & Rice, 2014) elaborates Beal’s (2020) claim that moral cognition is determined, at the most basic level, by “ontological frames” defining subjects, objects, and the proper relation between them. We expand this claim into a set of formal hypotheses that predict moral judgments based on how objects are “framed” in the relevant dimensions of “moral space.”}
}
@article{ODUNAYO2021761,
title = {A systematic mapping study of cloud policy languages and programming models},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {33},
number = {7},
pages = {761-768},
year = {2021},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1319157819301533},
author = {Isaac Odun-Ayo and Rowland Goddy-Worlu and Jamaiah Yahaya and Victor Geteloma},
keywords = {Cloud computing, Cloud computing policy languages, Programming models, Systematic mapping},
abstract = {Cloud computing can be considered as a disruptive technology that is making life easier for Cloud users. Determining a focus of research in a specific subject area is sometimes challenging. A systematic map enables a synthesis of a scheme for categorizing data in a field of interest. The goal of this research paper is to carry out a systematic mapping study of policy language and programming models on the cloud. The mapping involved contribution category such as method, research category such as evaluation and major topics extracted from the abstracts of primary studies. The result indicated there are more publications on evaluation research in term of security with 8.9%. There were more papers published on validation research, solution proposal and experience research on the topic of paradigms with 7.53%, 6.85% and 4.11% respectively. Also, there were more publications on philosophical research in terms of privacy with 4.11%. In addition, there were more articles published on opinion research in terms of the survey with 4.11%. On the other hand, there were no articles on metric in terms of framework, paradigms and accountability, and reliability to the best of the researchers’ knowledge. The outcome of this systematic study will be of benefit to cloud users, researchers, practitioners and providers.}
}
@article{MONTICOLO2020100124,
title = {OCEAN: A multi agent system dedicated to knowledge management},
journal = {Journal of Industrial Information Integration},
volume = {17},
pages = {100124},
year = {2020},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2019.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X19300809},
author = {Davy Monticolo and Inaya Lahoud and Pedro Chavez Barrios},
keywords = {Agent-based modeling, Ontology, Semantic approach, Knowledge management},
abstract = {Emphasis on knowledge and information is one of the challenges of the 21st century to differentiate the intelligent business enterprises. Enterprises have to develop their organization in order to capture, manage, and use information in a context of continually changing technology. Indeed knowledge and information are completely distributed in the information network of the company. In addition, knowledge is by nature, heterogeneous since it is provided from different information sources like the software, the technical report, the meeting statements, etc. We present in this paper the architecture of a multi-agent system, which allows the capitalization of the distributed and heterogeneous knowledge. We then present how the agents help business experts to design ontologies in detailing this problematic and how the agents extract knowledge from different users’ databases by using a semantic approach.}
}
@article{TRAPPEY2022,
title = {Intelligent RFQ Summarization Using Natural Language Processing, Text Mining, and Machine Learning Techniques},
journal = {Journal of Global Information Management},
volume = {30},
number = {1},
year = {2022},
issn = {1062-7375},
doi = {https://doi.org/10.4018/JGIM.309082},
url = {https://www.sciencedirect.com/science/article/pii/S1062737522002025},
author = {Amy J. C. Trappey and Ai-Che Chang and Charles V. Trappey and Jack Y. C. Chang Chien},
keywords = {  ,   ,   ,   ,   ,   },
abstract = {ABSTRACT
Request for quotation (RFQ) is a lengthy document soliciting vendor products and services according to rigid specifications. This research develops an integrated natural language processing (NLP), text mining, and machine learning approach for intelligent RFQ summarization. Over 1,300 power transformer RFQ requests are used to build a word-embedding model for training and testing. Domain keywords are extracted using N-gram TF-IDF. The method automatically extracts essential specifications such as voltage, capacity, and impedance from RFQs using text analytics. The K-means algorithm groups the sentences of each specification. The TextRank algorithm identifies important sentences of all specifications to generate RFQ summaries. The summarization system helps engineers shorten the time to identify all specifications and reduces the risk of missing important requirements during manual RFQ reading. The system helps improve the complex product design for manufacturers and improve the cost estimation and competitiveness of quotations in a highly competitive marketplace.}
}
@incollection{GOLEBIEWSKI2019884,
title = {Data Formats for Systems Biology and Quantitative Modeling},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {884-893},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20471-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338204718},
author = {Martin Golebiewski},
keywords = {CellML, COMBINE, FAIR data, Modeling, NormSys registry, SBGN, SBML, SED-ML, Standards, Systems biology},
abstract = {Data standards support the reliable exchange of information, the interoperability of tools, and the reproducibility of scientific results. In systems biology standards are agreed ways of structuring, describing, and associating models and data, as well as their respective parts, graphical visualization, and information about applied experimental or computational methods. Such standards also assist with describing how constituent parts interact together, or are linked, and how they are embedded in their environmental and experimental context. Here the focus will be on standards for formatting models and their content, and on metadata checklists and ontologies that support modeling.}
}
@article{BRYG2024109,
title = {New Approach for Detecting Smartphones Securely for Disassembly Tasks},
journal = {Procedia CIRP},
volume = {122},
pages = {109-114},
year = {2024},
note = {31st CIRP Conference on Life Cycle Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124000349},
author = {Maximilian Bryg and Simon Volz and Maximilian Lochner and Lucas Vidal and Thomas Bertram and Martin Kipfmüller},
keywords = {smartphone recycling, smartphone model recognition, convolutional neuronal network},
abstract = {The constantly increasing demand for raw materials is pushing the material footprint ever higher. As a result, planetary boundaries are being reached earlier and earlier each year. To counteract this trend, it is essential to reuse material and think in terms of cycles. This includes recycling activities, especially in high-wage countries. To make this economically viable, these processes need to be automated and secure. Many mobile devices contain lithium batteries, which are dangerous to humans and the environment if damaged. To recover the raw materials cost-effectively, a system is needed to remove these batteries securely. This paper presents a safe strategy to reliably identify old smartphones to provide appropriate disassembly strategies for a collection site. This enables the collection sites, a fully autonomous and, above all, safe extraction of the batteries from smartphones. This is done by using convolutional neural networks in combination with ontologies.}
}
@article{BENITEZANDRADES2020154,
title = {Social network analysis for personalized characterization and risk assessment of alcohol use disorders in adolescents using semantic technologies},
journal = {Future Generation Computer Systems},
volume = {106},
pages = {154-170},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19316796},
author = {José Alberto Benítez-Andrades and Isaías García-Rodríguez and Carmen Benavides and Héctor Alaiz-Moretón and Alejandro Rodríguez-González},
keywords = {Healthcare information system, Knowledge based systems, Ontologies, Semantic web, Social network analysis},
abstract = {Alcohol Use Disorder (AUD) is a major concern for public health organizations worldwide, especially as regards the adolescent population. The consumption of alcohol in adolescents is known to be influenced by seeing friends and even parents drinking alcohol. Building on this fact, a number of studies into alcohol consumption among adolescents have made use of Social Network Analysis (SNA) techniques to study the different social networks (peers, friends, family, etc.) with whom the adolescent is involved. These kinds of studies need an initial phase of data gathering by means of questionnaires and a subsequent analysis phase using the SNA techniques. The process involves a number of manual data handling stages that are time consuming and error-prone. The use of knowledge engineering techniques (including the construction of a domain ontology) to represent the information, allows the automation of all the activities, from the initial data collection to the results of the SNA study. This paper shows how a knowledge model is constructed, and compares the results obtained using the traditional method with this, fully automated model, detailing the main advantages of the latter. In the case of the SNA analysis, the validity of the results obtained with the knowledge engineering approach are compared to those obtained manually using the UCINET, Cytoscape, Pajek and Gephi to test the accuracy of the knowledge model.}
}
@article{DENICOLA2023100489,
title = {Development and measurement of a resilience indicator for cyber-socio-technical systems: The allostatic load},
journal = {Journal of Industrial Information Integration},
volume = {35},
pages = {100489},
year = {2023},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2023.100489},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X23000626},
author = {Antonio {De Nicola} and Maria Luisa Villani and Mark Sujan and John Watt and Francesco Costantino and Andrea Falegnami and Riccardo Patriarca},
keywords = {Resilience, Cyber-socio-technical system, Ontology, Semantic similarity, Business process, Leading indicator},
abstract = {Management of cyber-socio-technical processes often suffers from misalignments of process descriptions according to formal organization documents or manager views (Work-As-Imagined) with actual work practices as performed by sharp-end operators (Work-As-Done). Even if sometimes the accomplishment of a process requires workers to diverge from the Work-As-Imagined, the corresponding changes can potentially cause organizational tensions in the overall system and lead to safety incidents. This consideration led us to define a new resilience indicator, named allostatic load, to capture such misalignments, and the corresponding level of organizational tensions, a cyber-socio-technical system is exposed to. Then, we propose a method to measure it by leveraging semantic technologies, the Functional Resonance Analysis Method (FRAM) to model industrial processes, the WAx conceptual framework to keep track of the variety of the different process perspectives, and a crowd-based approach to elicit industrial knowledge. Finally, we discuss the feasibility of the approach in two real case studies related to a pharmaceutical manufacturing plant and an enterprise in the aluminium sector.}
}
@article{HECKLER2023120918,
title = {Thoth: An intelligent model for assisting individuals with suicidal ideation},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120918},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120918},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423014203},
author = {Wesllei Felipe Heckler and Luan Paris Feijó and Juliano Varella {de Carvalho} and Jorge Luis Victória Barbosa},
keywords = {Context information, Machine learning, Mental health, Suicidal ideation, Suicide prevention, Patient assistance},
abstract = {Suicide causes approximately 800,000 deaths worldwide annually, meaning one death every 40 s. Suicidal ideation is the first stage in the risk scale, in which the individuals have thoughts about being dead. Prevention strategies may focus on individuals in this stage. Therefore, this research proposes Thoth, an intelligent model for assisting people suffering from suicidal ideation by analyzing Context Information, such as sensor, psychological, and sociodemographic data. The main scientific contribution is the personalized assistance for individuals at risk of suicidal ideation by analyzing Context Information for anticipating risk identification. A prototype allowed the data collection of 3 patients between 25 and 36 days. Data augmentation techniques helped in creating a database that allowed the model evaluation. The experiments showed that Gated Recurrent Unit models achieved an F1-Score of 94.12%, 92.44%, and 92.61% for predicting suicidal ideation on the following day, considering Histories of 3, 5, and 7 days, respectively. The patients and therapists assessed the model through the Technology Acceptance Model and a semi-structured interview. The patients felt comfortable sharing personal data and providing accurate information to the tool. The therapists highlighted the constant asynchronous monitoring strategy employed by the model, allowing early actions to mitigate future risks.}
}
@article{XIANG2019197,
title = {The Symbolic Construction of Reality: The Xici and Ernst Cassirer’s Philosophy of Symbolic Forms},
journal = {Journal of Chinese Humanities},
volume = {4},
number = {2},
pages = {197-224},
year = {2019},
issn = {2352-1333},
doi = {https://doi.org/10.1163/23521341-12340064},
url = {https://www.sciencedirect.com/science/article/pii/S2352133319000049},
author = {Shuchen Xiang},
keywords = {Ernst Cassirer, humanism, linguistic turn, philosophy of language, philosophy of symbolic forms, symbol, , },
abstract = {This paper, unlike scholars who ascribe to it a copy theory of meaning, argues that the logic of the Xici is best described through “philosophy’s linguistic turn,” specifically Ernst Cassirer’s philosophy of symbolic forms. Cassirer’s concept of the symbol as a pluralistic, constitutive, and functional yet concrete and observable form, is comparable to the symbolic system in the Xici 系辭: xiang 象, gua 卦, yao 爻, and yi 易. Their similarity is due to a shared philosophical orientation: humanism. The characteristics of the Xici—the part-whole (structuralist) relationship typical of correlative cosmology, the simultaneously sensuous and conceptual nature of its symbols, the stress on order as opposed to unity, and the importance of symbols per se—for Cassirer are characteristics that were only possible in European intellectual history after a substance ontology was replaced by a functional one. For Cassirer, a functional ontology is closely associated with a humanism that celebrates creations (i.e., language) of the human mind in determining reality. This humanism is coherent with the intellectual context—Confucian humanism—contemporary with the period of the Xici’s composition. It would thus be inconsistent to concede this humanism to the Xici without also conceding that its understanding of the symbols is akin to that of the linguistic turn. Finally, even regardless of this comparative framework, the Xici runs into a paradox if we read it through a copy theory of meaning, paradoxes that immediately dissolve if we read it through the paradigm of the linguistic turn.}
}
@article{PARK2023100548,
title = {Visual language integration: A survey and open challenges},
journal = {Computer Science Review},
volume = {48},
pages = {100548},
year = {2023},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2023.100548},
url = {https://www.sciencedirect.com/science/article/pii/S1574013723000151},
author = {Sang-Min Park and Young-Gab Kim},
keywords = {Multimodal learning, Multi-task learning, End-to-end learning, Embodiment, Visual language interaction},
abstract = {With the recent development of deep learning technology comes the wide use of artificial intelligence (AI) models in various domains. AI shows good performance for definite-purpose tasks, such as image recognition and text classification. The recognition performance for every single task has become more accurate than feature engineering, enabling more work that could not be done before. In addition, with the development of generation technology (e.g., GPT-3), AI models are showing stable performances in each recognition and generation task. However, not many studies have focused on how to integrate these models efficiently to achieve comprehensive human interaction. Each model grows in size with improved performance, thereby consequently requiring more computing power and more complicated designs to train than before. This requirement increases the complexity of each model and requires more paired data, making model integration difficult. This study provides a survey on visual language integration with a hierarchical approach for reviewing the recent trends that have already been performed on AI models among research communities as the interaction component. We also compare herein the strengths of existing AI models and integration approaches and the limitations they face. Furthermore, we discuss the current related issues and which research is needed for visual language integration. More specifically, we identify four aspects of visual language integration models: multimodal learning, multi-task learning, end-to-end learning, and embodiment for embodied visual language interaction. Finally, we discuss some current open issues and challenges and conclude our survey by giving possible future directions.}
}
@article{GUPTA201949,
title = {Abstractive summarization: An overview of the state of the art},
journal = {Expert Systems with Applications},
volume = {121},
pages = {49-65},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2018.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418307735},
author = {Som Gupta and S. K Gupta},
keywords = {Abstractive summarization, Concept finding, Semantic-Based summarization, Ontology-Based summarization, Deep learning},
abstract = {Summarization, is to reduce the size of the document while preserving the meaning, is one of the most researched areas among the Natural Language Processing (NLP) community. Summarization techniques, on the basis of whether the exact sentences are considered as they appear in the original text or new sentences are generated using natural language processing techniques, are categorized into extractive and abstractive techniques. Extractive summarization has been a very extensively researched topic and has reached to its maturity stage. Now the research has shifted towards the abstractive summarization. The complexities underlying with the natural language text makes abstractive summarization a difficult and a challenging task. This paper presents a comprehensive review of the various works performed in abstractive summarization field. For this purpose, we have selected the recent papers on this topic from Elsevier, ACM, IEEE, Springer, ACL Anthology, Cornell University Library and Google Scholar. The papers are categorized according to the type of abstractive technique used. The paper lists down the various challenges and discusses the future direction for research in this field. Along with these, we have identified the advantages and disadvantages of various methods used for abstractive summarization. We have also listed down the various tools which have been used or developed by researchers for abstractive summarization. The paper also discusses the evaluation techniques being used for assessing the abstractive summaries.}
}
@article{VIAL2022100914,
title = {Not complex enough for complexity: Some intricacies of interpersonal synergies theory},
journal = {New Ideas in Psychology},
volume = {64},
pages = {100914},
year = {2022},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2021.100914},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X21000635},
author = {Iván Vial and Carlos Cornejo},
keywords = {Interpersonal coordination, Synchrony, Complexity, Synergies, Shared vitality, Affect attunement},
abstract = {Interpersonal coordination (IC) has become an area of growing interest in the last decades. Despite the accumulated evidence, we still do not have an encompassing explanative framework. However, a serious candidate is the interpersonal synergies theory (IST), a model part of the broader complexity approach. In the present paper, we analyze the suitability of IST as an explanation of IC. IST resolves the cognitivist theories’ difficulties on IC, but it exhibits inconsistencies that diminish its impact. We argue that IST can only fit from an epistemological (not an ontological) interpretation of the notion of emergence. Secondly, IST confuses ‘synergy’ with ‘coordination’, risking mixing explanans with explanandum. Third, as a formal theory, IST obliterates the meaning dimension of human movement. Therefore, it is necessary a more in-depth description of what motor coupling among people means. We argue that attending the vitality dynamics sheds light on the shared meaning implied in coordination.}
}
@article{CORREIA2025101563,
title = {The undercurrents of water stewardship in the Syilx Okanagan territory: Setting the stage for ethical space engagement},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101563},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101563},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125002918},
author = {Maria Correia and Sarah Alexis and Marcello Glo and Shruti Suresh and Aleksandra Dulic},
keywords = {Ethical space, Water stewardship, Indigenous rights, Cross-cultural collaboration, Colonial governance, Syilx Okanagan nation},
abstract = {‘Ethical space,’ as defined at the intersection of Indigenous and Western worldviews, draws attention to the unseen forces that shape cross-cultural relations. Applying ethical space as a conceptual lens, this study examines Indigenous and non-Indigenous perspectives on water stewardship in the Syilx Okanagan territory in British Columbia, Canada, with the aim of uncovering these invisible dynamics. Through qualitative inquiry with Syilx and non-Indigenous participants, we identify areas of convergence and divergence in understandings of water stewardship. We find that both groups recognized the exclusionary nature of colonial water governance. Indigenous participants emphasized inherent water rights tied to unceded territory, while non-Indigenous participants often overlooked these legal and historical dimensions. Water was framed as a sacred ‘relative’ by Syilx participants and as a ‘resource’ by non-Indigenous participants, reflecting contrasting ontologies and values. Despite these broad differences, there was consensus on the importance of building cultural competencies and addressing systemic governance inequities. Findings underscore the complexities of cross-cultural collaboration in water stewardship amidst broader political, economic, and social challenges. Future research should focus on ethical space as praxis in multi-institutional settings including the role of policy in supporting these efforts, and its potential for building new partnership models to address pressing social-ecological challenges.}
}
@article{ARSHAD2020101675,
title = {Formal knowledge model for online social network forensics},
journal = {Computers & Security},
volume = {89},
pages = {101675},
year = {2020},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2019.101675},
url = {https://www.sciencedirect.com/science/article/pii/S0167404819302160},
author = {Humaira Arshad and Aman Jantan and Gan Keng Hoon and Isaac Oludare Abiodun},
keywords = {Forensic automation, Knowledge model, Formal model, Forensic ontology, Online social network forensics},
abstract = {Currently, examining social media networks is an integral part of most investigations. However, getting a clear view of the events relevant to the incident from a large set of data, such as social media, is a challenging task. Automation of the forensic and analysis process is the only solution to manage large data sets and get useful information. However, automation in digital forensics is a technical issue with legal implications. The legal system accepts only those automated processes that are reproducible, explainable, and rigorously testable. Therefore, automated forensic processes must be based on formal theories, which are rare in digital forensics. This article explains a theoretical and formal knowledge model for forensic automation on online social networks. This model consists of an event-based knowledge model, which provides theoretical concepts that can assist in the construction and interpretation of the events related to the incident under investigation. The proposed model is implemented through an ontology to provide semantically rich and formal representation to the concepts. This article also describes the feasibility of legally acceptable automated analysis operators, based on a formal theory, for online social network forensics.}
}
@article{SLATER2021104904,
title = {Multi-faceted semantic clustering with text-derived phenotypes},
journal = {Computers in Biology and Medicine},
volume = {138},
pages = {104904},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104904},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521006983},
author = {Karin Slater and John A. Williams and Andreas Karwath and Hilary Fanning and Simon Ball and Paul N. Schofield and Robert Hoehndorf and Georgios V. Gkoutos},
keywords = {Ontology, Clustering, MIMIC-III, Semantic similarity, Cluster explanation},
abstract = {Identification of ontology concepts in clinical narrative text enables the creation of phenotype profiles that can be associated with clinical entities, such as patients or drugs. Constructing patient phenotype profiles using formal ontologies enables their analysis via semantic similarity, in turn enabling the use of background knowledge in clustering or classification analyses. However, traditional semantic similarity approaches collapse complex relationships between patient phenotypes into a unitary similarity scores for each pair of patients. Moreover, single scores may be based only on matching terms with the greatest information content (IC), ignoring other dimensions of patient similarity. This process necessarily leads to a loss of information in the resulting representation of patient similarity, and is especially apparent when using very large text-derived and highly multi-morbid phenotype profiles. Moreover, it renders finding a biological explanation for similarity very difficult; the black box problem. In this article, we explore the generation of multiple semantic similarity scores for patients based on different facets of their phenotypic manifestation, which we define through different sub-graphs in the Human Phenotype Ontology. We further present a new methodology for deriving sets of qualitative class descriptions for groups of entities described by ontology terms. Leveraging this strategy to obtain meaningful explanations for our semantic clusters alongside other evaluation techniques, we show that semantic clustering with ontology-derived facets enables the representation, and thus identification of, clinically relevant phenotype relationships not easily recoverable using overall clustering alone. In this way, we demonstrate the potential of faceted semantic clustering for gaining a deeper and more nuanced understanding of text-derived patient phenotypes.}
}
@article{GUARASCI2024124131,
title = {Classifying deceptive reviews for the cultural heritage domain: A lexicon-based approach for the Italian language},
journal = {Expert Systems with Applications},
volume = {252},
pages = {124131},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124131},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424009977},
author = {Raffaele Guarasci and Rosario Catelli and Massimo Esposito},
keywords = {Fake reviews detection, Lexicon-based, Cultural heritage},
abstract = {Nowadays, the spread of deceptive reviews is a problem that has reached critical dimensions, having a significant economic impact on business activities. This paper aims to estimate – at the quantitative and qualitative levels – the possibility of using particular words to disambiguate between truthful and deceptive text, focusing on reviews produced in the cultural heritage domain. For this purpose, a lexicon-based methodology has used two different lexicons: sentiment information, intensifiers, downtoners, and negation operators. As known in the literature, these elements are crucial in a classification process related to deceptiveness. The evaluation phase has considered quantitative metrics such as accuracy and F1 score and ad hoc developed metrics that consider specific linguistic parameters such as polarity and tone of voice intensifiers. A qualitative analysis of a subset of the corpus has also been carried out to understand better factors that impact the classification of deceptive review. Several linguistic features have been considered, ranging from the number of intensifiers to their type and position in phrases and sentences. A comparison between the performances of two different lexicons used has been added to the analysis.}
}
@incollection{KANZA2021129,
title = {Semantic Technologies in Drug Discovery},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {129-144},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11520-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012801238311520X},
author = {Samantha Kanza and Jeremy {Graham Frey}},
keywords = {Chemicals, Data integration, Drug discovery, Drugs, Drug-targets, Genomics, Human and computer readable data, Knowledgebases, Linked data, Molecular mechanisms, Ontologies, Pharmacogenomics, Proteomics, RDF, Semantic search, Semantic web, SPARQL},
abstract = {Semantic drug discovery has gained significant traction in recent years, with researchers becoming more aware that these technologies enable them to link together and query disparate datasets for information that cannot be extracted from a single dataset. This article provides a comprehensive reference source of the current knowledge available regarding Semantic Web technologies in drug discovery. The main aspects of Semantic Web technologies are explained, detailing the different ways in which they can be used in drug discovery. Over 1000 biomedical ontologies were reviewed as part of the work undertaken for this paper and 34 of the most relevant ontologies in the drug discovery field are categorized and described, followed by details of semantic applications and successes in drug discovery. Some core standards and guidelines have been established for sharing Semantic drug discovery data, both through making well established medical taxonomies available in a Semantic format, and by creating upper-level ontologies and guidelines for creating new ontologies in the biomedical domain. This article concludes that a majority of the prevalent ontologies in drug discovery follow these standards and provides advice for researchers wishing to use Semantic Web technologies in their drug discovery research.}
}
@article{DENG2022104530,
title = {Transforming knowledge management in the construction industry through information and communications technology: A 15-year review},
journal = {Automation in Construction},
volume = {142},
pages = {104530},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104530},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522004022},
author = {Hui Deng and Yiwen Xu and Yichuan Deng and Jiarui Lin},
keywords = {ICT, Knowledge management, Construction management, Review, Knowledge graph, Semantic network},
abstract = {Information and Communications Technologies (ICTs) play a vital role in the knowledge management (KM) of the Architecture, Engineering and Construction (AEC) industry, which is knowledge intensive and yet faces unique challenges in managing knowledge due to its complex and dynamic nature. In order to show the state-of-the-art of ICTs for knowledge management in the AEC industry, this paper conducts a fifteen-year review of 89 related papers and reports from the industry. A clear line of key technologies evolving from ontology, semantic network to knowledge graph has been revealed from the collected literature. The interactions between different ICTs, as well as their advantages and disadvantages for different knowledge management process are discussed. The study also finds certain imbalance in the development of the industry and academia, as well as cognitive barriers and lack of evaluation standards. Suggestions for future development are also proposed to benefit the research community and relevant practitioners.}
}
@article{ALI202123,
title = {An intelligent healthcare monitoring framework using wearable sensors and social networking data},
journal = {Future Generation Computer Systems},
volume = {114},
pages = {23-43},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.07.047},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X1931605X},
author = {Farman Ali and Shaker El-Sappagh and S.M. Riazul Islam and Amjad Ali and Muhammad Attique and Muhammad Imran and Kyung-Sup Kwak},
keywords = {Machine learning, Semantic knowledge, Big data analysis, Healthcare monitoring system, Wearable sensors, Social network analysis},
abstract = {Wearable sensors and social networking platforms play a key role in providing a new method to collect patient data for efficient healthcare monitoring. However, continuous patient monitoring using wearable sensors generates a large amount of healthcare data. In addition, the user-generated healthcare data on social networking sites come in large volumes and are unstructured. The existing healthcare monitoring systems are not efficient at extracting valuable information from sensors and social networking data, and they have difficulty analyzing it effectively. On top of that, the traditional machine learning approaches are not enough to process healthcare big data for abnormality prediction. Therefore, a novel healthcare monitoring framework based on the cloud environment and a big data analytics engine is proposed to precisely store and analyze healthcare data, and to improve the classification accuracy. The proposed big data analytics engine is based on data mining techniques, ontologies, and bidirectional long short-term memory (Bi-LSTM). Data mining techniques efficiently preprocess the healthcare data and reduce the dimensionality of the data. The proposed ontologies provide semantic knowledge about entities and aspects, and their relations in the domains of diabetes and blood pressure (BP). Bi-LSTM correctly classifies the healthcare data to predict drug side effects and abnormal conditions in patients. Also, the proposed system classifies the patients’ health condition using their healthcare data related to diabetes, BP, mental health, and drug reviews. This framework is developed employing the Protégé Web Ontology Language tool with Java. The results show that the proposed model precisely handles heterogeneous data and improves the accuracy of health condition classification and drug side effect predictions.}
}
@article{JOLLY2024,
title = {Exploring Biomedical Named Entity Recognition via SciSpaCy and BioBERT Models},
journal = {The Open Biomedical Engineering Journal},
volume = {18},
year = {2024},
issn = {1874-1207},
doi = {https://doi.org/10.2174/0118741207289680240510045617},
url = {https://www.sciencedirect.com/science/article/pii/S1874120724000031},
author = {Aman Jolly and Vikas Pandey and Indrasen Singh and Neha Sharma},
keywords = {Biomedical, BioNER, SciSpaCy, BioBERT, Natural language processing, Named entity recognition},
abstract = {Introduction
Biological Named Entity Recognition (BioNER) is a crucial preprocessing step for Bio-AI analysis.
Methods
Our paper explores the field of Biomedical Named Entity Recognition (BioNER) by closely analysing two advanced models, SciSpaCy and BioBERT. We have made two distinct contributions: Initially, we thoroughly train these models using a wide range of biological datasets, allowing for a methodical assessment of their performance in many areas. We offer detailed evaluations using important parameters like F1 scores and processing speed to provide precise insights into the effectiveness of BioNER activities.
Results
Furthermore, our study provides significant recommendations for choosing tools that are customised to meet unique BioNER needs, thereby enhancing the efficiency of Named Entity Recognition in the field of biomedical research. Our work focuses on tackling the complex challenges involved in BioNER and enhancing our understanding of model performance.
Conclusion
The goal of this research is to drive progress in this important field and enable more effective use of advanced data analysis tools for extracting valuable insights from biomedical literature.}
}
@article{HERNANDEZRAMIREZ2024414,
title = {The Future End of Design Work: A Critical Overview of Managerialism, Generative AI, and the Nature of Knowledge Work, and Why Craft Remains Relevant},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {10},
number = {4},
pages = {414-440},
year = {2024},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2024.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872624000960},
author = {Rodrigo Hernández-Ramírez and João Batalheiro Ferreira},
keywords = {creativity, design work, generative artificial intelligence (GenAI), knowledge work, managerialism},
abstract = {This article examines the transformation of design work under the influence of managerialism and the rise of Generative Artificial Intelligence (GenAI). Drawing on John Maynard Keynes’s projections of technological unemployment and the evolving nature of work, it argues that despite advancements in automation, work has not diminished but rather devalued. Design, understood as a type of knowledge work, faces an apparent existential crisis. GenAI grows adept at mimicking the output of creative processes. The article explores how the fear of the end of design work fueled by the rise of GenAI is rooted in a misunderstanding of design work. This misunderstanding is driven by managerialism—an ideology that prioritizes efficiency and quantifiable outcomes over the intrinsic value of work. Managerialism seeks to instrumentalize and automate design, turning it into a controllable procedure to generate quantifiable creative outputs. The article argues why design work cannot be turned into a procedure and automated using GenAI. Advocates of these systems claim they enhance productivity and open new opportunities. However, evidence so far shows that flawed GenAI models produce disappointing outcomes while operating at a significant environmental cost. The article concludes by arguing for a robust theory of design—one that acknowledges the unique ontological and epistemic boundaries of design work and underscores why design cannot be reduced to a procedural output.}
}
@article{PACHECOLOPEZ2021107439,
title = {Synthesis and assessment of waste-to-resource routes for circular economy},
journal = {Computers & Chemical Engineering},
volume = {153},
pages = {107439},
year = {2021},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2021.107439},
url = {https://www.sciencedirect.com/science/article/pii/S0098135421002179},
author = {Adrián Pacheco-López and Ana Somoza-Tornos and Moisès Graells and Antonio Espuña},
keywords = {Circular economy, Material upcycling, Waste-to-resource, Ontologies, Chemical recycling, Plastic waste treatment},
abstract = {The benefits of the circular economy have been proven during the past two decades, but its application poses some challenges. In particular, the increasing number of potential waste-to-resource processing alternatives obstructs the identification of the most promising ones, besides the lack of efficient knowledge management tools and standardized assessment procedures. This contribution presents a systematic way to generate and assess new processing paths including waste-to-resource technologies, based on the use of a predefined ontological framework. An ontology is filled with transformation processes; then, several alternative paths are generated and assessed, according to a potentially available waste. The resulting list is classified according to pre-established parameters, thus presenting which are the potentially best alternatives to close the material loops and recover chemical resources. The proposed method is tested through the generation and evaluation of different routes for the treatment of plastic waste materials, with a special focus on chemical recycling.}
}
@article{JAMADIKHIABANI2025127790,
title = {Cross-target stance detection: A survey of techniques, datasets, and challenges},
journal = {Expert Systems with Applications},
volume = {283},
pages = {127790},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127790},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425014125},
author = {Parisa {Jamadi Khiabani} and Arkaitz Zubiaga},
keywords = {Cross-target stance detection, Social media, Few-shot learning, Language models},
abstract = {Stance detection is the task of determining the viewpoint expressed in a text towards a given target. A specific direction within the task focuses on cross-target stance detection, where a model trained on samples pertaining to certain targets is then applied to a new, unseen target. With the increasing need to analyze and mine viewpoints and opinions online, the task has recently seen a significant surge in interest. This review paper examines research in cross-target stance detection over the last decade, highlighting the evolution from basic statistical methods to contemporary neural and LLM-based models. All this research has led to notable improvements in accuracy and adaptability. Innovative approaches include the use of topic-grouped attention and adversarial learning for zero-shot detection, as well as fine-tuning techniques that enhance model robustness. Additionally, prompt-tuning methods and the integration of external knowledge have further refined model performance. A comprehensive overview of the datasets used for evaluating these models is also provided, offering valuable insights into the progress and challenges in the field. We conclude by highlighting emerging research directions and suggesting avenues for future work.}
}
@article{AKHRIF202187,
title = {Completeness based classification algorithm: a novel approach for educational semantic data completeness assessment},
journal = {Interactive Technology and Smart Education},
volume = {19},
number = {1},
pages = {87-111},
year = {2021},
issn = {1741-5659},
doi = {https://doi.org/10.1108/ITSE-01-2021-0017},
url = {https://www.sciencedirect.com/science/article/pii/S174156592100023X},
author = {Ouidad Akhrif and Chaymae Benfaress and Mostapha {EL Jai} and Youness {El Bouzekri El Idrissi} and Nabil Hmina},
keywords = {Skills, Students, Distance learning, Modeling, Higher education, Worldwide web, Smart collaborative learning, Ontology, Heuristic, Educational data mining, Classification, Random forest, KNIME},
abstract = {Purpose
The purpose of this paper is to reveal the smart collaborative learning service. This concept aims to build teams of learners based on the complementarity of their skills, allowing flexible participation and offering interdisciplinary collaboration opportunities for all the learners. The success of this environment is related to predict efficient collaboration between the different teammates, allowing a smartly sharing knowledge in the Smart University environment.
Design/methodology/approach
A random forest (RF) approach is proposed, which is based on semantic modelization of the learner and the problem-solving allowing multidisciplinary collaboration, and heuristic completeness processing to build complementary teams. To achieve that, this paper established a Konstanz Information Miner workflow that integrates the main steps for building and evaluating the RF classifier, this workflow is divided into: extracting knowledge from the smart collaborative learning ontology, calculating the completeness using a novel heuristic and building the RF classifier.
Findings
The smart collaborative learning service enables efficient collaboration and democratized sharing of knowledge between learners, by using a semantic support decision support system. This service solves a frequent issue related to the composition of learning groups to serve pedagogical perspectives.
Originality/value
The present study harmonizes the integration of ontology, a new heuristic processing and supervised machine learning algorithm aiming at building an intelligent collaborative learning service that includes a qualified classifier of complementary teams of learners.}
}
@article{LEE2024100126,
title = {MicroGlycoDB: A database of microbial glycans using Semantic Web technologies},
journal = {BBA Advances},
volume = {6},
pages = {100126},
year = {2024},
issn = {2667-1603},
doi = {https://doi.org/10.1016/j.bbadva.2024.100126},
url = {https://www.sciencedirect.com/science/article/pii/S2667160324000140},
author = {Sunmyoung Lee and Louis-David Leclercq and Yann Guerardel and Christine M. Szymanski and Thomas Hurtaux and Tamara L. Doering and Takane Katayama and Kiyotaka Fujita and Kazuhiro Aoki and Kiyoko F. Aoki-Kinoshita},
keywords = {Microbes, Glycosylation, Database, Data integration, Semantic data},
abstract = {Glycoconjugates are present on microbial surfaces and play critical roles in modulating interactions with the environment and the host. Extensive research on microbial glycans, including elucidating the structural diversity of the glycan moieties of glycoconjugates and polysaccharides, has been carried out to investigate the function of glycans in modulating the interactions between the host and microbes, to explore their potential applications in the therapeutic targeting of pathogenic species, and in the use as probiotics in gut microbiomes. However, glycan-related information is dispersed across numerous databases and a vast amount of literature, which makes it laborious and time-consuming to identify and gather the relevant information about microbial glycosylation. This challenge can be addressed by a comprehensive database, which could offer insight into the fundamental processes underlying glycosylation. We have developed a MicroGlycoDB database to provide integrated glycan information on important model microorganisms. The data is described using Semantic Web Technologies, which allow microbial glycan data to be represented in a structured format accessible by machines, thus facilitating data sharing and integration with other resources that catalog features such as pathways, diseases, or interactions. This semantic data based on ontologies will contribute to the discovery of new knowledge in the field of microbiology, along with the expansion of information on the glycosylation of other microorganisms.}
}
@article{PALAZZO2021107806,
title = {Exploiting structured high-level knowledge for domain-specific visual classification},
journal = {Pattern Recognition},
volume = {112},
pages = {107806},
year = {2021},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2020.107806},
url = {https://www.sciencedirect.com/science/article/pii/S0031320320306099},
author = {S. Palazzo and F. Murabito and C. Pino and F. Rundo and D. Giordano and M. Shah and C. Spampinato},
keywords = {Fine-grained visual classification, Computational ontologies, Belief networks},
abstract = {In the last decade, deep learning models have yielded impressive performance on visual object recognition and image classification. However these methods still rely on learning visual data distributions and show difficulties in dealing with complex scenarios where visual appearance only is not enough to effectively tackle them. This is the case, for instance, of fine-grained image classification in domain-specific applications for which it is very complex to employ data-driven models because of the lack of large amounts of samples and that, instead, can be solved by resorting to specialized human knowledge. However, encoding this specialized knowledge and injecting it into deep models is not trivial. In this paper, we address this problem by: a) employing computational ontologies to model specialized knowledge in a structured representation and, b) building a hybrid visual-semantic classification framework. The classification method performs inference over a Bayesian Network graph, whose structure depends on the knowledge encoded in an ontology and evidences are built using the outputs of deep networks. We test our approach on a fine-grained classification task, employing an extremely complex dataset containing images from several fruit varieties as well as visual and semantic annotations. Since the classification is done at the variety level (e.g., discriminating between different cherry varieties), appearance changes slightly and expert domain knowledge — making using of contextual information — is required to perform classification accurately. Experimental results show that our approach significantly outperforms standard deep learning–based classification methods over the considered scenario as well as existing methods leveraging semantic information for classification. These results demonstrate, on one hand, the difficulty of purely-visual deep methods in tackling small and highly-specialized datasets and, on the other hard, the capabilities of our approach to effectively encode and use semantic knowledge for enhanced accuracy.}
}
@article{MATIJEVICGOSTOJIC2023301601,
title = {A knowledge-based system for supporting the soundness of digital forensic investigations},
journal = {Forensic Science International: Digital Investigation},
volume = {46},
pages = {301601},
year = {2023},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2023.301601},
url = {https://www.sciencedirect.com/science/article/pii/S2666281723001130},
author = {Milica {Matijević Gostojić} and Željko Vuković},
keywords = {Digital forensics, Evidence admissibility, Knowledge representation, Automated reasoning},
abstract = {Performing a technically and legally sound digital forensic investigation leads to digital evidence that can be used in courts of law. However, there is no single model of a standardized procedure that investigators should abide by. This paper presents a knowledge-based system that formally specifies information about investigative procedures in accordance with standards and guidelines such as ISO/IEC 27037, ISO/IEC 27041, ISO/IEC 27042, ISO/IEC 27043, NIST’s Guide to Integrating Forensic Techniques into Incident Response and Interpol’s Guidelines for Digital Forensics First Responders. The knowledge base is created in a description logic and it represents an ontological model. The model unifies concepts from different standards and guidelines, thus enabling the system to aid investigators in executing investigative procedures that will result in admissible digital evidence. The paper uses network forensics as a case study, but it can be customized to other digital forensics domains.}
}
@article{MANN2024108505,
title = {eSFILES: Intelligent process flowsheet synthesis using process knowledge, symbolic AI, and machine learning},
journal = {Computers & Chemical Engineering},
volume = {181},
pages = {108505},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108505},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423003757},
author = {Vipul Mann and Mauricio Sales-Cruz and Rafiqul Gani and Venkat Venkatasubramanian},
keywords = {Process design, Computer-aided flowsheet synthesis, Hybrid artificial intelligence, Flowsheet grammar syntax rules, Flowsheet hypergraph},
abstract = {Process flowsheet synthesis, design, and simulation require integrated approaches that combine domain knowledge and data-driven methods for fast, efficient, and reliable solutions. However, due to the recent surge in data and machine learning capabilities, there has been a shift towards building purely data-driven systems for process flowsheet synthesis and related problems. Such approaches have certain drawbacks. Here, we present a hybrid method that combines data-driven approaches with domain knowledge to represent process flowsheets and solve problems related to process synthesis, design, and simulation. We present an extended SFILES (or eSFILES) representation, a multi-level hierarchical flowsheet representation with varying degrees of process knowledge. At level 0, flow diagrams are represented as purely text-based SFILES strings. At level 1, the SFILES grammar, along with inferencing algorithms, is used to construct a flowsheet hypergraph explicitly representing flow diagram connectivity. At level 2, specifications needed for material and energy balance calculations are introduced, and, after simulation, the results are also added using annotated flowsheet hypergraphs. Finally, at level 3, a process ontology is connected with the annotated flowsheet hypergraph to include design and operation parameters as well as the detailed simulation results. We discuss this hierarchical framework using several case studies.}
}
@article{DOO2023877,
title = {Exploring the Clinical Translation of Generative Models Like ChatGPT: Promise and Pitfalls in Radiology, From Patients to Population Health},
journal = {Journal of the American College of Radiology},
volume = {20},
number = {9},
pages = {877-885},
year = {2023},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2023.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S1546144023005161},
author = {Florence X. Doo and Tessa S. Cook and Eliot L. Siegel and Anupam Joshi and Vishwa Parekh and Ameena Elahi and Paul H. Yi},
keywords = {generative artificial intelligence, radiology, limitations, large language models, ChatGPT},
abstract = {Generative artificial intelligence (AI) tools such as GPT-4, and the chatbot interface ChatGPT, show promise for a variety of applications in radiology and health care. However, like other AI tools, ChatGPT has limitations and potential pitfalls that must be considered before adopting it for teaching, clinical practice, and beyond. We summarize five major emerging use cases for ChatGPT and generative AI in radiology across the levels of increasing data complexity, along with pitfalls associated with each. As the use of AI in health care continues to grow, it is crucial for radiologists (and all physicians) to stay informed and ensure the safe translation of these new technologies.}
}
@article{AHMAD201833,
title = {Knowledge-based PPR modelling for assembly automation},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {21},
pages = {33-46},
year = {2018},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1755581718300014},
author = {Mussawar Ahmad and Borja Ramis Ferrer and Bilal Ahmad and Daniel Vera and Jose L. {Martinez Lastra} and Robert Harrison},
keywords = {Knowledge-based systems, Semantic web technologies, Assembly automation},
abstract = {The paradigm shift from mass production to mass customisation and reduced product lifecycles requires continuous re-engineering/configuration of modern manufacturing systems. Although efforts are being made to design and build manufacturing systems based on the paradigms of changeability, reconfigurability, and flexibility, the knowledge of the system's capability remains unstructured and isolated from product design and engineering tools. As a result, introducing product design changes are costly, time-consuming and error-prone. To address this problem, this research utilises a Product, Process, and Resource (PPR) ontology with a view to supporting changes through information integration and knowledge generation. The approach moves away from product-centric tools such as Product Lifecycle Management (PLM) and thus a heterarchical model of the system is created. The contribution of this work is to demonstrate how modular ontologies can be utilised in a practical and industrially relevant manner by integrating the data structure of a set of component-based virtual engineering tools into the Resource domain. The research presents a proof-of-concept of the proposed approach using an automated engine assembly station as a case study. Inferences are made from explicit knowledge through rules and mapping as to whether both Product and Process requirements are met by Resource domain capabilities. The approach used in this work has the potential to significantly improve the workflow as and when new products are introduced or modifications need to be made as the scope of change can be assessed rapidly resulting in more focused engineering and design work.}
}
@article{HU2021103580,
title = {Building energy performance assessment using linked data and cross-domain semantic reasoning},
journal = {Automation in Construction},
volume = {124},
pages = {103580},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103580},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521000315},
author = {Shushan Hu and Jiale Wang and Cathal Hoare and Yehong Li and Pieter Pauwels and James O'Donnell},
keywords = {Building energy performance, Cross-domain semantic reasoning, Semantic web, Data interoperability},
abstract = {Cross-domain information is essential for building energy performance assessment. The heterogeneous nature of this information is a major source for inefficient assessments. The semantic web provides a flexible pathway for addressing recognised interoperability issues. However, further implicit knowledge in cross-domain information could provide meaningful solutions for such assessments. This paper aims to develop a conceptual framework that links cross-domain information, infers implicit knowledge, and empowers building managers with insightful assessments. The framework integrates Web Ontology Language (OWL) ontologies, Resource Description Framework (RDF) instances, and a set of predefined rules to infer implicit knowledge, which can satisfy data requirements of performance metrics and enable meaningful performance assessments. Then building managers can identify inefficient building operations and improve energy efficiency while maintaining desired building functions. This approach reduces burdensome intervention from the managers when compared with traditional solutions. A demonstration highlights the engineering value by evaluating energy performance of a university building.}
}
@article{BAIS2025112,
title = {Assessing circular economy at company level: Comparison of tools and methodological challenges},
journal = {Sustainable Production and Consumption},
volume = {59},
pages = {112-126},
year = {2025},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2025.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352550925001460},
author = {Beatrice Bais and Margherita Molinaro and Guido Orzes},
keywords = {Circular economy, Holistic assessment, Company level, Assessment tools},
abstract = {While circular economy adoption is growing worldwide, its systematic assessment remains a challenge. This paper aims to shed light on the strengths and weaknesses of firm-level circular economy assessment tools and to theorize on the circular economy assessment issue. Four tools were selected and compared based on a qualitative comparative analysis as well as on their practical application to multiple case studies. The originality of this comparison lies in evaluating the extent to which these tools are affected by ontological and etymological issues and by epistemological and human cognitive bias. Such an evaluation was based on seven criteria rooted in the interpretivist research paradigm and grounded theory methodology. The results indicate that, albeit providing circular economy awareness and insights on high-level strategic circular economy practices, the tools can be improved in terms of transferability, credibility, confirmability, fit, and generality. Current holistic tools fail to consider some firms' specificities and to provide circular economy implications, besides being subject to user manipulation due to issues such as low reliability and human bias. Accordingly, the study provides suggestions to improve the development of future circular economy assessment tools and enhance practitioners' awareness of the effective use of these tools, having both academic and practical implications.}
}
@article{WANG2024103824,
title = {KnowCTI: Knowledge-based cyber threat intelligence entity and relation extraction},
journal = {Computers & Security},
volume = {141},
pages = {103824},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.103824},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824001251},
author = {Gaosheng Wang and Peipei Liu and Jintao Huang and Haoyu Bin and Xi Wang and Hongsong Zhu},
keywords = {Cyber threat intelligence, Entity and relation extraction, Knowledge graph, Attention mechanism, Graph attention network},
abstract = {Structured cyber threat intelligence enables security researchers to know the occurrence of cyber threats in time, thereby improving the efficiency of security defense and analysis. Previous works usually use general deep learning and NLP techniques to extract intelligence. Such methods suffer from insufficient semantic understanding in the field of security. To address these issues, we propose a novel method called Knowledge-based Cyber Threat Intelligence Entity and Relation Extraction (KnowCTI), which incorporates cybersecurity knowledge into the model to enhance the understanding of the realm of cybersecurity and has a full picture of threats with the threat intelligence graph generation. Specifically, we first build a cybersecurity knowledge base and train cybersecurity-aware knowledge embeddings based on the base. Secondly, we refine the most related knowledge triples by attention mechanism and gate mechanism, and then construct a sentence tree through these triples. Next, we employ graph attention networks to incorporate knowledge information into the sentence by considering the sentence tree as a graph. Finally, we consider entity extraction as a sequence labeling problem and relation extraction as a classification problem to decode the entities and relation triples according to the threat intelligence ontology we designed. Experimental results demonstrate the superior performance with the F1 score exceeding 90.16 and 81.83 on entity and relation extraction separately.}
}
@article{KALTENEGGER2024109172,
title = {A data management perspective on building material classification: A systematic review},
journal = {Journal of Building Engineering},
volume = {92},
pages = {109172},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109172},
url = {https://www.sciencedirect.com/science/article/pii/S235271022400740X},
author = {Julia Kaltenegger and Kirstine Meyer Frandsen and Ekaterina Petrova},
keywords = {Building simulation, Building information modelling, Material information modelling, Material properties, Ontologies},
abstract = {Material information, such as properties and indicators, is essential for building performance assessment. The interoperability between applications and the harmonisation of data related to buildings, materials, energy consumption, environmental performance, etc., has been widely discussed in research. Building Information Modelling (BIM) has enabled a more transparent and accurate data exchange between tools. Semantic data modelling and web technologies have significantly impacted building and material modelling domains, as they allow data structures to be linked based on their formal semantic representations. Yet, there is a missing link between material modelling, data modelling and building simulation, and reliable and scalable material information is often neglected. This study introduces a data management perspective on building material data, property definition and classification. The article presents an extensive systematic review of the intersection of BIM, Material Information Modelling, material databases and the respective material data exchange capabilities of existing Building Performance Simulation tools. Finally, the article proposes a material classification and mapping mechanism that relies on concepts and standards from the building and material domains. The findings indicate (i) inconsistent classification taxonomies throughout various simulation software, (ii) inconsistent aggregation of material information, and (iii) missing link between high and low aggregation levels of material information. The proposed material classification and mapping schema aims to harmonise material information definition from multiple sources and help access and retrieve such information in an accurate and scalable manner. As such, the study contributes to a deeper understanding of how material property data should be defined and modelled to enable more accurate and efficient material data exchange and performance assessment.}
}
@article{KWON2020101102,
title = {Enriching standards-based digital thread by fusing as-designed and as-inspected data using knowledge graphs},
journal = {Advanced Engineering Informatics},
volume = {46},
pages = {101102},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101102},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620300719},
author = {Soonjo Kwon and Laetitia V. Monnier and Raphael Barbau and William Z. Bernstein},
keywords = {Digital thread, Knowledge graph, Ontology, Industrial data standard, Product lifecycle, Data fusion},
abstract = {Realizing the digital thread is essential for linking and orchestrating data across the product lifecycle in smart manufacturing. Linking heterogeneous lifecycle data is critical to maintain associativity and traceability in a digital thread. Recently, researchers have successfully leveraged ontology models with knowledge graphs in engineering domains for threading different lifecycle data. One of the most successful of such efforts is OntoSTEP which enables the formal capture of information embedded in the STandard for Exchange of Product model data (STEP) data representation, or ISO 10303. Meanwhile, an emerging inspection standard, called the Quality Information Framework (QIF), has garnered significant attention as it can bring quality information into the digital thread. Implementing more automated methods for product quality assurance is challenging due to the lack of unified information models from design to inspection. To this end, we propose an approach to fuse as-designed data represented in STEP and as-inspected data represented in QIF in a standards-based digital thread based on ontology with knowledge graphs. Specifically, we present an automated pipeline for generating knowledge graphs representing STEP and QIF data, a mapping implementation to integrate STEP and QIF knowledge graphs, and rules and queries to demonstrate the integration’s potential for better decision making with respect to product quality assurance.}
}
@article{WATKINS2025108638,
title = {Neuradicon: Operational representation learning of neuroimaging reports},
journal = {Computer Methods and Programs in Biomedicine},
volume = {262},
pages = {108638},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2025.108638},
url = {https://www.sciencedirect.com/science/article/pii/S0169260725000550},
author = {Henry Watkins and Robert Gray and Adam Julius and Yee-Haur Mah and James Teo and Walter H.L. Pinaya and Paul Wright and Ashwani Jha and Holger Engleitner and Jorge Cardoso and Sebastien Ourselin and Geraint Rees and Rolf Jaeger and Parashkev Nachev},
keywords = {Natural language processing, Neurology, Neuroradiology, Artificial intelligence},
abstract = {Background and Objective:
Radiological reports typically summarize the content and interpretation of imaging studies in unstructured form that precludes quantitative analysis. This limits the monitoring of radiological services to throughput undifferentiated by content, impeding specific, targeted operational optimization. Here we present Neuradicon, a natural language processing (NLP) framework for quantitative analysis of neuroradiological reports.
Methods:
Our framework is a hybrid of rule-based and machine-learning models to represent neurological reports in succinct, quantitative form optimally suited to operational guidance. These include probabilistic models for text classification and tagging tasks, alongside auto-encoders for learning latent representations and statistical mapping of the latent space.
Results:
We demonstrate the application of Neuradicon to operational phenotyping of a corpus of 336,569 reports, and report excellent generalizability across time and two independent healthcare institutions. In particular, we report pathology classification metrics with f1-scores of 0.96 on prospective data, and semantic means of interrogating the phenotypes surfaced via latent space representations.
Conclusion:
Neuradicon allows the segmentation, analysis, classification, representation and interrogation of neuroradiological reports structure and content. It offers a blueprint for the extraction of rich, quantitative, actionable signals from unstructured text data in an operational context.}
}
@article{LIU2022100726,
title = {Standing with our hometowns? The relationship between residents' perceived threat from COVID-19 and intention to support tourism recovery in their hometown},
journal = {Journal of Destination Marketing & Management},
volume = {25},
pages = {100726},
year = {2022},
issn = {2212-571X},
doi = {https://doi.org/10.1016/j.jdmm.2022.100726},
url = {https://www.sciencedirect.com/science/article/pii/S2212571X22000385},
author = {Yan Liu and XinYue Cao and Xavier Font and XingPing Cao},
keywords = {Intention to support hometown, Perceived threat, Need to belong, Psychological reactance, Tourism recovery},
abstract = {A hometown is the place to which an individual has an affective bond resulting from either being born there or living there for lived for a long time. This article investigates people's intention to support the revival of tourism in their hometowns post-COVID-19. The research hypothesises that individuals are affected by the threat to ontological security, freedom of movement, and freedom of information, and this synthetic threat will affect their intention to support their hometown. Based on compensatory control theory and psychological reactance theory, the study investigates how the need to belong, combined with psychological reactance, reveals the underlying mechanisms of perceived threat on intention to support one's hometown. The survey responses from 658 residents in China were analysed using a structural equation model. The results showed that the perceived threat has a positive effect on intention to support one's hometown and need to belong mediates that relationship. Instead, despite perceived threat to their freedom, residents did not report psychological reactance when faced with hometown appeals. These results could help destinations to revive in the post-pandemic era; destination management organisations, especially in China, should be able to appeal to residents for promotional support without expecting psychological reactance.}
}
@article{GHOSH2021157,
title = {A knowledge organization framework for influencing tourism-centered place-making},
journal = {Journal of Documentation},
volume = {78},
number = {2},
pages = {157-176},
year = {2021},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-12-2020-0220},
url = {https://www.sciencedirect.com/science/article/pii/S0022041821000618},
author = {Shiv Shakti Ghosh and Sunil Kumar Chatterjee},
keywords = {Online reviews, Facet categories, Tourist review ontology, Tourist knowledge organization, Place-making},
abstract = {Purpose
This study demonstrates the synthesis of a knowledge organization framework from tourist reviews and an ontological model with its implementation in graph database, which is based on this framework. The aim is to influence place-making outcomes at tourist destinations.
Design/methodology/approach
The faceted classification approach has been used for generating and validating the framework based on online reviews about urban tourism parks. The framework was used to develop an ontology using Protégé ontology editor that was implemented using GraphDB.
Findings
Three fundamental facet categories, namely Component, Aspect and Outcome, each consisting of several sub-facets, were synthesized from the analyses of the reviews. Besides helping in constructing the ontology, the analysis also helped in calculating an importance-score for the reviews that helped in ranked information retrieval.
Research limitations/implications
The analyses of the reviews were done manually and may carry human bias. But it is robust as it is based on a canonical faceted methodology.
Practical implications
It is envisaged that this study will help tourist destination planners in decision-making by easing the utilization of tourist generated reviews by the knowledge management systems they use. Opinions of tourists will be induced in destination planning thereby helping in the production of quality “places.”
Originality/value
The presented faceted framework aims to specifically aid knowledge organization pertaining to online reviews related to tourist destinations. The focus is on organizing knowledge to facilitate tourism development for better place-making outcomes, which is an important area of research though it has little contributions.}
}
@article{HENDAWI2023,
title = {A Mobile App That Addresses Interpretability Challenges in Machine Learning–Based Diabetes Predictions: Survey-Based User Study},
journal = {JMIR Formative Research},
volume = {7},
year = {2023},
issn = {2561-326X},
doi = {https://doi.org/10.2196/50328},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X23005322},
author = {Rasha Hendawi and Juan Li and Souradip Roy},
keywords = {disease prediction, explainable AI, artificial intelligence, knowledge graph, machine learning, ontology, diabetes},
abstract = {Background
Machine learning approaches, including deep learning, have demonstrated remarkable effectiveness in the diagnosis and prediction of diabetes. However, these approaches often operate as opaque black boxes, leaving health care providers in the dark about the reasoning behind predictions. This opacity poses a barrier to the widespread adoption of machine learning in diabetes and health care, leading to confusion and eroding trust.
Objective
This study aimed to address this critical issue by developing and evaluating an explainable artificial intelligence (AI) platform, XAI4Diabetes, designed to empower health care professionals with a clear understanding of AI-generated predictions and recommendations for diabetes care. XAI4Diabetes not only delivers diabetes risk predictions but also furnishes easily interpretable explanations for complex machine learning models and their outcomes.
Methods
XAI4Diabetes features a versatile multimodule explanation framework that leverages machine learning, knowledge graphs, and ontologies. The platform comprises the following four essential modules: (1) knowledge base, (2) knowledge matching, (3) prediction, and (4) interpretation. By harnessing AI techniques, XAI4Diabetes forecasts diabetes risk and provides valuable insights into the prediction process and outcomes. A structured, survey-based user study assessed the app’s usability and influence on participants’ comprehension of machine learning predictions in real-world patient scenarios.
Results
A prototype mobile app was meticulously developed and subjected to thorough usability studies and satisfaction surveys. The evaluation study findings underscore the substantial improvement in medical professionals’ comprehension of key aspects, including the (1) diabetes prediction process, (2) data sets used for model training, (3) data features used, and (4) relative significance of different features in prediction outcomes. Most participants reported heightened understanding of and trust in AI predictions following their use of XAI4Diabetes. The satisfaction survey results further revealed a high level of overall user satisfaction with the tool.
Conclusions
This study introduces XAI4Diabetes, a versatile multi-model explainable prediction platform tailored to diabetes care. By enabling transparent diabetes risk predictions and delivering interpretable insights, XAI4Diabetes empowers health care professionals to comprehend the AI-driven decision-making process, thereby fostering transparency and trust. These advancements hold the potential to mitigate biases and facilitate the broader integration of AI in diabetes care.}
}
@article{ZREIK2022102015,
title = {Matching and analysing conservation–restoration trajectories},
journal = {Data & Knowledge Engineering},
volume = {139},
pages = {102015},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102015},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2200026X},
author = {Alaa Zreik and Zoubida Kedad},
keywords = {Trajectory matching, Ontology, Semantic trajectory, Semantic similarity, Trajectory analysis},
abstract = {The context of this work is an on-going project at the French National Library (BnF), which aims at providing predictions of the documents physical state based on their conservation–restoration histories. A document can be either in a good state and available to the readers, or damaged and unavailable to them. As libraries may contain millions of documents, the manual monitoring and analysis of their physical state is not realistic in practice. We therefore propose to analyse their conservation histories in order to derive reliable predictions of their physical state. To achieve this goal, we introduce in this paper the following contributions. First, we propose a representation of a document conservation history as a conservation–restoration trajectory, and we define its different types of events. We also propose a trajectory matching process that computes a similarity score between two conservation–restoration trajectories considering the terminological heterogeneity of the events, using an ontological model that represents the domain experts knowledge. Second, we provide a trajectory analysis process which identifies the most representative sequences of events of the deteriorated documents. Finally, we propose a prediction model for the physical state of the documents based on the trajectory analysis process. We present some experiments showing the effectiveness of the matching process as well as the prediction model.}
}
@article{SANTOS2023100995,
title = {MARTINE’s real-time local market simulation with a semantically interoperable society of multi-agent systems},
journal = {Sustainable Energy, Grids and Networks},
volume = {33},
pages = {100995},
year = {2023},
issn = {2352-4677},
doi = {https://doi.org/10.1016/j.segan.2023.100995},
url = {https://www.sciencedirect.com/science/article/pii/S2352467723000036},
author = {Gabriel Santos and Luís Gomes and Tiago Pinto and Pedro Faria and Zita Vale},
keywords = {Cyber–physical simulation, Local electricity market, Ontologies, Semantic interoperability, Society of multi-agent systems},
abstract = {There is a growing complexity, volatility, and unpredictability in the electric sector that hardens the decision-making process. To this end, the use of proper decision support tools and simulation platforms becomes essential. This paper presents the Multi-Agent based Real-Time INfrastructure for Energy (MARTINE) platform that allows real-time simulation and emulation of loads, resources, and infrastructures. MARTINE uses multi-agent systems that connect to physical resources and can represent additional simulated players that are not physically present in the simulation and emulation environment, enabling the creation of complex scenarios for testing and validation. MARTINE provides the seamless integration of real-time emulation with simulated and physical resources simultaneously in a unique simulation environment, which is only possible by supporting multi-agent systems. This work presents MARTINE’s integration in a semantically interoperable multi-agent systems society developed for the test, study, monitoring, and validation of the power system sector. The use of ontologies and semantic web technologies eases the interoperability between the heterogeneous systems. The case study scenario demonstrates the use of MARTINE in simulating a local community electricity market that combines real-time data from physical devices with simulated data and the use of semantic web techniques to make the system interoperable, configurable, and flexible.}
}
@article{ESPOSITO2025,
title = {Semantic Technologies Applied to Blockchain for Cloud Computing Services Marketplace Implementation},
journal = {International Journal on Semantic Web and Information Systems},
volume = {21},
number = {1},
year = {2025},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.387386},
url = {https://www.sciencedirect.com/science/article/pii/S1552628325000390},
author = {Antonio Esposito and Salvatore D'Angelo and Davide Casuccio},
keywords = {Cloud Computing, Blockchain, Semantics, Ontologies, Service Level Agreement, NFT, Service Discovery, Service Composition},
abstract = {ABSTRACT
In recent years, cloud marketplaces have emerged as dominant platforms for cloud service procurement. These marketplaces, led by major providers such as Azure, Amazon Web Services, and Google Cloud, feature services exclusively available on their own platforms, introducing limitations for end users. This paper explores the convergence of cloud computing, blockchain, and semantics, envisioning a marketplace where cloud providers can offer their cloud services and cloud consumers can efficiently discover, procure, and potentially resell these services. Semantic representations streamline service discovery and composition, enhancing interoperability. Blockchain technology and non-fungible tokens play pivotal roles in establishing a fully decentralized marketplace characterized by data verification and immutability. This enables autonomous contract validation and execution between cloud consumers and cloud providers, reducing reliance on intermediaries. This innovative fusion promises to reshape the landscape of cloud service procurement, making it more transparent and independent.}
}
@article{QIAN2025103614,
title = {Rural-urban interfaces and changing forms of relational and planetary rurality},
journal = {Journal of Rural Studies},
volume = {116},
pages = {103614},
year = {2025},
issn = {0743-0167},
doi = {https://doi.org/10.1016/j.jrurstud.2025.103614},
url = {https://www.sciencedirect.com/science/article/pii/S0743016725000543},
author = {Junxi Qian and Shenjing He and Darren Smith},
keywords = {Rurality, Rural-urban interface, Relationality, Worlding, Planetary rural geographies},
abstract = {This editorial introduction aims to frame the special issue entitled “Rural-urban interfaces and changing forms of relational and planetary rurality”. Problematising the dominant planetary urbanisation thesis, particularly its tendency in eliding alternative spaces, subjectivities, and politics to the global expansion of capitalist urban fabrics, this special issue seeks to rethink the rural-urban interface through the lenses of relationality and planetary rural geographies, highlighting the promiscuous interpenetration between the urban and the rural, the planetary significance of rurality, and, hence, the need to reassert the rural as a distinct spatial ontology and category. To move forward this theoretical agenda, this editorial situates our investigation of the rural-urban interface within a long pedigree of research on rural-urban interaction, coordination, or integration. However, such works are often leaned towards the dissemination of urban economic functions into rural places and often leave limited discursive space for a conceptual and theoretical rethinking of rurality per se. We advance the latter by building on a heuristic of “worlding” at the rural-urban interface, and bring this epistemology into a direct conversation with the six papers of this special issue.}
}
@article{MIN2022100484,
title = {Applications of knowledge graphs for food science and industry},
journal = {Patterns},
volume = {3},
number = {5},
pages = {100484},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100484},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922000691},
author = {Weiqing Min and Chunlin Liu and Leyi Xu and Shuqiang Jiang},
keywords = {knowledge graph, artificial intelligence, ontology, food science and industry, nutrition and health, new recipe development, food analysis},
abstract = {Summary
The deployment of various networks (e.g., Internet of Things [IoT] and mobile networks), databases (e.g., nutrition tables and food compositional databases), and social media (e.g., Instagram and Twitter) generates huge amounts of food data, which present researchers with an unprecedented opportunity to study various problems and applications in food science and industry via data-driven computational methods. However, these multi-source heterogeneous food data appear as information silos, leading to difficulty in fully exploiting these food data. The knowledge graph provides a unified and standardized conceptual terminology in a structured form, and thus can effectively organize these food data to benefit various applications. In this review, we provide a brief introduction to knowledge graphs and the evolution of food knowledge organization mainly from food ontology to food knowledge graphs. We then summarize seven representative applications of food knowledge graphs, such as new recipe development, diet-disease correlation discovery, and personalized dietary recommendation. We also discuss future directions in this field, such as multimodal food knowledge graph construction and food knowledge graphs for human health.}
}
@article{GAO2024104596,
title = {Clinical natural language processing for secondary uses},
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104596},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104596},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000145},
author = {Yanjun Gao and Diwakar Mahajan and Özlem Uzuner and Meliha Yetisgen}
}
@article{BAI2024112,
title = {A derived information framework for a dynamic knowledge graph and its application to smart cities},
journal = {Future Generation Computer Systems},
volume = {152},
pages = {112-126},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23003825},
author = {Jiaru Bai and Kok Foong Lee and Markus Hofmeister and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {Dynamic knowledge graph, Derived information, Data provenance, Directed acyclic graph, Smart cities},
abstract = {In this work, we develop a derived information framework to semantically annotate how a piece of information can be obtained from others in a dynamic knowledge graph. We encode this using the notion of a “derivation” and capture its metadata with a lightweight ontology. We provide an agent template designed to monitor derivations and to standardise agents performing this and related operations. We implement both synchronous and asynchronous communication modes for agents interacting with the knowledge graph. When occurring in conjunction, directed acyclic graphs of derivations can arise, with changing data propagating through the knowledge graph by means of agents’ actions. While the framework itself is domain-agnostic, we apply it in the context of smart cities as part of the World Avatar project and demonstrate that it is capable of handling sequential events across different timescales. Starting from source information, the framework automatically populates derived data and ensures they remain up to date upon access for a potential flood impact assessment use case.}
}
@article{PEREZPEREZ2022116616,
title = {A deep learning relation extraction approach to support a biomedical semi-automatic curation task: The case of the gluten bibliome},
journal = {Expert Systems with Applications},
volume = {195},
pages = {116616},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116616},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422001075},
author = {Martín Pérez-Pérez and Tânia Ferreira and Gilberto Igrejas and Florentino Fdez-Riverola},
keywords = {Text mining, Relation extraction, Deep learning, Ontology-based methods, Literature curation, Gluten},
abstract = {Discover relevant biomedical interactions in the literature is crucial for enhancing biology research. This curation process has an essential role in studying the different processes and interactions reported that affect the biological process (e.g., genome, metabolome, and transcriptome). In this sense, the objective of this work is twofold: reduce the manual effort required to curate and review the existing biochemical interactions reported in the gluten-related bibliome, while proposing a novel vector-space integrated into a deep learning model to assists manual curators in a real curation task by learning from their previous decisions. With this objective, the present work proposes a novel vector-space that combine (i) high-level lexical and syntactic inference features as Wordnets and Health-related domain ontologies, (ii) unsupervised semantic resources as word embedding, (iii) semantic and syntactic sentence knowledge, (iv) abbreviation resolution support, (v) several state-of-the-art Named-entity recognition methods, and, finally, (vi) different feature construction and optimization techniques to support a semi-automatic curation workflow. Therefore, the application of the proposed workflow over a classified set of 2,451 relevant gluten-related documents produces a total of 8,349 relevant and 471,813 irrelevant relations distributed in thirteen domain health-related categories. Experimental results showed that the proposed workflow is a valuable approach for a semi-automatic relation extraction task. It was able to obtain satisfactory results in the early stages of a real-world curation task and saved manual annotation efforts by learning from the decisions made by manual curators in iterative annotation rounds. The average F.score for the proposed relation categories was 0.731, being the lowest F.score at 0.47 and the highest F.score at 0.929. The different resources used in this work as well as the manually curated corpus are public available on our GitHub repository.}
}
@article{KAGAN2024100658,
title = {Toward a nomenclature consensus for diverse intelligent systems: Call for collaboration},
journal = {The Innovation},
volume = {5},
number = {5},
pages = {100658},
year = {2024},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2024.100658},
url = {https://www.sciencedirect.com/science/article/pii/S2666675824000961},
author = {Brett J. Kagan and Michael Mahlis and Anjali Bhat and Josh Bongard and Victor M. Cole and Phillip Corlett and Christopher Gyngell and Thomas Hartung and Bianca Jupp and Michael Levin and Tamra Lysaght and Nicholas Opie and Adeel Razi and Lena Smirnova and Ian Tennant and Peter Thestrup Wade and Ge Wang},
abstract = {Summary
Disagreements about language use are common both between and within fields. Where interests require multidisciplinary collaboration or the field of research has the potential to impact society at large, it becomes critical to minimize these disagreements where possible. The development of diverse intelligent systems, regardless of the substrate (e.g., silicon vs. biology), is a case where both conditions are met. Significant advancements have occurred in the development of technology progressing toward these diverse intelligence systems. Whether progress is silicon based, such as the use of large language models, or through synthetic biology methods, such as the development of organoids, a clear need for a community-based approach to seeking consensus on nomenclature is now vital. Here, we welcome collaboration from the wider scientific community, proposing a pathway forward to achieving this intention, highlighting key terms and fields of relevance, and suggesting potential consensus-making methods to be applied.}
}
@article{NIZAMIS2018382,
title = {A Semantic Framework for Agent-based Collaborative Manufacturing Eco-systems},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {382-387},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.323},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318314472},
author = {Alexandros G. Nizamis and Dimosthenis K. Ioannidis and Nikolaos T. Kaklanis and Dimitrios K. Tzovaras},
keywords = {Manufacturing eco-system, Semantic modeling, Ontology, Rule-based Matchmaking, Supply, demand chain},
abstract = {Manufacturing eco-systems aim to connect data and services between factories, suppliers and customers. Most of them are built as agent-based eco-systems which act as web-based operating systems for business connections between enterprises. The connection of supply and demand entities participating in an eco-system by exploiting knowledge and data from the business entities has become imperative for them, in order to adapt to the dynamically changing market requirements. This paper introduces a web-based semantic ontological framework designed for collaborative agent-based manufacturing eco-systems. The proposed framework and its core components enable the information modeling of the manufacturing services and the supply chain concepts. A Collaborative Manufacturing Services Ontology able to describe both manufacturing domain and e-commerce domain is offered alongside with an Application Programming Interface for the effortless manipulation of the ontological resources. Furthermore, a Rule-based Matchmaking engine able to match requesters with possible suppliers, and to evaluate offers from suppliers based on different requesters’ criteria and preferences is provided.}
}
@article{DU2019238,
title = {Language model-based automatic prefix abbreviation expansion method for biomedical big data analysis},
journal = {Future Generation Computer Systems},
volume = {98},
pages = {238-251},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18326529},
author = {Xiaokun Du and Rongbo Zhu and Yanhong Li and Ashiq Anjum},
keywords = {Abbreviation expansion, Biomedical text analysis, Language model},
abstract = {In biomedical domain, abbreviations are appearing more and more frequently in various data sets, which has caused significant obstacles to biomedical big data analysis. The dictionary-based approach has been adopted to process abbreviations, but it cannot handle ad hoc abbreviations, and it is impossible to cover all abbreviations. To overcome these drawbacks, this paper proposes an automatic abbreviation expansion method called LMAAE (Language Model-based Automatic Abbreviation Expansion). In this method, the abbreviation is firstly divided into blocks; then, expansion candidates are generated by restoring each block; and finally, the expansion candidates are filtered and clustered to acquire the final expansion result according to the language model and clustering method. Through restrict the abbreviation to prefix abbreviation, the search space of expansion is reduced sharply. And then, the search space is continuous reduced by restrained the effective and the length of the partition. In order to validate the effective of the method, two types of experiments are designed. For standard abbreviations, the expansion results include most of the expansion in dictionary. Therefore, it has a high precision. For ad hoc abbreviations, the precisions of schema matching, knowledge fusion are increased by using this method to handle the abbreviations. Although the recall for standard abbreviation needs to be improved, but this does not affect the good complement effect for the dictionary method.}
}
@article{ZHENG2023105067,
title = {Dynamic prompt-based virtual assistant framework for BIM information search},
journal = {Automation in Construction},
volume = {155},
pages = {105067},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105067},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523003278},
author = {Junwen Zheng and Martin Fischer},
keywords = {Building information modeling, Generative pre-trained transformer, Virtual assistant, Information search, Natural language processing, Artificial intelligence, BIMS-GPT, Prompt engineering, Large language model, Information retrieval},
abstract = {Efficient information search from building information models (BIMs) requires deep BIM knowledge or extensive engineering efforts for building natural language (NL)-based interfaces. To address this challenge, this paper introduces a dynamic prompt-based virtual assistant framework dubbed “BIMS-GPT” that integrates generative pre-trained transformer (GPT) technologies, supporting NL-based BIM search. To understand users' NL queries, extract relevant information from BIM databases, and deliver NL responses along with 3D visualizations, a dynamic prompt-based process was developed. In a case study, BIMS-GPT's functionality is demonstrated through a virtual assistant prototype for a hospital building. When evaluated with a BIM query dataset, the approach achieves accuracy rates of 99.5% for classifying NL queries with incorporating 2% of the data in prompts. This paper contributes to the advancement of effective and versatile virtual assistants for BIMs in the construction industry as it significantly enhances BIM accessibility while reducing the engineering and training data prerequisites for processing NL queries.}
}
@article{YIN2022127,
title = {In-silico analysis reveals the core targets and mechanisms of CA028, a new derivative of calycosin, in the treatment of colorectal cancer},
journal = {Intelligent Medicine},
volume = {2},
number = {3},
pages = {127-133},
year = {2022},
issn = {2667-1026},
doi = {https://doi.org/10.1016/j.imed.2022.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2667102622000080},
author = {Feiying Yin and Xing Zhang and Yu Li and Xiao Liang and Rong Li and Jian Chen},
keywords = {Colorectal cancer, Calycosin, CA028, Networking pharmacology, Function, Mechanism},
abstract = {Background
Colorectal cancer (CRC) is a type of malignant gastroenteric tumors associated with a high mortality rate worldwide. Calycosin, a natural phytoestrogen, possesses potent anti-cancer properties. We structurally modified calycosin to improve its physicochemical properties, and generated a novel small molecule termed CA028.
Methods
By using network pharmacology, followed by gene ontology and Kyoto Encyclopedia of Genes and Genomes enrichment analysis and molecular docking, we aimed to predict and disclose the biological functions and mechanism of CA028 in the treatment of CRC through bioinformatic analyses.
Results
By searching the online Swiss Target Prediction and TargetNet databases, we identified 150 genes shared by CA028 and CRC. Using the Search Tool for the Retrieval of Interacting Genes (STRING) database and Cytoscape software, we identified 14 hub-functional genes, namely the FYN proto-oncogene, a Src family tyrosine kinase (FYN), mitogen-activated protein kinase 1 (MAPK1), MAPK8, MAPK14, Rac family small GTPase 1 (RAC1), epidermal growth factor receptor (EGFR), protein tyrosine kinase 2 (PTK2), sphingosine-1-phosphate receptor 1 (S1PR1), S1PR2, Janus kinase 1 (JAK1), JAK2, the RELA proto-oncogene NF-κB subunit (RELA), bradykinin receptor B1 (BDKRB1), and BDKRB2. Additionally, biological docking analysis using the Autodock Vina software revealed that FYN and MAPK1 were the main pharmacological proteins of CA028 against CRC. The gene ontology analysis using R-language packages further revealed the anti-CRC functions of CA028, including biological processes, cell components, and molecular pathways.
Conclusion
CA028 exhibits effective pharmacological activity against CRC by suppressing the proliferation of CRC cells and improving the tumor microenvironment. Importantly, certain predicted genes (e.g., FYN and MAPK1) may be the pharmacological targets of CA028 in the treatment of CRC.}
}
@article{BARKI2022631,
title = {Reasoning about bladder cancer treatment outcomes using clinical trials within a knowledge-based clinical evidence approach},
journal = {Procedia Computer Science},
volume = {196},
pages = {631-639},
year = {2022},
note = {International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.058},
url = {https://www.sciencedirect.com/science/article/pii/S187705092102281X},
author = {Chamseddine Barki and Hanene Boussi Rahmouni and Salam Labidi},
keywords = {Adverse events, bladder cancer, efficacy, knowledge, protocol, randomized controlled trial, safety, side effects, treatments},
abstract = {Side effects (SEs) and adverse events (AEs) of bladder cancer (BC) treatment have become more prevalent and may impact the effectiveness of the prescribed therapy. Given, the substantial combination of various treatment types and interventions, it is crucial for the urologist and the team of specialists to discern the systematic and local SEs of these treatments for more awareness and vigilance. This was shown within randomized clinical trials (RCTs) which are complex, and their management needs many efforts. RCTs generate a big amount of knowledge that can serve as evidence for future clinical decision making. Current approaches do not discuss semantic integration of different resources. In this paper we propose a knowledge-based BC treatment effects and complication infrastructure (BCTECI) approach to reason and appraise the effectiveness of BC treatments within their related SEs. This optimized treatment outcomes. Referring to ontology features, a knowledge model with semantic queries and logic rules included evidence concluded from the assessed RCTs. Hence, this supports interoperability between RCTs resources and a common treatment ontology. For this, a comprehensive literature search of relevant RCTs was performed systematically in different electronic medical databases. All pertinent RCTs covering BC treatment cases and reporting prescriptions’ effectiveness were included in BCTECI knowledge model. This study provided the required taxonomy and semanticization and evaluated the effectiveness of treatment outcomes. Furthermore, it extended the clinical evidence-based knowledge for AEs anticipation with a more efficient treatment prescription and showed whether SEs impact the effectiveness of a prescribed treatment until revocation.}
}
@article{BURGER2018142,
title = {A framework for semi-automated co-evolution of security knowledge and system models},
journal = {Journal of Systems and Software},
volume = {139},
pages = {142-160},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S016412121830027X},
author = {Jens Bürger and Daniel Strüber and Stefan Gärtner and Thomas Ruhroth and Jan Jürjens and Kurt Schneider},
keywords = {Security requirements, Software evolution, Co-evolution, Software design, Security impact analysis},
abstract = {Security is an important and challenging quality aspect of software-intensive systems, becoming even more demanding regarding long-living systems. Novel attacks and changing laws lead to security issues that did not necessarily rise from a flawed initial design, but also when the system fails to keep up with a changing environment. Thus, security requires maintenance throughout the operation phase. Ongoing adaptations in response to changed security knowledge are inevitable. A necessary prerequisite for such adaptations is a good understanding of the security-relevant parts of the system and the security knowledge. We present a model-based framework for supporting the maintenance of security during the long-term evolution of a software system. It uses ontologies to manage the system-specific and the security knowledge. With model queries, graph transformation and differencing techniques, knowledge changes are analyzed and the system model is adapted. We introduce the novel concept of Security Maintenance Rules to couple the evolution of security knowledge with co-evolutions of the system model. As evaluation, community knowledge about vulnerabilities is used (Common Weakness Enumeration database). We show the applicability of the framework to the iTrust system from the medical care domain and hence show the benefits of supporting co-evolution for maintaining secure systems.}
}
@article{SIOUGKROU2018385,
title = {Semantically-enabled repositories in multi-disciplinary domains: The case of biorefineries},
journal = {Computers & Chemical Engineering},
volume = {116},
pages = {385-400},
year = {2018},
note = {Multi-scale Systems Engineering – in memory & honor of Professor C.A. Floudas},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2018.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S009813541830348X},
author = {Eirini Siougkrou and Filopoimin Lykokanellos and Foteini Barla and Antonis C. Kokossis},
keywords = {Ontology engineering, Biorefineries, Biorenewables, Repository, Synthesis of value chain},
abstract = {There is an increased use of problem representations (i.e. superstructures in synthesis problems; networks in route problems; graphs; ordered graphs in various systems representations) following on significant advances in optimization technologies that hold capabilities to solve, robustly, large-scale problems. In an attempt to systematically tackle disparate domains and build high-throughput functions, the paper contributes with a semantically-enabled approach systematized and engineered by ontologies. The aim is to develop an intelligent environment with capabilities to build and scale-up system representations, automatically. The work is demonstrated on problems akin to biorenewables and biorefineries; an identical approach is possible to the general problem. Using relations and rules defined among entities, semantics are deployed to model and expand domains (biorefinery pathways) whereas enabling extracting and creating knowledge. The repository, already on a web-based platform and available as open-source, essentially upgrades conventional representations with capabilities to share (import/export) and integrate its content externally.}
}
@article{LI2023979,
title = {Construction of risk response scenarios for the emergency material support system},
journal = {Procedia Computer Science},
volume = {221},
pages = {979-983},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.077},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923008359},
author = {Longfei Li and Xiaolei Sun and Weilan Suo},
keywords = {Emergency material support system (EMSS), Scenario construction, Ontological method},
abstract = {Abstracts
Representing disaster scenarios and evaluating the emergency material support system (EMSS) is crucial to enhance emergency material support capabilities. Current representation methods for EMSS mostly focused on the task response procedure during emergencies, rarely involved the response process analysis for disaster scenarios. This study utilizes an ontological method to construct a representation of risk response scenarios for the EMSS. It can be achieved by representing scenario feature elements, scenario structure elements, scenario constraint elements, and scenario attribute elements through the four dimensions. The scenario representation can generate different setting schemes for EMSS. An example scenario was presented based on the measures implemented by the Chinese government during the COVID-19 epidemic's closure of Wuhan. The findings of this research can provide valuable support for making risk-informed decisions regarding EMSS.}
}
@article{ANAND2025101069,
title = {Algorithms in the orchard: An embedding-based expert answering system for apple rust},
journal = {Smart Agricultural Technology},
volume = {12},
pages = {101069},
year = {2025},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2025.101069},
url = {https://www.sciencedirect.com/science/article/pii/S2772375525003028},
author = {Astha Anand and Jian Shen and Armin Bernd Cremers and Marc Jacobs},
keywords = {Embeddings, Retrieval-augmented generation, Knowledge graph, Apple rust, Large language model, Generative artificial intelligence, Agricultural pest control},
abstract = {As sustainable agricultural practices gain importance, the need for intelligent pest control decision-making has grown. This paper introduces SEEDS: Similarity-based Expert Embedding Decision System, a Retrieval-Augmented Generation (RAG) based agricultural question-answering (QA) system. It is built upon a domain-specific knowledge graph (KG), representing Cedar Apple Rust disease, its host and causative agents, plant defense molecules against apple rust infection, and various pesticides. Utilizing the OpenAI embedding model, the system generates embeddings for user queries and KG data, employing similarity metrics to rank KG entries, facilitating accurate and relevant pest control recommendations. SEEDS is a promising niche AI tool in plant protection, setting the stage for scalable, extensible QA frameworks in precision agriculture. The results signify not only a step forward in agricultural expert systems but also highlight the potential for expanding this approach to other crops and pests, marking a substantial advancement in the use of AI for agricultural pest control.}
}
@article{KUCERA201869,
title = {Semantic BMS: Allowing usage of building automation data in facility benchmarking},
journal = {Advanced Engineering Informatics},
volume = {35},
pages = {69-84},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S147403461730023X},
author = {Adam Kučera and Tomáš Pitner},
keywords = {Facility management, Building automation systems, Semantic sensor network ontology, Decision support, Data integration},
abstract = {Facility benchmarking and evaluation of facility performance are the crucial tasks in reaching efficient, economical and sustainable facility operation. Modern buildings are equipped with building automation systems (BAS) that contain vast numbers of various sensors that can be utilised in performance assessment. However, such systems lack convenient tools for data inspection, which limits their use in building performance and efficiency analysis and benchmarking especially on large sites. The paper presents a middleware layer designed to enrich BAS data with additional semantic information. As a semantic model, an adaptation of the Semantic Sensor Network (SSN) ontology for the field of building operation analysis is used. The middleware provides convenient interfaces for querying the model. The proposed system provides the facility managers with a convenient way to use the BAS data for benchmarking and decision support.}
}
@article{SANPRASIT2021115226,
title = {Intelligent approach to automated star-schema construction using a knowledge base},
journal = {Expert Systems with Applications},
volume = {182},
pages = {115226},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115226},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421006588},
author = {Non Sanprasit and Katechan Jampachaisri and Taravichet Titijaroonroj and Kraisak Kesorn},
keywords = {Data warehouse, Intelligent system, Multidimensional model, Ontology, Semantic approach, Star schema},
abstract = {Most data-warehouse construction processes are performed manually by experts, which is laborious, time-consuming, and prone to error. Furthermore, special knowledge is required to design complex multidimensional models, such as a star schema. This predicament has motivated computer scientists to propose automation techniques to generate such models. For this reason, we present a new strategy that incorporates knowledge-based models into a framework, named the Semantic-based Star-schema Designer, that assists the automation of star schema construction. Our models provide reasoning capabilities needed by star schema designs, including those that can disambiguate heterogeneous terms, detect appropriate data types and attribute sizes, and organize data hierarchies to support online analytical processes. We also propose strategies to overcome the uncertainty arising when attribute names are not available in the data source. The names of unknown attributes are thus predicted using an arithmetic coding technique to infer column names. Our system also generates star schema from semi-structured data (e.g., comma-separated-value files and spreadsheets), which do not provide primary keys, foreign keys, or relationship cardinalities between tables. Our framework facilitates star schema construction and their relationship information without human intervention using homegrown algorithms. Experiments demonstrate that our technique predicts column names and data types that enable the effective generation of star schema better than baseline approaches.}
}
@article{QIU2019157,
title = {Geoscience keyphrase extraction algorithm using enhanced word embedding},
journal = {Expert Systems with Applications},
volume = {125},
pages = {157-169},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419301009},
author = {Qinjun Qiu and Zhong Xie and Liang Wu and Wenjia Li},
keywords = {Keyphrase extraction, Ontology, Word2vec, Geoscience domain},
abstract = {A large amount of unstructured textual data about geoscience structures and minerals is buried in geoscience documents and is unused. Automatic keyphrase extraction provides opportunities to leverage this wealth of data for analysis and knowledge discovery. However, keyphrase extraction remains a complicated task, and the performance of state-of-the-art approaches is still low. Automatic discovery of high-quality and meaningful keyphrases requires the application of useful knowledge and suitable techniques. Seeing both challenges and opportunities in the situation described above, this paper proposes an ontology and enhanced word embedding-based (OEWE) methodology for automatic keyphrase extraction from geoscience documents. We first develop a quantitative analysis for keyphrase extraction evaluation based on conditional probability and the naive Bayesian model, which is valuable when human-annotated keyphrases are not available. The domain ontology is then performed on a multiway tree to enrich the domain-specific knowledge on certain concepts and relationships in a domain. Simultaneously, word2vec, a model of a word distribution using deep learning, is updated by applying the geological ontology, and it links domain background information and identifies infrequent but representative keyphrases. We use two homemade geoscience datasets to evaluate the performance of OEWE. We compare our method with frequency, term frequency-inverse document frequency (TF-IDF), TextRank and rapid automatic keyword extraction (RAKE), finding that our method achieves average F1 scores of 30.1% and 40.7% on two manually annotated datasets.}
}
@article{DEWI20244195,
title = {Adjusted Reasoning Module for Deep Visual Question Answering Using Vision Transformer},
journal = {Computers, Materials and Continua},
volume = {81},
number = {3},
pages = {4195-4216},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.057453},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008403},
author = {Christine Dewi and Hanna Prillysca Chernovita and Stephen Abednego Philemon and Christian {Adi Ananta} and Abbott Po Shun Chen},
keywords = {VQA, vision transformer, multimodal data, deep learning},
abstract = {Visual Question Answering (VQA) is an interdisciplinary artificial intelligence (AI) activity that integrates computer vision and natural language processing. Its purpose is to empower machines to respond to questions by utilizing visual information. A VQA system typically takes an image and a natural language query as input and produces a textual answer as output. One major obstacle in VQA is identifying a successful method to extract and merge textual and visual data. We examine “Fusion” Models that use information from both the text encoder and picture encoder to efficiently perform the visual question-answering challenge. For the transformer model, we utilize BERT and RoBERTa, which analyze textual data. The image encoder designed for processing image data utilizes ViT (Vision Transformer), Deit (Data-efficient Image Transformer), and BeIT (Image Transformers). The reasoning module of VQA was updated and layer normalization was incorporated to enhance the performance outcome of our effort. In comparison to the results of previous research, our proposed method suggests a substantial enhancement in efficacy. Our experiment obtained a 60.4% accuracy with the PathVQA dataset and a 69.2% accuracy with the VizWiz dataset.}
}
@article{WALTHER201929,
title = {The Research Core Dataset (KDSF) in the Linked Data context},
journal = {Procedia Computer Science},
volume = {146},
pages = {29-38},
year = {2019},
note = {14th International Conference on Current Research Information Systems, CRIS2018, FAIRness of Research Information},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.074},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930078X},
author = {Tatiana Walther and Christian Hauschke and Anna Kasprzik},
keywords = {research information management, Linked Data, VIVO, Research Core Dataset, ontology matching},
abstract = {This paper describes our efforts to implement the Research Core Dataset (“Kerndatensatz Forschung”; KDSF) as an ontology in VIVO. KDSF is used in VIVO to record the required metadata on incoming data and to produce reports as an output. While both processes need an elaborate adaptation of the KDSF specification, this paper focusses on the adaptation of the KDSF basic data model for recording data in VIVO. In this context, the VIVO and KDSF ontologies were compared with respect to domain, syntax, structure, and granularity in order to identify correspondences and mismatches. To produce an alignment, different matching approaches have been applied. Furthermore, we made necessary modifications and extensions on KDSF classes and properties.}
}
@article{ELNOZAHY201956,
title = {Question Answering System to Support University Students’ Orientation, Recruitment and Retention},
journal = {Procedia Computer Science},
volume = {164},
pages = {56-63},
year = {2019},
note = {CENTERIS 2019 - International Conference on ENTERprise Information Systems / ProjMAN 2019 - International Conference on Project MANagement / HCist 2019 - International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN/HCist 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.12.154},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919321933},
author = {Walaa A. Elnozahy and Ghada A. {El Khayat} and Lilia Cheniti-Belcadhi and Bilal Said},
keywords = {Ontology, Question Answering, Knowledge Extraction, Open Education Data, Linked data, University Admission, Decision Support, Student recruitment},
abstract = {Educational institutions are creating more and more programs and developing new techniques to identify and select the right students. Many universities started to develop their own ontologies to get an understanding of their students by analyzing their data. In this paper, an information extraction framework is proposed as a part of the research project “LET’SeGA” to support educational institutions in selecting students for different programs. This would also help target specific student segments in marketing decisions for academic programs. In the proposed framework, an ontological model is created for universities’ data which is used to support students’ recruitment and retention ensuring students’ success after admission. Future work will focus on testing the framework over a time period to verify that it supports the recruitment of appropriate student profiles.}
}
@article{MURAD2025,
title = {Artificial Intelligence in Benign Prostatic Hyperplasia},
journal = {Urologic Clinics of North America},
year = {2025},
issn = {0094-0143},
doi = {https://doi.org/10.1016/j.ucl.2025.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0094014325000606},
author = {Liam Murad and David Bouhadana and Aalya Hamouda and Adel Arezki and Ethan Layne and Conner Ganjavi and Giovanni E. Cacciamani and Alexander P. Glaser and Brian T. Helfand and Kevin C. Zorn},
keywords = {Artificial intelligence, Benign prostatic hyperplasia, BPH, Lower urinary tract symptoms, LUTS, Prostate cancer, Machine learning}
}
@article{HNAINI2024101704,
title = {E-SCORE: A web-based tool for security requirements engineering},
journal = {SoftwareX},
volume = {26},
pages = {101704},
year = {2024},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2024.101704},
url = {https://www.sciencedirect.com/science/article/pii/S235271102400075X},
author = {Hiba Hnaini and Raúl Mazo and Joël Champeau and Paola Vallejo and Jose Galindo},
keywords = {Security criteria ontology, Security criteria, Security analysis, Security requirements, Web interface, Software},
abstract = {As digital systems continue to grow in popularity, they also become more vulnerable to various forms of attacks with various motives, including financial gain and political influence. In response, engineers must consider system security from the design phase. However, defining security requirements at this stage can be challenging. To address this challenge, we propose E-SCORE, a web-based tool that streamlines the security requirements engineering process. E-SCORE implements the SCORE (Security Criteria Ontology for security Requirements Engineering) (i) to suggest security mechanisms and additional criteria to enhance security coverage and (ii) to facilitate security analysis of the system. An example of banking system usage is provided. Through our approach, we could define ten additional security requirements for a single requirement. Therefore, E-SCORE offers a valuable resource for engineers to ensure the security of digital systems across various domains.}
}
@article{NICOLINI20251318,
title = {Fine-tuning of conditional Transformers improves in silico enzyme prediction and generation},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {1318-1334},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.03.037},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025001072},
author = {Marco Nicolini and Emanuele Saitto and Ruben Emilio {Jimenez Franco} and Emanuele Cavalleri and Aldo Javier {Galeano Alfonso} and Dario Malchiodi and Alberto Paccanaro and Peter N. Robinson and Elena Casiraghi and Giorgio Valentini},
keywords = {Large Language Models, Protein Language Models, Fine-tuning of Large Language Models, Conditional Transformers,  enzyme design and modeling},
abstract = {We introduce Finenzyme, a Protein Language Model (PLM) that employs a multifaceted learning strategy based on transfer learning from a decoder-based Transformer, conditional learning using specific functional keywords, and fine-tuning for the in silico modeling of enzymes. Our experiments show that Finenzyme significantly enhances generalist PLMs like ProGen for the in silico prediction and generation of enzymes belonging to specific Enzyme Commission (EC) categories. Our in silico experiments demonstrate that Finenzyme generated sequences can diverge from natural ones, while retaining similar predicted tertiary structure, predicted functions and the active sites of their natural counterparts. We show that embedded representations of the generated sequences obtained from the embeddings computed by both Finenzyme and ESMFold closely resemble those of natural ones, thus making them suitable for downstream tasks, including e.g. EC classification. Clustering analysis based on the primary and predicted tertiary structure of sequences reveals that the generated enzymes form clusters that largely overlap with those of natural enzymes. These overall in silico validation experiments indicate that Finenzyme effectively captures the structural and functional properties of target enzymes, and can in perspective support targeted enzyme engineering tasks.}
}
@article{PAUZI2023111616,
title = {Applications of natural language processing in software traceability: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111616},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111616},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223000110},
author = {Zaki Pauzi and Andrea Capiluppi},
keywords = {Software traceability, Information retrieval, Natural language processing},
abstract = {A key part of software evolution and maintenance is the continuous integration from collaborative efforts, often resulting in complex traceability challenges between software artifacts: features and modules remain scattered in the source code, and traceability links become harder to recover. In this paper, we perform a systematic mapping study dealing with recent research recovering these links through information retrieval, with a particular focus on natural language processing (NLP). Our search strategy gathered a total of 96 papers in focus of our study, covering a period from 2013 to 2021. We conducted trend analysis on NLP techniques and tools involved, and traceability efforts (applying NLP) across the software development life cycle (SDLC). Based on our study, we have identified the following key issues, barriers, and setbacks: syntax convention, configuration, translation, explainability, properties representation, tacit knowledge dependency, scalability, and data availability. Based on these, we consolidated the following open challenges: representation similarity across artifacts, the effectiveness of NLP for traceability, and achieving scalable, adaptive, and explainable models. To address these challenges, we recommend a holistic framework for NLP solutions to achieve effective traceability and efforts in achieving interoperability and explainability in NLP models for traceability.}
}
@article{ZHENG2024101568,
title = {Tree knowledge structure for better insight: Capturing biomedical science-technology knowledge linkage with MeSH},
journal = {Journal of Informetrics},
volume = {18},
number = {4},
pages = {101568},
year = {2024},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2024.101568},
url = {https://www.sciencedirect.com/science/article/pii/S1751157724000816},
author = {Zhejun Zheng and Yaxue Ma and Zhichao Ba and Lei Pei},
keywords = {Science-technology linkage, Ontology, Medical subject heading, Knowledge structure},
abstract = {Measuring the knowledge linkage between science and technology (S&T) is crucial for understanding the interactions between S&T and assisting decision-makers in strategizing research and development investments. Conventional analyses of S&T knowledge linkage have frequently overlooked the semantic structure of knowledge elements thereby introducing biases in the measurements. To address this issue, this study introduces a novel method predicated on the tree semantic structure, which quantifies the S&T linkage by considering the hierarchy and category of knowledge elements within an ontological framework. In this method, knowledge trees are constructed to represent the core knowledge of S&T literature, incorporating hierarchically organized MeSH descriptors. These knowledge trees are subsequently utilized to measure the knowledge linkage between S&T by integrating intra-branch knowledge similarity and inter-branch knowledge distribution. An empirical analysis was conducted on a substantial corpus of scientific publications and patents within the biomedicine sector. The findings predominantly revealed a stronger knowledge linkage between S&T in recent years, relative to the early 2000 s. It was also observed that patents are more inclined to include broader concepts in their titles and abstracts, in contract to the more specific concepts found in scientific publications. S&T literatures have increasingly focused on knowledge related to diseases, equipment, and health care. To verify the reliability of the proposed method, validation was performed with alternative measurements of knowledge linkage. In comparison to single-feature-based linkage measurements and network-based approaches, our proposed method demonstrates superior adaptability in capturing S&T linkage, especially when there is a marked disparity in the sample sizes of S&T literature. This study not only enriches the measurements of S&T knowledge linkage, but also furnishes empirical insights into the evolving patterns of S&T linkage within the biomedical domain.}
}
@article{KIRBY2019193,
title = {Decision-based behavior modeling of software-intensive systems},
journal = {Procedia Computer Science},
volume = {153},
pages = {193-201},
year = {2019},
note = {17th Annual Conference on Systems Engineering Research (CSER)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.070},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930729X},
author = {James Kirby},
keywords = {Software-intensive systems, Modeling, Decisions, Behavior, Ontology},
abstract = {This paper assumes a software-intensive system embedded in an environment of entities and their attributes. Behavior of the system, for which software is responsible, is its effect on selected attributes in the environment. A useful model would record relevant entities and attributes; distinguish attributes the system affects; record that effect with sufficient precision to be executed; organize it to maintain intellectual control and constrain the impact of change during development, sustainment, and assurance; and organize it to make efficient use of processing and communication resources.}
}
@article{LIU2025108482,
title = {SGTB: A graph representation learning model combining transformer and BERT for optimizing gene expression analysis in spatial transcriptomics data},
journal = {Computational Biology and Chemistry},
volume = {118},
pages = {108482},
year = {2025},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2025.108482},
url = {https://www.sciencedirect.com/science/article/pii/S1476927125001422},
author = {Farong Liu and Sheng Ren and Jie Li and Haoyang Lv and Fenghui Jiang and  {Bin Yu}},
keywords = {Spatial transcriptomics, Gene differential expression, Gene regulatory network construction, Graph convolutional networks, Transformer, BERT language models},
abstract = {In recent years, spatial transcriptomics (ST) has emerged as an innovative technology that enables the simultaneous acquisition of gene expression information and its spatial distribution at the single-cell or regional level, providing deeper insights into cellular interactions and tissue organization, this technology provides a more holistic view of tissue organization and intercellular dynamics. However, existing methods still face certain limitations in data representation capabilities, making it challenging to fully capture complex spatial dependencies and global features. To address this, this paper proposes an innovative spatial multi-scale graph convolutional network (SGTB) based on large language models, integrating graph convolutional networks (GCN), Transformer, and BERT language models to optimize the representation of spatial transcriptomics data. The Graph Convolutional Network (GCN) employs a multi-layer architecture to extract features from gene expression matrices. Through iterative aggregation of neighborhood information, it captures spatial dependencies among cells and gene co-expression patterns, thereby constructing hierarchical cell embeddings. Subsequently, the model integrates an attention mechanism to assign weights to critical features and leverages Transformer layers to model global relationships, refining the ability of learned representations to reflect variations in spatial patterns. Finally, the model incorporates the BERT language model, mapping cell embeddings into textual inputs to exploit its deep semantic representation capabilities for high-dimensional feature extraction. These features are then fused with the embeddings generated by the Transformer, further optimizing feature learning for spatial transcriptomics data. This approach holds significant application value in improving the accuracy of tasks such as cell type classification and gene regulatory network construction, providing a novel computational framework for deep mining of spatial multi-scale biological data.}
}
@article{REIMANN2018694,
title = {Methodology and model for predicting energy consumption in manufacturing at multiple scales},
journal = {Procedia Manufacturing},
volume = {21},
pages = {694-701},
year = {2018},
note = {15th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.02.173},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918302130},
author = {Jan Reimann and Ken Wenzel and Marko Friedemann and Matthias Putz},
keywords = {Energy efficiency, Predictive Model, Ontology},
abstract = {Certain fields of manufacturing, like casting, forming or cutting, may cause high energy load. Especially under the consideration of renewable energy sources it is beneficial to negotiate production schedules and consumption forecasts with the energy supplier. This would enable an optimized management of energy sources and infrastructure components on the supplier side, helping to reduce costs. Optimal and balanced expenses for production would be the consequence. The problem of power consumption prediction in manufacturing was subject of many studies in the past. Most of them either consider the physical modeling of processes at a very detailed level, or they introduce tailored prediction models for specific production processes. Thus, it is hard to apply their results to other uses cases in different scenarios. As a consequence, a generic methodology and model regarding power consumption prediction in manufacturing is required in order to cover the variety of processes, machines and materials. Furthermore, an approach must support flexible levels of granularity for predicting the energy consumption of manufacturing processes. On the one hand, a whole factory may be the object of investigation while, on the other hand, predictions for finer-grained levels, such as certain parts of a machine, are required to allow for specific optimizations. Our contribution is twofold. First, we propose a generic model for the specification of the power-consuming machine. A tree-based compositional approach supports arbitrary levels, depending on the structure of the machine, or external factors, such as company policies. This approach is highly extensible since the models are stored in ontologies. Second, we propose a methodology for static and dynamic modeling of power consumption for every structural level. Based on that model the prediction can be realized. In addition, we provide an example implementation and prediction for a continuous casting machine process.}
}
@article{ZHAI2022e12056,
title = {A systematic review on cross-culture, humor and empathy dimensions in conversational chatbots: the case of second language acquisition},
journal = {Heliyon},
volume = {8},
number = {12},
pages = {e12056},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e12056},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022033448},
author = {Chunpeng Zhai and Santoso Wibowo},
keywords = {Artificial intelligence, Chatbot, Second language learning, Culture, Empathy, Humor},
abstract = {The advancement of information and communication technologies has led to an increasing use of conversational chatbots in the learning and teaching sector, especially for the second language (L2) acquisition. In the field of second language acquisition, the use of AI chatbots has been explored, mainly studying pedagogical approaches. However, there is a limited study in the development of empathetic strategies for dealing with learners' emotional discomfort, the impact of humor and the consideration of learners' cultural backgrounds. Thus, this study reviews the existing studies on AI second language (L2) chatbots to investigate the development of empathetic strategies for enhancing learners' learning outcomes. To achieve the aim of this study, prior studies from 2012 and 2022 of several popular databases, including Web of Science, ProQuest, IEEE and ScienceDirect are collected and analyzed. This study found that three dimensions such as cultural, empathetic and humorous dimensions have a positive influence on the application of AI L2 chatbots for enhancing learners' learning outcomes. This study also found that the development of an AI chatbot in L2 education has plenty of room for improvement. Several recommendations are made for enhancing the use of AI L2 chatbots which include integrating cross-cultural empathetic responses in conversational L2 chatbots, identifying how learners perceive and react to the learning content, and investigating the effects of cross-culture humor on learners’ language proficiency.}
}
@article{DECARDINELSON2024108723,
title = {Generative AI and process systems engineering: The next frontier},
journal = {Computers & Chemical Engineering},
volume = {187},
pages = {108723},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108723},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424001418},
author = {Benjamin Decardi-Nelson and Abdulelah S. Alshehri and Akshay Ajagekar and Fengqi You},
keywords = {Generative AI, Process systems engineering, Large language models, Multiscale},
abstract = {This review article explores how emerging generative artificial intelligence (GenAI) models, such as large language models (LLMs), can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly foundation models (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could potentially advance PSE methodologies, providing insights and prospects for each area. Furthermore, the article identifies and discusses potential challenges in fully leveraging GenAI within PSE, including multiscale modeling, data requirements, evaluation metrics and benchmarks, and trust and safety, thereby deepening the discourse on effective GenAI integration into systems analysis, design, optimization, operations, monitoring, and control. This paper provides a guide for future research focused on the applications of emerging GenAI in PSE.}
}
@article{SINGH2025100630,
title = {BpGDB: A genomic resource database of Bunium persicum for genetics and breeding},
journal = {Journal of Applied Research on Medicinal and Aromatic Plants},
volume = {46},
pages = {100630},
year = {2025},
issn = {2214-7861},
doi = {https://doi.org/10.1016/j.jarmap.2025.100630},
url = {https://www.sciencedirect.com/science/article/pii/S2214786125000105},
author = {Akshay Singh and Nancy Sharma and Sangita Bansal and Rakesh Singh and G.P. Singh},
keywords = {, Apiaceae, Transcriptome, Simple sequence repeats (SSRs), Transcription factors (TFs), Gene ontology (GO)},
abstract = {Bunium persicum or black cumin, holds significant medicinal and culinary importance within the Apiaceae family. Its seeds are prized in Persian and Indian cuisines for flavour-enhancing and preservative properties. Traditional medicine employs it for treating digestive disorders, urinary ailments, diabetes, obesity, and lactation enhancement. Despite its economic value, genomic resources for this plant species are scarce. To address this gap and foster genetic research, we introduce "BpGDB," the first comprehensive genomic resources database for B. persicum. BpGDB features 5939 EST-SSR markers derived from 88,309 non-redundant transcripts assembled de novo. Additionally, it incorporates 37,232 genomic SSR markers obtained from recent de novo genome sequencing efforts of our laboratory. It also includes B. persicum transcripts annotated using NCBI-NR and gene ontology (GO) databases, encompassing key gene families such as cytochrome P450, protein kinases, heat shock proteins (HSPs) etc. and biosynthetic enzymes for bioactive compounds of medicinal value. Moreover, BpGDB catalogues 4617 transcription factors (TFs) categorized into 57 families, along with their coding sequence (CDS), translated protein information. This database is user friendly and interactive with advanced search functionalities and multiple search options including SSR search, TF search, GO search etc. Users can directly access the desired information/data by either clicking on the interactive pie charts or through the menu given at the left side bar available on the homepage of database. It is aimed to facilitate researchers globally in understanding genetic traits related to therapeutic and culinary properties, while enhancing breeding strategies to improve crop productivity that may include marker-assisted selection, linkage mapping, genetic diversity studies, and population analyses in the species. Furthermore, it will open avenues for comparative genomics study within the Apiaceae family, facilitating the exploration and harnessing of the therapeutic potential of B. persicum.}
}
@article{TOZZI202212,
title = {Colonizing the rains: Disentangling more-than-human technopolitics of drought protection in the archive},
journal = {Geoforum},
volume = {135},
pages = {12-24},
year = {2022},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2022.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016718522001464},
author = {Arianna Tozzi and Stefan Bouzarovski and Caitlin Henry},
keywords = {More-than-human, Technopolitics, Rainfed agriculture, Ontologies, Water, India},
abstract = {Preoccupations for the widening gap between irrigated and rainfed areas are central to debates addressing the agrarian crisis in semi-arid India. Yet policies are driven by a catch-up mentality that points towards an irrigated model of agriculture, demarcating rainfed areas as spaces of rural marginality. To unpack the historical causes behind this ‘irrigation at all costs’ mindset, the paper traces how the rainfed/irrigated gap became constituted through policies of drought-protection during British colonial time. Focussing on the Bombay Deccan after the establishment of the British Raj, it frames drought-protection as a more-than-human technopolitics to explore the performative power of technopolitical practices to bring water worlds into being. Through a critical reading of the colonial archive, we trace the ontological work of drought-protection as a practice that rearranged existing human-water relations to materialize a reality of water ‘as irrigation’. Grounded on linear and predictable flows, this irrigated ontology divided the landscape along an irrigated-as-protected and rainfed-as-unprotected logic. Encountering a world that followed geographies of water ‘as precipitation’ however created sites of contestation blurring the partition envisioned by engineering plans. Rather than the imposition of hydrological power from above, colonizing the rains represents a contested project whereby certain water worlds became present and real while others discarded and less real. Contributing to scholarship shaking the ontological ground underneath water management regimes, we suggest a reflexive turn for these practices, as they must confront their power to materialize (water) realities and the possibility to enact a decolonial technopolitics beyond water’s liquid form.}
}
@article{YVARS2022269,
title = {Towards a correct by construction design of complex systems: The MBSS approach},
journal = {Procedia CIRP},
volume = {109},
pages = {269-274},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.248},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122006977},
author = {Pierre-Alain Yvars and Laurent Zimmer},
keywords = {Model-based approach, System Synthesis, constraint programming, sub-definite system},
abstract = {We present in this paper the Model Based System Synthesis (MBSS) approach for the design of complex systems that are correct by construction. Where the usual Model Based System Engineering (MBSE) approach offers formalisms and tools to represent a candidate system, to analyze it, to simulate it and even to optimize it, MBSS proposes to represent the global design problem using a problem representation language and then to solve it by using adapted synthesis tools producing one or several solutions necessarily satisfying the expressed requirements. The two approaches are therefore complementary; the MBSS being more adapted to the preliminary design and system integration stages. After presenting the different categories of problems encountered in system design (sizing, configuration, allocation, architecture generation), MBSS and MBSE will be positioned in relation to each other. The main concepts of MBSS will be detailed in order to understand the specific representation needs of the approach. The structural and behavioral notions related to the sub-definite systems will be explained as well as the links to be established with the functional and non-functional requirements. The approach is illustrated using the DEPS design problem specification language and the DEPS Studio modeling and solving tool on a system design case study. The DEPS language combines structural modeling features specific to object-oriented principles and ontology definition capabilities for engineers with problem specification features from constraint programming. DEPS Studio is an integrated modeling and solving environment designed to model and resolve system synthesis problems. It allows the engineer to edit, compile, debug and solve problems expressed in DEPS. It integrates a mixed constraint programming solver. The approach can be applied on physical systems, software intensive or mixed systems (embedded or cyber-physical).}
}
@article{LIU2023109538,
title = {MAYA: Exploring multiform attributes of node to align YANG data models},
journal = {Computer Networks},
volume = {222},
pages = {109538},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109538},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622005722},
author = {Yongbo Liu and Yongqiang Dong and Jun Shen and Chong Feng},
keywords = {NETCONF, YANG model, Network management, Schema matching, Ontology alignment, Semantic similarity},
abstract = {Nowadays Network Configuration Protocol (NETCONF) has become an essential part of network management for its flexibility and scalability. Vendors intend to use NETCONF to replace Command-Line Interface (CLI) and Simple Network Management Protocol (SNMP) to manage devices. Yet Another Next Generation (YANG) models are tailored to model NETCONF protocol messages. However, there exists heterogeneity issue with YANG models as a consequence of vendors proposing proprietary YANG models which differ from each other in structure or content. Thus, managing network devices from different vendors requires expert knowledge and plenty of resources. In this paper, we present MAYA, a solution to automatically accomplish alignment of YANG models from different vendors or organizations by exploring multiform node attributes. In MAYA, different semantic similarity techniques are used to measure distance between different attributes, such as name, description and type, in nodes from various YANG models. A customized SMP based matching algorithm for YANG models alignment is proposed to generate mapping relations between models based on the semantic similarity. The real cases analysis and experiments show that MAYA is able to meet the demands in production on the problem of YANG model alignment.}
}
@article{YANG2025131494,
title = {TemPrompt: Multi-task prompt learning for temporal relation extraction in RAG-based crowdsourcing systems},
journal = {Neurocomputing},
pages = {131494},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.131494},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225021666},
author = {Jing Yang and Yu Zhao and Linyao Yang and Xiao Wang and Long Chen and Fei-Yue Wang},
keywords = {Temporal relation extraction, Temporal event reasoning, Contrastive learning, Prompt tuning, Retrieval-augmented generation},
abstract = {Temporal relation extraction (TRE) aims to grasp the evolution of events or actions, and thus shape the workflow of associated tasks, so it is recognized as a pivotal technology for facilitating the rational scheduling and efficient execution of crowdsourcing tasks. However, existing methods still struggle with limited and unevenly distributed annotated data. Inspired by the abundant global knowledge stored within pre-trained language models (PLMs), some studies have explored using prompts to guide PLMs in completing TRE, but their improvements are still unsatisfactory, as the model treats all the tokens equally, resulting in a limited understanding of temporal order. In this paper, we propose a multi-task prompt learning framework for TRE (TemPrompt), incorporating prompt tuning and contrastive learning to tackle these issues. In the framework, we design temporal event reasoning in the form of masked language modeling as auxiliary tasks to enable the PLM to distinguish tokens essential for temporal reasoning from those serving general contextual purposes, thereby fostering the model’s comprehension of temporal knowledge. Additionally, to elicit more effective prompts for PLMs, we introduce a task-oriented prompt construction approach that thoroughly takes the myriad factors of TRE into consideration for the automatic generation of high-quality and easy-to-interpret prompts. Contrastive learning is employed to further mitigate data issues by extracting more distinctive sample representations. Experimental results demonstrate that TemPrompt outperforms all baseline methods across most metrics and exhibits strong generalization to unseen events. Two case studies—one on designing and manufacturing printed circuit boards and the other on developing defect detection systems—are provided to validate its feasibility and effectiveness in crowdsourcing scenarios.}
}
@incollection{PATNAIKUNI202187,
title = {Chapter 7 - Probabilistic, syntactic, and semantic reasoning using MEBN, OWL, and PCFG in healthcare},
editor = {Sarika Jain and Vishal Jain and Valentina Emilia Balas},
booktitle = {Web Semantics},
publisher = {Academic Press},
pages = {87-94},
year = {2021},
isbn = {978-0-12-822468-7},
doi = {https://doi.org/10.1016/B978-0-12-822468-7.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128224687000092},
author = {Shrinivasan Patnaikuni and Sachin R. Gengaje},
keywords = {Semantic web, ontology, MEBN, PCFG, syntactico-semantic reasoning},
abstract = {With the advent of state of the art of artificial intelligence technologies like deep learning, several digital healthcare records are yet to be fully explored. Several data processing and analytics methods treat the data as numbers and strings and attempt to make a sense out of it using data mining and machine learning techniques. The current data processing and data analytics methods are not new to handling electronic healthcare records, yet they suffer from a typical problem of missing out the semantic information associated with the healthcare records which humans excel at. Semantic knowledge bases in the form of ontologies and the insightful syntactic patterns in healthcare data, especially clinical text documents, could be of great help for several medical research projects aiming at developing intelligent semantic reasoning methods for supporting healthcare decision support systems. The chapter thoroughly introduces the key concepts and terminologies in syntactic and semantic reasoning using Multi Entity Bayesian Networks (MEBN), Web Ontology Language (OWL), and probabilistic context-free grammars (PCFG) with a special focus on applications to the healthcare domain.}
}
@article{NUNEZMARCOS2023118993,
title = {A survey on Sign Language machine translation},
journal = {Expert Systems with Applications},
volume = {213},
pages = {118993},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118993},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422020115},
author = {Adrián Núñez-Marcos and Olatz Perez-de-Viñaspre and Gorka Labaka},
keywords = {Machine learning, Sign language translation, Sign Languages, Survey},
abstract = {Sign Languages (SLs) are employed by deaf and hard-of-hearing (DHH) people to communicate on a daily basis. However, the communication with hearing people still faces some barriers, mainly because of the scarce knowledge about SLs among hearing people. Hence, tools to allow the communication between users of either sign or spoken languages must be encouraged. A stepping stone in this direction is the research of the sign language translation (SLT) task, which aims to produce a spoken language translation of a sign language video or vice versa. By implementing these types of translators in portable devices, we will make considerable progress towards a barrier-free communication between DHH and hearing people. That is why, in this work, we focus on reviewing the literature on SLT and provide the necessary background about SLs. Besides, we summarise the available datasets and the results found in the literature for one of the most used datasets, the RWTH-PHOENIX-2014T. Moreover, the survey lists the challenges that need to be tackled within the SLT research and also for the adoption of SLT technologies, and proposes future research lines.}
}
@article{BUSCALDI2024103583,
title = {Citation prediction by leveraging transformers and natural language processing heuristics},
journal = {Information Processing & Management},
volume = {61},
number = {1},
pages = {103583},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103583},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323003205},
author = {Davide Buscaldi and Danilo Dessí and Enrico Motta and Marco Murgia and Francesco Osborne and Diego {Reforgiato Recupero}},
keywords = {Citation prediction, Transformers architecture, Mask-filling, Named entity recognition, BERT},
abstract = {In scientific papers, it is common practice to cite other articles to substantiate claims, provide evidence for factual assertions, reference limitations, and research gaps, and fulfill various other purposes. When authors include a citation in a given sentence, there are two considerations they need to take into account: (i) where in the sentence to place the citation and (ii) which citation to choose to support the underlying claim. In this paper, we focus on the first task as it allows multiple potential approaches that rely on the researcher’s individual style and the specific norms and conventions of the relevant scientific community. We propose two automatic methodologies that leverage transformers architecture for either solving a Mask-Filling problem or a Named Entity Recognition problem. On top of the results of the proposed methodologies, we apply ad-hoc Natural Language Processing heuristics to further improve their outcome. We also introduce s2orc-9K, an open dataset for fine-tuning models on this task. A formal evaluation demonstrates that the generative approach significantly outperforms five alternative methods when fine-tuned on the novel dataset. Furthermore, this model’s results show no statistically significant deviation from the outputs of three senior researchers.}
}
@article{GUIZANI2021843,
title = {An approach for selecting a business process modeling language that best meets the requirements of a modeler},
journal = {Procedia Computer Science},
volume = {181},
pages = {843-851},
year = {2021},
note = {CENTERIS 2020 - International Conference on ENTERprise Information Systems / ProjMAN 2020 - International Conference on Project MANagement / HCist 2020 - International Conference on Health and Social Care Information Systems and Technologies 2020, CENTERIS/ProjMAN/HCist 2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.01.238},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921002817},
author = {Khouloud Guizani and Sonia Ayachi Ghannouchi},
keywords = {Business process modelling, modelling languages, multicriteria decision, AHP method},
abstract = {The work presented in this paper consists in an approach for selecting a business process modeling language using multi-criteria decision, more precisely the AHP (Analytic Hierarchy Process) method. The selected business process modeling language should best meet the requirements and objectives of the modeler according to the process to be modeled. The comparative framework established for our approach consists in constructing a decision problem solved using the AHP multi-criteria analysis method. The prioritized criterion describes the modeler requirements and the alternatives represent the set of business process modeling languages under study. The comparative framework is based on seven criteria: Expressiveness, Flexibility, Formality, Readability, Support Tools, Usability and Ease of Learning. Our approach is, essentially, justified by the fact that none of the existing languages fully supports all the criteria. Multi-criteria decision support analysis is therefore the most appropriate approach for their comparison.}
}
@article{HE2021267,
title = {From context-aware to knowledge-aware: Boosting OOV tokens recognition in slot tagging with background knowledge},
journal = {Neurocomputing},
volume = {445},
pages = {267-275},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.01.134},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221002575},
author = {Keqing He and Yuanmeng Yan and Weiran Xu},
keywords = {Slot tagging, Contextual representation, Background knowledge, Knowledge Integration, Multi-level graph attention},
abstract = {Neural-based context-aware models for slot tagging tasks in language understanding have achieved state-of-the-art performance, especially deep contextualized models, such as ELMo, BERT. However, the presence of out-of-vocab (OOV) words significantly degrades the performance of neural-based models, especially in a few-shot scenario. In this paper, we propose a novel knowledge-aware slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge. Besides, we use multi-level graph attention to explicitly reason via lexical relations. We aim to leverage both linguistic regularities covered by deep language models (LM) and high-quality background knowledge derived from curated knowledge bases (KB). Consequently, our model could infer rare and unseen words in the test dataset by incorporating contextual semantics learned from the training dataset and lexical relations from ontology. The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets. We also show through detailed analysis that incorporating background knowledge effectively alleviates issues of data scarcity.}
}
@article{MORETTI2022229,
title = {Built environment data modelling: a review of current approaches and standards supporting Asset Management},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {19},
pages = {229-234},
year = {2022},
note = {5th IFAC Workshop on Advanced Maintenance Engineering, Services and Technologies AMEST 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.212},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322014306},
author = {Nicola Moretti and Xiang Xie and Jorge Merino Garcia and Janet Chang and Ajith Kumar Parlikad},
keywords = {Data Modelling, Information Management, Asset Management, AECO, built environment, digital twin},
abstract = {Information Management is crucial in the Asset Management domain. Well-structured information management processes allow to access the needed data, in the right format, and enables the development of cross-domain Asset Management services. Several studies can be found in literature, on the capabilities and applications of digital tools in Architecture Constructions and Operations (AECO), demonstrating how digitisation has changed the traditional processes. Moreover, many data modelling approaches and standards can be used to support digital Asset Management applications. Due to the advancement of Digital Twin related research, the interest for the ontological and data modelling approaches have increased in recent years. This article aims at organising the body of knowledge on data modelling in AECO, through a critical review of existing approaches and standards. The literature is studied and the main trends are highlighted. The most relevant articles are selected and a contents analysis is carried out. The study shows that digital Asset Management applications require interdisciplinarity and data access across different domain; two main approaches for the development of intermediate data models can be identified in literature and a unique data model able to represent multiple scales and domains cannot be found. Thus, some future research works are proposed as a conclusion of the literature review.}
}
@incollection{COBB2024,
title = {Constructivism in Language Learning and Applied Linguistics},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00020-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032395504100020X},
author = {Tom Cobb},
keywords = {Constructivist/ism, Language learning, Language acquisition, Usage based, Sociocultural, Language acquisition device (LAD), Chomsky, Krashen},
abstract = {Constructivism as it applies to learning and teaching is often grouped with student-centered learning, discovery learning, project-based learning, and other “progressive” approaches to education. However, while constructivism is compatible with any of these, it is not the same, but rather a learning theory that may or may not underpin any of them. In this article, constructivism will be presented as both a learning theory, a theory about how learning happens, and an epistemology, a theory of what can be learned or known. We begin with a brief account of its appearance in philosophy and its role in recent general education before looking in more depth at its appropriateness to accounts of language learning/acquisition. The argument is that constructivism is currently our effective though unacknowledged learning theory, the only one that fits with the facts we know about the world and its make-up, and that acknowledging this is of particular importance in applied linguistics and language acquisition research.}
}
@article{ALSARAYREH2023215,
title = {Inverse design and AI/Deep generative networks in food design: A comprehensive review},
journal = {Trends in Food Science & Technology},
volume = {138},
pages = {215-228},
year = {2023},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2023.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0924224423001693},
author = {Mahmoud Al-Sarayreh and Mariza {Gomes Reis} and Alistair Carr and Marlon Martins dos Reis},
keywords = {Food design, Food formulation, Inverse design, Deep generative networks, Data-driven approaches, Deep learning},
abstract = {Background
Food material science has evolved to support the development of food products by connecting food structure, sensory, nutrition, food processing, and digestion with impact in consumers. However, food design has not evolved to deal with this increased complexity of food systems. And the ability to understand, capture the attention, and transform consumer demands into the chemical and physical attributes of the final product remains one of the biggest challenges in the food industry. As a result, new ways to support food design are necessary.
Scope and approach
This review describes the state-of-the-art of applications in food design utilizing artificial intelligence (AI)/Deep Generative Networks, including available resources and emerging capabilities and its relationship with the concept of inverse design.
Key findings and conclusions
Food design and formulation involve complex processes and many design parameters need to be considered while developing data-driven approaches. Most approaches identified are based on the association among ingredients, but less focus has been given to functional properties. Representation of data remains a real challenge and a very important research gap toward achieving a real and applicable concept of digital food design. Overall methods based on deep learning and natural language processing are the most utilized. Deep generative-based approaches have been rarely described and remain a critical research area.}
}
@article{SEINEN2025,
title = {Using Structured Codes and Free-Text Notes to Measure Information Complementarity in Electronic Health Records: Feasibility and Validation Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/66910},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125002341},
author = {Tom M Seinen and Jan A Kors and Erik M {van Mulligen} and Peter R Rijnbeek},
keywords = {natural language processing, named entity recognition, clinical concept extraction, machine learning, electronic health records, EHR, word embeddings, clinical concept similarity, text mining, code, free-text, information, electronic record, data, patient records, framework, structured data, unstructured data},
abstract = {Background
Electronic health records (EHRs) consist of both structured data (eg, diagnostic codes) and unstructured data (eg, clinical notes). It is commonly believed that unstructured clinical narratives provide more comprehensive information. However, this assumption lacks large-scale validation and direct validation methods.
Objective
This study aims to quantitatively compare the information in structured and unstructured EHR data and directly validate whether unstructured data offers more extensive information across a patient population.
Methods
We analyzed both structured and unstructured data from patient records and visits in a large Dutch primary care EHR database between January 2021 and January 2024. Clinical concepts were identified from free-text notes using an extraction framework tailored for Dutch and compared with concepts from structured data. Concept embeddings were generated to measure semantic similarity between structured and extracted concepts through cosine similarity. A similarity threshold was systematically determined via annotated matches and minimized weighted Gini impurity. We then quantified the concept overlap between structured and unstructured data across various concept domains and patient populations.
Results
In a population of 1.8 million patients, only 13% of extracted concepts from patient records and 7% from individual visits had similar structured counterparts. Conversely, 42% of structured concepts in records and 25% in visits had similar matches in unstructured data. Condition concepts had the highest overlap, followed by measurements and drug concepts. Subpopulation visits, such as those with chronic conditions or psychological disorders, showed different proportions of data overlap, indicating varied reliance on structured versus unstructured data across clinical contexts.
Conclusions
Our study demonstrates the feasibility of quantifying the information difference between structured and unstructured data, showing that the unstructured data provides important additional information in the studied database and populations. The annotated concept matches are made publicly available for the clinical natural language processing community. Despite some limitations, our proposed methodology proves versatile, and its application can lead to more robust and insightful observational clinical research.}
}
@article{SANTIAGO2025101452,
title = {Anti-colonial methodology in school psychology research with Latine communities},
journal = {Journal of School Psychology},
volume = {110},
pages = {101452},
year = {2025},
issn = {0022-4405},
doi = {https://doi.org/10.1016/j.jsp.2025.101452},
url = {https://www.sciencedirect.com/science/article/pii/S0022440525000251},
author = {Rachel T. Santiago and David Aguayo},
keywords = {Latine, Culturally responsive practices, Methodology, School psychology, Anti-colonial research},
abstract = {Latine youth are one of the fastest-growing youth populations in the U.S., with tremendous diversity in culture and language. Despite the importance of conducting culturally relevant, responsible research with Latine youth and families, school psychology research often does not include theories or methods aligned with Latine cultures. The purpose of the present paper is to discuss key theories relevant to research with Latine communities and provide actionable recommendations for researchers. Recommendations are discussed within five domains: rethinking knowledge production, distinguishing and measuring race and ethnicity, oral storytelling and testimonio, the role of community, and the role of researchers in the process. Implications for practice, limitations, and future research directions are discussed.}
}
@article{SWORNA2023103761,
title = {NLP methods in host-based intrusion detection systems: A systematic review and future directions},
journal = {Journal of Network and Computer Applications},
volume = {220},
pages = {103761},
year = {2023},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2023.103761},
url = {https://www.sciencedirect.com/science/article/pii/S1084804523001807},
author = {Zarrin Tasnim Sworna and Zahra Mousavi and Muhammad Ali Babar},
keywords = {Natural language processing, Host-based intrusion detection, Cyber security, Anomaly detection},
abstract = {Host-based Intrusion Detection System (HIDS) is an effective last line of defense for defending against cyber security attacks after perimeter defenses (e.g., Network-based Intrusion Detection System and Firewall) have failed or been bypassed. HIDS is widely adopted in the industry as HIDS is ranked among the top two most used security tools by Security Operation Centers (SOC) of organizations. Although effective and efficient HIDS is highly desirable for industrial organizations, the evolution of increasingly complex attack patterns causes several challenges resulting in performance degradation of HIDS (e.g., high false alert rate creating alert fatigue for SOC staff). Since Natural Language Processing (NLP) methods are better suited for identifying complex attack patterns, an increasing number of HIDS are leveraging the advances in NLP that have shown effective and efficient performance in precisely detecting low footprint, zero-day attacks and predicting an attacker’s next steps. This active research trend of using NLP in HIDS demands a synthesized and comprehensive body of knowledge of NLP-based HIDS. Despite the drastically growing adoption of NLP in HIDS development, there has been relatively little effort allocated to systematically analyze and synthesize the available peer review literature to understand how NLP is used in HIDS development. The lack of a synthesized and comprehensive body of knowledge on such an important topic motivated us to conduct a Systematic Literature Review (SLR) of the papers on the end-to-end pipeline of the use of NLP in HIDS development. For the end-to-end NLP-based HIDS development pipeline, we identify, taxonomically categorize and systematically compare the state-of-the-art of NLP methods usage in HIDS, attacks detected by these NLP methods, datasets and evaluation metrics which are used to evaluate the NLP-based HIDS. We highlight the relevant prevalent practices, considerations, advantages and limitations to support the HIDS developers. We also outline the future research directions for the NLP-based HIDS development.}
}
@article{PICCIALLI2025128404,
title = {AgentAI: A comprehensive survey on autonomous agents in distributed AI for industry 4.0},
journal = {Expert Systems with Applications},
volume = {291},
pages = {128404},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128404},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425020238},
author = {Francesco Piccialli and Diletta Chiaro and Sundas Sarwar and Donato Cerciello and Pian Qi and Valeria Mele},
keywords = {AgentAI, AgenticAI, Industry 4.0, Distributed artificial intelligence, Autonomous decision-making},
abstract = {AgentAI represents a transformative approach within distributed Artificial Intelligence (AI) in which autonomous agents work either individually or collaboratively in decentralized environments to address challenging problems. AgentAI enhances scalability, robustness, and flexibility by utilizing advanced communication, learning, and decision-making capabilities, making it integral to diverse applications in Industry 4.0. The ability of AI systems to interpret sensory data in open-world environments has seen significant advancements in recent years. This progress emphasizes the need to move beyond reductionist approaches and embrace more embodied and cohesive systems, which integrate foundational models into agent-driven actions. Existing surveys often focus on isolated domains or specific autonomy levels, lacking a cohesive analysis that spans the full spectrum of AgentAI development in Industry 4.0. This survey explicitly fills this gap by introducing a multi-domain taxonomy and by systematically analyzing both non-autonomous and fully autonomous AgentAI systems, offering a comprehensive synthesis not previously available in the literature. Additionally, the paper extends the discussion to Industry 5.0 and 6.0, exploring the evolution of AgentAI from automation to collaboration and, ultimately, to fully autonomous systems. This comprehensive analysis highlights the potential of AgentAI in driving industries toward a more efficient, sustainable, and adaptable future.}
}
@article{LONGO2021104269,
title = {Caspar: Towards decision making helpers agents for IoT, based on natural language and first order logic reasoning},
journal = {Engineering Applications of Artificial Intelligence},
volume = {104},
pages = {104269},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104269},
url = {https://www.sciencedirect.com/science/article/pii/S0952197621001160},
author = {Carmelo Fabio Longo and Francesco Longo and Corrado Santoro},
keywords = {Internet of Things, Computational linguistic, Artificial Intelligence, First order logic, Cognitive architectures, Meta-reasoning},
abstract = {In the last decade, the market of Internet of Things has become quite disruptive, together with commercial clouds providing connection between every sort of devices and the global network, supported by vocal assistants. On the other hands, such commercial products are limited to work on limited domains, although easily scalable, without aspiring to higher level of reasoning in the field of Decisions Making. In this work, we show a way towards the design of an architecture for building cognitive agents leveraging Natural Language Processing. Such agents will be not based on clouds and do not require any semantic training, plus they will be able of deduction on facts and rules in First Order Logic inferred directly from Natural Language. After the description of the architecture and its underlying components, a case-study is provided to show the effectiveness in cases of direct commands and routines, subordinated also by a Meta-Reasoning in a conceptual space, parsing the utterances with promising real-time performances.}
}
@article{SHENG2024303,
title = {Network pharmacology and experimental validation to reveal the pharmacological mechanisms of Qizhu prescription for treating breast cancer},
journal = {Journal of Traditional Chinese Medical Sciences},
volume = {11},
number = {3},
pages = {303-315},
year = {2024},
issn = {2095-7548},
doi = {https://doi.org/10.1016/j.jtcms.2024.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S2095754824000413},
author = {Jiayu Sheng and Junyi Cheng and Wenjie Chu and Mengting Dong and Ke Jiang},
keywords = {Traditional Chinese medicine, Qizhu prescription, Breast cancer, Quercetin, Network pharmacology},
abstract = {Objective
To investigate the mechanism underlying the effects exerted by the Qizhu prescription (QZP) in breast cancer (BC), and the respective targets.
Methods
Expression data from the ArrayExpress and The Cancer Genome Atlas (TCGA) were used to identify differentially expressed genes (DEGs) in BC. Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses were performed on the DEGs to identify genes involved in protein–protein interactions. Molecular docking was used to explore the dynamic relationship between active molecules and targets. Cell function experiments and animal studies were conducted to evaluate the effects of hub genes and active QZP compounds on BC cell behavior.
Results
Among the 25 evaluated BC-related targets of QZP, matrix metalloproteinase-1 (MMP1) and epidermal growth factor receptor (EGFR) exhibited the highest degrees of dysregulation. GO and KEGG enrichment analyses revealed that the anti-BC targets of QZP primarily affected drug responses and pathways in cancer cells. Molecular docking analysis suggested potential interactions between EGFR and quercetin/luteolin, as well as between MMP1 and luteolin/kaempferol/quercetin. Quercetin significantly reduced BC cell proliferation, migration, invasion, and tumor development in vivo. Treatment of BC cells with quercetin decreased the expression or activation of several associated proteins.
Conclusion
The findings of our study provide new insights into the therapeutic potential of traditional Chinese medicine against BC, with particular reference to QZP.}
}
@article{TU2024965,
title = {Architecture for data-centric and semantic-enhanced industrial metaverse: Bridging physical factories and virtual landscape},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {965-979},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.05.016},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524001134},
author = {Xinyi Tu and Riku Ala-Laurinaho and Chao Yang and Juuso Autiosalo and Kari Tammi},
keywords = {Industrial metaverse, Virtual–physical continuum, Digital twins, Extended reality, Architecture, Industry 5.0},
abstract = {The metaverse paradigm has recently captured increasing scholarly and industrial attention, particularly within the scope of human-centric Industry 5.0. In this context, the metaverse promises a transformative confluence of the physical and digital realms, offering unparalleled avenues for human augmentation in industrial applications. Yet, while several conceptual metaverse architectures and illustrative case studies have emerged, they scarcely delve deep into the nuanced practice of cultivating the industrial metaverse for factory-scale applications. Addressing this research gap, this work introduces a novel architecture for a data-centric and semantic-enhanced industrial metaverse. The architecture intricately weaves the physical factory domain with the metaverse, fortified by a suite of ten modules, facilitating data flow and knowledge synchronization with the integration of digital twins and semantic models. The practical application and relevance of this architecture are further accentuated through a case study focused on in-plant material flow tracking. Emerging results underline that our architecture encapsulates the essential components for constructing a factory-scale industrial metaverse. Future research will be geared towards a comprehensive validation of the proposed metaverse architecture, culminating in tangible implementations across diverse industrial contexts.}
}
@article{TALLERAS2018894,
title = {User conceptualizations of derivative relationships in the bibliographic universe},
journal = {Journal of Documentation},
volume = {74},
number = {4},
pages = {894-916},
year = {2018},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-10-2017-0139},
url = {https://www.sciencedirect.com/science/article/pii/S0022041818000619},
author = {Kim Tallerås and Jørn Helge B. Dahl and Nils Pharo},
keywords = {User studies, Cataloguing, Ontologies, Metadata, Linked data, FRBR, Bibliographic systems, Information modelling, Mental models, Conceptualizations},
abstract = {Purpose
Considerable effort is devoted to developing new models for organizing bibliographic metadata. However, such models have been repeatedly criticized for their lack of proper user testing. The purpose of this paper is to present a study on how non-experts in bibliographic systems map the bibliographic universe and, in particular, how they conceptualize relationships between independent but strongly related entities.
Design/methodology/approach
The study is based on an open concept-mapping task performed to externalize the conceptualizations of 98 novice students. The conceptualizations of the resulting concept maps are identified and analyzed statistically.
Findings
The study shows that the participants’ conceptualizations have great variety, differing in detail and granularity. These conceptualizations can be categorized into two main groups according to derivative relationships: those that apply a single-entity model directly relating document entities and those (the majority) that apply a multi-entity model relating documents through a high-level collocating node. These high-level nodes seem to be most adequately interpreted either as superwork devices collocating documents belonging to the same bibliographic family or as devices collocating documents belonging to a shared fictional world.
Originality/value
The findings can guide the work to develop bibliographic standards. Based on the diversity of the conceptualizations, the findings also emphasize the need for more user testing of both conceptual models and the bibliographic end-user systems implementing those models.}
}
@incollection{ZHENG2025360,
title = {Machine Learning in Bioinformatics},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {360-371},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00166-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001664},
author = {Huiru Zheng and Jyotsna Talreja Wassan and Haiying Wang},
keywords = {Algorithms, Bioinformatics, Deep learning, Generative AI, Genomics, Machine learning, Natural language processing, Proteomics},
abstract = {The unprecedented growth in scale and type of biological data has attracted the use of machine learning in bioinformatics to build informative models for understanding the underlying biological processes. In this encyclopedia chapter, we aim to provide readers with a general introduction to a few key machine learning models useful in bioinformatics, including the most recently developed techniques of deep neural networks. The chapter discusses how different machine-learning models may be suited to varied biological data. Furthermore, applications and recommendations of machine learning in the field of bioinformatics are highlighted in the chapter.}
}
@article{NISSEN20245017,
title = {Modeling and Predicting Welfare of Farmed Atlantic Salmon: Integrative Domain Representation and Reasoning Strategies},
journal = {Procedia Computer Science},
volume = {246},
pages = {5017-5026},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.574},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026243},
author = {Oscar Nissen and Aya Saad},
keywords = {Knowledge representation, graph database, reasoning strategies, Robust AI, fish welfare assessment, aquaculture},
abstract = {In the rapidly expanding field of aquaculture, ensuring the welfare of fish is essential for sustainability and productivity. Recognizing how environmental conditions and operational practices impact fish health, there is a need for robust methodologies that can predict and assess fish welfare effectively. This paper introduces a systematic modeling approach designed to advance fish welfare assessment by integrating diverse data sources including historical, environmental, operational data, and time-series analysis into a cohesive analytical framework. The proposed model focuses on developing and implementing inference rules that extract meaningful insights from heterogeneous datasets collected from multiple sources, such as direct observations, sensor outputs, and operational logs. This integration facilitates the use of machine learning techniques to assess and predict the welfare of fish groups. Time-series data provide a baseline for evaluating current conditions, while environmental and operational data contribute to realtime welfare assessments. In addition, the model features the construction of a fish welfare journal that significantly enhances its reasoning capabilities by providing deeper insights into the welfare status of fish over time. The utility of the model is demonstrated through its ability to predict and forecast welfare scores based on changes in environmental and operational conditions, which is vital for effective decision making, planning, and scheduling in fish farming operations. We showcase the practical applications of our model with a numerical example that illustrates its use in a real-world scenario. This example highlights how integrated data and inference methodologies can be employed to deduce welfare scores and make informed decisions about fish farming practices. Taking this example further, we train a predictive model to forecast welfare scores, illustrating the benefits of the proposed model, which not only dynamically calculates welfare scores, but also integrates time-series historical data to assess the performance of fish farming operations. Overall, this work not only deepens our understanding of fish welfare dynamics but also helps establish a more refined and robust welfare assessment protocol, extending the predictive capabilities into future operational planning and management.}
}
@article{DEIMAZAR2023105246,
title = {Machine learning models to detect and predict patient safety events using electronic health records: A systematic review},
journal = {International Journal of Medical Informatics},
volume = {180},
pages = {105246},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105246},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623002642},
author = {Ghasem Deimazar and Abbas Sheikhtaheri},
keywords = {Artificial intelligence, Machine learning, Deep learning, Natural language processing (NLP), Patient safety, Electronic health record (EHR), Adverse drug event, Adverse drug reaction},
abstract = {Introduction
Identifying patient safety events using electronic health records (EHRs) and automated machine learning-based detection methods can help improve the efficiency and quality of healthcare service provision.
Objective
This study aimed to systematically review machine learning-based methods and techniques, as well as their results for patient safety event management using EHRs.
Methods
We reviewed the studies that focused on machine learning techniques, including automatic prediction and detection of patient safety events and medical errors through EHR analysis to manage patient safety events. The data were collected by searching Scopus, PubMed (Medline), Web of Science, EMBASE, and IEEE Xplore databases.
Results
After screening, 41 papers were reviewed. Support vector machine (SVM), random forest, conditional random field (CRF), and bidirectional long short-term memory with conditional random field (BiLSTM-CRF) algorithms were mostly applied to predict, identify, and classify patient safety events using EHRs; however, they had different performances. BiLSTM-CRF was employed in most of the studies to extract and identify concepts, e.g., adverse drug events (ADEs) and adverse drug reactions (ADRs), as well as relationships between drug and severity, drug and ADEs, drug and ADRs. Recurrent neural networks (RNN) and BiLSTM-CRF had the best results in detecting ADEs compared to other patient safety events. Linear classifiers and Naive Bayes (NB) had the highest performance for ADR detection. Logistic regression had the best results in detecting surgical site infections. According to the findings, the quality of articles has non-significantly improved in recent years, but they had low average scores.
Conclusions
Machine learning can be useful in automatic detection and prediction of patient safety events. However, most of these algorithms have not yet been externally validated or prospectively tested. Therefore, further studies are required to improve the performance of these automated systems.}
}
@article{BENMILED2023e14636,
title = {Feature engineering from medical notes: A case study of dementia detection},
journal = {Heliyon},
volume = {9},
number = {3},
pages = {e14636},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e14636},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023018431},
author = {Zina {Ben Miled} and Paul R. Dexter and Randall W. Grout and Malaz Boustani},
keywords = {Dementia, BERT, Clinical BERT, UMLS, Medical notes, EMR},
abstract = {Background and objectives
Medical notes are narratives that describe the health of the patient in free text format. These notes can be more informative than structured data such as the history of medications or disease conditions. They are routinely collected and can be used to evaluate the patient's risk for developing chronic diseases such as dementia. This study investigates different methodologies for transforming routine care notes into dementia risk classifiers and evaluates the generalizability of these classifiers to new patients and new health care institutions.
Methods
The notes collected over the relevant history of the patient are lengthy. In this study, TF-ICF is used to select keywords with the highest discriminative ability between at risk dementia patients and healthy controls. The medical notes are then summarized in the form of occurrences of the selected keywords. Two different encodings of the summary are compared. The first encoding consists of the average of the vector embedding of each keyword occurrence as produced by the BERT or Clinical BERT pre-trained language models. The second encoding aggregates the keywords according to UMLS concepts and uses each concept as an exposure variable. For both encodings, misspellings of the selected keywords are also considered in an effort to improve the predictive performance of the classifiers. A neural network is developed over the first encoding and a gradient boosted trees model is applied to the second encoding. Patients from a single health care institution are used to develop all the classifiers which are then evaluated on held-out patients from the same health care institution as well as test patients from two other health care institutions.
Results
The results indicate that it is possible to identify patients at risk for dementia one year ahead of the onset of the disease using medical notes with an AUC of 75% when a gradient boosted trees model is used in conjunction with exposure variables derived from UMLS concepts. However, this performance is not maintained with an embedded feature space and when the classifier is applied to patients from other health care institutions. Moreover, an analysis of the top predictors of the gradient boosted trees model indicates that different features inform the classification depending on whether or not spelling variants of the keywords are included.
Conclusion
The present study demonstrates that medical notes can enable risk prediction models for complex chronic diseases such as dementia. However, additional research efforts are needed to improve the generalizability of these models. These efforts should take into consideration the length and localization of the medical notes; the availability of sufficient training data for each disease condition; and the variabilities resulting from different feature engineering techniques.}
}
@incollection{KRISHNA202129,
title = {Chapter 2 - Emerging social information networks applications and architectures},
editor = {Fadi Al-Turjman and B.D. Deebak},
booktitle = {Security in IoT Social Networks},
publisher = {Academic Press},
pages = {29-53},
year = {2021},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-821599-9},
doi = {https://doi.org/10.1016/B978-0-12-821599-9.00002-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128215999000029},
author = {Dasari Siva Krishna and Patruni Muralidhara Rao and Thammada Srinivasa Rao},
keywords = {Ontology, Query execution engine, Resource description framework, Semantic social network architecture, Social information architecture},
abstract = {Social information architecture (SIA) is a part of information architecture, which deals with the social aspect of conceptualizing, modeling, and organizing information. Nowadays, SIA can be an elastic, growing network for the majority of social applications, namely Facebook, Google Plus, LinkedIn, Instagram, Twitter, etc. The main drawback of SIA can be a dynamic growth in fostering social connections and information that is shared via social spaces on the web. In this regard, a semantic social network architecture (SSNA) can be built to avoid the dynamic change in SIA. In SSNA, the semantic connections are required to make a relation among the social entities. Nonetheless, the network can be built systemically connected with the semantic information of the application. These relations are semantically related to the resources including text, image, audio, video data, etc., that are shared by the social applications made available in SSNA. The e-commerce industry has good relations and use RDFa (resource description frame in attribute). Today these social networking frameworks are primarily used to bring users to a better web experience. In this scenario, search engines are genuinely benefited in accessing relevant results from the metadata. For instance, Facebook devised a protocol named Open Graph Protocol that has a similar working model as the resource description framework (RDF). Also, Microsoft, Google, and Yahoo widely use schema.org based on RDF representation. In view of SSNA, a form of knowledge representation (ontology) has been used to develop a semantic application for the social network. To develop such applications, various key challenges are identified such as knowledge representation, data management, processing management, information retrieval system, proof, and trust. In semantic information retrieval, a query execution engine plays a vital role. In this chapter, we are mainly focused on some key issues while developing such semantic applications in the SSNA.}
}
@article{TRAPPEY2020101980,
title = {Identify trademark legal case precedents - Using machine learning to enable semantic analysis of judgments},
journal = {World Patent Information},
volume = {62},
pages = {101980},
year = {2020},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2020.101980},
url = {https://www.sciencedirect.com/science/article/pii/S0172219019300638},
author = {Charles V. Trappey and Amy J.C. Trappey and Bo-Hung Liu},
keywords = {Trademark infringement, Clustering, Latent dirichlet allocation, Precedence analysis, Recommendation platform},
abstract = {Legal case precedents have a considerable impact on the development of litigation strategies. This research uses the neural network language modeling (NNLM) approach to analyze and identify judgment documents of US trademark (TM) litigation cases as precedents of a given target case. In this research, the NNLM has been trained using 4835 TM litigation documents. There are more than 800,000 words in the entire training text set including more than 150,000 vocabularies. The words in TM legal documents are vectorized to train the NN model for e-discovery of semantically correlated precedents and their features. Specifically, non-supervised machine learning (ML) methods, including clustering and Latent Dirichlet Allocation (LDA), are applied to form the TM legal document clusters, topics, and key terminologies used to characterize the TM case descriptions and precedents. The definition of the clusters, topics and corresponding key terms enhance the ability of the system to recommend and explain similar case judgments for any given TM case of interest or a cease and desist letter with detailed claims of infringement. Further, the intelligent approach provides macro and micro views for companies to research TM litigation trends as a means to better protect their brand equity.}
}
@article{VARMA2021e08035,
title = {Graph NLU enabled question answering system},
journal = {Heliyon},
volume = {7},
number = {9},
pages = {e08035},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e08035},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021021381},
author = {Sandeep Varma and Shivam Shivam and Snigdha Biswas and Pritam Saha and Khushi Jalan},
keywords = {Conversational analytics, Graph traversal, Knowledge graph, Natural language query, Question answering, Structured data, Tabular data},
abstract = {With a huge amount of information being stored as structured data, there is an increasing need for retrieving exact answers to questions from tables. Answering natural language questions on structured data usually involves semantic parsing of query to a machine understandable format which is then used to retrieve information from the database. Training semantic parsers for domain specific tasks is a tedious job and does not guarantee accurate results. In this paper, we used conversational analytics tool to create the user interface and to get the required entities and intents from the query thus avoiding the traditional semantic parsing approach. We then make use of Knowledge Graph for querying in structured data domain. Knowledge graphs can be easily leveraged for question answering systems, to use them as the database. We extract appropriate answers for different types of queries which have been illustrated in the Results section.}
}
@article{FLATER2018144,
title = {Architecture for software-assisted quantity calculus},
journal = {Computer Standards & Interfaces},
volume = {56},
pages = {144-147},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917303069},
author = {David Flater},
keywords = {SI, Quantity, Unit, Uncertainty, Value, Unit 1},
abstract = {A quantity value, such as 5 kg, consists of a number and a reference (often an International System of Units (SI) unit) that together express the magnitude of a quantity. Many software libraries, packages, and ontologies that implement “quantities and units” functions are available. Although all of them begin with SI and associated practices, they differ in how they address issues such as ad hoc counting units, ratios of two quantities of the same kind, and uncertainty. This short article describes an architecture that addresses the complete set of functions in a simple and consistent fashion. Its goal is to encourage more convergent thinking about the functions and the underlying concepts so that the many disparate implementations, present and future, will become more consistent with one another.}
}
@article{SHARMA2021100028,
title = {Deep learning based semantic personalized recommendation system},
journal = {International Journal of Information Management Data Insights},
volume = {1},
number = {2},
pages = {100028},
year = {2021},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2021.100028},
url = {https://www.sciencedirect.com/science/article/pii/S2667096821000215},
author = {Sunny Sharma and Vijay Rana and Vivek Kumar},
keywords = {Deep learning, Recommendation system, Semantics, Personalization},
abstract = {The past decade has seen significant development in the number of personalized recommendation applications on the World Wide Web. It aims to assist users to retrieve relevant items from a large repository of contents by providing items or services of likely interest based on examined evidence of the users’ preferences and desires. However, this vision is complex due to the huge amount of information aka media-rich information available on the web. Most of the systems formulated so far use the metadata linked with the digital contents, but such systems fail to generate significant recommendations results. In these circumstances, a semantic personalized recommendation system (SPRS) plays an important role to take away the semantic gap between high-level semantic contents and low-level media features. The proposed system recommends personalized sets of videos to users depending on their previous activity on the site and exploits a domain ontology and user items content to the domain concepts. To evaluate the performance of the framework, items’ prediction is executed by utilizing the proposed framework, and performance is determined by comparing the predicted and actual ratings of the items in terms of Predictive Accuracy Metrics, precision, and recall.}
}
@article{KAHN2025115027,
title = {More than 50 years of consumer behavior research: What will the future look like?},
journal = {Journal of Business Research},
volume = {186},
pages = {115027},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.115027},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324005319},
author = {Barbara E. Kahn and Anne V. Wilson},
abstract = {To understand how consumer behavior research has evolved and what the future might hold, we first summarize the intellectual trajectory of scholarship in the area and briefly describe the research paradigms that developed over time. We report on the trends in research topics over the years and the “hot topics” projected for the near future. We also discuss the internal and external forces that fundamentally shape research and scholars. These forces provide both constraints and opportunities that will define the future of the field. We predict that some forces, unfortunately, incentivize scholars to examine more minor, less influential research questions. However, new sources of data and areas of inquiry are simultaneously providing opportunities for innovation and creativity in exploring how consumer behavior will change or evolve in response to macroeconomic factors, such as social issues, political movements, or rapid technological advances.}
}
@article{YANG2023103945,
title = {Semi-automatic representation of design code based on knowledge graph for automated compliance checking},
journal = {Computers in Industry},
volume = {150},
pages = {103945},
year = {2023},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2023.103945},
url = {https://www.sciencedirect.com/science/article/pii/S0166361523000957},
author = {Mingsong Yang and Qin Zhao and Lei Zhu and Haining Meng and Kehai Chen and Zongjian Li and Xinhong Hei},
keywords = {Automated compliance checking (ACC), Design code representation, Knowledge graph (KG), Building information model (BIM), Building design},
abstract = {Automated compliance checking (ACC) intends to verify the compliance of designs in construction industry by design codes. The ability to interpret and represent semantic information of design codes determines the maximum application scope of ACC. However, design codes are clause texts written in natural languages and existing ACC studies usually use relatively low-complexity code clause samples. At present, the lack of an accurate representation model for design codes leads to difficulties in representing the implicit information, nested logic, and complex relations contained in high-complexity clauses in codes. To address this problem, this research establishes a new representation model based on knowledge graph (KG). Four schemas are proposed into the model including order, complex, event and integration schemas. Further, an accompanying methodology for semi-automatic construction of design code KG (DCKG) is proposed. It includes four parts: interpretation, reconstruction, organization, and implementation. Where the implementation part develops a code annotation platform. In the case study and experiment, a scenario of checking a building information model (BIM) of metro station by GB50157–2013 Code for Design of Metro is adopted to validate the newly proposed representation model and the automated compliance process. The results show that the proposed model and method are correct and feasible, and our model outperforms other models in the representation ability of design codes.}
}
@article{HARNVORAVONGCHAI2025101858,
title = {Proteomic profiling of pig placenta reveals key biomarkers linked to sow reproductive performance},
journal = {Journal of Agriculture and Food Research},
volume = {21},
pages = {101858},
year = {2025},
issn = {2666-1543},
doi = {https://doi.org/10.1016/j.jafr.2025.101858},
url = {https://www.sciencedirect.com/science/article/pii/S2666154325002297},
author = {Phurt Harnvoravongchai and Matthew Phanchana and Nutthida Pholmanee and Boonyarut Ladda and Thanyapit Thita and Puey Ounjai and Sittiruk Roytrakul and Tavan Janvilisri},
keywords = {Reproductive performance, Placenta, Proteomics, Biomarkers, Mass spectrometry},
abstract = {Reproductive performance is a key factor in swine farming, yet the molecular mechanisms underlying critical reproductive traits remain poorly understood. This study utilized mass spectrometry-based proteomics to analyze protein expression profiles in pig placenta, categorizing samples by sow age, parity, and stillbirth incidence. A total of 8261 proteins were identified, with gene ontology analysis highlighting roles in proteolysis, transcriptional regulation, and immune response. Comparative analysis revealed 87 differentially expressed proteins (DEPs) between young and old sows, 115 DEPs between low and high parity sows, and 103 DEPs linked to stillbirth incidence. Notable proteins, such as laminin G2, methionine sulfoxide reductase B3, and FERM domain-containing 7, were significantly correlated with reproductive traits. Integrative analysis identified YEATS domain-containing 4 as a consistently downregulated protein across all traits, while nephrocystin 1 was linked to low birth weight and stillbirth incidence. Functional enrichment analysis underscored the role of cytoskeletal proteins and transporters in placental nutrient exchange and fetal development. This study expands the understanding of pig placental proteomics and identifies potential biomarkers for improving sow reproductive performance. Future studies will focus on validating these findings and translating them into practical applications for breeding selection and reproductive management in swine production.}
}
@article{HWANKIM2023105067,
title = {How do automated reasoning features impact the usability of a clinical task management system? Development and usability testing of a prototype},
journal = {International Journal of Medical Informatics},
volume = {174},
pages = {105067},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105067},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623000850},
author = {Su {Hwan Kim} and Jessica Jin and Meryem Sevinchan and Alan Davies},
keywords = {Task management, Automated reasoning, Ontologies, Knowledge representation, Usability, User experience},
abstract = {Background
Electronic clinical task management systems (ECTMSs) have been developed and adopted by care providers to improve care coordination. Some systems utilised automated reasoning (AR) to enable more intelligent task management functionalities, such as automated task allocation. Yet, the impact of such features on usability remains unclear. Poor usability of health information systems has been described to cause frustration and contribute to patient safety incidents.
Aim
To design AR features for an ECTMS and to evaluate their impact on usability.
Methods
In this mixed methods study, four ECTMS feature prototypes were co-designed with two clinicians. For each prototype, one AR variant and one non-AR variant with equivalent functionalities were developed. A moderated usability testing was conducted with seven clinicians to obtain ease-of-use ratings of prototypes and measure task durations. Parameters related to demographics and attitudes of participants were obtained via a questionnaire. A framework analysis was performed to summarise qualitative feedback. To determine statistical relationships of study variables, Spearmańs rank coefficients were calculated and presented as a correlation matrix.
Results
Three out of four prototypes received higher median ease-of-use ratings for AR variants and were associated with shorter average task durations. Multiple clinical use cases suitable for AR were identified. Preference for AR was found to moderately correlate with digital proficiency and prior experience with ECTMSs. Insufficient trust in automation, alert fatigue, and system customisation were identified as challenges in the adoption of AR features.
Conclusions
This study provides evidence for the potential of AR to enhance usability in ECTMSs. Consideration of psychological and organisational context of users in the feature design was found to be decisive for usability. Future research should explore implications for operational and clinical outcomes.}
}
@article{NEJKOVIC2020103438,
title = {Semantic approach to RIoT autonomous robots mission coordination},
journal = {Robotics and Autonomous Systems},
volume = {126},
pages = {103438},
year = {2020},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2020.103438},
url = {https://www.sciencedirect.com/science/article/pii/S0921889019306414},
author = {Valentina Nejkovic and Nenad Petrovic and Milorad Tosic and Nenad Milosevic},
keywords = {Coordination, Autonomous robots, Internet of Things, Ontology, Robot sensing systems, Semantic technology, Testbeds},
abstract = {Internet of Things (IoT) has recently become the key for innovation and progress in many industrial sectors and scientific areas. However, it brings many challenges and issues, such as growing number of connected devices, heterogeneity, amount of generated data, security and privacy issues, interoperability and many others. Since devices not only collect data, but also take actions that affect the environment, device coordination in the context of IoT systems is becoming more and more important, especially if the IoT convergence with robotics, known as “Internet of Robotic Things” (RIoT), is taken into consideration. In novel cyber–physical systems coordination is very important for situations when many devices working parallel have higher potential to achieve the given task more effectively, than a single device operating independently. RIoT experimentation testbeds facilitate development of such cyber–physical systems where devices need to be aware of the environment while interacting with other devices in order to achieve a common goal. In this paper, we propose a semantic-driven framework for automated autonomous robots coordination in the context of RIoT-based experimentation testbeds. Framework for automatic coordinated mission generation within the robotics experimentation platform testbed is evaluated. Results of evaluation are presented and discussed.}
}
@article{HUANG2023104792,
title = {BIM and IoT data fusion: The data process model perspective},
journal = {Automation in Construction},
volume = {149},
pages = {104792},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104792},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523000523},
author = {Xiongwei Huang and Yongping Liu and Lizhen Huang and Erling Onstein and Christoph Merschbrock},
keywords = {Building information modeling, Internet of Things, Data fusion, Information fusion, Digital twin},
abstract = {Digitalization in the architecture, engineering, and construction (AEC) industry has highlighted the process of gathering and combining data from numerous sources. This paper uses data fusion from information science to investigate how data from information systems like Building Information Modeling (BIM) and the Internet of Things (IoT) could be coupled to enable a data-driven AEC. However, given the large amount of data, the technological diversity, the heterogeneous data schema, and hierarchical data abstraction, BIM and IoT data fusion are not addressed systematically. This study aims to develop a BIM and IoT data fusion framework to facilitate the appropriate data process flow for various applications. To investigate the research topic, an integrative review was conducted. Specifically, a general and tailored four-step data fusion process model was proposed as the review’s guiding framework. Critical evaluations were incorporated into a road map to provide a comprehensive perspective of BIM and IoT data fusion. In addition, based on the findings, a two-level and interconnected data fusion framework was proposed, which illustrates the data process flow and differentiates the data abstraction level, data processing technologies and purpose of the fusion. Furthermore, the fusion challenges and future trends are also highlighted.}
}
@article{CUCCIO2019157,
title = {Rethinking the abstract/concrete concepts dichotomy: Comment on “Words as social tools: Language, sociality and inner grounding in abstract concepts” by Anna M. Borghi et al.},
journal = {Physics of Life Reviews},
volume = {29},
pages = {157-160},
year = {2019},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300703},
author = {V. Cuccio and F. Caruana}
}
@article{BLOCK20181351,
title = {Approach for a simulation-based and event-driven production planning and control in decentralized manufacturing execution systems},
journal = {Procedia CIRP},
volume = {72},
pages = {1351-1356},
year = {2018},
note = {51st CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.204},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118303639},
author = {Christian Block and Dominik Lins and Bernd Kuhlenkötter},
keywords = {Event-Based Planning, Distributed System, Manufacturing Execution System, Simulation},
abstract = {Short-term production planning and control (PPC) is still human centered. Unplanned events disturb manufacturing processes and require a quick rescheduling. However, these planning processes are complex tasks in which the employee has to be supported. In addition to the need of a real-time information set, the acquisition of which is supported by new technologies, an assistant system has to facilitate decision making in today’s PPC. This paper presents an approach for an event-driven PPC based on a manufacturing ontology, simulation and optimization. The approach leads to an enhancement of current production systems on the basis of decentralized manufacturing execution systems (MES).}
}
@article{STICKLER2021102424,
title = {Investigating language teachers’ ideals in images and interviews},
journal = {System},
volume = {97},
pages = {102424},
year = {2021},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2020.102424},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X20307843},
author = {Ursula Stickler},
keywords = {Language teachers, Ideal teachers, Voice-centred relational method (VCRM), Visual methods},
abstract = {Ideals have been linked to identity, motivation and stamina. To investigate language teachers’ ideals in depth, this study employs a novel complex, three-step method, analysing data collected through drawings and one-to-one reflective interviews with a voice centred relational method (VCRM) for analysis. VCRM conserves the interviewees’ personal voice and includes the researcher as part of the process of analysis. This paper will show how the complex method can be employed and how the findings derived from it allow insights into the forces that keep teachers going, their origins, and their potential for integrating change. Thus, qualitative data collection methods combined with a voice-centred analysis method can be harnessed to enhance our understanding of teacher ideals and the tensions between external demands, internal standards, and different cultures language teachers have to cope with.}
}
@article{DAROCHAFRANCO2024100708,
title = {Managing and controlling digital role-playing game elements: A current state of affairs},
journal = {Entertainment Computing},
volume = {51},
pages = {100708},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100708},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000764},
author = {Artur de Oliveira {da Rocha Franco} and Windson Viana {de Carvalho} and José Wellington Franco {da Silva} and José Gilvan Rodrigues Maia and Miguel Franklin {de Castro}},
keywords = {RPG, Artificial intelligence, Procedural content generation},
abstract = {Role-playing games (RPGs) are interactive gaming experiences designed and driven by the intricate development of narratives and characters. However, these games also highlight elements, such as combat, exploration, creativity, and crafting. Game developers helped to shape the market by incorporating RPGs into digital media; however, they suffered restrictions in their game design and storytelling caused by technological limitations. Fortunately, academia has studied these challenges and mitigated these latter issues to better understand and connect digital RPGs with their analog versions. Our research offers a comprehensive overview of studies centered on technologies for managing, generating, and controlling digital RPG elements. We aim to characterize Artificial Intelligence research within this domain and elucidate the diverse techniques employed. In this context, we examined 72 papers about Procedural Content Generation (PCG) in RPG, which were identified by mixing a database search with a snowballing method. In this paper, we provide an overview of PCG techniques investigated within the realm of RPGs and present the trending approaches for future developments in the field.}
}
@article{RESSIOREC2025104170,
title = {Caring policy-relevant knowledge? The case of the Brazilian Platform for Biodiversity and Ecosystem Services},
journal = {Environmental Science & Policy},
volume = {171},
pages = {104170},
year = {2025},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2025.104170},
url = {https://www.sciencedirect.com/science/article/pii/S1462901125001868},
author = {Adriana {Ressiore C.} and Giulia De Fusco and David Ludwig and Charbel N. El-Hani and Esther Turnhout},
keywords = {Science-policy interface, Care, Linear model, Decision-makers, Indigenous and local knowledge, Brazil},
abstract = {Science-policy interfaces like the Brazilian Platform for Biodiversity and Ecosystem Services (BPBES) aim to provide policy-relevant knowledge that guides decision-makers in addressing the current biodiversity crisis. At the same time, dominant approaches to policy-relevant knowledge have been widely challenged for relying on a misguided linear model that treats science and policy as separate domains, presenting the former through depoliticized ideals of neutrality and objectivity while prioritizing efficiency, standardization, and measurable outputs over transdisciplinary collaboration, inclusivity, and plurality of knowledge systems. This article focuses on “care” as an embodied, situated, and relational practice that could open pathways to policy-relevant knowledge that is inclusive and responsive to diverse human and non-human needs. Through semi-structured interviews and analysis of Summaries for Decision-Makers (SDMs) in BPBES, we investigate how different forms of care shape the content, creation process, and impact of SDMs. Our findings reveal that care is present across all of these dimensions but also that systemic barriers limit its practices. In particular, we argue that the legacy of the linear model often creates tensions with care perspectives as they can often be seen as too subjective and as threatening the credibility of BPBES. We, therefore, conclude that there remain substantial challenges to articulating a vision and practice of “caring policy-relevant knowledge” that embraces care as central to shaping relations between science and policy.}
}
@article{ALBERS2018168,
title = {Using semantic metadata for continuous development of requirements and goals in the Smart Mobility domain – An empirical study},
journal = {Procedia CIRP},
volume = {70},
pages = {168-173},
year = {2018},
note = {28th CIRP Design Conference 2018, 23-25 May 2018, Nantes, France},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.03.070},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118301732},
author = {Albert Albers and Armin Kurrle},
keywords = {Systems-of-Systems, Product Generation Development, Semantic Web, RDF, Metadata, Requirements Engineering},
abstract = {The continuous development and consistent documentation of requirements and goals is a major challenge in product development. Inconsistency can lead to immense changes in development time, costs or quality. If major inconsistencies are not detected, a project failure may occur. As systems-of-systems (SoS) are developed by many stakeholders from independent organizational units using many different methods, processes and tools, this brings new challenges to obtain consistency and continuity on multiple levels and in different phases of the product development process. As changes to requirements can have a wide impact on the distributed and partly independent working development teams, requirements and goals as well as their heterogeneous interrelations and boundary conditions have to be modelled as a continuous system of objectives. The KaRDF approach developed at the IPEK – Institute of Product Engineering at the Karlsruhe Institute of Technology uses the semantic web technology RDF to link development artefacts independent of their representation in order to build consistent product models for systems-of-systems on different level of detail supporting multiple development and product generations. This research work presents the results of an empirical study in the Smart Mobility domain where KaRDF was used to support the development of SoS. The KaRDF ontology and inference rules are implemented in a web-based software tool which is used by project managers from a German car manufacturer for adding semantic metadata to their development artefacts. The results show that through usage of the defined semantic metadata structure and inference rules, continuity and consistency in product development of systems-of-systems is improved and the ability of the approach to integrate with heterogeneous methods, processes and tools used in a systems-of-systems development environment is proven.}
}
@article{ABUSALIH2024e25383,
title = {A systematic literature review of knowledge graph construction and application in education},
journal = {Heliyon},
volume = {10},
number = {3},
pages = {e25383},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e25383},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024014142},
author = {Bilal Abu-Salih and Salihah Alotaibi},
keywords = {Knowledge graphs, Knowledge graph construction, Education, Learning, Systematic literature review, Survey},
abstract = {In the dynamic landscape of modern education, the search for improved pedagogical methods, enriched learning experiences, and empowered educators remains a perpetual pursuit. In recent years, a remarkable technological innovation has asserted its dominance in education: Knowledge Graphs (KGs). These structured representations of knowledge are increasingly proving to be indispensable tools, fostering advancements driven by the growing recognition of their essential role in enriching personalised learning, curriculum design, concept mapping, and educational content recommendation systems. In this paper, a systematic literature review (SLR) has been conducted to comprehensively examine KG construction methodologies and their applications across five key domains in education. In each examined study, we highlight the specific KG functionalities, knowledge extraction techniques, knowledge base characteristics, resource requirements, evaluation criteria, and limitations. This paper distinguishes itself by offering a broad overview of KGs in education, analyzing state-of-the-art methodologies, and identifying research gaps and limitations, paving the way for future advancements.}
}
@article{METWALLY2025178136,
title = {Integrated network pharmacology and in vivo experimental approaches unveil the modulatory effect of telmisartan on autophagy in a rat model of nonalcoholic steatohepatitis.},
journal = {European Journal of Pharmacology},
pages = {178136},
year = {2025},
issn = {0014-2999},
doi = {https://doi.org/10.1016/j.ejphar.2025.178136},
url = {https://www.sciencedirect.com/science/article/pii/S0014299925008908},
author = {Sami S. Metwally and Rasha H. Abdel-Ghany and Atef S. Elgharbawy and Mohamed Mohsen and Amira Ebrahim Alsemeh and Esraa M. Zakaria},
keywords = {Telmisartan, pioglitazone, network pharmacology, NASH, autophagy},
abstract = {Abstract:
Defective autophagy contributes to the progression of various diseases, including nonalcoholic steatohepatitis (NASH). While telmisartan’s hepatoprotective effects are well established, its impact on hepatic autophagy in NASH remains unclear. This study employed network pharmacology combined with in vivo experiments to predict and validate telmisartan’s effects on hepatic autophagy in a rat model of NASH. Additionally, we investigated whether telmisartan could augment pioglitazone’s protective effects. Network pharmacology identified 97 common molecular targets shared by telmisartan and NASH. Key targets included Signal Transducer and Activator of Transcription 3 (STAT3), B-cell lymphoma 2 (Bcl2), and Nuclear Factor kappa-light-chain-enhancer of activated B cells (NF-κB), regulators of cellular autophagy, suggesting telmisartan’s potential interaction with autophagy-related pathways. To validate these findings, NASH was induced in adult male rats via an 18-week high fructose, fat, and salt diet, with telmisartan, pioglitazone, or their combination administered orally during the final 10 weeks. Blood pressure, glycemic, lipid, and liver function parameters were quantified in serum. Besides, hepatic markers of autophagy (Beclin1, LC3, and p62), inflammation, oxidative stress, fibrosis, and apoptosis were measured. Moreover, liver histology and collagen deposition were examined. Telmisartan significantly enhanced hepatic autophagy more than pioglitazone. Furthermore, combination therapy synergistically increased autophagy beyond the effects of either drug alone. Both drugs similarly ameliorated hepatic inflammation and oxidative stress markers. These results demonstrate that telmisartan’s hepatoprotective effects are partly mediated by restoration of hepatic autophagy. Moreover, the data suggest that combined telmisartan and pioglitazone therapy may provide enhanced benefit in NASH treatment, warranting further clinical investigation.}
}
@article{FIELDS2018186,
title = {Conscious agent networks: Formal analysis and application to cognition},
journal = {Cognitive Systems Research},
volume = {47},
pages = {186-213},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2017.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041717300827},
author = {Chris Fields and Donald D. Hoffman and Chetan Prakash and Manish Singh},
keywords = {Active inference, Complex networks, Computation, Learning, Memory, Planning, Predictive coding, Self representation, Reference frame, Turing completeness},
abstract = {Networks of “conscious agents” (CAs) as defined by Hoffman and Prakash (2014) are shown to provide a robust and intuitive representation of perceptual and cognitive processes in the context of the Interface Theory of Perception (Hoffman, Singh and Prakash, 2015). The behavior of the simplest CA networks is analyzed exhaustively. The construction of short- and long-term memories and the implementation of attention, categorization and case-based planning are demonstrated. These results show that robust perception and cognition can be modelled independently of any ontological assumptions about the world in which an agent is embedded. Any agent-world interaction can, in particular, also be represented as an agent-agent interaction.}
}
@article{BOYLE2021103515,
title = {Coaching practices: Building teacher capability to enhance continuity in the early years},
journal = {Teaching and Teacher Education},
volume = {108},
pages = {103515},
year = {2021},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2021.103515},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X21002407},
author = {Tess Boyle and Anne Petriwskyj and Susan Grieshaber and Lesley Jones},
keywords = {Continuity, Coaching practices, Teacher capability, Transitions to school},
abstract = {Continuity of learning and development for children transitioning from pre-compulsory to compulsory education remains challenging in many educational contexts. There is little evidence about the potential of coaching to build teacher capability as a strategy to enhance continuity for children. This article reports details of how a collective case study and the theory of practice architectures framed an investigation of coaching practices aimed at building teacher capability to address student continuity issues. The research endorses a situated (site ontological) approach to building teacher capability to enhance continuity in the early years. Findings identify how site-based conditions influenced (enabled and constrained) coaching practices and transitional continuity.}
}
@article{LIKAVCAN20181666,
title = {Technology appropriation in a de-growing economy},
journal = {Journal of Cleaner Production},
volume = {197},
pages = {1666-1675},
year = {2018},
note = {Technology and Degrowth},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.12.134},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616321977},
author = {Lukáš Likavčan and Manuel Scholz-Wäckerle},
keywords = {Political economy, Technological change, De-growth, Actor-network theory, Habilitation, Information and communication technologies},
abstract = {This article proposes a concept of technology appropriation as a means of ideological repurposing of technology in a de-growing economy. Its economic and institutional foundations are set forth using an endogenous theory of economic development in the spirit of Marx, Schumpeter and Veblen. Technological and institutional change lead interdependently to the emergence of socio-technical complexes which are subject to ideological appropriation. These complexes are analyzed as actor-networks from an ontological perspective and exemplified using special cases in the information and communication technologies (ICTs). The approach contributes to synthesizing critical political economy with actor-network theory. Technology appropriation is elaborated as a means of repurposing existing technologies in terms of recombination, with an emphasis on the strategy of habilitation. This political economic strategy delivers new insights for a solidary mode of economic reproduction in a low-carbon de-growing economy.}
}
@article{FAN2025106752,
title = {Exploring the molecular mechanism of Chinese herbal extracts used to improve skin wound healing},
journal = {Fitoterapia},
volume = {185},
pages = {106752},
year = {2025},
issn = {0367-326X},
doi = {https://doi.org/10.1016/j.fitote.2025.106752},
url = {https://www.sciencedirect.com/science/article/pii/S0367326X25003788},
author = {Shan Fan and Xiaoqun Shen and Qing Liu and Wenhui Jiang and Liuyin Wang and Yakun Hao and Jia Yan and Shengguo Ji},
keywords = {Compound Pyocutaneous liniments, Skin ulcers, Metabolomics, Network pharmacology, Molecular docking},
abstract = {This study investigates the chemical composition and therapeutic mechanisms of Compound Pyocutaneous Liniments (CPL), a traditional Chinese medicine, in treating skin ulcers. Using UPLC-Q-Orbitrap-MS, 66 bioactive components were identified, including flavonoids, phenylpropanoids, and anthraquinones. Acute oral toxicity tests demonstrated that CPL has high safety within the tested dose range. Skin irritation tests revealed that CPL does not cause skin irritation. In a rat skin ulcer model, CPL significantly accelerated wound healing, enhanced collagen deposition, and upregulated CD34, VEGF, and EGF expression. Serum and skin metabolomics revealed CPL's regulation of glycerophospholipid and sphingolipid metabolism. Network pharmacology predicted TNF, AKT1, and EGFR as core targets, with pathways such as EGFR tyrosine kinase inhibitor resistance and VEGF signaling implicated. Molecular docking validated strong interactions between CPL components (e.g., luteolin) and these targets. These findings demonstrate CPL's multi-component, multi-target, and multi-pathway mechanisms, supporting its clinical application for skin ulcers.}
}
@article{EDER2021407,
title = {Data quality for federated medical data lakes},
journal = {International Journal of Web Information Systems},
volume = {17},
number = {5},
pages = {407-426},
year = {2021},
issn = {1744-0084},
doi = {https://doi.org/10.1108/IJWIS-03-2021-0026},
url = {https://www.sciencedirect.com/science/article/pii/S1744008421000288},
author = {Johann Eder and Vladimir A. Shekhovtsov},
keywords = {Biobank, Metadata, Data quality, Data lake, Privacy, LOINC, Metadata and ontologies},
abstract = {Purpose
Medical research requires biological material and data collected through biobanks in reliable processes with quality assurance. Medical studies based on data with unknown or questionable quality are useless or even dangerous, as evidenced by recent examples of withdrawn studies. Medical data sets consist of highly sensitive personal data, which has to be protected carefully and is available for research only after the approval of ethics committees. The purpose of this research is to propose an architecture to support researchers to efficiently and effectively identify relevant collections of material and data with documented quality for their research projects while observing strict privacy rules.
Design/methodology/approach
Following a design science approach, this paper develops a conceptual model for capturing and relating metadata of medical data in biobanks to support medical research.
Findings
This study describes the landscape of biobanks as federated medical data lakes such as the collections of samples and their annotations in the European federation of biobanks (Biobanking and Biomolecular Resources Research Infrastructure – European Research Infrastructure Consortium, BBMRI-ERIC) and develops a conceptual model capturing schema information with quality annotation. This paper discusses the quality dimensions for data sets for medical research in-depth and proposes representations of both the metadata and data quality documentation with the aim to support researchers to effectively and efficiently identify suitable data sets for medical studies.
Originality/value
This novel conceptual model for metadata for medical data lakes has a unique focus on the high privacy requirements of the data sets contained in medical data lakes and also stands out in the detailed representation of data quality and metadata quality of medical data sets.}
}
@article{TSAI2022100077,
title = {Multiagent mobility and lifestyle recommender system for individuals with visual impairment},
journal = {Neuroscience Informatics},
volume = {2},
number = {4},
pages = {100077},
year = {2022},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2022.100077},
url = {https://www.sciencedirect.com/science/article/pii/S2772528622000395},
author = {Kuo-Pao Tsai and Feng-Chao Yang and Chuan-Yi Tang},
keywords = {Context awareness, People with visual impairment, Recommender systems, Multiagent},
abstract = {Background: Individuals with visual impairment currently rely on walking sticks and guide dogs for mobility. However, both tools require the user to have a mental map of the area and cannot help the user establish detailed information about their surroundings, including weather, location, and businesses. Purpose and Methods: This study designed a navigation and recommendation system with context awareness for individuals with visual impairment. The study used Process for Agent Societies Specification and Implementation (PASSI), which is a multiagent development methodology that follows the Foundation for Intelligent Physical Agents framework. The model used the Agent Unified Modeling Language (AUML). Results: The developed system contains a context awareness module and a multiagent system. The context awareness module collects data on user context through sensors and constructs a user profile. The user profile is transferred to the multiagent system for service recommendations. The multiagent system has four agents: a consultant agent, search agent, combination agent, and dispatch agent and integrates machine and deep learning. AUML tools were used to describe the implementation and structure of the system through use-case graphics and kit, sequence, class, and status diagrams. Conclusions: The developed system understands the needs of the user through the context awareness module and finds services that best meet the user's needs through the agent recommendation mechanism. The system can be used on Android phones and tablets and improves the ease with which individuals with visual impairment can obtain the services they need.}
}
@article{BALLANDIES2021104335,
title = {Mobile link prediction: Automated creation and crowdsourced validation of knowledge graphs},
journal = {Microprocessors and Microsystems},
volume = {87},
pages = {104335},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104335},
url = {https://www.sciencedirect.com/science/article/pii/S0141933121004944},
author = {Mark C. Ballandies and Evangelos Pournaras},
keywords = {Knowledge graph, Ontology, Cyber–physical-social system, Link prediction, Genetic programming, Crowdsourcing},
abstract = {Building trustworthy knowledge graphs for cyber–physical social systems (CPSS) is a challenge. In particular, current approaches relying on human experts have limited scalability, while automated approaches are often not validated by users resulting in knowledge graphs of questionable quality. This paper introduces a novel pervasive knowledge graph builder for mobile devices that brings together automation, experts’ and crowdsourced citizens’ knowledge. The knowledge graph grows via automated link predictions using genetic programming that are validated by humans for improving transparency and calibrating accuracy. The knowledge graph builder is designed for pervasive devices such as smartphones and preserves privacy by localizing all computations. The accuracy, practicality, and usability of the knowledge graph builder is evaluated in a real-world social experiment that involves a smartphone implementation and a Smart City application scenario. The proposed methodology of knowledge graph building outperforms a baseline method in terms of accuracy while demonstrating its efficient calculations on smartphones and the feasibility of the pervasive human supervision process in terms of high interactions throughput. These findings promise new opportunities to crowdsource and operate pervasive reasoning systems for cyber–physical social systems in Smart Cities.}
}
@article{VANASSCHE2023100753,
title = {Declarative RDF graph generation from heterogeneous (semi-)structured data: A systematic literature review},
journal = {Journal of Web Semantics},
volume = {75},
pages = {100753},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100753},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000373},
author = {Dylan {Van Assche} and Thomas Delva and Gerald Haesendonck and Pieter Heyvaert and Ben {De Meester} and Anastasia Dimou},
keywords = {Knowledge graph construction, Schema transformations, Data transformations, Survey, Declarative},
abstract = {More and more data in various formats are integrated into knowledge graphs. However, there is no overview of existing approaches for generating knowledge graphs from heterogeneous (semi-)structured data, making it difficult to select the right one for a certain use case. To support better decision making, we study the existing approaches for generating knowledge graphs from heterogeneous (semi-)structured data relying on mapping languages. In this paper, we investigated existing mapping languages for schema and data transformations, and corresponding materialization and virtualization systems that generate knowledge graphs. We gather and unify 52 articles regarding knowledge graph generation from heterogeneous (semi-)structured data. We assess 15 characteristics on mapping languages for schema transformations, 5 characteristics for data transformations, and 14 characteristics for systems. Our survey paper provides an overview of the mapping languages and systems proposed the past two decades. Our work paves the way towards a better adoption of knowledge graph generation, as the right mapping language and system can be selected for each use case.}
}
@article{TANG2025110445,
title = {Computer-aided drug discovery of a dual-target inhibitor for ovarian cancer: therapeutic intervention targeting CDK1/TTK signaling pathway and structural insights in the NCI-60},
journal = {Computers in Biology and Medicine},
volume = {193},
pages = {110445},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110445},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525007966},
author = {Sung-Ling Tang and Maryam Rachmawati Sumitra and Yu-Cheng Kuo and Han-Lin Hsu and Muhamad Ansar and Sheng-Liang Huang and Shih-Yu Lee and Hong-Jaan Wang and Bashir Lawal and Alexander T.H. Wu and Lung-Ching Chen and Hsu-Shan Huang},
keywords = {Ovarian cancer, Dual-target small molecule inhibitor, Molecular docking, NCI-60 panel, Machine learning, Molecular dynamics simulations},
abstract = {Ovarian cancer remains the third most prevalent and deadliest gynecologic malignancy worldwide, with most patients eventually developing resistance to platinum-based chemotherapy. This highlights a critical unmet need for innovative multitargeted therapies to address current treatment challenges. In this study, we identified 35 differentially expressed genes (DEGs) through integrated analysis of four GEO ovarian cancer datasets, with validation using TCGA data. Gene Ontology (GO) and KEGG enrichment analyses highlighted key tumor-associated pathways, and protein–protein interaction (PPI) network modeling prioritized CDK1 and TTK as high-value therapeutic targets. We evaluated the association between molecular genomic features and drug responses across the NCI-60 ovarian cancer cell line panel (IGROV1, OVCAR-3, OVCAR-4, OVCAR-5, OVCAR-8, NCI/ADR-RES, and SK-OV-3), using a series of salicylanilide-derived compounds and four FDA-approved drugs (cabozantinib, paclitaxel, rapamycin, and niclosamide) from the NCI Developmental Therapeutics Program (DTP). Among these, NSC765690 (MCC22) emerged as the most promising candidate. It demonstrated potent antiproliferative activity, high target selectivity, and strong binding affinity to both CDK1 and TTK. Multi-omics integration, combined with AI-driven network modeling, further elucidated NSC765690's mechanism of action and its relevance to ovarian cancer pathogenesis. Additionally, ADMET and pharmacokinetic profiling confirmed its favorable drug-like properties and low predicted toxicity. Collectively, these findings establish NSC765690 as a potent dual-target inhibitor and exemplify a rational, data-driven drug discovery pipeline for overcoming chemotherapy resistance in ovarian cancer.}
}
@article{YUAN2022118572,
title = {The space quadrant and intelligent occlusion calculation methods for the external heat flux of China space Station},
journal = {Applied Thermal Engineering},
volume = {212},
pages = {118572},
year = {2022},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2022.118572},
url = {https://www.sciencedirect.com/science/article/pii/S1359431122005221},
author = {Man Yuan and Yun-Ze Li and Yuehang Sun and Binpeng Ye},
keywords = {External heat flux, Space quadrant, Intelligent occlusion, Spacecraft},
abstract = {In the thermal control design of spacecraft and the study of its infrared radiation characteristics, the influence of external heat flux must be considered emphatically. In order to solve the complex space occlusion problem of China Space Station, the centroid ontology coordinate system is first defined as the reference coordinate system in this paper. Then, the space quadrant and intelligent occlusion calculation methods are proposed. The former is mainly to solve the shielding of the external heat flux of spacecraft body. The latter is to solve the shielding of the external heat flux of large external hangings. The results show that the maximum occlusion time is up to 6.5 min and the maximum occlusion difference is about 0.7 in the same quadrant at different solar incident angles. Moreover, if the shielding of external hangings is not considered, the time is more than 15 min when the external heat flux difference exceeds 4000 W in the sunlight area, and the maximum difference is about 6000 W. Meaningfully, the space quadrant and intelligent occlusion calculation methods proposed in this paper can effectively solve most of the occlusion problems and is suitable for various spacecraft.}
}
@article{SEVERSON2023243,
title = {Transcriptome-wide characterization of alternative splicing in five drug-type cultivars of Cannabis sativa},
journal = {Botany},
volume = {101},
number = {7},
pages = {243-254},
year = {2023},
issn = {1916-2790},
doi = {https://doi.org/10.1139/cjb-2022-0099},
url = {https://www.sciencedirect.com/science/article/pii/S1916279023000307},
author = {Tonya F. Severson and Keith L. Adams},
keywords = { L., alternative splicing, trichomes, transcriptome},
abstract = {Cannabis sativa L. is widely used for fiber, medicinal, and other purposes, and many cultivars exist, yielding varying proportions of cannabinoids and terpenes. There is considerable interest in characterizing genomes and transcriptomes of C. sativa. Alternative splicing (AS) is a fundamental aspect of gene expression that results in multiple types of mRNAs produced by differential splicing. Transcriptome-wide identification of AS events in drug-type cultivars of C. sativa has not been reported. Here, we identified AS events using a transcriptome dataset derived from five drug-type cultivars with divergent chemotypes. Intron retention is the most common event type, followed by alternative acceptor, alternative donor, and skipped exons. We also sought to assess conservation of AS events among cultivars. We found 547 events (5%) unique to a single cultivar, 2661 (25%) shared by 2–4 cultivars, and 7569 (70%) common to all 5 cultivars. Genes with AS events in each set were analyzed for gene ontology enrichment, showing that genes with AS unique to a single cultivar are enriched for molecular functions related to interactions with ATP and processes involving transport within cells and across membranes. These results provide insights into the conservation and variation of AS events in multiple cultivars of C. sativa.}
}
@incollection{KEIKHOSROKIANI20201,
title = {Chapter 1 - Introduction to Mobile Medical Information System (mMIS) development},
editor = {Pantea Keikhosrokiani},
booktitle = {Perspectives in the Development of Mobile Medical Information Systems},
publisher = {Academic Press},
pages = {1-22},
year = {2020},
isbn = {978-0-12-817657-3},
doi = {https://doi.org/10.1016/B978-0-12-817657-3.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128176573000018},
author = {Pantea Keikhosrokiani},
keywords = {Big data, Communication technologies, iHeart, Internet of Things (IoT), mMIS development life cycle, Mobile Medical Information System, Ontological modeling, Policies and regulations, Privacy and security},
abstract = {The main goal of this chapter is to introduce the main required concept for developing a Mobile Medical Information System (mMIS). Therefore, mMIS is defined along with System Development Life Cycle to provide instruction for its development. In addition, communication technologies which are required to develop mMIS are introduced followed by the concept of Internet of Things and the required security and privacy. The concept of big data in mMIS is discussed afterward. Policies and regulations and ontology modeling for mMIS are mentioned after the concept of big data. Finally, iHeart, which is used as an example of mMIS for better understanding of the concepts in each chapter, is explained.}
}
@article{ABDELMADJID2022317,
title = {Uncertain Decision-Making Requirements Formalizing with Complement Fuzzy UML Model},
journal = {Procedia Computer Science},
volume = {198},
pages = {317-322},
year = {2022},
note = {12th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 11th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.12.247},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921024868},
author = {Larbi Abdelmadjid and Malki Mimoun},
keywords = {DIS, Uncertainty, Decisional Requirements, Belief theory, Expression, UML},
abstract = {Unified Modeling Language (UML) is not a design method but rather a graphic language to represent and communicate the different aspects of an information system. Among the most important research areas in information systems is the enrichment of existing models with a larger collection of semantic concepts, hence the appearance of its extension Fuzzy UML, which despite its advantages to represent qualitative concepts, also suffers from certain restrictions for the uncertain data representation and whose the literature shows that few works are interested in the resolution of this type of problem. In the context of expressing uncertain decision-making requirements, no known solution is proposed to represent data reflecting uncertain requirements. Our contribution presents a flexible process for expressing decision-making requirements by evaluating uncertainty by formalization using our CFUML model and the representation of decision-maker preferences by one or more mass functions using belief theory. In this article, we will only present our proposed model CFUML.}
}
@incollection{DILLON202031,
title = {Critical Geographic Information System},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {31-36},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10530-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102295510530X},
author = {Mahmoudi Dillon},
keywords = {Big data, Cartography, Counter-mapping, Critical human geography, Geographic information science (GIS), Strategic positivism},
abstract = {Critical GIS combines the technical held of geographic information science (GIS) with heterodox social theory. The result is a rich held whose technical focus incorporates cartography, computation, big data, and information science, with theoretical moorings in critical human geography, feminism, STS, and scholar-activism. A series of critiques of the technoscientific nature of traditional GIS undergird the formation of critical GIS as a subdiscipline and continues to contribute to its evolving definitions. One notable challenge for any definition of a critical GIS is the continued development of the technologies of GIS itself and the social, political, and economic transformations that reflect and feedback into the continued evolution of technology. Critical GIS is thus dynamic but has rich histories that include critiques and contributions of scholars that mirror and technology studies (STS), feminism and GIS, ontology, research, and participatory GIS (PGIS).}
}
@article{TASKIN2019676,
title = {Natural language processing applications in library and information science},
journal = {Online Information Review},
volume = {43},
number = {4},
pages = {676-690},
year = {2019},
issn = {1468-4527},
doi = {https://doi.org/10.1108/OIR-07-2018-0217},
url = {https://www.sciencedirect.com/science/article/pii/S1468452719000039},
author = {Zehra Taskin and Umut Al},
keywords = {Social network analysis, Bibliometrics, Library and information science, Citespace, VOSviewer, Natural language processing},
abstract = {Purpose
With the recent developments in information technologies, natural language processing (NLP) practices have made tasks in many areas easier and more practical. Nowadays, especially when big data are used in most research, NLP provides fast and easy methods for processing these data. The purpose of this paper is to identify subfields of library and information science (LIS) where NLP can be used and to provide a guide based on bibliometrics and social network analyses for researchers who intend to study this subject.
Design/methodology/approach
Within the scope of this study, 6,607 publications, including NLP methods published in the field of LIS, are examined and visualized by social network analysis methods.
Findings
After evaluating the obtained results, the subject categories of publications, frequently used keywords in these publications and the relationships between these words are revealed. Finally, the core journals and articles are classified thematically for researchers working in the field of LIS and planning to apply NLP in their research.
Originality/value
The results of this paper draw a general framework for LIS field and guides researchers on new techniques that may be useful in the field.}
}
@article{LINDAHL2019100,
title = {Exploring the mixed methods research paradigm in language teaching and learning},
journal = {Linguistics and Education},
volume = {49},
pages = {100-101},
year = {2019},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0898589818300111},
author = {Kristen Lindahl}
}
@article{ZHENG2025114041,
title = {RcTIFY3 regulation of rose black spot disease relies on its synergistic interaction with salicylic acid to antagonize jasmonic acid},
journal = {Scientia Horticulturae},
volume = {342},
pages = {114041},
year = {2025},
issn = {0304-4238},
doi = {https://doi.org/10.1016/j.scienta.2025.114041},
url = {https://www.sciencedirect.com/science/article/pii/S0304423825000925},
author = {Xiaowen Zheng and Xingyu Liu and Guoqing Han and Xiaojian Geng and Fashou Liu and Xiuting Ju and Tingliang Xu and Nan Tang},
keywords = {, , Jasmonic acid, Salicylic acid, VIGS, Transient overexpression},
abstract = {Rose black spot disease, caused by Marssonina rosae, poses a significant threat to rose cultivation, leading to leaf defoliation and potentially plant death. This study identified 12 TIFY family members in roses by analyzing transcriptomes from both susceptible and resistant lines under M. rosae infection. Among these, RcTIFY3 was selected through Gene Ontology, Kyoto Encyclopedia of Genes and Genomes, and Fragments Per Kilobase of transcript per Million mapped read expression pattern analyses, suggesting its pivotal role in hormone mobilization and disease progression. Real-time fluorescence quantification was employed to monitor RcTIFY3 expression during the rose's response to M. rosae. The study also explored the dependency of RcTIFY3’s role in black spot disease resistance on salicylic acid (SA) and JA through exogenous application of SA, JA, and their respective synthetic inhibitors. Using TRV-mediated virus-induced gene silencing and transient overexpression techniques, RcTIFY3’s interactions with hormones during the black spot disease response were validated. This research concludes that RcTIFY3’s regulation of black spot disease is mediated by its synergistic interaction with SA, antagonizing JA, which positively influences disease development.}
}
@article{YUSKEVICH2021121103,
title = {A metamodel of an informational structure for model-based technology roadmapping},
journal = {Technological Forecasting and Social Change},
volume = {173},
pages = {121103},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2021.121103},
url = {https://www.sciencedirect.com/science/article/pii/S0040162521005369},
author = {Ilya Yuskevich and Andreas Makoto Hein and Kahina Amokrane-Ferka and Abdelkrim Doufene and Marija Jankovic},
keywords = {Technology roadmapping, Model-driven Engineering, Metamodeling},
abstract = {Recent contributions in the field of technology roadmapping often aim to apply various numerical models and tools to facilitate the roadmapping process and enrich its outcomes. This trend resulted in the emergence of so-called model-based technology roadmapping. We consider it as the future development of the traditional document-based paradigm. One of the general approaches to support the model-based roadmapping is to develop a roadmap's metamodel that would define it independently from the application context and link it to the existing roadmapping literature. In this paper, we attempt to create such a metamodel by generalizing and formalizing existing document-based roadmaps. We validate our metamodel via reproducing three very different roadmaps from the literature, not included in the set of roadmaps from which the metamodel was created, using the novel formal approach. The fact that these roadmaps were reproduced using the proposed metamodel indicates its applicability to many classes of real roadmaps. The results of this work seem beneficial for architects of the software tools for roadmapping and for the regular participants of roadmapping sessions.}
}
@article{KOWSZYK2023101161,
title = {Conflict management in the extractive industries: A comparison of four mining projects in Latin America},
journal = {The Extractive Industries and Society},
volume = {13},
pages = {101161},
year = {2023},
issn = {2214-790X},
doi = {https://doi.org/10.1016/j.exis.2022.101161},
url = {https://www.sciencedirect.com/science/article/pii/S2214790X22001332},
author = {Yanina Kowszyk and Frank Vanclay and Rajiv Maher},
keywords = {Extractivism, Social conflict, Social license to operate, Extractive industries and society, Business and human rights, Environmental, Social and governance (ESG) factors},
abstract = {Increasing global demand for minerals has led to increasing socio-environmental conflicts in the mining sector. To understand why mining company managers (especially those in the areas of community relations, corporate affairs, social performance, and corporate social responsibility) fail to adequately manage conflict, we examined the conflict management approaches used by managers of large mining operations in Latin America. We interviewed a few managers per mine and analysed secondary data relating to the conflicts associated with the Nueva Unión (Chile), Peñasquito (Mexico), Vazante (Brazil), and Yanacocha (Peru) mines. We found that the conflict management style being used did not incorporate understandings from political ecology or environmental justice. We consider these perspectives could improve conflict management in the extractive industries, which would reduce the environmental and social impacts experienced by host communities, the cost of conflict borne by companies and communities, and would increase the social licence to operate of companies and their operations. We discuss various key issues including: worldviews and ontological differences; the distribution of costs and benefits from the extractive industries; power imbalance; corporate structure and strategy towards community and environmental issues; and the adequacy of the response of the national and international social justice systems.}
}
@incollection{GUPTA20201,
title = {Chapter 1 - Application and techniques of opinion mining},
editor = {Siddhartha Bhattacharyya and Václav Snášel and Deepak Gupta and Ashish Khanna},
booktitle = {Hybrid Computational Intelligence},
publisher = {Academic Press},
pages = {1-23},
year = {2020},
series = {Hybrid Computational Intelligence for Pattern Analysis and Understanding},
isbn = {978-0-12-818699-2},
doi = {https://doi.org/10.1016/B978-0-12-818699-2.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186992000019},
author = {Neha Gupta and Rashmi Agrawal},
keywords = {Opinion mining, sentiment analysis, information retrieval, business intelligence, sentiment classification, textual analysis},
abstract = {The study and analysis of human behavior using a computer modeling approach is known as opinion mining or sentiment analysis. Data mining, Web extraction, text mining, etc. are the key areas of opinion mining. Social media platforms are gaining popularity and are becoming essential components of most people’s lives. Various social networking websites, like Facebook, Twitter, and WhatsApp, are generating a huge amount of data and the mining of these data helps in discovering hidden and useful information with high potential. The calculation and evaluation of average inclinations to any opinion/sentiment toward any entity helps both the organization and the individual to get the right opinion about the ongoing trends or unfamiliar things. Various computational intelligence techniques are also used to analyze the sentiments of users. In this chapter the authors cover the fundamental concepts of opinion mining and sentiment analysis. The chapter also includes various techniques of opinion mining, along with various tools used to analyze opinions. Some key areas related to feature extraction, ontologies, and deep learning have also been discussed. Toward the end of the chapter research and future directions, along with references, have been given for further study.}
}
@article{STRIKLIEVERS201845,
title = {Sensory language across lexical categories},
journal = {Lingua},
volume = {204},
pages = {45-61},
year = {2018},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2017.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0024384117303558},
author = {Francesca {Strik Lievers} and Bodo Winter},
keywords = {Sound, Perception, Parts-of-speech, Nouns, Verbs},
abstract = {Being able to talk about what humans perceive with their senses is one of the fundamental capacities of language. But how do languages encode perceptual information? In this paper, we analyze how experiences from different senses (sight, sound, touch, taste, and smell) are encoded differentially across lexical categories (nouns, verbs, adjectives) in the English language. Three independently collected lists of perception-related words show that sound concepts are more prone to being expressed as verbs. Additional data show that nouns rated to strongly relate to motion are also rated to strongly relate to sound, more so than is the case for color-related nouns. We argue that the association of sound with verbs is due to sound concepts being inherently more dynamic, motion-related and event-based, in contrast to other sensory perceptions which are phenomenologically less strongly associated with motion and change. Overall, our results are the first to show differential encoding of perception-related concepts across different lexical categories. Our analyses of lexical patterns furthermore provide empirical evidence for the interconnection between semantics and grammar.}
}
@article{JONEK2024139,
title = {Manual assembly planning with AI Image Generators},
journal = {Procedia CIRP},
volume = {130},
pages = {139-144},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.068},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124012216},
author = {Michael Jonek and Malte Bast and Martin Manns},
keywords = {Production planning, AI-assisted planning, AI art, Assembly instructions, Manual assembly},
abstract = {For small and medium-sized enterprises (SMEs), the planning of manual assembly activities represents a significant cost and resource factor, requiring precision and meticulous organization. To ensure a stable competitive and economical production, the steps involved in manual assembly must be optimized. In today’s digital era, Artificial Intelligences (AI) offer innovative approaches and opportunities to streamline processes. In addition to LLM AIs, AI image generators are currently attracting a lot of attention as they generate very realistic and detailed images based on a given description. Images or visualisations of the situation are often used to make work instructions in manual assembly easier to understand. AI image generators can be used to visualise assembly process steps for automatically generated work instructions. In this research, a quantitative measure is proposed that can be used to rate how correctly the actual situation is depicted by highlighting incorrect or false objects and operations and assessing the accuracy of the context. The measure is validated by qualitatively evaluating images created with DALL-E 3 in an user study by both workers and planning experts from industry and comparing them with the quantitative measure. This will enable further research in the field of automated work planning and the comparison of different AI image generation tools for use in assembly planning.}
}
@article{DEY2021,
title = {A Pipeline to Understand Emerging Illness Via Social Media Data Analysis: Case Study on Breast Implant Illness},
journal = {JMIR Medical Informatics},
volume = {9},
number = {11},
year = {2021},
issn = {2291-9694},
doi = {https://doi.org/10.2196/29768},
url = {https://www.sciencedirect.com/science/article/pii/S2291969421000661},
author = {Vishal Dey and Peter Krasniak and Minh Nguyen and Clara Lee and Xia Ning},
keywords = {breast implant illness, social media, natural language processing, topic modeling},
abstract = {Background
A new illness can come to public attention through social media before it is medically defined, formally documented, or systematically studied. One example is a condition known as breast implant illness (BII), which has been extensively discussed on social media, although it is vaguely defined in the medical literature.
Objective
The objective of this study is to construct a data analysis pipeline to understand emerging illnesses using social media data and to apply the pipeline to understand the key attributes of BII.
Methods
We constructed a pipeline of social media data analysis using natural language processing and topic modeling. Mentions related to signs, symptoms, diseases, disorders, and medical procedures were extracted from social media data using the clinical Text Analysis and Knowledge Extraction System. We mapped the mentions to standard medical concepts and then summarized these mapped concepts as topics using latent Dirichlet allocation. Finally, we applied this pipeline to understand BII from several BII-dedicated social media sites.
Results
Our pipeline identified topics related to toxicity, cancer, and mental health issues that were highly associated with BII. Our pipeline also showed that cancers, autoimmune disorders, and mental health problems were emerging concerns associated with breast implants, based on social media discussions. Furthermore, the pipeline identified mentions such as rupture, infection, pain, and fatigue as common self-reported issues among the public, as well as concerns about toxicity from silicone implants.
Conclusions
Our study could inspire future studies on the suggested symptoms and factors of BII. Our study provides the first analysis and derived knowledge of BII from social media using natural language processing techniques and demonstrates the potential of using social media information to better understand similar emerging illnesses.}
}
@article{YAO2020760,
title = {MicroPhenoDB Associates Metagenomic Data with Pathogenic Microbes, Microbial Core Genes, and Human Disease Phenotypes},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {18},
number = {6},
pages = {760-772},
year = {2020},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2020.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1672022920301698},
author = {Guocai Yao and Wenliang Zhang and Minglei Yang and Huan Yang and Jianbo Wang and Haiyue Zhang and Lai Wei and Zhi Xie and Weizhong Li},
keywords = {Pathogenic microbes, Metagenomic data, Disease phenotypes, Microbe-disease association, COVID-19},
abstract = {Microbes play important roles in human health and disease. The interaction between microbes and hosts is a reciprocal relationship, which remains largely under-explored. Current computational resources lack manually and consistently curated data to connect metagenomic data to pathogenic microbes, microbial core genes, and disease phenotypes. We developed the MicroPhenoDB database by manually curating and consistently integrating microbe-disease association data. MicroPhenoDB provides 5677 non-redundant associations between 1781 microbes and 542 human disease phenotypes across more than 22 human body sites. MicroPhenoDB also provides 696,934 relationships between 27,277 unique clade-specific core genes and 685 microbes. Disease phenotypes are classified and described using the Experimental Factor Ontology (EFO). A refined score model was developed to prioritize the associations based on evidential metrics. The sequence search option in MicroPhenoDB enables rapid identification of existing pathogenic microbes in samples without running the usual metagenomic data processing and assembly. MicroPhenoDB offers data browsing, searching, and visualization through user-friendly web interfaces and web service application programming interfaces. MicroPhenoDB is the first database platform to detail the relationships between pathogenic microbes, core genes, and disease phenotypes. It will accelerate metagenomic data analysis and assist studies in decoding microbes related to human diseases. MicroPhenoDB is available through http://www.liwzlab.cn/microphenodb and http://lilab2.sysu.edu.cn/microphenodb.}
}
@article{CHALKIADAKI2025100413,
title = {GABA/Glutamate Neuron Differentiation Imbalance and Increased AKT/mTOR Signaling in CNTNAP2−/− Cerebral Organoids},
journal = {Biological Psychiatry Global Open Science},
volume = {5},
number = {1},
pages = {100413},
year = {2025},
issn = {2667-1743},
doi = {https://doi.org/10.1016/j.bpsgos.2024.100413},
url = {https://www.sciencedirect.com/science/article/pii/S2667174324001265},
author = {Kleanthi Chalkiadaki and Elpida Statoulla and Maria Zafeiri and Georgia Voudouri and Theoklitos Amvrosiadis and Alexandra Typou and Niki Theodoridou and Dimitrios Moschovas and Apostolos Avgeropoulos and Martina Samiotaki and John O. Mason and Christos G. Gkogkas},
keywords = {AKT/mTOR, Autism, Cerebral organoids, CNTNAP2, GABA/glutamate, Neurodevelopment},
abstract = {Background
The polygenic nature of autism spectrum disorder (ASD) requires the identification of converging genetic pathways during early development to elucidate its complexity and varied manifestations.
Methods
We developed a human cerebral organoid model from induced pluripotent stem cells with targeted genome editing to abolish protein expression of the CNTNAP2 ASD risk gene.
Results
CNTNAP2−/− cerebral organoids displayed accelerated cell cycle, ventricular zone disorganization, and increased cortical folding. Proteomic analysis revealed disruptions in glutamatergic/GABAergic (gamma-aminobutyric acidergic) synaptic pathways and neurodevelopment, and transcriptomic analysis revealed differentially expressed genes belonging to inhibitory neuron–related gene networks. Interestingly, there was a weak correlation between the 2 datasets, suggesting nuanced translational control mechanisms. Along these lines, we found upregulated AKT/mTOR (mechanistic target of rapamycin) signaling in CNTNAP2−/− organoids. Spatial transcriptomic analysis of CNTNAP2−/− ventricular-like zones demonstrated pervasive changes in gene expression, implicating upregulation of cell cycle regulation, synaptic, and glutamatergic/GABAergic pathways. We noted significant overlap of all day-30 organoid omics datasets differentially expressed genes from idiopathic ASD (macrocephaly) induced pluripotent stem cell–derived telencephalic organoids, where FOXG1 was upregulated. Moreover, we detected increased GAD1-expressing and decreased TBR1-expressing cells, suggesting altered GABAergic/glutamatergic neuron development.
Conclusions
These findings potentially highlight a shared mechanism in the early cortical development of various forms of ASD, further elucidate the role of CNTNAP2 in ASD pathophysiology and cortical development, and pave the way for targeted therapies that use cerebral organoids as preclinical models.}
}
@article{AFZAL2023e12202,
title = {Revealing genetic links of Type 2 diabetes that lead to the development of Alzheimer’s disease},
journal = {Heliyon},
volume = {9},
number = {1},
pages = {e12202},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e12202},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022034909},
author = {Muhammad Afzal and Khalid Saad Alharbi and Sami I. Alzarea and Najiah M. Alyamani and Imran Kazmi and Emine Güven},
keywords = {Alzheimer’s disease, Type 2 diabetes mellitus, Neovascular unit, Differential expression, Ageing brain},
abstract = {Background
A factor leading to Alzheimer’s Disease (AD), portrayed by peripheral insulin resistance, is Type 2 diabetes mellitus (T2D). The likelihood of T2D cases would be at boosted danger in alternating AD cases has severe social consequences. Several genes have been detected via gene expression profiling or different techniques; despite the consideration of the utility of numerous of these genes stays insufficient.
Methods
This project is designed to uncover the mutual genomics motifs between AD and T2D via non-negative matrix factorization (NMF) of differentially expressed genes (DEGs) of T2D Mellitus of human cortical neurons of the neurovascular unit gene expression data. A rank factorization value is calculated by employing the combination of the NMF model with the unit invariant knee (UIK) point method. The metagenes are further determined by remarking the enriched Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway and gene ontology (GO) enrichment tools. In this study, the most highly expressed genes of metagenes are subjected to protein-protein interaction (PPI) network study to discover the most significant biomarkers of T2D Mellitus in the ageing brain.
Results
We screened the most important shared genes (CDKN1A, COL22A1, EIF4A, GFAP, SLC1A1, and VIM) and essential human molecular pathways that motivate these diseases. The study aimed to validate the most significant hub genes using network-based methods which detected the corresponding relationship between AD and T2D.
Conclusions
Using in silico tools, the computational pipeline has broadly examined transformed pathways and discovered promising biomarkers and drug targets. We validated the most significant hub genes using network-based methods which detected the corresponding relationship between AD and T2D. These consequences on brain cells hypothetically reserve to diabetic Alzheimer’s so-called type 3 diabetes (T3D) and may offer promising methodologies for curative intrusion.}
}
@article{PANG2025100579,
title = {Integrating network pharmacology and experimental validation to explore the mechanism of action of BushenQiangxin Formula for the treatment of chronic heart failure},
journal = {Pharmacological Research - Modern Chinese Medicine},
volume = {14},
pages = {100579},
year = {2025},
issn = {2667-1425},
doi = {https://doi.org/10.1016/j.prmcm.2025.100579},
url = {https://www.sciencedirect.com/science/article/pii/S2667142525000089},
author = {Jinhua Pang and Jiejun Hou and Jinyi Chen and Wenchu Zhao and Tiantian Tang and Taotao Li and Ding Liu and Jinkai Li and Xuan Wang},
keywords = {BushenQiangxin Formula, chronic heart failure, network pharmacology, molecular docking;IL-17 signaling pathway;IL-6},
abstract = {Objective
: To evaluate the mechanism of action of the BushenQiangxin (BSQX) formula in treating chronic heart failure (CHF) based on network pharmacology and experimental zoology.
Methods
: The targets and components of BSQX in CHF were assessed based on network pharmacology and data from the Gene Expression Omnibus (GEO) database. The drug–disease interaction network diagram was constructed, and the possible pathways affected by BSQX in CHF were analyzed. Molecular docking between the key components of BSQX and the key targets of CHF was performed using the Discovery Studio software. Finally, a CHF rat model was established. The serum interleukin (IL)-6 level was evaluated using the enzyme-linked immunosorbent assay in each rat group. The structural changes in the left ventricular myocardial tissues were observed via hematoxylin & eosin staining, and the expression of cTnI, IL-17, and nuclear factor kappa B (NF-KB) proteins was measured via immunohistochemistry in the cardiac muscle tissue.
Results
: Network pharmacology and data from the GSE84796 database were used to screen 25 core targets of BSQX for CHF. The major signaling pathways, including the IL-17 signaling pathway, tumor necrosis factor signaling pathway, AGE-RAGE signaling pathway, which are associated with diabetic complications, lipid, and atherosclerosis were identified via Gene Ontology and Kyoto Encyclopedia of Genes and Genomes enrichment analysis. The molecular docking results showed good binding between the core targets (IL6, MMP9, CCL2, and STAT1) and active compounds (such as quercetin, lignocerol, oleanolic acid, pentacosanoic acid, and isobetulinic acid). Hematoxylin & eosin staining in animal experiments showed that BSQX could reduce the degree of inflammatory infiltration and fibrosis of cardiomyocytes in CHF to a certain extent. The enzyme-linked immunosorbent assay and immunohistochemistry showed that BSQX could effectively reduce the levels of IL-17, IL-6, cTnI, and NF-KB in chronic rats.
Conclusion
: In CHF treatment, the mechanism of BSQX can be related to the inhibition of IL-17, IL-6, cTnI, and NF-KB production, inflammatory cell infiltration, and the attenuation of myocardial injury based on network pharmacology and experimental animal studies. Moreover, its mechanism of action may function via the IL-17 signaling pathway.}
}
@article{SORRELL20181267,
title = {Explaining sociotechnical transitions: A critical realist perspective},
journal = {Research Policy},
volume = {47},
number = {7},
pages = {1267-1282},
year = {2018},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2018.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0048733318300891},
author = {Steve Sorrell},
keywords = {Multilevel perspective, Critical realism, Emergence, Process theory},
abstract = {This paper identifies and evaluates the explicit and implicit philosophical assumptions underlying the so-called multilevel perspective on sociotechnical transitions (MLP). These include assumptions about the nature of reality (ontology), the status of claims about that reality (epistemology) and the appropriate choice of research methods The paper assesses the consistency of these assumptions with the philosophical tradition of critical realism and uses this tradition to highlight a number of potential weaknesses of the MLP. These include: the problematic conception of social structure and the misleading priority given to intangible rules; the tendency to use theory as a heuristic device rather than causal explanation; the ambition to develop an extremely versatile framework rather than testing competing explanations; the relative neglect of the necessity or contingency of particular causal mechanisms; and the reliance upon single, historical case studies with insufficient use of comparative methods. However, the paper also concludes that the flexibility of the MLP allows room for reconciliation, and provides some suggestions on how that could be achieved – including proposing an alternative, critical realist interpretation of sociotechnical systems.}
}
@article{JOSERUIZDEMENDOZAIBANEZ2023103541,
title = {Structural similarity in figurative language: A preliminary cognitive analysis},
journal = {Lingua},
volume = {290},
pages = {103541},
year = {2023},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2023.103541},
url = {https://www.sciencedirect.com/science/article/pii/S0024384123000657},
author = {Francisco {José Ruiz de Mendoza Ibáñez} and María {Sandra Peña Cervel}},
keywords = {Allegory, Analogy, Fable, Parable, Paragon-based antonomasia, Structural similarity},
abstract = {Structural similarity may be based on the structural characteristics of concrete entities (e.g., the heart relates to blood circulation in the way that a pump to a hydraulic system) or on the structural properties of situations and events (e.g., Your words were a dagger to my heart compares the emotional damage done by the hearer’s words to the physical harm caused by a dagger). In combination with metonymy, structural similarity gives rise to paragon-based antonomasia and allegory-like narratives. An example of paragon-based antonomasia is the Lennon of football, which, said about a great player, is based on structural similarity: the player and the musician are masters, each in his domain of expertise. Allegory-like narratives rely on a form of high-level structural similarity where each entity-denoting target element is elaborated through the member-for-class metonymy. For example, in “The Prodigal Son”, the regretful son’s return to his father asking for forgiveness represents any repentant sinner. In terms of structural similarity, God is to a repentant sinner what the forgiving father is to his returning son. Drawing on a selection of examples, this article reexamines the contribution of different types of structural similarity to figurative reasoning at various degrees of abstractness and complexity.}
}
@incollection{BURSE202131,
title = {Chapter 4 - Semantic interoperability: the future of healthcare},
editor = {Sarika Jain and Vishal Jain and Valentina Emilia Balas},
booktitle = {Web Semantics},
publisher = {Academic Press},
pages = {31-53},
year = {2021},
isbn = {978-0-12-822468-7},
doi = {https://doi.org/10.1016/B978-0-12-822468-7.00018-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128224687000183},
author = {Rashmi Burse and Michela Bertolotto and Dympna O'Sullivan and Gavin McArdle},
keywords = {Healthcare interoperability, fast healthcare interoperability resources (FHIR), semantic web technology, resource description framework schema (RDFS), web ontology language (OWL), protocol and RDF query language (SPARQL), medical ontologies, SNOMED},
abstract = {Healthcare interoperability has been a major challenge since the early 1980s and still remains an unsolved issue. Attempts in the past have addressed this problem using bespoke solutions, but a generic solution still eludes the healthcare community. This chapter discusses the various levels at which interoperability needs to be preserved for an uninterrupted exchange of healthcare information. The chapter reviews the various healthcare standards developed in an attempt to solve the interoperability problem at a syntactic level and then moves on to examine medical ontologies developed to solve the problem at a semantic level. The chapter explains the features of semantic web technology that can be leveraged at each level. A literature survey is carried out to gage the current contribution of semantic web technologies in this area along with an analysis of how semantic web technologies can be improved to better suit the health informatics domain and solve the healthcare interoperability challenge.}
}
@article{RORABAUGH2019822,
title = {Hunting social networks on the Salish Sea before and after the bow and arrow},
journal = {Journal of Archaeological Science: Reports},
volume = {23},
pages = {822-843},
year = {2019},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2018.11.040},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X18304954},
author = {Adam N. Rorabaugh},
keywords = {Social network analysis, Cultural transmission theory, Bow and arrow technology, Hunter-gatherer-fishers, Political autonomy, Salish Sea},
abstract = {Archaeologists have paid substantial attention to the social transformations coinciding with the widespread adoption of bow and arrow technologies. Social network analysis (SNA) is used to examine stone tool assemblages from the Salish Sea. SNA while widely applied a wide range of problems in lithic technologies has been an underutilized approach in the Pacific Northwest. Based on an application of cultural transmission theory, ethnography, and Coast Salish ontology, that haft styles reflect corporate group connections. Changes in the social networks are examined as reflected in haft styles from 3500 to 1000 BP, a time of shifts towards large plank house villages and the emergence of hereditary forms of social inequality in the region. Five social networks were constructed, each covering a 500-year period, to assess shifts in regional connections through time. There appears to be increased elaboration of social networks throughout the Salish Sea until 1600 BP, when the bow and arrow become widely adopted. These data suggest SNA of lithic haft styles shows a shift in hunting organization from a collective corporate group level activity to an individualized pursuit. The findings show the utility of SNA to address oscillations in Salish Sea society over time. New directions for future studies to examine shifts in corporate group relations in other aspects of precontact Coast Salish society are also provided.}
}
@article{LUO2019105,
title = {Construction and analysis of a dysregulated lncRNA-associated ceRNA network in a rat model of temporal lobe epilepsy},
journal = {Seizure},
volume = {69},
pages = {105-114},
year = {2019},
issn = {1059-1311},
doi = {https://doi.org/10.1016/j.seizure.2019.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S1059131118305715},
author = {Zhao Hui Luo and Alsharafi {Walid A} and Yuanyuan Xie and Hongyu Long and Wenbiao Xiao and Liqun Xu and Yujiao Fu and Li Feng and Bo Xiao},
keywords = {Long non-coding RNAs (lncRNAs), microRNAs (miRNAs), Competing endogenous RNA (ceRNA), Temporal lobe epilepsy (TLE), DAVID, Pathway},
abstract = {Purpose
The aim of this work was to investigate expression and cross-talk between long noncoding RNAs (lncRNAs) and microRNAs (miRNAs) in a rat model of temporal lobe epilepsy (TLE).
Methods
Noncoding RNA chips were used to explore the expression and relationship between lncRNAs and miRNAs in a rat model of TLE. The expression of different lncRNAs and mRNAs was analysed by Pearson’s correlation coefficient, and the function of each lncRNA was annotated by co-expressed genes based on gene ontology classification using DAVID. MiRNA-lncRNA interactions were predicted by using StarBase v2.0, and the competing endogenous RNA (ceRNA) relationship between lncRNAs and miRNAs was built by using Cytoscape software. Real-time PCR was used to verify chip results.
Results
According to the expression profile analysis, 54 lncRNAs, 36 miRNAs and 122 mRNAs were dysregulated in TLE rat model compared to normal controls. The functions of lncRNAs in epilepsy were annotated by their co-expressed genes based on the “guilt by association” strategy. DAVID analysis revealed that differentially expressed lncRNA functions were involved in “potassium channel activity”, “metal ion transmembrane transporter activity”, and “voltage-gated potassium channel activity”. Based on the ceRNA theory, 13 mRNAs, 10 miRNAs and 11 lncRNAs comprise the lncRNA-miRNA-mRNA ceRNA relationship in epilepsy.
Conclusions
The molecular functions of the differentially expressed genes play an important role in the pathogenesis of voltage-gated potassium channel activity. Further ceRNA analyses suggest that modulation of lncRNAs could emerge as a promising therapeutic target for TLE.}
}
@article{SANTOS201811,
title = {Partial meet pseudo-contractions},
journal = {International Journal of Approximate Reasoning},
volume = {103},
pages = {11-27},
year = {2018},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2018.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X1830104X},
author = {Yuri David Santos and Vinícius Bitencourt Matos and Márcio Moretto Ribeiro and Renata Wassermann},
keywords = {Belief revision, Limited reasoning, Ontologies, Pseudo-contractions},
abstract = {In the AGM paradigm for belief revision, epistemic states are represented by logically closed sets of sentences, the so-called belief sets. An alternative approach uses belief bases, arbitrary sets of sentences. Both approaches have their problems when it comes to contraction operations. Belief bases are more expressive, but, at the same time, they present a serious syntax dependence. Between those two extremes lie a whole gamut of operations called pseudo-contractions, some of which may be interesting alternatives to the classical ones, providing a good balance between syntax dependence and expressivity. In this paper we explore some very natural and general constructions for pseudo-contractions, showing some of their properties and giving their axiomatic characterizations. We also illustrate possible practical scenarios where they can be employed.}
}
@article{PRESTON2023100726,
title = {Toward structuring real-world data: Deep learning for extracting oncology information from clinical text with patient-level supervision},
journal = {Patterns},
volume = {4},
number = {4},
pages = {100726},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100726},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923000661},
author = {Sam Preston and Mu Wei and Rajesh Rao and Robert Tinn and Naoto Usuyama and Michael Lucas and Yu Gu and Roshanthi Weerasinghe and Soohee Lee and Brian Piening and Paul Tittel and Naveen Valluri and Tristan Naumann and Carlo Bifulco and Hoifung Poon},
keywords = {natural language processing, L01.224.050.375.580, data mining, L01.313.500.750.280.199, medical oncology, H02.403.429.515, neoplasm staging, E01.789.625},
abstract = {Summary
Most detailed patient information in real-world data (RWD) is only consistently available in free-text clinical documents. Manual curation is expensive and time consuming. Developing natural language processing (NLP) methods for structuring RWD is thus essential for scaling real-world evidence generation. We propose leveraging patient-level supervision from medical registries, which are often readily available and capture key patient information, for general RWD applications. We conduct an extensive study on 135,107 patients from the cancer registry of a large integrated delivery network (IDN) comprising healthcare systems in five western US states. Our deep-learning methods attain test area under the receiver operating characteristic curve (AUROC) values of 94%–99% for key tumor attributes and comparable performance on held-out data from separate health systems and states. Ablation results demonstrate the superiority of these advanced deep-learning methods. Error analysis shows that our NLP system sometimes even corrects errors in registrar labels.}
}
@incollection{2025539,
title = {Index},
editor = {Boris Galitsky},
booktitle = {Healthcare Applications of Neuro-Symbolic Artificial Intelligence},
publisher = {Academic Press},
pages = {539-555},
year = {2025},
isbn = {978-0-443-30046-2},
doi = {https://doi.org/10.1016/B978-0-443-30046-2.00021-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300462000211}
}
@article{XU2018146,
title = {Sensing and detecting traffic events using geosocial media data: A review},
journal = {Computers, Environment and Urban Systems},
volume = {72},
pages = {146-160},
year = {2018},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2018.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0198971518300164},
author = {Shishuo Xu and Songnian Li and Richard Wen},
keywords = {Traffic event, Event detection, Geosocial media, Twitter data stream},
abstract = {Social media platforms, or social networks, have allowed millions of users to post online content about topics related to our daily lives. Traffic is one of the many topics for which users generate content. People tend to post traffic related messages through the ever-expanding geosocial media platforms. Monitoring and analyzing this rich and continuous user-generated content can yield unprecedentedly valuable traffic related information, which can be mined to extract traffic events to enable users and organizations to acquire actionable knowledge. A great number of literature has reported on the methods developed for detecting traffic information from social media data, especially geosocial media data when geo-tagged. However, a systematic review to synthesize the state-of-the-art developments is missing. This paper presents a systematic review of a wide variety of techniques applied in detecting traffic events from geosocial media data, arranged based on their adoption in each stage of an event detection framework developed from the literature review. The paper also highlights some challenges and potential solutions. The aim of the paper is to provide a structured view on current state-of-art of the geosocial media based traffic event detection techniques, which can help researchers carry out further research in this area.}
}
@article{WERNER2024122495,
title = {Extracting section structure from resumes in Brazilian Portuguese},
journal = {Expert Systems with Applications},
volume = {242},
pages = {122495},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122495},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423029974},
author = {Matheus Werner and Eduardo Laber},
keywords = {Resume parsing, Natural language processing, Information extraction, Image classification, Text segmentation, Human resources},
abstract = {This paper presents a novel resume parser designed to effectively reorganize the textual content of any resume into its original section structure. Our work addresses two practical challenges overlooked by the existing literature: (i) ensuring the correct reading order of text retrieved from resume files and (ii) extracting individually all sections, as well as work experience and education subsections. By taking into account the observation that most resumes adhere to basic document templates, we reframe the reading order problem as a template identification task. Our experiments suggest that even a widely-used small model like EfficientNet-B0 can accurately identify common templates. Additionally, we propose a sequence tagging approach that simultaneously identifies all resume sections and some subsections. We implement and compare two solutions based on the well-known CRF and BERT models. Our evaluation provides strong evidence that the CRF can serve as a practical alternative to BERT, depending on hardware and budget constraints. They yield comparable results in terms of identifying resume sections, while BERT displays a substantial advantage when identifying education and work experience subsections. An interesting direction for future work is to expand our approach to ensure the correct ordering of a large family of templates.}
}
@article{GIUSTOZZI2019620,
title = {Abnormal Situations Interpretation in Industry 4.0 using Stream Reasoning},
journal = {Procedia Computer Science},
volume = {159},
pages = {620-629},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.217},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314012},
author = {Franco Giustozzi and Julien Saunier and Cecilia Zanni-Merk},
keywords = {Industry 4.0, Stream Reasoning, Condition Monitoring, Ontology},
abstract = {With the coming era of Industry 4.0, more assets and machines in plants are equipped with sensors which collect big amount of data for effective on-line equipment condition monitoring. Monitoring equipment conditions can not only reduce unplanned downtime by early detection of relevant situations like anomalies but also avoid unnecessary routine maintenance. For the detection of these situations it is necessary to integrate distributed, heterogeneous data sources and data streams. In this context, semantic web technologies are increasingly considered as key technologies to improve data integration. However, they are mainly used for data that is assumed not to change very often in time. In order to tackle this issue, stream reasoning combines reasoning and stream processing methods. Such a combination enables the processing of dynamic and heterogeneous data continuously produced from a large number of sources and implementing real-time services. This paper presents an approach that uses stream reasoning to identify in real time certain situations that lead to potential failures. Early detection enables to choose the most appropriate decision to avoid the interruption of manufacturing processes. In order to achieve this, data collected from sensors are enriched with contextual information. The use of stream reasoning allows the integration of data from different data sources, with different underlying meanings, different temporal resolutions as well as the processing of these data in real time.}
}
@article{QIAN2019118379,
title = {A cloud service platform integrating additive and subtractive manufacturing with high resource efficiency},
journal = {Journal of Cleaner Production},
volume = {241},
pages = {118379},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.118379},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619332494},
author = {Cheng Qian and Yingfeng Zhang and Yang Liu and Zhe Wang},
keywords = {Cloud manufacturing, Additive manufacturing, Subtractive manufacturing, Resource efficiency},
abstract = {Cloud manufacturing has been studied for years, yet commercial implementations are still limited. The recent advances in information technology have stimulated the free sharing of additive and subtractive manufacturing (A/SM) resources through cloud services. Currently, due to the lack of a general method to model manufacturing capabilities, as well as the absence of an open platform to integrate business and manufacturing processes, it is difficult to integrate A/SM resources within one platform efficiently and seamlessly. In this research, a service encapsulation model for A/SM resources was described using ontology modeling technique. A collaborative cloud platform integrating A/SM was designed that can provide optimal production plans considering time, cost, quality, and energy waste during manufacturing. The proposed platform and models were demonstrated by a prototype system and tested in a case study, which showed the integrated platform can increase the utilization rate of resources while reducing energy consumption. This research has provided a practical tool for virtualization, integration, and configuration of A/SM resource with high efficiency.}
}
@article{MATA2018837,
title = {A cross-domain framework for designing healthcare mobile applications mining social networks to generate recommendations of training and nutrition planning},
journal = {Telematics and Informatics},
volume = {35},
number = {4},
pages = {837-853},
year = {2018},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2017.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0736585316305020},
author = {Felix Mata and Miguel Torres-Ruiz and Roberto Zagal and Giovanni Guzman and Marco Moreno-Ibarra and Rolando Quintero},
abstract = {Nowadays, people are practicing physical exercise in order to maintain good health conditions. Such physical workouts are required by a plan, which should be designed and supervised by sport specialists and medical assistants. Thus, the exercise sessions shall start with consultation of a coach, doctor and dietician; however, many times this scenario is not presented. In typical activities such as running, cycling and fitness, people use health mobile apps with their smartphones, which offer support for these activities. Nevertheless, the functionality and operation of these applications are isolated, because many and long questionnaires are performed. Additionally, the physical and health state of a user is not considered. These issues would be taken into account for determining recommendations about the time for doing exercise and the kind of activity for each person. In this work, a social semantic mobile framework to generate recommendations where a mobile application allows sensing the physical performance, taking into consideration medical criteria with smartphones is proposed. The approach includes a semantic cross-information that comes from social network and official data as well as sport activities and medical knowledge. This knowledge is translated into application ontologies related directly to health, nutrition and training domains. The methodology also covers physical fitness tests and a monitoring tool for evaluating the nutrition plan and the correct execution of the training. As case study, the mobile application offers to evaluate the physical and health conditions of a runner, automatically generate a nutrition plan and training, monitor plans and recomputed them if users make changes in their routines. The data provided from the social network are used as feedback in the application, in order to make the training and nutrition plans more flexible by applying spatio-temporal analysis based on machine learning. Finally, the generated training and nutrition plans were validated by specialists, they have demonstrated 82% of effectiveness rate in exercise training routines and 86% in nutrition plans. In addition, the results were compared with isolated approaches and manual recommendations made by specialists, the obtained overall performance was 81%.}
}
@article{ZHANG2025111920,
title = {M3NetFlow: A multi-scale multi-hop graph AI model for integrative multi-omic data analysis},
journal = {iScience},
volume = {28},
number = {3},
pages = {111920},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.111920},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225001804},
author = {Heming Zhang and S. Peter Goedegebuure and Li Ding and David DeNardo and Ryan C. Fields and Michael Province and Yixin Chen and Philip Payne and Fuhai Li},
keywords = {Biocomputational method, Complex systems, Omics},
abstract = {Summary
Multi-omic data-driven studies are at the forefront of precision medicine by characterizing complex disease signaling systems across multiple views and levels. The integration and interpretation of multi-omic data are critical for identifying disease targets and deciphering disease signaling pathways. However, it remains an open problem due to the complex signaling interactions among many proteins. Herein, we propose a multi-scale multi-hop multi-omic network flow model, M3NetFlow, to facilitate both hypothesis-guided and generic multi-omic data analysis tasks. We evaluated M3NetFlow using two independent case studies: (1) uncovering mechanisms of synergy of drug combinations (hypothesis/anchor-target guided multi-omic analysis) and (2) identifying biomarkers of Alzheimer’s disease (generic multi-omic analysis). The evaluation and comparison results showed that M3NetFlow achieved the best prediction accuracy and identified a set of drug combination synergy- and disease-associated targets. The model can be directly applied to other multi-omic data-driven studies.}
}
@article{MARTINEZ2025106248,
title = {A conceptual data modeling framework with four levels of abstraction for environmental information},
journal = {Environmental Modelling & Software},
volume = {183},
pages = {106248},
year = {2025},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2024.106248},
url = {https://www.sciencedirect.com/science/article/pii/S1364815224003098},
author = {David Martínez and Laura Po and Raquel Trillo-Lado and José R.R. Viqueira},
keywords = {Conceptual data modeling, Environmental data, Data management, Meteorological data, Oceanographic data, Air quality data, Data integration},
abstract = {Environmental data generated by observation infrastructures and models is widely heterogeneous in both structure and semantics. The design and implementation of an ad hoc data model for each new dataset is costly and creates barriers for data integration. On the other hand, designing a single data model that supports any kind of environmental data has shown to be a complex task, and the resulting tools do not provide the required efficiency. In this paper, a new data modeling framework is proposed that enables the reuse of generic structures among different application domains and specific applications. The framework considers four levels of abstraction for the data models. Levels 1 and 2 provide general data model structures for environmental data, based on those defined by the Observations and Measurements (O&M) standard of the Open Geospatial Consortium (OGC). Level 3 incorporates generic data models for different application areas, whereas specific application models are designed at Level 4, reusing structures of the previous levels. Various use cases were implemented to illustrate the capabilities of the framework. A performance evaluation using six datasets of three different use cases has shown that the query response times achieved over the structures of Level 4 are very good compared to both ad hoc models and to a direct implementation of O&M in a Sensor Observation Service (SOS) tool. A qualitative evaluation shows that the framework fulfills a collection of general requirements not supported by any other existing solution.}
}
@article{SHABANI202422,
title = {Comprehensive microarray analysis of severe preeclampsia placenta to identify differentially expressed genes, biological pathways, hub genes, and their related non-coding RNAs},
journal = {Placenta},
volume = {155},
pages = {22-31},
year = {2024},
issn = {0143-4004},
doi = {https://doi.org/10.1016/j.placenta.2024.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0143400424006131},
author = {Maedeh Shabani and Maryam Eghbali and Ameneh Abiri and Maryam Abiri},
keywords = {Severe preeclampsia, Placenta, Microarray analysis, System biology, Gene ontology},
abstract = {Introduction
Preeclampsia (PE) is a serious pregnancy-related complication caused by high blood pressure in pregnant women. The severe form has more devastating effects. According to the growing evidence, the placenta is a crucial component in the pathogenesis of PE, and eliminating it will alleviate symptoms.
Methods
GEO's severe preeclampsia placenta microarray datasets; GSE147776, GSE66273, GSE102897, and GSE10588, were chosen to identify differentially expressed genes (DEGs) in different biological pathways. The analysis of hub genes and related non-coding RNAs was done as well.
Results
A total of 347 DEGs with adj p-value <0.05 and ǀlog2FoldChangeǀ> 0.5 were discovered between severe PEs and healthy pregnancies, including 204 over-expressed genes and 143 under-expressed genes. The MCC method identified ISG15, IFI44L, MX2, OAS2, MX1, FN1, LDHA, ITGB3, TKT, HK2 genes as the top ten hub genes. Interactions between hub genes and noncoding RNAs were also conducted. The most enriched pathways were as follows; HIF-1 signaling pathway; Pathways in cancer; Alanine, aspartate and glutamate metabolism; Arginine biosynthesis; Human papillomavirus infection; Glycolysis/Gluconeogenesis; Central carbon metabolism in cancer; Valine, leucine and isoleucine degradation; Cysteine and methionine metabolism; and Galactose metabolism.
Discussion
This is a secondary data analysis conducted on severe preeclampsia placenta to identify differentially expressed genes, biological pathways, hub-genes, and related noncoding RNAs. Functional studies are crucial to understanding the precise role of these genes in the pathogenesis of PE. Also, accepting a gene as a diagnostic or prognostic marker for early diagnosis and management of PE requires multiple lines of evidence.}
}
@article{SAKA2023101869,
title = {Conversational artificial intelligence in the AEC industry: A review of present status, challenges and opportunities},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101869},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101869},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622003275},
author = {Abdullahi B. Saka and Lukumon O. Oyedele and Lukman A. Akanbi and Sikiru A. Ganiyu and Daniel W.M. Chan and Sururah A. Bello},
keywords = {Artificial Intelligence, Conversational artificial intelligence, Conversational agents, Chatbot},
abstract = {The idea of developing a system that can converse and understand human languages has been around since the 1200 s. With the advancement in artificial intelligence (AI), Conversational AI came of age in 2010 with the launch of Apple’s Siri. Conversational AI systems leveraged Natural Language Processing (NLP) to understand and converse with humans via speech and text. These systems have been deployed in sectors such as aviation, tourism, and healthcare. However, the application of Conversational AI in the architecture engineering and construction (AEC) industry is lagging, and little is known about the state of research on Conversational AI. Thus, this study presents a systematic review of Conversational AI in the AEC industry to provide insights into the current development and conducted a Focus Group Discussion to highlight challenges and validate areas of opportunities. The findings reveal that Conversational AI applications hold immense benefits for the AEC industry, but it is currently underexplored. The major challenges for the under exploration were highlighted and discusses for intervention. Lastly, opportunities and future research directions of Conversational AI are projected and validated which would improve the productivity and efficiency of the industry. This study presents the status quo of a fast-emerging research area and serves as the first attempt in the AEC field. Its findings would provide insights into the new field which be of benefit to researchers and stakeholders in the AEC industry.}
}
@article{KOGAN2020103587,
title = {Towards a goal-oriented methodology for clinical-guideline-based management recommendations for patients with multimorbidity: GoCom and its preliminary evaluation},
journal = {Journal of Biomedical Informatics},
volume = {112},
pages = {103587},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103587},
url = {https://www.sciencedirect.com/science/article/pii/S153204642030215X},
author = {Alexandra Kogan and Mor Peleg and Samson W. Tu and Raviv Allon and Natanel Khaitov and Irit Hochberg},
keywords = {Multimorbidity, Comorbidity, Decision-support, Computer-interpretable guidelines},
abstract = {Patients with chronic multimorbidity are becoming more common as life expectancy increases, making it necessary for physicians to develop complex management plans. We are looking at the patient management process as a goal-attainment problem. Hence, our aim is to develop a goal-oriented methodology for providing decision support for managing patients with multimorbidity continuously, as the patient’s health state is progressing and new goals arise (e.g., treat ulcer, prevent osteoporosis). Our methodology allows us to detect and mitigate inconsistencies among guideline recommendations stemming from multiple clinical guidelines, while consulting medical ontologies and terminologies and relying on patient information standards. This methodology and its implementation as a decision-support system, called GoCom, starts with computer-interpretable clinical guidelines (CIGs) for single problems that are formalized using the PROforma CIG language. We previously published the architecture of the system as well as a CIG elicitation guide for enriching PROforma tasks with properties referring to vocabulary codes of goals and physiological effects of management plans. In this paper, we provide a formalization of the conceptual model of GoCom that generates, for each morbidity of the patient, a patient-specific goal tree that results from the PROforma engine’s enactment of the CIG with the patient’s data. We also present the “Controller” algorithm that drives the GoCom system. Given a new problem that a patient develops, the Controller detects inconsistencies among goals pertaining to different comorbid problems and consults the CIGs to generate alternative non-conflicted and goal-oriented management plans that address the multiple goals simultaneously. In this stage of our research, the inconsistencies that can be detected are of two types – starting vs. stopping medications that belong to the same medication class hierarchy, and detecting opposing physiological effect goals that are specified in concurrent CIGs (e.g., decreased blood pressure vs. increased blood pressure). However, the design of GoCom is modular and generic and allows the future introduction of additional interaction detection and mitigation strategies. Moreover, GoCom generates explanations of the alternative non-conflicted management plans, based on recommendations stemming from the clinical guidelines and reasoning patterns. GoCom’s functionality was evaluated using three cases of multimorbidity interactions that were checked by our three clinicians. Usefulness was evaluated with two studies. The first evaluation was a pilot study with ten 6th year medical students and the second evaluation was done with 27 6th medical students and interns. The participants solved complex realistic cases of multimorbidity patients: with and without decision-support, two cases in the first evaluation and 6 cases in the second evaluation. Use of GoCom increased completeness of the patient management plans produced by the medical students from 0.44 to 0.71 (P-value of 0.0005) in the first evaluation, and from 0.31 to 0.78 (P-value < 0.0001) in the second evaluation. Correctness in the first evaluation was very high with (0.98) or without the system (0.91), with non-significant difference (P-value ≥ 0.17). In the second evaluation, use of GoCom increased correctness from 0.68 to 0.83 (P-value of 0.001). In addition, GoCom’s explanation and visualization were perceived as useful by the vast majority of participants. While GoCom’s detection of goal interactions is currently limited to detection of starting vs. stopping the same medication or medication subclasses and detecting conflicting physiological effects of concurrent medications, the evaluation demonstrated potential of the system for improving clinical decision-making for multimorbidity patients.}
}
@article{BAARS2018145,
title = {An assessment of the scientific status of anthroposophic medicine, applying criteria from the philosophy of science},
journal = {Complementary Therapies in Medicine},
volume = {40},
pages = {145-150},
year = {2018},
issn = {0965-2299},
doi = {https://doi.org/10.1016/j.ctim.2018.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0965229917308804},
author = {Erik W. Baars and Helmut Kiene and Gunver S. Kienle and Peter Heusser and Harald J. Hamre},
keywords = {Anthroposophic medicine, Demarcation criteria, Philosophy of science, Scientific research field},
abstract = {Objectives
The objective was to evaluate the scientific status of anthroposophic medicine (AM) according to demarcation criteria proposed in contemporary philosophy of science.
Design
Criteria for what is science were retrieved from eight publications in the philosophy of science, focusing either on science in medicine or on the demarcation between science and pseudoscience or non-science. Criteria were combined, redundancies were excluded, and the final set of criteria was ordered in a logical sequence. The analysis yielded 11 demarcation criteria (community, domain, problems, goals, axiomatic basis, conceptual basis, quality of concepts, methodology, deontic basis, research products, tradition).
Results
Assessing the scientific status of AM according to the 11 criteria, all criteria were fulfilled by AM.
Discussion
AM is grounded on the notion that specific non-atomistic holistic formative forces exist and can be empirically and rationally assessed. From a position claiming that such holistic forces cannot possibly exist or cannot be empirically and rationally assessed, the axiomatic and conceptual basis of AM can be contested. However, such an a priori rejection is problematic in the presence of empirical evidence supporting the validity of holistic concepts, as discussed in the paper. Future research should therefore focus on the tenability of the ontological reductionist position in science and on the further validation of AM non-atomistic holistic concepts, methods and practices.
Conclusion
In this analysis, using criteria from philosophy of science, AM fulfilled all 11 criteria for what is science.}
}
@article{ALLARD2020553,
title = {Food Safety Genomics and Connections to One Health and the Clinical Microbiology Laboratory},
journal = {Clinics in Laboratory Medicine},
volume = {40},
number = {4},
pages = {553-563},
year = {2020},
note = {Current Issues in Clinical Microbiology},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2020.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S0272271220300652},
author = {Marc W. Allard and Jie Zheng and Guojie Cao and Ruth Timme and Eric Stevens and Eric W. Brown},
keywords = {Whole genome sequencing, Foodborne pathogen isolates, Data sharing, Ontology, Genetic diversity, Outbreak detection, Contamination}
}
@article{DAVID2025101450,
title = {Professional vision in the multilingual classroom},
journal = {Linguistics and Education},
volume = {88},
pages = {101450},
year = {2025},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2025.101450},
url = {https://www.sciencedirect.com/science/article/pii/S0898589825000671},
author = {Samuel S. David},
keywords = {Translanguaging pedagogies, Collaborative translation, Professional vision, Teacher learning, Teacher teams},
abstract = {Translanguaging pedagogies, in which multilingual students are encouraged to flexibly use all their language skills to engage in learning activities, can benefit students’ development of language and literacy skills. But how can teachers who do not speak students’ languages make sense of and respond to these interactions? This qualitative study centers a team of U.S. middle-grades language arts teachers adapting a translanguaging approach into their classrooms, and investigates how their individual trajectories of learning contributed to a collective professional vision for translanguaging pedagogy. Teacher professional vision describes how teachers learn to notice what students do, reason about the causes, and respond with goal-oriented action. Findings describe how the teachers developed new, shared practices around text selection, reading comprehension support, promoting student interaction and negotiation of textual meaning, and connecting translanguaging activity to learning objectives.}
}
@article{SETH2025,
title = {Technologies for Interoperable Internet of Medical Things Platforms to Manage Medical Emergencies in Home and Prehospital Care: Scoping Review},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/54470},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125001153},
author = {Mattias Seth and Hoor Jalo and Åsa Högstedt and Otto Medin and Bengt Arne Sjöqvist and Stefan Candefjord},
keywords = {Internet of Medical Things, enabling technologies, standards, cross-domain interoperability, scoping review, technology, medical emergency, internet, prehospital care, gerontology, global population, chronic disease, multimorbidity, health care system, home-based care, innovation, digital health, health informatics, telehealth, artificial intelligence},
abstract = {Background
The aging global population and the rising prevalence of chronic disease and multimorbidity have strained health care systems, driving the need for expanded health care resources. Transitioning to home-based care (HBC) may offer a sustainable solution, supported by technological innovations such as Internet of Medical Things (IoMT) platforms. However, the full potential of IoMT platforms to streamline health care delivery is often limited by interoperability challenges that hinder communication and pose risks to patient safety. Gaining more knowledge about addressing higher levels of interoperability issues is essential to unlock the full potential of IoMT platforms.
Objective
This scoping review aims to summarize best practices and technologies to overcome interoperability issues in IoMT platform development for prehospital care and HBC.
Methods
This review adheres to a protocol published in 2022. Our literature search followed a dual search strategy and was conducted up to August 2023 across 6 electronic databases: IEEE Xplore, PubMed, Scopus, ACM Digital Library, Sage Journals, and ScienceDirect. After the title, abstract, and full-text screening performed by 2 reviewers, 158 articles were selected for inclusion. To answer our 2 research questions, we used 2 models defined in the protocol: a 6-level interoperability model and a 5-level IoMT reference model. Data extraction and synthesis were conducted through thematic analysis using Dedoose. The findings, including commonly used technologies and standards, are presented through narrative descriptions and graphical representations.
Results
The primary technologies and standards reported for interoperable IoMT platforms in prehospital care and HBC included cloud computing (19/30, 63%), representational state transfer application programming interfaces (REST APIs; 17/30, 57%), Wi-Fi (17/30, 57%), gateways (15/30, 50%), and JSON (14/30, 47%). Message queuing telemetry transport (MQTT; 7/30, 23%) and WebSocket (7/30, 23%) were commonly used for real-time emergency alerts, while fog and edge computing were often combined with cloud computing for enhanced processing power and reduced latencies. By contrast, technologies associated with higher interoperability levels, such as blockchain (2/30, 7%), Kubernetes (3/30, 10%), and openEHR (2/30, 7%), were less frequently reported, indicating a focus on lower level of interoperability in most of the included studies (17/30, 57%).
Conclusions
IoMT platforms that support higher levels of interoperability have the potential to deliver personalized patient care, enhance overall patient experience, enable early disease detection, and minimize time delays. However, our findings highlight a prevailing emphasis on lower levels of interoperability within the IoMT research community. While blockchain, microservices, Docker, and openEHR are described as suitable solutions in the literature, these technologies seem to be seldom used in IoMT platforms for prehospital care and HBC. Recognizing the evident benefit of cross-domain interoperability, we advocate a stronger focus on collaborative initiatives and technologies to achieve higher levels of interoperability.
International Registered Report Identifier (IRRID)
RR2-10.2196/40243}
}
@article{BILOSLAVO2018746,
title = {An eco-critical perspective on business models: The value triangle as an approach to closing the sustainability gap},
journal = {Journal of Cleaner Production},
volume = {174},
pages = {746-762},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.10.281},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617325866},
author = {Roberto Biloslavo and Carlo Bagnoli and David Edgar},
keywords = {Business model canvas, Business model framework, Eco-criticism, Sustainability, Sustainable business model},
abstract = {Despite business models having been discussed thoroughly by academics there are still some epistemological and ontological issues that have yet to be resolved. Business models seem to have stalled at the technological era and have not fully engaged with the era of sustainability. The purpose of our paper is to add a new lens and richness to sustainable business model research by building on the need for more interdisciplinary approaches. This paper applies an eco-critical approach to analyse the 20 most often cited business model frameworks. We explore the conventional understanding of the business models based on the language applied and reflect on gaps in the current perspectives of sustainability. The analysis shows that existing business model frameworks exclude natural and social aspects of organisational environment from the discussion and tend to neglect the interrelationships between economic and not-economic actors as well as the intertemporal trade-offs. Based on the results of the analysis we propose a new sustainable business model framework named “Value Triangle”, which explicitly includes as core elements society incorporating the natural environment and future generations and three types of co-created and co-delivered value: public, partner and customer. The Value Triangle together with the corresponding canvas is presented through a business case for sustainability represented by Italian company Loccioni. The results show that the proposed business model framework and canvas allow managers to understand, analyse and evaluate their business models along all three dimensions of sustainability – economic, social, and environmental simultaneously. Such an understanding helps drive the field towards making a meaningful contribution to solving the UN global challenges and sustainability agenda.}
}
@article{CHEN2023136797,
title = {How do green industrial policies accelerate regional sustainability transition? A spatiotemporal evaluation of policy with a relationalist perspective},
journal = {Journal of Cleaner Production},
volume = {404},
pages = {136797},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.136797},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623009551},
author = {Luyi Chen and Zhaoyang Hu and Xiaoting Hu and Guannan Xu},
keywords = {Green industrial policy, Sustainability transition, Ontology, Policy evaluation, Spatial difference-in-differences model},
abstract = {For the last decade, more municipal governments have adopted Green Industrial Policies (GIPs) to accelerate the sustainability transition of the regional “socio-technical” system. However, previous research on policy analysis always emphasized place-specificity case analysis while neglecting the transactionalized and generalized effect of spatial factors. A relationalist perspective, including geographic, cognitive, and institutional proximity, was applied to elucidate the extensive influence of spatial factors on regional sustainability transition. By combining the enterprise's geographic proximity analysis, this study found that GIPs improved Total Factor Productivity (TFP) by 1.71% and Green Total Factor Productivity (GTFP) by 1.82% in cities that implemented the policy. Meanwhile, GIPs suppressed the TFP and GTFP of enterprises neighboring the cities that implemented the policy, with a drop of −11.57% and −6.29% respectively. This research also found that the improvement effect of GIPs was weaker when enterprises had higher cognitive proximity. Furthermore, institutional proximity revealed the balance between the environmental and economic effects of GIPs. This study demonstrated that GIPs triggered an adaptive circular interaction between enterprises and governments. In contrast with previous studies, this study first applied a relationalist perspective to quantitively evaluate GIPs and elucidate the mechanism of policies accelerating regional sustainability transition.}
}
@incollection{ARGENT2020273,
title = {Nature},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {273-283},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10794-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955107942},
author = {Neil Argent},
keywords = {Actor–network theory, Epistemology, Hybridity, Naturalism, Ontology, Poststructuralism},
abstract = {The topic of nature—of the physical environment with or without human interactions and interrelations with it—has been treated somewhat ambivalently by human geographers over the discipline's history. In opposition to the dualist ontology and epistemology which dominated the philosophy and practice of geography throughout the 19th and 20th centuries, human geography research on environment and human society–environmental interrelationships has sought to encourage new ways of living in the world, and of avoiding major human-induced environmental crises. Strongly influenced by Marxian, feminist, postmodernist, and poststructuralist standpoints, approaches to the research of nature have, since the early 1990s, spanned across a diverse range of topics, highlighting how nature is actively physically made over by humans in the process of capitalist exploitation, but also how what we come to think about and know as a supposedly external, biophysical nature is always mediated by discourses of, for instance, imperialism, racism, or sustainable development.}
}
@article{TAO2024102627,
title = {Smarter smart contracts for automatic BIM metadata compliance checking in blockchain-enabled common data environment},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102627},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102627},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624002751},
author = {Xingyu Tao and Zhaoji Wu and Yuqing Xu and Chengliang Zheng and Yihai Fang and Moumita Das and Hao Liu and Xingbo Gong and Jack C.P. Cheng},
keywords = {BIM, Knowledge Graph, Common data environment (CDE), Blockchain, Smart Contract, ISO 19,650 standards},
abstract = {Blockchain technology is gaining increasing attention in BIM-based collaboration to enhance BIM security (e.g., traceability, integrity, and immutability). Due to the block size limitation, most existing BIM-blockchain interactions focus on recording BIM metadata (or attributes) on the blockchain. However, verifying the correctness or compliance of the input metadata is often overlooked, resulting in the sharing of incorrect versions, disputes over data ownership, and corrupted documents. Two research gaps have been identified: (1) a lack of domain knowledge for metadata compliance checking in BIM collaborative design and (2) an absence of methods to perform compliance checking when interacting with blockchain. Therefore, this paper proposes a blockchain-enabled common data environment (BECDE) framework that leverages a knowledge graph (KG) and smart contract technology. This framework makes three contributions to the body of knowledge: (1) It explores the mechanism of integrating KG with smart contracts and the CDE workflow to enable compliance checking in a distributed blockchain environment. Within this framework, two essential technical elements—compliance checking rules (CCRs) and “smarter” smart contracts (SSCs)—are identified. (2) It establishes the KG of the ISO 19650 standards to generate CCRs and develops Semantic Web Rule Language (SWRL) algorithms to convert the natural-language-based CCRs into blockchain-readable rules. (3) It develops SSC algorithms by incorporating CCRs to automate checking BIM metadata compliance before appending them to the blockchain. The BECDE framework is validated in three actual project BIM design scenarios, with results showing that (1) the SSCs outperform existing BIM smart contracts by improving the quality of input data within the blockchain and (2) the computing performances of the SSCs—with latency at the millisecond level and throughput around 250 transactions per second—meet the requirements of BIM-based collaboration. By integrating domain knowledge into a blockchain, the BECDE framework facilitates a trustworthy BIM environment where project members can rely on both data security and quality.}
}
@article{REVILLALEON2023293,
title = {Artificial intelligence applications in implant dentistry: A systematic review},
journal = {The Journal of Prosthetic Dentistry},
volume = {129},
number = {2},
pages = {293-300},
year = {2023},
issn = {0022-3913},
doi = {https://doi.org/10.1016/j.prosdent.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0022391321002729},
author = {Marta Revilla-León and Miguel Gómez-Polo and Shantanu Vyas and Basir A. Barmak and German O. Galluci and Wael Att and Vinayak R. Krishnamurthy},
abstract = {Statement of problem
Artificial intelligence (AI) applications are growing in dental implant procedures. The current expansion and performance of AI models in implant dentistry applications have not yet been systematically documented and analyzed.
Purpose
The purpose of this systematic review was to assess the performance of AI models in implant dentistry for implant type recognition, implant success prediction by using patient risk factors and ontology criteria, and implant design optimization combining finite element analysis (FEA) calculations and AI models.
Material and methods
An electronic systematic review was completed in 5 databases: MEDLINE/PubMed, EMBASE, World of Science, Cochrane, and Scopus. A manual search was also conducted. Peer-reviewed studies that developed AI models for implant type recognition, implant success prediction, and implant design optimization were included. The search strategy included articles published until February 21, 2021. Two investigators independently evaluated the quality of the studies by applying the Joanna Briggs Institute (JBI) Critical Appraisal Checklist for Quasi-Experimental Studies (nonrandomized experimental studies). A third investigator was consulted to resolve lack of consensus.
Results
Seventeen articles were included: 7 investigations analyzed AI models for implant type recognition, 7 studies included AI prediction models for implant success forecast, and 3 studies evaluated AI models for optimization of implant designs. The AI models developed to recognize implant type by using periapical and panoramic images obtained an overall accuracy outcome ranging from 93.8% to 98%. The models to predict osteointegration success or implant success by using different input data varied among the studies, ranging from 62.4% to 80.5%. Finally, the studies that developed AI models to optimize implant designs seem to agree on the applicability of AI models to improve the design of dental implants. This improvement includes minimizing the stress at the implant-bone interface by 36.6% compared with the finite element model; optimizing the implant design porosity, length, and diameter to improve the finite element calculations; or accurately determining the elastic modulus of the implant-bone interface.
Conclusions
AI models for implant type recognition, implant success prediction, and implant design optimization have demonstrated great potential but are still in development. Additional studies are indispensable to the further development and assessment of the clinical performance of AI models for those implant dentistry applications reviewed.}
}
@article{ARTHUR2023103546,
title = {White travel imaginary and media contestations of race},
journal = {Annals of Tourism Research},
volume = {100},
pages = {103546},
year = {2023},
issn = {0160-7383},
doi = {https://doi.org/10.1016/j.annals.2023.103546},
url = {https://www.sciencedirect.com/science/article/pii/S0160738323000191},
author = {Tori Omega Arthur},
keywords = {white travel imaginary, social media, race, representation, digital space},
abstract = {Representations of Black travelers are often missing from travel media in favor of white able-bodied cisgender women and men. Lack of representation is a component of the white travel imaginary, a technological and physical as well as an epistemological and ontological convention that marginalizes Black experience in travel media. This study explicates a new theoretical paradigm for analyzing how race and representation function in the media. Employing visual and textual analysis as a method of exploring the white travel imaginary, I examine @historyofblacktravel and @blacktravelalliance, connected Instagram accounts highlighting the global movements of Black travelers. I show how scholars can employ the white travel imaginary framework when examining social media to critically analyze how travel is racially situated.}
}
@article{BOOT2022100578,
title = {An eye tracking experiment investigating synonymy in conceptual model validation},
journal = {International Journal of Accounting Information Systems},
volume = {47},
pages = {100578},
year = {2022},
issn = {1467-0895},
doi = {https://doi.org/10.1016/j.accinf.2022.100578},
url = {https://www.sciencedirect.com/science/article/pii/S1467089522000306},
author = {Walter R. Boot and Cheryl L. Dunn and Bachman P. Fulmer and Gregory J. Gerard and Severin V. Grabski},
keywords = {UML class diagram, REA, Conceptual model, Multiplicities, Validation, Eye tracking},
abstract = {A key advantage of conceptual models is that their quality can be evaluated and validated before beginning the costlier stages of information system development. Few research studies investigate the validation process for such models, particularly regarding multiplicities, even though multiplicity mistakes can be very costly. We investigated the validation of conceptual model multiplicities, varying how closely natural language statements of business rules match the models that purport to represent those rules. Participants in an eye tracking experiment completed validation tasks in which they viewed a statement and an accompanying UML class diagram in which a specified multiplicity was consistent with the statement (valid) or inconsistent with the statement (invalid). We varied whether the focal multiplicity was a minimum or a maximum and varied the class diagram’s semantics and order compared to that of the statement. Logistic regression was used to analyze the relationship between accuracy and the experimental manipulations and controls. The results show that the odds of accuracy in validating class diagrams that used synonyms instead of the exact statement terminology were only 0.46 times the odds of accuracy when the class diagram and statement words matched, showing a costly effect of synonymy. Interestingly, independent of the three levels of relative semantics, the odds of accuracy were 0.48 times when class diagrams were consistent with business rules as they were when class diagrams were inconsistent with business rules. To gain insight into cognition under correct task performance, we conducted additional linear regression analysis on various eye tracking metrics for only the accurate responses. Again, synonymy was observed to be costly, with a cognitive burden of increased integrative transitions between statement and model in the range of 39 to 66%.}
}
@article{SUPUNYA2025100945,
title = {Exploring interactional competence of English-medium-instruction students: A multimodal conversation and interpretative phenomenological analysis},
journal = {Learning, Culture and Social Interaction},
volume = {54},
pages = {100945},
year = {2025},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2025.100945},
url = {https://www.sciencedirect.com/science/article/pii/S2210656125000649},
author = {Nuntapat Supunya and Supong Tangkiengsirisin},
keywords = {Interactional competence, Multimodal conversation analysis, Interpretative phenomenological analysis, English medium instruction, Classroom interaction},
abstract = {The co-construction of intersubjectivity during disciplinary knowledge exchange among multilingual students in English-medium-instruction (EMI) programmes is intricate, expanding beyond language use to involve a range of multimodal resources employed during interactions. Such practice underpins the concept of interactional competence (IC). Despite the proliferation of IC studies advanced by conversation analysis (CA), the crystallisation of what constitutes this dynamic construct remains, necessitating methodological triangulation and a need for data-driven inquiries into its components. Central to this study was the exploration of EMI students' IC mediated through their use of resources-at-talk to co-construct intersubjectivity for knowledge acquisition during learning at a Thai university. A multimodal conversation analysis (MCA) was employed to analyse five video-recorded interactions across three months, complemented by an interpretative phenomenological analysis (IPA) of four subsequent stimulated recall interviews. Using purposive sampling, six EMI students (M = 4, F = 2) agreed to participate in this project. The inductive analyses revealed that EMI students' IC were mediated through eight interactional resources used – social actions, interactional mechanisms, linguistic knowledge, non-linguistic resources, pragmalinguistics, sociopragmatics, content knowledge, and psychological components. The analyses highlight a meaningful finding that IC appears to be psychology-driven, as evident in its influence on interlocutors' engagement, participation, and subsequent deployment of other resources. Understanding the knowledge co-construction process in EMI-student interaction brings pedagogical benefits to pre-EMI programmes, assisting them with EMI learning and real-world interactional success.}
}
@article{SHAKIL2024128255,
title = {Abstractive text summarization: State of the art, challenges, and improvements},
journal = {Neurocomputing},
volume = {603},
pages = {128255},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128255},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224010269},
author = {Hassan Shakil and Ahmad Farooq and Jugal Kalita},
keywords = {Automatic summarization, Abstractive summarization, Extractive summarization, Knowledge representation, Text generation},
abstract = {Specifically focusing on the landscape of abstractive text summarization, as opposed to extractive techniques, this survey presents a comprehensive overview, delving into state-of-the-art techniques, prevailing challenges, and prospective research directions. We categorize the techniques into traditional sequence-to-sequence models, pre-trained large language models, reinforcement learning, hierarchical methods, and multi-modal summarization. Unlike prior works that did not examine complexities, scalability and comparisons of techniques in detail, this review takes a comprehensive approach encompassing state-of-the-art methods, challenges, solutions, comparisons, limitations and charts out future improvements — providing researchers an extensive overview to advance abstractive summarization research. We provide vital comparison tables across techniques categorized — offering insights into model complexity, scalability and appropriate applications. The paper highlights challenges such as inadequate meaning representation, factual consistency, controllable text summarization, cross-lingual summarization, and evaluation metrics, among others. Solutions leveraging knowledge incorporation and other innovative strategies are proposed to address these challenges. The paper concludes by highlighting emerging research areas like factual inconsistency, domain-specific, cross-lingual, multilingual, and long-document summarization, as well as handling noisy data. Our objective is to provide researchers and practitioners with a structured overview of the domain, enabling them to better understand the current landscape and identify potential areas for further research and improvement.}
}
@incollection{VAINIKKA2020137,
title = {Self–Other},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {137-140},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10736-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102295510736X},
author = {Joni T. Vainikka},
keywords = {Culture–nature, Geographical self, History of geography, Identity, Individualism, Memory, Other, Reflexivity, Self, Time},
abstract = {The constitution of self and its relation to others is one of the core problems addressed by the social sciences. Within human geography, this dialectic is often accompanied by spatial markers of self, such as here and there, inside and outside, and us and them. Self is conscious of itself, an active biography that tries to answer the question “who am I?” Self is reflexive both of the constructing categories of society and the reciprocity, constituting social relations but also of the histories that have contributed to any coherence or essentialism that constructs the self. This reflexivity is not the same thing as individualization or a self-mastery but instead is as an active ability to reflect back one's identity, life-paths, and conscious membership of different identity categories. Reflexivity toward dimensions of both time and space can make a conceptualization self difficult, since self is constituted by the experiences and memories of the individual, conditioned by others, the norms of society and place with its human and non-human actors. While the act of othering often includes a power imbalance that overshadows difference within the other, it can be concluded that other is a condition of the self without ontological stability.}
}
@article{VENKATASUBRAMANIAN2025109195,
title = {The road less traveled by},
journal = {Computers & Chemical Engineering},
volume = {200},
pages = {109195},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2025.109195},
url = {https://www.sciencedirect.com/science/article/pii/S0098135425001991},
author = {Venkat Venkatasubramanian}
}
@incollection{TUCKER202243,
title = {10.05 - A Community Approach to Modeling Earthscapes},
editor = {John (Jack) F. Shroder},
booktitle = {Treatise on Geomorphology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {43-49},
year = {2022},
isbn = {978-0-12-818235-2},
doi = {https://doi.org/10.1016/B978-0-12-818234-5.00106-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128182345001061},
author = {Gregory E. Tucker and Rudy Slingerland and Jaia Syvitski},
keywords = {CSDMS, Earth surface processes, Geoscience, Numerical model, Open source, Scientific software},
abstract = {Developing a unified, predictive science of surface processes requires a quantitative understanding of critical surface-dynamics processes. An efficient approach to acquire this understanding is community modeling, defined here as the collective efforts of individuals to code, debug, test, document, run, and apply a suite of modeling components coupled in a framework or community modeling system. The modeling components each consist of modular code, commonly with a standardized interface to allow different modules to communicate with other components written in a different programming language. The framework is a set of agreed-upon protocols that allow the components to function together. Because of the framework, users can assemble components coded and vetted by specialists into complex models tuned to their specific objectives. The advantages of community modeling are efficient use of community resources and more effective integration of scientists and software specialists.}
}
@article{CIMADEVILLA2025105931,
title = {Why the relational data model matters for climate data management},
journal = {Computers & Geosciences},
volume = {201},
pages = {105931},
year = {2025},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2025.105931},
url = {https://www.sciencedirect.com/science/article/pii/S0098300425000810},
author = {Ezequiel Cimadevilla},
keywords = {Relational data model, Climate data management, Mathematical logic, Databases, netCDF},
abstract = {Efficient data management of climate data banks, in particular those generated by Global or Regional Climate Models, is an important requirement for precise understanding of current changes in the climate system. Current data management practices in the climate community are based on the analysis of binary files for storage of multidimensional arrays that require ad hoc software libraries for accessing the data. Several approaches are being developed to ease and facilitate climate data management and data analysis. However, the theoretical foundations that cause climate data manipulation difficulties remain unchallenged. The Relational Data Model was proposed as a formal solution for database management based on mathematical logic. It has been widely accepted in the industry and has survived the test of time. However, the foundational principles of the Relational Data Model have been overlooked by the climate data management community, mostly due to a lack of emphasis in the relevance of mathematical logic for database management and misunderstanding between physical and logical levels of abstraction. As a result, climate data management workflows lack the rigor and formality provided by the Relational Data Model. This work explains the Relational Data Model at the logical level of abstraction and provides the arguments, clarifies the misconceptions, and justifies its adoption for climate data management in the context of gridded data generated by climate models.}
}
@article{WU2025265,
title = {AI-driven multi-omics integration for multi-scale predictive modeling of genotype-environment-phenotype relationships},
journal = {Computational and Structural Biotechnology Journal},
volume = {27},
pages = {265-277},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.12.030},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024004513},
author = {You Wu and Lei Xie},
keywords = {Machine learning, Deep learning, Single cell, Omics data, Drug discovery, Precision medicine, Complex disease},
abstract = {Despite the wealth of single-cell multi-omics data, it remains challenging to predict the consequences of novel genetic and chemical perturbations in the human body. It requires knowledge of molecular interactions at all biological levels, encompassing disease models and humans. Current machine learning methods primarily establish statistical correlations between genotypes and phenotypes but struggle to identify physiologically significant causal factors, limiting their predictive power. Key challenges in predictive modeling include scarcity of labeled data, generalization across different domains, and disentangling causation from correlation. In light of recent advances in multi-omics data integration, we propose a new artificial intelligence (AI)-powered biology-inspired multi-scale modeling framework to tackle these issues. This framework will integrate multi-omics data across biological levels, organism hierarchies, and species to predict genotype-environment-phenotype relationships under various conditions. AI models inspired by biology may identify novel molecular targets, biomarkers, pharmaceutical agents, and personalized medicines for presently unmet medical needs.}
}
@article{CAUFIELD20212182,
title = {A Second Look at FAIR in Proteomic Investigations},
journal = {Journal of Proteome Research},
volume = {20},
number = {5},
pages = {2182-2186},
year = {2021},
issn = {1535-3907},
doi = {https://doi.org/10.1021/acs.jproteome.1c00177},
url = {https://www.sciencedirect.com/science/article/pii/S1535390721004443},
author = {J. Harry Caufield and John Fu and Ding Wang and Vladimir Guevara-Gonzalez and Wei Wang and Peipei Ping},
keywords = {data sharing, FAIR principles, ontologies, knowledgebases, standardization},
abstract = {Proteomics is, by definition, comprehensive and large-scale, seeking to unravel ome-level protein features with phenotypic information on an entire system, an organ, cells, or organisms. This scope consistently involves and extends beyond single experiments. Multitudinous resources now exist to assist in making the results of proteomics experiments more findable, accessible, interoperable, and reusable (FAIR), yet many tools are awaiting to be adopted by our community. Here we highlight strategies for expanding the impact of proteomics data beyond single studies. We show how linking specific terminologies, identifiers, and text (words) can unify individual data points across a wide spectrum of studies and, more importantly, how this approach may potentially reveal novel relationships. In this effort, we explain how data sets and methods can be rendered more linkable and how this maximizes their value. We also include a discussion on how data linking strategies benefit stakeholders across the proteomics community and beyond.
}
}
@article{LIU2020105390,
title = {A cross-region transfer learning method for classification of community service cases with small datasets},
journal = {Knowledge-Based Systems},
volume = {193},
pages = {105390},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105390},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119306331},
author = {Zhao-ge Liu and Xiang-yang Li and Li-min Qiao and Dilawar Khan Durrani},
keywords = {Community service, Case classification, Small datasets, Transfer learning, Domain adaptation},
abstract = {The precise classification of community service cases is the most fundamental aspect of intelligent community service systems. However, data imbalance makes it challenging to achieve the desired level of precise classification. Existing transfer learning methods use open Internet knowledge for identifying case features and mining potential feature relations. However, community service cases have the characteristics such as short text length and non-public content, which restrict the transfer learning modes. In this paper, a cross-region transfer learning method is proposed to solve the classification problem of cases with small datasets in data imbalance situation while considering the perspective of regional cooperation. First, an ontology modeling method is applied to standardize the case features, reducing the effect of semantic ambiguity on transfer results. Secondly, to improve the effectiveness of source domain classification, this paper utilizes an extended marginal fisher analysis where the distance is measured by the inner product between data. Next, the mapping matrix from target domain to source domain is learned through domain adaptation. Finally, the method is verified based on the empirical data from Lanzhou and Beidaihe in China. Experimental results on classifiers show the proposed method helps regions to improve case classification rates significantly through knowledge complementation. The proposed approach can be followed to build case-based community service systems of reasonable accuracy and limited sample sizes.}
}
@article{GENSEL2025102420,
title = {Advances in knowledge discovery and management, best papers of EGC 2024},
journal = {Data & Knowledge Engineering},
volume = {156},
pages = {102420},
year = {2025},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2025.102420},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X25000151},
author = {Jérôme Gensel and Christophe Cruz and Hocine Cherifi}
}
@article{HAMED2024108782,
title = {Safeguarding authenticity for mitigating the harms of generative AI: Issues, research agenda, and policies for detection, fact-checking, and ethical AI},
journal = {iScience},
volume = {27},
number = {2},
pages = {108782},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.108782},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224000038},
author = {Ahmed Abdeen Hamed and Malgorzata Zachara-Szymanska and Xindong Wu},
keywords = {Biocomputational method, Bioinformatics, Biological sciences, Computational bioinformatics, Natural sciences, Neural networks, Artificial intelligence, Artificial intelligence applications},
abstract = {Summary
As the influence of transformer-based approaches in general and generative artificial intelligence (AI) in particular continues to expand across various domains, concerns regarding authenticity and explainability are on the rise. Here, we share our perspective on the necessity of implementing effective detection, verification, and explainability mechanisms to counteract the potential harms arising from the proliferation of AI-generated inauthentic content and science. We recognize the transformative potential of generative AI, exemplified by ChatGPT, in the scientific landscape. However, we also emphasize the urgency of addressing associated challenges, particularly in light of the risks posed by disinformation, misinformation, and unreproducible science. This perspective serves as a response to the call for concerted efforts to safeguard the authenticity of information in the age of AI. By prioritizing detection, fact-checking, and explainability policies, we aim to foster a climate of trust, uphold ethical standards, and harness the full potential of AI for the betterment of science and society.}
}
@article{JI2023493,
title = {Construction and application of knowledge graph for grid dispatch fault handling based on pre-trained model},
journal = {Global Energy Interconnection},
volume = {6},
number = {4},
pages = {493-504},
year = {2023},
issn = {2096-5117},
doi = {https://doi.org/10.1016/j.gloei.2023.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S2096511723000683},
author = {Zhixiang Ji and Xiaohui Wang and Jie Zhang and Di Wu},
keywords = {Power-grid dispatch fault handling, Knowledge graph, Pre-trained model, Auxiliary decision-making},
abstract = {With the construction of new power systems, the power grid has become extremely large, with an increasing proportion of new energy and AC/DC hybrid connections. The dynamic characteristics and fault patterns of the power grid are complex; additionally, power grid control is difficult, operation risks are high, and the task of fault handling is arduous. Traditional power-grid fault handling relies primarily on human experience. The difference in and lack of knowledge reserve of control personnel restrict the accuracy and timeliness of fault handling. Therefore, this mode of operation is no longer suitable for the requirements of new systems. Based on the multi-source heterogeneous data of power grid dispatch, this paper proposes a joint entity–relationship extraction method for power-grid dispatch fault processing based on a pre-trained model, constructs a knowledge graph of power-grid dispatch fault processing and designs, and develops a fault-processing auxiliary decision-making system based on the knowledge graph. It was applied to study a provincial dispatch control center, and it effectively improved the accident processing ability and intelligent level of accident management and control of the power grid.}
}
@article{TEO2021100854,
title = {“Doing justice” in psychological methodology: From science and experiments to anecdotes},
journal = {New Ideas in Psychology},
volume = {61},
pages = {100854},
year = {2021},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2021.100854},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X21000039},
author = {Thomas Teo},
keywords = {Methodology, Methodologism, Ontology, Anecdotes, Epistemology, Justice},
abstract = {Methodology is intrinsically related to “object” and its quality is based on the degree to which a method is doing justice to the object, demonstrating the entanglement of ontic, epistemic, and ethical considerations. The intent of “doing justice” is at the core of methodology and is the de facto guiding principle for conducting research and for producing knowledge. Objects in psychology can range from subjectivity to science and conflicts emerge because of giving primacy to particular objects. Using this perspective, various meanings of doing justice, critics’ challenges, deviations from doing justice to an object, ethical-political dimensions and the dialectics of doing justice in relation to objects are discussed. If doing justice is at the core of methodology, then the issue becomes under what circumstances a particular method is doing justice in relation to a particular object. Contrasting the experiment with anecdotes, it is shown dialectically that the former has no privileged status in psychology, and that experiments that are not replicated only do justice as anecdotal evidence.}
}
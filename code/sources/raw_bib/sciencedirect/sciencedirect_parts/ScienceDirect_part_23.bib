@article{FUCHS2023103688,
title = {A post-Cartesian economic and Buddhist view on tourism},
journal = {Annals of Tourism Research},
volume = {103},
pages = {103688},
year = {2023},
issn = {0160-7383},
doi = {https://doi.org/10.1016/j.annals.2023.103688},
url = {https://www.sciencedirect.com/science/article/pii/S0160738323001615},
author = {Matthias Fuchs},
keywords = {Economic growth ideology, Post-Cartesian ontology, Post-mechanistic economic theory, Buddhist philosophy, Transformative tourism},
abstract = {Insuperable socio-economic and ecological crises demonstrate the need to challenge economic growth ideology that is often embedded in contemporary tourism science. By borrowing from Buddhist philosophy this essay describes inconsistencies in economic theorizing due to its adoption of the Cartesian ontology implying a mechanistic thinking form. Following philosopher Brodbeck (2014), economic science is neither an empirically exact science nor value-free but represents an implicit ethics. To build on this, the elements of a post-mechanistic economic theory are sketched (Brodbeck, 2001). The applicability of this concept is corroborated by instances of current tourism research. After reinterpreting the homo economicus and the nature of money an agenda for a transformative tourism science building upon post-Cartesian economic thinking and Buddhist philosophy is elaborated.}
}
@article{EZALDEEN2022100700,
title = {A hybrid E-learning recommendation integrating adaptive profiling and sentiment analysis},
journal = {Journal of Web Semantics},
volume = {72},
pages = {100700},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2021.100700},
url = {https://www.sciencedirect.com/science/article/pii/S1570826821000664},
author = {Hadi Ezaldeen and Rachita Misra and Sukant Kishoro Bisoy and Rawaa Alatrash and Rojalina Priyadarshini},
keywords = {Hybrid E-learning recommendation, Adaptive profiling, Semantic learner profile, Fine-grained sentiment analysis, Convolutional Neural Network, Word embeddings},
abstract = {This research proposes a novel framework named Enhanced e-Learning Hybrid Recommender System (ELHRS) that provides an appropriate e-content with the highest predicted ratings corresponding to the learner’s particular needs. To accomplish this, a new model is developed to deduce the Semantic Learner Profile automatically. It adaptively associates the learning patterns and rules depending on the learner’s behavior and the semantic relations computed in the semantic matrix that mutually links e-learning materials and terms. Here, a semantic-based approach for term expansion is introduced using DBpedia and WordNet ontologies. Further, various sentiment analysis models are proposed and incorporated as a part of the recommender system to predict ratings of e-learning resources from posted text reviews utilizing fine-grained sentiment classification on five discrete classes. Qualitative Natural Language Processing (NLP) methods with tailored-made Convolutional Neural Network (CNN) are developed and evaluated on our customized dataset collected for a specific domain and a public dataset. Two improved language models are introduced depending on Skip-Gram (S-G) and Continuous Bag of Words (CBOW) techniques. In addition, a robust language model based on hybridization of these couple of methods is developed to derive better vocabulary representation, yielding better accuracy 89.1% for the CNN-Three-Channel-Concatenation model. The suggested recommendation methodology depends on the learner’s preferences, other similar learners’ experience and background, deriving their opinions from the reviews towards the best learning resources. This assists the learners in finding the desired e-content at the proper time.}
}
@article{ZOUAOUI20222131,
title = {Multi-Agents Indexing System (MAIS) for Plagiarism Detection},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {5},
pages = {2131-2140},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2020.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S1319157820304092},
author = {Samia Zouaoui and Khaled Rezeg},
keywords = {Arabic Documents, Arabic Ontology, Semantic Indexing, Semantic Similarity, Plagiarism Detection},
abstract = {With the extensive availability of technological systems all over the world and the increasing diffusion of information and documents by users, especially for the Arabic population, the development of semantic plagiarism detection systems has become essential due to its importance for protecting the rights of authors. Developing such a system that can covers the content of all Arabic documents using ontology as a semantic resource would be a complex and time-consuming task and would require intelligent natural language-processing capabilities. In the context of Arabic plagiarism detection systems using an Arabic ontology and permitting these systems to support semantic representation to more efficiently verify the originality of the research and meet the needs of researchers, this paper presents a novel approach for addressing plagiarism named Multi-agents Indexing System. The proposed system is composed of three phases: (1) natural language processing phase, (2) indexing phase and (3) evaluation phase. Our experimentations are based on the training dataset released for the AraPlagDet curpus. The obtained results indicated that the proposed system has improved the performance of plagiarism detection in Arabic documents with semantic indexing and mutli-agents system.}
}
@article{RODRIGUEZJORDA2025101702,
title = {Linguistic relativity from an enactive perspective: the entanglement of language and cognition},
journal = {Language Sciences},
volume = {108},
pages = {101702},
year = {2025},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101702},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000913},
author = {Ulises {Rodríguez Jordá} and Ezequiel A. {Di Paolo}},
keywords = {Linguistic relativity, Enactive approach, Modularity, Post-cognitivism, Languaging},
abstract = {We seek to relate the fields of linguistic relativity (LR) and the enactive approach in cognitive science. We distinguish contemporary research on LR, starting after the mid-1990s, from earlier approaches to the field. Current studies are characterised by a nuanced methodology rooted in the psycholinguistics tradition. While improving on earlier research, they also move away from philosophically oriented discussions about the relation between language and cognition and focus instead on experimentally testing relativistic effects for specific cognitive domains. We claim that this procedure retains some fundamental assumptions from classical cognitive science, precisely those that are challenged by an enactive perspective. These include a commitment to the modularity of mind and a computational understanding of the interactions between cognitive domains. We contend that contemporary LR research is, in fact, compatible with these classical cognitivist ideas, despite superficial points of tension. We then survey recent post-cognitivist approaches to language in cognitive science and explore ways in which LR and the enactive framework could be mutually enriched. Whereas the structural or categorial aspects of language are central for LR research, these are usually downplayed in post-cognitivist approaches, often influenced by the integrationist distinction between first-order linguistic practices and second-order constructs. We advance a specifically enactive perspective that seeks to preserve the systematic features of language while also integrating them within a dynamical understanding of the relation between language and cognition at multiple timescales.}
}
@article{WANG20231,
title = {A safety management approach for Industry 5.0′s human-centered manufacturing based on digital twin},
journal = {Journal of Manufacturing Systems},
volume = {66},
pages = {1-12},
year = {2023},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2022.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0278612522002047},
author = {Haoqi Wang and Lindong Lv and Xupeng Li and Hao Li and Jiewu Leng and Yuyan Zhang and Vincent Thomson and Gen Liu and Xiaoyu Wen and Chunya Sun and Guofu Luo},
keywords = {Digital twin, Human-centered manufacturing, Industry 5.0, Semantic reasoning, Virtual dataset, Safety management},
abstract = {Safety management is fundamental for ensuring human-centered manufacturing as defined by Industry 5.0, which requires the integration of knowledge-driven, human-machine-environmental safety. However, three challenges need to be addressed to fill the gap between contemporary workshop safety management and the expected requirement: insights into the complex interactions of human-machine-environmental activities, understanding the causality of unsafe states, and the adaptability of safety management methods. A reasoning approach towards factory unsafe states based on Digital Twin is proposed to address these challenges. First, a machine-readable semantic reasoning framework is introduced. Second, the ontology of unsafe states during production is modeled. Then, a high-fidelity virtual Digital Twin Workshop is constructed, which can simulate various workshop unsafe states and generate a virtual dataset. The virtual dataset is then mixed with the real dataset to train and test the target detection network, which is used to detect unsafe instances mapped to the ontology for reasoning. Finally, an experiment demonstrates that the proposed approach can address the three challenges.}
}
@article{JAYASANKA2024107465,
title = {Automating building environmental assessment: A systematic review and future research directions},
journal = {Environmental Impact Assessment Review},
volume = {106},
pages = {107465},
year = {2024},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2024.107465},
url = {https://www.sciencedirect.com/science/article/pii/S0195925524000520},
author = {T.A.D.K. Jayasanka and Amos Darko and D.J. Edwards and Albert P.C. Chan and Farzad Jalaei},
keywords = {Building environmental assessment, Automation, Digital technologies, PRISMA, Review},
abstract = {Building environmental assessment (BEA) is critical to improving sustainability. However, the BEA process is inefficient, costly, and often inaccurate. Because automation has the potential to enhance the efficiency and accuracy of the BEA process, studies have focused on automating BEA (ABEA). Updated until now, a comprehensive analysis of prevailing literature on ABEA remains absent. This study conducts the first comprehensive systematic analysis appraising the state-of-the-art of research on ABEA. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guided to systematically analyse 91 relevant studies. Results uncover that only 29.7% of BEA systems worldwide have automated their processes, with the US LEED residing at the vanguard of automation efforts. The New Buildings scheme was mostly focused on, while largely ignoring other schemes, e.g., Existing Buildings. Five key digital approaches to ABEA were revealed, namely building information modelling (BIM) and plug-in software, BIM-ontology, data mining and machine learning, cloud-BIM, and digital twin-based approaches. Based on identified gaps, future research directions are proposed, specifically: using data mining and machine learning models for ABEA; development of a holistic cloud-based approach for real-time BEA; and digital twin for dynamic BEA. This study generates a deeper understanding of ABEA and its theoretical implications, such as major constructs and emerging perspectives, constitute a basis for holistic, and innovation in, BEA.}
}
@article{TRAPPEY2023102216,
title = {A comprehensive analysis of global patent landscape for recent R&D in agricultural drone technologies},
journal = {World Patent Information},
volume = {74},
pages = {102216},
year = {2023},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2023.102216},
url = {https://www.sciencedirect.com/science/article/pii/S0172219023000467},
author = {Amy J.C. Trappey and Ging-Bin Lin and Hong-Kai Chen and Ming-Chi Chen},
keywords = {Agricultural drone, Unmanned aerial vehicle (UAV), Knowledge ontology, Literature review, Patent landscape, Patent mining},
abstract = {The rapid development of information technology, along with advanced wireless communication technologies, has revolutionized the use of unmanned aerial vehicles (UAVs or drones) in many industries. In agriculture, due to the climate change and the growing global population, causing unprecedented demands and risks on food supplies, intelligent and automatic agricultural technologies are critical needs. UAVs offer a wide range of applications to transform traditional agricultural practices into Agriculture 4.0 that integrates advanced technology to optimize agricultural productivity, sustainability, and efficiency. To gain comprehensive insights into the current and future development of agricultural UAVs technologies, this research conducts extensive review and analysis of patents and non-patent literature in the field. By thoroughly examining the literature, the knowledge ontology of ag-UAV technologies is presented. Additionally, comprehensive macro- and micro-patent analyses identify the patenting trends and top tech-leaders for the key technologies related to agricultural drones. Moreover, utilizing regression modeling, technology maturity analysis, and technology-function matrix (TFM), the current and future R&D trends and the cold and hot spots of the technical innovations are identified. Through these detailed patent analyses, the state-of-the-art and potential advancements in agricultural UAV technologies are depicted, serving as crucial intelligence for R&D initiatives and IP strategies.}
}
@article{OJANSIVU202249,
title = {Using a ‘lens’ to re-search business markets, relationships and networks: Tensions, challenges and possibilities},
journal = {Industrial Marketing Management},
volume = {100},
pages = {49-61},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2021.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S001985012100211X},
author = {Ilkka Ojansivu and Christopher John Medlin and Poul Houman Andersen and Woonho Kim},
keywords = {Lens, Re-search, Ontology, meta-questions, business relationships, Networks},
abstract = {In this research, we wish to address the tension tucked away in scholarly work: the simultaneous need to break in and break out of academic communities and their ways of thinking. More precisely, we are interested in social re-search (i.e., searching again) processes and how scholars authenticate their research within an established cultural convention. For that purpose, we focus on the use of the term ‘lens’, which is omnipresent in research texts but rarely defined. Upon completing an integrative literature review and considering the embeddedness of a lens in culture, language, research communities and our ontological assumptions, we define a ‘research lens’ as a sociocultural representation and tool that helps to negotiate our scientific interpretation of the world. Our contribution to industrial marketing stems from surfacing and discussing four uses of a lens evident in the industrial marketing literature, introducing a metaphorical lens as a way to reform knowledge, and finally exemplifying how our lens tends to either mirror, reflect, symbolize or mirage the contours of our world without our full awareness of it.}
}
@article{BOUJELBEN2018860,
title = {A New Method For Rules Dependency Extraction},
journal = {Procedia Computer Science},
volume = {126},
pages = {860-869},
year = {2018},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 22nd International Conference, KES-2018, Belgrade, Serbia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918312985},
author = {Abir Boujelben and Ikram Amous},
keywords = {semantics, ontology, rule base, rules dependency, OWL, SWRL},
abstract = {Autonomous systems consist of agents that have access to interpretable information allowing them to make intelligent decisions. This requires the systems to have a good semantic knowledge base. A good semantic knowledge repository has to be backed by a well-defined domain ontology. For the sake of decidability, ontology languages do not provide the required expressiveness. Rules present an efficient support thereby enabling even better decision making. Their complexity and their exponentially growing number imply the need for an automatic management. In this paper, we introduce a new method for the automatic dependency extraction among rule bases associated to ontologies. This is founded on a novel technique of dependency extraction. A prototype of our proposal was implemented and applied on two different rule bases associated to ontologies from different domains. Our results succeeded in increasing the number of the extracted dependency relationships.}
}
@article{ZEKAOUI2025110651,
title = {SSMT-PANBERT: A single-stage multitask model for phenotype extraction and assertion negation detection in unstructured clinical text},
journal = {Computers in Biology and Medicine},
volume = {195},
pages = {110651},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2025.110651},
url = {https://www.sciencedirect.com/science/article/pii/S0010482525010029},
author = {Nour Eddine Zekaoui and Maryem Rhanoui and Siham Yousfi and Mounia Mikram},
keywords = {Phenotype extraction, Assertion negation detection, Transformer-based models, BERT},
abstract = {Automatic phenotype extraction and assertion negation detection from large-scale accessible Electronic Health Records (EHRs), including discharge summaries and radiology reports, is a crucial task for various healthcare applications, such as disease diagnosis and treatment planning. The unstructured nature of these documents poses significant challenges for manual processing. However, prior studies exhibit several limitations, such as being restricted to a single label per sentence or omitting the extraction and negation of medical concepts, which make them prone to fail in complex circumstances. In this paper, we capitalize on the advancement of state-of-the-art pre-trained language models (PLMs) to propose a single-stage multitask solution that jointly learns to extract phenotypes and detect their assertion or negation in an end-to-end fashion. Our proposed approach aims to provide practical assistance to healthcare professionals by handling complex and diverse clinical scenarios. We evaluate our method on a validation set derived from an annotated, balanced, and validated dataset based on MIMIC-III clinical notes. The annotations were rigorously reviewed by domain experts to ensure high reliability. The top-performing model in our experiments, SSMT-PANBERT, achieves an average Macro F1 score of 92.33% and a Micro F1 score of 91.66% on the validation set, outperforming traditional pipeline approaches in terms of Macro F1 (92.33% vs. 91.66%), while reducing training time by 37%, inference time by 18.2%, and GPU memory usage by 57%. These results demonstrate the effectiveness of our unified approach in handling complex clinical scenarios while providing significant computational advantages for real-world applications. Furthermore, we conduct a thorough analysis of the model's performance and identify potential areas for future improvement.}
}
@article{LU2025103833,
title = {From learner to ambassador: Navigating cultural identity in foreign language classrooms through a transpositioning lens},
journal = {System},
volume = {134},
pages = {103833},
year = {2025},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2025.103833},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X2500243X},
author = {Xiuchuan Lu and Ya Zuo},
keywords = {Cultural identity, Identity flow, Foreign language classroom, Transpositioning, Translanguaging},
abstract = {The concept of transpositioning, introduced by Li and Lee, describes how individuals fluidly shift their identities through dynamic translanguaging practices. Recent developments extend this framework to include transmodal practices, which complement and enhance meaning-making across linguistic, visual, and multimodal resources in multilingual settings. Drawing on this perspective, the present study explores how teachers and students co-construct and negotiate cultural identity in L3 Spanish classrooms. Based on 540 min of classroom observations, stimulated recall interviews with two teachers, and open-answer questionnaires from 40 students, our findings reveal that transpositioning occurs concurrently, with both teachers and students embodying multiple identities. We identified three states in the emergence of knowledge and communication, illustrating the complex flow of cultural identities within the relational dynamics of the classroom. This study concludes that both translanguaging and transmodal practices can facilitate identity navigation, helping participants to transcend linguistic and cultural barriers. These practices, in turn, enhance teaching effectiveness and contribute to a more interactive and flexible classroom environment.}
}
@article{NAJAFABADI2025,
title = {Hybrid AI in synthetic biology: next era in agriculture},
journal = {Trends in Plant Science},
year = {2025},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2025.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1360138525002377},
author = {Mohsen Yoosefzadeh Najafabadi and Scott A. Jackson},
keywords = {crop innovation, data integration, generative AI, predictive AI, sustainable agriculture},
abstract = {Synthetic biology holds great potential to transform agriculture, yet its progress is constrained by the complexity of multigenomic, multitrait, and multi-environment data. Desirable traits often arise from complex gene networks acting across diverse conditions, making them difficult to predict and optimize manually. In the past decade, artificial intelligence (AI) has supported this process, but its large data needs and poor integration limit its role to pattern recognition rather than explanatory trait design. We argue that hybrid AI can more effectively navigate multiomics complexity to engineer climate-smart, high-yield crops. Already reducing trial and error in crop engineering, guiding gRNA design, and identifying key regulators, hybrid AI has outperformed traditional data-driven approaches, but its full potential requires clear pipelines, curated datasets, and automation platforms.}
}
@article{LI2022104454,
title = {BIM-enabled semantic web for automated safety checks in subway construction},
journal = {Automation in Construction},
volume = {141},
pages = {104454},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104454},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003272},
author = {Xuewei Li and Dujuan Yang and Jingfeng Yuan and Alex Donkers and Xuan Liu},
keywords = {Subway construction, Automated safety checking, Semantic web, Building information modeling (BIM), Sensor data, SPARQL-based rule},
abstract = {Rule-based construction safety checking aims to inspect safety risk factors during the construction process to ensure that risk factor states are within safety thresholds. However, integrating multisource safety risk factors and text-based rules remains a challenge. This study proposes a semantic approach to integrate heterogeneous data under a building information modeling (BIM) environment and enable automated safety checking through SPARQL-based reasoning. In this framework, four interconnected ontologies are developed to provide a semantic schema for subway construction safety checking. Safety risk factors data are extracted from BIM and sensor data and then converted into integrated ontology instances. Text-based rules are automatically transformed into SPARQL-based checking rules. The proposed framework can improve knowledge sharing and promote real-time and automated safety checking. A case is illustrated to show how the framework can be applied, confirming the feasibility and effectiveness of the proposed framework.}
}
@article{ZHAO2023110069,
title = {Multi-task learning with graph attention networks for multi-domain task-oriented dialogue systems},
journal = {Knowledge-Based Systems},
volume = {259},
pages = {110069},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110069},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122011625},
author = {Meng Zhao and Lifang Wang and Zejun Jiang and Ronghan Li and Xinyu Lu and Zhongtian Hu},
keywords = {Task-oriented dialogue system, Natural response generation, Graph attention network, Memory network},
abstract = {A task-oriented dialogue system (TOD) is an important application of artificial intelligence. In the past few years, works on multi-domain TODs have attracted increased research attention and have seen much progress. A main challenge of such dialogue systems is finding ways to deal with cross-domain slot sharing and dialogue act temporal planning. However, existing studies seldom consider the models’ reasoning ability over the dialogue history; moreover, existing methods overlook the structure information of the ontology schema, which makes them inadequate for handling multi-domain TODs. In this paper, we present a multi-task learning framework equipped with graph attention networks (GATs) to probe the above two challenges. In the method, we explore a dialogue state GAT consisting of a dialogue context subgraph and an ontology schema subgraph to alleviate the cross-domain slot sharing issue. We further construct a GAT-enhanced memory network using the updated nodes in the ontology subgraph to filter out the irrelevant nodes to acquire the needed dialogue states. For dialogue act temporal planning, a similar GAT and corresponding memory network are proposed to obtain fine-grained dialogue act representation. Moreover, we design an entity detection task to improve the capability of soft gate, which determines whether the generated tokens are from the vocabulary or knowledge base. In the training phase, four training tasks are combined and optimized simultaneously to facilitate the response generation process. The experimental results for automatic and human evaluations show that the proposed model achieves superior results compared to the state-of-the-art models on the MultiWOZ 2.0 and MultiWOZ 2.1 datasets.}
}
@incollection{COUTO2025438,
title = {Semantic Similarity Definition},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {438-445},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00085-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000853},
author = {Francisco M. Couto and Andre Lamurias and Pedro Ruas},
keywords = {Biomedical, Biomedical ontologies, Knowledge graphs, Functional analysis, Information content, Information theory, Knowledge organization systems, Semantic similarity, Semantic web, Similarity measures, Vocabularies},
abstract = {In bioinformatics, semantic similarity has been used to compare different types of biomedical entities, such as proteins, compounds and phenotypes, based on their biological role instead on what they look like. This manuscript presents a definition of semantic similarity between biomedical entities described by a common semantic base (e.g. knowledge graph, ontology) following an information-theoretic perspective of semantic similarity. It defines the amount of information content two entries share in a semantic base, and, by extension, how to compare biomedical entities represented outside the semantic base but linked through a set of annotations. Software to check how semantic similarity works in practice is available at: https://github.com/lasigeBioTM/DiShIn/.}
}
@article{STENHOUSE2025,
title = {SpeciMate: Improving metadata extraction from digitised biological specimens},
journal = {Biodiversity Data Journal},
volume = {13},
year = {2025},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.13.e160553},
url = {https://www.sciencedirect.com/science/article/pii/S1314283625001691},
author = {Alan Stenhouse and Peter H. Thrall},
keywords = {specimen, digitisation, metadata, AI, software application, data curation},
abstract = {Background
The digitisation of natural history collections represents a critical step towards preserving and increasing accessibility to valuable scientific data. Despite their fundamental importance to taxonomy, ecology and conservation, the world’s natural history collections remain underutilised due to the labour-intensive process of extracting metadata from specimen labels.
New information
This paper describes SpeciMate, a software application that uses a human-AI collaborative approach to accelerate the extraction of metadata from digitised specimen images. The system leverages artificial intelligence web services including optical character recognition (OCR), automated translation and large language and multimodal models (LLMs) to extract structured metadata, while requiring human expertise for prompt engineering and data curation. We describe the application's architecture, functionality and workflows, which enable effective processing of various specimen types including herbarium sheets and insect slides. Our trials indicate that this tool significantly improves the efficiency of metadata extraction while maintaining high data quality. The combination of automated AI processing with human supervision and refinement represents a promising approach to accelerating the digitisation and databasing of natural history collections, thereby enabling broader access to these invaluable resources for research, education and conservation efforts.}
}
@article{STEPIEN2022101728,
title = {An approach for cross-data querying and spatial reasoning of tunnel alignments},
journal = {Advanced Engineering Informatics},
volume = {54},
pages = {101728},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101728},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001860},
author = {Marcel Stepien and Annika Jodehl and André Vonthron and Markus König and Markus Thewes},
keywords = {BIM, GIS, Querying, Spatial reasoning, Mechanized tunneling, Alignment planning},
abstract = {In mechanized tunneling projects, finding a low-risk and cost-effective alignment is an important task. Several alignment variants are usually created and each one is intensely scrutinized. Variants often have individual advantages and disadvantages and can lead to different constructive designs of a tunnel. In order to find the best alignment possible the variants have to be analyzed and evaluated based on requirements and evaluation criteria, such as safety, cost, built environment and operational requirements. To perform this evaluation and to enable comprehensive decision making, a holistic planning environment is examined that includes documents and models of different domains. In general, these domain specific data differ schematically and semantically, which consequently makes it challenging to combine and compare such diverse data. For this purpose, information from different sources must be linked and evaluated in a structured way. In particular, spatial relationships have to be investigated. Therefore, in this paper, ontology databases are utilized to merge BIM and GIS at data level to create an integrated model of the entire tunneling project. Relevant information for decision-making can then be derived, such as the location of private and public buildings that are in a certain vicinity of the planned alignment. On the one hand, the implementation of queries is a popular and frequently used approach to check for semantic properties. On the other hand, using a query language to derive information from geometric data can be challenging, due to the necessity of processing geometric data prior to and during query execution. Additionally, geometric definitions can differ in the considered coordinate reference system, the dimension or the structure. To handle geometry information by employing query languages, representations are methodically transformed to well-known text literals. A simplified and uniform geometric representation can be utilized for spatial reasoning, for example by adopting GeoSPARQL methods.}
}
@article{LIU20221591,
title = {OARD: Open annotations for rare diseases and their phenotypes based on real-world data},
journal = {The American Journal of Human Genetics},
volume = {109},
number = {9},
pages = {1591-1604},
year = {2022},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2022.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0002929722003196},
author = {Cong Liu and Casey N. Ta and Jim M. Havrilla and Jordan G. Nestor and Matthew E. Spotnitz and Andrew S. Geneslaw and Yu Hu and Wendy K. Chung and Kai Wang and Chunhua Weng},
keywords = {rare disease, human phenotype ontology, electronic health records, open data sharing, knowledge graph, natural language processing, phenotype association},
abstract = {Summary
Diagnosis for rare genetic diseases often relies on phenotype-driven methods, which hinge on the accuracy and completeness of the rare disease phenotypes in the underlying annotation knowledgebase. Existing knowledgebases are often manually curated with additional annotations found in published case reports. Despite their potential, real-world data such as electronic health records (EHRs) have not been fully exploited to derive rare disease annotations. Here, we present open annotation for rare diseases (OARD), a real-world-data-derived resource with annotation for rare-disease-related phenotypes. This resource is derived from the EHRs of two academic health institutions containing more than 10 million individuals spanning wide age ranges and different disease subgroups. By leveraging ontology mapping and advanced natural-language-processing (NLP) methods, OARD automatically and efficiently extracts concepts for both rare diseases and their phenotypic traits from billing codes and lab tests as well as over 100 million clinical narratives. The rare disease prevalence derived by OARD is highly correlated with those annotated in the original rare disease knowledgebase. By performing association analysis, we identified more than 1 million novel disease-phenotype association pairs that were previously missed by human annotation, and >60% were confirmed true associations via manual review of a list of sampled pairs. Compared to the manual curated annotation, OARD is 100% data driven and its pipeline can be shared across different institutions. By supporting privacy-preserving sharing of aggregated summary statistics, such as term frequencies and disease-phenotype associations, it fills an important gap to facilitate data-driven research in the rare disease community.}
}
@article{JURASKY2021102174,
title = {Transformation of semantic knowledge into simulation-based decision support},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {71},
pages = {102174},
year = {2021},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2021.102174},
url = {https://www.sciencedirect.com/science/article/pii/S0736584521000569},
author = {Wiking Jurasky and Patrick Moder and Michael Milde and Hans Ehm and Gunther Reinhart},
keywords = {Knowledge transformation, Decision support, Ontologies, Hybrid modeling, Pandemic simulation, Supply chain simulation},
abstract = {Simulation is capable to cope with the uncertain and dynamic nature of industrial value chains. However, in-depth system expertise is inevitable for mapping objects and constraints from the real world to a virtual model. This knowledge-intensity leads to long development times of respective projects, which contradicts the need for timely decision support. Since more and more companies use industrial knowledge graphs and ontologies to foster their knowledge management, this paper proposes a framework on how to efficiently derive a simulation model from such semantic knowledge bases. As part of the approach, a novel Simulation Ontology provides a standardized meta-model for hybrid simulations. Its instantiation enables the user to come up with a fully parameterized formal simulation model. Newly developed Mapping Rules facilitate this process by providing guidance on how to turn knowledge from existing ontologies, which describe the system to be simulated, into instances of the Simulation Ontology. The framework is completed by a parsing procedure for an automated transformation of this conceptual model into an executable one. This novel modeling approach makes model development more efficient by reducing its complexity. It is validated in a use case implementation from semiconductor manufacturing, where cross-domain knowledge was required in order to model and simulate the impacts of the COVID-19 pandemic on a global supply chain network.}
}
@article{NI201910,
title = {Knowledge model for emergency response based on contingency planning system of China},
journal = {International Journal of Information Management},
volume = {46},
pages = {10-22},
year = {2019},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2018.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0268401218305334},
author = {Zi-jian Ni and Lili Rong and Ning Wang and Shuo Cao},
keywords = {Conceptual model, Ontology, Documentary analysis, Top-level ontology, N-ary relation},
abstract = {China is severely exposed to natural hazards. Currently, there are more than 5.5 million contingency plans for handling various incidents. Similar to those produced in other counties, the paper-based plans in China are limited in that emergency responders cannot easily extract helpful information for them. In this paper, a knowledge-based system will be proposed for providing different stakeholders with helpful information in the emergency response. The conceptual model is the core for the whole system, which can link plans in the physical world and the ontology in the cyber world.}
}
@article{LIU2025124749,
title = {Building sustainable urban energy systems: The role of linked data in photovoltaic generation estimation at neighbourhood level},
journal = {Applied Energy},
volume = {378},
pages = {124749},
year = {2025},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.124749},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924021329},
author = {Xuan Liu and Dujuan Yang and Alex Donkers and Bauke {de Vries}},
keywords = {Digital twin, Electricity generation, Solar energy, Sustainable urban energy management, Semantic web technology, Linked data},
abstract = {The imperative of sustainable urban development demands reductions in energy consumption and carbon emissions. Solar energy emerges as a pivotal player in facilitating the vision of energy transition, serving as a significant renewable energy source for the urban sector. To advance the goals of energy transition and carbon neutrality, it is critical to comprehend the photovoltaic (PV) generation planning at the neighbourhood level, as it offers opportunities that do not exist at either the household level or city level. However, there is a lack of studies that focus on the integration of PV energy generation prediction at the neighbourhood level due to the complexity arising from the abundance of data from disparate disciplines. Supporting the estimation process for electric energy generation is important for neighbourhood level grid-resolving energy planning and management. Semantic web technologies present a promising approach to address the challenge. Through this method, we have developed the Neighbourhood Photovoltaic Generation Ontology (NPO), designed to integrate heterogeneous data to facilitate electric energy estimation processes. This approach streamlines PV energy generation estimation  and enriches the data structure by improving the interoperability of data across various formats. A case study in the Netherlands validated the methodology using monthly PV energy generation data, demonstrating that our semantic-based framework significantly enhances the estimation process. The findings demonstrate the potential of semantic web technologies for neighbourhood-level energy planning and management, offering a scalable model that can be adapted to other urban settings. Moreover, the research contributes to the body of knowledge by illustrating how linked data can be strategically support energy transition goals and carbon neutrality initiatives at the neighbourhood level.}
}
@article{AMOORE2024103134,
title = {A world model: On the political logics of generative AI},
journal = {Political Geography},
volume = {113},
pages = {103134},
year = {2024},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2024.103134},
url = {https://www.sciencedirect.com/science/article/pii/S0962629824000830},
author = {Louise Amoore and Alexander Campolo and Benjamin Jacobsen and Ludovico Rella},
abstract = {The computational logics of large language models (LLMs) or generative AI – from the early models of CLIP and BERT to the explosion of text and image generation via ChatGPT and DALL-E − are increasingly penetrating the social and political world. Not merely in the direct sense that generative AI models are being deployed to govern difficult problems, whether decisions on the battlefield or responses to pandemic, but also because generative AI is shaping and delimiting the political parameters of what can be known and actioned in the world. Contra the promise of a generalizable “world model” in computer science, the article addresses how and why generative AI gives rise to a model of the world, and with it a set of political logics and governing rationalities that have profound and enduring effects on how we live today. The article traces the genealogies of generative AI models, how they have come into being, and why some concepts and techniques that animate these models become durable forms of knowledge that actively shape the world, even long after a specific material commercial GPT model has moved on to a new iteration. Though generative AI retains significant traces of former scientific and computational regimes – in statistical practices, probabilistic knowledge, and so on – it is also dislocating epistemological arrangements and opening them to novel ways of perceiving, characterising, classifying, and knowing the world. Four defining aspects of the political logic of generative AI are elaborated: i) generativity as something more than the capacity to generate image or text outputs, so that a generative logic acts upon the world understood as estimates of “underlying distributions” in data; ii) latency as a political logic of compression in which (by contrast with claims to reduction or distortion) the thing that is hidden, unknown or latent becomes surfaced and amenable to being governed; iii) broken and parallelized sequences as the ordering device of the political logic of generative AI, where attention frameworks radically change the possibilities for governing non-linear problems; iv) pre-training and fine-tuning as a computational logic of generative AI that simultaneously shapes a “zero shot politics” oriented towards unencountered data and new tasks. Across each of the four aspects, the article maps the emerging contemporary political logic of generative AI.}
}
@article{TIBAU2024101331,
title = {ChatGPT for chatting and searching: Repurposing search behavior},
journal = {Library & Information Science Research},
volume = {46},
number = {4},
pages = {101331},
year = {2024},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2024.101331},
url = {https://www.sciencedirect.com/science/article/pii/S0740818824000525},
author = {Marcelo Tibau and Sean Wolfgand Matsui Siqueira and Bernardo Pereira Nunes},
keywords = {ChatGPT, Large language models (LLMs), Searching as learning (SaL), Search tactics, Search strategy adaptation, Conversational information systems},
abstract = {Generative AI tools, exemplified by ChatGPT, are transforming the way users interact with information by enabling dialogue-based querying instead of traditional keyword searches. While this conversational approach can simplify user interactions, it also presents challenges in structuring effective searches, refining prompts, and verifying AI-generated content. This study addresses these complexities by repurposing traditional search tactics for use in conversational AI environments, specifically to support the Searching as Learning (SaL) paradigm. Forty-five adapted tactics are introduced to aid users in defining information needs, refining queries, and evaluating ChatGPT's responses for relevance, utility, and reliability. Using the Efficient Search Tactic Identification (ESTI) method and constant comparison analysis, these tactics were mapped into a stratified model with seven categories. The framework provides a structured approach for users to leverage conversational agents more effectively, promoting critical thinking and iterative learning. This research underscores the importance of developing robust search strategies tailored to conversational AI environments, facilitating deeper learning and reflective information engagement. Additionally, it highlights the need for ongoing research into the design and evaluation of future chat-and-search systems.}
}
@article{FURXHI2025100583,
title = {The fruits of data shepherding: A collection of open FAIR datasets for titanium dioxide coated photocatalytic surfaces},
journal = {NanoImpact},
volume = {39},
pages = {100583},
year = {2025},
issn = {2452-0748},
doi = {https://doi.org/10.1016/j.impact.2025.100583},
url = {https://www.sciencedirect.com/science/article/pii/S2452074825000436},
author = {Irini Furxhi and Massimo Perucca and Giovanni Baldi and Valentina Dami and Andrea Cioni and Antti Joonas Koivisto and Rossella Bengalli and Paride Mantecca and Giulia Motta and Marie Carriere and Ozge Kose and Alessia Nicosia and Fabrizio Ravegnani and David Burrueco-Subirà and Ana Candalija and Joan Cabellos and Socorro Vázquez-Campos and Elma Lahive and Emily Eagles and Jesus-Maria Lopez {de Ipiña} and Juliana Oliveira and Patrick Conin and Ilaria Zanoni and Andrea Brigliadori and Lara Faccani and Tofail Syed and Ehtsham-Ul Haq and Charlie O'Mahony and Marina Serantoni and Magda Blosi and Thomas Exner and Anna Costa},
keywords = {Safe and sustainable by design, Data, FAIR, Open, Nanomaterials, Titanium dioxide},
abstract = {This paper presents a large-scale collaborative effort within a multi-partner consortium, to systematically structure, curate, and openly share data in alignment with the FAIR principles. The data result from a case study of titanium dioxide (TiO₂) nanomaterials (NMs) for photocatalytic depolluting surfaces, produced via various spray coating techniques under the Safe and Sustainable by Design (SSbD) approach. The data are publicly available through a dedicated Zenodo community (https://zenodo.org/communities/asina/records), comprising of individual records that separately host the data and the corresponding metadata. Each dataset is systematically named to reflect its context beginning with “ASINA dataset,” followed by i) the relevant life cycle stage (LCS) from synthesis to end-of-life, ii) the SSbD dimension (i.e., functionality, safety, and environmental aspects), and iii) the assessed features (e.g., physicochemical properties, hazard evaluation, functionality assessment) facilitating searchability. The data files include “descriptors” excel tab, which is a harmonized version derived from primary data for visualization, data integration and future modeling applications. Metadata are provided in separate records and include detailed information such as contributor name and affiliations, experimental protocols, instrumentation, dictionary definitions, ontologies, and licensing terms. The data and metadata files are mutually paired in Zenodo using related identifiers, where each data file includes the DOI of its corresponding metadata file, and vice versa. In total, 43 interlinked records are provided capturing the case study, offering structured and machine-actionable resources that support modeling, data integration and harmonization efforts within the nanosafety and nanoinformatics communities. This effort was coordinated through dedicated data shepherding, which enabled trust-building, metadata alignment, and consistent FAIR implementation across partners.}
}
@article{PRETEL2024107503,
title = {Analysing the synergies between Multi-agent Systems and Digital Twins: A systematic literature review},
journal = {Information and Software Technology},
volume = {174},
pages = {107503},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107503},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001083},
author = {Elena Pretel and Alejandro Moya and Elena Navarro and Víctor López-Jaquero and Pascual González},
keywords = {Digital twin, Multi-agent system, MAS, Literature review, Internet of Things},
abstract = {Context
Digital Twins (DTs) are used to augment physical entities by exploiting assorted computational approaches applied to the virtual twin counterpart. A DT is generally described as a physical entity, its virtual counterpart, and the data connections between them. Multi-Agent Systems (MAS) paradigm is alike DTs in many ways. Agents of MAS are entities operating and interacting in a specific environment, while exploring and collecting data to solve some tasks.
Objective
This paper presents the results of a systematic literature review (SLR) focused on the analysis of current proposals exploiting the synergies of DTs and MAS. This research aims to synthesize studies that focus on the use of MAS to support DTs development and MAS that exploit DTs, paving the way for future research.
Method
A SLR methodology was used to conduct a detailed study analysis of 64 primary studies out of a total of 220 studies that were initially identified. This SLR analyses three research questions related to the synergies between MAS and DT.
Results
The most relevant findings of this SLR and their implications for further research are the following: i) most of the analyzed proposals design digital shadows rather than DT; ii) they do not fully support the properties expected from a DT; iii) most of the MAS properties have not fully exploited for the development of DT; iv) ontologies are frequently used for specifying semantic models of the physical twin.
Conclusions
Based on the results of this SLR, our conclusions for the community are presented in a research agenda that highlights the need of innovative theoretical proposals and design frameworks that guide the development of DT. They should be defined exploiting the properties of MAS to unleash the full potential of DT. Finally, ontologies for machine learning models should be designed for its use in DT.}
}
@article{MARTINGOMEZ201858,
title = {Smart eco-industrial parks: A circular economy implementation based on industrial metabolism},
journal = {Resources, Conservation and Recycling},
volume = {135},
pages = {58-69},
year = {2018},
note = {Sustainable Resource Management and the Circular Economy},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2017.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S092134491730246X},
author = {Alejandro M. {Martín Gómez} and Francisco {Aguayo González} and Mariano {Marcos Bárcena}},
keywords = {Circular economy, Industrial metabolism, Sustainable manufacturing, Ontology, Multi-Agent system, Ecological network analysis},
abstract = {In order to conserve natural environments, the Circular Economy (CE) is considered as a suitable way to carry out the transition from current economic models to models of a more sustainable nature. From the biological perspective however, industrial systems are generally inefficient. Manufacturing systems from the biological perspective therefore require the incorporation of tools to support decision making, thereby enabling organizations to improve their functions and competitiveness in a global and integrated perspective. Accordingly, at meso level, eco-industrial parks are gaining importance as an approach towards ensuring CE. In this work, an ontological framework for CE, based on industrial metabolism, is developed as the technology for information and knowledge models to share the circularity of resources through industrial ecosystems, based on ecological, economic, and social criteria. The ontology developed is modelled using Ontology Web Language and integrated in an architecture based on bio-inspired Multi-Agent Systems (MAS). Moreover, a quantitative method, Ecological Network Analysis, is incorporated into MAS knowledge to analyze and establish relationships and metabolic pathways between companies, which can increase the circularity of technical nutrients and reduce biological nutrient extraction. The integrated model is applied to a case study on the product life cycle for the establishment of its metabolic pathway through an eco-industrial park. The subsequent incorporation of MAS thereby establishes the Smart Eco-Industrial Park.}
}
@article{MOKOS2020100030,
title = {A survey on the formalisation of system requirements and their validation},
journal = {Array},
volume = {7},
pages = {100030},
year = {2020},
issn = {2590-0056},
doi = {https://doi.org/10.1016/j.array.2020.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2590005620300151},
author = {Konstantinos Mokos and Panagiotis Katsaros},
keywords = {Requirement specification, Requirement formalisation, Semantic analysis, Model-based design, Component-based design, Formal verification},
abstract = {System requirements define conditions and capabilities to be met by a system under design. They are a partial definition in natural language, with inevitable ambiguities. Formalisation concerns with the transformation of requirements into a specification with unique interpretation, for resolving ambiguities, underspecified references and for assessing whether requirements are consistent, correct (i.e. valid for an acceptable solution) and attainable. Formalisation and validation of system requirements provides early evidence of adequate specification, for reducing the validation tests and high-cost corrective measures in the later system development phases. This article has the following contributions. First, we characterise the specification problem based on an ontology for some domain. Thus, requirements represent a particular system among many possible ones, and their specification takes the form of mapping their concepts to a semantic model of the system. Second, we analyse the state-of-the-art of pattern-based specification languages, which are used to avoid ambiguity. We then discuss the semantic analyses (missing requirements, inconsistencies etc.) supported in such a framework. Third, we survey related research on the derivation of formal properties from requirements, i.e. verifiable specifications that constrain the system’s structure and behaviour. Possible flaws in requirements may render the derived properties unsatisfiable or not realizable. Finally, this article discusses the important challenges for the current requirements analysis tools, towards being adopted in industrial-scale projects.}
}
@article{XIE2025105090,
title = {Are humans (higher) animals? On the rational awakening and life transcendence of death cognition},
journal = {Acta Psychologica},
volume = {257},
pages = {105090},
year = {2025},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2025.105090},
url = {https://www.sciencedirect.com/science/article/pii/S0001691825004032},
author = {Sherman XIE},
keywords = {Religious biology, Death and dying, Cognitive regression, Instrumental rationality, Metaphysical neglect, Ontological crisis, Epistemic fragmentation},
abstract = {This article delves into the fundamental cognitive question of whether humans are (superior) animals from a utilitarian perspective. Modern biology classifies humans as animals, yet nearly all religious doctrines assert a distinctiveness of humans from animals. Humans, as higher animals, indeed possess a certain degree of rationality and intelligence compared to other lower animals. However, the innate ignorance of humans has not transcended that of lower animals. When it comes to judging the priority, importance, and urgency of their interests, humans, as higher animals, often exhibit a similar narrow-mindedness and short-sightedness as lower animals. This is primarily evident in the fact that most people have no alertness whatsoever to the constant possibility of their own death, and they show a dismissive attitude towards the fundamental metaphysical issues that could help them address the inevitability of death. From a utilitarian perspective, the predominance of human instrumental rationality over value rationality will inevitably lead to ultimate destruction through war and ecological disasters.}
}
@article{RUAS2022104137,
title = {NILINKER: Attention-based approach to NIL Entity Linking},
journal = {Journal of Biomedical Informatics},
volume = {132},
pages = {104137},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104137},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422001526},
author = {Pedro Ruas and Francisco M. Couto},
keywords = {Biomedical text, Named Entity Linking, Knowledge Bases, Natural language processing, Neural networks, Text mining},
abstract = {The existence of unlinkable (NIL) entities is a major hurdle affecting the performance of Named Entity Linking approaches, and, consequently, the performance of downstream models that depend on them. Existing approaches to deal with NIL entities focus mainly on clustering and prediction and are limited to general entities. However, other domains, such as the biomedical sciences, are also prone to the existence of NIL entities, given the growing nature of scientific literature. We propose NILINKER, a model that includes a candidate retrieval module for biomedical NIL entities and a neural network that leverages the attention mechanism to find the top-k relevant concepts from target Knowledge Bases (MEDIC, CTD-Chemicals, ChEBI, HP, CTD-Anatomy and Gene Ontology-Biological Process) that may partially represent a given NIL entity. We also make available a new evaluation dataset designated by EvaNIL, suitable for training and evaluating models focusing on the NIL entity linking task. This dataset contains 846,165 documents (abstracts and full-text biomedical articles), including 1,071,776 annotations, distributed by six different partitions: EvaNIL-MEDIC, EvaNIL-CTD-Chemicals, EvaNIL-ChEBI, EvaNIL-HP, EvaNIL-CTD-Anatomy and EvaNIL-Gene Ontology-Biological Process. NILINKER was integrated into a graph-based Named Entity Linking model (REEL) and the results of the experiments show that this approach is able to increase the performance of the Named Entity Linking model.}
}
@article{EKRAMIPOOYA202365,
title = {Application of natural language processing and machine learning in prediction of deviations in the HAZOP study worksheet: A comparison of classifiers},
journal = {Process Safety and Environmental Protection},
volume = {176},
pages = {65-73},
year = {2023},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2023.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0957582023004846},
author = {Ali Ekramipooya and Mehrdad Boroushaki and Davood Rashtchian},
keywords = {Chemical process safety, Process hazard analysis, HAZOP study automation, Natural language processing, Machine Learning},
abstract = {The HAZOP (Hazard and Operability) study is one of the most well-known approaches in process hazard analysis. The HAZOP study is a systematic procedure a multidisciplinary team uses to find hazards and operability issues through brainstorming. The conventional HAZOP study requires much time, is also knowledge-intensive, and is susceptible to human mistakes. Therefore, there is a significant incentive to automate the HAZOP study. This study investigated the effectiveness of Natural Language Processing (NLP) and Machine Learning (ML) in HAZOP study automation. The case study used in this contribution is based on a conventional HAZOP study report. Initially, the causes were converted into feature vectors using NLP's simple sentence embedding technique (Bag of Words). Random oversampling was employed to manage the limited and imbalanced dataset. Finally, ML classifiers such as Decision Tree, linear Support Vector Machine, Random Forest, Logistic Regression, Gaussian Naïve Bays, and K-Nearest Neighbors were applied to predict deviations. Decision Tree outperformed other classifiers with 92% accuracy. This study's integrative approach applies even to small units and companies with limited training datasets. This is because it does not require a large training dataset.}
}
@article{HANG2025130534,
title = {A survey of Few-Shot Relation Extraction combining meta-learning with prompt learning},
journal = {Neurocomputing},
volume = {647},
pages = {130534},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.130534},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225012068},
author = {Tingting Hang and Wei Wu and Jun Feng and Hamza Djigal and Jun Huang},
keywords = {Natural language processing, Few-shot learning, Relation extraction, Meta-learning, Prompt learning},
abstract = {Few-shot Relation Extraction (FSRE) is challenging in Natural Language Processing (NLP). It requires models to accurately identify and categorize the relation between entities in texts based on limited annotated samples. Researchers have explored meta-learning and prompt-learning methods to address this challenge. Meta-learning aims to accelerate the model’s learning rate for new tasks by training across different tasks, while prompt learning seeks to enhance the model’s understanding of context. However, both methods face limitations in few-shot scenarios. To overcome these restrictions, studies combining meta-learning with prompt learning have emerged, garnering widespread academic interest. This paper presents a comprehensive survey of FSRE combining Meta-Learning with Prompt Learning. We first categorize existing research into three main groups: meta-learning-based, prompt learning-based, and hybrid approaches. Then, we provide a detailed analysis and discussion of the latest advancements in each category. Moreover, we extensively describe datasets commonly used in the FSRE task, evaluation metrics, and comparisons among various models. Finally, we explore current challenges in the field and forecast potential future research trends, offering insights for subsequent studies.}
}
@article{THOMAS2022108956,
title = {An adaptable, high-performance relation extraction system for complex sentences},
journal = {Knowledge-Based Systems},
volume = {251},
pages = {108956},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.108956},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122004634},
author = {Anu Thomas and Sangeetha Sivanesan},
keywords = {Information extraction, Natural language processing, Domain-specific relation extraction, Domain ontology, Semi-supervised learning, Knowledge based system, Judicial text, Open information extraction, Domain adaptability},
abstract = {The rapid proliferation of text data has lead to an increase in the use of Information Extraction (IE) techniques to automatically extract key information in a fast and effective manner. Relation Extraction (RE), a sub-task of IE focuses on extracting semantic relations from free natural language text and is crucial for further applications including Question Answering, Information Retrieval, Knowledge Base construction, Text Summarization, etc. Literature shows that supervised learning approaches were widely used in RE. However, the performance of supervised methodologies depend on the availability of domain-specific annotated datasets which is not viable for many of the domains including legal, financial, insurance etc. In recent times, Open Information Extraction (OIE) techniques address this issue, by facilitating domain-independent extraction of relations from large text corpora with no demand for domain-specific tagged data and predefined relation classes. Even though OIE systems are fast and simple to implement, they are less effective in handling complex sentences, and often produce redundant extractions. This paper proposes an efficient RE system to extract domain-specific relations from natural language text, consisting of Knowledge-based and Semi-supervised learning systems, integrated with domain ontology. We evaluated the performance of proposed work on ‘judicial domain” as a use case and found that it overcomes the flaws and limitations of existing RE approaches, by achieving better results in terms of precision and recall. On further analysis, we found that the proposed system outperforms existing cutting-edge OIE systems on varying sentence length and complexity.}
}
@article{XU2025103736,
title = {Language teachers’ use of research in a textbook writing community of practice},
journal = {System},
volume = {133},
pages = {103736},
year = {2025},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2025.103736},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X25001460},
author = {Jinfen Xu and Wenbo Liu},
keywords = {Language teachers, Research use, Textbook writing, Community of practice},
abstract = {The need for the use of research in practice to bridge the research-practice divide in language teacher education has long been debated by researchers. This case study explored how six language teachers, who collaborated with researchers in writing a tertiary English textbook series in China, used research in this textbook writing community of practice (CoP). Data from interviews, documents and artifacts showed that the textbook writing CoP provided a venue to facilitate the teachers' instrumental use of research that directly informed their material selection and task design. Moreover, through increased research exposure, the teachers demonstrated enriched understandings and knowledge about English as a Foreign Language (EFL) materials and language teaching, which is indicative of their conceptual research use. Further analysis revealed that the key dimensions of the textbook writing CoP, including mutual engagement, joint enterprise, and shared repertoire, contributed to the teachers' effective engagement with research. The social significance of the joint textbook writing enterprise provided a fresh impetus for their research engagement; Collaborative negotiation within the CoP promoted their research uptake; The shared repertoire offered tangible and conceptual resources that enabled their research use. This study suggests that a textbook writing CoP may serve as an effective mechanism to enhance practitioners’ use of research.}
}
@article{JURISICA2024102006,
title = {Explainable biology for improved therapies in precision medicine: AI is not enough},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {38},
number = {4},
pages = {102006},
year = {2024},
note = {Genetics of Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2024.102006},
url = {https://www.sciencedirect.com/science/article/pii/S1521694224000779},
author = {I Jurisica},
keywords = {Precision medicine, Rheumatoid arthritis, Artificial intelligence, Integrative computational biology},
abstract = {Technological advances and high-throughput bio-chemical assays are rapidly changing ways how we formulate and test biological hypotheses, and how we treat patients. Most complex diseases arise on a background of genetics, lifestyle and environment factors, and manifest themselves as a spectrum of symptoms. To fathom intricate biological processes and their changes from healthy to disease states, we need to systematically integrate and analyze multi-omics datasets, ontologies, and diverse annotations. Without proper management of such complex biological and clinical data, artificial intelligence (AI) algorithms alone cannot be effectively trained, validated, and successfully applied to provide trustworthy and patient-centric diagnosis, prognosis and treatment. Precision medicine requires to use multi-omics approaches effectively, and offers many opportunities for using AI, “big data” analytics, and integrative computational biology workflows. Advances in optical and biochemical assay technologies including sequencing, mass spectrometry and imaging modalities have transformed research by empowering us to simultaneously view all genes expressed, identify proteome-wide changes, and assess interacting partners of each individual protein within a dynamically changing biological system, at an individual cell level. While such views are already having an impact on our understanding of healthy and disease conditions, it remains challenging to extract useful information comprehensively and systematically from individual studies, ensure that signal is separated from noise, develop models, and provide hypotheses for further research. Data remain incomplete and are often poorly connected using fragmented biological networks. In addition, statistical and machine learning models are developed at a cohort level and often not validated at the individual patient level. Combining integrative computational biology and AI has the potential to improve understanding and treatment of diseases by identifying biomarkers and building explainable models characterizing individual patients. From systematic data analysis to more specific diagnostic, prognostic and predictive biomarkers, drug mechanism of action, and patient selection, such analyses influence multiple steps from prevention to disease characterization, and from prognosis to drug discovery. Data mining, machine learning, graph theory and advanced visualization may help identify diagnostic, prognostic and predictive biomarkers, and create causal models of disease. Intertwining computational prediction and modeling with biological experiments leads to faster, more biologically and clinically relevant discoveries. However, computational analysis results and models are going to be only as accurate and useful as correct and comprehensive are the networks, ontologies and datasets used to build them. High quality, curated data portals provide the necessary foundation for translational research. They help to identify better biomarkers, new drugs, precision treatments, and should lead to improved patient outcomes and their quality of life. Intertwining computational prediction and modeling with biological experiments, efficiently and effectively leads to more useful findings faster.}
}
@article{SAI2025106174,
title = {Identification and visual representation of explicit legal definitions, their relations and implicit actors in regulatory documents},
journal = {Computer Law & Security Review},
volume = {58},
pages = {106174},
year = {2025},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2025.106174},
url = {https://www.sciencedirect.com/science/article/pii/S2212473X25000471},
author = {Catherine Sai and Lukas Rossi and Anastasiya Damaratskaya and Karolin Winter and Stefanie Rinderle-Ma},
keywords = {Natural language processing, Legal knowledge extraction, Implicit actor, Knowledge representation, European regulations},
abstract = {The complexity and constantly rising volume of regulatory documents leads to tedious and error-prone manual analysis tasks. At the same time, Artificial Intelligence (AI) techniques offer new opportunities in handling legal information by, e.g., supporting legal stakeholder through automated knowledge acquisition. An example is the extraction of legal terms accompanied by their explanations in order to build a legal vocabulary or an ontology. A challenge aggravating this task is legal knowledge being implicitly stated. Thus, the occurrence of implicit actors, due to the usage of passive constructs in regulatory documents, is observed frequently. Consider the phrase “the provider keeps the data up to date” vs. “the data is kept up to date”. In the former phrase, the actor (provider) is explicit, while the latter requires additional context in order to determine who is keeping the data up to date. Hence, we provide an approach grounded in Natural Language Processing (NLP) to support the identification and clarification of explicit legal definitions and their relations. We then use this information to also identify implicit actors and make them explicit through insertion into the sentence. In addition, we provide a set of visual representations, including annotated documents, knowledge graphs, and statistics on how many legal definitions and implicit actors are present in an article. The evaluation is based on European regulations and demonstrates that explicit legal information can be used to clarify implicit information, enhancing the transparency and interpretability of complex legal documents.}
}
@article{SHAIKH2022127a,
title = {Integrated models, model languages, model repositories, simulation experiments, simulation tools and data visualizations enable facile model reuse with biosimulations},
journal = {Biophysical Journal},
volume = {121},
number = {3, Supplement 1},
pages = {127a},
year = {2022},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2021.11.2118},
url = {https://www.sciencedirect.com/science/article/pii/S0006349521030939},
author = {Bilal Shaikh and Lucian P. Smith and Michael L. Blinov and Herbert M. Sauro and Ion I. Moraru and Jonathan R. Karr}
}
@article{IM2024122123,
title = {Learning medical concept representation based on semantic information in medical textural data},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122123},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122123},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423026258},
author = {Sea Jung Im and Yue Xu and Jason Watson},
keywords = {Hospital readmission, Medical code representation, Medical ontology, Semantic, Attention mechanism, Deep learning},
abstract = {Electric Health Records (EHR) have been widely adopted by many hospitals to improve clinical decision making and re-admission prediction. Each patient admission usually contains both multiple medical codes as well as clinical notes. Accurate learning of the representations (also called embeddings) of medical concepts from EHR is a key strategy to improve prediction performance in healthcare. Existing works employ medical ontologies to improve the quality of representations but focus solely on the relationships amongst the medical codes, ignoring textual data such medical code descriptions, clinical notes, and patient demographics. In this paper, we propose a new model called Semantic-based Attention model using Textual data for Medical Concept Embedding (SATexMCE). SATexMCE consists of three parts: medical codes embedding, admission embedding, and prediction model. First, we generate representations of medical codes by using both the textual description of medical codes and also the relationships among medical codes. Then, we generate the admission representation based on the representation of medical codes in the admission, the clinical notes and the demographic information associated with the admission via several attention mechanisms. The admission embeddings are used to construct a Recurrent neural network model which is used to predict patients’ readmission and a disease in the next admission based on patient admission data. Experimental results show that the proposed SATexMCE model improves not only the performance of readmission prediction but also the quality of medical concept representations. Attention mechanism helps us measure the importance of different medical codes and understand the meaning of the words in clinical notes for predictions.}
}
@article{XIA2025103013,
title = {A knowledge graph construction and causal structure mining approach for non-stationary manufacturing systems},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {95},
pages = {103013},
year = {2025},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2025.103013},
url = {https://www.sciencedirect.com/science/article/pii/S0736584525000675},
author = {Mingyuan Xia and Xuandong Mo and Yahui Zhang and Xiaofeng Hu},
keywords = {Non-stationary manufacturing system, Manufacturing knowledge graph, Knowledge modeling, Causal inference},
abstract = {Knowledge graph (KG) is a method for managing multi-source heterogeneous data and forming knowledge for reasoning using graph structure. It has been extensively utilized in manufacturing systems to promote the advancement of intelligent manufacturing. In non-stationary manufacturing systems, the machining performance of individual elements demonstrates variability and dynamic fluctuations. The significant dynamics and uncertainties of a manufacturing system bring great challenges to KG's modeling, construction, and reasoning. To overcome these challenges, this paper proposes a Digital-Physical Manufacturing Knowledge Graph (DPMKG) construction and reasoning method. Firstly, an ontology-based knowledge representation model is developed to facilitate the integration of digital domain knowledge with the description of physical domain performance fluctuations, thereby establishing the schema layer of DPMKG. Secondly, a SysML model-driven construction pipeline is proposed to facilitate the correlation and integration of multi-source data from both digital and physical domains, thereby establishing the instance layer of DPMKG. Thirdly, a causal structure mining method for DPMKG is developed to enhance the analytical and reasoning capabilities in non-stationary manufacturing systems. Finally, an aero-engine casing machining system is employed as a case study to establish the DPMKG, and reasoning is performed on the process quality prediction task. The case study reveals that the proposed DPMKG modeling, construction, and reasoning approach can effectively describe and analyze performance fluctuations in the physical domain of a non-stationary manufacturing system. By integrating digital and physical domain knowledge, the extensive data can be effectively leveraged to generate knowledge for reasoning, thereby facilitating intelligent and refined control of non-stationary manufacturing systems.}
}
@article{ADAMO2021101762,
title = {Beyond arrows in process models: A user study on activity dependences and their rationales},
journal = {Information Systems},
volume = {100},
pages = {101762},
year = {2021},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101762},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000260},
author = {Greta Adamo and Chiara {Di Francescomarino} and Chiara Ghidini and Fabrizio Maria Maggi},
keywords = {Empirical study with human subjects, Business process activity dependences, Business process modelling, Business process re-design, Business process understandability},
abstract = {Despite the number and variety of business process modelling languages and notations available in the Business Process Management field, all of them mainly focus on a single type of relationship holding between business process activities, namely the activity execution order within the control flow. However, other types of relationships may hold between activities (e.g., co-occurrence or causal constraints) and the motivation behind these relationships can also be different (e.g., a norm or an ontological law-of-nature). In this paper, we focus on one type of these activity relationships whose semantics goes beyond the semantics of arrows in traditional business process modelling languages, i.e., on the so called occurrence dependences. In particular, we aim at evaluating whether making these occurrence dependences explicit in business process models could support business process modellers and analysts in their tasks. To this aim, we propose a notation for representing the occurrence dependences and their rationale, and carry out an empirical study with human subjects for evaluating their support in comprehension and redesign tasks; in addition, we qualitatively investigate the effort required for enriching business process models with these dependences.}
}
@article{BADE2024268,
title = {Lexicon-based Language Relatedness Analysis},
journal = {Procedia Computer Science},
volume = {244},
pages = {268-277},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.200},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030011},
author = {Girma Yohannis Bade and Olga Kolesnikova and José Luis Oropeza and Grigori Sidorov},
keywords = {Computational linguistics, NLP, cosine similarity, lexicon relatedness, Omotic languages},
abstract = {The field of computational linguistics has been impacting various issues in language disciplines. The enormous growth of machine learning algorithms and Natural Language Processing (NLP) empowers its advancement and brings huge benefits to societies. For instance, machine translation, text summarization, sentence auto-completion, and sentiment analysis are a few of its benefits. However, leveraging this opportunity for low-resourced languages is challenging due to the lack of available electronic datasets. This paper presents a lexicon-based language relatedness analysis on Ethiopian low-resourced languages. The languages Wolaita, Dawuro, Gamo, and Gofa belong to the Ethiopian Omotic language family and share rich linguistic cultures and similarities. However, the extent of their inter-relatedness remains unknown. To address this gap, we collected and prepared novel corpora from the Bible and academic texts. We employed the TF-IDF technique for feature extraction and used the cosine similarity method to measure the similarities among these languages. In addition to cosine similarity, we used Euclidean distance to measure the spatial distances between the languages. The experiment results showed that Wolaita and Gofa exhibited high relatedness (33.4%), while Dawuro and Gamo demonstrated low relatedness (12.1%).}
}
@article{ELHAJABDOU2021107584,
title = {Deep_CNN_LSTM_GO: Protein function prediction from amino-acid sequences},
journal = {Computational Biology and Chemistry},
volume = {95},
pages = {107584},
year = {2021},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2021.107584},
url = {https://www.sciencedirect.com/science/article/pii/S1476927121001547},
author = {Mohamed E.M. Elhaj-Abdou and Hassan El-Dib and Amr El-Helw and Mohamed El-Habrouk},
keywords = {Deep learning, Gene ontology, CNN, LSTM, Protein function prediction, MF, BP, CC, UniProt-SwissProt, CAFA},
abstract = {Protein amino acid sequences can be used to determine the functions of the protein. However, determining the function of a single protein requires many resources and a tremendous amount of time. Computational Intelligence methods such as Deep learning have been shown to predict the proteins' functions. This paper proposes a hybrid deep neural network model to predict an unknown protein's functions from sequences. The proposed model is named Deep_CNN_LSTM_GO. Deep_CNN_LSTM_GO is an Integration between Convolutional Neural network (CNN) and Long Short-Term Memory (LSTM) Neural Network to learn features from amino acid sequences and outputs the three different Gene Ontology (GO). The gene ontology represents the protein functions in the three sub-ontologies: Molecular Functions (MF), Biological Process (BP), and Cellular Component (CC). The proposed model has been trained and tested using UniProt-SwissProt's dataset. Another test has been done using Computational Assessment of Function Annotation (CAFA) on the three sub-ontologies. The proposed model outperforms different methods proposed in the field with better performance using three different evaluation metrics (Fmax, Smin, and AUPR) in the three sub-ontologies (MF, BP, CC).}
}
@article{DRAGOS2020928,
title = {A formal representation of appraisal categories for social data analysis},
journal = {Procedia Computer Science},
volume = {176},
pages = {928-937},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.088},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920319864},
author = {Valentina Dragos and Delphine Battistelli and Emmanuelle Kellodjoue},
keywords = {ontology, appraisal theory, social data, opinion detection, sentiment analysis},
abstract = {Human behavior is impacted by subjective states although currently the cyberspace becomes a replacement for real-life spaces and interactions. As social media platforms change people’s lives and impacts the way they communicate and group themselves into virtual networks of like-minded individuals, the analysis of online content offers valuable insights of processes taking place on the Internet. Social data mining revolves around subjective content analysis, which deals with the computational processing of texts conveying people’s evaluations, beliefs, attitudes and emotions. Opinion mining and sentiment analysis are the main paradigm of social media exploration and both concepts are often interchangeable. This paper investigates the use of appraisal categories to explore data gleaned for social media and describes the construction of a formal model describing the way language is used in the cyberspace to evaluate, express mood and affective states, construct personal standpoints and manage interpersonal interactions. The ontology offers a mean to investigate subjective content going beyond the traditional notions of opinion and sentiment. Pitfalls of building a formal model for appraisal categories are examined and limitations of using the model for social data exploration are discussed.}
}
@article{ZHANG2025e756,
title = {The Construction of a New Prognostic Model of Breast Cancer and the Exploration of Drug Sensitivity Based on Machine Learning for Glycosylation-Related Genes},
journal = {Clinical Breast Cancer},
volume = {25},
number = {6},
pages = {e756-e764},
year = {2025},
issn = {1526-8209},
doi = {https://doi.org/10.1016/j.clbc.2025.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S152682092500117X},
author = {Jia-Ning Zhang and Xi-Rui Zhou and Zi-Lu Yi and Xin-Yu Tian and Hong Liu},
keywords = {Breast cancer, Drug sensitivity, Machine learning, Molecular docking, Prognostic model},
abstract = {Aims
Breast cancer has become the number 1 killer threatening women's health. In recent years, glycosylation modification has played an increasingly important role in tumor progression. The aim of this study was to explore the key genes that may be involved in glycosylation modification, establish prognostic models, and further explore their biological functions.
Methods
Using data from TCGA and GEO databases, differentially expressed genes (DEGs) were identified. Subsequently, Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analyses were conducted to characterize the functions of the DEGs. LASSO regression analysis was performed to narrow down hub genes. Additionally, single-cell analysis, protein-protein interaction (PPI) network analysis, immune correlation analysis, drug sensitivity analysis, and molecular docking were carried out to investigate the functions of these hub genes.
Results
Initially, we identified 110 differentially expressed prognostic genes, among which 89 were potentially associated with glycosylation modification. Enrichment analysis revealed their involvement in oxytocin signaling, chemical carcinogen-DNA adduct formation, and C-type lectin receptor pathways. LASSO regression (Least Absolute Shrinkage and Selection Operator) analysis further refined the selection to 24 hub genes, which exhibited specific genetic interactions. Notably, the expression levels of these genes showed significant associations with various immune cells. Drug sensitivity analysis of the hub genes highlighted methotrexate as a potential therapeutic candidate. Finally, molecular docking demonstrated strong binding affinities between the target receptors and ligands.
Conclusions
In conclusion, we screened glycosylation-related Hub genes, constructed prognostic models, explored their biological functions, and proposed new insights for diagnosing and treating breast cancer.}
}
@article{SHUKLA2025102342,
title = {Commitments to doing differently: Paradigm shifts necessary for critical educational psychology research},
journal = {Contemporary Educational Psychology},
volume = {80},
pages = {102342},
year = {2025},
issn = {0361-476X},
doi = {https://doi.org/10.1016/j.cedpsych.2025.102342},
url = {https://www.sciencedirect.com/science/article/pii/S0361476X25000074},
author = {Sarita Y. Shukla and Falynn A. Thompson and Sarah B. Shear},
keywords = {Quantitative research methodology, Race, Positionality, Critical paradigms, Educational psychology},
abstract = {While there have been some in educational psychology who have taken up critical research practices, this paper urges more scholars in this field to utilize critical methodologies. We consider this reframing of our work by articulating the research paradigms that have been prevalent in educational psychology. We also shine light on the historical and contemporary problems in educational psychology research. We provide recommendations for critically examining the link between researcher positionality and research, exploring the roles of axiology, ontology, and epistemologies in positionality, and adopting critical frameworks to research.}
}
@article{WENK2024102773,
title = {Traits.build: A data model, workflow and R package for building harmonised ecological trait databases},
journal = {Ecological Informatics},
volume = {83},
pages = {102773},
year = {2024},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2024.102773},
url = {https://www.sciencedirect.com/science/article/pii/S1574954124003157},
author = {Elizabeth Wenk and Payal Bal and David Coleman and Rachael Gallagher and Sophie Yang and Daniel Falster},
keywords = {Trait database, FAIR data principles, Database model, R package, Trait ecology},
abstract = {Trait databases have proliferated over the past decades, facilitating research on the ecology, evolution, and conservation of taxa across the Tree of Life. Typically, teams of independent researchers build these databases, and each must develop their own workflow and output structure. This divests research hours from downstream tasks such as trait-based analysis and interpretation and the resultant datasets are often difficult to integrate due to disparate database structures. Here we introduce the {traits.build} R-package, which offers a generalised workflow for building trait databases. {traits.build} contains bespoke functions for propagating metadata files, extensive tutorials, and sample configuration files, allowing researchers to efficiently build a new trait database using open-source tools. In addition, the {traits.build} output structure is fully documented by a data model, ensuring the meaning of each variable and semantic relationship between variables is transparent and consistent. The data standard links to terms in previously published data standards, drawing strongly on DarwinCore and the Ecological Trait-data Standard, but also includes the ability to fully map location and context properties absent from these vocabularies. It is the first published database-building workflow that adheres to the Extensible Observation Ontology. Simultaneously developing a generalised workflow and publishing a data standard for the workflow provides {traits.build} users a straightforward pathway to build a new trait database that achieves the FAIR principles. The meaning of all variables in a {traits.build} database are already documented, allowing further integration with either other {traits.build} databases or indeed any other database with a documented data model. This follows the vision of the Open Traits Network to build trait databases whose data can be easily integrated for further analysis.}
}
@article{CHATTERJEE202578,
title = {Sovereignty-Aware Intrusion Detection on Streaming Data: Automatic Machine Learning Pipeline and Semantic Reasoning},
journal = {Procedia Computer Science},
volume = {254},
pages = {78-87},
year = {2025},
note = {International Conference on Digital Sovereignty (ICDS)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.02.066},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925004168},
author = {Ayan Chatterjee and Sundar Gopalakrishnan and Ayan Mondal},
keywords = {Intrusion Detection, Digital Sovereignty, Machine Learning, Streaming Data, Semantic Ontology},
abstract = {Intrusion Detection Systems (IDS) are critical in safeguarding network infrastructures against malicious attacks. Traditional IDSs often struggle with knowledge representation, real-time detection, and accuracy, especially when dealing with high-throughput data. This paper proposes a novel IDS framework that leverages machine learning models, streaming data, and semantic knowledge representation to enhance intrusion detection accuracy and scalability. Additionally, the study incorporates the concept of Digital Sovereignty, ensuring that data control, security, and privacy are maintained according to national and regional regulations. The proposed system integrates Apache Kafka for real-time data processing, an automatic machine learning pipeline (e.g., Tree-based Pipeline Optimization Tool (TPOT)) for classifying network traffic, and OWL-based semantic reasoning for advanced threat detection. The proposed system, evaluated on NSL-KDD and CIC-IDS-2017 datasets, demonstrated qualitative outcomes such as local compliance, reduced data storage needs due to real-time processing, and improved adaptability to local data laws. Experimental results reveal significant improvements in detection accuracy, processing efficiency, and Sovereignty alignment.}
}
@article{SPOLADORE2024374,
title = {Towards a knowledge-based decision support system to foster the return to work of wheelchair users},
journal = {Computational and Structural Biotechnology Journal},
volume = {24},
pages = {374-392},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024001570},
author = {Daniele Spoladore and Luca Negri and Sara Arlati and Atieh Mahroo and Margherita Fossati and Emilia Biffi and Angelo Davalli and Alberto Trombetta and Marco Sacco},
keywords = {Knowledge-based decision support system, Ontology engineering, Return to work, Clinical decision support system, Wheelchair user},
abstract = {Accidents at work may force workers to face abrupt changes in their daily life: one of the most impactful accident cases consists of the worker remaining in a wheelchair. Return To Work (RTW) of wheelchair users in their working age is still challenging, encompassing the expertise of clinical and rehabilitation personnel and social workers to match the workers’ residual capabilities with job requirements. This work describes a novel and prototypical knowledge-based Decision Support System (DSS) that matches workers’ residual capabilities with job requirements, thus helping vocational therapists and clinical personnel in the RTW decision-making process for WUs. The DSS leverages expert knowledge in the form of ontologies to represent the International Classification of Functioning, Disability, and Health (ICF) and the Occupational Information Network (O*NET). These taxonomies enable both workers’ health conditions and job requirements formalization, which are processed to assess the suitability of a job depending on a worker’s condition. Consequently, the DSS suggests a list of jobs a wheelchair user can still perform, exploiting his/her residual abilities at their best. The manuscript describes the theoretical approach and technological foundations of such DSS, illustrating its development, its output metric, and application. The developed solution was tested with real wheelchair users’ health conditions provided by the Italian National Institute for Insurance against Accidents at Work. The feasibility of an approach based on objective data was thus demonstrated, providing a novel point of view in the critical process of decision-making during RTW.}
}
@article{ZHENG2022104524,
title = {Knowledge-informed semantic alignment and rule interpretation for automated compliance checking},
journal = {Automation in Construction},
volume = {142},
pages = {104524},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104524},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522003971},
author = {Zhe Zheng and Yu-Cheng Zhou and Xin-Zheng Lu and Jia-Rui Lin},
keywords = {Automated rule checking (ARC), Automated compliance checking (ACC), Rule interpretation, Natural language processing (NLP), Semantic alignment, Knowledge modeling, Building information modeling (BIM)},
abstract = {As an essential prodecure to improve design quality in the construction industry, automated rule checking (ARC) requires intelligent rule interpretation from regulatory texts and precise alignment of concepts from different sources. However, there still exists semantic gaps between design models and regulatory texts, hindering the exploitation of ARC. Thus, a knowledge-informed framework for improved ARC is proposed based on natural language processing. Within the framework, an ontology is first established to represent domain knowledge, including concepts, synonyms, relationships, constraints, etc. Then, semantic alignment and conflict resolution are introduced to enhance the rule interpretation process based on predefined domain knowledge and unsupervised learning techniques. Finally, an algorithm is developed to identify the proper SPARQL function for each rule, and then to generate SPARQL-based queries for model checking purposes, thereby making it possible to interpret complex rules where extra implicit data needs to be inferred. Experiments show that the proposed framework and methods successfully filled the semantic gaps between design models and regulatory texts with domain knowledge, which achieves a 90.1% accuracy and substantially outperforms the commonly used keyword matching method. In addition, the proposed rule interpretation method proves to be 5 times faster than the manual interpretation by domain experts. This research contributes to the body of knowledge of a novel framework and the corresponding methods to enhance automated rule checking with domain knowledge.}
}
@article{ZHAO2024114879,
title = {Generating Java code pairing with ChatGPT},
journal = {Theoretical Computer Science},
volume = {1021},
pages = {114879},
year = {2024},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2024.114879},
url = {https://www.sciencedirect.com/science/article/pii/S0304397524004961},
author = {Zelong Zhao and Nan Zhang and Bin Yu and Zhenhua Duan},
keywords = {Code generation, Large language models, ChatGPT, Prompt engineering, Iterative prompting},
abstract = {The Large Language Models (LLMs) like ChatGPT 3.5 have created a new era of automatic code generation. However, the existing research primarily focuses on generating simple code based on datasets (such as HumanEval, etc.). Most of approaches pay less attention to complex and practical code generation. Therefore, in this paper, we propose a new approach called “Xd-CodeGen” which can be used to generate large scale Java code. This approach is composed of four phases: requirement analysis, modeling, code generation, and code verification. In the requirement analysis phase, ChatGPT 3.5 is utilized to decompose and restate user requirements. To do so, a knowledge graph is developed to describe entities and their relationship in detail. Further, Propositional Projection Temporal Logic (PPTL) formulas are employed to define the properties of requirements. In the modeling phase, we use knowledge graphs to enhance prompts and generate UML class and activity diagrams for each sub-requirement using ChatGPT 3.5. In the code generation phase, based on established UML models, we make use of prompt engineering and knowledge graph to generate Java code. In the code verification phase, a runtime verification at code level approach is employed to verify generated Java code. Finally, we apply the proposed approach to develop a practical Java web project.}
}
@article{POURABBASI2022546,
title = {Unveiling a novel model for promoting mobile phone waste management with a social media data analytical approach},
journal = {Sustainable Production and Consumption},
volume = {29},
pages = {546-563},
year = {2022},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2352550921003146},
author = {Mohadeseh Pourabbasi and Sajjad Shokouhyar},
keywords = {Mobile phone waste management, Product end-of-life (EOL), Ontology, Self-organizing map (SOM), Decision support system (DSS), Social media data analysis},
abstract = {With the enormous growth of the population, the intense improvement of the electronic industry, and the ever-incrementing use of mobile phones in today's life, the proper disposal of waste mobile phones has been of paramount importance. Consequently, the tremendous volume of end-of-life (EOL) mobile phones worldwide calls for a sustainable management system to decide on waste mobile phone recovery to minimize the cost and environmental impact. This paper presented a decision problem, namely, EOL option determination, to decrease mobile phone waste. Thus, this study suggests a decision support system (DSS) for the iPhone mobile phone (a subset of the electronics industry) in the EOL phase. Customers’ pervading use of social media has led to these platforms being used as a rich data source to extract information. Thus, the proposed DSS analyzes the Twitter databases, collecting mobile phone defect information from the customers. We suggest an ontology-based text mining and a data mining-based technique through the self-organizing map (SOM) for information discovery from the Twitter data. Finally, a multi-objective mathematical model based on sustainability dimensions is developed to correct EOL decisions based on the defective mobile phone components analyzed from Twitter data. The proposed DSS helps manufacturers when a product is returned and decide for proper EOL processes. This study provides a novel insight and can serve as a valuable reference for solving waste management problems using social media data.}
}
@article{SMITH2025102958,
title = {25 years of qualitative research in sport and exercise Psychology: How did it Go? and what now?},
journal = {Psychology of Sport and Exercise},
volume = {81},
pages = {102958},
year = {2025},
issn = {1469-0292},
doi = {https://doi.org/10.1016/j.psychsport.2025.102958},
url = {https://www.sciencedirect.com/science/article/pii/S1469029225001578},
author = {Brett Smith and Martin Roderick and Hester Hockin-Boyers and Javier Monforte and Toni Louise Williams and Patrick Jachyra and Caroline Dodd-Reynolds and Cassandra Phoenix},
keywords = {Qualitative research, Data collection, Analysis, Artificial intelligence, Digital methods},
abstract = {Qualitative research in sport and exercise psychology has grown over the last 25 years. The number of papers published in journals like Psychology of Sport and Exercise has increased. There has also been growth in terms of the variety and sophistication of work published. Such growth suggests qualitative research is in a good place. It is and it isn't. This paper aims to advance qualitative research and be a resource to guide future practice. It first celebrates qualitative research in sport and exercise psychology by offering examples of the positive qualities of work being done. The paper then highlights some problems by attending to the tokenistic engagement with epistemology and ontology plus some associated fallacies. Some thoughts about how qualitative research in sport and exercise psychology might develop are next offered. To help support the ongoing development of qualitative research we attend to interviewing, story completion, analysis, computer-assisted software, artificial intelligence (AI), and digital methods. The paper also provides a note on physical activity guidelines, qualitative research, and policy-making. It closes by reaffirming the need to approach qualitative research as a slow craft that involves people (not AI) making tough, well-thought-out ethical, theoretical, and methodological decisions.}
}
@article{SU2024104101,
title = {Knowledge-based digital twin system: Using a knowlege-driven approach for manufacturing process modeling},
journal = {Computers in Industry},
volume = {159-160},
pages = {104101},
year = {2024},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2024.104101},
url = {https://www.sciencedirect.com/science/article/pii/S0166361524000290},
author = {Chang Su and Yong Han and Xin Tang and Qi Jiang and Tao Wang and Qingchen He},
keywords = {Manufacturing process, Knowledge-based digital twin system, Knowledge graph, Knowledge-driven modeling approach, Knowledge inference},
abstract = {The Knowledge-Based Digital Twin System is a digital twin system developed on the foundation of a knowledge graph, aimed at serving the complex manufacturing process. This system embraces a knowledge-driven modeling approach, aspiring to construct a digital twin model for the manufacturing process, thereby enabling precise description, management, prediction, and optimization of the process. The core of this system lies in the comprehensive knowledge graph that encapsulates all pertinent information about the manufacturing process, facilitating dynamic modeling and iteration through knowledge matching and inference within the knowledge, geometry, and decision model. This approach not only ensures consistency across models but also addresses the challenge of coupling multi-source heterogeneous information, creating a holistic and precise information model. As the manufacturing process deepens and knowledge accumulates, the model's understanding of the process progressively enhances, promoting self-evolution and continuous optimization. The developed knowledge-decision-geometry model acts as the ontological layer within the digital twin framework, laying a foundational conceptual framework for the digital twin of the manufacturing process. Validated on an aero-engine blade production line in a factory, the results demonstrate that the knowledge model, as the core driver, enables continuous self-updating of the geometric model for an accurate depiction of the entire manufacturing process, while the decision model provides deep insights for decision-makers based on knowledge. The system not only effectively controls, predicts, and optimizes the manufacturing process but also continually evolves as the process advances. This research offers a new perspective on the realization of the digital twin for the manufacturing process, providing solid theoretical support with a knowledge-driven approach.}
}
@article{YANG2020350,
title = {Review of built heritage modelling: Integration of HBIM and other information techniques},
journal = {Journal of Cultural Heritage},
volume = {46},
pages = {350-360},
year = {2020},
issn = {1296-2074},
doi = {https://doi.org/10.1016/j.culher.2020.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S129620742030385X},
author = {Xiucheng Yang and Pierre Grussenmeyer and Mathieu Koehl and Hélène Macher and Arnadi Murtiyoso and Tania Landes},
keywords = {Built heritage, BIM, Parametric objects, Semantics, Geometric model, Knowledge},
abstract = {Built heritage documentation involves the 3D modelling of the geometry (typically using 3D computer graphics, photogrammetry and laser scanning techniques) and information management of semantic knowledge (i.e., using Geographic Information System (GIS) and ontology tools). The recent developed Building Information Modelling (BIM) technique combines 3D modelling and information management. One of its modern application is heritage documentation and has generated a new concept of Historic/Heritage Building Information Modelling (HBIM). This paper summarises the applications of these information techniques on the built heritage documentation. We utilise Web of Science Collection to monitor the publications on built heritage documentation. We analyse the research trend in heritage modelling by comparing the attention paid by researchers before and during the 2010s. The results show that photogrammetry is always the most popular method in heritage modelling. More and more works in heritage modelling have begun to use laser scanning, computer science, GIS and especially BIM techniques. Ontologies and 3D computer graphics are traditional ways for heritage documentation. Moreover, we pay attention to the roles of BIM on heritage documentation and conduct a detailed discussion on how to extend the HBIM capabilities by integrating with other techniques. The integration provides possible enhanced functions in HBIM, including accurate parametric modelling from computer graphics, automatic semantic segmentation of 3D point cloud from reality-based modelling, spatial information management and analysis by GIS, and knowledge modelling by ontology.}
}
@article{YANOSY2018821,
title = {Enhancing the common model of cognition with social cognitive components – “the rise of the humans”},
journal = {Procedia Computer Science},
volume = {145},
pages = {821-831},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918323111},
author = {John Yanosy and Chris Wicher},
keywords = {social cognitive, ontology, common model of cognition, knowledge worker},
abstract = {The intent of this paper is to add inspirational context/understanding for the Common Model of Cognition (CMC) by postulating that the model’s definitions and constituent cognitive concepts are influenced by the social and technology contexts individuals participate in and experience and that evolving this common understanding can be facilitated though collaboration to share data and create consistent definitions of the model and its components using an approach already proven to be successful in the biomedical research community. In order to facilitate collaboration and sharing of research data and enable reaching a consensus on the definitions of the CMC and its components, we propose the collaborative development of a framework of ontologies, such as has been accomplished by the biomedical research community with The Open Biological and Biomedical Ontologies (OBO) Foundry.}
}
@article{TRAJANO2021104328,
title = {MedPath: A process-based modeling language for designing care pathways},
journal = {International Journal of Medical Informatics},
volume = {146},
pages = {104328},
year = {2021},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2020.104328},
url = {https://www.sciencedirect.com/science/article/pii/S1386505619311591},
author = {Iago Avelino Trajano and João Bosco {Ferreira Filho} and Flávio Rubens {de Carvalho Sousa} and Ian Litchfield and Philip Weber},
keywords = {Care pathway, Model-driven engineering, Domain Specific Language, Metamodel},
abstract = {Context
Medical professionals and hospitals promote solutions like care pathways and Health Information Systems (HIS) to support medical conduct and improve the quality of medical care.
Purpose
This study proposes MedPath: a Domain Specific Language (DSL) for modeling care pathways based on the paradigms of Model-Based Engineering (MBE) that can be integrated into software solutions.
Procedures
We have developed MedPath's abstract syntax with the Eclipse Modeling Framework by employing Ecore technology and concrete syntax with the Eclipse Sirius.
Findings
We have modeled over 85 care pathways that are in use in 45 hospitals in Brazil. MedPath-originated pathways have been used over 3.2 million times since October 2017. We conducted a survey among the professionals who used MedPath to evaluate user satisfaction.
Conclusions
We believe MedPath can translate any care pathway into an action flow with its current abstractions. MedPath makes care pathways more easily integrated into HIS and electronic patient records, as it enables programmatic modeling and generates consumable artifacts.}
}
@article{LU2025113450,
title = {Temporal knowledge graph fusion with neural ordinary differential equations for the predictive maintenance of electromechanical equipment},
journal = {Knowledge-Based Systems},
volume = {317},
pages = {113450},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113450},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125004976},
author = {Jiawei Lu and Hanyuan Chen and Jianwei Chen and Zhongcheng Xiao and Ren Li and Gang Xiao and Qibing Wang},
keywords = {Predictive maintenance, Knowledge graph, Neural ordinary differential equations, Electromechanical equipment},
abstract = {Predictive Maintenance is the primary strategy for optimizing operational efficiency and reducing the maintenance costs of electromechanical equipment. However, existing Predictive Maintenance approaches suffer from significant shortcomings, such as the inability to learn the dynamic evolution of fault and maintenance events within massive, heterogeneous datasets and the lack of effective models to handle this complex data. To address these issues, we propose a temporal knowledge graph (TKG) reasoning method. First, we construct a TKG based on an ontology defined by the heterogeneous data features of electromechanical equipment. Second, we propose a Dynamic Graph Embedding model, which captures the dynamic evolution of the non-equal-interval events in the TKG by combining neural ordinary differential equations with a graph convolutional neural network. Furthermore, we design a Dynamic Hawkes Transformer to identify the evolutionary process and predicting future events based on historical fault and maintenance data. Finally, we use elevators as a case study to compare the proposed method with other advanced methods and demonstrate its effectiveness in TKG reasoning. Our proposed method excels in fault and maintenance event prediction, as well as time prediction, for electromechanical equipment.}
}
@article{MCGLINN2021103534,
title = {Publishing authoritative geospatial data to support interlinking of building information models},
journal = {Automation in Construction},
volume = {124},
pages = {103534},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103534},
url = {https://www.sciencedirect.com/science/article/pii/S0926580520311146},
author = {Kris McGlinn and Rob Brennan and Christophe Debruyne and Alan Meehan and Lorraine McNerney and Eamonn Clinton and Philip Kelly and Declan O'Sullivan},
keywords = {Building Information Modelling, Geographic Information Systems, Ontology Engineering, Resource Description Framework (RDF), Linked Data},
abstract = {Building Information Modelling (BIM) is a key enabler to support integration of building data within the buildings life cycle (BLC) and is an important aspect to support a wide range of use cases, related to intelligent automation, navigation, energy efficiency, sustainability and so forth. Open building data faces several challenges related to standardization, data interdependency, data access, and security. In addition to these technical challenges, there remains the barrier among BIM developers who wish to protect their intellectual property, as full 3D BIM development requires expertise and effort. This means that there is often limited availability of building data. However, a Linked Data approach to BIM, combined with a supporting national geospatial identifier infrastructure makes interlinking and controlled sharing of BIM models possible. In Ireland, the Ordnance Survey Ireland (OSi) maintains a substantial data set, called Prime2, which includes not only building GIS data (polygon footprint, geodetic coordinate), but also additional building specific data (e.g. form, function and status). The data set also includes change information, recording when changes took place and who captured and validated those changes. This paper presents the development of a national geospatial identifier infrastructure based on an OSi building ontology that supports capturing OSi building data using Resource Description Framework (RDF). The paper details the different steps required to generate the ontology and publish the data. First, an initial analysis of the data set to generate the ontology is discussed. This includes identification of mappings to existing standards, e.g. GeoSPARQL to handle geometries and PROV-O to handle provenance, to the development of R2RML mappings to generate the RDF and the method for deploying the ontology and the building graphs. This data is then made available dependent on different licensing agreements handled by an access control approach. Methods are then presented to support the interlinking of the authoritative data with other building data standards and data sets using geolocation, followed finally by discussion and future work.}
}
@article{MEIER2025104202,
title = {Structured knowledge-based causal discovery: Agentic streams of thought},
journal = {Information Processing & Management},
volume = {62},
number = {5},
pages = {104202},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104202},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325001438},
author = {Sven Meier and Pratik Narendra Raut and Felix Mahr and Nils Thielen and Jörg Franke and Florian Risch},
keywords = {Large Language Models, Agentic Systems, Causal discovery, Expert systems},
abstract = {Causal discovery—the systematic identification of cause-and-effect relationships among variables—forms the cornerstone of causal inference. Its application enables reliable predictions and targeted interventions across complex systems, from medical treatments to engineering processes. Traditional statistical causal discovery methods face significant limitations with high-dimensional data structures, while existing knowledge-based approaches rely on single large-scale models that raise fundamental concerns about computational efficiency and result reliability. The Agentic Stream of Thought (ASoT) addresses these limitations through a novel architecture that orchestrates multiple smaller open-source language models. The framework integrates hierarchical query decomposition with Model Compiler refinement, while dual-stream thought processing enables balanced analysis through parallel evaluation of competing hypotheses. Dedicated Direction and Transitive Processors enhance reasoning by resolving bidirectional relationships and refining transitive pathways. A two-tiered quality gate system and complementary consensus mechanisms—Delphi protocol and Ensemble Synthesis Method—iteratively refine outputs while mitigating hallucination risks. Empirical evaluations across causal discovery benchmarks and question-answering tasks demonstrate that this approach matches or exceeds state-of-the-art models while enabling local deployment, establishing that sophisticated orchestration of smaller models provides a more sustainable path than increasing model scale alone.}
}
@article{ARSLAN20244534,
title = {Exploring Business Events using Multi-source RAG},
journal = {Procedia Computer Science},
volume = {246},
pages = {4534-4540},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.303},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924023226},
author = {Muhammad Arslan and Saba Munawar and Christophe Cruz},
keywords = {Business events, Extraction methods, Large Language Models, Retrieval-Augmented Generation, Dynamic business environments},
abstract = {Business events signify crucial activities within a company, indicating growth opportunities and investment prospects. They encompass various developments such as recruitment drives, market expansions, mergers, and product launches. Understanding these events is vital for businesses seeking to stay updated with market dynamics, as they provide real-time insights into a company’s trajectory. Moreover, comprehending the business events of one company can offer strategic advantages to others, facilitating informed decision-making and fostering collaboration within the business ecosystem. Extracting information about these events involves diverse structured, semi-structured, and unstructured data sources, posing challenges for traditional extraction methods. Despite the promise shown by existing openly available LLMs driven by Generative Artificial Intelligence (GenAI), they face challenges when dealing with domain-specific queries. Retrieval-Augmented Generation (RAG) addresses this challenge by seamlessly integrating multiple external data sources of varying structures. In our study, we demonstrate how RAG with LLM facilitates precise extraction of business events, ensuring adaptability in dynamic business environments where datasets are constantly evolving.}
}
@article{WANG2025100696,
title = {Cloud based collaborative data compression technology for power Internet of Things},
journal = {Egyptian Informatics Journal},
volume = {30},
pages = {100696},
year = {2025},
issn = {1110-8665},
doi = {https://doi.org/10.1016/j.eij.2025.100696},
url = {https://www.sciencedirect.com/science/article/pii/S1110866525000891},
author = {Qiong Wang and Yongbo Zhou and Jianyong Gao},
keywords = {Cloud edge collaboration, Multitask computing, Power Internet of Things, Multi-source heterogeneity, Data compression, Tucker},
abstract = {To address the challenge of explosive data growth in power IoT systems, this study develops a cloud-edge collaborative multi-task computing framework for efficient compression of heterogeneous data. The proposed system builds upon a “microservice-containerization-Kubernetes” architecture that enables parallel processing of multi-source IoT data collected through perception layer devices. At the edge layer, a hybrid performance ontology algorithm first integrates diverse data sources, followed by a two-stage compression approach: wavelet transforms perform initial data aggregation, while tensor Tucker decomposition enables secondary compression for optimized data reduction. Experimental results demonstrate the framework’s effectiveness in maintaining stable IoT network operations while achieving compression ratios below 40%, significantly improving upon traditional methods in both efficiency and reliability for power IoT applications.}
}
@article{WALLACE2022115352,
title = {Tackling communication and analytical problems in environmental planning: Expert assessment of key definitions and their relationships},
journal = {Journal of Environmental Management},
volume = {317},
pages = {115352},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.115352},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722009252},
author = {Kenneth J. Wallace and Christian Wagner and David J. Pannell and Milena Kiatkoski Kim and Abbie A. Rogers},
keywords = {Environmental planning, Values, Assessing definitions, Interval-valued analysis, Framework evaluation, Ontology},
abstract = {Inadequate definition of key terms and their relationships generates significant communication and analytical problems in environmental planning. In this work, we evaluate an ontological framework for environmental planning designed to combat these problems. After outlining the framework and issues addressed, we describe its evaluation by a group of experts representing a range of expertise and institutions. Experts rated their level of agreement with 12 propositions concerning the definitions and models underpinning the framework. These propositions, in turn, were used to assess three assumptions regarding the expected effectiveness of the framework and its contribution to addressing the abovementioned planning problems. In addition to point-based best estimates of their agreement with propositions, expert ratings were also captured on a continuous interval-valued scale. The use of intervals addresses the challenge of measuring and modelling uncertainty associated with complex assessments such as those provided by experts. Combined with written anonymous expert comments, these data provide multiple perspectives on the level of support for the approach. We conclude that the framework can complement existing planning approaches and strengthen key definitions and related models, thus helping avoid communication and analytical problems in environmental planning. Finally, experts highlighted areas that require further development, and we provide recommendations for improving the framework.}
}
@article{DAMICO20222725,
title = {Detecting failure of a material handling system through a cognitive twin},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {2725-2730},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.128},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322021383},
author = {R.D. D'Amico and A. Sarkar and H. Karray and S. Addepalli and J.A. Erkoyuncu},
keywords = {Digital twin, cognitive twin, ontology, BFO, IOF, CCO, knowledge graph, SPARQL, material handling systems, Festo MPS},
abstract = {This paper describes a methodology for developing a digital twin (DT) based on a rich semantic model and principles of system engineering. The aim is to provide a general model of digital twins (DT) that can improve decision making based on semantic reasoning on real-time system monitoring. The methodology has been tested on a laboratory pilot plant that acts as a material handling system. The key contribution of this research is to propose a generic information model for DT using foundational ontology and principles of systems engineering. The efficacy of the proposed methodology is demonstrated by the automatic detection of a component level failure using semantic reasoning.}
}
@article{IHSAN2023101526,
title = {Improving in-text citation reason extraction and classification using supervised machine learning techniques},
journal = {Computer Speech & Language},
volume = {82},
pages = {101526},
year = {2023},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2023.101526},
url = {https://www.sciencedirect.com/science/article/pii/S0885230823000451},
author = {Imran Ihsan and Hameedur Rahman and Asadullah Shaikh and Adel Sulaiman and Khairan Rajab and Adel Rajab},
keywords = {Citation reason classification, Machine learning, Supervised learning},
abstract = {In the last decade, automatic extraction and classification of in-text citations have received immense popularity and have become one of the most frequently used techniques to evaluate research. Due to the large volume of in-text citations in various digital libraries such as Web of Science, Scopus, Google Scholar, Microsoft Academic, etc., machine learning models and natural language processing techniques are being used to extract, classify, and analyze them. Typical automatic in-text classification techniques use sentiment-based classes (Positive, Negative, and Neutral). However, there are cognitive-based schemes as well that classify in-text citations based on the author’s perspective. In such schemes, extracting citation reasons with high recall is challenging. To address this challenge, we have used eight citations’ context and reason classes defined by CCRO (Citation’s Context and Reasons Ontology) to develop a machine learning model to achieve high recall without compromising on precision. We have worked on Association for Computational Linguistics Corpus with over 7000 in-text citations, randomly annotated by experts in CCRO classes. Afterwards, an array of machine-learning models is implemented on the annotated dataset: Support Vector Machine (SVM), Naïve Bayesian (NB), and Random Forest (RF). We have used various part-of-speech (Nouns, Verbs, Adverbs, and Adjectives) as novel features. Our results show that we have outperformed the three comparative models by achieving 91% accuracy.}
}
@article{CHANDRASEKARAN20222257,
title = {Automating Transfer Credit Assessment-A Natural Language Processing-Based Approach},
journal = {Computers, Materials and Continua},
volume = {73},
number = {2},
pages = {2257-2274},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.027236},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822005409},
author = {Dhivya Chandrasekaran and Vijay Mago},
keywords = {Articulation agreements, higher education, natural language processing, semantic similarity},
abstract = {Student mobility or academic mobility involves students moving between institutions during their post-secondary education, and one of the challenging tasks in this process is to assess the transfer credits to be offered to the incoming student. In general, this process involves domain experts comparing the learning outcomes of the courses, to decide on offering transfer credits to the incoming students. This manual implementation is not only labor-intensive but also influenced by undue bias and administrative complexity. The proposed research article focuses on identifying a model that exploits the advancements in the field of Natural Language Processing (NLP) to effectively automate this process. Given the unique structure, domain specificity, and complexity of learning outcomes (LOs), a need for designing a tailor-made model arises. The proposed model uses a clustering-inspired methodology based on knowledge-based semantic similarity measures to assess the taxonomic similarity of LOs and a transformer-based semantic similarity model to assess the semantic similarity of the LOs. The similarity between LOs is further aggregated to form course to course similarity. Due to the lack of quality benchmark datasets, a new benchmark dataset containing seven course-to-course similarity measures is proposed. Understanding the inherent need for flexibility in the decision-making process the aggregation part of the model offers tunable parameters to accommodate different levels of leniency. While providing an efficient model to assess the similarity between courses with existing resources, this research work also steers future research attempts to apply NLP in the field of articulation in an ideal direction by highlighting the persisting research gaps.}
}
@article{KUSUMAWARDANI20241146,
title = {Named entity recognition in the medical domain for Indonesian language health consultation services using bidirectional-lstm-crf algorithm},
journal = {Procedia Computer Science},
volume = {245},
pages = {1146-1156},
year = {2024},
note = {9th International Conference on Computer Science and Computational Intelligence 2024 (ICCSCI 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.344},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924031521},
author = {Renny Pradina Kusumawardani and Katarina Nimas Kusumawati},
keywords = {Biomedical Named Entities, Named-Entity Recognition, Bidirectional Long Short Term Memory, Conditional Random Field},
abstract = {The increased use of health consultation platforms since the pandemic has led to a higher demand for active doctors to conduct consultations. In Indonesia, the average number of doctors is 0.4 doctors per thousand people, far fewer than in developed countries. A solution that can be utilized is the implementation of Natural Language Processing (NLP) and Artificial Intelligence (AI) technologies. These technologies can be used to reduce costs, provide alternative suggestions from the database, deliver appropriate answers, and enable users to find solutions corresponding to their problems. This can be automated using Named Entity Recognition (NER). NER is a part of information extraction used to identify entities in the medical domain, such as anatomical entities, proteins, and genes. The challenge faced in implementing this solution is the lack of Indonesian language datasets for NER that are relevant to the context of health consultation platforms. Therefore, the development of a medical field dataset in the Indonesian language is necessary. In the execution of this research, data used was taken from online health consultation platforms where the Q&A sections with doctors are freely accessible. The data was manually labeled under the supervision of experts. The data was trained using the Bidirectional-LSTM-CRF model and resulted in an accuracy of 0.9968. There is a state-of-the-art model, XLM-RoBERTa-large-indonesian-NER, which after fine-tuning, achieved an accuracy of 0.9851. However, using the F1 score metric, the XLM-RoBERTa model achieved the highest score for each tag compared to the Bidirectional-LSTM-CRF model.}
}
@article{SHEN2024105686,
title = {Knowledge-based semantic web technologies in the AEC sector},
journal = {Automation in Construction},
volume = {167},
pages = {105686},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105686},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524004229},
author = {Xiao-han Shen and Samad M.E. Sepasgozar and Michael J. Ostwald},
keywords = {Semantic web, Linked data, Ontology, Architecture engineering and construction, AEC knowledge graph, Information management, Systematic review},
abstract = {Semantic web technologies are essential for seamless data exchange between Building Information Modelling and technologies such as the Internet of Things and Geographic Information Systems. However, rapid advancements complicate their implementation, making past reviews obsolete. This paper investigates recent applications of semantic web technologies in the construction industry and addresses why and where semantic web technologies are adopted, how and when they are used, and what the significant knowledge gaps undermining semantic web technologies' applications are. These questions collectively provide a much-needed context for this evolving field. A systematic literature review covering the past five years identifies 65 relevant papers from 419 works. Findings indicate the dominance of the entire building lifecycle applications (27.7%), and a trend shifting to specific lifecycle stages. This study provides insights into the latest applications of semantic web technologies and their potential to enhance efficiency and interoperability in the AEC sector.}
}
@article{DIAS202468,
title = {Development and Implementation of the BrCris and BrCris-T Platform: Integration of Scientific and Technological Data in Brazil},
journal = {Procedia Computer Science},
volume = {249},
pages = {68-77},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.050},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924032617},
author = {Thiago Magela Rodrigues Dias and Rene Faustino Gabriel Junior and Vivian dos Santos Silva and Marcel Garcia {de Souza} and Alexandre Faria {de Oliveira} and Washington Luís R. de Carvalho Segundo},
keywords = {CRIS. BrCris. Open Science. Research Ecosystem},
abstract = {The BrCris project aims to create a unified platform integrating scientific and technological data across Brazil, promoting Open Science and adhering to FAIR principles (Findable, Accessible, Interoperable, and Reusable). Developed by the Brazilian Institute of Information in Science and Technology (IBICT), BrCris addresses the challenge of disparate data sources by utilizing models, ontologies, and semantic data based on the VIVO ontology to ensure comprehensive integration and accessibility. The platform aggregates data on researchers, projects, institutions, patents, software, and research outputs into a single repository, enhancing interoperability and usability. Key features include a logical level for entity-relationship modeling based on CERIF, and a semantic level for data visualization as a knowledge graph. The initial focus on scientific publications has laid the foundation for broader integration, including technological innovation data through the BrCris-T extension. This project underscores the significance of fostering collaboration, knowledge exchange, and technological advancement within Brazil's research ecosystem, while ensuring data quality and representation through continuous research and validation efforts. Launched in July 2023, BrCris stands as a pivotal tool for research management, decision-making, and the democratization of scientific knowledge, contributing to a more transparent, accessible, and collaborative scientific community.}
}
@article{BUONCOMPAGNI2022100952,
title = {OWLOOP: A modular API to describe OWL axioms in OOP objects hierarchies},
journal = {SoftwareX},
volume = {17},
pages = {100952},
year = {2022},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2021.100952},
url = {https://www.sciencedirect.com/science/article/pii/S2352711021001801},
author = {Luca Buoncompagni and Syed Yusha Kareem and Fulvio Mastrogiovanni},
keywords = {Object-Oriented Programming (OOP), Ontology Web Language (OWL), Application Programming Interface (API), Software architecture},
abstract = {OWLOOP is an Application Programming Interface (API) for using the Ontology Web Language (OWL) by the means of Object-Oriented Programming (OOP). It is common to design software architectures using the OOP paradigm for increasing their modularity. If the components of an architecture also exploit OWL ontologies for knowledge representation and reasoning, they would require to be interfaced with OWL axioms. Since OWL does not adhere to the OOP paradigm, such an interface often leads to boilerplate code affecting modularity, and OWLOOP is designed to address this issue as well as the associated computational aspects. We present an extension of the OWL-API to provide a general-purpose interface between OWL axioms subject to reasoning and modular OOP objects hierarchies.}
}
@article{BOURDIN2024396,
title = {NLP in SMEs for industry 4.0: opportunities and challenges},
journal = {Procedia Computer Science},
volume = {239},
pages = {396-403},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.186},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924014297},
author = {Mathieu Bourdin and Thomas Paviot and Robert Pellerin and Samir Lamouri},
keywords = {Natural Language Processing, Industry 4.0, Machine Learning, Large Language Models, SMEs},
abstract = {Natural Language Processing is the field of Computer Science that focuses on analyzing and processing natural language, mainly human text or speech. Recent trends in Natural Language Processing have led to the development of Large Language Models (LLMs): huge models trained on high amounts of data that achieve unprecedented performances in many tasks, such as answering questions, summarizing texts, or coding. These new tools have a wide range of applications and are being developed by many companies. However, Small and Medium Enterprises (SMEs) struggle to implement these new technologies, mainly because of the lack of resources. This paper aims to show the opportunities and challenges related to NLP-based solutions in SMEs based on a literature review. The main result is that NLP-based solutions have a wide range of applications in various companies, including SMEs, and may lead to many changes. However, there are still many obstacles to developing these tools in SMEs: SMEs lack specialized know-how to develop these solutions and do not often have standardized data. Moreover, there exists nearly no support for SMEs in the scientific literature to develop these tools.}
}
@article{SYCHEV2024101261,
title = {Educational models for cognition: Methodology of modeling intellectual skills for intelligent tutoring systems},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101261},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101261},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400055X},
author = {Oleg Sychev},
keywords = {Reasoning modeling, Constraint-based modeling, Intelligent tutoring systems},
abstract = {Automation of teaching people new skills requires modeling of human reasoning because human cognition involves active reasoning over the new subject domain to acquire skills that will later become automatic. The article presents Thought Process Trees — a language for modeling human reasoning that was created to facilitate the development of intelligent tutoring systems, which can perform the same reasoning that is expected of a student and find deficiencies in their line of thinking, providing explanatory messages and allowing them to learn from performance errors. The methodology of building trees which better reflect human learning is discussed, with examples of design choices during the modeling process and their consequences. The characteristics of educational modeling that impact building subject-domain models for intelligent tutoring systems are discussed. The trees were formalized and served as a basis for developing a framework for constructing intelligent tutoring systems. This significantly lowered the time required to build and debug a constraint-based subject-domain model. The framework has already been used to develop five intelligent tutoring systems and their prototypes and is being used to develop more of them.}
}
@article{POWELL2023,
title = {Examining the Use of Text Messages Among Multidisciplinary Care Teams to Reduce Avoidable Hospitalization of Nursing Home Residents with Dementia: Protocol for a Secondary Analysis},
journal = {JMIR Research Protocols},
volume = {12},
year = {2023},
issn = {1929-0748},
doi = {https://doi.org/10.2196/50231},
url = {https://www.sciencedirect.com/science/article/pii/S1929074823003190},
author = {Kimberly R Powell and Mihail Popescu and Suhwon Lee and David R Mehr and Gregory L Alexander},
keywords = {age-friendly health systems, Alzheimer disease, communication, dementia, nursing homes, older adults},
abstract = {Background
Reducing avoidable nursing home (NH)–to-hospital transfers of residents with Alzheimer disease or a related dementia (ADRD) has become a national priority due to the physical and emotional toll it places on residents and the high costs to Medicare and Medicaid. Technologies supporting the use of clinical text messages (TMs) could improve communication among health care team members and have considerable impact on reducing avoidable NH-to-hospital transfers. Although text messaging is a widely accepted mechanism of communication, clinical models of care using TMs are sparsely reported in the literature, especially in NHs. Protocols for assessing technologies that integrate TMs into care delivery models would be beneficial for end users of these systems. Without evidence to support clinical models of care using TMs, users are left to design their own methods and protocols for their use, which can create wide variability and potentially increase disparities in resident outcomes.
Objective
Our aim is to describe the protocol of a study designed to understand how members of the multidisciplinary team communicate using TMs and how salient and timely communication can be used to avert poor outcomes for NH residents with ADRD, including hospitalization.
Methods
This project is a secondary analysis of data collected from a Centers for Medicare & Medicaid Services (CMS)–funded demonstration project designed to reduce avoidable hospitalizations for long-stay NH residents. We will use two data sources: (1) TMs exchanged among the multidisciplinary team across the 7-year CMS study period (August 2013-September 2020) and (2) an adapted acute care transfer tool completed by advanced practice registered nurses to document retrospective details about NH-to-hospital transfers. The study is guided by an age-friendly model of care called the 4Ms (What Matters, Medications, Mentation, and Mobility) framework. We will use natural language processing, statistical methods, and social network analysis to generate a new ontology and to compare communication patterns found in TMs occurring around the time NH-to-hospital transfer decisions were made about residents with and without ADRD.
Results
After accounting for inclusion and exclusion criteria, we will analyze over 30,000 TMs pertaining to over 3600 NH-to-hospital transfers. Development of the 4M ontology is in progress, and the 3-year project is expected to run until mid-2025.
Conclusions
To our knowledge, this project will be the first to explore the content of TMs exchanged among a multidisciplinary team of care providers as they make decisions about NH-to-hospital resident transfers. Understanding how the presence of evidence-based elements of high-quality care relate to avoidable hospitalizations among NH residents with ADRD will generate knowledge regarding the future scalability of behavioral interventions. Without this knowledge, NHs will continue to rely on ineffective and outdated communication methods that fail to account for evidence-based elements of age-friendly care.
International Registered Report Identifier (IRRID)
DERR1-10.2196/50231}
}
@article{LING2024102725,
title = {Hybrid NLP-based extraction method to develop a knowledge graph for rock tunnel support design},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102725},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102725},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624003732},
author = {Jiaxin Ling and Xiaojun Li and Haijiang Li and Yi An and Yi Rui and Yi Shen and Hehua Zhu},
keywords = {Tunnel support design, Knowledge graph, Natural language processing, Knowledge extraction, Named entity recognition (NER), Ontology},
abstract = {In the realm of drill and blast (D&B) tunneling, the design of tunnel support emerges as a pivotal concern. As a typical knowledge-intensive task, the practical application of tunnel support design often relies extensively on experienced engineers or experts who formulate designs based on their expertise. Compared with data-driven approaches, knowledge-driven tunnel support design allows for a nuanced understanding of the intricate geological and engineering factors influencing support design, and yield results that are more transparent and explainable. However, in practice, the substantial knowledge sources and complex interrelationships between factors which have direct or indirect influences on support design hinder the implementation of knowledge-based support design methods. To solve such knowledge gap, this study proposed a hybrid natural language processing (NLP)-based method to develop a knowledge graph for rock tunnel design. Specifically, rule-based methods are used to extract entities and classification relationships from standards and specifications, and statistical and artificial intelligence (AI)-based methods are used for extracting entities and non-classification relationships from a large amount of scientific literature, academic papers, and support design schemes. A total of 947 entities, 3 kinds of classification relationships and 11 kinds of non-classification relationships were extracted. On the basis of extracted entities, relations, and attributes, a knowledge graph was developed using ontology-based method, providing functionalities such as knowledge element retrieval, semantic retrieval, and query expansion. The findings of this study are expected to provide practical recommendations for the design of the tunnel support and advance the existing knowledge about the tunnel design from a knowledge-driven perspective.}
}
@article{ALBUKHITAN2019385,
title = {Semantic Web Annotation using Deep Learning with Arabic Morphology},
journal = {Procedia Computer Science},
volume = {151},
pages = {385-392},
year = {2019},
note = {The 10th International Conference on Ambient Systems, Networks and Technologies (ANT 2019) / The 2nd International Conference on Emerging Data and Industry 4.0 (EDI40 2019) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919305150},
author = {Saeed Albukhitan and Ahmed Alnazer and Tarek Helmy},
keywords = {Deep Learning, Semantic Annotation, Arabic Language, Ontology},
abstract = {In order to realize the vision of Semantic Web, which is a Web of things instead of Web of documents, there is a need to convert existing Web of documents into Semantic content that could be processed by machines. Semantic annotation tool could be used to perform this task through using common and public ontologies. Due to exponential growth and the huge size of Web sources, there is a need to have a fast and automatic Semantic annotation of Web documents. The aim of this paper is to investigate the use of word embeddings from deep learning algorithms to semantically annotate the Arabic Web documents. To enhance the performance of the Semantic annotation, we utilized the complex morphological structure of Arabic words. Moreover, evaluating the performance of the proposed framework requires selecting a set of domain ontologies with relevant and annotated related documents. The proposed framework produces Semantic annotations for these documents by using different standard output formats. The initial results show a promising performance that will support the research in the Semantic Web with respect to Arabic language.}
}
@article{VISAKKO202039,
title = {Language and happiness: Cultural epistemologies and ideological conflicts in Finnish online discourses on the causes of happiness},
journal = {Language & Communication},
volume = {71},
pages = {39-54},
year = {2020},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2019.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0271530919302812},
author = {Tomi Visakko and Eero Voutilainen},
keywords = {Happiness, Online discourses, Self-help, Epistemology, Ideology, Identity},
abstract = {This article approaches “happiness” as a discursive construct. We examine different understandings of happiness as socially transmitted, linguistically formulated epistemological models by which humans reflexively evaluate and rationalize their experiences and identities. Our data consists of a set of Finnish online discourses that discuss the causes of happiness. We examine media platforms that mass-mediate popular discourses of happiness to individuals and allow individuals to voice their “indigenous” understandings. We analyze the linguistic, interactional, and interdiscursive characteristics of such views of happiness and show how they emerge and circulate in society disseminating mutually competing epistemologies of happiness. We also aim to show how the cross-cultural and culture-internal variation of these models becomes linked to ideological conflicts and politics of identity. On the one hand, our examples echo the kinds of individualistic, depoliticized views of happiness that have been seen as characteristic of modern media. On the other hand, they show that smaller-scale, local views of happiness tend to be more varied and attract explicit identity-political debates.}
}
@article{LLOPIS2023100922,
title = {A deep learning model for natural language querying in Cyber–Physical Systems},
journal = {Internet of Things},
volume = {24},
pages = {100922},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100922},
url = {https://www.sciencedirect.com/science/article/pii/S2542660523002457},
author = {Juan Alberto Llopis and Antonio Jesús Fernández-García and Javier Criado and Luis Iribarne and Rosa Ayala and James Z. Wang},
keywords = {Deep learning, Recommender system, Transformer, Web of Things, Natural language},
abstract = {As a result of technological advancements, the number of IoT devices and services is rapidly increasing. Due to the increasing complexity of IoT devices and the various ways they can operate and communicate, finding a specific device can be challenging because of the complex tasks they can perform. To help find devices in a timely and efficient manner, in environments where the user may not know what devices are available or how to access them, we propose a recommender system using deep learning for matching user queries in the form of a natural language sentence with Web of Things (WoT) devices or services. The Transformer, a recent attention-based algorithm that gets superior results for natural language problems, is used for the deep learning model. Our study shows that the Transformer can be a recommendation tool for finding relevant WoT devices in Cyber–Physical Systems (CPSs). With hashing as an encoding technique, the proposed model returns the relevant devices with a high grade of confidence. After experimentation, the proposed model is validated by comparing it with our current search system, and the results are discussed. The work can potentially impact real-world application scenarios when many different devices are involved.}
}
@article{KAY2022100069,
title = {Enhancing learning by Open Learner Model (OLM) driven data design},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100069},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100069},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000248},
author = {Judy Kay and Kathryn Bartimote and Kirsty Kitto and Bob Kummerfeld and Danny Liu and Peter Reimann},
keywords = {Open Learner Models (OLMs), Learner models, Student models, Scrutability, Ontologies, Self-regulated learning, Learning analytics},
abstract = {There is a huge and growing amount of data that is already captured in the many, diverse digital tools that support learning. Additionally, learning data is often inaccessible to teachers or served in a manner that fails to support or inform their teaching and design practice. We need systematic, learner-centred ways for teachers to design learning data that supports them. Drawing on decades of Artificial Intelligence in Education (AIED) research, we show how to make use of important AIED concepts: (1) learner models; (2) Open Learner Models (OLMs); (3) scrutability and (4) Ontologies. We show how these concepts can be used in the design of OLMs, interfaces that enable a learner to see and interact with an externalised representation of their learning progress. We extend this important work by demonstrating how OLMs can also drive a learner-centred design process of learning data. We draw on the work of Biggs on constructive alignment (Biggs, 1996, 1999, 2011), which has been so influential in education. Like Biggs, we propose a way for teachers to design the learning data in their subjects and we illustrate the approach with case studies. We show how teachers can use this approach today, essentially integrating the design of learning data along with the learning design for their subjects. We outline a research agenda for designing the collection of richer learning data. There are three core contributions of this paper. First, we present the terms OLM, learner model, scrutability and ontologies, as thinking tools for systematic design of learning data. Second, we show how to integrate this into the design and refinement of a subject. Finally, we present a research agenda for making this process both easier and more powerful.}
}
@article{GAVRIILIDIS20241886,
title = {A mini-review on perturbation modelling across single-cell omic modalities},
journal = {Computational and Structural Biotechnology Journal},
volume = {23},
pages = {1886-1896},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.04.058},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024001417},
author = {George I. Gavriilidis and Vasileios Vasileiou and Aspasia Orfanou and Naveed Ishaque and Fotis Psomopoulos},
keywords = {Perturbation, Single-cell RNA sequencing, ScRNAseq, Machine learning, Deep learning},
abstract = {Recent advances in single-cell omics technology have transformed the landscape of cellular and molecular research, enriching the scope and intricacy of cellular characterisation. Perturbation modelling seeks to comprehensively grasp the effects of external influences like disease onset or molecular knock-outs or external stimulants on cellular physiology, specifically on transcription factors, signal transducers, biological pathways, and dynamic cell states. Machine and deep learning tools transform complex perturbational phenomena in algorithmically tractable tasks to formulate predictions based on various types of single-cell datasets. However, the recent surge in tools and datasets makes it challenging for experimental biologists and computational scientists to keep track of the recent advances in this rapidly expanding filed of single-cell modelling. Here, we recapitulate the main objectives of perturbation modelling and summarise novel single-cell perturbation technologies based on genetic manipulation like CRISPR or compounds, spanning across omic modalities. We then concisely review a burgeoning group of computational methods extending from classical statistical inference methodologies to various machine and deep learning architectures like shallow models or autoencoders, to biologically informed approaches based on gene regulatory networks, and to combinatorial efforts reminiscent of ensemble learning. We also discuss the rising trend of large foundational models in single-cell perturbation modelling inspired by large language models. Lastly, we critically assess the challenges that underline single-cell perturbation modelling while pointing towards relevant future perspectives like perturbation atlases, multi-omics and spatial datasets, causal machine learning for interpretability, multi-task learning for performance and explainability as well as prospects for solving interoperability and benchmarking pitfalls.}
}
@article{SPOLADORE2024108193,
title = {A Knowledge-based Decision Support System for recommending safe recipes to individuals with dysphagia},
journal = {Computers in Biology and Medicine},
volume = {171},
pages = {108193},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108193},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524002774},
author = {Daniele Spoladore and Vera Colombo and Vania Campanella and Christian Lunetta and Marta Mondellini and Atieh Mahroo and Federica Cerri and Marco Sacco},
keywords = {Dysphagia, Ontology-based decision support system, Clinical decision support system, Neuromuscular diseases, Nutrition, Dysphagic patient support},
abstract = {Background
Dysphagia is a disorder that can be associated to several pathological conditions, including neuromuscular diseases, with significant impact on quality of life. Dysphagia often leads to malnutrition, as a consequence of the dietary changes made by patients or their caregivers, who may deliberately decide to reduce or avoid specific food consistencies (because they are not perceived as safe), and the lack of knowledge in how to process foods are critics. Such dietary changes often result in unbalanced nutrients intake, which can have significant consequences for frail patients. This paper presents the development of a prototypical novel ontology-based Decision Support System (DSS) to support neuromuscular patients with dysphagia (following a per-oral nutrition) and their caregivers in preparing nutritionally balanced and safe meals.
Method
After reviewing scientific literature, we developed in collaboration with Ear-Nose-Throat (ENT) specialists, neurologists, and dieticians the DSS formalizes expert knowledge to suggest recipes that are considered safe according to patient's consistency limitations and dysphagia severity and also nutritionally well-balanced.
Results
The prototype can be accessed via digital applications both by physicians to generate and verify the recommendations, and by the patients and their caregivers to follow the step-by-step procedures to autonomously prepare and process one or more recipe. The system is evaluated with 9 clinicians to assess the quality of the DSS's suggested recipes and its acceptance in clinical practice.
Conclusions
Preliminary results suggest a global positive outcome for the recipes inferred by the DSS and a good usability of the system.}
}
@article{SARKAR2019513,
title = {On Semantic Interoperability of Model-based Definition of Product Design},
journal = {Procedia Manufacturing},
volume = {38},
pages = {513-523},
year = {2019},
note = {29th International Conference on Flexible Automation and Intelligent Manufacturing ( FAIM 2019), June 24-28, 2019, Limerick, Ireland, Beyond Industry 4.0: Industrial Advances, Engineering Education and Intelligent Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.01.065},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920300664},
author = {Arkopaul Sarkar and Dušan Šormaz},
keywords = {Product design, Ontology, Model-based Design, OWL},
abstract = {Fundamentally, the engineering designs are collections of information, which contain the structural, qualitative and the behavioral (kinematic and static) specifications of some physical products that satisfy some customer requirements. Vast suits of information exchange standards, most notably ISO10303 (STEP), are currently being used for capturing product design in manufacturing industries. In recent years, a modern practice of capturing the product design information using model-based definition (MBD) (STEP AP242, ASME Y14.5) is becoming increasingly popular due to its ability to capture geometrical dimension and tolerances (GD&T) against managed models of 3D design features instead of traditional 2D drafting, facilitating communication of the design intents, and product model information (PMI). However, MBD suffers from difficulties in standardization of managed models due to diversity of views, stemming from cultural and traditional differences among the product designers and organizations. In the wake of semantic web technologies, researchers proposed number of semantic models for capturing product design concepts based on ontological taxonomy and rules, aiming to integrate heterogeneous sources of data based on contextual meaning of the information. In this article, we present an upper level ontology for capturing MBD based product designs, including axiomatic definitions of concepts related to product design (e.g. part, topology, feature, dimension, and tolerance). The primary goal of such semantic model is to increase the interoperability of MBD based product design and integrate domain-specific design features from manufacturability perspective (CNC, sheet metal, molding, 3D printing). In order to achieve such cross-domain interoperability, we based our design on Basic Formal Ontology (BFO). The applicability of the proposed ontology is elucidated by presenting usage pattern based on example design.}
}
@article{MAYOR2022,
title = {Developing a Long COVID Phenotype for Postacute COVID-19 in a National Primary Care Sentinel Cohort: Observational Retrospective Database Analysis},
journal = {JMIR Public Health and Surveillance},
volume = {8},
number = {8},
year = {2022},
issn = {2369-2960},
doi = {https://doi.org/10.2196/36989},
url = {https://www.sciencedirect.com/science/article/pii/S2369296022001806},
author = {Nikhil Mayor and Bernardo Meza-Torres and Cecilia Okusi and Gayathri Delanerolle and Martin Chapman and Wenjuan Wang and Sneha Anand and Michael Feher and Jack Macartney and Rachel Byford and Mark Joy and Piers Gatenby and Vasa Curcin and Trisha Greenhalgh and Brendan Delaney and Simon {de Lusignan}},
keywords = {medical record systems, computerized, Systematized Nomenclature of Medicine, postacute COVID-19 syndrome, phenotype, COVID-19, long COVID, ethnicity, social class, general practitioners, data accuracy, data extracts, biomedical ontologies, SARS-CoV-2, hospitalization, epidemiology, surveillance, public health, BioPortal, electronic health record, disease management, digital tool},
abstract = {Background
Following COVID-19, up to 40% of people have ongoing health problems, referred to as postacute COVID-19 or long COVID (LC). LC varies from a single persisting symptom to a complex multisystem disease. Research has flagged that this condition is underrecorded in primary care records, and seeks to better define its clinical characteristics and management. Phenotypes provide a standard method for case definition and identification from routine data and are usually machine-processable. An LC phenotype can underpin research into this condition.
Objective
This study aims to develop a phenotype for LC to inform the epidemiology and future research into this condition. We compared clinical symptoms in people with LC before and after their index infection, recorded from March 1, 2020, to April 1, 2021. We also compared people recorded as having acute infection with those with LC who were hospitalized and those who were not.
Methods
We used data from the Primary Care Sentinel Cohort (PCSC) of the Oxford Royal College of General Practitioners (RCGP) Research and Surveillance Centre (RSC) database. This network was recruited to be nationally representative of the English population. We developed an LC phenotype using our established 3-step ontological method: (1) ontological step (defining the reasoning process underpinning the phenotype, (2) coding step (exploring what clinical terms are available, and (3) logical extract model (testing performance). We created a version of this phenotype using Protégé in the ontology web language for BioPortal and using PhenoFlow. Next, we used the phenotype to compare people with LC (1) with regard to their symptoms in the year prior to acquiring COVID-19 and (2) with people with acute COVID-19. We also compared hospitalized people with LC with those not hospitalized. We compared sociodemographic details, comorbidities, and Office of National Statistics–defined LC symptoms between groups. We used descriptive statistics and logistic regression.
Results
The long-COVID phenotype differentiated people hospitalized with LC from people who were not and where no index infection was identified. The PCSC (N=7.4 million) includes 428,479 patients with acute COVID-19 diagnosis confirmed by a laboratory test and 10,772 patients with clinically diagnosed COVID-19. A total of 7471 (1.74%, 95% CI 1.70-1.78) people were coded as having LC, 1009 (13.5%, 95% CI 12.7-14.3) had a hospital admission related to acute COVID-19, and 6462 (86.5%, 95% CI 85.7-87.3) were not hospitalized, of whom 2728 (42.2%) had no COVID-19 index date recorded. In addition, 1009 (13.5%, 95% CI 12.73-14.28) people with LC were hospitalized compared to 17,993 (4.5%, 95% CI 4.48-4.61; P<.001) with uncomplicated COVID-19.
Conclusions
Our LC phenotype enables the identification of individuals with the condition in routine data sets, facilitating their comparison with unaffected people through retrospective research. This phenotype and study protocol to explore its face validity contributes to a better understanding of LC.}
}
@article{LEPUSCHITZ2018107,
title = {Model-based development and application generation for the batch process industry},
journal = {Manufacturing Letters},
volume = {15},
pages = {107-110},
year = {2018},
note = {Industry 4.0 and Smart Manufacturing},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2017.12.016},
url = {https://www.sciencedirect.com/science/article/pii/S2213846317301013},
author = {Wilfried Lepuschitz and Alvaro Lobato-Jimenez and Andreas Grün and Timon Höbert and Munir Merdan},
keywords = {Batch process, Ontology, Model-based development, Automated generation, SCADA},
abstract = {In the context of required flexibility for production systems due to dynamic product life cycles and market demands, the application of semantic technologies is regarded as a beneficial approach. This paper describes the development workflow of an ontology-based model for representing concepts of the batch process domain towards the automated generation of an application in an industrial supervisory control and data acquisition (SCADA) system. The presented approach shows the feasibility of model-based development for industrial practice by integrating the ontology in an SQL database, which is accessible for the utilized industrial SCADA software.}
}
@article{CHEN20211884,
title = {Multi-sourced Modelling for Strip Breakage using Knowledge Graph Embeddings},
journal = {Procedia CIRP},
volume = {104},
pages = {1884-1889},
year = {2021},
note = {54th CIRP CMS 2021 - Towards Digitalized Manufacturing 4.0},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.11.318},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121012166},
author = {Zheyuan Chen and Ying Liu and Agustin Valera-Medina and Fiona Robinson},
keywords = {Strip Breakage, Cold Rolling, Multi-sourced data, Ontology, Knowledge Graph},
abstract = {Strip breakage is an undesired production failure in cold rolling. Typically, conventional studies focused on cause analyses, and existing data-driven approaches only rely on a single data source, resulting in a limited amount of information. Hence, we propose an approach for modelling breakage using multiple data sources. Many breakage-relevant features from multiple sources are identified and used, and these features are integrated using a breakage-centric ontology which is then used to create knowledge graphs. Through ontology construction and knowledge embedding, a real-world study using data from a cold-rolled strip manufacturer was conducted using the proposed approach.}
}
@article{BISWAS20231326,
title = {Demonstrating efficiency through data connectivity between asset management systems and IM},
journal = {Transportation Research Procedia},
volume = {72},
pages = {1326-1333},
year = {2023},
note = {TRA Lisbon 2022 Conference Proceedings Transport Research Arena (TRA Lisbon 2022),14th-17th November 2022, Lisboa, Portugal},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.11.594},
url = {https://www.sciencedirect.com/science/article/pii/S235214652300892X},
author = {Sukalpa Biswas and Alex Wright and John Proust and Tadas Andriejauskas and Carl {Van Geem} and Darko Kokot and António Antunes and Vânia Marecos and José Barateiro and Shubham Bhusari and Jelena Petrović},
keywords = {Asset management, BIM, Data dictionary, Linked data, Ontology, Digital Twin},
abstract = {The CoDEC (Connected Data for Effective Collaboration) project has developed standardised tools – an Asset Data Dictionary and associated Ontology – to support data integration between Building Information Modelling (BIM) and Asset Management Systems (AMS). CoDEC has also developed an OpenAPI (Application Protocol Interface) to enable automatic data connection between the two systems. Having developed the standardised process, the potential benefits offered for connecting and linking data have been shown in three pilot projects to demonstrate (1) how tunnel monitoring data can enrich BIM models of tunnels (2) how data from bridge sensors can be linked and visualised and (3) how highway data from a BIM model can be linked to GIS-based AMS for the operational phase. This paper describes the project outcomes.}
}
@article{XU20247,
title = {Automatic semantic modeling of structured data sources with cross-modal retrieval},
journal = {Pattern Recognition Letters},
volume = {177},
pages = {7-14},
year = {2024},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2023.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167865523003240},
author = {Ruiqing Xu and Wolfgang Mayer and Hailong Chu and Yitao Zhang and Hong-Yu Zhang and Yulong Wang and Youfa Liu and Zaiwen Feng},
keywords = {Semantic model, Ontology, Cross-modal retrieval, Attention mechanism, Graph representation learning},
abstract = {Analyzing and modeling the implicit semantic relationships in data sources is the key to achieving integration and sharing of heterogeneous data information. However, manual modeling of data semantics is a laborious and error-prone task that demands significant human effort and expertise. The paper proposes a novel explainable representation learning-based method that adopts an attention-based table-graph cross-modal retrieval model as a rating function during incremental search for automatic semantic modeling. Our supervised model utilizes the graph representation learning technique to extract latent semantics from data and aims to retrieve the most reliable semantic model for structured data sources. Experimental results demonstrate the effectiveness and robustness of our method.}
}
@article{SLENTER202481,
title = {Discovering life's directed metabolic (sub)paths to interpret human biochemical markers using the DSMN tool††Electronic supplementary information (ESI) available: Data and processing scripts for this paper, including metabolic aging biomarker data are available at GitHub at [https://github.com/cyneo4j/DSMN]; the Neo4j database containing the DSMN is available at Zenodo at [https://doi.org/10.5281/zenodo.7113243]. See DOI: https://doi.org/10.1039/d3dd00069a},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {81-98},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00069a},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000093},
author = {Denise Slenter and Martina Kutmon and Chris T. Evelo and Egon L. Willighagen},
abstract = {Metabolomics data analysis for phenotype identification commonly reveals only a small set of biochemical markers, often containing overlapping metabolites for individual phenotypes. Differentiation between distinctive sample groups requires understanding the underlying causes of metabolic changes. However, combining biomarker data with knowledge of metabolic conversions from pathway databases is still a time-consuming process due to their scattered availability. Here, we integrate several resources through ontological linking into one unweighted, directed, labeled bipartite property graph database for human metabolic reactions: the Directed Small Moleicules Network (DSMN). This approach resolves several issues currently experienced in metabolic graph modeling and data visualization for metabolomics data, by generating (sub)networks of explainable biochemical relationships. Three datasets measuring human biomarkers for healthy aging were used to validate the results from shortest path calculations on the biochemical reactions captured in the DSMN. The DSMN is a fast solution to find and visualize biological pathways relevant to sparse metabolomics datasets. The generic nature of this approach opens up the possibility to integrate other omics data, such as proteomics and transcriptomics.}
}
@article{CHENG2024109361,
title = {A link prediction method for Chinese financial event knowledge graph based on graph attention networks and convolutional neural networks},
journal = {Engineering Applications of Artificial Intelligence},
volume = {138},
pages = {109361},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109361},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624015197},
author = {Haitao Cheng and Ke Wang and Xiaoying Tan},
keywords = {Chinese financial event knowledge graph, Link prediction, Graph attention network, Convolutional neural network, Large language model},
abstract = {Finance is a knowledge-intensive domain in nature, with its data containing a significant amount of interconnected information. Constructing a financial knowledge graph is an important application for transforming financial text/web content into machine-readable data. However, the complexity of Chinese financial knowledge and the dynamic and evolving nature of Chinese financial data often lead to incomplete knowledge graphs. To address this challenge, we propose a novel link prediction method for Chinese financial event knowledge graph based on Graph Attention Networks and Convolutional Neural Networks. Our method begins with the construction of the foundational Chinese financial event knowledge graph using a relational triple extraction module integrated with a large language model framework, along with a Prompting with Iterative Verification (PiVe) module for validation. To enhance the completeness of the knowledge graph, we introduce an encoder-decoder framework, where a graph attention network with joint embeddings of financial event entities and relations acts as the encoder, while a Convolutional Knowledge Base embedding model (ConvKB) serves as the decoder. This framework effectively aggregates crucial neighbor information and captures global relationships among entity and relation embeddings. Extensive comparative experiments demonstrate the utility and accuracy of this method, ultimately enabling the effective completion of Chinese financial event knowledge graphs.}
}
@article{ABDELFATTAHSALEH2024121433,
title = {TxLASM: A novel language agnostic summarization model for text documents},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121433},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121433},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019358},
author = {Ahmed {Abdelfattah Saleh} and Li Weigang},
keywords = {Language agnostic summarization, Extractive summarization model, Domain agnostic summarization, TxLASM, Text shape-encoding},
abstract = {In Natural Language Processing (NLP) domain, the majority of automatic text summarization approaches depend on a prior knowledge of the language and/or the domain of the text being summarized. Such approaches requires language dependent part-of-speech taggers, parsers, databases, pre-structured lexicons, etc. In this research, we propose a novel automatic text summarization model, Text Documents - Language Agnostic Summarization Model (TxLASM), which is able to perform extractive text summarization task in language/domain agnostic manner. TxLASM depends on specific characteristics of the major elements of the text being summarized rather than its domain, context, or language and thus rules out the need for language dependent pre-processing tools, taggers, parsers, lexicons or databases. Within TxLASM, we present a novel technique for encoding the shapes of major text elements (paragraphs, sentences, n-grams and words); moreover, we present language independent preprocessing algorithms to normalize words and perform relative stemming or lemmatization. Those algorithms and its Shape-Coding technique enable the TxLASM to extract intrinsic features of text elements and score them statistically, and subsequently extract a representative summary that is independent of the text language, domain and context. TxLASM was applied on an English and Portuguese benchmark datasets, and the results were compared to twelve state-of-the-art approaches presented in recent literature. In addition, the model was applied on French and Spanish news datasets, and the results were compared to those obtained by standard commercial summarization tools. TxLASM has outperformed all the SOTA approaches as well as the commercial tools in all four languages while maintaining its language and domain agnostic nature.}
}
@article{ZHU2025861,
title = {A knowledge graph-based operational data representation method of digital twin shop-floor towards smart manufacturing},
journal = {Procedia CIRP},
volume = {134},
pages = {861-866},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.02.211},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125005955},
author = {Yuanzhe Zhu and Ying Cheng and Qinglin Qi and Fei Tao},
keywords = {Digital twin shop-floor (DTS), data representation, knowledge graph (KG), ontology model, smart manufacturing},
abstract = {As a means to achieve the integration of cyber-physical systems, the digital twin shop-floor (DTS) provides extensive data about shop-floor operations through dynamic interaction. Operational data collected during the manufacturing process is characterized by its multi-dimensional, heterogeneous, and time-series nature. To fully leverage this information and facilitate informed decision-making, it is essential to develop an effective method for representing the data in a unified and integrated manner. This paper proposes a knowledge graph-based operational data representation method for the DTS. The knowledge graph captures the entities and relationships related to the DTS, providing semantic integration of production line and process information. Operational data from various information systems are analyzed and managed comprehensively, utilizing the knowledge graph-based method to establish a unified, quantified, formal, and logical representation of the DTS operational data. This multi-dimensional knowledge structure encompasses manufacturing resource information, manufacturing process information, manufacturing task information, and manufacturing execution information. The proposed method is demonstrated through an example of a refrigerator manufacturing shopfloor. The resulting data representation facilitates DTS operational data analysis and cognitive decision-making promoting smart production during the manufacturing process. Additionally, this method explores the organization, acquisition, and reuse of knowledge in the DTS, along with the self-learning and self-cognitive capabilities of smart manufacturing systems.}
}
@article{LUCZAJ2024101047,
title = {Hidden emotional costs of home accommodation. The lived experiences of Ukrainian refugees in Polish homes},
journal = {Emotion, Space and Society},
volume = {53},
pages = {101047},
year = {2024},
issn = {1755-4586},
doi = {https://doi.org/10.1016/j.emospa.2024.101047},
url = {https://www.sciencedirect.com/science/article/pii/S1755458624000483},
author = {Kamil Luczaj and Iwona Leonowicz-Bukała and Olha Krasko}
}
@article{JANG2025103100,
title = {Semantic elaboration of low-LOD BIMs: Inferring functional requirements using graph neural networks},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103100},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103100},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007511},
author = {Suhyung Jang and Ghang Lee and Minkyeong Park and Jaekun Lee and Seungah Suh and Bonsang Koo},
keywords = {Semantic elaboration, Building information model (BIM), Threshold-enhanced triangle intersection (TETI), Graph neural network (GNN), Large language model (LLM) embedding},
abstract = {This study proposes a method to automatically subcategorize early object types in low levels of development (LODs) into detailed types (i.e., subtypes) with distinct functional requirements, such as insulation, waterproofing, and load-bearing. While rough cost estimation is possible in the early design phase without detailed object classifications, its accuracy is often limited. Subcategorizing generic objects like walls and columns into more detailed types enhances the precision of early-stage engineering analyses, including cost estimation, load assessments, and material takeoffs. Existing automated object subclassification methods rely on information extracted from highly detailed models, which are unavailable in early-stage building information models (BIMs) due to a lack of geometric and attributive distinctions. This study addresses these limitations by leveraging functional requirements inferred from object connections and placement in early BIMs, achieved using a graph neural network (GNN). To convert BIMs into graphs, a novel threshold-enhanced triangle intersection (TETI) algorithm is introduced, overcoming inaccuracies and exception-handling issues in existing methods. The study explores two GNN-based approaches: node property prediction and node prediction. The former distinguished generic object types into 14 detailed categories, but cost estimation required greater specificity. The latter successfully classified objects into 42 subtypes, with the best results achieved using semantically rich embeddings from a large language model (LLM) and GraphSAGE with three SAGE convolution layers, three hops, and 1,024 dimensions, yielding a weighted F1-score of 0.8766. This approach significantly reduces input data requirements compared to existing methods, enabling more accurate early identification of functional requirements in low-LOD BIMs and supporting both early engineering analyses and detailing processes.}
}
@article{BONCINELLI2025108373,
title = {C-frame thinking: Embedding behavioral economics into ecological economics},
journal = {Ecological Economics},
volume = {227},
pages = {108373},
year = {2025},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2024.108373},
url = {https://www.sciencedirect.com/science/article/pii/S0921800924002702},
author = {Leonardo Boncinelli and Luzie Dallinger and Tiziano Distefano},
keywords = {c-frame, Ecological economics, Dual-processing theory, Complexity, Collective decision making},
abstract = {This paper aims to explore opportunities for integrating Behavioral Economics (BE) into Ecological Economics (EE). By examining the frames of analysis for both disciplines, this study categorizes BE as operating at the individual level (i-frame), while EE addresses systemic aspects of society (s-frame) and extends its considerations to the biosphere (n-frame), advocating for collective action through bottom-up intermediate-level interventions (c-frame). The study posits that EE can benefit from BE’s rich insights into human behavior and decision-making, especially for c-frame action strategies. However, integrating these disciplines requires finding common ontological and epistemological ground to avoid eclecticism and methodological flaws. The integration is approached in two steps: first, adapting BE epistemology to the systems thinking approach of EE, and second, addressing the ontological gap in BE regarding the world surrounding the individual. This paper argues that embedding BE within EE’s ontology points to the necessity of c-frame thinking for human decision-making. A case study of the ex-GKN factory in Italy demonstrates the practical benefits of c-frame thinking in a complex decision process. An alliance of workers, researchers, and civil society movements collaboratively developed a future plan that considered the needs of all stakeholders, showcasing the effectiveness of collective action.}
}
@article{KUPERS20241469,
title = {Processing paradoxes through chiasmic organising – the contribution of Merleau-Ponty process-oriented philosophy for a post-dualistic approach towards paradoxes in organisations},
journal = {Journal of Organizational Change Management},
volume = {37},
number = {7},
pages = {1469-1489},
year = {2024},
issn = {0953-4814},
doi = {https://doi.org/10.1108/JOCM-03-2024-0140},
url = {https://www.sciencedirect.com/science/article/pii/S0953481424000964},
author = {Wendelin Küpers},
keywords = {Merleau-Ponty, Phenomenology, Paradox, Chiasm, In-between, Process philosophy},
abstract = {Purpose
This paper aims to critically examine traditional approaches to paradoxes and propose a new approach and perspective that views “chiasmic” organizing as a intertwining combination of structure and processes that facilitate the handling of multiple interrelations for processing paradoxes and harness their creative potential in organizations.
Design/methodology/approach
Employing a cross-disciplinary approach, a literature review and a critical lens, along with conceptual work (typology), are used to identify problems and deficiencies in existing research on paradoxes. Specifically, it draws on Merleau-Ponty's process-oriented phenomenology and post-Cartesian ontology to gain a comprehensive understanding of post-dualistic forms of chiasmic organizing and its relationship with paradoxical phenomena.
Findings
The process-oriented phenomenology and post-Cartesian ontology used in this article offer valuable insights and a critical approach to comprehend post-dualistic forms of chiasmic organizing in relation to paradoxes. This understanding can help in tapping into the energizing and creative potential of paradoxes. The paper also highlights the significance of the “in(ter)-between” as a reversible principle in chiasmic organizing and proposes some implications.
Research limitations/implications
Limitations and implications of this study are identified and discussed.
Practical implications
The paper offers practical implications for organizations in processing paradoxes.
Originality/value
This paper contributes to the existing literature by providing a conceptual critique and proposing a novel understanding of chiasmic organizing as an intertwining structure and mediating processes by employing a process-oriented phenomenology and post-Cartesian ontology. It also offers innovative ways to approach paradoxes and tap into their creative potentials, which can bring about change in organizations.}
}
@article{SASS2024473,
title = {Schizophrenia, the very idea: On self-disorder, hyperreflexivity, and the diagnostic concept},
journal = {Schizophrenia Research},
volume = {267},
pages = {473-486},
year = {2024},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2024.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0920996424001221},
author = {Louis Sass and Jasper Feyaerts},
keywords = {Self-disorder in schizophrenia, Ipseity, Phenomenological psychopathology, Hyperreflexivity, Ontological paranoia, Solipsism},
abstract = {The purpose of the present article is to consider schizophrenia—the very idea—from the perspective of phenomenological psychopathology, with special attention to the problematic nature of the diagnostic concept as well as to the prospect and challenges inherent in focusing on subjective experience. First, we address historical and philosophical topics relevant to the legitimacy of diagnostic categorization—in general and regarding “schizophrenia” in particular. William James's pragmatist approach to categorization is discussed. Then we offer a version of the well-known basic-self or ipseity-disturbance model (IDM) of schizophrenia, but in a significantly revised form (IDMrevised). The revised model better acknowledges the diverse and even seemingly contradictory nature of schizophrenic symptoms while, at the same time, interpreting these in a more unitary fashion via the key concept of hyperreflexivity—a form of exaggerated self-awareness that tends to undermine normal world-directedness and the stability of self-experience. Particular attention is paid to forms of exaggerated “self-presence” that are sometimes neglected yet imbue classically schizophrenic experiences involving subjectivism or quasi-solipsism and/or all-inclusive or ontological forms of paranoia. We focus on the distinctively paradoxical nature of schizophrenic symptomatology. In concluding we consider precursors in the work of Klaus Conrad, Kimura Bin and Henri Grivois. Finally we defend the concept of schizophrenia by considering its distinctive way of altering certain core aspects of the human condition itself.}
}
@article{DAMICO2022613,
title = {Cognitive digital twin: An approach to improve the maintenance management},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {38},
pages = {613-630},
year = {2022},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2022.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1755581722001158},
author = {Rosario Davide D’Amico and John Ahmet Erkoyuncu and Sri Addepalli and Steve Penver},
keywords = {Basic Formal Ontology (BFO), Cognitive twin, Digital twin, Ontology, Predictive maintenance, Top-level ontology (TLO)},
abstract = {Digital twin (DT) technology allows the user to monitor the asset, specifically over the operation and service phase of the life cycle, which is the longest-lasting phase for complex engineering assets. This paper aims to present a thematic review of DTs in terms of the technology used, applications, and limitations specifically in the context of maintenance. This review includes a systematic literature review of 59 articles on semantic digital twins in the maintenance context. Key performance indicators and explanations of the main concepts constituting the DT have been presented. This article contains a description of the evolution of DTs together with their characterisation for maintenance purposes. It provides an ontological approach to develop DT and improve the maintenance management leading to the creation of a structured DT or a Cognitive Twin (CT). Moreover, it points out that using a top-level ontology approach should be the starting point for the creation of CT. Enabling the creation of the digital framework that will break down silos, ensuring a perfect integration in a network of twins’ scenario.}
}
@article{LAREYRE202357,
title = {Comprehensive Review of Natural Language Processing (NLP) in Vascular Surgery},
journal = {EJVES Vascular Forum},
volume = {60},
pages = {57-63},
year = {2023},
issn = {2666-688X},
doi = {https://doi.org/10.1016/j.ejvsvf.2023.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666688X23000758},
author = {Fabien Lareyre and Bahaa Nasr and Arindam Chaudhuri and Gilles {Di Lorenzo} and Mathieu Carlier and Juliette Raffort},
keywords = {Artificial Intelligence, Literature search, Natural Language Processing, Vascular surgery},
abstract = {Objective
The use of Natural Language Processing (NLP) has attracted increased interest in healthcare with various potential applications including identification and extraction of health information, development of chatbots and virtual assistants. The aim of this comprehensive literature review was to provide an overview of NLP applications in vascular surgery, identify current limitations, and discuss future perspectives in the field.
Data sources
The MEDLINE database was searched on April 2023.
Review methods
The database was searched using a combination of keywords to identify studies reporting the use of NLP and chatbots in three main vascular diseases. Keywords used included Natural Language Processing, chatbot, chatGPT, aortic disease, carotid, peripheral artery disease, vascular, and vascular surgery.
Results
Given the heterogeneity of study design, techniques, and aims, a comprehensive literature review was performed to provide an overview of NLP applications in vascular surgery. By enabling identification and extraction of information on patients with vascular diseases, such technology could help to analyse data from healthcare information systems to provide feedback on current practice and help in optimising patient care. In addition, chatbots and NLP driven techniques have the potential to be used as virtual assistants for both health professionals and patients.
Conclusion
While Artificial Intelligence and NLP technology could be used to enhance care for patients with vascular diseases, many challenges remain including the need to define guidelines and clear consensus on how to evaluate and validate these innovations before their implementation into clinical practice.}
}
@article{GANTER2025109487,
title = {The language of Contextual Attribute Logics – Introduction and survey},
journal = {International Journal of Approximate Reasoning},
volume = {186},
pages = {109487},
year = {2025},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2025.109487},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X25001288},
author = {Bernhard Ganter},
keywords = {Formal concept analysis, Contextual logic},
abstract = {This article provides an introductory overview of Contextual Attribute Logic(s), a branch of Contextual Logic founded by Rudolf Wille. It presents a logic language that is based on mathematical logic but uses a different terminology to better serve the interpretative goal of Formal Concept Analysis.}
}
@article{NEJI2022,
title = {A Semantic and Smart Framework for Handling Multilingual Linguistic Knowledge:},
journal = {International Journal of Information Technology and Web Engineering},
volume = {17},
number = {1},
year = {2022},
issn = {1554-1045},
doi = {https://doi.org/10.4018/IJITWE.314571},
url = {https://www.sciencedirect.com/science/article/pii/S1554104522000225},
author = {Mariem Neji and Fatma Ghorbel and Bilel Gargouri and Nada Mimouni and Elisabeth Métais},
keywords = {Linguistic Domain Ontology, Multilingualism, Natural Language Processing, Smart Framework, User Friendly Visualization},
abstract = {ABSTRACT
The authors propose a semantic and smart assistance framework for handling linguistic knowledge, called LingFramework. It targets both expert and novice users. It aims to assist the user in understanding the different aspects of the linguistic domain and ease the process of proposing lingware applications. LingFramework is based on a multilingual linguistic domain ontology called LingOnto. It allows (1) representing linguistic data, linguistic processing functionalities and linguistic processing features, and (2) reasoning, via a SWRL-based reasoning engine, about the linguistic knowledge. Currently, it covers English, French, and Arabic languages. To facilitate the interaction with LingOnto, an ontology visualization tool called LingGraph is proposed. It offers an easy-to-use interface for users not familiar with ontologies. It provides a SPARQL pattern-based approach to allow a smart search interaction functionality. LingFramework is applied to assist the user in identifying valid linguistic processing pipelines related to lingware applications. The evaluation results are promising.}
}
@article{FRAILENAVARRO2023105122,
title = {Clinical named entity recognition and relation extraction using natural language processing of medical free text: A systematic review},
journal = {International Journal of Medical Informatics},
volume = {177},
pages = {105122},
year = {2023},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2023.105122},
url = {https://www.sciencedirect.com/science/article/pii/S1386505623001405},
author = {David {Fraile Navarro} and Kiran Ijaz and Dana Rezazadegan and Hania Rahimi-Ardabili and Mark Dras and Enrico Coiera and Shlomo Berkovsky},
abstract = {Background
Natural Language Processing (NLP) applications have developed over the past years in various fields including its application to clinical free text for named entity recognition and relation extraction. However, there has been rapid developments the last few years that there's currently no overview of it. Moreover, it is unclear how these models and tools have been translated into clinical practice. We aim to synthesize and review these developments.
Methods
We reviewed literature from 2010 to date, searching PubMed, Scopus, the Association of Computational Linguistics (ACL), and Association of Computer Machinery (ACM) libraries for studies of NLP systems performing general-purpose (i.e., not disease- or treatment-specific) information extraction and relation extraction tasks in unstructured clinical text (e.g., discharge summaries).
Results
We included in the review 94 studies with 30 studies published in the last three years. Machine learning methods were used in 68 studies, rule-based in 5 studies, and both in 22 studies. 63 studies focused on Named Entity Recognition, 13 on Relation Extraction and 18 performed both. The most frequently extracted entities were “problem”, “test” and “treatment”. 72 studies used public datasets and 22 studies used proprietary datasets alone. Only 14 studies defined clearly a clinical or information task to be addressed by the system and just three studies reported its use outside the experimental setting. Only 7 studies shared a pre-trained model and only 8 an available software tool.
Discussion
Machine learning-based methods have dominated the NLP field on information extraction tasks. More recently, Transformer-based language models are taking the lead and showing the strongest performance. However, these developments are mostly based on a few datasets and generic annotations, with very few real-world use cases. This may raise questions about the generalizability of findings, translation into practice and highlights the need for robust clinical evaluation.}
}
@article{REZGUI2018520,
title = {Towards a common and semantic representation of e-portfolios},
journal = {Data Technologies and Applications},
volume = {52},
number = {4},
pages = {520-538},
year = {2018},
issn = {2514-9288},
doi = {https://doi.org/10.1108/DTA-01-2018-0008},
url = {https://www.sciencedirect.com/science/article/pii/S2514928818000196},
author = {Kalthoum Rezgui and Hédia Mhiri and Khaled Ghédira},
keywords = {Ontology, Evidence, Competency assessment, e-Portfolio, Leap2A, Semantic annotation},
abstract = {Purpose
Since the early 1980s, a paradigm shift, caused by the work undertaken in the field of cognitive psychology, has occurred. This shift is known as the move from teacher-centered instruction to learner-centered or learning-centered instruction, and emphasizes the importance of building new knowledge on previous ones, interacting with peers, making meaningful and reflective learning and being engaged in his own path to foster learning. This new vision of teaching has created a need for new learning and assessment instruments that are better adapted to these pedagogical realities. In this context, the electronic portfolio or e-portfolio is one of the most versatile and effective tools that have been proposed for this purpose. More specifically, the interest in e-portfolios has grown considerably with the emergence of the competency-based approach and portfolio-based competency assessments. The purpose of this paper is to describe a semantic-based representation of e-portfolios, defined on the basis of official e-portfolio standards and specifications. Moreover, a comparative study of several well-known e-portfolio solutions has been carried out based on different facets, such as functional features, technical and organizational features. The objective is to identify those features that are mostly supported by e-portfolio solution providers and accordingly to gain a fairly accurate idea of the common structure of e-portfolios. In addition, the authors take advantage of an already implemented ontological model describing competency-related characteristics of learners and learning objects and combine it with the e-portfolio ontology, with a view to support a more reliable and authentic competency assessment.
Design/methodology/approach
The proposed e-portfolio ontology was built following the ontology development methodology Methontology (Fernandez et al., 1997). In addition, it was constructed using the Protégé ontology environment (Protégé, 2007) and was implemented in OWL (Web Ontology Language) (Antoniou and Harmelen, 2004).
Findings
The proposed e-portfolio ontology provides humans with a shared vocabulary that enables capturing the most important elements in e-portfolios and serves as the basis for the semantic interoperability for machines.
Originality/value
The main advantage of the e-portfolio ontology lies in its ability to provide a common and semantically enriched representation of e-portfolio artifacts, thus facilitating the interoperability and exchange of competency evidences between different learning systems and platforms. In addition, capturing the semantics of e-portfolios helps to make them utilizable by intelligent applications.}
}
@article{LI2025103597,
title = {Teachers' AI readiness in Chinese as a Foreign Language education: Scale development and validation},
journal = {System},
volume = {129},
pages = {103597},
year = {2025},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2025.103597},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X25000077},
author = {Nuoen Li and Yu Liang},
keywords = {AI readiness, AI-assisted language teaching, Chinese as a Foreign Language teacher, Scale development, Nomological network},
abstract = {The rapid development of artificial intelligence (AI) has presented a promising opportunity to enhance educational practices. To promote the integration of AI in Chinese as a Foreign Language (CFL) education, this study adopted Creswell and Plano Clark's (2011) exploratory instrument design model to develop a CFL Teachers' AI Readiness Scale (CFLT-AIRS) for assessing teachers' propensity to embrace AI technologies. The dimensions and corresponding items were developed through in-depth interviews, literature analysis, expert reviews, and a pilot test in sequence. The initial scale was validated through two rounds of data collection for exploratory factor analysis (EFA) (n = 333) and confirmatory factor analysis (CFA) (n = 353), respectively. The results indicated that CFL teachers' AI readiness can be conceptualized from three perspectives: personal assets (AI-TPACK and technological innovativeness), value-cost beliefs (perceived value and perceived cost), and contextual resource evaluations (institutional support and facilitating condition). Based on the developed 23-item AI readiness scale, the nomological validity was further established by exploring the influences of teachers' AI readiness on satisfaction with AI and continuous intention to use AI. The findings offer a clearer conceptual framework for AI readiness from the teachers' perspective and provide valuable insights into the integration of AI in CFL education.}
}
@article{MOFATTEH2025100499,
title = {EnerChain: A decentralized knowledge management framework for smart energy systems with smart manufacturing agents via blockchain technology},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {11},
number = {1},
pages = {100499},
year = {2025},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2025.100499},
url = {https://www.sciencedirect.com/science/article/pii/S2199853125000344},
author = {Mohammad Yaser Mofatteh and Ujjwal Khadka and Omid {Fatahi Valilai}},
keywords = {Smart energy systems, Knowledge management, Blockchain, Smart contract, Sustainability},
abstract = {Energy management can be designed from different perspectives including production, distribution, and consumption. Focusing on consumption perspective, manufacturing systems can be enhanced by enabling smart machines as agents which operate with their own knowledge representation models in a shopfloor. These agents can benefit from industry 4.0 enablers like IoT including sensors, controllers, and actuators. This paper focuses on how these agents can interoperate with each other and exchange knowledge to optimize energy consumption. Since different knowledge models may not be capable of interacting with other ones based on their different provider semantics. This paper explores the application of blockchain technology for secure, decentralized storage and sharing knowledge models in smart energy systems. The research introduces EnerChain as a blockchain-integrated and a decentralized application (DApp) system prototype that employs smart contracts for access management and conflict resolution. It also incorporates the InterPlanetary File System (IPFS) for efficient off-chain storage, addressing scalability concerns. The feasibility and practicality of this approach are demonstrated through the development of EnerChain. The findings highlight the significant potential of blockchain technology in facilitating efficient knowledge model management for smart shopfloors. Additionally, an operational scenario has been evaluated as a case study for the proposed conceptual model to illustrate how it can solve energy conflicts in a smart environment. An impact analysis at the end of this research shows that EnerChain can make annual 27.5 TWh reduction in residential energy consumption which yields to annual 7.8 million tonnes reduction in CO2 emissions and annual €8.25 billion financial benefits.}
}
@article{ESPINOSA2022101000,
title = {Reducing power disparities in large-scale mining governance through counter-expertise: A synthesis of case studies from Ecuador},
journal = {The Extractive Industries and Society},
volume = {9},
pages = {101000},
year = {2022},
issn = {2214-790X},
doi = {https://doi.org/10.1016/j.exis.2021.101000},
url = {https://www.sciencedirect.com/science/article/pii/S2214790X21001714},
author = {Cristina Espinosa},
keywords = {Natural resource governance, Counter-expertise, Knowledge, Participation, Ecuador},
abstract = {Among actors with stakes in natural resource governance, not all knowledge is equally distributed or considered to be legitimate. Instead, knowledge is arranged in a hierarchical manner, which in turn translates into power disparities. The socio-ecological dynamics entangled in large-scale mining exemplify this tendency. Engineers, technicians and environmental scientists are given a special role in defining, evaluating and implementing large-scale mineral extraction scenarios. The technical expertise and skills of these actors is referred to as ‘techno-scientific’ or ‘expert’ knowledge. Mining-affected communities are commonly excluded from formal expert-based processes. In response, these communities reject the technological trajectories developed by experts in extractive industries and governments. They engage in counter-expertise: practices of alternative knowledge mobilization and (co-)production that challenge official techno-scientific assessments of safety and risk. To do so, these communities deploy local, techno-scientific and legal knowledge. In addition, they develop alliances that span across rural and urban contexts and local, national and international scales. Such efforts converge with larger, ongoing projects of ontological self-determination coupled with experiments of self-governance and local and regional autonomy that prefigure the reorganization of society. At the intersection of Political Ecology, Foucauldian theorizations of power/knowledge, Feminist Science and Technology Studies and Latin American Decolonial Studies, these rather novel epistemic dynamics are studied in three emblematic cases of resistance to large-scale mining in Ecuador.}
}
@article{CLANCY20231403,
title = {Transcriptomics secondary analysis of severe human infection with SARS-CoV-2 identifies gene expression changes and predicts three transcriptional biomarkers in leukocytes},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {1403-1413},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S200103702300048X},
author = {Jeffrey Clancy and Curtis S. Hoffmann and Brett E. Pickett},
keywords = {SARS-CoV-2, COVID-19, RNA-sequencing, Data mining, Biomarkers, RNA, Virus, Bioinformatics},
abstract = {SARS-CoV-2 is the causative agent of COVID-19, which has greatly affected human health since it first emerged. Defining the human factors and biomarkers that differentiate severe SARS-CoV-2 infection from mild infection has become of increasing interest to clinicians. To help address this need, we retrieved 269 public RNA-seq human transcriptome samples from GEO that had qualitative disease severity metadata. We then subjected these samples to a robust RNA-seq data processing workflow to calculate gene expression in PBMCs, whole blood, and leukocytes, as well as to predict transcriptional biomarkers in PBMCs and leukocytes. This process involved using Salmon for read mapping, edgeR to calculate significant differential expression levels, and gene ontology enrichment using Camera. We then performed a random forest machine learning analysis on the read counts data to identify genes that best classified samples based on the COVID-19 severity phenotype. This approach produced a ranked list of leukocyte genes based on their Gini values that includes TGFBI, TTYH2, and CD4, which are associated with both the immune response and inflammation. Our results show that these three genes can potentially classify samples with severe COVID-19 with accuracy of ∼88% and an area under the receiver operating characteristic curve of 92.6--indicating acceptable specificity and sensitivity. We expect that our findings can help contribute to the development of improved diagnostics that may aid in identifying severe COVID-19 cases, guide clinical treatment, and improve mortality rates.}
}
@article{GREYWOLF2025,
title = {Exploring a Shared History of Colonization, Historical Trauma, and Links to Alcohol Use With Native Hawaiians: Qualitative Study},
journal = {Asian Pacific Island Nursing Journal},
volume = {9},
year = {2025},
issn = {2373-6658},
doi = {https://doi.org/10.2196/68106},
url = {https://www.sciencedirect.com/science/article/pii/S2373665825000130},
author = {Cynthia Taylor Greywolf and Donna Marie Palakiko and Pallav Pokhrel and Elizabeth A Vandewater and Merle Kataoka-Yahiro and John Casken},
keywords = {historical trauma, intergenerational trauma, Indigenous health, alcohol use, substance misuse, health disparity},
abstract = {Background
Most studies using historical trauma theory have focused on American Indian tribes. There remains a dearth of research exploring historical trauma and substance use among Native Hawaiians. Native Hawaiians and American Indians experience a startlingly high degree of physical and mental health disparities and alcohol and other substance misuse. Indigenous scholars posit that historical trauma is intergenerationally transmitted to subsequent generations and is the primary cause of today’s health and substance use disparities among these Indigenous populations.
Objective
This study aimed to explore the lived experiences of colonization, historical trauma, and alcohol use among Native Hawaiians living in rural Hawaii.
Methods
This qualitative study was guided by Husserl’s transcendental phenomenological design. The historical trauma conceptual framework and story theory guided the study. The Native Hawaiian Talk-Story method was used to collect data from 10 Native Hawaiian adult participants in one-to-one interviews. The modified Stevick-Keen-Colaizzi method was used for data analysis.
Results
In total, four themes emerged: (1) alcohol did not exist in Hawaii before European explorers arrived; (2) alcohol helped expand colonialism in Hawaii; (3) alcohol is used today as a coping strategy for feelings of grief and anger over losses (land, people, cultural traditions, and language); and (4) the kupuna (elders) teach the younger generations to drink alcohol.
Conclusions
Native Hawaiians, like American Indians, experienced historical trauma, which is transmitted intergenerationally, resulting in mental and physical health disparities, substance misuse, and feelings of discrimination. The introduction of alcohol by European explorers provides the foundation for problematic alcohol use among Native Hawaiians today.}
}
@article{HAQ2022100294,
title = {Connecting the dots in clinical document understanding with Relation Extraction at scale},
journal = {Software Impacts},
volume = {12},
pages = {100294},
year = {2022},
issn = {2665-9638},
doi = {https://doi.org/10.1016/j.simpa.2022.100294},
url = {https://www.sciencedirect.com/science/article/pii/S2665963822000392},
author = {Hasham Ul Haq and Veysel Kocaman and David Talby},
keywords = {NLP, Relation Extraction, Named Entity Recognition, Sparknlp},
abstract = {We present a text mining framework based on top of the Spark NLP library — comprising of Named Entity Recognition (NER) and Relation Extraction (RE) models, which expands on previous work in three main ways. First, we release new RE model architectures that obtain state-of-the-art F1 scores on 5 out of 7 benchmark datasets. Second, we introduce a modular approach to train and stack multiple models in a single nlp pipeline in a production grade library with little coding. Third, we apply these models in practical applications including knowledge graph generation, prescription parsing, and robust ontology mapping.}
}
@article{ZHANG2025106182,
title = {Enhancing bridge inspection data quality using machine learning},
journal = {Automation in Construction},
volume = {175},
pages = {106182},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106182},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002225},
author = {Chenhong Zhang and Xiaoming Lei and Ye Xia},
keywords = {Bridge inspection data, Text classification, Quality control, Class imbalance},
abstract = {Bridge condition assessment is often compromised by errors in inspection data, limiting reliable maintenance and management decisions. This paper investigates how to enhance inspection data quality by automatically identifying and correcting the inaccurate assessment of structural conditions. A model that integrates textual and quantitative features is proposed to identify defect and condition ratings through defect descriptions, with corresponding dynamic partitioning strategy to detect ambiguous data, and a down-sampling and bagging ensemble to address class imbalance. Validated with ten years of real inspection data from 464 bridges, results show 98 % accuracy in correcting condition scores and 100 % accuracy in condition-level identification. These findings underscore the method's potential to improve the reliability of condition assessment and strengthen bridge management decision-making. Future research can focus on refining condition level identification algorithms for severely deteriorated structures.}
}
@article{SCHWABE2019205,
title = {Applying rule-based model-checking to construction site layout planning tasks},
journal = {Automation in Construction},
volume = {97},
pages = {205-219},
year = {2019},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2018.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517310208},
author = {Kevin Schwabe and Jochen Teizer and Markus König},
keywords = {Building information modeling (BIM), Business rule management system (BRMS), Construction site layout planning, Drools, Offset geometry, Rule language, engine and checking},
abstract = {Building Information Modeling (BIM) is a widely established method in the architecture, engineering, construction, and facilities management (AEC/FM) industry. Although BIM focuses on processes throughout the lifecycle of the built environment, the applications in the planning phase, e.g. the generation of construction site layouts, have not reached their full potential yet. One important example herein is the allocation and dimensioning of resources (e.g., building materials and equipment) which is typically carried out by humans according to clearly defined rules and best practices. This paper presents model-based rule checking for the planning of construction site layouts. We demonstrate that existing Business Rule Management Systems (BRMS), such as the open-source rule engine Drools, can be used. We combine Drools with the Industry Foundation Classes (IFC) to retrieve data from a building information model and use the information within the rule engine. We define general model requirements and implement a sample set of prototype rules. We also introduce the concept of offset geometry for rules that, for example, demand a known safety distance between temporary construction site elements. The developed approaches are explained and evaluated in field-realistic, practical case studies. Finally, we present a discussion how the application of the developed rule-based system may assist human decision making in tasks such as safe construction sites layout planning.}
}
@article{MALIK20181202,
title = {A Novel Approach to Web-Based Review Analysis Using Opinion Mining},
journal = {Procedia Computer Science},
volume = {132},
pages = {1202-1209},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918307671},
author = {Monica Malik and Sharib Habib and Parul Agarwal},
keywords = {Priority, Weights, Web-based reviews, Opinion Mining, Ontology, Modified OGC},
abstract = {With the advent of E-Commerce, a huge amount of data is being generated these days. This data can be useful if knowledge can be extracted from the business perspective. People often use websites reviews for decision making about whether the products should be bought or not. Opinion Mining enables the process of selection and decision making easier. Though several techniques exist for opinion mining based on decision making in this paper the approach is novel. In this paper the approach used is in addition to the opinions generated from the reviews collected from E-Commerce websites and calculating the overall sentiment for decision making, this paper also incorporates a priority for particular features of a product entered by the buyer for making the final decision. This has been incorporated in the form of additional weights which can be entered by the user and adjusted according to the priority. The reason to do this is that priority for a particular feature of a product may vary from person to person. Moreover, the final decision lay in the buyer’s hand in addition to the opinions collected and analysed from reviews.}
}
@article{SU2025615,
title = {Screening for Biomarkers Related to Pigmentation and Formation in Vitiligo},
journal = {Combinatorial Chemistry & High Throughput Screening},
volume = {28},
number = {4},
pages = {615-626},
year = {2025},
issn = {1386-2073},
doi = {https://doi.org/10.2174/0113862073275508231229112157},
url = {https://www.sciencedirect.com/science/article/pii/S1386207325000734},
author = {Mengyun Su and Ying Shi},
keywords = {  , NB-UVB therapy, biomarker, hub genes, PPI, RT-qPCR},
abstract = {Background
Vitiligo is an autoimmune skin disorder primarily characterized by the absence of melanocytes, leading to the development of white patches on the patient's skin. Narrowband Ultraviolet B (NB-UVB) therapy is among the most effective approaches for stimulating the reformation of hyperpigmentation. This treatment utilizes a narrow spectrum of NB-UVB wavelengths ranging from 311 to 313 nm to irradiate the affected area, thereby preventing the destruction of migrating and proliferating melanocytes. Nevertheless, the molecular alterations occurring in both the hair follicle and the interfollicular epidermis during NB-UVB treatment remain unknown.
Methods
In this study, we conducted a comprehensive analysis of the consistency of differentially expressed genes (DEGs) within the enrichment pathways both before and after NB-UVB treatment, utilizing a bioinformatics approach. Furthermore, we employed CYTOHUBBA and Random Forest algorithms to identify and sequence hub genes from the pool of DEGs. Following validation of these hub genes through ROC curve analysis, we proceeded to construct an interaction network between these hub genes, miRNA, and drugs. Real-Time Quantitative Polymerase Chain Reaction (RT-qPCR) was used to further verify the difference in the expression of hub genes between the disease group and the control group.
Results
Gene Set Enrichment Analysis of DEGs indicated strong associations with vitiligo in most pathways. Subsequently, we conducted Gene Ontology and Metascape enrichment analyses on the overlapping genes from DEGs. We identified key genes (COL11A1, IGFBP7, LOX, NTRK2, SDC2, SEMA4D, and VEGFA) within the Protein-Protein Interaction (PPI) network. We further explored potential drugs that could be used for the clinical treatment of vitiligo through the drug-hub gene interaction network. Finally, the results of RT-qPCR experiments demonstrated that the expression levels of the identified hub genes in both groups were consistent with the bioinformatics analysis results.
Conclusion
The hub genes obtained in this study may be a biomarker related to the development of vitiligo pigmentation. Our research not only contributes to a better understanding of the treatment mechanisms of vitiligo but also provides valuable insights for future personalized medical approaches and targeted therapies for vitiligo.}
}
@article{REMAKI2025,
title = {Improving Phenotyping of Patients With Immune-Mediated Inflammatory Diseases Through Automated Processing of Discharge Summaries: Multicenter Cohort Study},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/68704},
url = {https://www.sciencedirect.com/science/article/pii/S2291969425000687},
author = {Adam Remaki and Jacques Ung and Pierre Pages and Perceval Wajsburt and Elise Liu and Guillaume Faure and Thomas Petit-Jean and Xavier Tannier and Christel Gérardin},
keywords = {secondary use of clinical data for research and surveillance, clinical informatics, clinical data warehouse, electronic health record, data science, artificial intelligence, AI, natural language processing, ontologies, classifications, coding, tools, programs and algorithms, immune-mediated inflammatory diseases},
abstract = {Background
Valuable insights gathered by clinicians during their inquiries and documented in textual reports are often unavailable in the structured data recorded in electronic health records (EHRs).
Objective
This study aimed to highlight that mining unstructured textual data with natural language processing techniques complements the available structured data and enables more comprehensive patient phenotyping. A proof-of-concept for patients diagnosed with specific autoimmune diseases is presented, in which the extraction of information on laboratory tests and drug treatments is performed.
Methods
We collected EHRs available in the clinical data warehouse of the Greater Paris University Hospitals from 2012 to 2021 for patients hospitalized and diagnosed with 1 of 4 immune-mediated inflammatory diseases: systemic lupus erythematosus, systemic sclerosis, antiphospholipid syndrome, and Takayasu arteritis. Then, we built, trained, and validated natural language processing algorithms on 103 discharge summaries selected from the cohort and annotated by a clinician. Finally, all discharge summaries in the cohort were processed with the algorithms, and the extracted data on laboratory tests and drug treatments were compared with the structured data.
Results
Named entity recognition followed by normalization yielded F1-scores of 71.1 (95% CI 63.6-77.8) for the laboratory tests and 89.3 (95% CI 85.9-91.6) for the drugs. Application of the algorithms to 18,604 EHRs increased the detection of antibody results and drug treatments. For instance, among patients in the systemic lupus erythematosus cohort with positive antinuclear antibodies, the rate increased from 18.34% (752/4102) to 71.87% (2949/4102), making the results more consistent with the literature.
Conclusions
While challenges remain in standardizing laboratory tests, particularly with abbreviations, this work, based on secondary use of clinical data, demonstrates that automated processing of discharge summaries enriched the information available in structured data and facilitated more comprehensive patient profiling.}
}
@article{LI202029,
title = {Teaching English for Research Publication Purposes (ERPP): A review of language teachers’ pedagogical initiatives},
journal = {English for Specific Purposes},
volume = {59},
pages = {29-41},
year = {2020},
issn = {0889-4906},
doi = {https://doi.org/10.1016/j.esp.2020.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0889490620300156},
author = {Yongyan Li and John Flowerdew},
keywords = {English for Research Publication Purposes (ERPP), ERPP pedagogy, Writing for scholarly publication, English as an Additional Language (EAL), English for Academic Purposes (EAP)},
abstract = {Academics and research students around the world have increasingly come under pressure to publish in high-ranking English-medium international journals. At the same time, it has been widely recognised that English for Research Publication Purposes (ERPP) pedagogical support can be crucial to the publication success of those scholars and students who use English as an Additional Language (EAL). There has consequently been a growing interest in developing ERPP instructional initiatives. This paper surveys existing reported ERPP pedagogical practices, aiming to answer the research question “What does the literature tell us about the contexts and pedagogical strategies of ERPP instruction led by language teachers?” A selection of 31 articles was retrieved as the focal literature and seven categories of meaning were derived: rationales and local contexts, theoretical underpinnings and pedagogical approaches, writing tasks, instructor and peer feedback, language focus, challenging issues, and specialised vs. specialist knowledge. In view of the huge demand for ERPP intervention, the field can still be considered as underdeveloped. By synthesising the relevant literature to date, this review is timely and will be of value to course designers and ERPP practitioners present and future.}
}
@article{HAN2024121052,
title = {LegalAsst: Human-centered and AI-empowered machine to enhance court productivity and legal assistance},
journal = {Information Sciences},
volume = {679},
pages = {121052},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121052},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524009666},
author = {Wenjuan Han and Jiaxin Shen and Yanyao Liu and Zhan Shi and Jinan Xu and Fangxu Hu and Hao Chen and Yan Gong and Xueli Yu and Huaqing Wang and Zhijing Liu and Yajie Yang and Tianshui Shi and Mengyao Ge},
keywords = {Tools to assist in the trial, Explainable process, Multi-level inference, Traceable decision, Controllable judgment, Artificial intelligence technology},
abstract = {We propose autonomous software (namely, LegalAsst ) as a step toward an AI-empowered but human-centered machine focused on enhancing court productivity and legal assistance. LegalAsst aims to provide explainable, traceable, and controllable legal assistance and references for lawyers, judges, government officials, and the general public. To achieve this goal, it collates, processes, distills, and visualizes the whole judgment procedure. It streamlines and semi-automates the judgment procedure through case analysis, legislation analysis, and judicial decision-making. Specifically, to make laws and cases easier to navigate and understand, we incorporate structured representations to perform them. Then based on structured representations, we take a step further by introducing a decision-tree-based judgment, making the entire judging process visible and tractable. Our system not only tracks the procedural aspects of judgments but also incorporates modification capabilities, enabling the consideration of the most up-to-date legislation and societal factors to generate more adaptable judgment outcomes.1}
}
@article{KILICOGLU2024104588,
title = {Semantics-enabled biomedical literature analytics},
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104588},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104588},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000066},
author = {Halil Kilicoglu and Faezeh Ensan and Bridget McInnes and Lucy Lu Wang}
}
@article{JENG2019242,
title = {Dynamic learning paths framework based on collective intelligence from learners},
journal = {Computers in Human Behavior},
volume = {100},
pages = {242-251},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218304527},
author = {Yu-Lin Jeng and Yong-Ming Huang},
keywords = {Learning paths, Collective intelligence, Learning performance, Learning materials},
abstract = {Learning maps allow learners to organize and personalize their learning materials, thus helping them to more effectively achieve their learning objectives. Accordingly, there has been ongoing research about learning maps with the goal of developing a comprehensive, easy to use, and powerful learning map. At present, the two most frequently used maps have either an ontological basis or take their design from the Petri net. These both provide a useful learning tool for learners. The ontology-based learning map represents integral concepts of knowledge and the relationships among concepts; however, it lacks the ability to control the learners' progress. The Petri net-based map can handle and personalize learning progress and designing the map is relatively easy, but its representation of the subject matter is relatively weak. The aim of this study is to design useful learning sequences and a representation interface that combine the above strengths. To do this, it offers the Dynamic Learning Paths Framework (DLPF), which is based on schema theory and the concept of collective intelligence. With the DLPF system, learners can provide feedback and contribute to a specific learning schema by submitting extra learning material. The self-improvement mechanism in the DLPF is designed to maintain the quality of learning materials to avoid the bias of collective intelligence. To evaluate the DLPF, questionnaires were developed and experiments to ascertain learning performance experiment were conducted. The results show that the proposed framework provides the well-organized learning materials, presents subject material effectively and can contribute to an improvement in learners’ academic performance.}
}
@article{CORNO201941,
title = {A high-level semantic approach to End-User Development in the Internet of Things},
journal = {International Journal of Human-Computer Studies},
volume = {125},
pages = {41-54},
year = {2019},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2018.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918301228},
author = {Fulvio Corno and Luigi {De Russis} and Alberto {Monge Roffarello}},
keywords = {End-User Development, Internet of Things, Trigger-action programming, Semantic Web, Abstraction},
abstract = {Various programming environments for End-User Development (EUD) allow the composition of Internet of Things (IoT) applications, i.e., connections between IoT objects to personalize their joint behavior. These environments, however, only support a one-to-one mapping between pairs of object instances, and adopt a low level of abstraction that forces users to be aware of every single technology they may encounter in their applications. As a consequence, numerous open questions remain: would a “higher level” of abstraction help users creating their IoT applications more effectively and efficiently compared with the contemporary low-level representation? Which representation would users prefer? How high-level IoT applications could be actually executed? To answer these questions, we introduce EUPont, a high-level semantic model for EUD in the IoT. EUPont allows the creation of high-level IoT applications, able to adapt to different contextual situations. By integrating the ontology in the architecture of an EUD platform, we demonstrate how the semantic capabilities of the model allow the execution of high-level IoT applications. Furthermore, we evaluate the approach in a user study with 30 participants, by comparing a Web interface for composing IoT applications powered by EUPont with the one employed by a widely used EUD platform. Results show that the high-level approach is understandable, and it allows users to create IoT applications more correctly and quickly than contemporary solutions.}
}
@article{MARINO2024101072,
title = {RummaGEO: Automatic mining of human and mouse gene sets from GEO},
journal = {Patterns},
volume = {5},
number = {10},
pages = {101072},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101072},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002319},
author = {Giacomo B. Marino and Daniel J.B. Clarke and Alexander Lachmann and Eden Z. Deng and Avi Ma’ayan},
keywords = {transcriptomics, gene expression, gene set enrichment analysis, data mining, data integration, signature search, LINCS, CFDE, ARCHS4, RNA sequencing},
abstract = {Summary
The Gene Expression Omnibus (GEO) has millions of samples from thousands of studies. While users of GEO can search the metadata describing studies, there is a need for methods to search GEO at the data level. RummaGEO is a gene expression signature search engine for human and mouse RNA sequencing perturbation studies extracted from GEO. To develop RummaGEO, we automatically identified groups of samples and computed differential expressions to extract gene sets from each study. The contents of RummaGEO are served for gene set, PubMed, and metadata search. Next, we analyzed the contents of RummaGEO to identify patterns and perform global analyses. Overall, RummaGEO provides a resource that is enabling users to identify relevant GEO studies based on their own gene expression results. Users of RummaGEO can incorporate RummaGEO into their analysis workflows for integrative analyses and hypothesis generation.}
}
@article{WAN201980,
title = {A knowledge based machine tool maintenance planning system using case-based reasoning techniques},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {58},
pages = {80-96},
year = {2019},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2019.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0736584518301418},
author = {Shan Wan and Dongbo Li and James Gao and Jing Li},
keywords = {Manufacturing system, CNC machine tool, Maintenance, Knowledge management, Case-based reasoning},
abstract = {In advanced manufacturing systems, Computer Numerical Control (CNC) machine tools are important equipment to manufacture product components of high precision, whilst from equipment maintenance point of view, they are regarded as the ‘products’ provided by machine tool manufacturers. Therefore, the reliability of CNC machine tools affects not only the quality of the components they manufacture, but also the reputation and profits of equipment suppliers. This paper presents a novel knowledge based maintenance planning system to facilitate information and knowledge sharing between all stakeholders including machine tool manufacturers, users (manufacturing systems), maintenance service providers and part suppliers (for machine tools), in the emerging ‘Product-Service’ business model. Case Based Reasoning principles have been implemented to improve the efficiency of maintenance planning. Ontologies were adopted to represent field knowledge using adaptation guided retrievals based on semantic similarity and correlation. The adaption algorithm has been developed based on the Casual Theory and the dependence relationship to generate the solution for required maintenance problems. The proposed system was implemented using Content Management technologies and has been verified using an example CNC machine tool. The results were commented by industrial collaborators as very promising and further exploitation in industry was recommended.}
}
@article{VASSILIADES2022,
title = {An Open-Ended Web Knowledge Retrieval Framework for the Household Domain With Explanation and Learning Through Argumentation},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.309421},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000230},
author = {Alexandros Vassiliades and Nick Bassiliades and Theodore Patkos and Dimitris Vrakas},
keywords = {Argumentation, Explainability, Household Domain, Knowledge Retrieval, Learning},
abstract = {ABSTRACT
The authors present a knowledge retrieval framework for the household domain enhanced with external knowledge sources that can argue over the information that it returns and learn new knowledge through an argumentation dialogue. The framework provides access to commonsense knowledge about household environments and performs semantic matching between entities from the web knowledge graph ConceptNet, using semantic knowledge from DBpedia and WordNet, with the ones existing in the knowledge graph. They offer a set of predefined SPARQL templates that directly address the ontology on which their knowledge retrieval framework is built and querying through SPARQL. The framework also features an argumentation component, where the user can argue against the answers of the knowledge retrieval component of the framework under two different scenarios: the missing knowledge scenario, where an entity should be in the answers, and the wrong knowledge scenario, where an entity should not be in the answers. This argumentation dialogue can end up in learning a new piece of knowledge when the user wins the dialogue.}
}
@article{RATHNASIRI2023102085,
title = {Data-driven approaches to built environment flood resilience: A scientometric and critical review},
journal = {Advanced Engineering Informatics},
volume = {57},
pages = {102085},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102085},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623002136},
author = {Pavithra Rathnasiri and Onaopepo Adeniyi and Niraj Thurairajah},
keywords = {Built assets, Data-driven, Computational methods, Community, Environment, Flood, Resilience, Society},
abstract = {Environmental hazards such as floods significantly frustrate the functionality of built assets. In addressing flood-induced challenges, data usage has become important. Despite existing vast flood-related research, no research has presented a comprehensive insight into global studies on data-driven built environment flood resilience. Hence, this study conducted a comprehensive review of data-driven approaches to flood resilience. Scientometric analysis revealed emerging countries, authorships, keywords, and research hotspots. The critical review revealed data-centric approaches such as Machine Learning (ML), Artificial Intelligence (AI), Flood Simulations, Bayesian Modelling, Building Information Modelling (BIM) and Geographic Information Systems (GIS). However, they were mainly deployed in hydraulic flood simulations for prediction, monitoring, risk, and damage assessments. Further, the potentials of computational methods in tackling built environment resilience challenges were identified. Deploying the approaches in the future requires a better understanding of the status quo. These methods include hybrid data-driven approaches, ontology-based knowledge representation, multiscale modelling, knowledge graphs, blockchain technology, convolutional neural networks, automated approaches integrated with social media data, data assimilation, BIM models linked with sensors and satellite imagery and ML and AI-based digital twin models. Nevertheless, reference to data-informed built-asset resilience decisions and clear-cut implications on built-asset resilience improvement remain indistinct in many studies. This suggests that more opportunities exist to contextualise data for built environment flood resilience. This study concluded with a conceptual map of flood context, methodologies, data types engaged, and future computational methods with directions for future research.}
}
@article{HASSAN2018476,
title = {Power, trust and control},
journal = {Journal of Accounting in Emerging Economies},
volume = {8},
number = {4},
pages = {476-494},
year = {2018},
issn = {2042-1168},
doi = {https://doi.org/10.1108/JAEE-08-2017-0080},
url = {https://www.sciencedirect.com/science/article/pii/S2042116818000146},
author = {Mostafa Kamal Hassan and Samar Mouakket},
keywords = {Accounting, Emerging economies, ERP, Structuration theory, Customization, Trust and control},
abstract = {Purpose
The purpose of this paper is to explore political behaviours associated with the implementation of an enterprise resource planning (ERP) system in a public service organisation from an emerging market country, the United Arab Emirates (UAE).
Design/methodology/approach
The authors’ theoretical framework is based on the notions of trust, agent reflexivity, ontological security, routines, control and power proposed by Giddens (1984, 1990). The authors explore how the political behaviour of organisation members emanates from the introduction of an ERP system (particularly its accounting modules), and how the interaction between individual power, trust and control shaped its implementation process. The case study methodology relied on diverse data collection methods including semi-structured interviews, documentary evidence and personal observation.
Findings
The authors show that the accounting-based ERP system created an episode of discomfort in the organisation, which facilitated reflexivity and critical reflection by organisation members and led to a re-assessment of ways of thinking pre- and post-dating the implementation of the ERP system. The findings illustrate the entangled relationship between the new accounting-based ERP system and the feelings of trust emerging during organisational change.
Practical implications
Although case studies are intrinsically limited in terms of generalisability, the authors’ investigation provides practical insights into the management of the needs of trust, ontological security and sources of power experienced by organisation members, since the fulfilment of such needs is the underlying pillar which the success of ERP systems rests upon.
Originality/value
This study is one of the first to apply Giddens’ (1984, 1990) conceptualisation to examine organisation change caused by the implementation of an accounting-based ERP system in an emerging market economy.}
}
@article{GRINDSTED201857,
title = {Geoscience and sustainability – In between keywords and buzzwords},
journal = {Geoforum},
volume = {91},
pages = {57-60},
year = {2018},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2018.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S0016718518300654},
author = {Thomas Skou Grindsted},
keywords = {Sustainability science, Anthropocene, Neoliberal universities, Use of concepts, Political ecologies of reference making},
abstract = {This paper explores how scientists entangle themselves in between keywords and buzzwords when they make use of concepts like sustainability. It sketches out theoretical distinctions between keywords and buzzwords. Then it turns to the concept of nature discussing the paradox that nature embraces the same fuzzy, slippery and contingent character as does sustainability, yet the former has a deep ontological status, the latter does not. The paper explores a related paradox: natural sciences claim we live in the Anthropocene, in which humans have transformed geochemical cycles, e.g. of methane and carbon dioxide as much as they changed between glacial and interglacial periods. Yet, science favors (external) nature as a keyword, sustainability as a buzzword. This should cause deep reflections on how scientists make use of the power of reference in between keywords and buzzwords – as well as critical reflection on the institutionalization of such concepts.}
}
@article{MOFATTEH20252358,
title = {A Smart Adaptive Transportation Planning Model Including Real-time Drivers’ Knowledge using Answer Set Programming and Knowledge Graphs},
journal = {Procedia Computer Science},
volume = {253},
pages = {2358-2368},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.296},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925003047},
author = {Mohammad Yaser Mofatteh and Reyan Abbas and Nicholas Desire Seddoh and Suraj Sedhumadhavan and Mintra Thunyaluck and Omid Fatahi Valilai},
keywords = {Adaptive Tour planning, Answer Set Programming, Knowledge Graph, Artificial General Intelligence (AGI), Smart transportation},
abstract = {The rapid advancement of Artificial Intelligence (AI) has catalysed transformative developments across various domains, including tour planning. This paradigm shift has unlocked new opportunities for leveraging AI-driven solutions to optimize tour itineraries, enhance user experiences, and streamline logistical operations. In particular, Artificial General Intelligence (AGI) techniques have been instrumental in revolutionizing tour planning by offering innovative approaches to route optimization, resource allocation, and personalized itinerary generation. This abstract explores the burgeoning landscape of AGI applications in tour planning, highlighting its potential to reshape the tourism industry and improve the efficiency and effectiveness of tour management processes. This research proposes an adaptive tour planning model aimed at addressing the evolving needs and challenges of modern tour operators. Focusing on the Vehicle Routing Problem (VRP), the proposed model integrates novel constraints, including weather and traffic conditions, to enhance its robustness and applicability in real-world scenarios. By incorporating these dynamic factors, the proposed model offers more resilient and responsive routing solutions, capable of adapting to changing environmental conditions and unforeseen disruptions. This innovative approach lays the foundation for the development of more efficient and reliable tour planning systems, paving the way for enhanced user satisfaction and operational efficiency in the tourism sector.}
}
@article{ZHOU2024101288,
title = {How do we understand ‘age’ and ‘aging’? Cultural constructions of the ‘aging’ experience in British English and Chinese from a linguistic perspective},
journal = {Journal of Aging Studies},
volume = {71},
pages = {101288},
year = {2024},
issn = {0890-4065},
doi = {https://doi.org/10.1016/j.jaging.2024.101288},
url = {https://www.sciencedirect.com/science/article/pii/S0890406524000835},
author = {Taochen Zhou},
keywords = {Culture, Metaphor, Aging, corpus, Cross-linguistics, Media discourse},
abstract = {This study investigates the cognitive constructions surrounding the aging experience in British English and Mandarin Chinese. The study employs corpus data to explore how fixed phrases manifest the perceptions of ‘age’, ‘aging’, and by extension ‘old age’. It lays out the linguistic patterns that are common in each language. By analyzing the similarities and differences, the findings show that the same biological phenomenon is not expressed in the same linguistic patterns consistently across languages, and that culture plays an important role in structuring conceptual preferences. Most distinctively, ‘age’ in Chinese can be a separate entity with an upward-oriented path on the aging JOURNEY which is unfound in English. This study sheds light on the associations between language, thought and culture to foster sensitive communication under the background that aging perceptions may have an impact on older adults' general wellbeing and health behavior.}
}
@article{CAO2020129,
title = {IFML-Based Web Application Modeling},
journal = {Procedia Computer Science},
volume = {166},
pages = {129-133},
year = {2020},
note = {Proceedings of the 3rd International Conference on Mechatronics and Intelligent Robotics (ICMIR-2019)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.034},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920301563},
author = {Rong Kai Cao and Xiaoyan Liu},
keywords = {Model-driven development, Interactive flow modeling language, WebRatio, Software development},
abstract = {In order to solve the design problem of the increasingly complex web application front-end interface, this paper proposes a modeling method based on IFML to generate user interface. Firstly, it introduces the extension of IFML to web application, the advantages of the model conversion method and the development environment. Secondly, the specific method from IFML mapping to the design environment is given. Finally, the conclusions and prospects for the future are given.}
}
@article{CHEN2024e26065,
title = {Inference of gene networks using gene expression data with applications},
journal = {Heliyon},
volume = {10},
number = {5},
pages = {e26065},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e26065},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024020966},
author = {Chi-Kan Chen},
keywords = {ARD, Cancer, Co-expression, Gene expression, Hub, Inference, Network, Partial correlation},
abstract = {Gene networks (GNs) use graphs to represent the interaction relationships between genes. Large-scale GNs are often sparse and contain hub genes that interact with many other genes. In this paper, we propose a novel method called NetARD, which utilizes Automatic Relevance Determination (ARD) to estimate partial correlations, to infer GNs with the hub genes from gene expression data. We test NetARD on simulated GNs and in silico GNs, and it outperforms existing methods. In our high-throughput gene expression data analysis, we integrate the NetARD into a method called GN Co-expression Extension (GNCE). This approach infers the GNs of co-expressed genes, with genes from a predefined GN serving as hub genes. We validate this approach by extending the core GN of transcription factor genes of E. coli using microarray data. In an application example, we identify biological process (BP) Gene Ontology (GO) terms that are significantly involved in cancer progression. This task is accomplished by analyzing the GN inferred through GNCE using the core GN associated with the colorectal cancer pathway and RNA-seq data.}
}
@article{YAO2024e37402,
title = {Deciphering the multidimensional impact of IGFBP1 expression on cancer prognosis, genetic alterations, and cellular functionality: A comprehensive Pan-cancer analysis},
journal = {Heliyon},
volume = {10},
number = {18},
pages = {e37402},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e37402},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024134330},
author = {Zengwu Yao and Junping Han and Jinhui Wu and Miaomiao Li and Ruyue Chen and Mi Jian and Zhensong Yang and Xixun Wang and Yifei Zhang and Jinchen Hu and Lixin Jiang},
keywords = {, Pan-cancer analysis, Prognosis, Genetic alterations, Immune infiltration, Tumor microenvironment},
abstract = {Objectives
IGF-binding protein 1 (IGFBP1) is a key regulator of insulin-like growth factors, impacting biological processes, including cancer progression and prognosis.
Materials and methods
This study investigates genetic alterations affecting IGFBP1 expression in tumors using data from The Cancer Genome Atlas (TCGA) PanCancer Atlas via cBioPortal. We analyzed samples from 32 cancer types for mutation sites, including deep deletions, amplifications, and mutations. RNA-seq data were normalized using log2(value + 1). Statistical analyses, including survival outcomes, were conducted using R packages like ggplot2, stats, and car. Kaplan-Meier survival curves and log-rank tests assessed overall survival (OS) and progression-free survival (PFS). Univariate Cox regression was used to develop nomogram models for OS. Functional consequences of IGFBP1 mutations were explored through protein structure, stability, and IGF interaction analyses. Protein-protein interaction networks and functional enrichment were analyzed using GEPIA2, STRING, and Cytoscape. Gene Ontology (GO), KEGG, and Gene Set Enrichment Analysis (GSEA) provided insights into affected biological pathways.
Results
Pan-cancer analysis revealed diverse expression patterns, including significant upregulation in cutaneous melanoma (SKCM) and downregulation in lung adenocarcinoma (LUAD) and stomach adenocarcinoma (STAD). Specifically, elevated IGFBP1 expression in SKCM patients led to a 25 % improvement in 5-year survival. In contrast, higher IGFBP1 levels in LUAD and OV patients resulted in a 30 % and 20 % decrease in survival, respectively. Elevated IGFBP1 levels are significantly linked to advanced tumor stage and grade in OV and LUAD, affecting prognostic outcomes. Nomogram models for OV, SKCM, LUAD, and STAD showed IGFBP1's predictive strength with AUC values ranging from 0.70 to 0.85, indicating its diagnostic potential. Genetic analyses revealed mutations in IGFBP1 in 12 % of STAD cases and 10 % of UCEC cases, indicating significant genetic variation. Immune analysis showed that high IGFBP1 expression significantly influenced immune cell infiltration, particularly macrophages and CD8+ T cells, thereby affecting survival in LUAD and OV. Functional enrichment and gene set enrichment analysis identified IGFBP1 involvement in crucial pathways, such as cell cycle regulation, immune response, and PD-1 signaling, highlighting its biological impact. Additionally, IGFBP1 expression delineates distinct molecular and immune subtypes, correlating with specific cancer behaviors and immune patterns.
Conclusions
These findings highlight IGFBP1's potential as a biomarker and therapeutic target, particularly for immunoregulation and cancer subtype stratification.}
}
@article{BERENDT2021100113,
title = {FactRank: Developing automated claim detection for Dutch-language fact-checkers},
journal = {Online Social Networks and Media},
volume = {22},
pages = {100113},
year = {2021},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2020.100113},
url = {https://www.sciencedirect.com/science/article/pii/S2468696420300549},
author = {Bettina Berendt and Peter Burger and Rafael Hautekiet and Jan Jagers and Alexander Pleijter and Peter {Van Aelst}},
abstract = {Fact-checking has always been a central task of journalism, but given the ever-growing amount and speed of news offline and online, as well as the growing amounts of misinformation and disinformation, it is becoming increasingly important to support human fact-checkers with (semi-)automated methods to make their work more efficient. Within fact-checking, the detection of check-worthy claims is a crucial initial step, since it limits the number of claims that require or deserve to be checked for their truthfulness. In this paper, we present FactRank, a novel claim detection tool for journalists specifically created for the Dutch language. To the best of our knowledge, this is the first and still the only such tool for Dutch. FactRank thus complements existing online claim detection tools for English and (a small number of) other languages. FactRank performs similarly to claim detection in ClaimBuster, the state-of-the-art fact-checking tool for English. Our comparisons with a human baseline also indicate that given how much even expert human fact-checkers disagree, there may be a natural “upper bound” on the accuracy of check-worthiness detection by machine-learning methods. The specific quality of FactRank derives from the interdisciplinary and iterative process in which it was created, which includes not only a high-performance deep-learning neural network architecture, but also a principled approach to defining and operationalising the concept of check-worthiness via a detailed codebook. This codebook was created jointly by expert fact-checkers from the two countries that have Dutch as an official language (Belgium/Flanders and the Netherlands). We expect FactRank to be very useful exactly because of the way we defined check-worthiness, and because of how we have made this explicit and traceable.}
}
@article{WOLFENGAGEN2024101185,
title = {Semantic configuration model with natural transformations},
journal = {Cognitive Systems Research},
volume = {83},
pages = {101185},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101185},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001195},
author = {Viacheslav Wolfengagen and Larisa Ismailova and Sergey Kosikov and Igor Slieptsov and Sebastian Dohrn and Alexander Marenkov and Vladislav Zaytsev},
keywords = {Information process, Configuration, Morphing, Cognitive preference, Semantic net, Functor, Natural transformation},
abstract = {In the present work, efforts have been made to create a configuration-based approach to knowledge extraction. The notion of granularity is developed, which allows fine-tuning the expressive possibilities of the semantic network. As known, the central issues for knowledge-based systems are what’s-in-a-node and what’s-in-a-link. As shown, the answer can be obtained from the functor-as-object representation. Then the nodes are functors, and the main links are natural transformations. Such a model is applicable to represent morphing, and the object is considered as a process, which is in a harmony with current ideas on computing. It is possible to represent information channels that carry out the transformations of processes. The possibility of generating displaced concepts and the generation of families of their morphs is shown, the evolvent of stages of knowledge and properties of the process serve as parameters.}
}
@article{PUTNIK2022678,
title = {Engineering is Design and only Design - Part I: The value of making a distinctive sign},
journal = {Procedia CIRP},
volume = {109},
pages = {678-683},
year = {2022},
note = {32nd CIRP Design Conference (CIRP Design 2022) - Design in a changing world},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.05.313},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122007636},
author = {Goran D. Putnik and Zlata Putnik and Pedro Pinheiro and Cátia Alves},
keywords = {Design, engineering, sign, semiotics},
abstract = {The paper addresses the question: what is engineering? We intuitively know engineering applications such as manufacturing, production, industry, management, business. The answer is not consensual because it is not easy. Furthermore, the ontological question brings us to a second question. What distinguishes engineering from other areas? It is the creative ability that distinguishes engineering. And this artificial faculty only exists in Design. Epistemology in science promotes the existence of herds, increasingly specialized groups of knowledge production. Nevertheless, engineers assume themselves as makers, and in the growing diversity promoted by specialization, they will certainly give different answers when asked about their work. We aggregate all of them as sign-makers. Therefore, engineering is Design and only Design. We reject other views. The argument presented on the phenomenological level considers them false. This paper demonstrates that it is mandatory to create a distinctive sign, which places engineering as relevant in organizations. Without the sign described in semiotics, engineering, which could pretend to be everything, becomes trivial.}
}
@article{JIN2023104752,
title = {Standard terms as analytical variables for collective data sharing in construction management},
journal = {Automation in Construction},
volume = {148},
pages = {104752},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104752},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523000122},
author = {Zhenhui Jin and Seunghee Kang and Yunsub Lee and Youngsoo Jung},
keywords = {Standard variable terms, Construction management, Bibliometric analysis, Data analysis, Data sharing},
abstract = {In order to facilitate data analysis techniques for solving construction management (CM) problems, especially in a collaborative manner, it is essential to define the data schemas of various construction information as the common variables. However, one of the critical barriers is that there is no standard set of variables addressed for collaborative research and practice. Therefore, defining standard variable terms is essential to share, utilize, and analyze construction data systematically and efficiently. In this context, the purpose of this paper is to propose a structured set of standard CM variable terms to enable the global collective analyses of CM data, not only for human managers but also for automated machine inferences. To address this issue, firstly, bibliographic data was extracted from Scopus, focusing on CM research, and the VOSviewer was used to analyze the bibliographic information. Proposed standard variable terms were then organized into a hierarchical structure with two levels, including eight variables in the first level and fifty-seven variables in the second level. Additionally, examples of utilizing standard variable terms were presented to the traditional analytical techniques, ontology, and artificial intelligence (AI) techniques to illustrate the viability of the proposed variable terms in CM. The proposed standard variable terms can be used as the basis for further detailed taxonomy toward collective advanced analytics implementation in CM. This approach will significantly enhance automated knowledge sharing between different applications and machines, resulting in global collective intelligence among unspecified CM professionals.}
}
@article{WANG2024131155,
title = {Multimodal knowledge graph construction for risk identification in water diversion projects},
journal = {Journal of Hydrology},
volume = {635},
pages = {131155},
year = {2024},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2024.131155},
url = {https://www.sciencedirect.com/science/article/pii/S002216942400550X},
author = {Lihu Wang and Xuemei Liu and Yang Liu and Hairui Li and Jiaqi Liu and Libo Yang},
keywords = {Water diversion projects, Multimodal knowledge graph, Risk identification, Convolutional neural networks, Pre-trained models},
abstract = {The operation risks of water diversion projects involve numerous influencing factors, complex interrelationships, and heterogeneous data from multiple sources. This study presents a multimodal knowledge graph construction approach for water diversion projects, aiming to comprehend and identify the key risks associated with engineering operation and their propagation patterns. Utilizing term-masked pre-trained language models enhances comprehension of specialized terminology and identifies risk entities within the text. Employing high-order residual convolutional neural networks improves the processing capability for complex graph data, extracting risk information from images. Aggregating multimodal knowledge graphs based on the semantic relationships among entities and conditional probability to determine the coupled features of different risks. Employing complex network theory, analyze node degree and betweenness centrality to identify the diffusion effects and propagation levels of risks. The results indicate that the knowledge extraction accuracy of our method is high (with an average F1 score of 95.85%), enabling the qualitative analysis and quantitative calculation of operational risks in engineering. Relevant studies can effectively enhance the reliability of engineering safety management and reduce the impact of engineering risks on water supply security.}
}
@article{LOMBARDI2024e00322,
title = {Semantic modelling and HBIM: A new multidisciplinary workflow for archaeological heritage},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {32},
pages = {e00322},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00322},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000079},
author = {Matteo Lombardi and Dario Rizzi},
keywords = {Digital archaeology, Semantic modelling, HBIM, Blender, BlenderBIM, Extended matrix},
abstract = {The aim of the study is to describe a methodological approach to represent, interpret, model and manage pluristratified archaeological contexts. The proposed methodology envisages a digital workflow, a BIM-thinking strategy, which integrates geometric and texture data obtained from laser and photogrammetric scans with information about construction techniques and materials, archaeological reports and documentation. The integration is based on a balanced combination of open-source and proprietary solutions, allowing professionals to work with their “comfort software” and assuring interoperability through the adoption of Open Standards. Experimentations are being conducted exploring the potential of connecting semantic 3D modelling and virtual reconstructions based on archaeological data made with Blender and the Extended Matrix Tool, with BIM software and capabilities thanks to the BlenderBIM addon. The proposed workflow, in combination with the described data-sharing-oriented process, adopts a new approach towards 3D models in order to promote a more sustainable mindset towards 3D dataset life-cycle by optimizing their usage and reducing waste on different levels, such as re-documenting the same structure twice. The expected overall result is the ability to generate semantic models that can enhance our understanding of the context as much as foster multidisciplinary BIM (Building Information Modelling) collaboration thus improving archaeological research, documentation and conservation practices.}
}
@article{SERRADELPINO2023103112,
title = {Postnormal Prometheus: How to anticipate and navigate postnormal times},
journal = {Futures},
volume = {147},
pages = {103112},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103112},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723000174},
author = {Jordi {Serra del Pino}},
keywords = {Postnormal Times, Complexity, Concurrency, Connectivity, Chaos, Contradictions},
abstract = {Despite being labelled as a theory of change, so far, postnormal times theory has only offered a broad explanation of why and how change happens in postnormal times. It has merely stated that it is the joint action of complexity, chaos, and contradictions (the postnormal factors) what prompts postnormal change, but has not produced a more thorough explanation of its development. This has made postnormal times to be perceived as unavoidable or fated. This article uses the simile of Prometheus, the classic titan, to determine up to which point postnormal times are truly ineluctable. Accepting that the three postnormal factors need to act jointly to cause postnormal change but rises in complexity appear to act as a catalyser. So, to have an operative way to estimate these grows in complexity levels, the article proposes to break down complexity into concurrency and connectivity. Both parameters can be measured much more easily and provide a workable way to measure these rises in complexity. Ultimately, the goal is to develop an approach to better understand postnormal change and to anticipate it. Thus, by improving our capacity to comprehend and foresee postnormal change we will also enhance our capacity to navigate postnormal times.}
}
@article{KAPLAN2025101470,
title = {A complex dynamic systems perspective on the roles of culture, context, and identity in psychoeducational interventions},
journal = {Journal of School Psychology},
volume = {110},
pages = {101470},
year = {2025},
issn = {0022-4405},
doi = {https://doi.org/10.1016/j.jsp.2025.101470},
url = {https://www.sciencedirect.com/science/article/pii/S0022440525000433},
author = {Avi Kaplan and Joanna K. Garner and Stephen Whitney},
keywords = {Intervention, Complexity, Paradigm, Culture, Identity, Randomized Control Trials, Context, Design-Based Research},
abstract = {In this paper, we apply a Complex Dynamic Systems (CDS) perspective to reconsider current causal assumptions about educational contexts and the roles of culture, context, and identity in psychoeducational interventions. Focusing on phenomena such as teachers' and students' engagement, motivation, development, and wellbeing, we emphasize the phenomenon's conceptual unit-of-analysis for interventions as “the agent(s) in their authentic lived environment.” Different from assumptions about causality in prevalent approaches to designing and evaluating interventions (e.g., RCT) as cross-contextual, independent, and mostly linear, a CDS perspective affords accounting for contextual and treatment-related heterogeneity and dynamism by viewing causal processes as emergent, non-deterministic, and changing due to factors' inter-dependence, shifting stability, and contextual embeddedness. We describe a CDS model of culturally and contextually based identity, motivation, and action—the Dynamic Systems Model of Role Identity (DSMRI)—to illustrate the application of CDS to psychoeducational interventions. We exemplify this perspective's implications for designing and evaluating psychoeducational interventions as design-based case studies that ground analyses at the unit-of-analysis of the “agent(s)-in-context.”}
}
@article{LAWANI2020320,
title = {Critical realism: what you should know and how to apply it},
journal = {Qualitative Research Journal},
volume = {21},
number = {3},
pages = {320-333},
year = {2020},
issn = {1443-9883},
doi = {https://doi.org/10.1108/QRJ-08-2020-0101},
url = {https://www.sciencedirect.com/science/article/pii/S1443988320000144},
author = {Ama Lawani},
keywords = {Critical realism, Systems thinking, Data analysis, Case study},
abstract = {Purpose
The purpose of this paper is to discuss the critical realism (CR) philosophical viewpoint and how it can be applied in qualitative research. CR is a relatively new and viable philosophical paradigm proposed as an alternative to the more predominant paradigms of positivism, interpretivism and pragmatism. This paper reviews the concept, its benefits and limitation. It goes further to provide an example of how CR is used as a philosophical and methodological framework with the systems thinking theory to applied qualitative research.
Design/methodology/approach
A study of project management challenges in a Nigerian government organisation is used to demonstrate a qualitative research approach, which includes a coding process and data analysis that is consistent with CR ontology and epistemology.
Findings
CR focuses primarily on closed systems. However, a more accurate explanation of reality is obtained in addition to the identification of contextual causal mechanisms in the context of study when a general systems theory is applied.
Research limitations/implications
The knowledge about the nature of relationships obtained in the context of study may not necessarily be replicated in another context. However, this paper elucidates a CR process that is generalisable by demonstrating how a theory is applied in a different context.
Originality/value
The paper demonstrates how systems theory is used to understand interactions in a CR paradigm. It engages with CR approach critically and illustrates a clear example of how CR can be applied in social research.}
}
@article{BABICEANU201947,
title = {Cyber resilience protection for industrial internet of things: A software-defined networking approach},
journal = {Computers in Industry},
volume = {104},
pages = {47-58},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2018.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166361517306954},
author = {Radu F. Babiceanu and Remzi Seker},
keywords = {Software-defined networking, Cybersecurity-resilience mechanisms, Manufacturing logical control, Industrial internet of things},
abstract = {In addition to productivity and quality output, for many years, manufacturing systems were also designed with reliability and safety requirements in mind. In the recent decade or so, the approach seems not adequate anymore. The current manufacturing global operations ask for more stringent requirements than ever before, which include privacy and security of transactions, among others. Manufacturing control is not new, but the use of cloud environments to integrate distributed manufacturing facilities and entirely control the production processes across those facilities is an active research area denoted in terms such as: virtual factory, cloud manufacturing, Industry 4.0, Industrial Internet of Things (IIoT), and more recently, software-defined networking-based (SDN-based) manufacturing. In computer networking domain, SDN is known as a network architecture that decouples the network data and control mechanisms. SDN architecture assigns the entire data control to a logically centralized control plane that can be software-programmed based on specific application needs. From the security point of view, this translates in the fact that anyone with access to the computers that run the network control software could potentially get control over the entire network. This paper proposes an integrated modeling environment that addresses the virtual manufacturing system assurance through cybersecurity and resilience mechanisms for SDN applications. First, the paper proposes a SDN-based manufacturing testbed and a combined cybersecurity-resilience ontology to be used for the requirements capture of the virtual manufacturing network design stages. Then, the paper outlines the framework for SDN-based cybersecurity-resilience protection mechanisms for virtual manufacturing applications, and ends with the envisioned future research needed for implementing the proposed framework.}
}
@article{KOBSA202364,
title = {Correlation between aortic valve protein levels and vector flow mapping of wall shear stress and oscillatory shear index in patients supported with continuous-flow left ventricular assist devices},
journal = {The Journal of Heart and Lung Transplantation},
volume = {42},
number = {1},
pages = {64-75},
year = {2023},
issn = {1053-2498},
doi = {https://doi.org/10.1016/j.healun.2022.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S1053249822021623},
author = {Serge Kobsa and Koichi Akiyama and Samantha K. Nemeth and Paul A. Kurlansky and Yoshifumi Naka and Koji Takeda and Keiichi Itatani and Emily G. Werth and Lewis M. Brown and Giovanni Ferrari and Hiroo Takayama},
keywords = {vector flow mapping, lvad, Aortic Insufficiency, proteomics, wall shear stress},
abstract = {Background
Continuous-flow left ventricular assist devices commonly lead to aortic regurgitation, which results in decreased pump efficiency and worsening heart failure. We hypothesized that non-physiological wall shear stress and oscillatory shear index alter the abundance of structural proteins in aortic valves of left ventricular assist device (LVAD) patients.
Methods
Doppler images of aortic valves of patients undergoing heart transplants were obtained. Eight patients had been supported with LVADs, whereas 10 were not. Aortic valve tissue was collected and protein levels were analyzed using mass spectrometry. Echocardiographic images were analyzed and wall shear stress and oscillatory shear index were calculated. The relationship between normalized levels of individual proteins and in vivo echocardiographic measurements was evaluated.
Results
Of the 57 proteins of interest, there was a strong negative correlation between levels of 15 proteins and the wall shear stress (R < -0.500, p ≤ 0.05), and a moderate negative correlation between 16 proteins and wall shear stress (R −0.500 to −0.300, p ≤ 0.05). Gene ontology analysis demonstrated clusters of proteins involved in cellular structure. Proteins negatively correlated with WSS included those with cytoskeletal, actin/myosin, cell-cell junction and extracellular functions.
C
In aortic valve tissue, 31 proteins were identified involved in cellular structure and extracellular junctions with a negative correlation between their levels and wall shear stress. These findings suggest an association between the forces acting on the aortic valve (AV) and leaflet protein abundance, and may form a mechanical basis for the increased risk of aortic leaflet degeneration in LVAD patients.}
}
@article{MENG20181,
title = {Privacy-aware cloud service selection approach based on P-Spec policy models and privacy sensitivities},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {1-11},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17321271},
author = {Yunfei Meng and Zhiqiu Huang and Yu Zhou and Changbo Ke},
keywords = {Policy matching, Service selection, Privacy sensitivities},
abstract = {In cloud computing era, massive free and function-powerful cloud services can be selected by consumers at any time. However, owing to lacking of privacy policy transparency mechanism and privacy policy comparison mechanism, it is difficult for service consumers to distinguish what is a trusted service and what is a malicious service. To solve these problems, we conceive a comprehensive framework whose primary goal is to set up a policy matching engine as a service mediator to assist service consumers to select out a group privacy trusted services whose privacy policy can comply with consumer’s privacy preferences. Accordingly, we propose a formal policy specification language named P-Spec, which can be utilized to describe the service’s privacy policies and consumer’s privacy preferences explicitly. We further propose a privacy-aware service selection approach, which consists of a group of P-Spec policy models, introduced privacy metrics and a specific policy matching algorithm based on privacy sensitivities. To verify the effectiveness and feasibility of our approach, we implement a proof-of-concept prototype to carry out the relevant experimental studies. The experimental results illustrate our approach can still work well with increasing the scale of policy models. We further utilize the relevant linear fit theories to predict the execution performance of our approach in real cloud, the final predicted results illustrate its performance is permitted and can be improved in real. Lastly, we compare our P-Spec language with some other policy languages and evaluate the comparative results.}
}
@article{FABRE2024264,
title = {Knowledge Graphs – The Future of Integration in CRIS Systems for Uses of Assistance to Scientific Reasoning},
journal = {Procedia Computer Science},
volume = {249},
pages = {264-279},
year = {2024},
note = {16th International Conference on Current Research Information Systems (CRIS 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.11.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924032836},
author = {Renaud Fabre and Otmane Azeroual},
keywords = {Knowledge graphs, data integration, knowledge representation, CRIS systems, semantic data model, scientific uses, comprehensive information},
abstract = {Knowledge graphs (KGs) are gaining prominence for their efficacy in data integration and knowledge representation within Current Research Information Systems (CRIS) Systems. By employing a semantic data model to represent entities, attributes, and their relationships, they prove versatile across scientific applications. In the era of Science platformization, particularly within CRIS systems, KGs serve to amalgamate diverse data sources and formats, facilitating the creation of interconnected data models. This enables stakeholders to access comprehensive, consistent information pertinent to their endeavors. This paper examines the pivotal roles of KGs in current and future data integration within CRIS systems, emphasizing their contributions to scientific reasoning. While their benefits include flexible knowledge modeling, support for semantic queries, and interoperability with various data sources, they face systemic limitations, particularly in methodological and technological aspects, hindering classical scientific investigations. The paper underscores the necessity for novel approaches to address these limitations, offering insights, use cases, and best practices for implementing KGs in CRIS systems. This paves the way for research institutions and scientific organizations to enhance their data analytics capabilities and support scientific reasoning effectively.}
}
@article{MITCHELL2022,
title = {A Question-and-Answer System to Extract Data From Free-Text Oncological Pathology Reports (CancerBERT Network): Development Study},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {3},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/27210},
url = {https://www.sciencedirect.com/science/article/pii/S1438887122002333},
author = {Joseph Ross Mitchell and Phillip Szepietowski and Rachel Howard and Phillip Reisman and Jennie D Jones and Patricia Lewis and Brooke L Fridley and Dana E Rollison},
keywords = {natural language processing, NLP, BERT, transformer, pathology, ICD-O-3, deep learning, cancer},
abstract = {Background
Information in pathology reports is critical for cancer care. Natural language processing (NLP) systems used to extract information from pathology reports are often narrow in scope or require extensive tuning. Consequently, there is growing interest in automated deep learning approaches. A powerful new NLP algorithm, bidirectional encoder representations from transformers (BERT), was published in late 2018. BERT set new performance standards on tasks as diverse as question answering, named entity recognition, speech recognition, and more.
Objective
The aim of this study is to develop a BERT-based system to automatically extract detailed tumor site and histology information from free-text oncological pathology reports.
Methods
We pursued three specific aims: extract accurate tumor site and histology descriptions from free-text pathology reports, accommodate the diverse terminology used to indicate the same pathology, and provide accurate standardized tumor site and histology codes for use by downstream applications. We first trained a base language model to comprehend the technical language in pathology reports. This involved unsupervised learning on a training corpus of 275,605 electronic pathology reports from 164,531 unique patients that included 121 million words. Next, we trained a question-and-answer (Q&A) model that connects a Q&A layer to the base pathology language model to answer pathology questions. Our Q&A system was designed to search for the answers to two predefined questions in each pathology report: What organ contains the tumor? and What is the kind of tumor or carcinoma? This involved supervised training on 8197 pathology reports, each with ground truth answers to these 2 questions determined by certified tumor registrars. The data set included 214 tumor sites and 193 histologies. The tumor site and histology phrases extracted by the Q&A model were used to predict International Classification of Diseases for Oncology, Third Edition (ICD-O-3), site and histology codes. This involved fine-tuning two additional BERT models: one to predict site codes and another to predict histology codes. Our final system includes a network of 3 BERT-based models. We call this CancerBERT network (caBERTnet). We evaluated caBERTnet using a sequestered test data set of 2050 pathology reports with ground truth answers determined by certified tumor registrars.
Results
caBERTnet’s accuracies for predicting group-level site and histology codes were 93.53% (1895/2026) and 97.6% (1993/2042), respectively. The top 5 accuracies for predicting fine-grained ICD-O-3 site and histology codes with 5 or more samples each in the training data set were 92.95% (1794/1930) and 96.01% (1853/1930), respectively.
Conclusions
We have developed an NLP system that outperforms existing algorithms at predicting ICD-O-3 codes across an extensive range of tumor sites and histologies. Our new system could help reduce treatment delays, increase enrollment in clinical trials of new therapies, and improve patient outcomes.}
}
@article{KALITA2020190,
title = {Searching the great metadata timeline},
journal = {Library Hi Tech},
volume = {39},
number = {1},
pages = {190-204},
year = {2020},
issn = {0737-8831},
doi = {https://doi.org/10.1108/LHT-08-2019-0168},
url = {https://www.sciencedirect.com/science/article/pii/S0737883120000287},
author = {Deepjyoti Kalita and Dipen Deka},
keywords = {Metadata, Ontology, Metadata history, Library cataloguing, Linked data},
abstract = {Purpose
The purpose of this paper is to make a systematic review of the library metadata development history listing out the most significant landmarks and influencing events from Thomas Bodley's rules to the latest BIBFRAME architecture, compare their significance and suitability in the modern-day Web environment.
Design/methodology/approach
Four time divisions were identified, namely pre-1900 era, 1900–1950, post-1950 to pre-Web era and post-Web era based on pre-set information available to the authors regarding catalogue rules. Under these four divisions, relevant information sources regarding the purpose of the study were identified; various metadata standards released at different times were consulted.
Findings
Library catalogue standards have undergone transitive changes from one form to another primarily influenced by the changing work environment and different forms of resource availability in libraries. Modern-day metadata standards are influenced by the opportunities provided by the World Wide Web towards libraries and work as a suitable base for data organisation at par with Semantic Web standards.
Research limitations/implications
Information organisation processes have gone towards a more data-centric approach than earlier document-centric nature in current Semantic Web environment. Libraries had to make a move in this process, and modern-day guidelines in this regard bring the possibility of large-scale discovery services through curated information resources.
Originality/value
The study discovers relationships between key events in the course of development of metadata standards and provides suggestions and predictions regarding it's future developments.}
}
@article{GATWIRI2024422,
title = {Becoming both: “students” and “experts” of race in Australian higher education contexts},
journal = {Journal for Multicultural Education},
volume = {18},
number = {4},
pages = {422-434},
year = {2024},
issn = {2053-535X},
doi = {https://doi.org/10.1108/JME-12-2023-0139},
url = {https://www.sciencedirect.com/science/article/pii/S2053535X2400048X},
author = {Kathomi Gatwiri and Hyacinth Udah},
keywords = {Coloniality, Race, Whiteness, Australia, Higher education},
abstract = {Purpose
This paper aims to highlight how Black African academics who live and work under coloniality are systematically seen as “out of place” and how this positioning compounds their experiences of interpersonal and systemic marginalisation within predominantly white universities.
Design/methodology/approach
This is a conceptual paper that theorises the experiences of two Black African academics in Australian higher education. It takes a form of autoethnography, to demonstrate the intersectional barriers and setbacks within white academia that interact with gender, class and migranthood, potentially undermining their academic progression and/or professional well-being.
Findings
Black African academics in white-majority workplaces repeatedly report experiences of microaggressions, hyper-surveillance and epistemic Othering. This is characterised by research alienation, funding gaps and being passed over for promotion leading to feelings of exclusion and fractured belonging within academia.
Originality/value
The paper argues that while the Coloniality of Power within institutions of higher learning continues to racialise Black African academics as Other, the Coloniality of Knowledge marginalises their intellectual, theoretical and experiential perspectives and contributions. The power of Coloniality and white supremacy are implicated in the epistemic impositions, erasures and negations of the ontological legitimacy and contributions of Black academics in higher education institutions.}
}
@incollection{KALEMI2018495,
title = {Semantic Networking Facility for the Biorefining Community},
editor = {Anton Friedl and Jiří J. Klemeš and Stefan Radl and Petar S. Varbanov and Thomas Wallek},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {43},
pages = {495-500},
year = {2018},
booktitle = {28th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64235-6.50088-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642356500887},
author = {Edlira Kalemi and Linsey Koo and Franjo Cecelja},
keywords = {wiki, semantic technologies, biorefining community, knowledge sharing},
abstract = {Biorefining is a dynamic field with ever growing number of computer models developed, heterogeneous data acquired and generally new knowledge generated in large volumes, all to serve functions at different scales and for different purposes. Sharing and reusing of these resources, especially models and data, inherently saves developing time, but also enables solving ever more complex problems and addressing ever more complex functions. This, in turn, necessitates efficient networking tool to allow for publishing, discovery, but also inferring new knowledge and integration of resources, dominantly computer models. This paper introduces a semantic networking facility developed to enable an efficient reuse and sharing of knowledge and resources in the domain of biorefining. This networking facility, which is built around model integration platform, allows for knowledge and other resources in the domain of biorefining to be accessible by humans, but also by agents and services. It is organised as a collaborative, WiKi like platform, aiming to facilitate collaboration and to bring biorefining community together by sharing knowledge and resources. The operation of the proposed facility is supported by a biorefining domain ontology, but also by Semantic Media Wiki extension. The features of the Biorefining Semantic Wiki include semantic repository of the biorefining models, model discovery and integration tool, supply chain networking and demonstration, documentation, including published article repository and discovery, forum for the community and with active social media of the biorefining community. The operation of Biorefining Semantic Wiki has been implemented as a semantic web service and its operation verified in practice.}
}
@article{MOSCATO2024112682,
title = {ALDANER: Active Learning based Data Augmentation for Named Entity Recognition},
journal = {Knowledge-Based Systems},
volume = {305},
pages = {112682},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112682},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124013169},
author = {Vincenzo Moscato and Marco Postiglione and Giancarlo Sperlì and Andrea Vignali},
keywords = {Data augmentation, Named Entity Recognition, Active Learning},
abstract = {Training Named Entity Recognition (NER) models typically necessitates the use of extensively annotated datasets. This requirement presents a significant challenge due to the labor-intensive and costly nature of manual annotation, especially in specialized domains such as medicine and finance. To address data scarcity, two strategies have emerged as effective: (1) Active Learning (AL), which autonomously identifies samples that would most enhance model performance if annotated, and (2) data augmentation, which automatically generates new samples. However, while AL reduces human effort, it does not eliminate it entirely, and data augmentation often leads to incomplete and noisy annotations, presenting new hurdles in NER model training. In this study, we integrate AL principles into a data augmentation framework, named Active Learning-based Data Augmentation for NER (ALDANER), to prioritize the selection of informative samples from an augmented pool and mitigate the impact of noisy annotations. Our experiments across various benchmark datasets and few-shot scenarios demonstrate that our approach surpasses several data augmentation baselines, offering insights into promising avenues for future research.}
}
@article{COX2025,
title = {Conversion of Sensitive Data to the Observational Medical Outcomes Partnership Common Data Model: Protocol for the Development and Use of Carrot},
journal = {JMIR Research Protocols},
volume = {14},
year = {2025},
issn = {1929-0748},
doi = {https://doi.org/10.2196/60917},
url = {https://www.sciencedirect.com/science/article/pii/S1929074825001817},
author = {Samuel Cox and Erum Masood and Vasiliki Panagi and Calum Macdonald and Gordon Milligan and Scott Horban and Roberto Santos and Chris Hall and Daniel Lea and Simon Tarr and Shahzad Mumtaz and Emeka Akashili and Andy Rae and Esmond Urwin and Christian Cole and Aziz Sheikh and Emily Jefferson and Philip Roy Quinlan},
keywords = {data standardization, OMOP, Observational Medical Outcomes Partnership, ETL, extract, transform, and load, data discovery, transparency, Carrot tool, common data model, data standard, health care, data model, data protection, data privacy, open-source},
abstract = {Background
The use of data standards is low across the health care system, and converting data to a common data model (CDM) is usually required to undertake international research. One such model is the Observational Medical Outcomes Partnership (OMOP) CDM. It has gained substantial traction across researchers and those who have developed data platforms. The Observational Health Care Data Sciences and Informatics (OHDSI) partnership manages OMOP and provides many open-source tools to assist in converting data to the OMOP CDM. The challenge, however, is in the skills, knowledge, know-how, and capacity within teams to convert their data to OMOP. The European Health Care Data Evidence Network provided funds to allow data owners to bring in external resources to do the required conversions. The Carrot software (University of Nottingham) is a new set of open-source tools designed to help address these challenges while not requiring data access by external resources.
Objective
The use of data protection rules is increasing, and privacy by design is a core principle under the European and UK legislations related to data protection. Our aims for the Carrot software were to have a standardized mechanism for managing the data curation process, capturing the rules used to convert the data, and creating a platform that can reuse rules across projects to drive standardization of process and improve the speed without compromising on quality. Most importantly, we aimed to deliver this design-by-privacy approach without requiring data access to those creating the rules.
Methods
The software was developed using Agile approaches by both software engineers and data engineers, who would ultimately use the system. Experts in OMOP were used to ensure the approaches were correct. An incremental release program was initiated to ensure we delivered continuous progress.
Results
Carrot has been delivered and used on a project called COVID-Curated and Open Analysis and Research Platform (CO-CONNECT) to assist in the process of allowing datasets to be discovered via a federated platform. It has been used to create over 45,000 rules, and over 5 million patient records have been converted. This has been achieved while maintaining our principle of not allowing access to the underlying data by the team creating the rules. It has also facilitated the reuse of existing rules, with most rules being reused rather than manually curated.
Conclusions
Carrot has demonstrated how it can be used alongside existing OHDSI tools with a focus on the mapping stage. The COVID-Curated and Open Analysis and Research Platform project successfully managed to reuse rules across datasets. The approach is valid and brings the benefits expected, with future work continuing to optimize the generation of rules.
International Registered Report Identifier (IRRID)
RR1-10.2196/60917}
}
@article{YU2022374,
title = {Research on Semantic Model of Information Sharing Based on Cognition},
journal = {Procedia Computer Science},
volume = {208},
pages = {374-383},
year = {2022},
note = {7th International Conference on Intelligent, Interactive Systems and Applications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.053},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014946},
author = {Jun Yu and Manwei Wang and Changshou Luo and Qingfeng Wei and Yaming Zheng},
keywords = {Semantic cognition, semantic description, json-ld;distributed, shared resources},
abstract = {In view of the current demand of heterogeneous, cross domain, many service platforms, wide distribution and low-cost resource sharing platform construction of agricultural public welfare information service resources, this paper puts forward a set of standardized and orderly data acquisition and interpretation sharing methods. Methods combined with the characteristics of multi domain resource sharing, metadata semantic modeling based on cognition is proposed to establish the mapping between metadata cognition and metadata semantics to resource description model, build a basic framework of resource sharing, extract the minimum core metadata required for resource sharing, define the basic rules for the existence of resource sharing data, and give the resource sharing description model. At the same time, in order to clearly explain the semantic annotation information in the model, the construction method of shared resource semantics is introduced. In order to reduce the difficulty of resource description, sharing and publishing on heterogeneous platforms, a shared resource description tool based on "resource sharing semantic description model" is introduced. This method combines the advantages of existing methods. At the same time, through the production of description tools and the introduction of semantics, the development difficulty and development volume of each service platform are reduced, the development cost is saved, the data sharing efficiency is improved, and the requirements of low-cost, cross domain, light dependence, resource readability and machine solvability for the construction of agricultural public welfare information service resource sharing platform are better solved.}
}
@article{QUAN2025101618,
title = {Identification of key hub genes and potential therapeutic drugs for nasopharyngeal carcinoma: Insights into molecular mechanisms and treatment strategies},
journal = {Brazilian Journal of Otorhinolaryngology},
volume = {91},
number = {4},
pages = {101618},
year = {2025},
issn = {1808-8694},
doi = {https://doi.org/10.1016/j.bjorl.2025.101618},
url = {https://www.sciencedirect.com/science/article/pii/S1808869425000618},
author = {Haiyan Quan and Hongguo Yin and Zhen Wang and Yuan Lv and Qiong Sun and Ting Yin},
keywords = {Nasopharyngeal carcinoma, Key genes, Drug screening, Calcitriol},
abstract = {Objective
Nasopharyngeal Carcinoma (NPC) is a highly malignant cancer with a high incidence in East and Southeast Asia, including southern China. Despite advances in treatment, the prognosis for advanced NPC remains poor due to high recurrence and metastasis rates. The molecular mechanisms driving NPC progression are not fully understood, and identifying key genes and potential therapeutic agents is critical. This study aims to uncover critical genes and screen therapeutic drugs, providing insights into NPC pathogenesis and novel treatment strategies.
Methods
Three GEO datasets (GSE12452, GSE53819, and GSE61218) were analyzed to identify overlapping Differentially Expressed Genes (DEGs) in NPC. Gene Ontology (GO), Kyoto Encyclopedia of Genes and Genomes (KEGG), and Gene Set Enrichment Analysis (GSEA) were used to explore the biological roles of DEGs. Protein-Protein Interaction (PPI) and mRNA-miRNA-lncRNA interaction networks were constructed to identify key hub genes. Potential therapeutic drugs were predicted via a Drug-Gene Interaction network. The overexpression of hub genes was validated in NPC cells using CCK-8 assays, and the anti-proliferative effects of three drugs ‒ valproic acid, cyclosporine, and calcitriol ‒ were tested.
Results
Eight hub genes (ASPM, BIRC5, BUB1B, CDK1, KIF23, PBK, TOP2A, and TTK) were identified, with ASPM reported for the first time in the context of NPC. Overexpression of these genes significantly promoted NPC cell proliferation. Among the tested drugs, calcitriol exhibited the most potent anti-proliferative effect, with IC50 values of 0.90 μM, 0.47 μM, and 0.31 μM at 24-, 48-, and 72-hs, respectively.
Conclusion
This study identified eight key genes as potential biomarkers for NPC and validated calcitriol as a promising therapeutic agent, providing a foundation for further research into NPC treatment.
Level of evidence
Level 2 (Individual cross-sectional studies or systematic review of surveys that allow matching to local circumstances).}
}
@article{SIMMONDS20181992,
title = {Enabling the marketing systems orientation: re-establishing the ontic necessity of relations},
journal = {Kybernetes},
volume = {47},
number = {10},
pages = {1992-2011},
year = {2018},
issn = {0368-492X},
doi = {https://doi.org/10.1108/K-09-2017-0352},
url = {https://www.sciencedirect.com/science/article/pii/S0368492X18001093},
author = {Hamish Simmonds},
keywords = {Systems, Marketing systems, Metatheory, Relations},
abstract = {Purpose
This paper aims to critically reflect on the growing systems orientation in marketing research and the approaches used to understand marketing systems. In response, the paper offers an integrative metatheory built on the ontic necessity and subsequently constitutive and causal efficacy of relations.
Design/methodology/approach
This conceptual paper is built on a logic of critique, identifying the generative absences that produce problems in the frameworks in use and attempting to rectify these problems by offering an alternative meta-theoretical structure. This paper draws from critical realism, systems thinking and relational sociology.
Findings
This paper advocates for an emergentist ontology for marketing systems built on the value of both substance and relation as co-principles of existence and the subsequent irreducible stratification derived from this. This position suggests the following propositions: the ontological premise of being is reliant on relations; the social world is constructed of stratified levels of organisation in which entities, their properties and powers emerge by virtue of these relations; these entities operate in complex and mutually modifying interrelations; stability and change is the result of this complex interplay of temporally/spatially stratified relations; and time and space are properties and potential powers of organisation.
Originality/value
This paper considers a number of inconsistencies in current approaches to the study of marketing systems arguing these arise based on the absence of a view of relations that supports an effective theory of emergence. In response, the paper develops a set of ontological presuppositions regarding the nature of marketing systems and a subsequent set of epistemic conditions as an integrative metatheoretical position, through which these systems are better understood and analysed. The paper argues that these improve our ability to theorise about the multi-dimensionality of these systems.}
}
@article{MA2024110739,
title = {Computational modeling of mast cell tryptase family informs selective inhibitor development},
journal = {iScience},
volume = {27},
number = {9},
pages = {110739},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.110739},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224019643},
author = {Ying Ma and Bole Li and Xiangqin Zhao and Yi Lu and Xuesong Li and Jin Zhang and Yifei Wang and Jie Zhang and Lulu Wang and Shuai Meng and Jihui Hao},
keywords = {Immunology, Bioinformatics, Cancer},
abstract = {Summary
Mast cell tryptases, a family of serine proteases involved in inflammatory responses and cancer development, present challenges in structural characterization and inhibitor development. We employed state-of-the-art protein structure prediction algorithms to model the three-dimensional structures of tryptases α, β, δ, γ, and ε with high accuracy. Computational docking identified potential substrates and inhibitors, suggesting overlapping yet distinct activities. Tryptases β, δ, and ε were predicted to act on phenolic compounds, with β and ε additionally hydrolyzing cyanides. Tryptase δ may possess unique formyl-CoA dehydrogenase activity. Virtual screening revealed 63 compounds exhibiting strong binding to tryptase β (TPSB2), 12 exceeding the affinity of the known inhibitor. Notably, the top hit (3-chloro-4-methylbenzimidamide) displayed over 10-fold selectivity for tryptase β over other isoforms. Our integrative approach combining protein modeling, functional annotation, and molecular docking provides a framework for characterizing tryptase isoforms and developing selective inhibitors of therapeutic potential in inflammatory and cancer conditions.}
}
@article{RUDENKO20241216,
title = {Interaction between BIM and FE models in structural health monitoring},
journal = {Procedia Structural Integrity},
volume = {64},
pages = {1216-1223},
year = {2024},
note = {SMAR 2024 – 7th International Conference on Smart Monitoring, Assessment and Rehabilitation of Civil Structures},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2024.09.169},
url = {https://www.sciencedirect.com/science/article/pii/S2452321624007613},
author = {Iryna Rudenko and Yuri Petryna},
keywords = {Building Information Modeling, Industry Foundation Classes, finite element model, structural health monitoring},
abstract = {Building Information Modeling (BIM) is already widely used in civil engineering projects. Digital building models that contain geometric as well as semantic information are usually generated and could be then managed throughout a structural service life. A BIM model could generally serve as a primary source of any required information on a building, including the finite element (FE) models or monitoring systems as well. The interaction between BIM and FE models is of great importance for structural engineering as it helps increase productivity and minimize mistakes due to human factors. Moreover, with the help of structural health monitoring (SHM), it should be possible to update BIM and FE models to the current state of the structure and to maintain the remaining service life. Obviously, FE models of different complexity and element dimensionality are required for the same structure in view of various structural or material limit states considered during the structural design phase and the service life as well. The present contribution describes the development of an approach that allows FE models of different complexity to be consistently extracted from the same BIM model. It focuses on the openBIM technology incorporating the Industry Foundation Classes (IFC) format. The corresponding IFC file is enriched with FEM and SHM relevant information, for example, FE size and dimensionality as well as positions and conditions of the sensors. Using such information in the BIM model, the FE model for ANSYS APDL can be consistently created. The simulation or monitoring results can be subsequently introduced back into the original BIM model, thus describing the actual structural state. The approach will be illustrated by an example of a laboratory structure.}
}
@article{DING2024102622,
title = {Product color emotional design based on 3D knowledge graph},
journal = {Displays},
volume = {81},
pages = {102622},
year = {2024},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2023.102622},
url = {https://www.sciencedirect.com/science/article/pii/S0141938223002561},
author = {Man Ding and Mingyu Sun and Shijian Luo},
keywords = {Product color emotion design, RotatE knowledge graph, PageRank, Big data, 3D Knowledge Graph},
abstract = {To address the problem of fragmentation, integration difficulties in fuzzy front-end information, and ambiguity in color emotion knowledge representation and conversion within the current product color emotion design stage, this paper proposes a method based on 3D Knowledge Graph. The proposed approach aims to integrate product color emotion design into the “data knowledge + artificial intelligence” growth model, facilitating a beneficial knowledge output cycle driven by data. As for the proposed approach, it divides the design problem into three stages: in the first one, big data web crawler technology and natural language processing were employed to extract knowledge related to product color emotion design. In the second phase, the construction of a product color emotion imagery association model using the RotatE knowledge graph, thereby achieving knowledge fusion of product color emotion design, and all accumulated knowledge data is integrated into a 3D Knowledge Graph for visualization. Finally, in the third phase, the PageRank algorithm is applied to calculate primary and auxiliary color weight parameters, simulating the product color synergy mechanism and determining the color synergy effects. Then, realize the knowledge generation of product color emotional design. This approach combines the human visual experience feedback with extensive big data analysis, and accurately outputs the product color emotion design scheme that meets the user's emotional needs. Moreover, the effectiveness and applicability of the method are verified by an illustrative example involving modern machine tool.}
}
@article{BURGGRAF2020100025,
title = {Knowledge-based problem solving in physical product development––A methodological review},
journal = {Expert Systems with Applications: X},
volume = {5},
pages = {100025},
year = {2020},
issn = {2590-1885},
doi = {https://doi.org/10.1016/j.eswax.2020.100025},
url = {https://www.sciencedirect.com/science/article/pii/S2590188520300044},
author = {Peter Burggräf and Johannes Wagner and Tim Weißer},
keywords = {Manufacturing problem solving, Product development, Knowledge-based system, Case-based reasoning, Machine learning},
abstract = {The manufacturing of products at low maturity levels (referred to as physical product development) requires knowledge intensive nonconformance problem solving, yet constituting a major difficulty in industry. Due to the exponential increase of failure cost during the product development process however, problems have to be effectively remedied as early as possible. Facing shortened innovation cycles, problem solving efficiency simultaneously constitutes a competitive factor. The purpose of this theoretical review is therefore the analysis of relevant approaches contributing to knowledge-based problem solving in physical product development, to synthesize a comprehensive construct as well as to derive novel conceptualizations. The latter demonstrably emerges from natural language processing, case ontologies and machine-/deep learning support, embedded in a distributed case-based reasoning architecture. Building on this, we likewise encourage researchers and professionals to propose new studies dedicated to the field of problem solving in physical product development.}
}
@article{MORENOFERNANDEZ2025100452,
title = {A Hidden Mark of a Troubled Past: Neuroimaging and Transcriptomic Analyses Reveal Interactive Effects of Maternal Immune Activation and Adolescent THC Exposure Suggestive of Increased Neuropsychiatric Risk},
journal = {Biological Psychiatry Global Open Science},
volume = {5},
number = {3},
pages = {100452},
year = {2025},
issn = {2667-1743},
doi = {https://doi.org/10.1016/j.bpsgos.2025.100452},
url = {https://www.sciencedirect.com/science/article/pii/S2667174325000060},
author = {Mario Moreno-Fernández and Víctor Luján and Shishir Baliyan and Celia Poza and Roberto Capellán and Natalia {de las Heras-Martínez} and Miguel Ángel Morcillo and Marta Oteo and Emilio Ambrosio and Marcos Ucha and Alejandro Higuera-Matas},
keywords = {Adolescence, Cannabinoids, Maternal immune activation, Orbitofrontal cortex, PET, Transcriptome},
abstract = {Background
Maternal exposure to infections during gestation has been shown to predispose individuals to neuropsychiatric disorders. Additionally, clinical data suggest that cannabis use may trigger the onset of schizophrenia in vulnerable individuals. However, the direction of causality remains unclear.
Methods
To elucidate this issue, we utilized a rat model of maternal immune activation combined with exposure to increasing doses of Δ9-tetrahydrocannabinol during adolescence in both male and female rats. We investigated several behaviors in adulthood relevant for neuropsychiatric disorders, including impairments in working memory, deficits in sensorimotor gating, alterations in social behavior, anhedonia, and potential changes in implicit learning (conditioned taste aversion). Furthermore, we conducted a longitudinal positron emission tomography study to target affected brain regions and, subsequently, collected brain samples of one such region (the orbitofrontal cortex) for RNA sequencing analyses, which were also performed on peripheral blood mononuclear cells to identify peripheral biomarkers.
Results
While adolescent Δ9-tetrahydrocannabinol did not unmask latent behavioral disruptions, positron emission tomography scans revealed several brain alterations dependent on the combination of both hits. Additionally, the transcriptomic studies demonstrated that maternal immune activation affected dopaminergic, glutamatergic, and serotoninergic genes, with the combination of both exposures in most cases shifting the expression from downregulation to upregulation. In peripheral cells, interactive effects were observed on inflammatory pathways, and some genes were proposed as biomarkers.
Conclusions
These results suggest that the combination of these 2 vulnerability factors leaves a lasting mark on the body, potentially predisposing individuals to neuropsychiatric disorders even before behavioral alterations manifest.}
}
@article{ANGSKUN2019347,
title = {FLORA: a hierarchical fuzzy system for online accommodation review analysis},
journal = {Journal of Systems and Information Technology},
volume = {21},
number = {3},
pages = {347-367},
year = {2019},
issn = {1328-7265},
doi = {https://doi.org/10.1108/JSIT-03-2018-0046},
url = {https://www.sciencedirect.com/science/article/pii/S1328726519000090},
author = {Thara Angskun and Jitimon Angskun},
keywords = {Tourist preference, Accommodation reviews, Semantic analysis, Hierarchical fuzzy system},
abstract = {Purpose
This paper aims to introduce a hierarchical fuzzy system for an online review analysis named FLORA. FLORA enables tourists to decide their destination without reading numerous reviews from experienced tourists. It summarizes reviews and visualizes them through a hierarchical structure. The visualization does not only present overall quality of an accommodation, but it also presents the condition of the bed, hospitality of the front desk receptionist and much more in a snap.
Design/methodology/approach
FLORA is a complete system which acquires online reviews, analyzes sentiments, computes feature scores and summarizes results in a hierarchical view. FLORA is designed to use an overall score, rated by real tourists as a baseline for accuracy comparison. The accuracy of FLORA has achieved by a novel sentiment analysis process (as part of a knowledge acquisition engine) based on semantic analysis and a novel rating technique, called hierarchical fuzzy calculation, in the knowledge inference engine.
Findings
The performance comparison of FLORA against related work has been assessed in two aspects. The first aspect focuses on review analysis with binary format representation. The results reveal that the hierarchical fuzzy method, with probability weighting of FLORA, is achieved with the highest values in precision, recall and F-measure. The second aspect looks at review analysis with a five-point rating scale rating by comparing with one of the most advanced research methods, called fuzzy domain ontology. The results reveal that the hierarchical fuzzy method, with probability weighting of FLORA, returns the closest results to the tourist-defined rating.
Research limitations/implications
This research advances knowledge of online review analysis by contributing a novel sentiment analysis process and a novel rating technique. The FLORA system has two limitations. First, the reviews are based on individual expression, which is an arbitrary distinction and not always grammatically correct. Consequently, some opinions may not be extracted because the context free grammar rules are insufficient. Second, natural languages evolve and diversify all the time. Many emerging words or phrases, including idioms, proverbs and slang, are often used in online reviews. Thus, those words or phrases need to be manually updated in the knowledge base.
Practical implications
This research contributes to the tourism business and assists travelers by introducing comprehensive and easy to understand information about each accommodation to travelers. Although the FLORA system was originally designed and tested with accommodation reviews, it can also be used with reviews of any products or services by updating data in the knowledge base. Thus, businesses, which have online reviews for their products or services, can benefit from the FLORA system.
Originality/value
This research proposes a FLORA system which analyzes sentiments from online reviews, computes feature scores and summarizes results in a hierarchical view. Moreover, this work is able to use the overall score, rated by real tourists, as a baseline for accuracy comparison. The main theoretical implication is a novel sentiment analysis process based on semantic analysis and a novel rating technique called hierarchical fuzzy calculation.}
}
@article{NEB202070,
title = {Automatic generation of assembly graphs based on 3D models and assembly features},
journal = {Procedia CIRP},
volume = {88},
pages = {70-75},
year = {2020},
note = {13th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 17-19 July 2019, Gulf of Naples, Italy},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120303280},
author = {Alexander Neb and Julian Hitzer},
keywords = {Assembly planning, Assembly modelling, AutomationML},
abstract = {The vision to generate Assembly Sequences directly from a virtual 3D model is linked to some unsolved challenges. One of these challenges is the generation of the Assembly Graphs. This approach is based on a 3D Model of a product assembly and Assembly Features generated from a CAD system. The Assembly Graph is described by AutomationML (Automation Markup Language). AutomationML is a neutral data format to describe real plant components. The focus of this work is to generate automatically an Assembly Graphs by just using information from CAD systems.}
}
@article{VELASCOBENITO2023108995,
title = {An empirically supported approach to the treatment of imprecision in vague reasoning},
journal = {International Journal of Approximate Reasoning},
volume = {161},
pages = {108995},
year = {2023},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2023.108995},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X23001263},
author = {Gael {Velasco Benito} and Alejandro {Sobrino Cerdeiriña} and Alberto Bugarín-Diz},
keywords = {Linguistic vagueness, Computing with words, Approximate reasoning},
abstract = {The aim of this paper is to propose a new approach for the automatic treatment of linguistic vagueness. Our motivation is the feeling that most existing approaches dealing with linguistic information are based on converting vague meaning into crisp meaning using some conversions to precise measurements. As a result, existing approaches are adequate and easy to implement, but do not closely model the human thought process. To help alleviate this deficiency, we propose the use of linguistic relations to provide a natural language interface to an end user. We show a possible linguistic Prolog model based on an extension of the syntactic unification algorithm using synonymy and antonymy, as well as the extension of the resolution principle. Our approach does not aim to provide a well-founded formal semantics for such a linguistic Prolog, but a simple model supported by two experiments focused on the use of vague language, both of them executed in Spanish (an analysis of the data of the first experiment it is also available in that language at [1]). Thus, the purpose of this paper is to contribute to the mechanization of approximate reasoning by being respectful of the semantics of the vague terms involved in it; i.e., by paying attention to how they are evaluated by linguistic users under experimentation.}
}
@article{BISTARELLI2023679,
title = {*-chain: A framework for automating the modeling of blockchain based supply chain tracing systems},
journal = {Future Generation Computer Systems},
volume = {149},
pages = {679-700},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2023.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X23002625},
author = {Stefano Bistarelli and Francesco Faloci and Paolo Mori},
keywords = {Supply chain, Blockchain, Distributed ledger technology, Domain specific graphical language, Smart contracts, Automatic smart contract generation},
abstract = {Nowadays, creating a blockchain-based system for supply chain tracing is a complex task. This paper defines a model, a graphical domain specific language, and a set of tools aimed at helping supply chain domain experts to create blockchain based tracing systems for their supply chains. Starting from a graphical representation of the supply chain, the solidity smart contracts implementing the related tracing system are automatically generated by our framework. Small interventions of programmers are required to customize and finalize such smart contracts. A set of web based interfaces to interact with such smart contracts are also automatically generated. We are confident that our results will increase blockchain usage for supply chain traceability thanks to the automatic process of smart contract generation.}
}
@article{CHEN2025103265,
title = {Self-supervised representation learning for geospatial objects: A survey},
journal = {Information Fusion},
volume = {123},
pages = {103265},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103265},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525003380},
author = {Yile Chen and Weiming Huang and Kaiqi Zhao and Yue Jiang and Gao Cong},
keywords = {Geospatial artificial intelligence, Spatial data mining, Self-supervised learning, Spatial representation learning, Geospatial foundation models},
abstract = {The proliferation of various data sources in urban and territorial environments has significantly facilitated the development of geospatial artificial intelligence (GeoAI) across a wide range of geospatial applications. However, geospatial data, which is inherently linked to geospatial objects, often exhibits data heterogeneity that necessitates specialized fusion and representation strategies while simultaneously being inherently sparse in labels for downstream tasks. Consequently, there is a growing demand for techniques that can effectively leverage geospatial data without heavy reliance on task-specific labels and model designs. This need aligns with the principles of self-supervised learning (SSL), which has garnered increasing attention for its ability to learn effective and generalizable representations directly from data without extensive labeled supervision. This paper presents a comprehensive and up-to-date survey of SSL techniques specifically applied to or developed for geospatial objects in three primary vector geometric types: Point, Polyline, and Polygon. We systematically categorize various SSL techniques into predictive and contrastive methods, and analyze their adaptation to different data types for representation learning across various downstream tasks. Furthermore, we examine the emerging trends in SSL for geospatial objects, particularly the gradual advancements towards geospatial foundation models. Finally, we discuss key challenges in current research and outline promising directions for future investigation. By offering a structured analysis of existing studies, this paper aims to inspire continued progress in integrating SSL with geospatial objects, and the development of geospatial foundation models in a longer term.}
}
@article{WYMAN2022101140,
title = {Selection levels on vocal individuality: strategic use or byproduct},
journal = {Current Opinion in Behavioral Sciences},
volume = {46},
pages = {101140},
year = {2022},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2022.101140},
url = {https://www.sciencedirect.com/science/article/pii/S2352154622000468},
author = {Megan T Wyman and Britta Walkenhorst and Marta B. Manser},
abstract = {In animals, large variation for vocal individuality between and within call types exist, yet we know little on what level selection is taking place. Identifying the selection pressures causing this variation in individuality will provide insight into the evolutionary relationships between cognitive and behavioral processes and communication systems, particularly in group-living species where repeated interactions are common. Analyzing a species’ full, large vocal repertoire on individual signatures, its biological function, and the respective selection pressures is challenging. Here, we emphasize that comparing the acoustic individual distinctiveness between life-history stages and different subjects within a call type will allow the identification of selection pressures and enhance the understanding of variation in individuality and its potential strategic use by senders.}
}
@article{AFZAL2020,
title = {Clinical Context–Aware Biomedical Text Summarization Using Deep Neural Network: Model Development and Validation},
journal = {Journal of Medical Internet Research},
volume = {22},
number = {10},
year = {2020},
issn = {1438-8871},
doi = {https://doi.org/10.2196/19810},
url = {https://www.sciencedirect.com/science/article/pii/S1438887120010031},
author = {Muhammad Afzal and Fakhare Alam and Khalid Mahmood Malik and Ghaus M Malik},
keywords = {biomedical informatics, automatic text summarization, deep neural network, word embedding, semantic similarity, brain aneurysm},
abstract = {Background
Automatic text summarization (ATS) enables users to retrieve meaningful evidence from big data of biomedical repositories to make complex clinical decisions. Deep neural and recurrent networks outperform traditional machine-learning techniques in areas of natural language processing and computer vision; however, they are yet to be explored in the ATS domain, particularly for medical text summarization.
Objective
Traditional approaches in ATS for biomedical text suffer from fundamental issues such as an inability to capture clinical context, quality of evidence, and purpose-driven selection of passages for the summary. We aimed to circumvent these limitations through achieving precise, succinct, and coherent information extraction from credible published biomedical resources, and to construct a simplified summary containing the most informative content that can offer a review particular to clinical needs.
Methods
In our proposed approach, we introduce a novel framework, termed Biomed-Summarizer, that provides quality-aware Patient/Problem, Intervention, Comparison, and Outcome (PICO)-based intelligent and context-enabled summarization of biomedical text. Biomed-Summarizer integrates the prognosis quality recognition model with a clinical context–aware model to locate text sequences in the body of a biomedical article for use in the final summary. First, we developed a deep neural network binary classifier for quality recognition to acquire scientifically sound studies and filter out others. Second, we developed a bidirectional long-short term memory recurrent neural network as a clinical context–aware classifier, which was trained on semantically enriched features generated using a word-embedding tokenizer for identification of meaningful sentences representing PICO text sequences. Third, we calculated the similarity between query and PICO text sequences using Jaccard similarity with semantic enrichments, where the semantic enrichments are obtained using medical ontologies. Last, we generated a representative summary from the high-scoring PICO sequences aggregated by study type, publication credibility, and freshness score.
Results
Evaluation of the prognosis quality recognition model using a large dataset of biomedical literature related to intracranial aneurysm showed an accuracy of 95.41% (2562/2686) in terms of recognizing quality articles. The clinical context–aware multiclass classifier outperformed the traditional machine-learning algorithms, including support vector machine, gradient boosted tree, linear regression, K-nearest neighbor, and naïve Bayes, by achieving 93% (16127/17341) accuracy for classifying five categories: aim, population, intervention, results, and outcome. The semantic similarity algorithm achieved a significant Pearson correlation coefficient of 0.61 (0-1 scale) on a well-known BIOSSES dataset (with 100 pair sentences) after semantic enrichment, representing an improvement of 8.9% over baseline Jaccard similarity. Finally, we found a highly positive correlation among the evaluations performed by three domain experts concerning different metrics, suggesting that the automated summarization is satisfactory.
Conclusions
By employing the proposed method Biomed-Summarizer, high accuracy in ATS was achieved, enabling seamless curation of research evidence from the biomedical literature to use for clinical decision-making.}
}
@article{BORUAH201863,
title = {Object Recognition based on Surface Detection - A Review},
journal = {Procedia Computer Science},
volume = {133},
pages = {63-74},
year = {2018},
note = {International Conference on Robotics and Smart Manufacturing (RoSMa2018)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918309530},
author = {Abhijit Boruah and Nayan M. Kakoty and Tazid Ali},
keywords = {Tactile, haptic, kinesthetic, recognition, perception, prosthetic, knowledge, reasoning, ontology},
abstract = {The task of object recognition for prosthetic hands to perform effective grasping has been among the prime objectives in the domain of rehabilitation robotics. Pertaining to the sensors being implemented in the existing approaches, the object recognition techniques can be divided into vision based and haptic based categories. Most of the works on object recognition for prosthetic hands have adapted hybrid approaches by utilizing both vision and tactile sensors together. Nevertheless, classification of visual features along with tactile features incurred high algorithmic complexity and excess hardware requirements. Moreover, embedding a vision based system for a prosthetic limb does not seem to be natural and practically plausible solution. Therefore, an approach towards tactile based object recognition with advantages of the reported hybrid system seems assuring. In this review, existing approaches towards tactile sensing for prosthetic hands are identified. A comparative study and analysis on usability of tactile sensing methods is also considered for discussion in this paper. Based on this review, knowledge representational approaches for object recognition by prosthetic hands are suggested as one of the prominent future directions of research.}
}
@article{HUANG2025100042,
title = {Uncovering prognostic markers and therapeutic targets in acute myeloid leukemia: Insights from differential gene expression and Mendelian randomization analysis},
journal = {Medicine in Omics},
volume = {13},
pages = {100042},
year = {2025},
issn = {2590-1249},
doi = {https://doi.org/10.1016/j.meomic.2025.100042},
url = {https://www.sciencedirect.com/science/article/pii/S2590124925000021},
author = {Yaqi Huang and Xiao Zhu and Dongpei Li},
keywords = {Mendelian Randomization, Acute myeloid leukemia, FLT3LG, KEGG},
abstract = {The development and prognosis of acute myeloid leukemia (AML) are influenced by multiple factors. This study utilized bioinformatics analysis to explore differentially expressed genes (DEGs) in acute myeloid leukemia (AML) and non-tumor tissues, evaluating their prognostic significance. Target gene data from The Cancer Genome Atlas (TCGA) and Genotype-Tissue Expression (GTEx) databases were extracted for analysis. Over 100 DEGs were identified, with MIR9-1 exhibiting downregulated expression in AML. Survival analysis revealed significant differences in overall survival rates between subgroups, with Cluster 2 showing better outcomes. Notable DEGs, including DEFA1B, FLT3LG, CUX1, and ZMYM2, were identified. Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway analysis highlighted relevant signaling pathways. Mendelian Randomization (MR) analysis unveiled a negative correlation between the “transcriptional misregulation in cancer pathway” and “hypermethylation of CpG island pathway” with AML. Sensitivity analysis demonstrated no heterogeneity or pleiotropy. Bayesian Weighted Mendelian Randomization (BWMR) validated MR results. Overall, this study identified potential therapeutic targets like FLT3LG, elucidated key genes for AML prognosis, and revealed protective roles of pathways through comprehensive bioinformatics analysis and Mendelian randomization.}
}
@article{WERMANN2025111095,
title = {KTWIN: A serverless Kubernetes-based Digital Twin platform},
journal = {Computer Networks},
volume = {259},
pages = {111095},
year = {2025},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2025.111095},
url = {https://www.sciencedirect.com/science/article/pii/S1389128625000635},
author = {Alexandre Gustavo Wermann and Juliano Araujo Wickboldt},
keywords = {Digital Twin, Serverless, Kubernetes, Event-Driven Architecture, Internet of Things},
abstract = {Digital Twins (DTs) systems are virtual representations of physical assets allowing organizations to gain insights and improve existing processes. In practice, DTs require proper modeling, coherent development and seamless deployment along cloud and edge landscapes relying on established patterns to reduce operational costs. In this work, we propose KTWIN a Kubernetes-based Serverless Platform for Digital Twins. KTWIN was developed using the state-of-the-art open-source Cloud Native tools, allowing DT operators to easily define models through open standards and configure details of the underlying services and infrastructure. The experiments carried out with the developed prototype show that KTWIN can provide a higher level of abstraction to model and deploy a Digital Twin use case without compromising the solution scalability. The tests performed also show cost savings ranging between 60% and 80% compared to overprovisioned scenarios.}
}
@article{WANG2022193,
title = {Automatic frequency estimation of contributory factors for confined space accidents},
journal = {Process Safety and Environmental Protection},
volume = {157},
pages = {193-207},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2021.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0957582021006005},
author = {Bingyu Wang and Jinsong Zhao},
keywords = {Confined space, Accident reports, Text-mining, BERT-BiLSTM-CRF, CNN},
abstract = {Although the dangers of working in confined spaces have been known for many years, fatal accidents related to working in confined spaces still frequently occur. Considerable research has been conducted to identify potential contributory factors of confined space incidents through analyzing accident reports. However, accident databases are usually read and interpreted manually by human experts. The process of analyzing confined space accident reports can be time-consuming and labor-intensive. As the number of accident records increases, it is difficult for the experts to manually review all the reports. Moreover, different individuals may reach various conclusions from the same accident report. Some analysts may fail to capture all the meaningful and relevant causal factors. Automatic information extraction using special rules and ontology-based approaches can be used to mine reports of confined space accidents. However, such approaches tend to suffer from the problem of weak generalization. To overcome this limitation and improve the performance of contributory factors analysis, an improved deep learning based framework is proposed in this paper to automatically extract and classify contributory factors from confined space accident reports using BERT-BiLSTM-CRF and CNN models. Research results suggested that the proposed framework can be used as a feasible method to qualitatively and quantitatively explore the contributory factors of confined space accidents. By analyzing a large quantity of confined space accident reports, the frequency of contributory factors can be estimated automatically. This outcome is helpful to significantly improve the risk assessment quality of confined space works.}
}
@article{THONUS2020100725,
title = {The Disciplinary Identity of Second Language Writing},
journal = {Journal of Second Language Writing},
volume = {49},
pages = {100725},
year = {2020},
issn = {1060-3743},
doi = {https://doi.org/10.1016/j.jslw.2020.100725},
url = {https://www.sciencedirect.com/science/article/pii/S1060374320300163},
author = {Terese Thonus},
keywords = {Second language writing, Applied linguistics, Composition, Disciplinarity, Interdisciplinarity, Transdisciplinarity, Translingual writing},
abstract = {Second language writing has been variously described as a field, as an interdiscipline, and a transdiscipline. Not only are these characterizations of second language writing inaccurate; they are also precarious, fostering an intellectual climate in which other theories and practices emerge as substitutes. In fact, SLW acts as if it were a discipline, so the label discipline makes sense epistemologically and practically. Owning a disciplinary identity strengthens SLW’s theoretical and practical claims and supports researchers, practitioners, graduate students, and above all, multilingual writers.}
}
@article{SOSA2023110528,
title = {GOCompare: An R package to compare functional enrichment analysis between two species},
journal = {Genomics},
volume = {115},
number = {1},
pages = {110528},
year = {2023},
issn = {0888-7543},
doi = {https://doi.org/10.1016/j.ygeno.2022.110528},
url = {https://www.sciencedirect.com/science/article/pii/S0888754322002737},
author = {Chrystian C. Sosa and Diana Carolina Clavijo-Buriticá and Victor Hugo García-Merchán and Nicolas López-Rozo and Camila Riccio-Rengifo and Maria Victoria Diaz and David Arango Londoño and Mauricio Alberto Quimbaya},
keywords = {Undirected graphs, Geneset enrichment analysis, Comparative genomics, Aluminum tolerance},
abstract = {Functional enrichment analysis is a cornerstone in bioinformatics as it makes possible to identify functional information by using a gene list as source. Different tools are available to compare gene ontology (GO) terms, based on a directed acyclic graph structure or content-based algorithms which are time-consuming and require a priori information of GO terms. Nevertheless, quantitative procedures to compare GO terms among gene lists and species are not available. Here we present a computational procedure, implemented in R, to infer functional information derived from comparative strategies. GOCompare provides a framework for functional comparative genomics starting from comparable lists from GO terms. The program uses functional enrichment analysis (FEA) results and implement graph theory to identify statistically relevant GO terms for both, GO categories and analyzed species. Thus, GOCompare allows finding new functional information complementing current FEA approaches and extending their use to a comparative perspective. To test our approach GO terms were obtained for a list of aluminum tolerance-associated genes in Oryza sativa subsp. japonica and their orthologues in Arabidopsis thaliana. GOCompare was able to detect functional similarities for reactive oxygen species and ion binding capabilities which are common in plants as molecular mechanisms to tolerate aluminum toxicity. Consequently, the R package exhibited a good performance when implemented in complex datasets, allowing to establish hypothesis that might explain a biological process from a functional perspective, and narrowing down the possible landscapes to design wet lab experiments.}
}
@article{GOH201840,
title = {FPSWizard: A web-based CBR-RBR system for supporting the design of active fall protection systems},
journal = {Automation in Construction},
volume = {85},
pages = {40-50},
year = {2018},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2017.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0926580517301383},
author = {Yang Miang Goh and Brian H.W. Guo},
keywords = {Case-based reasoning, Rule-based reasoning, Fall from height, Active fall protection system, Construction safety},
abstract = {Fall from height is a perennial problem in the construction industry. Active fall protection system (AFPS) is frequently a must in situations where working conditions are difficult and other controls are not feasible or inadequate. However, the design and selection of AFPS are still problematic in the construction industry. This paper aims to develop an online knowledge-based system, FPSWizard, to support the design and selection of AFPS. The hybrid system adopts a combination of case-based reasoning (CBR) and rule-based reasoning (RBR) to improve retrieval performance. FPSWizard is meant to recommend suitable AFPS based on similar past design cases. Potential end users, such as professional engineers and safety professionals, can use the system as a decision support system when they are selecting and designing a solution to the work-at-height problem at hand. A total of fifty stored cases were created based on actual work scenarios and AFPS designs in the construction industry. A case structure was also created using the AFPS-Ontology. The system was assessed using a leave-one-out cross validation approach, where fifty cases in the case base were used to test the retrieval performance of the system. The hybrid CBR-RBR approach had an average positive predictive value (PPV) (or precision) of 90%. In comparison, a pure CBR approach had an average PPV of 76%. FPSWizard forms an important part of an intelligent system which provides comprehensive solutions to fall from height. This paper also made important strides towards intelligent safety engineering and management in the construction industry.}
}
@article{R2025100238,
title = {Enhancing drug discovery and patient care through advanced analytics with the power of NLP and machine learning in pharmaceutical data interpretation},
journal = {SLAS Technology},
volume = {31},
pages = {100238},
year = {2025},
issn = {2472-6303},
doi = {https://doi.org/10.1016/j.slast.2024.100238},
url = {https://www.sciencedirect.com/science/article/pii/S2472630324001201},
author = {Nagalakshmi R and Surbhi Bhatia Khan and Ananthoju Vijay kumar and Mahesh {T R} and Mohammad Alojail and Saurabh Raj Sangwan and Mo Saraee},
keywords = {Natural language processing, Pharmaceutical analytics, Healthcare technology, Personalized Medicine, Text Mining, BERT},
abstract = {This study delves into the transformative potential of Machine Learning (ML) and Natural Language Processing (NLP) within the pharmaceutical industry, spotlighting their significant impact on enhancing medical research methodologies and optimizing healthcare service delivery. Utilizing a vast dataset sourced from a well-established online pharmacy, this research employs sophisticated ML algorithms and cutting-edge NLP techniques to critically analyze medical descriptions and optimize recommendation systems for drug prescriptions and patient care management. Key technological integrations include BERT embeddings, which provide nuanced contextual understanding of complex medical texts, and cosine similarity measures coupled with TF-IDF vectorization to significantly enhance the precision and reliability of text-based medical recommendations. By meticulously adjusting the cosine similarity thresholds from 0.2 to 0.5, our tailored models have consistently achieved a remarkable accuracy rate of 97 %, illustrating their effectiveness in predicting suitable medical treatments and interventions. These results not only highlight the revolutionary capabilities of NLP and ML in harnessing data-driven insights for healthcare but also lay a robust groundwork for future advancements in personalized medicine and bespoke treatment pathways. Comprehensive analysis demonstrates the scalability and adaptability of these technologies in real-world healthcare settings, potentially leading to substantial improvements in patient outcomes and operational efficiencies within the healthcare system.}
}
@article{KANKE202064,
title = {Knowledge curation work in Wikidata WikiProject discussions},
journal = {Library Hi Tech},
volume = {39},
number = {1},
pages = {64-79},
year = {2020},
issn = {0737-8831},
doi = {https://doi.org/10.1108/LHT-04-2019-0087},
url = {https://www.sciencedirect.com/science/article/pii/S073788312000041X},
author = {Timothy Kanke},
keywords = {Knowledge organization, Qualitative research, Wikidata, Curation, Online curation communities, WikiProjects},
abstract = {Purpose
The purpose of this paper is to investigate how editors participate in Wikidata and how they organize their work.
Design/methodology/approach
This qualitative study used content analysis of discussions involving data curation and negotiation in Wikidata. Activity theory was used as a conceptual framework for data collection and analysis.
Findings
The analysis identified six activities: conceptualizing the curation process, appraising objects, ingesting objects from external sources, creating collaborative infrastructure, re-organizing collaborative infrastructure and welcoming newcomers. Many of the norms and rules that were identified help regulate the activities in Wikidata.
Research limitations/implications
This study mapped Wikidata activities to curation and ontology frameworks. Results from this study provided implications for academic studies on online peer-curation work.
Practical implications
An understanding of the activities in Wikidata will help inform communities wishing to contribute data to or reuse data from Wikidata, as well as inform the design of other similar online peer-curation communities, scientific research institutional repositories, digital archives and libraries.
Originality/value
Wikidata is one of the largest knowledge curation projects on the web. The data from this project are used by other Wikimedia projects such as Wikipedia, as well as major search engines. This study explores an aspect of Wikidata WikiProject editors to the author’s knowledge has yet to be researched.}
}
@article{ZHANG2024102609,
title = {Knowledge graph and function block based Digital Twin modeling for robotic machining of large-scale components},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {85},
pages = {102609},
year = {2024},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102609},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000844},
author = {Xuexin Zhang and Lianyu Zheng and Wei Fan and Wei Ji and Lingjun Mao and Lihui Wang},
keywords = {Robotic machining, Large-scale components, Digital Twin, Knowledge graph, Function block, Process planning and execution},
abstract = {Robotic machining is a potential method for machining large-scale components (LSCs) due to its low cost and high flexibility. However, the low stiffness of robots and complex machining process of LSCs result in a lack of alignment between the physical process and digital models, making it difficult to realize the robotic machining of LSCs. The recent Digital Twin (DT) concept shows potential in terms of representing and modeling physical processes. Therefore, this study proposes a robotic machining DT for LSCs. However, the current DT is not capable of knowledge representation, multi-source data integration, optimization algorithm implementation, and real-time control. To address these issues, Knowledge Graph (KG) and Function Block (FB) are employed in the proposed robotic machining DT. Here, robotic machining related information, such as the machining parameters and errors, is represented in the virtual space by building the KG, whereas the FBs are responsible for integrating and applying the algorithms for process execution and optimization based on real-world events. Moreover, a novel adaptive process adjustment strategy is proposed to improve the efficiency of the process execution. Finally, a prototype system of the robotic machining DT is developed and validated by an experiment on robotic milling of the assembly interface for an LSC. The results demonstrate that the robotic machining is successfully optimized and improved by the proposed method.}
}
@article{TROUMPOUKIS2024505,
title = {European AI and EO convergence via a novel community-driven framework for data-intensive innovation},
journal = {Future Generation Computer Systems},
volume = {160},
pages = {505-521},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24003133},
author = {Antonis Troumpoukis and Iraklis Klampanos and Despina-Athanasia Pantazi and Mohanad Albughdadi and Vasileios Baousis and Omar Barrilero and Alexandra Bojor and Pedro Branco and Lorenzo Bruzzone and Andreina Chietera and Philippe Fournand and Richard Hall and Michele Lazzarini and Adrian Luna and Alexandros Nousias and Christos Perentis and George Petrakis and Dharmen Punjani and David Röbl and George Stamoulis and Eleni Tsalapati and Indrė Urbanavičiūtė and Giulio Weikmann and Xenia Ziouvelou and Marcin Ziółkowski and Manolis Koubarakis and Vangelis Karkaletsis},
keywords = {Artificial Intelligence, Earth observation, DIAS, Applications, Case-study, Methodology, Platform},
abstract = {Artificial Intelligence (AI) represents a collection of tools and methodologies that have the potential to revolutionise various aspects of human activity. Earth observation (EO) data, including satellite and in-situ, are essential in a number of high impact applications, ranging from security and energy to agriculture and health. In this paper, we present the AI4Copernicus framework for bridging the two domains within the European context to enable data-centred innovation. In order to achieve this goal, AI4Copernicus has developed and enriches the European AI-on-demand platform with a number of application bootstrapping services and tools to accelerate uptake and innovation, whilst it provides integration over AI-on-Demand services and the Copernicus ecosystem, targeting the highly successful Data and Information Access Service (DIAS) Cloud platforms. More specifically, by employing procedures for onboarding and validating models and tools, and by utilising a host of meticulously reviewed and supervised open calls-enabled projects, and containerisation best-practices, AI4Copernicus deployed and made available several products on DIAS platforms. Moreover, these products and resources have been made available on the AI-on-Demand platform catalogue for discovery, use and further development. The AI4Copernicus framework is being used by a number of business-driven projects and SMEs spanning several application domains. This article provides an overview of the European AI and EO context as well as the AI4Copernicus technological framework and tools offered. Further, we present real world use-cases as well as a community-centred evaluation of our framework based on usage and feedback received from several projects.}
}
@article{AJANOVIC2021100310,
title = {How interdisciplinarity helps knowledge production: Reflections on a doctoral dissertation},
journal = {Journal of Hospitality, Leisure, Sport & Tourism Education},
volume = {28},
pages = {100310},
year = {2021},
issn = {1473-8376},
doi = {https://doi.org/10.1016/j.jhlste.2021.100310},
url = {https://www.sciencedirect.com/science/article/pii/S1473837621000113},
author = {Edina Ajanovic and Beykan Çizel},
keywords = {Tourism epistemology, Interdisciplinarity, PhD thesis, Persuasive communication, Online review platforms, Hermeneutical approach},
abstract = {For the last several decades, there has been an ongoing discussion regarding disciplinary approaches towards the dissemination of theoretical/methodological frameworks and results that will enrich knowledge in the tourism field. However, there is still a lack of researchers' explanations and reflections regarding the benefits of involving into an interdisciplinary approach, which can lead to new forms of knowledge creation. Through this reflective study, the goal is to understand the process of writing and “being” the interdisciplinary PhD thesis writer in tourism, with interpretations referring to the elements of interdisciplinarity proposed in already existing tourism literature. By presenting the research process and results of the PhD thesis on persuasive communication occurring on online travel review platforms, the aim is to provide not only an interpretation of epistemology, but also of the ontology of conceptualizing and conducting tourism research in an interdisciplinary spirit and how this can stimulate further research on certain topic.}
}
@incollection{CORCORAN2023328,
title = {Attitudes toward inclusion},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {328-333},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.12067-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305120676},
author = {Tim Corcoran},
keywords = {Attitudes, Inclusion, Disability, Psychology},
abstract = {When examining things like attitudes, it is common to accept such psychological matter as empirically accessible things belonging to individuals. Further, as existing psychological constructs, attitudes are often located inside the heads of people. Knowledge of an individual's attitudes usually depend on survey or interview responses regulated by the structure and language of the questions asked. This chapter considers ways in which attitudes toward inclusion have been understood in education research. Two distinct challenges present for future work in this area: (i) categorical challenges at the special/inclusive education nexus, and (ii) onto-epistemological options between different kinds of psychology.}
}
@article{BUCHS2020106553,
title = {Can social ecological economics of water reinforce the “big tent”?},
journal = {Ecological Economics},
volume = {169},
pages = {106553},
year = {2020},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2019.106553},
url = {https://www.sciencedirect.com/science/article/pii/S0921800919302095},
author = {Arnaud Buchs and Olivier Petit and Philippe Roman},
keywords = {Water, Social ecological economics, “Big tent”, Content analysis, Methodological pluralism},
abstract = {This paper seeks to characterize the importance of the social and political dimensions of the literature dedicated to water in the field of ecological economics. It attempts to assess the relevance of Spash's division of the community into three “camps”, namely “new resource economists”, “social ecological economists” and “new environmental pragmatists” through the literature focusing on water issues published in leading scientific journals. We begin with an analysis of the main ontological, epistemological and methodological tenets of the three “camps”. We then analyze the relevance and limits of such categorization for water research through papers published in Ecological Economics. We then explore the field of ecological economics of water through textual statistics obtained from research abstracts published in five selected journals since the late 1980s. Our results raise questions regarding the relevance of the partition of the ecological economics community thanks to a Venn diagram that presents limited overlaps. We promote an inclusive representation of the “big tent” of ecological economics, thus suggesting new perspectives for the debate on methodological pluralism in Ecological Economics. To conclude, a series of recommendations are suggested to promote water social ecological economics, and strengthen pluralism within the community.}
}
@article{TSOPRA201824,
title = {Using preference learning for detecting inconsistencies in clinical practice guidelines: Methods and application to antibiotherapy},
journal = {Artificial Intelligence in Medicine},
volume = {89},
pages = {24-33},
year = {2018},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2018.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0933365718300873},
author = {Rosy Tsopra and Jean-Baptiste Lamy and Karima Sedki},
keywords = {Preference learning, Antibiotherapy, Clinical practice guidelines, Inconsistencies in guidelines},
abstract = {Clinical practice guidelines provide evidence-based recommendations. However, many problems are reported, such as contradictions and inconsistencies. For example, guidelines recommend sulfamethoxazole/trimethoprim in child sinusitis, but they also state that there is a high bacteria resistance in this context. In this paper, we propose a method for the semi-automatic detection of inconsistencies in guidelines using preference learning, and we apply this method to antibiotherapy in primary care. The preference model was learned from the recommendations and from a knowledge base describing the domain. We successfully built a generic model suitable for all infectious diseases and patient profiles. This model includes both preferences and necessary features. It allowed the detection of 106 candidate inconsistencies which were analyzed by a medical expert. 55 inconsistencies were validated. We showed that therapeutic strategies of guidelines in antibiotherapy can be formalized by a preference model. In conclusion, we proposed an original approach, based on preferences, for modeling clinical guidelines. This model could be used in future clinical decision support systems for helping physicians to prescribe antibiotics.}
}
@article{HUANG2023101887,
title = {A smart conflict resolution model using multi-layer knowledge graph for conceptual design},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101887},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101887},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000150},
author = {Zechuan Huang and Xin Guo and Ying Liu and Wu Zhao and Kai Zhang},
keywords = {Knowledge graph, Conflict resolution, Conceptual design, Inventive principle, FBS},
abstract = {Reducing the impact of conflicts on requirement-function-structure mapping in the early stage of product design is an important measure to achieve conceptual innovation, which relies on accurate reasoning of multi-domain knowledge. As product requirements become more personalized and diverse, traditional discrete knowledge organization and reasoning methods are difficult to adapt to the challenges of continuity and precision in conceptual solution. Knowledge graphs with complex networks have obvious advantages in association detection, knowledge visualization, and explainable reasoning of implicit knowledge, which offer innovative opportunities for conflict resolution in conceptual design. Therefore, a smart conflict resolution model using a multi-layer Knowledge Graph for Conceptual Design(mKGCD) is proposed in this study. A knowledge expression form of FBS-oriented design patent vocabulary is proposed, which is used for knowledge entity recognition and relation extraction based on natural language processing. A label mapping method based on inventive principles is used for patent classification and a four-layer semantic network for conflict resolution is constructed. Through semantic distance calculation, the designer's requirements for function/behavior/structure are smart deployed to obtain appropriate knowledge. A case study of the conceptual design of a collapsible installation and handling equipment demonstrates the feasibility of the proposed approach. The proposed method can not only meet the functional solution and innovation in the context of different design requirements, but also effectively improve the design efficiency in the iterative design process by means of multiple meanings of one graph.}
}
@article{FUMAGALLI2022102040,
title = {Conceptual model visual simulation and the inductive learning of missing domain constraints},
journal = {Data & Knowledge Engineering},
volume = {140},
pages = {102040},
year = {2022},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2022.102040},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X2200043X},
author = {Mattia Fumagalli and Tiago Prince Sales and Fernanda Araujo Baião and Giancarlo Guizzardi},
keywords = {Conceptual modeling, Constraints learning, Model validation, Inductive learning, Inductive Logic Programming, Model simulation},
abstract = {Conceptual modeling plays a fundamental role in information systems engineering, and in data and systems interoperability. To play their role as instruments for domain modeling, conceptual models must contain the exact set of constraints that represent the worldview of the relevant domain stakeholders. However, as empirical results show, conceptual modelers are subject to cognitive limitations and biases and, hence, in practice, they systematically produce models that fall short in that respect. Moreover, automating the process of formally assessing conceptual models in this sense (i.e., model validation) is notoriously hard, mainly because the intended worldview at hand lies in the mind of these stakeholders. In this paper, we provide a novel approach to model validation and automated constraint learning that combines, on one hand, Model Finding via the visual simulation of that model’s valid instances and, on the other hand, Inductive Logic Programming techniques. In our approach, we properly channel the results produced by the application of a visual model finding technique as input to a learning process. We then show how the approach is able to support the modeler in identifying missing constraints from the original model. The approach is validated against a catalog of empirically-elicited conceptual modeling anti-patterns. As we show here, the approach is able to support the automated learning of constraints that are needed to rectify a number of relevant anti-patterns in this catalog.}
}
@article{ZHENG2021101258,
title = {Knowledge-based integrated product design framework towards sustainable low-carbon manufacturing},
journal = {Advanced Engineering Informatics},
volume = {48},
pages = {101258},
year = {2021},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2021.101258},
url = {https://www.sciencedirect.com/science/article/pii/S1474034621000136},
author = {Hao Zheng and Shang Yang and Shanhe Lou and Yicong Gao and Yixiong Feng},
keywords = {Low-carbon product design, Multi-objective optimization, Design knowledge representation, Low-carbon manufacturing},
abstract = {With a global challenge on the serious ecological problems, low-carbon manufacturing aiming to reduce carbon emission and resource consumption is gaining the ever-increasing attention. Due to the significant impact on the product lifecycle, low-carbon product design is considered as an effective and attractive approach to improve the eco-market trade-off of electromechanical products. Existing low-carbon product design approaches focus on solving specific low-carbon problems, and how to explore and navigate the integrative design space considering low-carbon and knowledge in a holistic perspective is rarely discussed. In response, this paper proposes a knowledge-based integrated product design framework to support low-carbon product development. An ontology-based knowledge modelling approach is put forward to represent the multidisciplinary design knowledge to facilitate knowledge sharing and integration. Subsequently, a function–structure synthesis approach based on case-based reasoning is presented to narrow down the design space to generate suitable design solutions for achieving desired functions. A multi-objective mathematical model is established, and the multi-objective particle swarm optimization is adopted to solve the low-carbon product optimization. Furthermore, a decision-making ranking approach based on the closeness degree is employed to prioritize the potential solutions from Pareto set. Finally, a case study of low-carbon product design of hydraulic machine is demonstrated to show the effectiveness.}
}
@article{WANG2025112377,
title = {Identifying falling-from-height hazards in building information models: From the perspectives of time and location},
journal = {Journal of Building Engineering},
volume = {104},
pages = {112377},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112377},
url = {https://www.sciencedirect.com/science/article/pii/S235271022500614X},
author = {Qiankun Wang and Chuxiong Shen and Zeng Guo and Chao Tang},
keywords = {Falling-from-Height, Building information modeling, Voxelization, Temporal information, Spatial information},
abstract = {Falling-from-Height (FFH) is the leading hazard among the 'Fatal Five' in the architectural/engineering/construction (AEC) industry, often resulting in severe consequences. Therefore, preventing these accidents is critical at the earliest stage. However, current FFH hazard identification methods suffer from a high workload, computational intensity, and lack of comprehensiveness. We propose a voxelization-based method to identify the temporal and spatial information on FFH hazards in Building Information Modeling (BIM). This method considers the falling, accessibility, and temporal conditions. We compared this method with a traditional FFH hazard identification method. The results indicate that the proposed method can rapidly identify all potential FFH hazard information, with a recall of 100 % and a precision of 81.5 %. It is 30 times faster than the offset geometry method and is not affected by the curvature of model surfaces, improving the effectiveness of FFH hazard identification. The proposed method is highly suitable for preventing FFH accidents and can be implemented during the design and construction preparation stages, increasing the effectiveness of FFH accident control.}
}
@article{DURIAKOVA2025111757,
title = {A decentralised architecture for secure exchange of assets in data spaces: The case of SEDIMARK},
journal = {Data in Brief},
volume = {61},
pages = {111757},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2025.111757},
url = {https://www.sciencedirect.com/science/article/pii/S2352340925004846},
author = {Erika Duriakova and Diarmuid O’Reilly-Morgan and Maroua Bahri and Maxime Costalonga and Gabriel Danciu and Septimiu Nechifor and Stefan-Cristian Jarcau and Tarek Elsaleh and Peipei Wu and Franck Le Gall and Grigorios Koutantos and Panagiotis Vlacheas and Luis Sánchez and Juan Ramón Santana and Pablo Sotres and Elias Tragos},
keywords = {Decentralised data spaces, System architecture, Marketplace, Data quality, Artificial intelligence},
abstract = {The European Union's (EU) data strategy aims to create a single market for seamless data flow while ensuring proper governance, privacy, and data protection. In this paper, we present SEDIMARK, an EU project, that builds on this strategy by developing a fully decentralised, secure data marketplace. The goal of SEDIMARK is to build a complete toolbox that enables users to purchase and process data assets. The toolbox includes tools for data cleaning, decentralised machine learning models and secure data exchange. SEDIMARK offers users full control over data assets by enabling them to keep their data locally and thus removing the need for central servers. With customisable pipelines and tools, SEDIMARK supports a wide range of users, from novices to experts, promoting seamless collaboration and fair access to high-quality datasets across Europe. The decentralised connectivity in SEDIMARK is achieved with the use of Distributed Ledger Technology (DLT). Furthermore, SEDIMARK’s architecture features a unique Connector component using Self Sovereign Identities (SSI), fostering trust and secure interactions. Transactions in SEDIMARK are stored in a Registry, a decentralised, immutable, non-repudiable and permissionless database. Together the technologies used in SEDIMARK ensure privacy, trust and data quality for secure management, sharing, and monetisation of assets in data spaces.}
}
@article{YUAN2025103161,
title = {A novel user scenario and behavior sequence recognition approach based on vision-context fusion architecture},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103161},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103161},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000540},
author = {Wenyu Yuan and Danni Chang and Chenlu Mao and Luyao Wang and Ke Ren and Ting Han},
keywords = {Scenario recognition, Human behavior sequence prediction, Smart home context, Feature fusion, Multi-modal data},
abstract = {Understanding user scenario and behavior is essential for the development of human-centered intelligent service systems. However, the presence of cluttered objects, uncertain human behaviors, and overlapping timelines in daily life scenarios complicates the problem of scenario understanding. This paper aims to address the challenges of identifying and predicting user scenario and behavior sequences through a multimodal data fusion approach, focusing on the integration of visual and environmental data to capture subtle scenario and behavioral features. For the purpose, a novel Vision-Context Fusion Scenario Recognition (VCFSR) approach was proposed, encompassing three stages. First, four categories of context data related to home scenarios were acquired: physical context, time context, user context, and inferred context. Second, scenarios were represented as multidimensional data relationships through modeling technologies. Third, a scenario recognition model was developed, comprising context feature processing, visual feature handling, and multimodal feature fusion. For illustration, a smart home environment was built, and twenty-six participants were recruited to perform various home activities. Integral sensors were used to collect environmental context data, and video data was captured simultaneously, both of which jointly form a multimodal dataset. Results demonstrated that the VCFSR model achieved an average accuracy of 98.1 %, outperforming traditional machine learning models such as decision trees and support vector machines. This method was then employed for fine-grained human behavior sequence prediction tasks, showing good performance in predicting behavior sequences across all scenarios constructed in this study. Furthermore, the results of ablation experiments revealed that the multimodal feature fusion method increased the average accuracy by at least 1.8 % compared to single-modality data-driven methods. This novel approach to user behavior modeling simultaneously handles the relationship threads across scenarios and the rich details provided by visual data, paving the way for advanced intelligent services in complex interactive environments such as smart homes and hospitals.}
}
@article{KUCKERTZ2024101064,
title = {DataDesc: A framework for creating and sharing technical metadata for research software interfaces},
journal = {Patterns},
volume = {5},
number = {11},
pages = {101064},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101064},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002149},
author = {Patrick Kuckertz and Jan Göpfert and Oliver Karras and David Neuroth and Julian Schönau and Rodrigo Pueblas and Stephan Ferenz and Felix Engel and Noah Pflugradt and Jann M. Weinand and Astrid Nieße and Sören Auer and Detlef Stolten},
keywords = {research data management, RDM, FAIR, software metadata, metadata schema, interface description, semantic software description, software publication, software reuse, machine actionable, software documentation},
abstract = {Summary
The reuse of research software is central to research efficiency and academic exchange. The application of software enables researchers to reproduce, validate, and expand upon study findings. The analysis of open-source code aids in the comprehension, comparison, and integration of approaches. Often, however, no further use occurs because relevant software cannot be found or is incompatible with existing research processes. This results in repetitive software development, which impedes the advancement of individual researchers and entire research communities. In this article, the DataDesc (Data Description) framework is presented—an approach to describing data models of software interfaces with machine-actionable metadata. In addition to a specialized metadata schema, an exchange format and support tools for easy collection and the automated publishing of software documentation are introduced. This approach practically increases the FAIRness, i.e., findability, accessibility, interoperability, and reusability, of research software as well as effectively promotes its impact on research.}
}
@article{BANKS201964,
title = {The common player-avatar interaction scale (cPAX): Expansion and cross-language validation},
journal = {International Journal of Human-Computer Studies},
volume = {129},
pages = {64-73},
year = {2019},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2019.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1071581918304610},
author = {Jaime Banks and Nicholas David Bowman and Jih-Hsuan Tammy Lin and Daniel Pietschmann and Joe A. Wasserman},
abstract = {The connection between player and avatar is understood to be central to the experience and effects of massively multiplayer online (MMO) gaming experiences, and these connections emerge from the interplays of both social and ludic characteristics. The comprehensive social/ludic measure of this player-avatar interaction (PAX), however, features some dimensions with theoretical/operational gaps and limited reliability, and is available only in English (despite evidence of potential cultural variations in player-avatar relations). The present study aimed to a) enhance and refine the PAX metric, and b) translate and validate a common metric that bridges English, German, and traditional Chinese languages to facilitate future comparative research. Through exploratory factor analysis of data from MMO players in each of these language-based populations, an improved 15-item common Player Avatar Interaction (cPAX) scale is presented, with four dimensions: relational closeness, anthropomorphic autonomy, critical concern, and sense of control. The metric is shown to be reliable within and across populations, and construct validity tests show expected associations between scale dimensions and both player-avatar relationship types and senses of human-like relatedness.}
}
@article{BHAWNANI2025114434,
title = {Development of an enhanced Heart Attack Diagnosis Model using Knowledge Distillation and Frequent Sequence Pattern Mining},
journal = {Knowledge-Based Systems},
pages = {114434},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.114434},
url = {https://www.sciencedirect.com/science/article/pii/S095070512501473X},
author = {Dinesh Kumar Bhawnani and Sunita Soni and Arpana Rawal},
keywords = {Deep learning, Attention Based Densely Connected Capsule Model, Depthwise Separable Convolutional Neural Network, Improved Coot Optimization Algorithm, PREFIXSPAN, Frequent Sequence Pattern Mining},
abstract = {Deep learning (DL) algorithms have displayed their effectiveness in predicting sequence modelling compared to various systems. Nevertheless, some limitations of existing methods are the demand for enormous databases, computational expense, and the risk of overfitting. To address these problems, this study proposes a novel DL technique using knowledge distillation and sequence illness pattern recognition from medical databases. Firstly, the input data is pre-processed using the data cleaning method. The size of the sequence dataset and the duration of the sequential patterns are both considered during the process of using PREFIXSPAN to manage long sequential patterns. In the proposed strategy, a lightweight student network is employed to train a strong teacher network, which is produced by a Knowledge Distillation framework. A teacher network is assessed by the Attention Based Densely Connected Capsule Model (Attention-DC). An efficient, low-weight Depthwise Separable Convolutional Neural Network (DSCNN) model is then chosen as the student network. This study uses three datasets to solve enormous database issues. The KD helps prevent the student model from overfitting to noise or specific patterns in the training data. The Improved Coot Optimization Algorithm (ICOA) is applied to adjust the parameter. The hyperparameters used to optimize the performance of the proposed model are Epochs (300), learning rate (0.001), and batch size (32), respectively. The experiments use the resources of three different datasets, and Python is employed to analyze the results. The proposed technique achieves accuracy of 99.512%, 99.329% and 99.351% for the heart disease, cardiovascular disease, and Diabetes dataset.}
}
@article{ZHANG2022426,
title = {RKD-VNE: Virtual network embedding algorithm assisted by resource knowledge description and deep reinforcement learning in IIoT scenario},
journal = {Future Generation Computer Systems},
volume = {135},
pages = {426-437},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X2200173X},
author = {Peiying Zhang and Peng Gan and Neeraj Kumar and Ching-Hsien Hsu and Shigen Shen and Shibao Li},
keywords = {Industrial Internet of Things, Virtual network embedding, Social attribute perception, Virtual network security, Resource knowledge description, Deep reinforcement learning},
abstract = {In the era of Industry 4.0, the Industrial Internet of Things (IIoT) is developing rapidly, various IIoT applications pose new challenges to the existing network architecture. On the one hand, these applications put forward higher requirements for the efficient use of network resources. On the other hand, these applications generate massive amounts of information, and they pursue a more secure network environment. Therefore, in order to ensure security while effectively allocating network resources, this paper puts forward a virtual network embedding (VNE) algorithm assisted by resource knowledge description (RKD) and deep reinforcement learning (DRL). First, we use social attribute perception to measure the security of each physical node and regard it as one of the attributes of the physical node. Then, RKD is used to standardize resource constraints before the virtual network is embedded. Finally, the DRL agent derives the probability of the physical node being embedded according to the physical network attributes, and embeds the virtual node according to the probability. Simulation experiments show that compared with the BaseLine algorithm, the RKD-VNE algorithm proposed in this paper has obvious advantages in the general performance of VNE, especially in terms of long-term revenue consumption rate increased by 24.3%.}
}
@article{BOGE2021101,
title = {Quantum reality: A pragmaticized neo-Kantian approach},
journal = {Studies in History and Philosophy of Science Part A},
volume = {87},
pages = {101-113},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2021.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0039368121000431},
author = {Florian J. Boge},
keywords = {Symmetries, Kantian philosophy of science, Quantum theory, Decoherence},
abstract = {Despite remarkable efforts, it remains notoriously difficult to equip quantum theory with a coherent ontology. Hence, Healey (2017, 12) has recently suggested that “quantum theory has no physical ontology and states no facts about physical objects or events”, and Fuchs et al. (2014, 752) similarly hold that “quantum mechanics itself does not deal directly with the objective world”. While intriguing, these positions either raise the question of how talk of ‘physical reality’ can even remain meaningful, or they must ultimately embrace a hidden variables-view, in tension with their original project. I here offer a neo-Kantian alternative. In particular, I will show how constitutive elements in the sense of Reichenbach (1920) and Friedman (1999, 2001) can be identified within quantum theory, through considerations of symmetries that allow the constitution of a ‘quantum reality’, without invoking any notion of a radically mind-independent reality. The resulting conception will inherit elements from pragmatist and ‘QBist’ approaches, but also differ from them in crucial respects. Furthermore, going beyond the Friedmanian program, I will show how non-fundamental and approximate symmetries can be relevant for identifying constitutive principles.}
}
@article{RAJAN2025100043,
title = {Network pharmacology and in-silico approaches to elucidate the antimicrobial mechanism of Aesculus assamica Griff. for the treatment of skin infection},
journal = {In Silico Research in Biomedicine},
volume = {1},
pages = {100043},
year = {2025},
issn = {3050-7871},
doi = {https://doi.org/10.1016/j.insi.2025.100043},
url = {https://www.sciencedirect.com/science/article/pii/S3050787125000423},
author = {Ravi Kumar Rajan and Farak Ali and Abdul Baquee Ahmed},
keywords = {Aesculus assamica Griff., Network pharmacology, In-silico, STAT3, mTOR, 1-HTC (1-hentriacontanol), T-B (theasapogenol B), Oleanolic acid (OA)},
abstract = {The genus Aesculus, recognized for its therapeutic applications in traditional medicine, includes Aesculus assamica Griff. (AA), an underexplored species native to the eastern Himalayas. Traditionally, AA has been used for its anti-inflammatory, antifungal, and cytotoxic properties. Its stem and bark are also used for fish poisoning, likely due to the presence of saponins and rotenones. Despite its ethnomedicinal relevance, AA remains poorly studied with limited scientific documentation. This study investigates the therapeutic potential of AA in treating bacterial skin infections—a growing public health concern in India. An integrative in silico approach was employed, incorporating network pharmacology, molecular docking, ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiling, DFT (Density Functional Theory) calculations, and antibacterial activity prediction using antiBac-Pred to identify potential anti-infective leads from AA. Network pharmacology analysis identified nine overlapping targets associated with skin infections, with STAT3, mTOR, PTGS2, and TLR4 emerging as central nodes in the protein–protein interaction network. KEGG (Kyoto Encyclopedia of Genes and Genomes) and GO (Gene Ontology) enrichment analyses suggested a key role for the HIF-1 signaling pathway, particularly through STAT3 and mTOR. Molecular docking revealed strong binding affinities of compounds assamicin-I & II (ASS-I and ASS-II) toward STAT3 (−46.7 and −47.2 kcal/mol), while 1-hentriacontanol (1-HTC) showed high affinity for mTOR (−63.04 kcal/mol), indicating potential to modulate host immune responses. DFT calculations confirmed the electronic stability and reactivity of 1-HTC and the triterpenoid compounds. ADMET analysis demonstrated favorable pharmacokinetic profiles for most triterpenoids and long-chain alcohols. Notably, 1-HTC, β-sitosterol (β-s), and oleanolic acid (OA) exhibited good oral bioavailability and drug-likeness, whereas ASS-I and ASS-II, despite minor rule violations, appear suitable for topical or non-oral applications. Most phytoconstituents also showed low metabolic liability and minimal central nervous system penetration, supporting their potential use in treating peripheral infections such as skin diseases. Among the compounds, 1-HTC emerged as the most promising lead for broad-spectrum and anaerobic bacterial inhibition, with stearic acid identified as a backup candidate. Additionally, theasapogenol B (T-B) and oleanolic acid (OA) may be valuable in selective or combination therapies targeting pathogens like Staphylococcus lugdunensis.}
}
@article{LENG2025528,
title = {Review of manufacturing system design in the interplay of Industry 4.0 and Industry 5.0 (Part II): Design processes and enablers},
journal = {Journal of Manufacturing Systems},
volume = {79},
pages = {528-562},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525000366},
author = {Jiewu Leng and Jiwei Guo and Junxing Xie and Xueliang Zhou and Ang Liu and Xi Gu and Dimitris Mourtzis and Qinglin Qi and Qiang Liu and Weiming Shen and Lihui Wang},
keywords = {Manufacturing system design, Production system design, Smart manufacturing, Industry 5.0, Design methods},
abstract = {Following up on our previous review paper ‘Review of manufacturing system design in the interplay of Industry 4.0 and Industry 5.0 (Part I): Design thinking and modeling methods’ [1], based on the proposed Thinking-Modelling-Process-Enabler (TMPE) framework of Manufacturing System Design (MSD), this paper (Part II of the two-part review) further reviews the Process and Enabler dimensions of MSD in the interplay of Industry 4.0 and Industry 5.0. MSD methods are reviewed from the single-dimensional design process and cross-dimensional design process perspectives, respectively. MSD methods are reorganized and categorized from the key enabler's perspective. Finally, challenges are discussed along with directions for future research in the domain of MSD. This review is anticipated to offer novel insights for advancing MSD research and engineering in the interplay of Industry 4.0 and Industry 5.0.}
}
@article{BRUNSON2020112730,
title = {Behind the measures of maternal and reproductive health: Ethnographic accounts of inventory and intervention},
journal = {Social Science & Medicine},
volume = {254},
pages = {112730},
year = {2020},
note = {Behind the Measures of Maternal and Reproductive Health: Ethnographic Accounts of Inventory and Intervention},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2019.112730},
url = {https://www.sciencedirect.com/science/article/pii/S0277953619307257},
author = {Jan Brunson and Siri Suh},
keywords = {Maternal health, Reproductive health, Metrics, Indicators, Global health, Reproductive governance, Ethnography},
abstract = {Ontologies of intervention in global health involve a voracious appetite for data - collection of data as evidence of what is intervention is needed, the establishment of metrics to organize and make sense of that data, further surveillance and measures to determine whether interventions were successful and targets were met, and, increasingly, predictions that determine whether interventions will provide good returns on investments. This part-special issue, an ethnographic interrogation of contemporary metrics and ontologies of intervention enacted in the global South, investigates “behind the measures” of maternal and reproductive health: the imperfect but pragmatic processes of quantification, inventory, and recording; how metrics are imbued with meaning, morality, and power; and how targets and indicators shape or drive individual and institutional behavior, as well as policy and program creation.}
}
@article{VOGLER20232647,
title = {Brokering between tenants for an international materials acceleration platform},
journal = {Matter},
volume = {6},
number = {9},
pages = {2647-2665},
year = {2023},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2023.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S2590238523003739},
author = {Monika Vogler and Jonas Busk and Hamidreza Hajiyani and Peter Bjørn Jørgensen and Nehzat Safaei and Ivano E. Castelli and Francisco Fernando Ramirez and Johan Carlsson and Giovanni Pizzi and Simon Clark and Felix Hanke and Arghya Bhowmik and Helge S. Stein},
keywords = {cooperative research, fault tolerant, autonomous experiments, multilocation, multimodal, materials acceleration platform},
abstract = {Summary
The efficient utilization of resources in accelerated materials science necessitates flexible, reconfigurable software-defined research workflows. We demonstrate a brokering approach to modular and asynchronous research orchestration to integrate multiple laboratories in a cooperative multitenancy platform across disciplines and modalities. To the best of our knowledge, this constitutes the first internationally distributed materials acceleration platform (MAP) linked via a passive brokering server, which is demonstrated through a battery electrolyte workflow capable of determining density, viscosity, ionic conductivity, heat capacity, diffusion coefficients, transference numbers, and radial distribution functions that ran in five countries over the course of 2 weeks. We discuss the lessons learned from multitenancy and fault tolerance and chart a way to a universal battery MAP with fully ontology-linked schemas and cost-aware orchestration.}
}
@article{SREEKALA2025178591,
title = {Genomic and biochemical investigations in the biomineralizing potential of an isolated marine ureolytic Bacillus sp. N₉},
journal = {Science of The Total Environment},
volume = {964},
pages = {178591},
year = {2025},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2025.178591},
url = {https://www.sciencedirect.com/science/article/pii/S0048969725002256},
author = {Aparna Ganapathy Vilasam Sreekala and Suma Mohan Saraswathy and Vinod Kumar Nathan and Kiran Babu Uppuluri},
keywords = {Biomineralization, Soil stabilization, Genomic assembly, Gene function annotation, Urease gene cascade},
abstract = {Microbially Induced Calcium Carbonate Precipitation (MICP) plays a significant role in coastal soil stabilization and erosion prevention. In the present study, the biomineralizing potential of a newly isolated Bacillus sp. N₉ was investigated through MICP. The isolated Bacillus sp. N₉ induced calcium carbonate (CaCO3) precipitation in broth, agar, and real beach rock was evaluated by X-ray Fluorescence, Scanning Electron Microscopy, and X-ray Diffraction. The visual appearance of calcites as white crystals on the rock and the sealing of cracks in the beach sand cubes confirmed the CaCO3 precipitation by the isolated strain. The pH changes, soluble calcium (Ca2+) concentration levels and urease activity during the growth of the isolated Bacillus sp. N₉ were studied. High urease activity was observed, resulting in an increased production of carbonate (CO32−) ions, which in turn promoted a higher rate of CaCO₃ precipitation. Further, the assembly of the 3.27 Mb genome of the isolated Bacillus sp. N₉ was evaluated using Nanopore sequencing technology, and various extracellular proteins, including urease, were identified. The gene annotation through PROKKA and RAST predicted 6700 and 7317 protein-coding sequences. The pathway annotation analysis through gene ontology, KEGG, and COG inferred the presence of genes in proteolytic characteristics, carbohydrate metabolism, and amino acid derivatives. The present study provides valuable insights into the isolated native Bacillus sp. N₉, demonstrating its potential to produce high urease activity and calcite precipitation through combined genomics and in vitro techniques.}
}
@incollection{DENECKE2021521,
title = {Biomedical Standards and Open Health Data},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {521-531},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11527-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383115272},
author = {Kerstin Denecke},
keywords = {Artificial intelligence, Biomedical standards, FAIR, Health Level 7, Ontologies, Open data, Open health data, Semantic interoperability, Semantics, SNOMED CT, Terminologies},
abstract = {Recent movements such as open data, crowdhealth, participatory surveillance, quantified self, or individuals becoming “data donors” have the potential to generate new findings about diseases, to improve diagnostics and deliver healthcare and treatment. This chapter provides insights into the topic of open data concerning healthcare. Examples will be given on open health data that are available. Analysis of open health data can help to identify causes of diseases, effects of treatments, or even correlations between environmental factors and disease progression. However, there are also major concerns on privacy, confidentiality and control of data about individuals once data become open. These concerns will be highlighted. Ensuring usefulness of open health data requires high data quality, enrichment with metadata and the development of standards and best practices to guide the way data are presented and organized. This ensures that data are not only accessible, but also readable, comprehensible and utilizable by various users and algorithms. Biomedical standards, classification systems, and ontologies as well as communication protocols for health data have already been available for clinical practice for years and should be considered when opening health data. We will provide an overview on relevant biomedical standards to guide toward standardized representations in the context of open health data.}
}
@incollection{NEUSTEIN2022233,
title = {Chapter 9 - Conceptual spaces and scientific data models},
editor = {Amy Neustein and Nathaniel Christen},
booktitle = {Innovative Data Integration and Conceptual Space Modeling for COVID, Cancer, and Cardiac Care},
publisher = {Academic Press},
pages = {233-269},
year = {2022},
isbn = {978-0-323-85197-8},
doi = {https://doi.org/10.1016/B978-0-32-385197-8.00016-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323851978000167},
author = {Amy Neustein and Nathaniel Christen},
keywords = {conceptual space theory, syntagmatic graph, role semantics, Quantum , verb-centric grammar, dimensional analysis, emergent syntax, instantiation},
abstract = {This chapter will more substantially develop our approach to Conceptual Space Theory in natural (as well as programming) language contexts, that was initiated in earlier chapters. We present further philosophical motivations for the structural details of our proposed “Syntagmatic Graph” representations and examine techniques for integrating conceptual spaces with linguistic paradigms such as Conceptual Role Semantics and situational semantics. Our central argument is that the classical linguistic concept of “thematic roles” provides an alternative framework for analyzing the semantic integration of multiple conceptual spaces, contrasted with “quantitative blend” models endemic to conceptual space theory proper. Therefore we propose “role-indexed” Conceptual Space models, which have distinct semantic and syntactic patterns, juxtaposing this theory to existing formalizations of conceptual spaces in (for example) Quantum NLP. With that natural-language foundation as a motivation, we then consider semantic models as they could be more concretely applied to scientific data sets.}
}
@article{HOARE202296,
title = {A novel cell line panel reveals non-genetic mediators of platinum resistance and phenotypic diversity in high grade serous ovarian cancer},
journal = {Gynecologic Oncology},
volume = {167},
number = {1},
pages = {96-106},
year = {2022},
issn = {0090-8258},
doi = {https://doi.org/10.1016/j.ygyno.2022.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S009082582200511X},
author = {J.I. Hoare and H. Hockings and J. Saxena and V.L. Silva and M.J. Haughey and G.E. Wood and F. Nicolini and H. Mirza and I.A. McNeish and W. Huang and E. Maniati and T.A. Graham and M. Lockley},
keywords = {High grade serous ovarian cancer, Drug resistance, Platinum, Chemotherapy, Evolution, Transcription, Gene expression, Non-genetic, Extracellular matrix},
abstract = {Objectives
Resistance to cancer therapy is an enduring challenge and accurate and reliable preclinical models are lacking. We interrogated this unmet need using high grade serous ovarian cancer (HGSC) as a disease model.
Methods
We created five in vitro and two in vivo platinum-resistant HGSC models and characterised the entire cell panel via whole genome sequencing, RNASeq and creation of intraperitoneal models.
Results
Mutational signature analysis indicated that platinum-resistant cell lines evolved from a pre-existing ancestral clone but a unifying mutational cause for drug resistance was not identified. However, cisplatin-resistant and carboplatin-resistant cells evolved recurrent changes in gene expression that significantly overlapped with independent samples obtained from multiple patients with relapsed HGSC. Gene Ontology Biological Pathways (GOBP) related to the tumour microenvironment, particularly the extracellular matrix, were repeatedly enriched in cisplatin-resistant cells, carboplatin-resistant cells and also in human resistant/refractory samples. The majority of significantly over-represented GOBP however, evolved uniquely in either cisplatin- or carboplatin-resistant cell lines resulting in diverse intraperitoneal behaviours that reflect different clinical manifestations of relapsed human HGSC.
Conclusions
Our clinically relevant and usable models reveal a key role for non-genetic factors in the evolution of chemotherapy resistance. Biological pathways relevant to the extracellular matrix were repeatedly expressed by resistant cancer cells in multiple settings. This suggests that recurrent gene expression changes provide a fitness advantage during platinum therapy and also that cancer cell-intrinsic mechanisms influence the tumour microenvironment during the evolution of drug resistance. Candidate genes and pathways identified here could reveal therapeutic opportunities in platinum-resistant HGSC.}
}
@article{WANG2023430,
title = {Ferrostatin-1 Inhibits Toll-Like Receptor 4/NF-κB Signaling to Alleviate Intervertebral Disc Degeneration in Rats},
journal = {The American Journal of Pathology},
volume = {193},
number = {4},
pages = {430-441},
year = {2023},
issn = {0002-9440},
doi = {https://doi.org/10.1016/j.ajpath.2022.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S0002944023000317},
author = {Ying-Guang Wang and Xiao-Jun Yu and Yun-Kun Qu and Rui Lu and Meng-Wei Li and Hao-Ran Xu and Shan-Xi Wang and Xin-Zhen Guo and Hao Kang and Hongbo You and Yong Xu},
abstract = {Ferrostatin-1 (Fer-1), an inhibitor of ferroptosis, is implicated in intervertebral disc degeneration (IDD). The current study explored the role of Fer-1 in IDD via the toll-like receptor 4 (TLR4)/NF-κB signaling pathway. IDD-related gene expression microarray GSE124272 and high-throughput sequencing data set GSE175710 were obtained through the Gene Expression Omnibus database. Differentially expressed genes in IDD were identified, followed by implementation of protein-protein interaction network analysis and receiver operating characteristic curve analysis. The main pathways in IDD were obtained through Gene Ontology and Kyoto Encyclopedia of Genes and Genomes functional analyses, and target genes of Fer-1 were obtained through PubChem and PharmMapper websites. Finally, GPX4, FTH, and TLR4 expression was determined in a IDD rat model. Three key co-expression modules involved in IDD were obtained through Weighted Gene Co-Expression Network Analysis. Thirteen differentially expressed genes were found to be associated with IDD, and eight key genes (TLR4, BCL2A1, CXCL1, IL1R1, NAMPT, SOCS3, XCL1, and IRAK3) were found to affect IDD. These eight key genes had the diagnostic potential for IDD. The NF-κB signaling pathway was shown to play a predominant role in IDD development. Network pharmacologic analysis indicated a role of Fer-1 in suppressing ferroptosis and ameliorating IDD via the TLR4/NF-κB signaling pathway, which was verified by an in vivo animal experiment. The study showed that Fer-1 down-regulates TLR4 to inactivate NF-κB signaling pathway, suppressing ferroptosis and ultimately alleviating IDD in rats}
}
@article{MANDOLINI201931,
title = {A standard data model for life cycle analysis of industrial products: A support for eco-design initiatives},
journal = {Computers in Industry},
volume = {109},
pages = {31-44},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518307449},
author = {Marco Mandolini and Marco Marconi and Marta Rossi and Claudio Favi and Michele Germani},
keywords = {Inter-operability, Data-exchange, Product life cycle, Environmental analysis, Eco-design, Life cycle inventory},
abstract = {The eco-design of industrial products is a complex task that requires a high level of expertise in environmental science and a very large amount of data about the product under development. Product data for eco-design are not limited to geometrical and technical aspects; they also include information related to the product life cycle. The present paper aims to define a life cycle standard data model (LCSDM) that manages and shares life cycle information along the product development process. The LCSDM is defined as a common and structured framework for data collection in comparative evaluations. The need of a “standard” data model emerges in the context of life cycle assessment (LCA), mainly due to the subjectivity related to the life cycle inventory phase. The standard structure of the LCSDM facilitates the interoperability of eco-design software tools by creating a common framework for the implementation of eco-design initiatives inside product manufacturing companies. The LCSDM is a data structure that is able to represent the relationships among parts and assemblies. Each part or assembly is defined by a set of nodes that characterize the life cycle phases (e.g., Material, Manufacturing, Use, End-of-life, and Transport). A list of attributes is identified according to the environmental features that describe the product life cycle. The LCSDM structure is implemented in an encoding document for data sharing through a generic software language (e.g., XML – eXtensible Markup Language). The implementation of the proposed LCSDM in the design department of a manufacturing company using an eco-design software platform leads to the following benefits: (i) the fulfilment of the LCSDM (XML file) along the product development process, (ii) the use of a unique standard for data sharing among the several eco-design software tools, and (iii) the creation of a robust framework for life cycle assessment. The main drawback of the proposed LCSDM is related to the initial effort required to set up the design software platform (which consists of both standard and eco-design tools) to be able to read, fill, store and share the LCSDM.}
}
@article{EVANS20241509,
title = {Developments and applications of the OPTIMADE API for materials discovery, design, and data exchange††Electronic supplementary information (ESI) available: Copy of Table 1 with web links. See DOI: https://doi.org/10.1039/d4dd00039k},
journal = {Digital Discovery},
volume = {3},
number = {8},
pages = {1509-1533},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d4dd00039k},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24001219},
author = {Matthew L. Evans and Johan Bergsma and Andrius Merkys and Casper W. Andersen and Oskar B. Andersson and Daniel Beltrán and Evgeny Blokhin and Tara M. Boland and Rubén {Castañeda Balderas} and Kamal Choudhary and Alberto {Díaz Díaz} and Rodrigo {Domínguez García} and Hagen Eckert and Kristjan Eimre and María Elena {Fuentes Montero} and Adam M. Krajewski and Jens Jørgen Mortensen and José Manuel {Nápoles Duarte} and Jacob Pietryga and Ji Qi and Felipe de Jesús {Trejo Carrillo} and Antanas Vaitkus and Jusong Yu and Adam Zettel and Pedro Baptista {de Castro} and Johan Carlsson and Tiago F. T. Cerqueira and Simon Divilov and Hamidreza Hajiyani and Felix Hanke and Kevin Jose and Corey Oses and Janosh Riebesell and Jonathan Schmidt and Donald Winston and Christen Xie and Xiaoyu Yang and Sara Bonella and Silvana Botti and Stefano Curtarolo and Claudia Draxl and Luis Edmundo {Fuentes Cobas} and Adam Hospital and Zi-Kui Liu and Miguel A. L. Marques and Nicola Marzari and Andrew J. Morris and Shyue Ping Ong and Modesto Orozco and Kristin A. Persson and Kristian S. Thygesen and Chris Wolverton and Markus Scheidgen and Cormac Toher and Gareth J. Conduit and Giovanni Pizzi and Saulius Gražulis and Gian-Marco Rignanese and Rickard Armiento},
abstract = {The Open Databases Integration for Materials Design (OPTIMADE) application programming interface (API) empowers users with holistic access to a growing federation of databases, enhancing the accessibility and discoverability of materials and chemical data. Since the first release of the OPTIMADE specification (v1.0), the API has undergone significant development, leading to the v1.2 release, and has underpinned multiple scientific studies. In this work, we highlight the latest features of the API format, accompanying software tools, and provide an update on the implementation of OPTIMADE in contributing materials databases. We end by providing several use cases that demonstrate the utility of the OPTIMADE API in materials research that continue to drive its ongoing development.}
}
@incollection{COUTO2019870,
title = {Semantic Similarity Definition},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {870-876},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20401-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338204019},
author = {Francisco M. Couto and Andre Lamurias},
keywords = {Biomedical ontologies, Biomedical vocabularies, Functional analysis, Information content, Information theory, Knowledge organization systems, Semantic similarity, Semantic web, Similarity measures},
abstract = {In bioinformatics, semantic similarity has been used to compare different types of biomedical entities, such as proteins, compounds and phenotypes, based on their biological role instead on what they look like. This manuscript presents a definition of semantic similarity between biomedical entities described by a common semantic base (e.g., ontology) following an information-theoretic perspective of semantic similarity. It defines the amount of information content two entries share in a semantic base, and, by extension, how to compare biomedical entities represented outside the semantic base but linked through a set of annotations. Software to check how semantic similarity works in practice is available at: https://github.com/lasigeBioTM/DiShIn/.}
}
@article{SATO2025104905,
title = {Using complexity theory to understand teaching: Re-framing perspectives from preservice teachers},
journal = {Teaching and Teacher Education},
volume = {156},
pages = {104905},
year = {2025},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2024.104905},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X24004384},
author = {Mistilina Sato and Ting Ma and Jane Abbiss},
abstract = {This paper is an exploratory analysis of preservice teachers' understandings of teaching as complex activity through the explicit use of complexity theory. Interviews with 38 primary and secondary preservice teachers enrolled in a one-year, graduate-level, university-based teacher education programme in Aotearoa New Zealand indicate limitations in the conceptual language these preservice teachers have for naming and understanding this complexity in teaching. Drawing on the concepts within complexity theory, we recast preservice teachers’ descriptions of teaching, offering new language for describing the emergent dynamics of teaching in relation to its non-linear and networked activity, working between predictability and uncertainty, and the necessity of adaptation.}
}
@article{WANG20241000,
title = {Unveiling the efficacy and mechanism of Danggui-Shaoyao-San in treating nephrotic syndrome: A meta-analysis and network pharmacology study},
journal = {South African Journal of Botany},
volume = {174},
pages = {1000-1016},
year = {2024},
issn = {0254-6299},
doi = {https://doi.org/10.1016/j.sajb.2024.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0254629924004125},
author = {Heyong Wang and Lanyue Xiong and Jun Wang and Shaobo Wu and Yang Chen and Dongyan Lan and Dianxing Yang},
keywords = {Danggui-Shaoyao-San, Nephrotic syndrome, Meta-analysis, Network pharmacology, Randomized controlled trial},
abstract = {Aim of the study
The purpose of this study was to investigate the efficacy and mechanism of DSS in treating nephrotic syndrome through meta-analysis and network pharmacology methods.
Materials and methods
To conduct a comprehensive search for randomized controlled trials on the efficacy of Danggui Shaoyao San (DSS) in the treatment of nephrotic syndrome, we searched across eight electronic databases from their establishment to May 1, 2023. The Cochrane Collaboration's Risk of Bias 2 tool was used to evaluate the quality of evidence regarding bias risk and outcomes. Statistical analysis was performed using RevMan software (version 5.4). Furthermore, network pharmacology was employed to validate the underlying mechanism.
Results
This study included 14 articles that involved 1256 patients with nephrotic syndrome. The clinical efficacy of DSS against nephrotic syndrome was significantly higher than that of Western medical treatment alone. In comparison with the control groups, which were administered Western medicines alone, the use of DSS significantly increased plasma albumin levels (mean difference [MD] = 4.32, 95 % confidence interval [CI] [2.37, 6.24], p < 0.00001) and reduced 24 h urinary protein excretion (MD = −0.92, 95 % CI [−1.11, −0.73], p < 0.00001), total cholesterol levels (MD = −1.23, 95 % CI [−1.76, −0.70], p < 0.00001), triglyceride levels (MD = −0.37, 95 % CI [−0.54, −0.20], p < 0.00001). Furthermore, the combination of DSS with Western medicines achieved better control of adverse reactions in comparison with the control groups (relative risk = 0.37, 95 % CI [0.29, 0.49], p < 0.00001). Network pharmacology analysis identified 39 important active compounds and 18 core target genes involved in the treatment of primary nephrotic syndrome by DSS. Gene Ontology enrichment analysis identified 2126 biological processes, 64 cellular components, and 115 molecular functions, while Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis identified 149 signaling pathways. Key active compounds include ginsenoside Rg5, ginsenoside Rg1, schisandrin A, berberine, gallic acid, quercetin, and β-sitosterol, while potential core target genes include IL6, AKT1, TNF, VEGFA, IL1B, and TP53. KEGG pathway analysis indicated that DSS is involved in multiple mechanisms, such as neural development, synaptic plasticity, programmed cell death, inflammation, and immunity.
Conclusions
DSS has higher efficacy and safety in the treatment of nephrotic syndrome and acts on nephrotic syndrome via mechanisms that involve multiple targets, components, and pathways.}
}
@article{GUAN2024100070,
title = {Drug discovery and development in the era of artificial intelligence: From machine learning to large language models},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {1},
pages = {100070},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100070},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000289},
author = {Shenghui Guan and Guanyu Wang},
keywords = {Machine learning, Drug Discovery, Bioinformatics},
abstract = {Drug Research and Development (R&D) is a complex and difficult process, and current drug R&D faces the challenges of long time span, high investment, and high failure rate. Machine learning, with its powerful learning ability to characterize big data and complex networks, is increasingly effective to improve the efficiency and success rate of drug R&D. Here we review some recent examples of the application of machine learning methods in six areas: disease gene prediction, virtual screening, drug molecule generation, molecular attribute prediction, and prediction of drug combination synergism. We also discuss the advantages of integrative learning in multi-attribute prediction. Integrative models based on base learners constructed from data of different dimensions on the one hand fully utilize the information contained in these data, and on the other hand improve the average prediction performance. Finally, we envision a new paradigm for drug discovery and development: a large language model acts as a central hub to organize public resources into a knowledge base, validating the knowledge with computational software and smaller predictive models, as well as high-throughput automated screening platforms based on organoidal technologies, to speed up development and reduce the differences in efficacy between disease models and humans to improve the success rate of a drug.}
}
@article{BEHR20245699,
title = {Generating knowledge graphs through text mining of catalysis research related literature††Electronic supplementary information (ESI) available: https://github.com/AleSteB/CatalysisIE_Knowledge_Graph_Generator. See DOI: https://doi.org/10.1039/d4cy00369a},
journal = {Catalysis Science & Technology},
volume = {14},
number = {19},
pages = {5699-5713},
year = {2024},
issn = {2044-4753},
doi = {https://doi.org/10.1039/d4cy00369a},
url = {https://www.sciencedirect.com/science/article/pii/S2044475324004696},
author = {Alexander S. Behr and Diana Chernenko and Dominik Koßmann and Arjun Neyyathala and Schirin Hanf and Stephan A. Schunk and Norbert Kockmann},
abstract = {Structured research data management in catalysis is crucial, especially for large amounts of data, and should be guided by FAIR principles for easy access and compatibility of data. Ontologies help to organize knowledge in a structured and FAIR way. The increasing numbers of scientific publications call for automated methods to preselect and access the desired knowledge while minimizing the effort to search for relevant publications. While ontology learning can be used to create structured knowledge graphs, named entity recognition allows detection and categorization of important information in text. This work combines ontology learning and named entity recognition for automated extraction of key data from publications and organization of the implicit knowledge in a machine- and user-readable knowledge graph and data. CatalysisIE is a pre-trained model for such information extraction for catalysis research. This model is used and extended in this work based on a new data set, increasing the precision and recall of the model with regard to the data set. Validation of the presented workflow is presented on two datasets regarding catalysis research. Preformulated SPARQL-queries are provided to show the usability and applicability of the resulting knowledge graph for researchers.}
}
@article{MAO2024101413,
title = {Research on the joint event extraction method orientates food live e-commerce},
journal = {Electronic Commerce Research and Applications},
volume = {66},
pages = {101413},
year = {2024},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2024.101413},
url = {https://www.sciencedirect.com/science/article/pii/S1567422324000589},
author = {DianHui Mao and YiMing Liu and RuiXuan Li and JunHua Chen and YuanRong Hao and JianWei Wu},
keywords = {Event Extraction, Ontology construction, Knowledge Graph, Food e-commerce live streaming},
abstract = {In the evolving landscape of food e-commerce live streaming, the profusion of textual data, marked by an excess of promotional vernacular and unstructured formats, presents a formidable challenge for event extraction. Addressing these hurdles, we introduce a tailored ontology-based method alongside FMLEE (Food Marketing Live Event Extraction), a joint event extraction algorithm. This approach simplifies the event identification process through meticulous segmentation and the development of an ontology comprising 5 event categories and 19 argument roles. By integrating context-aware embeddings derived from pre-trained language models and applying an adversarial learning tactic, our methodology not only bolsters the robustness of our model but also significantly refines its accuracy in discerning relevant events within the scarce-resource milieu of food live streaming promotions. The effectiveness of the FMLEE model is validated by its achievement of an F1 score of 73.05%, with the inclusion of adversarial learning contributing to a 2.61% enhancement in performance. This evidences our novel contribution to the domain, offering robust technical support for the optimal exploitation of information within the sphere of food live streaming promotions. Simultaneously, this aids in the investigation of innovative applications for consumer engagement within marketing strategies and the smart regulation of marketing activities.}
}
@article{LI20243832,
title = {Automated compliance checking for BIM models based on Chinese-NLP and knowledge graph: an integrative conceptual framework},
journal = {Engineering, Construction and Architectural Management},
volume = {32},
number = {6},
pages = {3832-3856},
year = {2024},
issn = {0969-9988},
doi = {https://doi.org/10.1108/ECAM-10-2023-1037},
url = {https://www.sciencedirect.com/science/article/pii/S0969998824000833},
author = {Sihao Li and Jiali Wang and Zhao Xu},
keywords = {Building information modeling, Knowledge graph, Natural language processing, Deep learning, Automatic checking},
abstract = {Purpose
The compliance checking of Building Information Modeling (BIM) models is crucial throughout the lifecycle of construction. The increasing amount and complexity of information carried by BIM models have made compliance checking more challenging, and manual methods are prone to errors. Therefore, this study aims to propose an integrative conceptual framework for automated compliance checking of BIM models, allowing for the identification of errors within BIM models.
Design/methodology/approach
This study first analyzed the typical building standards in the field of architecture and fire protection, and then the ontology of these elements is developed. Based on this, a building standard corpus is built, and deep learning models are trained to automatically label the building standard texts. The Neo4j is utilized for knowledge graph construction and storage, and a data extraction method based on the Dynamo is designed to obtain checking data files. After that, a matching algorithm is devised to express the logical rules of knowledge graph triples, resulting in automated compliance checking for BIM models.
Findings
Case validation results showed that this theoretical framework can achieve the automatic construction of domain knowledge graphs and automatic checking of BIM model compliance. Compared with traditional methods, this method has a higher degree of automation and portability.
Originality/value
This study introduces knowledge graphs and natural language processing technology into the field of BIM model checking and completes the automated process of constructing domain knowledge graphs and checking BIM model data. The validation of its functionality and usability through two case studies on a self-developed BIM checking platform.}
}
@article{NGUYEN2025102420,
title = {O47: Fine-tuned large language models on clinical notes and structured ICD-10 codes facilitate making genetic test decisions for rare diseases},
journal = {Genetics in Medicine Open},
volume = {3},
pages = {102420},
year = {2025},
note = {2025 ACMG Annual Clinical Genetics Meeting},
issn = {2949-7744},
doi = {https://doi.org/10.1016/j.gimo.2025.102420},
url = {https://www.sciencedirect.com/science/article/pii/S2949774425004595},
author = {Quan Nguyen and Cong Liu and Ian Campbell and Chunhua Weng and Kai Wang}
}
@article{SUN2025103227,
title = {The role of natural language processing in improving cancer care: A scoping review with narrative synthesis},
journal = {Artificial Intelligence in Medicine},
volume = {168},
pages = {103227},
year = {2025},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2025.103227},
url = {https://www.sciencedirect.com/science/article/pii/S0933365725001629},
author = {Mengxuan Sun and Ehud Reiter and Lisa Duncan and Rosalind Adam},
keywords = {Natural language processing, Cancer care, Patient education, Summarise report, Record keeping, Evidence-based decision making},
abstract = {Objectives
To review studies of Natural Language Processing (NLP) systems that assist in cancer care, explore use cases and summarise current research progress.
Methods
A scoping review, searching six databases (1) MEDLINE, (2) Embase, (3) IEEE Xplore, (4) ACM Digital Library, (5) Web of Science, and (6) ACL Anthology. Studies were included that reported NLP systems that had been used to improve cancer management by patients or clinicians. Studies were synthesised descriptively and using content analysis.
Results
Twenty-nine studies were included. Studies mainly applied NLP in mixed cancer types (n = 10, 34.48 %) and breast cancer (n = 8, 27.59 %). NLP was used in four main ways: (1) to support patient education and self-management; (2) to improve efficiency in clinical care by summarising, extracting, and categorising data, and supporting record-keeping; (3) to support prevention and early detection of patient problems or cancer recurrence; and (4) to improve cancer treatment by supporting clinicians to make evidence-based treatment decisions. Studies highlighted a wide variety of use cases for NLP technologies in cancer care. However, few technologies have been evaluated within clinical settings, none have been evaluated against clinical outcomes, and none have been implemented into clinical care.
Conclusion
NLP has the potential to improve cancer care via several mechanisms, including information extraction and classification, which could enable automation and personalization of care processes. Additionally, NLP tools such as chatbots show promise in improving patient communication and support. However, there are deficiencies in the evaluation and clinical integration challenges. Interdisciplinary collaboration between computer scientists and clinicians will be essential if NLP technologies are to fulfil their potential to improve patient experience and outcomes. Registered Protocol: https://doi.org/10.17605/OSF.IO/G9DSR}
}
@article{CHOWDHURY2025129906,
title = {Handling language prior and compositional reasoning issues in Visual Question Answering system},
journal = {Neurocomputing},
volume = {635},
pages = {129906},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129906},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225005788},
author = {Souvik Chowdhury and Badal Soni},
keywords = {Visual Question Answering, Large language model, In-context learning, Computer vision, Natural language processing, Generative artificial intelligence},
abstract = {Visual Question Answering (VQA) models often suffer from language bias, favoring common but incorrect answers, and struggle with compositional reasoning in complex queries. This paper proposes a unified approach using a multimodal large language model enhanced with adaptive prompts designed for specific tasks. Our method directly addresses these issues by reducing language bias and improving compositional reasoning. Extensive evaluations on benchmark datasets, including VQA v2.0, VQACP, TDIUC, GQA, Visual7 W, TextVQA, and STVQA show that our approach outperforms state-of-the-art models, achieving accuracy improvements of 8% to 9%. These results demonstrate the effectiveness of our method in enhancing VQA accuracy, making it a significant advancement for more reliable and robust applications in real-world scenarios.}
}
@article{ZHOU2025103142,
title = {Augmenting general-purpose large-language models with domain-specific multimodal knowledge graph for question-answering in construction project management},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103142},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103142},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000357},
author = {Shenghua Zhou and Keyan Liu and Dezhi Li and Chun Fu and Yan Ning and Wenying Ji and Xuefan Liu and Bo Xiao and Ran Wei},
keywords = {GLM, Construction Project Management, QA, Multimodal Knowledge Graph},
abstract = {Current studies on Question-Answering of Construction Project Management (CPM-QA) face challenges, including the small-scale CPM-related knowledge repositories, the limited effectiveness of QA methods using grammar rules or tiny machine-learning models, and the shortage of testing sets for comparing QA performance. Hence, this research augments general-purpose large-language models (GLMs) with the multimodal CPM knowledge graph (CPM-KG) for CPM-QA. It encompasses (i) building the multimodal CPM-KG covering 36 CPM subfields, (ii) combining CPM-KG and GLMs through three stages, (iii) developing a 2435-question CPM-QA testing set, and (iv) assessing and comparing CPM-QA accuracies for eight pairs of original and CPM-KG-augmented GLMs. The results demonstrate that CPM-KG-augmented GLMs’ CPM-QA accuracy rate is 30.0 % superior to original GLMs on average, and top-performing CPM-KG-augmented GLMs (e.g., ERNIE-Bot 4.0) pass CRCEEs. Within 36 CPM subfields, CPM-QA accuracy enhancements resulting from CPM-KG are between 12.2 % and 57.8 %. Furthermore, CPM-KG leads to CPM-QA accuracy enhancements of 19.6 % for single-answer, 48.0 % for multiple-answer, 30.6 % for text-only, and 20.4 % for image-embedded questions. The multimodal CPM-KG also outperforms the text-only single-modal CPM-KG in enhancing CPM-QA performance. This work contributes to unveiling the significance of CPM-specific knowledge in augmenting GLMs, sharing a reusable multimodal CPM-KG-formatted knowledge repository, and delivering a testing set of CPM-QA.}
}
@article{XHANI2025200547,
title = {The Role and Applications of Semantic Interoperability Tools and eXplainable AI in the Development of Smart Food Systems: Findings from a Systematic Literature Review},
journal = {Intelligent Systems with Applications},
volume = {27},
pages = {200547},
year = {2025},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2025.200547},
url = {https://www.sciencedirect.com/science/article/pii/S2667305325000730},
author = {Donika Xhani and Gayane Sedrakyan and Anand Gavai and Renata Guizzardi and Jos {van Hillegersberg}},
keywords = {Semantic interoperability, Explainable AI, Ontology, Recommender system, Food},
abstract = {Smart food systems generate vast and diverse data across the supply chain, yet inconsistent data structures and limited interoperability hinder their full potential. Achieving semantic interoperability, where systems can exchange and interpret data with shared meaning, is essential for enabling intelligent integration and decision-making. Tools such as ontologies, knowledge graphs, and reasoning engines play a key role in this process. In this paper, we refer to these as Semantic Interoperability (SI) tools: a broad category that includes technologies grounded in Semantic Web standards (e.g., RDF, OWL, SPARQL) but emphasizes their applied role in aligning meaning across heterogeneous systems. Coupled with eXplainable Artificial Intelligence (XAI), these technologies enhance transparency and trust in AI-driven decisions, such as personalized food recommendations tailored to an individual’s health conditions and preferences. This paper presents a Systematic Literature Review (SLR) examining the role of semantic interoperability tools and XAI in the development of smart food systems. Through an analysis of 39 studies, the review identifies key semantic technologies and XAI methods used in food systems, with a focus on their application in intelligent food recommendation systems. The findings reveal that while significant progress has been made, current systems often lack adequate transparency and personalization, limiting user trust and engagement. To address these gaps, the paper proposes the integration of semantic interoperability tools with XAI to create smarter, more reliable food systems. As part of this effort, the paper introduces the conceptual model for the Semantic Explainable Food Recommendation Ontology (SEFRO), a work-in-progress ontology, designed to connect entities and relationships within food systems in an intelligent manner, with the goal of enabling personalized, explainable, and interoperable food recommendations that meet the growing demands for smart food systems.}
}
@article{VAZRALA2025107325,
title = {RBTM: A Hybrid gradient Regression-Based transformer model for biomedical question answering},
journal = {Biomedical Signal Processing and Control},
volume = {102},
pages = {107325},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.107325},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424013831},
author = {Suneetha Vazrala and Thayyaba {Khatoon Mohammed}},
keywords = {Biomedical QA, Semantic similarity, Concept modelling, Deep learning, SNOMED-CT, Concept2Vec, Transformer model, Hybrid Gradient Regression-Based Transformer Model (RBTM)},
abstract = {Background
The biomedical field faces substantial challenges in addressing health-related queries due to the vast and complex body of literature available. Traditional keyword-based search methods and current Question Answering (QA) systems are often inadequate in capturing the nuanced semantic relationships within biomedical texts, which hampers the delivery of accurate and relevant answers.
Aim
This research aims to develop an advanced QA system capable of capturing complex conceptual relationships within biomedical literature to improve accuracy and relevance in responses.
Methods
A Hybrid Gradient Regression-Based Transformer Model (RBTM) that integrates semantic similarity quantification with deep learning is proposed. The methodology includes three main phases: component identification, semantic similarity measurement at component and sentence levels, and similarity scoring. The model uses the LemmaChase Lemmatizer for feature extraction, SNOMED-CT ontology for domain-specific concept identification, and the concept2Vec approach to generate enhanced vector representations of input phrases. RBTM, combining XGBoost with transformer architecture, calculates similarity scores that guide answer selection.
Results
The proposed RBTM model was evaluated on the MedQuAD dataset, achieving high performance with a notable accuracy of 99.09%, an R2 score of 97.07%, and an MSE of 0.00227. These results demonstrate RBTM’s robustness and its superiority over existing models in accurately identifying relevant biomedical answers.
Conclusion
The RBTM model represents a solution for biomedical QA by effectively addressing limitations in current systems, such as computational complexity and semantic inaccuracies. Future work will involve expanding the SNOMED-CT ontology and incorporating reinforcement learning to further enhance accuracy and applicability in biomedical information retrieval.}
}
@article{KOSOV20242292,
title = {Advancing XAI: new properties to broaden semantic-based explanations of black-box learning models},
journal = {Procedia Computer Science},
volume = {246},
pages = {2292-2301},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.560},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026085},
author = {Pavel Kosov and Nahla {El Kadhi} and Cecilia Zanni-Merk and Latafat Gardashova},
keywords = {XAI, Ontology, Expert Knowledge, Explainability},
abstract = {For a long time, experts in different areas where Artificial Intelligence (AI) is widely applied have been requesting more clarity for the decisions made by AI. DARPA came up with a new framework for eXplainable AI (XAI) where the system exploits an explainable model to provide explanations through the explanation interface to users based on the level of their expertise. Later, ontologies were integrated in various ways, which paved the way for clearer explanations. Provided enough Expert Knowledge, ontologies can be a potent tool in XAI. Based on the ideas of Bellucci et al., their explainable system provides comprehensive explanations based on ”visible” properties found in images by Machine Learning (ML) models and described via ontologies. However, we believe that any property, not only visible ones, can be used to explore the data. New ”explanatory” properties are proposed to be used for explanations. Our system exploits ML models and more user-oriented Expert Knowledge using a wider range of properties for objects to build a more profound XAI.}
}
@article{ELLOUZE2025100308,
title = {Management of psychological emergency cases on social media: A hybrid approach combining knowledge graphs and graph neural networks},
journal = {Online Social Networks and Media},
volume = {46},
pages = {100308},
year = {2025},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2025.100308},
url = {https://www.sciencedirect.com/science/article/pii/S2468696425000096},
author = {Mourad Ellouze and Sonda Rekik and Lamia {Hadrich Belguith}},
keywords = {Psychological emergency, Personality disorder, Social media, Natural language processing, Ontology, Graph neural networks},
abstract = {The effects of psychological crises are evolving at an astounding rate nowadays, presenting a significant challenge for everyone involved in tracking these disorders. Therefore, we propose in this paper a hybrid approach based on linguistic processing and numerical techniques allowing to: (i) identify the presence of psychological emergencies among social network users by analyzing their textual production, (ii) determine the specific type of emergency case, (iii) elaborate a graph for each type of emergency, reflecting the different dimensions linked to the psychological emergency, allowing for a better diagnosis of the situation and providing an overall view of the crisis type, (iv) combine the separate graphs for each emergency to address the various semantic aspects. The work was accomplished using advanced language model techniques, knowledge graphs and neural network graphs. The combination of these techniques ensures that their advantages are leveraged while overcoming their limitations in terms of result generalization. The evaluation of different parts related to detecting the presence of psychological problems, predicting specific type of emergency cases, and detecting links between knowledge graphs was measured using the F-measure metric. The values derived from this measure, corresponding to the evaluation of these three tasks, are, respectively, 83%, 87% and 80%. For the evaluation of the elaboration of each graph related to specific type of emergency cases, this was accomplished using qualitative metric standards. The results obtained can be considered encouraging given the significant scale of our approach.}
}
@article{BARB202088,
title = {Applications of Natural Language Techniques to Enhance Curricular Coherence},
journal = {Procedia Computer Science},
volume = {168},
pages = {88-96},
year = {2020},
note = {“Complex Adaptive Systems”Malvern, PennsylvaniaNovember 13-15, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.263},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920304026},
author = {Adrian S. Barb and Nil Kilicay-Ergin},
keywords = {Ontology, Text Mining, Data Mining, curricular coherence},
abstract = {An effective and highly efficient curriculum is the foundation of an optimal content delivery to students and may lead to the success of the academic program. Ensuring the effectiveness of curricular processes goes beyond the successful design of individual courses and it is subject to careful planning in accordance to appropriate policies and procedures that enable a high level of curriculum coherence. Such curriculum can guarantee that students are presented with a complete set of interrelated topics related to the area of study that can enable students to connect topics in individual courses and apply them in professional settings. The goal of this article is to evaluate the curriculum coherence of an Information Science program using ontologies and natural language processing techniques. Curricular coherence is evaluated using a possibilistic approach which evaluates academic gaps and overlaps that exist in the core courses offered in the degree. The use of ontologies enables us to perform a qualitative study of curricular coherence at different levels of ontological abstraction using mereotopological principles. The knowledge inferred from this process can be used at the universities to optimize their course offering and content for better academic content.}
}
@article{ABDULKAREEM2024108528,
title = {Fine-grained food image classification and recipe extraction using a customized deep neural network and NLP},
journal = {Computers in Biology and Medicine},
volume = {175},
pages = {108528},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108528},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524006127},
author = {Razia Sulthana {Abdul Kareem} and Timothy Tilford and Stoyan Stoyanov},
keywords = {Image classification, Ingredient identification, Recipe extraction, Deep neural networks, Domain ontology, Natural language processing},
abstract = {Global eating habits cause health issues leading people to mindful eating. This has directed attention to applying deep learning to food-related data. The proposed work develops a new framework integrating neural network and natural language processing for classification of food images and automated recipe extraction. It address the challenges of intra-class variability and inter-class similarity in food images that have received shallow attention in the literature. Firstly, a customized lightweight deep convolution neural network model, MResNet-50 for classifying food images is proposed. Secondly, automated ingredient processing and recipe extraction is done using natural language processing algorithms: Word2Vec and Transformers in conjunction. Thirdly, a representational semi-structured domain ontology is built to store the relationship between cuisine, food item, and ingredients. The accuracy of the proposed framework on the Food-101 and UECFOOD256 datasets is increased by 2.4% and 7.5%, respectively, outperforming existing models in literature such as DeepFood, CNN-Food, Wiser, and other pre-trained neural networks.}
}
@article{FENG2025109587,
title = {Cognitive Digital Twins of the natural environment: Framework and application},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109587},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109587},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624017457},
author = {Jun Feng and Hailin Tang and Siyuan Zhou and Yang Cai and Jianxin Zhang},
keywords = {Cognitive digital twin, Natural environment, Framework, Knowledge graph, Ontology},
abstract = {Digital Twin (DT) technology offers a method of creating digital models of natural systems to enhance their ability to withstand natural disasters. Currently, DT of the natural environment is in its initial phases, lacking adaptive capabilities and relying on human-assisted modeling. The key to endowing DT of the natural environment with greater autonomy lies in the integration of expert knowledge. Knowledge graphs can efficiently arrange and structurally store expert knowledge, thereby supporting the autonomous functionality of DT. This paper introduces the concept of Cognitive Digital Twin(CDT) derived from the industrial domain and presents a framework for CDT of the natural environment. This framework is centered around knowledge graph technology, aiming to provide more insights and guidance for system development. This framework integrates human cognition by constructing knowledge graphs of objects, models, events, and scene modes. Moreover, these knowledge graphs support agents for the dynamic adjustment of processes, as well as the adaptation and parameter optimization of related models. As a use case, we utilize this framework to implement digital twin watersheds. We develop appropriate ontologies and agents to facilitate the construction of cognitive digital watersheds for various regions. Cognitive digital watersheds effectively fulfill the application needs of integrated flood forecasting and control scheduling. This application validates the framework’s effectiveness and provides a reference for constructing CDTs of other natural systems.}
}
@article{CIMA2024104176,
title = {Controlled query evaluation in description logics through consistent query answering},
journal = {Artificial Intelligence},
volume = {334},
pages = {104176},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2024.104176},
url = {https://www.sciencedirect.com/science/article/pii/S0004370224001127},
author = {Gianluca Cima and Domenico Lembo and Riccardo Rosati and Domenico Fabio Savo},
keywords = {Description logics, Ontologies, Confidentiality preservation, Query answering, Data complexity},
abstract = {Controlled Query Evaluation (CQE) is a framework for the protection of confidential data, where a policy given in terms of logic formulae indicates which information must be kept private. Functions called censors filter query answering so that no answers are returned that may lead a user to infer data protected by the policy. The preferred censors, called optimal censors, are the ones that conceal only what is necessary, thus maximizing the returned answers. Typically, given a policy over a data or knowledge base, several optimal censors exist. Our research on CQE is based on the following intuition: confidential data are those that violate the logical assertions specifying the policy, and thus censoring them in query answering is similar to processing queries in the presence of inconsistent data as studied in Consistent Query Answering (CQA). In this paper, we investigate the relationship between CQE and CQA in the context of Description Logic ontologies. We borrow the idea from CQA that query answering is a form of skeptical reasoning that takes into account all possible optimal censors. This approach leads to a revised notion of CQE, which allows us to avoid making an arbitrary choice on the censor to be selected, as done by previous research on the topic. We then study the data complexity of query answering in our CQE framework, for conjunctive queries issued over ontologies specified in the popular Description Logics DL-LiteR and EL⊥. In our analysis, we consider some variants of the censor language, which is the language used by the censor to enforce the policy. Whereas the problem is in general intractable for simple censor languages, we show that for DL-LiteR ontologies it is first-order rewritable, and thus in AC0 in data complexity, for the most expressive censor language we propose.}
}
@article{QI2025104779,
title = {A knowledge graph-driven method for automated Geo-computations: Illustrated with soil erosion and soil potential productivity cases in China},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {143},
pages = {104779},
year = {2025},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2025.104779},
url = {https://www.sciencedirect.com/science/article/pii/S1569843225004261},
author = {Yanmin Qi and Yunqiang Zhu and Shu Wang and Ping Fu and Zhenji Gao and Stuart Marsh and Amin Farjudian},
keywords = {Knowledge graph, Automated system, Geographic computation, Geographic model, Soil erosion, Soil potential productivity},
abstract = {Geo-computation is a crucial process in geographic information science that selects geo-computational models and matches geographic data based on a geo-computational task for detecting, predicting, and simulating geographic entities, events, and phenomena. However, current geo-computations require expertise from users to effectively configure the models, data, and procedures for specialized tasks, which particularly poses challenges for users, especially novice users, as their attention is often drawn to technical details rather than computational analysis of the task. Therefore, we propose a systematic descriptive and procedural method driven by knowledge graphs to capture, organize, and process the essential features of components, relationships, and dynamic computational procedures in geo-computations, aiming to reduce manual involvement and assist in automating model selection and data matching. Then, an application prototype system is developed to implement automated geo-computations that are driven by knowledge graphs. Two application cases, namely, soil erosion and soil potential productivity, are computed to illustrate the accessibility of automated geo-computations supported by our proposed method. As demonstrated by the cases studied, the proposed knowledge graph-driven method improves the efficiency of model selection and configuration, enhances the value of open data, and advances integration of data and models for automated geo-computations.}
}
@article{XIA2025112476,
title = {Graph-based BIM generation method for integrated design of steel modular buildings},
journal = {Journal of Building Engineering},
volume = {106},
pages = {112476},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112476},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225007132},
author = {Yi Xia and Jiepeng Liu and Hongtuo Qi and Liqiang Wang and Jiachen Li and Liang Feng},
keywords = {Steel modular building, BIM, Graph model, Integrated design},
abstract = {Different from traditional building types, steel modular buildings are product-oriented. It is essential to conduct an integrated design process for this building type. Proposing a systematic framework to organize information and developing a unified model for representing steel modular buildings and multiple tasks are highly desired. In the current paper, a graph-based building information model (GBIM) generation method is proposed. Firstly, a progressive ontology development procedure is conducted to integrate domain knowledge and avoid conflicts. The defined ontology is instantiated through graph models, and corresponding modification and data query process are proposed for model adaptation. Based on the generated GBIMs, visualization, semantic enrichment and model transformation can be efficiently conducted for different design purposes. In addition, the proposed GBIM method is further utilized to organize information for project management and facilitate generative design of steel modular buildings.}
}
@article{MARINI2024103303,
title = {Multimodal representations of biomedical knowledge from limited training whole slide images and reports using deep learning},
journal = {Medical Image Analysis},
volume = {97},
pages = {103303},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103303},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524002287},
author = {Niccolò Marini and Stefano Marchesin and Marek Wodzinski and Alessandro Caputo and Damian Podareanu and Bryan Cardenas Guevara and Svetla Boytcheva and Simona Vatrano and Filippo Fraggetta and Francesco Ciompi and Gianmaria Silvello and Henning Müller and Manfredo Atzori},
keywords = {Computational pathology, Multimodal Learning, Medical ontology, Natural language processing, Colon cancer},
abstract = {The increasing availability of biomedical data creates valuable resources for developing new deep learning algorithms to support experts, especially in domains where collecting large volumes of annotated data is not trivial. Biomedical data include several modalities containing complementary information, such as medical images and reports: images are often large and encode low-level information, while reports include a summarized high-level description of the findings identified within data and often only concerning a small part of the image. However, only a few methods allow to effectively link the visual content of images with the textual content of reports, preventing medical specialists from properly benefitting from the recent opportunities offered by deep learning models. This paper introduces a multimodal architecture creating a robust biomedical data representation encoding fine-grained text representations within image embeddings. The architecture aims to tackle data scarcity (combining supervised and self-supervised learning) and to create multimodal biomedical ontologies. The architecture is trained on over 6,000 colon whole slide Images (WSI), paired with the corresponding report, collected from two digital pathology workflows. The evaluation of the multimodal architecture involves three tasks: WSI classification (on data from pathology workflow and from public repositories), multimodal data retrieval, and linking between textual and visual concepts. Noticeably, the latter two tasks are available by architectural design without further training, showing that the multimodal architecture that can be adopted as a backbone to solve peculiar tasks. The multimodal data representation outperforms the unimodal one on the classification of colon WSIs and allows to halve the data needed to reach accurate performance, reducing the computational power required and thus the carbon footprint. The combination of images and reports exploiting self-supervised algorithms allows to mine databases without needing new annotations provided by experts, extracting new information. In particular, the multimodal visual ontology, linking semantic concepts to images, may pave the way to advancements in medicine and biomedical analysis domains, not limited to histopathology.}
}
@article{LI2024111947,
title = {A semantic knowledge-based method for home service robot to grasp an object},
journal = {Knowledge-Based Systems},
volume = {297},
pages = {111947},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111947},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124005811},
author = {Cici Li and Guohui Tian and Mengyang Zhang},
keywords = {Home service robot, Grasp, Semantic knowledge, JSHOP2},
abstract = {To help the home service robot grasp an object with the constraints from the object itself, task and environment at the same time, we propose a knowledge-based method. Firstly, we construct five ontologies (object, task, environment, constraint and probabilistic ontology) to represent the five in a unified format. The remarkable things are that the object ontology is part-based and the constraints (grasp type, grasp location, approach direction, opening width, trajectory constraint and grasp force) are throughout the whole grasp process. To reason the constraints out with the findings, we propose a collaborative reasoning mechanism. It contains ontological reasoning, rule-based reasoning and probabilistic reasoning. Secondly, we propose to use SWRL (Semantic Web Rule Language) rules to label the object’s segments with functional parts. For its textual form, this method is fast. Thirdly, JSHOP2 planner is used to make the inferred constraints as the parameters of grasp subactions. Once the subaction list is output, the corresponding executable codes are called. Finally, we use Tiago robot to grasp different objects in different task and environment in our laboratory.}
}
@article{SHAMSHIRI2024105200,
title = {Text mining and natural language processing in construction},
journal = {Automation in Construction},
volume = {158},
pages = {105200},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.105200},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523004600},
author = {Alireza Shamshiri and Kyeong Rok Ryu and June Young Park},
keywords = {Text mining, Natural language processing, Machine learning, Computational linguistics, Language models, Construction, Project management},
abstract = {Text mining (TM) and natural language processing (NLP) have stirred interest within the construction field, as they offer enhanced capabilities for managing and analyzing text-based information. This highlights the need for a systematic review to identify the status quo, gaps, and future directions from the perspective of construction management. A review was conducted by aligning the objectives of 205 publications with the specific domains, areas, tasks, and processes outlined in construction management practices. This review reveals multiple facets of the construction sector empowered by TM/NLP approaches and highlights essential voids demanding consideration for automation possibilities and minimizing manual tasks. Ultimately, following identified obstacles, the review results indicate potential research opportunities: (1) strengthening overlooked construction aspects, (2) coupling diverse data formats, and (3) leveraging pre-trained language models and reinforcement learning. The findings will provide vital insights, fostering further progress in TM/NLP research and its applications in academia and industry.}
}
@article{XIONG2025103688,
title = {DR-RAG: Domain-Rule-based Retrieval-Augmented Generation for aviation digital model design},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103688},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103688},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005816},
author = {Xirui Xiong and Hongming Cai and Han Yu and Bingqing Shen and Pan Hu},
keywords = {Aviation design, Product design & manufacturing, Large language model, Knowledge graph, Retrieval-Augmented Generation},
abstract = {In the rapidly evolving landscape of aviation manufacturing, the increasing complexity of aircraft design demands advanced tools for knowledge integration and reasoning. This paper presents DR-RAG (Domain-Rule-based Retrieval-Augmented Generation), a novel framework that synergizes domain knowledge graphs, rule-based reasoning and digital twin technology to address design challenges. DR-RAG constructs a dynamic knowledge graph by extracting entities from multi-source data using a hybrid R2D-LLM approach. We create a rule base via fusion rules mining algorithms. These rules are integrated into a retrieval-augmented generation pipeline with a binary classifier guides rule selection and prompt templates enhance LLM reasoning. A digital twin model visualizes design outcomes and feeds simulation feedback into the knowledge graph, creating a closed-loop system for continuous improvement. Experimental results demonstrate that DR-RAG outperforms other methods in knowledge retrieval tasks related to aircraft design, providing an efficient and reliable solution for knowledge processing in this domain. This research not only offers a new decision-support tool for aviation manufacturing design but also contributes to the application of large language models in complex engineering fields.}
}
@article{SONG2025105706,
title = {Knowledge graph-based alarm management in petrochemical enterprises: A study on fusion and analysis of multi-source heterogeneous information},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {97},
pages = {105706},
year = {2025},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2025.105706},
url = {https://www.sciencedirect.com/science/article/pii/S0950423025001640},
author = {Xiaomiao Song and Fabo Yin and Dongfeng Zhao},
keywords = {Petrochemical enterprises, Alarm management, Knowledge graph, Ontology},
abstract = {In response to the increasing emphasis on alarm management in the petrochemical industry, there has been an explosive growth in relevant information. However, this information is often scattered across different systems and databases, stored in various forms such as documents, tables, and images, making it challenging to uniformly store, share, and utilize multi-source heterogeneous information. This commonly leads to the problem of “Information Islands.” In order to effectively leverage knowledge in the field of alarm management in the petrochemical industry and overcome the challenge of non-interoperable information, a method for fusing multi-source heterogeneous information in petrochemical enterprise alarm management based on knowledge graph is proposed. This method aims to standardize the management of alarm-related information and achieve information fusion. Initially, the approach utilizes data from petrochemical enterprises and publicly available data in the field of alarm management to establish both local and global ontologies. Subsequently, mapping algorithms are designed to achieve a more accurate construction of the hybrid ontology. Based on this foundation, a knowledge graph for alarm management in the petrochemical industry is established. Additionally, corresponding modules for information storage and retrieval are developed. Through the application demonstration using real alarm management information from a petrochemical enterprise, the results indicate that the proposed method for fusing multi-source heterogeneous information in petrochemical enterprise alarm management can effectively achieve information fusion.}
}
@article{YOUNG2025103805,
title = {Natural Language Processing to Extract Head and Neck Cancer Data From Unstructured Electronic Health Records},
journal = {Clinical Oncology},
volume = {41},
pages = {103805},
year = {2025},
issn = {0936-6555},
doi = {https://doi.org/10.1016/j.clon.2025.103805},
url = {https://www.sciencedirect.com/science/article/pii/S0936655525000603},
author = {T. Young and J. {Au Yeung} and K. Sambasivan and D. Adjogatse and A. Kong and I. Petkar and M. {Reis Ferreira} and M. Lei and A. King and J. Teo and T. {Guerrero Urbano}},
keywords = {Artificial intelligence, data mining, natural language processing, real-world data},
abstract = {Aims
Patient data is frequently stored as unstructured data within Electronic Health Records (EHRs), requiring manual curation. AI tools using Natural Language Processing (NLP) may rapidly curate accurate real-world unstructured EHRs to enrich datasets. We evaluated this approach for Head and Neck Cancer (HNC) patient data extraction using an open-source general-purpose healthcare NLP tool (CogStack).
Materials and Methods
CogStack was applied to extract relevant SNOMED-CT concepts from HNC patients' documents, generating outputs denoting the identifications of each concept for each patient. Outputs were compared to manually curated ground truth HNC datasets to calculate pre-training performance. Supervised model training was then performed using SNOMED-CT concept annotation on clinical documents, and the updated model was re-evaluated. A second training cycle was performed before the final evaluation. A thresholding approach (multiple detections needed to qualify a concept as ‘present’) was used to increase precision. The final model was evaluated on an unseen test cohort. F1 score (harmonic mean of precision and recall) was used for evaluation.
Results
Pre-training, the F1 score was incalculable for 19.5% of concepts due to insufficient recall. Following one training cycle, F1 score became calculable for all concepts (median 0.692). After further training, the final model demonstrated improvement in the median F1 score (0.708). Test cohort median F1 score was 0.750. Thresholding analysis developed a concept-specific best threshold approach, resulting in a median F1 score of 0.778 in the test cohort, where 50 out of 109 SNOMED-CT concepts met pre-set criteria to be considered adequately fine-tuned.
Conclusions
NLP can mine unstructured cancer data following limited training. Certain concepts such as histopathology terms remained poorly retrieved. Model performance is maintained when applied to a test cohort, demonstrating good generalisability. Concept-specific thresholding strategy improved performance. Fine-tuning annotations were incorporated into the NLP parent model for future performance. CogStack has been applied to extract data for 50 concepts with validated performance for our entire retrospective HNC cohort.}
}
@article{LI2025128342,
title = {Exploring biomedical relation extraction by combining natural language inference and dual dependency trees},
journal = {Expert Systems with Applications},
volume = {289},
pages = {128342},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128342},
url = {https://www.sciencedirect.com/science/article/pii/S095741742501961X},
author = {Xueying Li and Di Zhao and Jiana Meng and Hongfei Lin},
keywords = {Biomedical relation extraction, Natural language inference, Dual dependency trees, Contrastive learning loss function, Voting decision mechanism},
abstract = {The task of relation extraction primarily focuses on predicting the relations between pairs of entities in text, which is crucial for applications such as drug discovery and disease diagnosis. However, this field faces two major challenges: First, traditional relation extraction models can utilize dependency trees to capture contextual information, but they still struggle to accurately identify the important semantic associations between entity words. Second, these models perform poorly when dealing with the complex structure of biomedical texts and domain-specific terms, particularly when applying cross-domain methods, which limits the models’ ability to fully utilize textual information. We propose a novel approach that integrates Natural Language Inference by leveraging the textual entailment task to enhance the model’s understanding and prediction of biomedical relations. This approach utilizes an improved contrastive learning loss function to better differentiate the entailment scores between relations. Additionally, a dual dependency tree is adopted to capture reliable dependency information and filter out noise by utilizing both inter-entity and intra-entity dependency connections and types. We further introduce an innovative voting decision mechanism to improve the model’s ability to process and comprehend biomedical terms, thereby enhancing its performance on complex sentences. Experimental results show that our approach achieves state-of-the-art performance across three biomedical datasets, demonstrating its effectiveness and potential. Furthermore, ablation studies confirm the contribution of each technique, highlighting the importance of integrating NLI with dual dependency trees.}
}
@article{ANIKIN201864,
title = {Semantic treebanks and their uses for multi-level modelling of natural-language texts},
journal = {Procedia Computer Science},
volume = {145},
pages = {64-71},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918322968},
author = {Anton Anikin and Oleg Sychev},
keywords = {Treebanks, Natural Language Processing, Ontology, Text Modelling},
abstract = {Using multi-level models is necessary for relevant knowledge extraction during the analysis of large volumes of natural language texts. Such tasks are relevant for solving various problems in the field of analysis and generation of textual information. Such modelling of natural language texts requires big amount of texts with multi-layered annotations holding lexical, syntactical, semantic and narrative information. Annotated text collections are called text corpora; text corpora with syntactical annotations are called treebanks because syntactical information is stored in a tree form. The latest development in that field is semantic treebanks, that combine syntactical trees of sentences with the formal representation of their meaning in logical form. Many semantic treebanks, with shallow and deep semantic information, were developed in last years. Several approaches for manual and automatic building of semantic treebanks were developed. There are few generally accepted standards in that fast developing area, so different semantic banks vary significantly in the type of information they contain, especially on lexical level. In this paper, authors review semantic treebanks that could be used for text modeling, the characteristics the different semantic banks and the sort of data they contain on the semantic, narrative, syntactical and lexical levels, as well as the size and composition of relevant corpora and major tools for working with data in those banks. The approaches reviewed can be used for wide range of decision-making tasks in the field of analysis and generation of textual information, e.g. for information resources annotating and retrieval in the task of ontology-based collaborative development of domain information space for learning and scientific research [4], for generating and rewriting texts of different types (fiction, marketing, scientific etc.) and for different auditory [5] and many others.}
}
@article{SADICK2025112735,
title = {What did the occupant say? Fine-tuning and evaluating a large language model for efficient analysis of multi-domain indoor environmental quality feedback},
journal = {Building and Environment},
volume = {274},
pages = {112735},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2025.112735},
url = {https://www.sciencedirect.com/science/article/pii/S0360132325002173},
author = {Abdul-Manan Sadick and Giorgia Chinazzo},
keywords = {Indoor environmental quality, Occupant feedback, Multi-domain, Natural language processing, Artificial intelligence},
abstract = {Qualitative feedback from occupants on indoor environmental quality (IEQ) in unstructured text can provide valuable insights into the causes of comfort and discomfort in buildings. This feedback can be collected from open-ended survey questions, interviews, crowdsourced data, or innovative home automation technology that can transform voice inputs into text. However, manual text data processing is time-consuming and requires significant efforts to extract relevant insights, such as text classification into IEQ categories (i.e., visual, thermal, air quality and acoustic). Most IEQ studies that automated text feedback classification into IEQ categories relied on keyword matching, which cannot understand the context of some keywords, potentially leading to incorrect classification. To address this issue, we automated the detection and categorisation of unstructured IEQ feedback by adopting the Bidirectional Encoder Representations from Transformers (BERT) language model architecture and fine-tuning it on 14,622 manually labelled IEQ text feedback. The resulting model, IEQ-BERT, achieved a prediction accuracy of 93 % and macro average precision, recall, and F1-scores of 0.93, 0.94, and 0.93, respectively, across the five considered classes (i.e., acoustic, indoor air quality, thermal, visual, or No IEQ). Therefore, the model can effectively distinguish text concerning IEQ and identify which IEQ domain – acoustic, indoor air quality, thermal, visual, or their combinations – is being reported. IEQ-BERT can be used alone or integrated into building automation systems to identify patterns and trends of occupant feedback, prioritise areas for improvement, and support the development of targeted strategies to improve IEQ. This research contributes to developing efficient methods for analysing occupant feedback, ultimately leading to improved building performance and occupant quality of life.}
}
@article{BABAEI2025100791,
title = {An information integration framework toward cross-organizational management of integrated energy systems},
journal = {Journal of Industrial Information Integration},
volume = {44},
pages = {100791},
year = {2025},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2025.100791},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X25000159},
author = {F. Babaei and R. Bozorgmehry Boozarjomehry and Z. Kheirkhah Ravandi and M.R. Pishvaie},
keywords = {Supply chain, Integrated energy systems, Interoperability, Industrial information integration, Cross-organizational knowledge management, Ontology},
abstract = {Integrating information systems in supply chains and energy systems presents significant challenges due to diverse knowledge domains and cross-organizational processes. This study bridges the gap by employing industrial information integration engineering concepts. We propose a domain ontology framework to integrate supply chain conceptions, upon which several application-level semantic models in energy networks are developed. These ontologies, functioning as interoperable systems, enhance information sharing and data integration across strategic, tactical, and operational decision-making levels. Our proposed framework adheres to Industry 4.0 principles, offering a novel formalization of essential supply chain concepts and activities, ensuring logical consistency. This dual-level ontological approach surpasses previous models by enabling vertical and horizontal integration across supply chain hierarchies. It facilitates seamless communication between supply chain constituents, expert modelers, and software agents. Additionally, the application-level ontologies for energy networks capture various organizational operations, multi-energy vectors, demands, and conversion technologies. These semantic models reduce the knowledge management gap in integrated energy systems, aligning with Industry 4.0 objectives. Two scenarios demonstrate the framework's capabilities: virtual agents coordinate the water-energy nexus and configure integrated energy systems. Results indicate that the domain and application knowledge integration systems comprehensively cover corresponding business processes across operational hierarchies. Thus, the proposed framework supports intra- and inter-agent communications, with ontologies serving as knowledge repositories, ultimately facilitating better industrial integration.}
}
@article{WALLER2024103940,
title = {Questionable devices: Applying a large language model to deliberate carbon removal},
journal = {Environmental Science & Policy},
volume = {162},
pages = {103940},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103940},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124002740},
author = {Dr. Laurie Waller and Dr. David Moats and Dr. Emily Cox and Dr. Rob Bellamy},
keywords = {Carbon removal, Deliberation, Devices, Publics, Experiments in participation, Large language models, Generative AI},
abstract = {This paper presents a device-centred approach to deliberation, developed in deliberative workshops appraising methods for removing carbon dioxide from the air. Our approach involved deploying the Large Language Model application ChatGPT (sometimes termed “generative AI”) to elicit questions and generate texts about carbon removal. We develop the notion of the “questionable” device to foreground the informational unruliness ChatGPT introduced into the deliberations. The analysis highlights occasions where the deliberative apparatus became a focus of collective critique, including over: issue definitions, expert-curated resources, lay identities and social classifications. However, in this set-up ChatGPT was all too often engaged unquestioningly as an instrument for informing discussion; its instrumental lure disguising the unruliness it introduced into the workshops. In concluding, we elaborate the notion of questionable devices and reflect on the way carbon removal has been “devised” as a field in want of informed deliberation.}
}
@article{MEHRMOLAEI2025100813,
title = {A decade systematic review of fusion techniques in financial market prediction},
journal = {Computer Science Review},
volume = {58},
pages = {100813},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100813},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000899},
author = {Soheila Mehrmolaei and Mohammad {Saniee Abadeh}},
keywords = {Financial market, Fusion, Deep learning, Data analysis, Heterogeneous resources, LLMs},
abstract = {Financial markets are structured systems that facilitate investment, trading, and economic growth at both global and local levels. Accurate and reliable prediction in these markets is essential for making profitable decisions. However, the inherent complexity of financial markets, influenced by numerous external factors, makes it difficult to perform analysis and forecasting. In this context, fusion techniques have emerged as a strong and well-known approach, integrating data and features from multiple sources to enhance prediction accuracy. This paper presents a systematic review of studies on fusion techniques in financial market prediction, covering research published between 2016 and 2025. The primary objective is to provide a comprehensive understanding of the role of fusion techniques in financial market prediction from both macro and micro perspectives. To achieve this, we categorize fusion techniques based on level of integration, analyzing their benefits, limitations, and applications. Additionally, we discuss the necessity of fusion approaches, open challenges, and potential future advancements in this domain. Our review emphasizes the growing adoption of multimodal text data fusion using large language models (LLMs) as a promising trend to enhance prediction reliability. Also, we identify key research directions and emerging trends that are expected to shape the future development of fusion-based financial market prediction methods.}
}
@article{TAN2025106596,
title = {Beyond connected digital twins – Can digital twins really deliver sustainable cities?},
journal = {Sustainable Cities and Society},
volume = {131},
pages = {106596},
year = {2025},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2025.106596},
url = {https://www.sciencedirect.com/science/article/pii/S2210670725004706},
author = {Yong Ren Tan and Markus Hofmeister and Shin Zert Phua and George Brownbridge and Kushagar Rustagi and Jethro Akroyd and Sebastian Mosbach and Amit Bhave and Markus Kraft},
keywords = {Digital twins, Knowledge graphs, The World Avatar, Smart cities, Data integration},
abstract = {We examine the evolution of digital twins and introduce the ideas underlying The World Avatar (TWA), moving from a representation of physical entities to an approach founded on the idea of a general knowledge representation of the world at large. The ‘world’ in TWA refers not to planet Earth but to the space of all possible concepts. Unlike traditional digital twins, which often struggle with interoperability due to siloed data and proprietary standards, TWA is designed to enable the representation of any concept, allowing it to embrace any domain in terms of things and actions. This is achieved by leveraging semantic technologies, including ontologies, knowledge graphs and semantic agents in combination with machine learning to address the issue of interoperability. We show examples relating to flood risk modelling, energy planning and social equality assessments. The examples show how the design of TWA has been used to achieve scalable and seamless integration of data, models and computational agents to overcome the limitations of siloed approaches, and how the digital twinning approach underlying TWA can contribute to addressing urban development challenges to provide cross-domain insights that allow actionable, data-driven decision-making. The ideas underlying TWA are proposed as the ultimate evolution of digital twinning approaches, laying the foundation for scalable, interoperable solutions aligned with global sustainability goals, with particular relevance for sustainable urban development.}
}
@article{ROMANO2024,
title = {The Alzheimer’s Knowledge Base: A Knowledge Graph for Alzheimer Disease Research},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/46777},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124001857},
author = {Joseph D Romano and Van Truong and Rachit Kumar and Mythreye Venkatesan and Britney E Graham and Yun Hao and Nick Matsumoto and Xi Li and Zhiping Wang and Marylyn D Ritchie and Li Shen and Jason H Moore},
keywords = {Alzheimer disease, knowledge graph, knowledge base, artificial intelligence, drug repurposing, drug discovery, open source, Alzheimer, etiology, heterogeneous graph, therapeutic targets, machine learning, therapeutic discovery},
abstract = {Background
As global populations age and become susceptible to neurodegenerative illnesses, new therapies for Alzheimer disease (AD) are urgently needed. Existing data resources for drug discovery and repurposing fail to capture relationships central to the disease’s etiology and response to drugs.
Objective
We designed the Alzheimer’s Knowledge Base (AlzKB) to alleviate this need by providing a comprehensive knowledge representation of AD etiology and candidate therapeutics.
Methods
We designed the AlzKB as a large, heterogeneous graph knowledge base assembled using 22 diverse external data sources describing biological and pharmaceutical entities at different levels of organization (eg, chemicals, genes, anatomy, and diseases). AlzKB uses a Web Ontology Language 2 ontology to enforce semantic consistency and allow for ontological inference. We provide a public version of AlzKB and allow users to run and modify local versions of the knowledge base.
Results
AlzKB is freely available on the web and currently contains 118,902 entities with 1,309,527 relationships between those entities. To demonstrate its value, we used graph data science and machine learning to (1) propose new therapeutic targets based on similarities of AD to Parkinson disease and (2) repurpose existing drugs that may treat AD. For each use case, AlzKB recovers known therapeutic associations while proposing biologically plausible new ones.
Conclusions
AlzKB is a new, publicly available knowledge resource that enables researchers to discover complex translational associations for AD drug discovery. Through 2 use cases, we show that it is a valuable tool for proposing novel therapeutic hypotheses based on public biomedical knowledge.}
}
@article{DING20251296,
title = {A cognitive digital twin modeling method of robotic production line},
journal = {Manufacturing Letters},
volume = {44},
pages = {1296-1305},
year = {2025},
note = {53rd SME North American Manufacturing Research Conference (NAMRC 53)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2025.06.149},
url = {https://www.sciencedirect.com/science/article/pii/S2213846325001816},
author = {Jie Ding and Ruifang Li and Ziheng Liu and Jiayi Liu and Wenjun Xu},
keywords = {Cognitive digital twin, Robotic production line, Modeling method},
abstract = {Considering the change of manufacturing tasks, the current modeling method of robotic production line is difficult to extract the knowledge contained in the perception data. In this paper, ontology modeling is used to model the knowledge of robotic production line. The Cognitive Digital Twin (CDT) model is constructed from the aspects of cognition, geometry, physics, behavior and rules. The CDT model can help robotic production line to respond to changing manufacturing tasks quickly. To verify the cognitive ability of CDT model, the case studies are conducted in automotive Body in White (BIW) robotic production line. The comparison experiments with and without cognitive module were designed to verify the proposed method. The results show that the proposed CDT modeling method can enhance the cognitive ability of digital twin model and then improve the operation efficiency of the robotic production line.}
}
@article{ISLAM2020100304,
title = {Identification of the core ontologies and signature genes of polycystic ovary syndrome (PCOS): A bioinformatics analysis},
journal = {Informatics in Medicine Unlocked},
volume = {18},
pages = {100304},
year = {2020},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2020.100304},
url = {https://www.sciencedirect.com/science/article/pii/S2352914819304034},
author = {Md Rakibul Islam and Md Liton Ahmed and Bikash {Kumar Paul} and Touhid Bhuiyan and Kawsar Ahmed and Mohammad Ali Moni},
keywords = {Microarray, Gene ontology, Polycystic ovary syndrome, Differential expressed genes, Bioinformatics},
abstract = {Worldwide polycystic ovary syndrome (PCOS) is one of the most common hormonal disorders in women of reproductive age. However, there is a lack of genetic study of the internal mechanisms of PCOS. Herein, we identify core genes involved in the pathogenesis of PCOS by using bioinformatics analysis. For the study, the dataset GSE124226 was collected from the Gene Expression Omnibus (GEO) database. The differentially expressed genes (DEGs) were obtained by using the R package limma. We found a total of 180 DEGs in which 73 were overexpressed and 107 were down-expressed. The functional analysis was analysed using the DAVID database and software tools for the identified up-regulated and down-regulated DEGs. We generated protein-protein interaction (PPI) networks by using Cytoscape and identified four hub genes (RARA, KPNB1, REL, and MAP1B) from the PPI network according to the degree score using cytoHubba; module analysis was also performed by using the MCODE plugin. Finally, we used the identified hub genes to reveal significant drug signatures, which may be useful as therapeutic targets for PCOS.}
}
@article{CHUNFANG202536,
title = {TCMLCM: an intelligent question-answering model for traditional Chinese medicine lung cancer based on the KG2TRAG method},
journal = {Digital Chinese Medicine},
volume = {8},
number = {1},
pages = {36-45},
year = {2025},
issn = {2589-3777},
doi = {https://doi.org/10.1016/j.dcmed.2025.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S2589377725000291},
author = {Zhou Chunfang and Gong Qingyue and Zhan Wendong and Zhu Jinyang and Luan Huidan},
keywords = {Traditional Chinese medicine (TCM), Lung cancer, Question-answering, Large language model, Fine-tuning, Knowledge graph, KG2TRAG method},
abstract = {Objective
To improve the accuracy and professionalism of question-answering (QA) model in traditional Chinese medicine (TCM) lung cancer by integrating large language models with structured knowledge graphs using the knowledge graph (KG) to text-enhanced retrieval-augmented generation (KG2TRAG) method.
Methods
The TCM lung cancer model (TCMLCM) was constructed by fine-tuning ChatGLM2-6B on the specialized datasets Tianchi TCM, HuangDi, and ShenNong-TCM-Dataset, as well as a TCM lung cancer KG. The KG2TRAG method was applied to enhance the knowledge retrieval, which can convert KG triples into natural language text via ChatGPT-aided linearization, leveraging large language models (LLMs) for context-aware reasoning. For a comprehensive comparison, MedicalGPT, HuatuoGPT, and BenTsao were selected as the baseline models. Performance was evaluated using bilingual evaluation understudy (BLEU), recall-oriented understudy for gisting evaluation (ROUGE), accuracy, and the domain-specific TCM-LCEval metrics, with validation from TCM oncology experts assessing answer accuracy, professionalism, and usability.
Results
The TCMLCM model achieved the optimal performance across all metrics, including a BLEU score of 32.15%, ROUGE-L of 59.08%, and an accuracy rate of 79.68%. Notably, in the TCM-LCEval assessment specific to the field of TCM, its performance was 3% − 12% higher than that of the baseline model. Expert evaluations highlighted superior performance in accuracy and professionalism.
Conclusion
TCMLCM can provide an innovative solution for TCM lung cancer QA, demonstrating the feasibility of integrating structured KGs with LLMs. This work advances intelligent TCM healthcare tools and lays a foundation for future AI-driven applications in traditional medicine.}
}
@article{OCHIENG2020100024,
title = {PAROT: Translating natural language to SPARQL},
journal = {Expert Systems with Applications: X},
volume = {5},
pages = {100024},
year = {2020},
issn = {2590-1885},
doi = {https://doi.org/10.1016/j.eswax.2020.100024},
url = {https://www.sciencedirect.com/science/article/pii/S2590188520300032},
author = {Peter Ochieng},
keywords = {SPARQL, Natural language processing, Ontologies, Query},
abstract = {This paper provides a dependency based framework for converting natural language to SPARQL. We present a tool known as PAROT (which echos answers from ontologies) which is able to handle user’s queries that contain compound sentences, negation, scalar adjectives and numbered list. PAROT employs a number of dependency based heuristics to convert user’s queries to user’s triples. The user’s triples are then processed by the lexicon into ontology triples. It is these ontology triples that are used to construct SPARQL queries. From the experiments conducted, PAROT provides state of the art results.}
}
@article{SAEED2024103771,
title = {SUMEX: A hybrid framework for Semantic textUal siMilarity and EXplanation generation},
journal = {Information Processing & Management},
volume = {61},
number = {5},
pages = {103771},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103771},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001316},
author = {Sumaira Saeed and Quratulain Rajput and Sajjad Haider},
keywords = {Semantic Textual Similarity(STS), Explanation generation, Natural language processing, Embeddings, Clinical notes, ontology},
abstract = {Measuring semantic similarity between two pieces of text is a widely known problem in Natural language processing(NLP). It has many applications, such as finding similar medical notes of patients to accelerate the diagnosis process, plagiarism detection, and document clustering. Most state-of-the-art models are based on machine/deep learning and lack sufficient explanations for their results, limiting their adoption in critical domains like healthcare. This paper presents a hybrid framework SUMEX (Semantic textUal siMilarity and EXplanation generation) that uniquely combines ontology with a state-of-the-art embedding-based model for semantic textual similarity. The primary strength of the framework is that it explains its results in human-understandable natural language, which is vital in critical domains such as healthcare. Experiments have been conducted on two datasets of clinical notes using four embeddings: ScispaCy, BioWord2Vec, ClinicalBERT, and a customized Word2Vec trained on clinical notes. The SUMEX framework outperforms the embedding-based model on the benchmark datasets of ClinicalSTS by improving average precision scores by 7 % and reducing the false-positives-rate by 23 %. On the Patients Similarity Dataset, the average top-five and top-three precision scores were improved by 14% and 10%, respectively, using SUMEX. The SUMEX also generates explanations for its results in natural language. The domain experts evaluated the quality of the explanations. The results show that the generated explanations are of significantly good quality, with a score of 90 % and 93 % for measures of Completeness and Correctness, respectively. In addition, ChatGPT was also used for similarity score and generating explanations. The experiments show that the SUMEX framework performed better than the ChatGPT.}
}
@article{LIN2025112006,
title = {Scenario-based improved bayesian network model for construction safety assessment},
journal = {Engineering Applications of Artificial Intelligence},
volume = {160},
pages = {112006},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.112006},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625020147},
author = {Zelong Lin and Dewei Kong and Wei Li and W.M. Edmund Loh and C.J. Wong and Zhijian Sun and Wei He},
keywords = {Construction accident safety assessment, Scenario theory, Bayesian network structure learning, Decomposition optimization strategy},
abstract = {Assessing the safety of construction projects amidst potential accidents is crucial, especially with the rapid growth of modern infrastructure, which increases the risk of incidents. Existing safety assessment methods often fall short due to uncertainties and a lack of reliable analyses. To address this, a new method based on Scenario Theory (ST) and an improved Bayesian Network (BN) is proposed. This approach uses multi-source data to create a construction accident ontology and safety assessment paradigm. It refines the traditional BN algorithm to establish a construction accident BN and develops task and capability assessment functions. The method's effectiveness is validated through simulations and a real construction project in Tianjin, China. It improves safety by assessing engineering projects' emergency management capabilities in the face of accidents, benefiting construction firms and government agencies.}
}
@article{JING2025111460,
title = {A SysML-driven co-design platform applying an MBSE-PDM integration framework and cross-stage change propagation method},
journal = {Computers & Industrial Engineering},
volume = {209},
pages = {111460},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111460},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225006060},
author = {Xishuang Jing and Boyan Shen and Jielin Yu and Chengyang Zhang and Fubao Xie and Siyu Chen and Gang Zhao},
keywords = {MBSE, PDM, Co-design, SysML, Ontology, Change propagation},
abstract = {The co-design of complex system products, such as aero-engines, requires an integrated design environment that supports diverse modeling and management capabilities. This includes formal system modeling functions and data management functions, and the ability to evaluate the impact of design modifications to facilitate collaborative changes. This paper presents a SysML-driven co-design platform that integrates Model-Based Systems Engineering (MBSE) and Product Data Management (PDM) within a unified framework, along with a change propagation method applicable across multiple design stages. This paper introduces the concept of Co-design Session (CS), which establishes a data management framework aligned with the SysML-based system model. It manages data storage and interaction based on MBSE results, facilitating MBSE-PDM integration. Additionally, an ontology-based SysML model change propagation method is developed to manage early-stage design modifications, followed by an ontology model for design variables and constraints to support later-stage changes. Finally, a co-design platform is implemented, supporting SysML-based modeling, simulation, CAD visualization, and the proposed methods. The effectiveness of the proposed approach and platform is validated through a case study on the design of a High-flow Dual Variable Cycle Engine (HDVCE), demonstrating its capability to enhance data consistency, traceability, and multi-team co-design.}
}
@article{SONNENSCHEIN2024120232,
title = {Validating and constructing behavioral models for simulation and projection using automated knowledge extraction},
journal = {Information Sciences},
volume = {662},
pages = {120232},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120232},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524001452},
author = {Tabea S. Sonnenschein and G. Ardine {de Wit} and Nicolette R. {den Braver} and Roel C.H. Vermeulen and Simon Scheider},
keywords = {Validation, Knowledge extraction, Knowledge synthesis, Knowledge graph, Behavior modeling, Simulation, Ontology, BERT, Named-entity recognition},
abstract = {Human behavior may be one of the most challenging phenomena to model and validate. This paper proposes a method for automatically extracting and compiling evidence on human behavior determinants into a knowledge graph. The method (1) extracts associations of behavior determinants and choice options in relation to study groups and moderators from published studies using Natural Language Processing and Deep Learning, (2) synthesizes the extracted evidence into a knowledge graph, and (3) sub-selects the model components and relationships that are relevant and robust. The method can be used to either (4a) construct a structurally valid simulation model before proceeding with calibration or (4b) to validate the structure of existing simulation models. To demonstrate the feasibility of the method, we discuss an example implementation with mode of transport as behavior choice. We find that including non-frequently studied significant behavior determinants drastically improves the model's explanatory power in comparison to only including frequently studied variables. The paper serves as a proof-of-concept which can be reused, extended or adapted for various purposes.}
}
@article{ESCRICH2019104,
title = {Gene ontology analysis of transcriptome data from DMBA-induced mammary tumors of rats fed a high-corn oil and a high-extra virgin olive oil diet},
journal = {Data in Brief},
volume = {22},
pages = {104-108},
year = {2019},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2018.11.135},
url = {https://www.sciencedirect.com/science/article/pii/S2352340918315282},
author = {Raquel Escrich and Marta Cubedo and Eduard Escrich and Raquel Moral},
abstract = {Breast cancer is the most common malignancy in women worldwide, and dietary lipids are important environmental factors influencing its etiology. In this work we present data in relation to the transcriptional effects of two high-fat diets, one high in corn oil (HCO) and one high in extra-virgin olive oil (HOO), administered from weaning or after induction, on 7,12-dimethylbenz(a)anthracene (DMBA)-induced rat mammary tumors. Raw data were deposited at ArrayExpress under accession number E-MTAB-3541. We compared the gene expression profiles of the mammary tumors from the high-fat diet groups with those from the control group, finding different effects of diets depending on timing and type of dietary intervention. Lists of differentially expressed genes were analyzed to find overrepresented categories of biological significance. Here we provide information about the cell functions categories overrepresented in significantly modulated genes by effect of the high-fat diets. Further investigations of such functions are described in “A high corn oil diet strongly stimulates mammary carcinogenesis, while a high extra virgin olive oil diet has a weak effect, through changes in metabolism, immune system function, and proliferation/apoptosis pathways” (Escrich et al., in press) [1].}
}
@article{JAIN2022,
title = {Language-Agnostic Knowledge Representation for a Truly Multilingual Semantic Web},
journal = {International Journal of Information System Modeling and Design},
volume = {13},
number = {1},
year = {2022},
issn = {1947-8186},
doi = {https://doi.org/10.4018/IJISMD.297045},
url = {https://www.sciencedirect.com/science/article/pii/S1947818622000412},
author = {Sarika Jain and Anastasiia Kysliak},
keywords = {Globalization, Knowledge Representation, Multilingualism, Ontology, Semantic Web},
abstract = {ABSTRACT
As the internet user communities and the information on the web are increasing exponentially, meaningful search in a multilingual setting is required. It is crucial to develop a simple to use, granular, and adaptable knowledge representation system that could constitute the core of the Semantic Web. It should be well-prepared for multilingual and multicultural settings by virtue of internationalizing the code. In this work, a model for ontology-based language-agnostic Semantic Web application architecture is offered which is independent of today’s heterogeneous encoding of language resources and provides smooth exploitation. The suggested system architecture enables a quick and meaningful search for information independent of the end user’s preferred language and cultural conventions, an easy extension of the application with new information, as well as its easy localization for new language contexts. The model’s usability is proven by a prototype application simulation in the domain of Indian biodiversity.}
}
@article{CHATTERJEE2024570,
title = {Checking Counterfeit Critiques on Commodities using Ensemble Classifiers Enhancing Information Credibility},
journal = {Procedia Computer Science},
volume = {233},
pages = {570-579},
year = {2024},
note = {5th International Conference on Innovative Data Communication Technologies and Application (ICIDCA 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.246},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924006057},
author = {Ram Chatterjee and Mrinal Pandey and Hardeo Kumar Thakur and Anand Gupta},
keywords = {Information credibility, Large Language Models, Amazon Mechanical Turks, Elastic-net Classifier, LightGBM Trees Classifier},
abstract = {The conundrum of the ubiquitous deceptive reviews has overruled the online ontology with the obsession of obscure but obligatory posting of product reviews for the customers to believe, behold and beget the online product marketing. This mandates contemporary research in the direction to delve deeper on the application and analysis of deceiving online reviews with matured and advanced AI models functional on large scale datasets to effectively and efficiently demarcate between the genuine and the sham. The research counteracts the counterfeiting product reviews via the applications, assessment and analysis of the befitting AI models - Elastic-net Classifier model based on block coordinate descent with Wordcloud and its further performance enhancement through LightGBM Trees Classifier with Grid Search and Early Stopping support, with Log-Loss as performance metric for experimentation to gain insight into the intricacies of detection, diagnosis and diminution of fake product reviews. The paper also delineates discriminative and affirmative aspects of the dataset quality, statistics, stability and standards inherent and coherent to the creation of the dataset using Large Language Models (LLMs) intrinsic to the zeitgeist juncture of recent times promoting machines to produce large scale, cost effective bogus reviews in lieu of the Amazon Mechanical Turks. The results obtained with the Log-Loss holdout score of 0.1462 conforming the LightGBM classifier proves its performance better than the Elastic-Net classifier, conforming it as better than the ROC-AUC in terms of its proximity to the prediction probability for the matching actual/true value.}
}
@article{AKOKA2024102351,
title = {Unraveling the foundations and the evolution of conceptual modeling—Intellectual structure, current themes, and trajectories},
journal = {Data & Knowledge Engineering},
volume = {154},
pages = {102351},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102351},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000752},
author = {Jacky Akoka and Isabelle Comyn-Wattiau and Nicolas Prat and Veda C. Storey},
keywords = {Conceptual modeling, Entity-relationship model, Bibliometric analysis, Main path analysis, Co-citation analysis, Bibliographic coupling analysis, Ontologies},
abstract = {The field of conceptual modeling has now been in existence for over five decades. To understand how this field has evolved and should continue to evolve, it is useful to examine the contributions made over time and the themes that have emerged. In this research, we apply bibliometric analysis to a corpus of over 4700 research papers spanning from 1976 to 2023. We successively apply co-citation, bibliographic coupling, and main path analysis. Co-citation and citation networks are produced that surface the intellectual structure of the field, the main themes, and the relationships among major and influential research papers over time. We identify four areas in the intellectual structure of the field: conceptual modeling and databases; grammars and guidelines for conceptual modeling; requirements engineering and information systems design methodologies; and ontology constructs for conceptual modeling. Between 2017 and 2023, we distinguish nine research themes, including domain-specific conceptual modeling and applications, ontologies and applications, genomics, and datastores and multi-model data. The main path analysis identifies several trajectories among the major and most influential papers. This leads to insights into the lineage of key, influential papers in conceptual modeling research. The primordial nature of the main paths identified encompasses two important aspects. The first revolves around refining and complementing the entity-relationship model. The second identifies the contribution of ontologies for conceptual modeling to make the models more robust. Based on the findings from this bibliometric analysis, we propose several directions for future conceptual modeling research.}
}
@article{KIM20198,
title = {Distilling a Materials Synthesis Ontology},
journal = {Matter},
volume = {1},
number = {1},
pages = {8-12},
year = {2019},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2019.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S2590238519300360},
author = {Edward Kim and Kevin Huang and Olga Kononova and Gerbrand Ceder and Elsa Olivetti},
abstract = {Methods sections adopt a common practice of past-tense narrative using passive voice. Here, we discuss issues with current and historical writing conventions in materials science literature and propose a structured way to facilitate reproducibility, clarity, and machine readability.}
}
@article{BUI2019299,
title = {Risk management in local authorities: An application of Schatzki's social site ontology},
journal = {The British Accounting Review},
volume = {51},
number = {3},
pages = {299-315},
year = {2019},
issn = {0890-8389},
doi = {https://doi.org/10.1016/j.bar.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0890838919300010},
author = {Binh Bui and Carolyn J. Cordery and Zhichao Wang},
keywords = {Accounting, Risk management, Schatzki's social site ontology, Practice theory, Qualitative case study},
abstract = {Prior research has devoted limited attention to studying changes in organisational risk management (RM) practices. This is despite continuous dissatisfaction from academics and practitioners with organisations' ability to manage risks. We draw on Schatzki's social site ontology to study RM practices of two New Zealand local authorities that both experienced (earthquake) risk events and whose RM practices could be expected to change. We extend recent research utilising Schatzki, by finding that practical intelligibility and general understanding mutually affect each other in the organising of practices. Further, we extend Nama and Lowe’s (2014) addition to Schatzki by highlighting the importance of including teleological structures and accounting devices into the mutually constitutive relationship between general understanding and affectivity. Finally, we contribute to RM literature by proposing that changing the general understanding (in addition to the mere implementation of RM tools) is an important way of making RM change fundamental and sustainable.}
}
@article{KACFAHEMANI2019130,
title = {NALDO: From natural language definitions to OWL expressions},
journal = {Data & Knowledge Engineering},
volume = {122},
pages = {130-141},
year = {2019},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18306086},
author = {Cheikh {Kacfah Emani} and Catarina {Ferreira Da Silva} and Bruno Fiès and Parisa Ghodous},
keywords = {Ontologies, Natural language definitions, Ontology enrichment, OWL DL, Semantic web},
abstract = {Domain ontologies are pivotal for Semantic Web applications. The richness of an ontology goes in hand with its usefulness and efficiency. Unfortunately, manually enriching an ontology is very time-consuming. In this paper, we propose to enrich an ontology automatically by obtaining logical expressions of concepts. We present NALDO, a novel approach that provides an OWL DL (Web Ontology Language Description Logics) expression of a concept from two inputs: (1) the natural language definition of the concept and (2) an ontology describing the domain of this concept. NALDO uses as much as possible entities provided by the domain ontology, however it can suggest, when needed, new entities. The expressiveness of expressions provided by NALDO covers value and cardinality restrictions, subsumption and equivalence. We evaluate our approach against the definitions and the corresponding ontologies of the BEAUFORD benchmark. Our results show that NALDO is able to perform the correct identification of formal entities with an F1-measure up to 0.79.}
}
@article{LEE2025116159,
title = {Agentic Built Environments: a review},
journal = {Energy and Buildings},
volume = {346},
pages = {116159},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2025.116159},
url = {https://www.sciencedirect.com/science/article/pii/S0378778825008898},
author = {Jeyoon Lee and Jihwan Song and Jabeom Koo and Sebin Choi and Jaemin Hwang and Syed Mostasim {Hasnain Saif} and Yuxin Li and Jiteng Li and Jaehyun Yoo and Gowoon Lee and Minju Seok and Sungmin Yoon},
keywords = {Agentic AI, Large language models, Knowledge engineering, Built environments},
abstract = {Agentic artificial intelligence (AI) holds significant potential for enhancing functional capabilities and effectiveness in domain-specific applications across built environments. While recent studies have primarily focused on the architectural components or technical mechanisms of large language model (LLM)-based AI agents, there remains a lack of comprehensive literature reviews addressing their various application domains, functional roles, and learning approaches within the built environment. Therefore, this study reviews the current landscape of Agentic AI applications in the built environment and proposes a classification structure that encompasses applications, functional roles, and learning approaches. First, this paper examines five representative applications within the built environment. Second, it categorizes the roles of AI agents according to the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, emphasizing their progression from data interpretation to decision support. Finally, this review identifies four core learning approaches adopted by AI agents. Based on this classification framework, this paper defines Agentic Built Environment as virtual assistants embedded with Agentic AI that are capable of providing intelligent services throughout the entire building lifecycle. It also presents the current Level of Development (LoD) of the Agentic Built Environment, identifies existing limitations, and proposes future directions for developing scalable AI agents that support AI-powered services and intelligent decision-making throughout the building lifecycle.}
}
@article{XU2025106217,
title = {Automated openBIM-based discrete event simulation modeling for cradle-to-site embodied carbon assessment},
journal = {Automation in Construction},
volume = {175},
pages = {106217},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2025.106217},
url = {https://www.sciencedirect.com/science/article/pii/S0926580525002572},
author = {Yuqing Xu and Xingbo Gong and Xingyu Tao and Helen H.L. Kwok and Charinee Limsawasd and Jack C.P. Cheng},
keywords = {Building information modeling (BIM), openBIM, Carbon management, Discrete event simulation (DES)},
abstract = {Assessing cradle-to-site embodied carbon (EC) emissions enables stakeholders to make carbon-reduction decisions early. Discrete event simulation (DES) is useful for analyzing cradle-to-site EC by simulating construction operations. However, developing a DES model for cradle-to-site EC assessment in construction projects is time-consuming and error-prone. Therefore, this paper proposes an automated openBIM-based DES modeling approach. It starts with developing an integrated ontology of EC assessment, building information modeling (BIM) elements, and DES modeling to identify data requirements. It is followed by a BIM data verification flow using open standards, including extending Industry Foundation Classes (IFC) files and checking data integrity in IFC. Lastly, a general IFC-to-XML conversion tool is developed to convert processed IFC inputs into XML files, enabling the automation of DES modeling for cradle-to-site EC assessment. This openBIM-based DES solution was demonstrated in an actual project building. The results indicate that our approach can significantly improve DES modeling efficiency.}
}
@article{CHANDRA2023110645,
title = {Semantic web-based diagnosis and treatment of vector-borne diseases using SWRL rules},
journal = {Knowledge-Based Systems},
volume = {274},
pages = {110645},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110645},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123003957},
author = {Ritesh Chandra and Sadhana Tiwari and Sonali Agarwal and Navjot Singh},
keywords = {Semantic web, Decision support system, Basic Formal Ontology, NVBDCP, Vector borne diseases},
abstract = {Vector-borne diseases (VBDs) are a kind of infection caused through the transmission of vectors generated by the bites of infected parasites, bacteria, and viruses, such as ticks, mosquitoes, triatomine bugs, blackflies, and sandflies. If these diseases are not properly treated within a reasonable time frame, the mortality rate may rise. In this work, we propose a set of ontologies that will help in the diagnosis and treatment of vector-borne diseases. For developing VBD’s ontology, electronic health records taken from the Indian Health Records website, text data generated from Indian government medical mobile applications, and doctors’ prescribed handwritten notes of patients are used as input. This data is then converted into correct text using Optical Character Recognition (OCR) and a spelling checker after pre-processing. Natural Language Processing (NLP) is applied for entity extraction from text data for making Resource Description Framework (RDF) medical data with the help of the Patient Clinical Data (PCD) ontology. Afterwards, Basic Formal Ontology (BFO), National Vector Borne Disease Control Program (NVBDCP) guidelines, and RDF medical data are used to develop ontologies for VBDs, and Semantic Web Rule Language (SWRL) rules are applied for diagnosis and treatment. The developed ontology helps in the construction of decision support systems (DSS) for the NVBDCP to control these diseases.}
}
@article{HAN2025146079,
title = {A method for constructing fault knowledge graphs based on an improved hidden Markov Model: A case study for papermaking industry},
journal = {Journal of Cleaner Production},
volume = {520},
pages = {146079},
year = {2025},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2025.146079},
url = {https://www.sciencedirect.com/science/article/pii/S0959652625014295},
author = {Yulin Han and Huanhuan Zhang and Yi Man},
keywords = {Fault diagnosis, Knowledge graph, Text segmentation, Knowledge extraction, Machine learning},
abstract = {As a technology and knowledge-intensive industry, the process industry, central to sustainable manufacturing goals, faces challenges with large volumes of dispersed data, high integration of production units, and complex workflows. Existing methods struggle to analyze unstructured mechanism and experience knowledge, leading to information silos. To support cleaner production through enhanced fault diagnosis and prevention, this study leverages knowledge graph theory. An improved Hidden Markov Model for industrial text segmentation is proposed, demonstrating a 3.2 % accuracy increase over general tools. By utilizing this method to effectively process unstructured data and extract valuable knowledge, a dedicated fault knowledge graph framework and ontology model for process industries is constructed. This knowledge graph is then integrated with machine learning algorithms to build an industrial status diagnosis model; crucially, it enables intelligent feature selection, bypassing complex dimensionality reduction tasks common in previous approaches. Through a case study on tissue paper break faults, the framework is demonstrated by establishing a paper break fault knowledge graph and diagnosis model. This approach provides causal reasoning for proactive interventions that reduce scrap rates and optimize resource utilization, key drivers for improving eco-efficiency and advancing green, sustainable operations within the process industries.}
}
@article{WU201973,
title = {Natural-language-based intelligent retrieval engine for BIM object database},
journal = {Computers in Industry},
volume = {108},
pages = {73-88},
year = {2019},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2019.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0166361518304020},
author = {Songfei Wu and Qiyu Shen and Yichuan Deng and Jack Cheng},
keywords = {BIM object database, NLP, Intelligent retrieval},
abstract = {Rapid growth of building components in the BIM object database increases the difficulty of the efficient query of components that users require. Retrieval technology such as Autodesk Seek in America and BIMobject in Europe, which are widely used in BIM databases, are unable to understand what the search field truly means, causing a lack of completion and a low accuracy rate for results incapable of meeting the demands of users. To tackle such a problem, this paper puts forward a natural-language-based intelligent retrieval engine for the BIM object database and Revit modeling. First, a domain ontology is constructed for semantic understanding, and the BIM object database framework is established for testing our search engine. Second, “target keyword” and “restriction sequence” proposed are extracted from the natural sentences of users. Then, a final query is formed, combining concepts of “keyword” and “restriction sequence”, and its concepts are expanded through the semantic relationship in ontology. Finally, the results are presented after mapping from the final query to the BIM object database and ranking of results. Compared with traditional keyword-based methods, the experimental results demonstrate that our method outperforms the traditional methods.}
}
@article{AMAR2024,
title = {Electronic Health Record and Semantic Issues Using Fast Healthcare Interoperability Resources: Systematic Mapping Review},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45209},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124000384},
author = {Fouzia Amar and Alain April and Alain Abran},
keywords = {electronic health record, EHR, Health Level Seven International Fast Healthcare Interoperability Resources, HL7 FHIR, interoperability, web ontology language, OWL, ontology, semantic, terminology, resource description framework, RDF, machine learning, ML, natural language processing, NLP},
abstract = {Background
The increasing use of electronic health records and the Internet of Things has led to interoperability issues at different levels (structural and semantic). Standards are important not only for successfully exchanging data but also for appropriately interpreting them (semantic interoperability). Thus, to facilitate the semantic interoperability of data exchanged in health care, considerable resources have been deployed to improve the quality of shared clinical data by structuring and mapping them to the Fast Healthcare Interoperability Resources (FHIR) standard.
Objective
The aims of this study are 2-fold: to inventory the studies on FHIR semantic interoperability resources and terminologies and to identify and classify the approaches and contributions proposed in these studies.
Methods
A systematic mapping review (SMR) was conducted using 10 electronic databases as sources of information for inventory and review studies published during 2012 to 2022 on the development and improvement of semantic interoperability using the FHIR standard.
Results
A total of 70 FHIR studies were selected and analyzed to identify FHIR resource types and terminologies from a semantic perspective. The proposed semantic approaches were classified into 6 categories, namely mapping (31/126, 24.6%), terminology services (18/126, 14.3%), resource description framework or web ontology language–based proposals (24/126, 19%), annotation proposals (18/126, 14.3%), machine learning (ML) and natural language processing (NLP) proposals (20/126, 15.9%), and ontology-based proposals (15/126, 11.9%). From 2012 to 2022, there has been continued research in 6 categories of approaches as well as in new and emerging annotations and ML and NLP proposals. This SMR also classifies the contributions of the selected studies into 5 categories: framework or architecture proposals, model proposals, technique proposals, comparison services, and tool proposals. The most frequent type of contribution is the proposal of a framework or architecture to enable semantic interoperability.
Conclusions
This SMR provides a classification of the different solutions proposed to address semantic interoperability using FHIR at different levels: collecting, extracting and annotating data, modeling electronic health record data from legacy systems, and applying transformation and mapping to FHIR models and terminologies. The use of ML and NLP for unstructured data is promising and has been applied to specific use case scenarios. In addition, terminology services are needed to accelerate their use and adoption; furthermore, techniques and tools to automate annotation and ontology comparison should help reduce human interaction.}
}
@incollection{PUTS2024137,
title = {Chapter 6 - Natural language processing in oncology},
editor = {John Kang and Tim Rattay and Barry S. Rosenstein},
booktitle = {Machine Learning and Artificial Intelligence in Radiation Oncology},
publisher = {Academic Press},
pages = {137-161},
year = {2024},
isbn = {978-0-12-822000-9},
doi = {https://doi.org/10.1016/B978-0-12-822000-9.00004-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128220009000045},
author = {Sander Puts and Catharina Zegers and Stuti Nayak and Martijn Nobel and Andre Dekker},
keywords = {NLP, Ontologies, Structured data, Text, Transfer learning, Transformers, Word embeddings},
abstract = {The subfield of machine learning focusing on naturally spoken or written language by humans is called natural language processing (NLP). A major task for NLP is transforming free text into structured data and linking text to ontologies or terminologies. Use cases of NLP in the clinical domain are the extraction of diagnoses and staging information, patient matching for clinical trials and the extraction of progress and outcome data from clinical notes. Clinicians can contribute to an NLP system by labeling data or by providing input for rules. NLP approaches include rule-based, traditional machine-learning approaches and neural or deep-learning approaches. With the introduction of new deep-learning models and improved techniques, such as the transformer model, there have been striking performance improvements across a wide range of NLP tasks. It is likely that NLP systems will become more and more widespread in clinical practice in the next years.}
}
@incollection{MARTINIS20253,
title = {Natural Language Processing Approaches in Bioinformatics},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {3-18},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00179-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027001792},
author = {Maria Chiara Martinis and Zucco Chiara},
keywords = {Active learning, Biomedical natural language processing, Semantical analysis and Syntactical analysis},
abstract = {In this article, we provide an overview of the natural language processing and its bioinformatics applications. We describe the historical evolution of NLP, and summarize the common NLP sub-problems in the field, as well as their research progress in the biomedical domain. In addition, we discuss an advanced topic of applying active learning methods to the NLP systems, to solve the practical issue of lacking training data in the field.}
}
@article{TRAN2025101410,
title = {HyPepTox-Fuse: An interpretable hybrid framework for accurate peptide toxicity prediction fusing protein language model-based embeddings with conventional descriptors},
journal = {Journal of Pharmaceutical Analysis},
volume = {15},
number = {8},
pages = {101410},
year = {2025},
issn = {2095-1779},
doi = {https://doi.org/10.1016/j.jpha.2025.101410},
url = {https://www.sciencedirect.com/science/article/pii/S2095177925002278},
author = {Duong Thanh Tran and Nhat Truong Pham and Nguyen Doan Hieu Nguyen and Leyi Wei and Balachandran Manavalan},
keywords = {Peptide toxicity, Hybrid framework, Multi-head attention, Transformer, Deep learning, Machine learning, Protein language model},
abstract = {Peptide-based therapeutics hold great promise for the treatment of various diseases; however, their clinical application is often hindered by toxicity challenges. The accurate prediction of peptide toxicity is crucial for designing safe peptide-based therapeutics. While traditional experimental approaches are time-consuming and expensive, computational methods have emerged as viable alternatives, including similarity-based and machine learning (ML)-/deep learning (DL)-based methods. However, existing methods often struggle with robustness and generalizability. To address these challenges, we propose HyPepTox-Fuse, a novel framework that fuses protein language model (PLM)-based embeddings with conventional descriptors. HyPepTox-Fuse integrates ensemble PLM-based embeddings to achieve richer peptide representations by leveraging a cross-modal multi-head attention mechanism and Transformer architecture. A robust feature ranking and selection pipeline further refines conventional descriptors, thus enhancing prediction performance. Our framework outperforms state-of-the-art methods in cross-validation and independent evaluations, offering a scalable and reliable tool for peptide toxicity prediction. Moreover, we conducted a case study to validate the robustness and generalizability of HyPepTox-Fuse, highlighting its effectiveness in enhancing model performance. Furthermore, the HyPepTox-Fuse server is freely accessible at https://balalab-skku.org/HyPepTox-Fuse/ and the source code is publicly available at https://github.com/cbbl-skku-org/HyPepTox-Fuse/. The study thus presents an intuitive platform for predicting peptide toxicity and supports reproducibility through openly available datasets.}
}
@article{KEYNES2020130,
title = {Ontology, sovereignty, legitimacy: two key moments when history curriculum was challenged in public discourse and the curricular effects, Australia 1950s and 2000s},
journal = {History of Education Review},
volume = {50},
number = {2},
pages = {130-145},
year = {2020},
issn = {0819-8691},
doi = {https://doi.org/10.1108/HER-07-2020-0043},
url = {https://www.sciencedirect.com/science/article/pii/S0819869120000409},
author = {Matthew R. Keynes and Beth Marsden},
keywords = {Australia, Curriculum, Aboriginal history, Settler-colonialism, Nation-building, History education, },
abstract = {Purpose
The purpose of this paper is to examine the ways that history curriculum has worked to legitimise dispossession through narratives that elide questions of Indigenous sovereignty, and which construct and consolidate white settler identity and possession.
Design/methodology/approach
The paper uses two case studies to compare history education documentation and materials at key moments where dominant narratives of settler legitimacy were challenged in public discourse: (1) the post-war humanitarian agenda of fostering “international understanding” and; (2) the release and educational recommendations of the 1997 Bringing them Home Report.
Findings
The paper shows that in two moments where narratives of settler legitimacy were challenged in public discourse, the legitimacy of settler possession was reiterated in history curricula in various ways.
Practical implications
This research suggests that the prevailing constructivist framework for history education has not sufficiently challenged criticisms of the representation of Aboriginal history and the history of settler-colonialism in the history syllabus.
Originality/value
The paper introduces two case studies of history curriculum and shows how, in different but resonant ways, curricular reforms worked to bolster the liberal credentials of the settler state.}
}
@article{BAI20252123,
title = {twa: The World Avatar Python package for dynamic knowledge graphs and its application in reticular chemistry††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d5dd00069f},
journal = {Digital Discovery},
volume = {4},
number = {8},
pages = {2123-2135},
year = {2025},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d5dd00069f},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X25001287},
author = {Jiaru Bai and Simon D. Rihm and Aleksandar Kondinski and Fabio Saluz and Xinhong Deng and George Brownbridge and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
abstract = {Data-driven discovery is crucial in scientific domains, yet the lack of standardised data management hinders reproducibility. In chemical science, this is exacerbated by fragmented data formats. The World Avatar (TWA) addresses these challenges via a dynamic knowledge graph historically provided in Java-based toolkits. We present twa, an open-source Python package that lowers the barrier to semantic data management. Its object-graph mapper (OGM) synchronises Python class hierarchies with RDF knowledge graphs, streamlining ontology-driven data integration and automated workflows. We demonstrate twa's capacity to unify fragmented chemical data and accelerate research through use cases in molecular design and AI-assisted synthesis protocol extraction for metal–organic polyhedra (MOPs). Our approach expands the existing OntoMOPs knowledge graph by adding 799 new MOPs derived from combinatorial assembly models. By abstracting complex SPARQL queries behind a user-friendly interface, twa fosters transparent, reproducible knowledge-driven discovery. The package is freely available via pip install twa or https://pypi.org/project/twa/.}
}
@article{REFORGIATORECUPERO2020102094,
title = {Knowledge acquisition from parsing natural language expressions for humanoid robot action commands},
journal = {Information Processing & Management},
volume = {57},
number = {6},
pages = {102094},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2019.102094},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319303322},
author = {Diego {Reforgiato Recupero} and Federico Spiga},
keywords = {Humanoid robot, Robot action ontology, Language understanding, Human-Robot dialogue, Ontology design},
abstract = {In this paper we propose an approach that allows the NAO humanoid robot to execute natural language commands spoken by the user. To provide the robot with knowledge, we have defined an action robot ontology. The ontology is fed to an NLP engine that performs a machine reading of the input text (in natural language) given by a user and tries to identify action commands for the robot to execute. The system can work in two modes: STATELESS and STATEFUL. In STATELESS mode, each human expression correctly interpreted by the robot as an action command is performed by NAO which returns in its default posture afterwards. When in STATEFUL mode, the robot has knowledge of its current posture and performs the command only if it is compatible with its current state. In this mode, the robot does not return to its default posture. For example, if the user had told the robot to stand on its right leg in a first command, the robot cannot perform a following command stating to stand on its left leg as the two actions (raise left leg and raise right leg are incompatible). For each action that the robot can perform we modeled a corresponding element in the ontology that also includes a list of associated compatible and non-compatible actions. Our system also handles compound expressions (e.g., move your arms up) and multiple expressions (different commands within one sentence) that the robot understands and performs.}
}
@incollection{BICKHARD2025261,
title = {Persons: The emergence of Homo Socius},
editor = {Mark H. Bickhard},
booktitle = {The Whole Person},
publisher = {Academic Press},
pages = {261-442},
year = {2025},
isbn = {978-0-443-33050-6},
doi = {https://doi.org/10.1016/B978-0-443-33050-6.00010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443330506000100},
author = {Mark H. Bickhard},
keywords = {development, learning, constructivism, developmental domain, social development, language development, encodingism, person development, agency, rationality, personality and psychopathology, ethics},
abstract = {Social realities, persons, and language have co-evolved, with intrinsic ontological involvements with each other. This chapter presents these models and examines their interrelations and evolutionary and historical developments. The central model is that of convention: convention arises as a form of resolution of a fundamental epistemological problem that arises among social agents. The model is a descendent of Lewis’s model. Social realities are proposed as multiple kinds of conventions, with crucial forms having emerged historically. Persons emerge as historical-culturally constituted social agents, that have an intrinsic normative ontology. Language is a conventional means of interacting with social realities, and thus with the persons who constitute them. This is a very different model from standard models of utterances as transmissions of encoded messages. Persons are developmentally emergent processes. Development is an aspect of the same constructive processes that constitute learning. The emergence of developmental domains is characterized, and social, language, and person development are modeled. Among the crucial further developmental emergences within person and social processes are those of agency, rationality, personality and psychopathology, and ethics. Some of the consequences of this overall model for these normative emergences are examined; process and emergence models offer alternative perspectives on each of these domains.}
}
@article{WANG2020,
title = {Using Natural Language Processing Techniques to Provide Personalized Educational Materials for Chronic Disease Patients in China: Development and Assessment of a Knowledge-Based Health Recommender System},
journal = {JMIR Medical Informatics},
volume = {8},
number = {4},
year = {2020},
issn = {2291-9694},
doi = {https://doi.org/10.2196/17642},
url = {https://www.sciencedirect.com/science/article/pii/S2291969420001556},
author = {Zheyu Wang and Haoce Huang and Liping Cui and Juan Chen and Jiye An and Huilong Duan and Huiqing Ge and Ning Deng},
keywords = {health education, ontology, natural language processing, chronic disease, recommender system},
abstract = {Background
Health education emerged as an important intervention for improving the awareness and self-management abilities of chronic disease patients. The development of information technologies has changed the form of patient educational materials from traditional paper materials to electronic materials. To date, the amount of patient educational materials on the internet is tremendous, with variable quality, which makes it hard to identify the most valuable materials by individuals lacking medical backgrounds.
Objective
The aim of this study was to develop a health recommender system to provide appropriate educational materials for chronic disease patients in China and evaluate the effect of this system.
Methods
A knowledge-based recommender system was implemented using ontology and several natural language processing (NLP) techniques. The development process was divided into 3 stages. In stage 1, an ontology was constructed to describe patient characteristics contained in the data. In stage 2, an algorithm was designed and implemented to generate recommendations based on the ontology. Patient data and educational materials were mapped to the ontology and converted into vectors of the same length, and then recommendations were generated according to similarity between these vectors. In stage 3, the ontology and algorithm were incorporated into an mHealth system for practical use. Keyword extraction algorithms and pretrained word embeddings were used to preprocess educational materials. Three strategies were proposed to improve the performance of keyword extraction. System evaluation was based on a manually assembled test collection for 50 patients and 100 educational documents. Recommendation performance was assessed using the macro precision of top-ranked documents and the overall mean average precision (MAP).
Results
The constructed ontology contained 40 classes, 31 object properties, 67 data properties, and 32 individuals. A total of 80 SWRL rules were defined to implement the semantic logic of mapping patient original data to the ontology vector space. The recommender system was implemented as a separate Web service connected with patients' smartphones. According to the evaluation results, our system can achieve a macro precision up to 0.970 for the top 1 recommendation and an overall MAP score up to 0.628.
Conclusions
This study demonstrated that a knowledge-based health recommender system has the potential to accurately recommend educational materials to chronic disease patients. Traditional NLP techniques combined with improvement strategies for specific language and domain proved to be effective for improving system performance. One direction for future work is to explore the effect of such systems from the perspective of patients in a practical setting.}
}
@article{PINTO2025100302,
title = {Model and service for privacy in decentralized online social networks},
journal = {Journal of Electronic Science and Technology},
volume = {23},
number = {1},
pages = {100302},
year = {2025},
issn = {1674-862X},
doi = {https://doi.org/10.1016/j.jnlest.2025.100302},
url = {https://www.sciencedirect.com/science/article/pii/S1674862X25000035},
author = {George Pacheco Pinto and José Ronaldo Leles and Cíntia {da Costa Souza} and Paulo R. {de Souza} and Frederico Araújo Durão and Cássio Prazeres},
keywords = {Access control, Decentralized online social network, Ontology, Privacy},
abstract = {Intensely using online social networks (OSNs) makes users concerned about privacy of data. Given the centralized nature of these platforms, and since each platform has a particular storage mechanism, authentication, and access control, their users do not have the control and the right over their data. Therefore, users cannot easily switch between similar platforms or transfer data from one platform to another. These issues imply, among other things, a threat to privacy since such users depend on the interests of the service provider responsible for administering OSNs. As a strategy for the decentralization of the OSNs and, consequently, as a solution to the privacy problems in these environments, the so-called decentralized online social networks (DOSNs) have emerged. Unlike OSNs, DOSNs are decentralized content management platforms because they do not use centralized service providers. Although DOSNs address some of the privacy issues encountered in OSNs, DOSNs also pose significant challenges to consider, for example, access control to user profile information with high granularity. This work proposes developing an ontological model and a service to support privacy in DOSNs. The model describes the main concepts of privacy access control in DOSNs and their relationships. In addition, the service will consume the model to apply access control according to the policies represented in the model. Our model was evaluated in two phases to verify its compliance with the proposed domain. Finally, we evaluated our service with a performance evaluation, and the results were satisfactory concerning the response time of access control requests.}
}
@article{BIJU20241903,
title = {From Text to Action: NLP Techniques for Washing Machine Manual Processing},
journal = {Procedia Computer Science},
volume = {235},
pages = {1903-1919},
year = {2024},
note = {International Conference on Machine Learning and Data Engineering (ICMLDE 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.04.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924008573},
author = {Vinai George Biju and Bibin Babu and Ali Asghar and Boppuru Rudra Prathap and Vandana Reddy},
keywords = {Extraction, Segmentation of text, Ontology, BERT, Hugging Face, Text Summarization, Translation, Q/A Pipeline},
abstract = {This scientific research study focuses on the advancements in Natural Language Processing (NLP) driven by large-scale parallel corpora and presents a comprehensive methodology for creating a parallel, multilingual corpus using NLP techniques and semantic technologies, with a particular focus on washing machine manuals. The study highlights the significant progress made in NLP through the utilization of large-scale parallel corpora and advanced NLP techniques. The successful creation of a parallel, multilingual corpus for washing machine manuals, coupled with the integration of semantic technologies and ontology modeling, demonstrates the broad applicability and potential of NLP in diverse domains.The research covers various aspects, including text extraction, segmentation, and the development of specialized pipelines for question-answering, translation, and text summarization tailored for washing machine manuals. Translation experiments using fine-tuned models demonstrated the feasibility of providing washing machine manuals in local languages, expanding accessibility and understanding for users worldwide. Additionally, the study explored text summarization using a powerful transformer-based model, which exhibited remarkable proficiency in generating concise and coherent summaries from complex input texts. The implementation of a question-answering pipeline showcased the effectiveness of various language models in handling question-answering tasks with high accuracy and effectiveness.Additionally, the article discusses the processes of data collection, information preparation, ontology creation, alignment strategies, and text analytics. Furthermore, the study addresses the challenges and potential future developments in this field, offering insights into the promising applications of NLP in the context of washing machine manuals.}
}
@article{ZHOU2025976,
title = {Knowledge-driven innovation in industrial maintenance: A neural-enhanced model-based definition framework for lifecycle maintenance process information propagation},
journal = {Journal of Manufacturing Systems},
volume = {82},
pages = {976-999},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525002006},
author = {Qidi Zhou and Dong Zhou and Chao Dai and Jiayu Chen and Ziyue Guo},
keywords = {Maintenance process information, Knowledge driven, Neural-enhanced network, Model-based definition, Lifecycle information propagation},
abstract = {Under intensifying global competitive pressures, the digital strategic transformation of enterprises requires industrial information propagation across heterogeneous systems and lifecycle stages. These disparate transmission carriers and heterogeneous implementation mechanisms result in the inconsistent propagation of maintenance process information (MPI) in industrial information flows. These challenges render the structured data and knowledge in MPI, including maintenance activities, resource allocations, procedural instructions, and operational parameters, prone to ineffective dissemination across lifecycle phases and introduce risks of catastrophic operational failure. However, the direct application of current industrial information propagation methods, such as model-based definition (MBD) and intelligent information generation, encounters two obstacles: an incomplete standardization system for MPI definitions and construction and a mismatch between heterogeneous semistructured maintenance texts and the MPI. Therefore, a knowledge-driven neural-enhanced MBD framework for lifecycle MPI propagation is proposed. First, a lifecycle MPI propagation architecture is established to provide subsequent normative guidance. Second, an ontology-driven definition and construction method for MBD-based MPI is specified to address the obstacles posed by incomplete standardization systems. Third, an intelligent generation method for MBD-based MPI is constructed to overcome the obstacles of semantic mismatches. Finally, using aviation equipment as an example, the accuracy of the generated MPI and the feasibility of the innovative framework are verified via comparisons with current neural-enhanced models and results from multiple participants. The framework addresses lifecycle MPI propagation challenges through systematic knowledge formalization and neural-enhanced generation, advancing Industry 5.0’s vision of human-centric, resilient maintenance systems.}
}
@article{FOMINA2020507,
title = {Parametric and semantic analytical search indexes in hieroglyphic languages},
journal = {Procedia Computer Science},
volume = {169},
pages = {507-512},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303410},
author = {Julia Fomina and Denis Safikanov and Alexey Artamonov and Evgeniy Tretyakov},
keywords = {Big Data, search index, life cycle, text processing, database, Chinese language Relevance},
abstract = {Nowadays tremendous amounts of heterogeneous information known as Big Data have completely changed the modern scientific landscape. On the one hand, Big Data provides experts with massive opportunities for conducting research in almost every filed of human endeavor, developing novel technologies and disseminating scientific knowledge. On the other hand, a strong need for creating new techniques to handle Big Data has emerged. This paper is devoted to analytical search indexes, which allow researchers to obtain the information of interest from Big Data. Two types of analytical search indexes, namely parametric and semantic, are considered in the paper. Parametric search index will allow researchers to find information about a technology or material with specific physical parameters which values lie in the given interval, as opposed to the substring search, that enables researchers to find only a particular parameter value. The idea behind using semantic search index in this paper is based on the notion of the technology life cycle, which will allow identifying the current state of a particular technology. Existing models of life cycle have been analyzed and a new model has been suggested. The ontology of physical parameters and the life cycle ontology have been developed. The next step has been the development of the algorithm, which uses the corresponding ontologies to split, filter and mark texts and saves the results to the database for further use as search indexes. The Chinese language has been chosen as a hieroglyphic language for conducting this research.}
}
@article{HUANG2024362,
title = {From explainable to interpretable deep learning for natural language processing in healthcare: How far from reality?},
journal = {Computational and Structural Biotechnology Journal},
volume = {24},
pages = {362-373},
year = {2024},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2024.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S2001037024001508},
author = {Guangming Huang and Yingya Li and Shoaib Jameel and Yunfei Long and Giorgos Papanastasiou},
keywords = {Explainable, Interpretable, Deep learning, NLP, Healthcare},
abstract = {Deep learning (DL) has substantially enhanced natural language processing (NLP) in healthcare research. However, the increasing complexity of DL-based NLP necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review of explainable and interpretable DL in healthcare NLP. The term “eXplainable and Interpretable Artificial Intelligence” (XIAI) is introduced to distinguish XAI from IAI. Different models are further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms are the most prevalent emerging IAI technique. The use of IAI is growing, distinguishing it from XAI. The major challenges identified are that most XIAI does not explore “global” modelling processes, the lack of best practices, and the lack of systematic evaluation and benchmarks. One important opportunity is to use attention mechanisms to enhance multi-modal XIAI for personalized medicine. Additionally, combining DL with causal logic holds promise. Our discussion encourages the integration of XIAI in Large Language Models (LLMs) and domain-specific smaller models. In conclusion, XIAI adoption in healthcare requires dedicated in-house expertise. Collaboration with domain experts, end-users, and policymakers can lead to ready-to-use XIAI methods across NLP and medical tasks. While challenges exist, XIAI techniques offer a valuable foundation for interpretable NLP algorithms in healthcare.}
}
@article{PATLAKAS2024102448,
title = {Semantic web-based automated compliance checking with integration of Finite Element analysis},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102448},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102448},
url = {https://www.sciencedirect.com/science/article/pii/S147403462400096X},
author = {Panagiotis Patlakas and Ioannis Christovasilis and Lorenzo Riparbelli and Franco KT Cheung and Edlira Vakaj},
keywords = {Automatic Compliance Checking, Web Ontology Language (OWL), Industry Foundation Classes (IFC), Semantic Web, Finite Element Analysis (FEA)},
abstract = {Automatic Compliance Checking (ACC) is a promising response to the challenges involved in meeting building and planning regulations, and increasingly utilised by researchers in the context of Building Information Modelling (BIM) and the Industry Foundation Classes (IFC). However, engineers often use computational methods, such as Finite Element Analysis (FEA), to solve challenging engineering problems. Such methods have received little attention in the context of ACC and BIM. The work presented here proposes a holistic approach, integrating FEA in the IFC schema, and utilising a Semantic Web-based system for ACC. A top-level ontology, the Building Compliance Ontology (BCO) is proposed, with the aim of covering all aspects of compliance in the Built Environment. One novelty of BCO is that it revolves around modelling statements in the building regulations, as opposed to the more common approach of modelling building components. In order to evaluate the effectiveness and applicability of this process, a real-world problem is used: the compliance to the structural engineering design codes of the structural members of a modular steel structure. An open-source solver is utilised for the FEA, and the IFC model is enriched with its results, using an open-source middleware for the Input/Output. BCO is extended to the field of structural engineering, and appropriate ACC classes and SHACL checks are implemented. Compliance is checked automatically by running SHACL scripts in a Semantic Web environment.}
}
@article{KERRE2025111255,
title = {An instruction dataset for extracting quantum cascade laser properties from scientific text},
journal = {Data in Brief},
volume = {58},
pages = {111255},
year = {2025},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2024.111255},
url = {https://www.sciencedirect.com/science/article/pii/S2352340924012174},
author = {Deperias Kerre and Anne Laurent and Kenneth Maussang and Dickson Owuor},
keywords = {Information extraction, Large language models, Machine learning, Quantum cascade lasers},
abstract = {Quantum Cascade Lasers (QCL) are promising semiconductor lasers, compact and powerful, but of complex design. Availability of structured data of the QCL properties can support data mining activities that seek to understand the relationship between these properties, for instance between the design and performance features. The main open source of QCL data is in scientific text which in most cases is usually unstructured. One of the ways to extract and organize this data is by utilizing Information Extraction techniques. These techniques can accelerate the process of curating QCL properties data from scientific articles for further analysis. One of the main challenges in developing machine learning algorithms for extraction of QCL properties from text is lack of quality training data for these algorithms. Large Language Models (LLMs) have demonstrated great capabilities in materials property extraction from text. They however experience challenges with domain specific properties, for instance the heterostructure and design types in the QCL domain hence for adaptation. In this paper, we present an original instruction dataset for training and evaluation of LLMs for QCL properties extraction from text. The data is generated by augmenting sample sentences from scientific articles with GPT-3.5 instruct with a few shot strategy. The dataset then is manually annotated with the help of QCL experts and is composed of 1300 rows of training examples consisting of an Instruction, Input Text and the Output.}
}
@article{LI2024110697,
title = {Developing an automatic integration approach to generate brick model from imperfect building information modelling},
journal = {Journal of Building Engineering},
volume = {97},
pages = {110697},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.110697},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224022654},
author = {Mingchen Li and Zhe Wang and Gabe Fierro and Chi Hou Cecil Man and Pok Man Patrick So and Kin Fung Calvin Leung},
keywords = {Building information modeling (BIM), Brick schema, Ontology, Data integration, Knowledge graph},
abstract = {Recent advanced architectural algorithms, such as real-time optimization control and fault diagnostics, have garnered significant interest for their ability to reduce building energy consumption and improve equipment maintenance efficiency. However, deploying these algorithms in actual buildings often involves significant manual effort and time to align building data points with algorithmic requirements. In response, standardized ontologies like the Brick Schema have been introduced. This ontology streamlines the representation of building topology, equipment, and data points, as well as algorithm inputs, enabling quicker algorithm integration and deployment. Despite these advancements, manually constructing Brick models from scratch remains time-consuming, laborious, and prone to errors, presenting a new challenge in rapid model development. To overcome these issues, some researchers have turned to universal BIM formats, such as IFC/Revit, for accelerated Brick modeling. Nevertheless, the prevalence of errors in human-generated BIM models often impedes effective conversion. To efficiently convert imperfect IFC models into Brick models, this paper presents a novel BIM to Brick methodology comprising three key steps: IFC inspection and repair, IFC to Brick semantic modeling, and dataset mapping. This method ensures high-quality reuse of even imperfect IFC models. Our case study demonstrates that this method can produce an ideal Brick model from a flawed BIM model, meeting the semantic metadata requirements specified by the BuildingMOTIF tool. This innovative approach substantially reduces the effort required to generate Brick models, effectively handling the inaccuracies and noise inherent in real-world BIM.}
}
@article{ZHANG2025103705,
title = {A knowledge graphs construction method enhanced by multimodal large language model for industrial equipment operation and maintenance},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103705},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103705},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625005981},
author = {Zhengping Zhang and Junyuan Yu and Bo Yang and Kaze Du and Shilong Wang and Xing Qi},
keywords = {Equipment operation and maintenance, Multi-modal knowledge graph, Multimodal large language modals, Attention mechanism},
abstract = {The industrial equipment Operation and Maintenance (O&M) is a core component in ensuring production safety and efficiency, urgently requiring the support of intelligent technologies. Knowledge graphs, which represent equipment and faults in graph structures, are widely utilized to enable efficient association and rapid retrieval of maintenance knowledge, thereby being extensively applied in intelligent decision-making for the equipment O&M. Traditional knowledge graph construction methods, which rely on a single textual modality, are confronted with challenges such as the scarcity of annotated samples, difficulties in dynamically associating old and new equipment, and insufficient parsing of complex equipment relationships. As a result, issues like missing graph entities and broken causal chains are often encountered, thereby negatively impacting the quality of maintenance decision-making. Therefore, a dual-attention model enhanced by multimodal large language models (MllmDA-KGC) is proposed in this paper. Multimodal large language model(MLLM) is introduced to fully utilize multi-modal knowledge from the O&M domain, thereby enabling a more effective understanding and modeling of complex O&M knowledge. As a result, the quality of knowledge graph construction is significantly improved. In the MllmDA-KGC framework, first, QWEN2-VL is introduced into a dual-stream Transformer architecture to achieve dynamic alignment between images and text while supplementing semantics. As a result, the precision of identifying relationships between parts and problem entities in the O&M domain is significantly enhanced; second, the MT-Transformer module is proposed, which integrates causal convolution, dilated convolution, and Memory_Bank mechanisms to achieve cross-modal temporal embedding fusion. As a result, the continuity of associations between new and old parts, as well as the precision of causal chain embeddings, is significantly improved; third, a multimodal dynamic weight attention-guided module is designed, in which weighted key-value guided attention mechanisms are introduced to focus on critical aspects. Schematic diagrams and textual features are fused to enhance the precision of entity relationship modeling between parts and faults; finally, to fully leverage the multimodal understanding capabilities of the MLLM, the image embeddings and positional embeddings generated and marked by MLLM are integrated into the feature embeddings of RoBERTa. Subsequently, CRF and Softmax are combined to accomplish the MNER (Multimodal Named Entity Recognition) and MRE (Multimodal Relation Extraction) tasks, thereby enabling the construction of a multimodal equipment O&M knowledge graph. In this paper, the vehicle O&M dataset from an automobile company was utilized to validate the proposed method. The experimental results showed that the F1 scores of the model in the MNER and MRE tasks reached 88.40% and 93.79%, respectively, demonstrating its effectiveness in constructing a multimodal knowledge graph for equipment O&M. Furthermore, during the process of graph inference, the performance of multimodal graph inference was significantly better than that of the unimodal approach, further confirming the superiority of the multimodal knowledge graph.}
}
@article{GIAGNOLINI20251003,
title = {Comparative insights into semantic archival modelling: evaluating RiC-O and ArchOnto representation capabilities},
journal = {Journal of Documentation},
volume = {81},
number = {4},
pages = {1003-1031},
year = {2025},
issn = {0022-0418},
doi = {https://doi.org/10.1108/JD-12-2024-0310},
url = {https://www.sciencedirect.com/science/article/pii/S0022041825000281},
author = {Lucia Giagnolini and Inês Koch and Francesca Tomasi and Carla {Teixeira Lopes}},
keywords = {RiC-O, ArchOnto, Archival modelling, Linked open data, Semantic web, Archival, Description},
abstract = {Purpose
This study aims to comparatively evaluate two semantic models, ArchOnto (CIDOC CRM based) and Records in Contexts Ontology (RiC-O), for archival representation within the Linked Open Data framework. The research seeks to critically analyse their ability to represent archival documents, events, activities, and provenance through the application on a case study of historical baptism records.
Design/methodology/approach
The study adopted a comparative approach, utilising the two models to represent a dataset of baptism records from a Portuguese parish spanning several centuries. This involved information extraction and conversion processes, transforming XML EAD finding aids into RDF to facilitate more explicit semantic representation and analysis.
Findings
The analysis revealed distinctive strengths and limitations of each semantic model, providing nuanced insights into their respective capacities for archival description. The findings guide cultural heritage institutions in selecting and implementing the most suitable semantic model for their needs and pave the way for semantic alignment between the two models.
Research limitations/implications
Although the case study explored the representation of a wide range of features, potential limitations include the specific contextual constraints of parish records and the need for broader comparative studies across diverse archival contexts.
Originality/value
This paper offers original insights into semantic modelling for archival representations by providing a detailed comparative analysis of two ontological approaches. It offers valuable perspectives for archivists, digital humanities researchers, and cultural heritage professionals seeking to enhance the semantic richness of archival descriptions.}
}
@article{SEZGIN2023,
title = {Extracting Medical Information From Free-Text and Unstructured Patient-Generated Health Data Using Natural Language Processing Methods: Feasibility Study With Real-world Data},
journal = {JMIR Formative Research},
volume = {7},
year = {2023},
issn = {2561-326X},
doi = {https://doi.org/10.2196/43014},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X23000306},
author = {Emre Sezgin and Syed-Amad Hussain and Steve Rust and Yungui Huang},
keywords = {patient-generated health data, natural language processing, named entity recognition, patient health records, text notes, voice, audio real-world data},
abstract = {Background
Patient-generated health data (PGHD) captured via smart devices or digital health technologies can reflect an individual health journey. PGHD enables tracking and monitoring of personal health conditions, symptoms, and medications out of the clinic, which is crucial for self-care and shared clinical decisions. In addition to self-reported measures and structured PGHD (eg, self-screening, sensor-based biometric data), free-text and unstructured PGHD (eg, patient care note, medical diary) can provide a broader view of a patient’s journey and health condition. Natural language processing (NLP) is used to process and analyze unstructured data to create meaningful summaries and insights, showing promise to improve the utilization of PGHD.
Objective
Our aim is to understand and demonstrate the feasibility of an NLP pipeline to extract medication and symptom information from real-world patient and caregiver data.
Methods
We report a secondary data analysis, using a data set collected from 24 parents of children with special health care needs (CSHCN) who were recruited via a nonrandom sampling approach. Participants used a voice-interactive app for 2 weeks, generating free-text patient notes (audio transcription or text entry). We built an NLP pipeline using a zero-shot approach (adaptive to low-resource settings). We used named entity recognition (NER) and medical ontologies (RXNorm and SNOMED CT [Systematized Nomenclature of Medicine Clinical Terms]) to identify medication and symptoms. Sentence-level dependency parse trees and part-of-speech tags were used to extract additional entity information using the syntactic properties of a note. We assessed the data; evaluated the pipeline with the patient notes; and reported the precision, recall, and F1 scores.
Results
In total, 87 patient notes are included (audio transcriptions n=78 and text entries n=9) from 24 parents who have at least one CSHCN. The participants were between the ages of 26 and 59 years. The majority were White (n=22, 92%), had more than one child (n=16, 67%), lived in Ohio (n=22, 92%), had mid- or upper-mid household income (n=15, 62.5%), and had higher level education (n=24, 58%). Out of 87 notes, 30 were drug and medication related, and 46 were symptom related. We captured medication instances (medication, unit, quantity, and date) and symptoms satisfactorily (precision >0.65, recall >0.77, F1>0.72). These results indicate the potential when using NER and dependency parsing through an NLP pipeline on information extraction from unstructured PGHD.
Conclusions
The proposed NLP pipeline was found to be feasible for use with real-world unstructured PGHD to accomplish medication and symptom extraction. Unstructured PGHD can be leveraged to inform clinical decision-making, remote monitoring, and self-care including medical adherence and chronic disease management. With customizable information extraction methods using NER and medical ontologies, NLP models can feasibly extract a broad range of clinical information from unstructured PGHD in low-resource settings (eg, a limited number of patient notes or training data).}
}
@article{WAN2025105929,
title = {Enabling scalable Model Predictive Control design for building HVAC systems using semantic data modelling},
journal = {Automation in Construction},
volume = {170},
pages = {105929},
year = {2025},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105929},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524006654},
author = {Lu Wan and Ferdinand Rossa and Torsten Welfonder and Ekaterina Petrova and Pieter Pauwels},
keywords = {Model Predictive Control, Semantic data modelling, Ontologies, Building Information Modelling, HVAC},
abstract = {Model Predictive Control (MPC) is a promising optimal control technique to reduce the energy consumption of Heating, Ventilation, and Air Conditioning systems in buildings. However, MPC currently involves significant manual efforts in data preparation, control model design, and software interface design. Better semantic representations of buildings, their systems, and telemetry data could help address these challenges. This paper proposes a standard semantic information model and tooling, tailored to BIM software, to streamline MPC design. The approach is tested in an office building, and the generated semantic graph is validated against a use case, where an MPC controller uses Resistance and Capacitance (RC) models that need to be parameterized. The results show that the automatically identified RC models achieve three-hour-ahead temperature predictions for two different rooms within 0.3 °C accuracy. This indicates that semantic data modelling can enable a scalable MPC configuration workflow and more efficient algorithm development and deployment in the future.}
}
@article{JUST20252567,
title = {The Independent Event Log Layer (IELL): Semantic Integration of Industrial IoT Event Logs},
journal = {Procedia Computer Science},
volume = {253},
pages = {2567-2574},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.316},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925003242},
author = {Valentin P. Just and Steindl Gernot and Wolfgang Kastner},
keywords = {IoT, IIoT, Event log, Process-Mining, Ontology, RDF},
abstract = {The Industrial Internet of Things (IIoT) has significantly transformed manufacturing by enabling the integration of physical and digital systems, resulting in extensive data generation. However, extracting actionable insights from this heterogeneous data poses significant challenges due to its distributed nature and the varied architectures and formats from multiple vendors. A unified access method for data processing is essential to overcome these obstacles. This paper introduces the concept of an Independent Event Log Layer (IELL), leveraging the Resource Description Framework (RDF) and ontology-based knowledge representation to standardise and analyse event logs from disparate formats like eXtensible Event Stream (XES) and Comma-Separated Values (CSV). Utilising the RDF Mapping Language (RML), we propose a novel approach to convert event logs into RDF files, creating a unified knowledge base that enhances process mining capabilities. This semantic abstraction facilitates advanced knowledge retrieval and analysis, linking various events and attributes to optimise IIoT processes. A proof-of-concept implementation demonstrates the feasibility of our approach using openly available event log data and RML tooling. The findings underscore the potential of IELL to streamline process mining in IIoT environments, providing unified access for knowledge retrieval and process optimisation.}
}
@article{SHOHAM2024109089,
title = {MedConceptsQA: Open source medical concepts QA benchmark},
journal = {Computers in Biology and Medicine},
volume = {182},
pages = {109089},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109089},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524011740},
author = {Ofir Ben Shoham and Nadav Rappoport},
keywords = {Benchmark, Large Language Models, LLM, Machine learning, Clinical knowledge, Health care},
abstract = {Background:
Clinical data often includes both standardized medical codes and natural language texts. This highlights the need for Clinical Large Language Models to understand these codes and their differences. We introduce a benchmark for evaluating the understanding of medical codes by various Large Language Models.
Methods:
We present MedConceptsQA, a dedicated open source benchmark for medical concepts question answering. The benchmark comprises of questions of various medical concepts across different vocabularies: diagnoses, procedures, and drugs. The questions are categorized into three levels of difficulty: easy, medium, and hard. We conduct evaluations of the benchmark using various Large Language Models.
Results:
Our findings show that most of the pre-trained clinical Large Language Models achieved accuracy levels close to random guessing on this benchmark, despite being pre-trained on medical data. However, GPT-4 achieves an absolute average improvement of 9-11% (9% for few-shot learning and 11% for zero-shot learning) compared to Llama3-OpenBioLLM-70B, the clinical Large Language Model that achieved the best results.
Conclusion:
Our benchmark serves as a valuable resource for evaluating the abilities of Large Language Models to interpret medical codes and distinguish between medical concepts. We demonstrate that most of the current state-of-the-art clinical Large Language Models achieve random guess performance, whereas GPT-3.5, GPT-4, and Llama3-70B outperform these clinical models, despite their primary focus during pre-training not being on the medical domain. Our benchmark is available at https://huggingface.co/datasets/ofir408/MedConceptsQA.}
}
@article{XIE2024100359,
title = {Dynamic knowledge graph approach for modelling the decarbonisation of power systems},
journal = {Energy and AI},
volume = {17},
pages = {100359},
year = {2024},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2024.100359},
url = {https://www.sciencedirect.com/science/article/pii/S2666546824000259},
author = {Wanni Xie and Feroz Farazi and John Atherton and Jiaru Bai and Sebastian Mosbach and Jethro Akroyd and Markus Kraft},
keywords = {Power system modelling, Decarbonisation, Dynamic knowledge graph, Ontology and Semantic Web},
abstract = {This paper presents a dynamic knowledge graph approach that offers a reusable, interoperable, and extensible framework for modelling power systems. Domain ontologies have been developed to support a linked data representation of infrastructure data, socio-demographic data, areal attributes like demand, and models describing power systems. The knowledge graph links the data with a hierarchical representation of administrative regions, supporting geospatial queries to retrieve information about the population within the vicinity of a power plant, the number of power plants, total generation capacity, and demand within specific areas. Computational agents were developed to operate on the knowledge graph. The agents performed tasks including data uploading, updating, retrieval, processing, model construction and scenario analysis. A derived information framework was used to track the provenance of information calculated by agents involved in each scenario. The knowledge graph was populated with data describing the UK power system. Two alternative models of the transmission grid with different levels of structural resolution were instantiated, providing the foundation for the power system simulation and optimisation tasks performed by the agents. The application of the dynamic knowledge graph was demonstrated via a case study that investigates clean energy transition trajectories based on the deployment of Small Modular Reactors in the UK.}
}
@article{LI2025100809,
title = {A review of background, methods, limitations and opportunities of knowledge graph completion},
journal = {Computer Science Review},
volume = {58},
pages = {100809},
year = {2025},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2025.100809},
url = {https://www.sciencedirect.com/science/article/pii/S1574013725000851},
author = {Daiyi Li and Yaoyao Liang and Shenyi Qian and Huaiguang Wu and Wei Jia and Yilong Fu and Yifan Sun},
keywords = {Knowledge graph completion, Embedded-based completion, Path-based completion, Neural network-based completion, Large language model-based completion},
abstract = {Knowledge graph completion (KGC), as a pivotal technology for extracting hidden knowledge from large-scale data, has evolved into a systematic research framework through the development of knowledge graph (KG) technology in recent years. To address the many challenges faced by current research, this study systematically reviews the fundamental theories and methodological systems in the field of KGC, grouping them into four categories: embedding-based, path-based, neural network-based, and large language model (LLM)-based approaches. Current research indicates that traditional closed-domain KGC relies on standard KG embedding or relational path models, which remain effective for completing structured data. However, there are notable limitations in handling unseen entities and relations in open scenarios. With breakthroughs in neural networks and LLMs, open-domain KGC has begun to emerge, although a lack of systematic analysis and classification of model architectures still persists. To address this gap, this review conducts a multi-dimensional academic investigation, clarifying the foundational research landscape and core methodological distinctions, establishing a model classification framework that spans both closed and open domains and integrating mainstream dataset resources within the field. Furthermore, the review explores the challenges and future directions of technological development, including critical issues such as complex knowledge reasoning, improvements in domain adaptability improvement, and the deep integration of LLMs with KGs, providing theoretical foundations and practical references to guide subsequent research efforts.}
}
@article{SEDELL2021162,
title = {No fly zone? Spatializing regimes of perceptibility, uncertainty, and the ontological fight over quarantine pests in California},
journal = {Geoforum},
volume = {123},
pages = {162-172},
year = {2021},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2019.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0016718519301162},
author = {Jennifer K. Sedell},
keywords = {Quarantine pests, Invasive species management, Eradication, Agricultural trade, California},
abstract = {For nearly 40 years, California has been periodically gripped by controversies over government efforts to protect agricultural trade by eradicating invasive insects in cities. The analysis focuses on one thread that has linked and stabilized these controversies: disagreement over whether or not the local population of invasive insects is already established and thus ineradicable. Examining the controversies over programs to eradicate the Mediterranean fruit fly in the Los Angeles basin (1980 to present), light brown apple moth on California’s north coast (2007–2010) and the Japanese beetle in the Sacramento Area (2011–2016), I describe two competing models for understanding invasive insect populations. Through a comparison of these regimes of perceptibility, to borrow Michelle Murphy’s term, I demonstrate the importance of temporal and spatial scales in determining pest presence and absence. Further, I argue that the dominant regulatory regime of perceptibility, the Multiple and Recent Incursion (MRI) model, constructs certainty in trade relationships by re-establishing production areas as pest-free through the insistence that the insects are invasive, out of the ordinary, and temporary. The competing regime of perceptibility, the Sub-Detectable Established Local Population (SELP) model, challenges the construction of certainty on which the MRI both relies and helps produce. In doing so, I show that the SELP model threatens California’s position with trade partners but also opens up possibilities for new agro-ecological and political arrangements.}
}
@article{BRANNSTROM2024101203,
title = {A formal understanding of computational empathy in interactive agents},
journal = {Cognitive Systems Research},
volume = {85},
pages = {101203},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101203},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001377},
author = {Andreas Brännström and Joel Wester and Juan Carlos Nieves},
keywords = {Computational empathy, Conversational agents, Human–agent interaction, Knowledge engineering},
abstract = {Interactive software agents, such as chatbots, are progressively being used in the area of health and well-being. In such applications, where agents engage with users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding agents’ empathic capabilities. In the current state-of-the-art, there are no tools to do that. In order to understand empathic capabilities in interactive software agents, we need a precise notion of empathy. The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition. Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in interactive agents for health and well-being, a formal definition—an ontology—of empathy is developed. We present the potential of the formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa. Our findings suggest that our definition captures necessary conditions for assessing empathy in interactive agents, and how it can uncover and explain trends in changing perceptions of empathy over time. The definition, implemented in Web Ontology Language (OWL), may serve as an automated tool, enabling systems to recognize empathy in interactions—be it an interactive agent evaluating its own empathic performance or an intelligent system assessing the empathic capability of its interlocutors.}
}
@article{WANG2025112189,
title = {Improving knowledge management in building engineering with hybrid retrieval-augmented generation framework},
journal = {Journal of Building Engineering},
volume = {103},
pages = {112189},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112189},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225004255},
author = {Zhiqi Wang and Zhongcun Liu and Weizhen Lu and Lu Jia},
keywords = {Retrieval augmented generation, Large language model, Knowledge engineering, Risk and error recognition, Reasoning},
abstract = {Domain knowledge is a critical asset that provides the foundation for a range of building activities, including but not limited to design, construction, operation, and maintenance. However, existing approaches to knowledge engineering often require extensive up-front work and are limited in generalizability. In recent years, large language models (LLMs) have undergone significant growth and demonstrated notable generalization and reasoning capabilities. Nevertheless, the deployment of LLMs in the field of buildings has been constrained by several factors, including hallucinations, a lack of domain-specific knowledge, difficulties in updating knowledge, and data security concerns. This study aims to develop an improved retrieval augmented generation (RAG) framework based on domain characteristics for general, intelligent, and scalable AI-based applications. The primary advancement is the proposal of a hybrid index based on vectors, a property graph, and keywords, accompanied by a corresponding hybrid retrieval strategy. To evaluate the proposed framework comprehensively, three tasks were designed and executed: basic queries, risk and error recognition in complex queries, and open text generation. The results show that the improved RAG significantly outperforms the baseline and the native LLM in all three tasks. This study advances the integration of domain knowledge with generative AI and demonstrates the potential of LLM-RAG-based solutions for application in building lifecycle management.}
}
@article{CELINO2025100850,
title = {Procedural knowledge management in Industry 5.0: Challenges and opportunities for knowledge graphs},
journal = {Journal of Web Semantics},
volume = {84},
pages = {100850},
year = {2025},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2024.100850},
url = {https://www.sciencedirect.com/science/article/pii/S1570826824000362},
author = {Irene Celino and Valentina Anita Carriero and Antonia Azzini and Ilaria Baroni and Mario Scrocca},
keywords = {Procedural knowledge, Industry 5.0, Knowledge graphs, Artificial Intelligence},
abstract = {With digital transformation, industrial companies today are facing the challenges to change and innovate their business, by leveraging digital technologies and tools to support their processes and their operations. One of their main challenges is the management of the company knowledge, especially when tacit and owned by industry workers. In this paper, we illustrate how knowledge graphs can be the turning point to allow industry workers digitize and exploit the knowledge about the “what”, the “how” and the “why” of their everyday activities. In particular, we focus on the “how” by illustrating the challenges related to procedural knowledge management, i.e., the knowledge about processes and workflows that employees need to follow, and comply with, to correctly execute their tasks, in order to improve efficiency and effectiveness, to reduce risks and human errors and to optimize operations. We also explain the relationship in this context between knowledge graphs and sub-symbolic AI approaches.}
}
@article{ABUSALIH2018949,
title = {Twitter mining for ontology-based domain discovery incorporating machine learning},
journal = {Journal of Knowledge Management},
volume = {22},
number = {5},
pages = {949-981},
year = {2018},
issn = {1367-3270},
doi = {https://doi.org/10.1108/JKM-11-2016-0489},
url = {https://www.sciencedirect.com/science/article/pii/S1367327018000017},
author = {Bilal Abu-Salih and Pornpit Wongthongtham and Chan {Yan Kit}},
keywords = {Ontology, Machine learning, Twitter mining, Domain discovery, Domain-based trustworthiness},
abstract = {Purpose
This paper aims to obtain the domain of the textual content generated by users of online social network (OSN) platforms. Understanding a users’ domain (s) of interest is a significant step towards addressing their domain-based trustworthiness through an accurate understanding of their content in their OSNs.
Design/methodology/approach
This study uses a Twitter mining approach for domain-based classification of users and their textual content. The proposed approach incorporates machine learning modules. The approach comprises two analysis phases: the time-aware semantic analysis of users’ historical content incorporating five commonly used machine learning classifiers. This framework classifies users into two main categories: politics-related and non-politics-related categories. In the second stage, the likelihood predictions obtained in the first phase will be used to predict the domain of future users’ tweets.
Findings
Experiments have been conducted to validate the mechanism proposed in the study framework, further supported by the excellent performance of the harnessed evaluation metrics. The experiments conducted verify the applicability of the framework to an effective domain-based classification for Twitter users and their content, as evident in the outstanding results of several performance evaluation metrics.
Research limitations/implications
This study is limited to an on/off domain classification for content of OSNs. Hence, we have selected a politics domain because of Twitter’s popularity as an opulent source of political deliberations. Such data abundance facilitates data aggregation and improves the results of the data analysis. Furthermore, the currently implemented machine learning approaches assume that uncertainty and incompleteness do not affect the accuracy of the Twitter classification. In fact, data uncertainty and incompleteness may exist. In the future, the authors will formulate the data uncertainty and incompleteness into fuzzy numbers which can be used to address imprecise, uncertain and vague data.
Practical implications
This study proposes a practical framework comprising significant implications for a variety of business-related applications, such as the voice of customer/voice of market, recommendation systems, the discovery of domain-based influencers and opinion mining through tracking and simulation. In particular, the factual grasp of the domains of interest extracted at the user level or post level enhances the customer-to-business engagement. This contributes to an accurate analysis of customer reviews and opinions to improve brand loyalty, customer service, etc.
Originality/value
This paper fills a gap in the existing literature by presenting a consolidated framework for Twitter mining that aims to uncover the deficiency of the current state-of-the-art approaches to topic distillation and domain discovery. The overall approach is promising in the fortification of Twitter mining towards a better understanding of users’ domains of interest.}
}
@article{KR2025129596,
title = {Knowledge Graph Life Cycle for Cognitive Agents – A Case Study on Automated Negotiation in Smart Grids},
journal = {Expert Systems with Applications},
pages = {129596},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.129596},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425032117},
author = {Dan E. Kr and Ernesto C. Martínez},
keywords = {Knowledge Graph, Cognitive Agents, Ontology, Graph Data Science, Context-Aware Automated Negotiation, Smart Grid},
abstract = {Knowledge Graphs (KGs) can enhance cognitive artificial agents by improving their semantic understanding, adaptability to dynamic contexts, explainability, continuous learning, and informed, case-based decision-making. However, the development of any KG rarely follows an explicit and structured procedure to improve its consistency, modifiability, and interoperability. This ultimately restricts the utility of KGs for real-world agentic AI systems. To overcome this limitation, this paper proposes a life cycle for developing and evolving KGs in domain-specific applications, encompassing three phases to i) conceptualize the problem domain and competency questions, ii) formalize a schema using ontologies, and iii) implement the KG to foster agents’ cognition through queries and graph data science. The proposed approach is validated with a case study on context-aware automated negotiations within smart grids, where agents negotiate for energy trading while considering private and contextual circumstances. Agents may take actions with or without the aid of a KG, and can adopt one of three negotiation strategy configurations: heuristic, metaheuristic, or reinforcement learning-based. Negotiation outcomes consistently indicate that, regardless of the configuration employed, the use of a KG improves agents’ rewards by at least 1.21% and up to 90.91%. Results highlight that the proposed life cycle enables the integration of contextual and domain-specific data and metadata into a KG that enhances agents’ learning, while also allowing for the development and selection of relevant queries and graph data science algorithms to improve the strategic behavior of negotiation agents with cognitive abilities.}
}
@article{TRILLO2024452,
title = {A Group Decision-Making Approach Leveraging Preference Relations Derived from Large Language Model},
journal = {Procedia Computer Science},
volume = {242},
pages = {452-459},
year = {2024},
note = {11th International Conference on Information Technology and Quantitative Management (ITQM 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.08.161},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924018805},
author = {José Ramón Trillo and María Ángeles Martínez and Sławomir Zadrożny and Janusz Kacprzyk and Enrique Herrera-Viedma and Francisco Javier Cabrerizo},
keywords = {Sentiment Analysis, Large Language Model, Group Decision-Making, Detection System, Consensus Method},
abstract = {Group decision-making involves selecting among limited options. Experts share their perspectives in a comparative debate but then evaluate alternatives using preference relations, which can result in inconsistencies between their expressions and assessments. To address this, a method is proposed that automates the generation of these relationships from the debate comments, classifying them into positive and negative using sentiment analysis, namely with the Large Language Model. A new operator is introduced that weights these comments to calculate preference relations. Furthermore, modification of the relationships is allowed if the experts so wish. Moreover, another operator is incorporated that adjusts the weight of each expert according to his or her active participation in the discussion, assigning more weight to those who contribute more comments. Finally, this innovative method promotes coherence and equal participation in group decision-making by employing an innovative sentiment analysis detection system.}
}
@article{KABAL20242617,
title = {Enhancing Domain-Independent Knowledge Graph Construction through OpenIE Cleaning and LLMs Validation},
journal = {Procedia Computer Science},
volume = {246},
pages = {2617-2626},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.436},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024761},
author = {Othmane Kabal and Mounira Harzallah and Fabrice Guillet and Ryutaro Ichise},
keywords = {Knowledge graph, From raw text, Domain-independent building, Knowledge graph construction pipeline, LLMs based validation, Information Extraction, LLMs for KG},
abstract = {In the challenging context of Knowledge Graph (KG) construction from text, traditional approaches often rely on Open Information Extraction (OpenIE) pipelines. However, they are prone to generating many incorrect triplets. While domain specific Named Entity Recognition (NER) is commonly used to enhance the results, it compromises the domain independence and misses crucial triplets. To address these limitations, we introduce G-T2KG, a novel pipeline for KG construction that aims to preserve the domain independence while reducing incorrect triplets, thus offering a cost-effective solution without the need for domain-specific adaptations. Our pipeline utilizes state-of-the-art OpenIE combined with both a noun phrase-based cleaning and a LLMs based validation. It is evaluated using gold standards in two distinct domains (i.e., computer science and music) that we have constructed in the context of this study. On computer science corpus, the experimental results demonstrate a higher recall as compared to state-of-the-art approaches, and a higher precision notably increased by the integration of LLMs. Experiments on the music corpus show good performance, underscoring the versatility and effectiveness of G-T2KG in domain-independent KG construction.}
}
@article{AMITH2025,
title = {Rendering knowledge graphs from aerospace dentistry processes for clinical decision support systems},
journal = {Acta Astronautica},
year = {2025},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2025.08.023},
url = {https://www.sciencedirect.com/science/article/pii/S0094576525005247},
author = {Muhammad “Tuan” Amith and Jessica Vu and Serena Hou and Vinu Sista and Yang Gong and Ronak Shah and Ana C. Neumann},
keywords = {Dental emergencies, Aerospace dentistry, Process modeling, Health informatics, Knowledge graphs, Ontology},
abstract = {NASA and commercial space flight companies expect to make long-term missions into deep space with the Artemis project and Mars mission. Health care in deep space is a priority as the missions require limited crew and equipment to accommodate astronauts’ needs. Analyzing the probable clinical procedures and workflows for this setting is needed to ensure quality care and safety. This paper introduces Business Process Modeling and Notation (BPMN) and OWL2-based modeling to represent aerospace dental workflows for low-earth orbit and potential deep space missions. We developed BPMN process models and created ontology-based knowledge graph representations for individual dental procedures using a semi-automated processing pipeline. The output of this project yielded four OWL2-based knowledge graphs that describe semantic descriptions of a dental abscess, extractions, dislodged restorations, and dental trauma. With computable knowledge graphs of these processes, we can integrate these models for software applications for remote clinical tools, like decision support systems. Future work will involve extending these prototype knowledge graphs and developing knowledge base applications that leverage these process artifacts.}
}
@article{JUST2024102883,
title = {Natural language processing for innovation search – Reviewing an emerging non-human innovation intermediary},
journal = {Technovation},
volume = {129},
pages = {102883},
year = {2024},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2023.102883},
url = {https://www.sciencedirect.com/science/article/pii/S0166497223001943},
author = {Julian Just},
keywords = {Natural language processing, Innovation search, Innovation intermediation, Front-end of innovation, AI-based innovation management, Systematic literature review},
abstract = {Applying artificial intelligence (AI), especially natural language processing (NLP), to harness large amounts of information from patent databases, online communities, social media, or crowdsourcing platforms is becoming increasingly popular to help organizations find promising solutions. In the era of non-human innovation intermediaries, we should begin to view NLP not only as a useful technology applied in different innovation practices but also as an intermediary orchestrating valuable information. Previous research has not taken this perspective, and knowledge about its intermediation activities and functions is limited. This study reviews 167 academic articles to better understand how NLP approaches can enrich intermediation in early-stage innovation search. It identifies 18 distinctive innovation practices taking over activities like forecasting trends, illustrating technology and idea landscapes, filtering out distinctive contributions, recombining domain-specific and analogous knowledge, or matching problems with solutions. While certain NLP capabilities complement each other, the analysis shows that the choice of the most appropriate approach depends on the characteristics of the innovation practice. Innovation researchers and practitioners should rethink current roles and responsibilities in AI-based innovation processes. As seen in the recent emergence of large language models (LLMs), the rapidly evolving field offers many future research opportunities and practical benefits.}
}
@article{ZACHARIS2025104037,
title = {Optimising AI models for intelligence extraction in the life cycle of Cybersecurity Threat Landscape generation},
journal = {Journal of Information Security and Applications},
volume = {90},
pages = {104037},
year = {2025},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2025.104037},
url = {https://www.sciencedirect.com/science/article/pii/S2214212625000754},
author = {Alexandros Zacharis and Razvan Gavrila and Constantinos Patsakis and Christos Douligeris},
keywords = {Machine learning, Cybersecurity Threat Landscape, Threat intelligence, Named entity recognition, Large language model},
abstract = {The increasing complexity and frequency of cyber attacks in the modern digital environment demand continuous vigilance and proactive strategies to manage risks effectively. Conventional approaches to generating intelligence for Cybersecurity Threat Landscape (CTL) reports are often resource-intensive and time-consuming, as they depend on manual identification, collection, and analysis of relevant electronically stored information (ESI). This study investigates the potential of artificial intelligence (AI) to transform CTL generation, reducing manual classification and tagging while improving efficiency and accuracy. We focus on evaluating the classification performance of several Large Language Models (LLMs), including Gemini 1.5 Pro, GPT-4o, but also Bidirectional Encoder Representations from Transformers (BERT) based models like TRAM and TTPHunter along with custom Named Entity Recognition (NER) models, using a dataset previously annotated by human experts. Our findings demonstrate the promising results of AI-driven intelligence extraction for CTL report generation, streamlining cybersecurity operations by automating routine tasks and providing precise and timely threat intelligence. However, the variability in model performance suggests the importance of hybrid approaches needed to achieve the accuracy of human annotation. Therefore, we propose a novel voting agreement-based methodology, harvesting the most from the combined AI model capabilities to effectively address the complexities of cybersecurity threat intelligence extraction.}
}
@article{SATO2025102763,
title = {Understanding partnership principles: Seeking meaning through metaphor analysis},
journal = {International Journal of Educational Research},
volume = {134},
pages = {102763},
year = {2025},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2025.102763},
url = {https://www.sciencedirect.com/science/article/pii/S0883035525002368},
author = {Mistilina Sato and Maraea Garisau Turketo and Chris Astall},
keywords = {School-university partnerships, University-school partnerships, Initial teacher education, Partnership principles, Metaphor analysis, New Zealand},
abstract = {The purpose of this study is to understand how school, early childhood, and community partners understand the nature of a set of Partnership Principles on which university-school partnerships were founded. Ten university partners (principals, teachers, and a student) were interviewed to elicit their understanding of the principles for partnership: authentic, reciprocal, equitable, enduring, and innovative. Interviews were coded using metaphor analysis, identifying orientational, structural, and ontological metaphors used by participants to describe both the individual principles and their general understanding of the partnership endeavours. Results show that the partners use a robust set of metaphors to describe each of the partnership principles, with the exception of the principle of equitable. This principle was identified primarily as the central “space” of the partnership work and as a necessary direction “forward” for the work. The greatest divergence between the university and partnership language was around the principle of innovation. Partners expressed more urgency in wanting faster-paced and action-oriented work related to this principle. Overall, metaphorical language centred on three key themes: a common purpose of working equitably in order to reach equal educational outcomes for children and students; the importance of people and relationships as what make a partnership sustainable and enduring; and the necessity for action toward change. The ontological metaphors used by both Māori and non-Māori partners drew on Māori world views to a great extent. This affirms practices of indigenising university-based education and provides insight into how to further engage in equity-focused education in this New Zealand context.}
}
@article{ZHANG2025110157,
title = {A survey on learning with noisy labels in Natural Language Processing: How to train models with label noise},
journal = {Engineering Applications of Artificial Intelligence},
volume = {146},
pages = {110157},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110157},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625001575},
author = {Han Zhang and Yazhou Zhang and Jiajun Li and Junxiu Liu and Lixia Ji},
keywords = {Natural Language Processing, Learning with noisy labels, Deep learning, Label noise},
abstract = {When applying deep neural network language models to related systems (e.g., question answering systems, chatbots, and intelligent assistants), many datasets contain different types or degrees of label noise. Label noise can lead to a decline in model performance and an increase in resource consumption. Therefore, learning with noisy labels is becoming an important task in Natural Language Processing (NLP). This paper aims to collect, analyze, and evaluate methods for learning with label noise in NLP. First, we analyze the relationship between data feature extraction, prediction output, and optimization in the context of noise robustness to help researchers understand the mechanisms behind noise generation. Based on this, we classified the noise processing methods into five types according to the training process: feature vector, transition matrix, prediction confidence, loss improvement, and data weighting. We analyze each method and conduct a systematic evaluation across six metrics. In addition, we summarized the commonly used resources such as datasets, open source codes, etc. Finally, we also analyzed the challenges faced in current research and the potential opportunities. As a comprehensive survey, this work will help researchers and industry developers to understand the current state of research and unique challenges facing label-noise learning, which facilitate the selection and combination of different methods in applications to further advancements.}
}
@incollection{MILANO2025233,
title = {Computing Languages for Bioinformatics: R},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {233-239},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00030-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027000300},
author = {Marianna Milano},
keywords = {Bioinformatics, Computing language, R software},
abstract = {Bioinformatic analyses involve different tasks and processes. In order to manage various bioinformatics applications, different programs have been written by using various available computing languages. The languages used to tackle bioinformatics problem and related analysis are, for instance, R, a statistical programming language, scripting languages such as Perl and Python, and a compiled languages such as C, C++, Java. Among these, R is becoming one of the most widely used software tools for bioinformatics. This is mainly due to its flexibility, and data handling and modeling capabilities.}
}
@article{GAN2023106660,
title = {Knowledge graph construction based on ship collision accident reports to improve maritime traffic safety},
journal = {Ocean & Coastal Management},
volume = {240},
pages = {106660},
year = {2023},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2023.106660},
url = {https://www.sciencedirect.com/science/article/pii/S0964569123001850},
author = {Langxiong Gan and Beiyan Ye and Zhiqiu Huang and Yi Xu and Qiaohong Chen and Yaqing Shu},
keywords = {Ship collision accident knowledge graph (SCAKG), Ontology module, Ship accident reports, Maritime traffic safety, Natural language processing},
abstract = {As an important data source, marine accident investigation reports are frequently used for accident analysis. However, it is hard to extract effective information since key knowledge is normally hidden in large blocks of text. In most cases, the collection of accident-related data is done manually. In this paper, a new knowledge graph construction approach to explore ship collision accidents is proposed, aiming to show the correlation among important factors of the accidents. In this research, 241 investigation reports on ship collision accidents from 2018 to 2021 published on the official website of the China Maritime Safety Administration (CMSA) were collected and analyzed. Then, the ship collision accident ontology module is constructed. According to the ontology information in the accident reports, entities were divided into context-based metadata and content-based metadata, which were used for describing different types of data. To extract the information for the accident report with semi-structured data, an information extraction module based on ontology was proposed. In this process, natural language processing (NLP) was used to obtain text information about the ontology. On this basis, the Ship Collision Accident Knowledge Graph (SCAKG) including 910 entity nodes and 1920 relation edges was constructed and stored in the graph database Neo4j. Finally, two case retrievals were conducted using the SCAKG to show the potential utilization of the method. The results show the effectiveness of the proposed approach in terms of discovering the internal relationship of the accident and could be used to expedite the judicial process, which simplifies the process of marine accident investigation.}
}
@article{SALEEM2025128684,
title = {PassionNet: An innovative framework for duplicate and conflicting requirements identification},
journal = {Expert Systems with Applications},
volume = {293},
pages = {128684},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.128684},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425023024},
author = {Summra Saleem and Muhammad Nabeel Asim and Andreas Dengel},
keywords = {Software requirements, Large language models, Conflict/duplicate requirements, Similarity knowledge},
abstract = {Early detection of duplicate and conflicting requirements in software development lifecycle is crucial to achieve software project efficiency, quality, and market success. Primarily, duplicate detection requires identifying semantic equivalence, intent alignment, functional overlap, and domain-specific terminology variations between differently worded requirements. Whereas, conflict detection demands recognising logical contradictions, constraint violations, resource conflicts and temporal incompatibilities between requirements. To handle multi-dimensional demands of two different task types, researchers have developed 32 AI based duplicate and conflicting requirement detection predictors. However, despite the utility of sophisticated large language models (LLMs) and sampling techniques, existing approaches significantly lack in performance because they fail to comprehensively handle multi-dimensional demands of both tasks. To address these gaps, this paper presents a modular framework “PassionNet” which implements a novel strategy of integrating 10 different multi-dimensional similarity assessments with the contextual understanding of 8 unique language model variants. The framework enables three distinct pipeline types: language model-based pipelines that capture semantic intent, similarity knowledge-driven pipelines that detect lexical, structural and distributional patterns, and hybrid pipelines that combine both approaches to simultaneously assess all dimensions of requirement relationships. Our experimental evaluation of 760 pipelines across six public datasets demonstrates that hybrid pipelines outperform the other two approaches in terms of F1-score as compared to state-of-the-art methods. Specifically, the hybrid pipeline achieves an improvement in F1-score of approximately 4 % on the WorldVista dataset, 5 % on the UAV dataset and 3 % on the Pure dataset as compared to the state-of-the-art models. Statistical validation through t-tests confirms the significance of these improvements (p < 0.1 with 10 permutations, approaching zero with 1000 permutations). The results provide empirical evidence that effective requirement analysis requires simultaneously assessing semantic, lexical, structural, and logical dimensions of requirements rather than focusing on isolated aspects. To facilitate software engineers, researchers and practitioners, PassionNet web application is deployed at https://sds_requirement_engineering.opendfki.de/.}
}
@article{FUTIA2018187,
title = {Training Neural Language Models with SPARQL queries for Semi-Automatic Semantic Mapping},
journal = {Procedia Computer Science},
volume = {137},
pages = {187-198},
year = {2018},
note = {Proceedings of the 14th International Conference on Semantic Systems 10th – 13th of September 2018 Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.09.018},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918316235},
author = {Giuseppe Futia and Antonio Vetro and Alessio Melandri and Juan Carlos {De Martin}},
keywords = {Knowledge Graph, Semantic Mapping, SPARQL, Neural Language Model},
abstract = {Knowledge graphs are labeled and directed multi-graphs that encode information in the form of entities and relationships. They are gaining attention in different areas of computer science: from the improvement of search engines to the development of virtual personal assistants. Currently, an open challenge in building large-scale knowledge graphs from structured data available on the Web (HTML tables, CSVs, JSONs) is the semantic integration of heterogeneous data sources. In fact, such diverse and scattered information rarely provide a formal description of metadata that is required to accomplish the integration task. In this paper we propose an approach based on neural networks to reconstruct the semantics of data sources to produce high quality knowledge graphs in terms of semantic accuracy. We developed a neural language model trained on a set of SPARQL queries performed on knowledge graphs. Through this model it is possible to semi-automatically generate a semantic map between the attributes of a data source and a domain ontology.}
}
@article{NGUYEN2025127688,
title = {Developing an automated framework for eco-label information categorization using web crawling and Natural Language Processing techniques},
journal = {Expert Systems with Applications},
volume = {282},
pages = {127688},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127688},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425013107},
author = {Ho Anh Thu Nguyen and Duy Hoang Pham and Byeol Kim and Yonghan Ahn and Nahyun Kwon},
keywords = {Green building material, Eco-label, Information management, Machine learning, Natural language processing},
abstract = {Eco-labels are extensively employed to assess the environmental performance of building materials. However, their management is often fragmented across disparate online databases with inconsistent data structures, presenting significant challenges for efficient information acquisition and management. This study explores the application of web crawling techniques, Natural Language Processing (NLP), and machine learning (ML) models to collect and categorize eco-label information, with the objective of advancing the automation of information management processes. The results demonstrate that the categorization models exhibit high performance, achieving F1-scores exceeding 0.95 on the test set and at least 0.76 when validating datasets incorporating temporally updated information. However, the limited availability of data for certain eco-labels, such as Forest Stewardship Council certification and Green Screen, substantially degrades model performance with updated data. Notably, traditional ML models leveraging manual feature engineering outperform deep learning models with automatic feature extraction when applied to web-crawled data. Furthermore, the TF-IDF feature extraction technique surpasses other n-gram-based approaches, with model performance declining as n-gram length increases. This study establishes a systematic framework that informs the selection of reliable data sources, feature engineering strategies, and ML algorithms for integrating web crawling, thereby enhancing the automation of eco-label information management.}
}
@article{GOFF2020101776,
title = {Visionary evaluation: Approaching Aboriginal ontological equity in water management evaluation},
journal = {Evaluation and Program Planning},
volume = {79},
pages = {101776},
year = {2020},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2019.101776},
url = {https://www.sciencedirect.com/science/article/pii/S0149718919300242},
author = {Susan Goff},
keywords = {Standpoint Theory, Participatory Research, First Nations Peoples, Cross-cultural engagement, Natural resource management, Systemic critical thinking},
abstract = {The 2017 Traditional Owner evaluation of the implementation of the Murray-Darling Basin Plan developed an approach to evaluation that tested the use of Standpoint Theory in the field of natural resource management. This methodological choice was intended to enable First Nation approaches to data generation and use in equal measure to non-indigenous approaches. The method is implemented as a nested, up-hierarchy of scale, enabling a pan-optican dimension of vision from "below" and "above". The paper does not present the evaluative results regarding the implementation of the Plan because that information is co-owned by the participating Nations for their uses. Instead, and in respect of that arrangement, the paper presents the evaluation practices funded by the Murray-Darling Basin Authority. The methodology was negotiated and implemented with the Nations in the pilot study as a co-production across cultural boundaries. The approach was then evaluated by the participants, and these results are reported. All those reviewing the methodology were directly involved in some aspect of the evaluation, 64 % of whom identified as Traditional Owners, 67 % of whom were involved in high level decision-making about the evaluation approach. Traditional Owners rated cultural competence of the tested approach at 68 %, the benefits of the approach at 75 %, satisfaction with the standard of the evaluation at 72 %, and satisfaction with complying with the Basin Plan’s requirements for evaluation at 78 %. Recommendations for broader engagement and better science communication are made.}
}
@article{PHOGAT202555,
title = {ZFP-CanPred: Predicting the effect of mutations in zinc-finger proteins in cancers using protein language models},
journal = {Methods},
volume = {235},
pages = {55-63},
year = {2025},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2025.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S1046202325000295},
author = {Amit Phogat and Sowmya Ramaswamy Krishnan and Medha Pandey and M. Michael Gromiha},
keywords = {ZFP-CanPred, Zinc-fingers, Cancer, Driver, Neutral, Mutations, Neural network, Protein language model},
abstract = {Zinc-finger proteins (ZNFs) constitute the largest family of transcription factors and play crucial roles in various cellular processes. Missense mutations in ZNFs significantly alter protein-DNA interactions, potentially leading to the development of various types of cancers. This study presents ZFP-CanPred, a novel deep learning-based model for predicting cancer-associated driver mutations in ZNFs. The representations derived from protein language models (PLMs) from the structural neighbourhood of mutated sites were utilized to train ZFP-CanPred for differentiating between cancer-causing and neutral mutations. ZFP-CanPred, achieved a superior performance with an accuracy of 0.72, F1-score of 0.79, and area under the Receiver Operating Characteristics (ROC) Curve (AUC) of 0.74, on an independent test set. In a comparative analysis against 11 existing prediction tools using a curated dataset of 331 mutations, ZFP-CanPred demonstrated the highest AU-ROC of 0.74, outperforming both generic and cancer-specific methods. The model’s balanced performance across specificity and sensitivity addresses a significant limitation of current methodologies. The source code and other related files are available on GitHub at https://github.com/amitphogat/ZFP-CanPred.git. We envisage that the present study contributes to understand the oncogenic processes and developing targeted therapeutic strategies.}
}
@article{PATRICIO202571,
title = {A two-step concept-based approach for enhanced interpretability and trust in skin lesion diagnosis},
journal = {Computational and Structural Biotechnology Journal},
volume = {28},
pages = {71-79},
year = {2025},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2025.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S2001037025000418},
author = {Cristiano Patrício and Luís F. Teixeira and João C. Neves},
keywords = {Concept bottleneck models, Vision-language models, Interpretability, Skin cancer, Dermoscopy},
abstract = {The main challenges hindering the adoption of deep learning-based systems in clinical settings are the scarcity of annotated data and the lack of interpretability and trust in these systems. Concept Bottleneck Models (CBMs) offer inherent interpretability by constraining the final disease prediction on a set of human-understandable concepts. However, this inherent interpretability comes at the cost of greater annotation burden. Additionally, adding new concepts requires retraining the entire system. In this work, we introduce a novel two-step methodology that addresses both of these challenges. By simulating the two stages of a CBM, we utilize a pretrained Vision Language Model (VLM) to automatically predict clinical concepts, and an off-the-shelf Large Language Model (LLM) to generate disease diagnoses grounded on the predicted concepts. Furthermore, our approach supports test-time human intervention, enabling corrections to predicted concepts, which improves final diagnoses and enhances transparency in decision-making. We validate our approach on three skin lesion datasets, demonstrating that it outperforms traditional CBMs and state-of-the-art explainable methods, all without requiring any training and utilizing only a few annotated examples. The code is available at https://github.com/CristianoPatricio/2-step-concept-based-skin-diagnosis.}
}
@article{STELLATO2023105888,
title = {LegalHTML: Semantic mark-up of legal acts using web technologies},
journal = {Computer Law & Security Review},
volume = {51},
pages = {105888},
year = {2023},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2023.105888},
url = {https://www.sciencedirect.com/science/article/pii/S0267364923000985},
author = {Armando Stellato and Manuel Fiorelli},
keywords = {Legal acts, Legal document, Consolidation, Metadata, HTML, Semantic web, Ontologies, Official journal, Law publishing},
abstract = {We introduce here LegalHTML, an extension of the HTML language thought for representing legal acts. LegalHTML has been conceived in the context of an exploratory study conducted for the Publications Office of the European Union, with the objective of overcoming the proliferation of formats for the electronic redaction of legal acts, dedicated to different steps of the editorial process (e.g. first draft, content editing, proof reading, introducing semantics, publishing) and of realizing a model and a language that could bind all processes and exigencies under a common umbrella. LegalHTML satisfies these requirements by providing an explicit domain language addressing all structural aspects of an act, such as articles, paragraphs, items, references and an associated ontology (foreseeing both inline annotations through RDFa and explicit RDF code within script elements) providing rich semantics to describe the editorial and jurisdictional history of the act and to insert references to entities of the domain. Being based on HTML, presentation is also offered by the same language, an aspect missing from all most notable standards for the legal domain. Furthermore, LegalHTML addresses consolidation of an act and its subsequent modifications into a single document using a tree-based representation of the original content and of its modified versions. Finally, alongside the language & ontology, we implemented a CSS stylesheet for the default rendering of LegalHTML documents and a JavaScript file imbuing documents with an API supporting TOC generation, footnote cross-references and the said point-in-time visualization of consolidated legal acts.}
}
@incollection{BERMAN2020187,
title = {6 - Drawing inferences from classifications and ontologies},
editor = {Jules J. Berman},
booktitle = {Logic and Critical Thinking in the Biomedical Sciences},
publisher = {Academic Press},
pages = {187-208},
year = {2020},
isbn = {978-0-12-821364-3},
doi = {https://doi.org/10.1016/B978-0-12-821364-3.00006-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128213643000061},
author = {Jules J. Berman},
keywords = {Rules of classification, Ontology, Ontologic competence, Paradoxes of classification, Phylogenetics},
abstract = {Too often, scientists think of classifications as an organizational tool whose sole purpose is the facilitation of data retrieval. This is simply not true. Classifications have a feature that computer scientists refer to as “competence,” the ability to draw inferences and to make predictions about the behavior and properties of the members of their domain. Competence is achieved by the logical rules that dictate how classifications are designed and by the simplicity of classifications that permit humans to comprehend how millions of data objects can be encapsulated within a few dozen-related classes. This chapter provides multiple examples wherein important scientific discoveries have been inferred from competent classifications.}
}
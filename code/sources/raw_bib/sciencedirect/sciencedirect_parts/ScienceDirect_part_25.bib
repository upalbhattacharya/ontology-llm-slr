@article{CHAVESFRAGA2020100596,
title = {GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain},
journal = {Journal of Web Semantics},
volume = {65},
pages = {100596},
year = {2020},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2020.100596},
url = {https://www.sciencedirect.com/science/article/pii/S1570826820300354},
author = {David Chaves-Fraga and Freddy Priyatna and Andrea Cimmino and Jhon Toledo and Edna Ruckhaus and Oscar Corcho},
keywords = {Virtual knowledge graph, Benchmark, Query translation, Data integration, GTFS},
abstract = {A large number of datasets are being made available on the Web using a variety of formats and according to diverse data models. Ontology Based Data Integration (OBDI) has been traditionally proposed as a mechanism to facilitate access to such heterogeneous datasets, providing a unified view over their data by means of ontologies. Recently, the term “Virtual Knowledge Graph Access” has begun to be used to refer to the mechanisms that provide query-based access to knowledge graphs virtually generated from heterogeneous data sources. Several OBDI engines exist in the state of the art, with overlapping capabilities but also clear differences among them (in terms of the data formats that they can deal with, mapping languages that they support, query expressivity that they allow, etc.). These engines have been evaluated with different testbeds and benchmarks. However, their heterogeneity has made it difficult to come up with a common comprehensive benchmark that allows for comparisons among them to facilitate their selection by practitioners, and more importantly, for their continuous improvement by the teams that maintain them. In this paper we present GTFS-Madrid-Bench, a benchmark to evaluate OBDI engines that can be used for the provision of access mechanisms to virtual knowledge graphs. Our proposal introduces several scenarios that aim at measuring the query capabilities, performance and scalability of all these engines, considering their heterogeneity. The data sources used in our benchmark are derived from the GTFS data files of the subway network of Madrid. They have been transformed into several formats (CSV, JSON, SQL and XML) and scaled up. The query set aims at addressing a representative number of SPARQL 1.1 features while covering usual queries that data consumers may be interested in.}
}
@article{RATHEE2025441,
title = {An improved and decentralized/distributed healthcare framework for disabled people through AI models},
journal = {Alexandria Engineering Journal},
volume = {125},
pages = {441-448},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1110016825003011},
author = {Geetanjali Rathee and Sahil Garg and Georges Kaddoum and Samah M. Alzanin and Mohammad Mehedi Hassan},
keywords = {Decentralized healthcare system, AI models for disabled people, e-health in Saudi Arabia, AI in healthcare, Saudi vision 2030, Disabled accessibility},
abstract = {Access to adequate healthcare is critical for everyone, but people with disabilities often face considerable challenges in receiving reliable and timely medical treatment. The Vision 2030 plan in Saudi Arabia intends to change the healthcare system by incorporating new technologies that increase accessibility, efficiency, and service delivery. However, current healthcare systems continue to suffer from delays, inefficient data processing, and accessibility concerns, especially for the visually impaired. This study proposes a more decentralized healthcare system that uses artificial intelligence (AI) and machine learning (ML) models to improve healthcare services for individuals with disabilities. The system achieves real-time data processing, reduces latency, and enhances decision-making accuracy by combining federated learning and zero-shot architectures. Furthermore, smart technologies such as the Internet of Things (IoT) and natural language processing (NLP) provide seamless data collection and analysis, allowing healthcare practitioners to provide prompt and personalized treatment. The suggested solution solves crucial issues such as inefficiencies in data processing, delays in obtaining medical information, and limits in current healthcare processes. This platform improves impaired people’s freedom and mobility by delivering remote healthcare solutions using AI-powered diagnostics and real-time monitoring. This study contributes to a more inclusive and efficient healthcare system in Saudi Arabia by bridging the gap between technology and accessibility, which aligns with the Vision 2030 objective of providing fair healthcare services to everyone.}
}
@article{BELOUETTAR2025118980,
title = {DeeMa-Hub: Cloud-enabled semantic platform for data-driven multiscale co-design and co-simulation of composite materials and structures},
journal = {Composite Structures},
volume = {360},
pages = {118980},
year = {2025},
issn = {0263-8223},
doi = {https://doi.org/10.1016/j.compstruct.2025.118980},
url = {https://www.sciencedirect.com/science/article/pii/S026382232500145X},
author = {Salim Belouettar and Mahdi Ben Amor and Bořek Patzák and Heng Hu},
keywords = {Computational hub, Interoperability framework, Co-simulation, Collaboration, Ontology, Composite materials and structures, Decision making, BPMN, Co-simulation platform, Innovation hub},
abstract = {This paper presents DeeMa-Hub, a cloud-based microservices ecosystem designed to support collaborative research and decision making in composite materials design and manufacturing. DeeMa-Hub integrates data management, modeling, co-simulation, and decision-making functionalities into a cohesive collaborative platform. The architecture is organized into three main layers: the Data Layer, the Modeling and Simulation Layer, and the Collaborative Interface Layer, enabling efficient data handling, complex computational workflows, and team-driven processes. The platform’s cloud infrastructure, leveraging Microsoft Azure services, provides scalability, accessibility, and robust data security, accommodating the demands of high-performance composite materials simulations and data analysis. Central to the platform’s operation is the Modeling and Simulation Layer, managed by MuPIF (Multi-Physics Interoperability Framework), which supports interoperable, multi-scale adn multi-physical simulation workflows. This layer uses standardized APIs to handle diverse simulation models and data structures, ensuring integration with various simulation platforms and enabling workflow automation. The Collaborative Layer utilizes Business Process Model and Notation (BPMN) and Decision Model and Notation (DMN) frameworks to facilitate process management and decision logic, allowing users to collaboratively design and control workflows and decisions in real time. DeeMa-Hub’s cloud-deployed structure supports dynamic scaling, automated resource allocation, and high availability through Azure’s autoscaling and load balancing. The platform is equipped with Azure DevOps for continuous integration and deployment, enabling rapid updates. Through its structured, scalable design, DeeMa-Hub provides a secure, flexible environment for composite materials research, promoting collaborative, data-driven innovation.}
}
@article{COSTA2020103384,
title = {Alternatives for facilitating automatic transformation of BIM data using semantic query languages},
journal = {Automation in Construction},
volume = {120},
pages = {103384},
year = {2020},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2020.103384},
url = {https://www.sciencedirect.com/science/article/pii/S092658052030964X},
author = {G. Costa and A. Sicilia},
keywords = {Data transformation, Semantic web, SPARQL Construct, SPARQL-generate, Benchmark},
abstract = {In the Architecture Engineering and Construction (AEC) industry, Building Information Model (BIM) authoring tools enable the creation of digital representations of buildings. Each tool implements its own building data model, which makes it difficult to achieve the desired interoperability when building data have to be exchanged with other software (e.g., building energy simulation tools). The representation of BIM models through open standards (e.g., IFC) and Semantic Web technologies can facilitate building data transformation in an automated and flexible way. This is achieved by taking advantage of the logical basis of the Resource Description Framework (RDF) data model and queries created in the Semantic Web query languages. The result is a pragmatic mechanism to transform the data from one data domain to another. This article analyses the potential of Semantic Web query languages to facilitate the data transformation of building data through different alternatives. The first contribution is the identification of fourteen data mapping patterns and three cases of data transformation that enable transforming one data model into another, considering the semantic and structural differences between them. The second contribution is the review and comparison of query languages to carry out the transformations through two different alternatives: using SPARQL-Generate and SPARQL Construct queries. And finally, the third contribution is the definition of a metric to assess the complexity of SPARQL queries.}
}
@article{LEROY2025101747,
title = {Indigenous communities and mining activities in Central America and Mexico: A systematic review},
journal = {The Extractive Industries and Society},
volume = {24},
pages = {101747},
year = {2025},
issn = {2214-790X},
doi = {https://doi.org/10.1016/j.exis.2025.101747},
url = {https://www.sciencedirect.com/science/article/pii/S2214790X25001364},
author = {David Leroy},
keywords = {Literature review, Indigenous peoples, Extractivism, Latin america},
abstract = {Mining activities pose an increasing threat to Indigenous peoples in Central America and Mexico, as their territories become the focus of expanding extractive interests. This systematic literature review provides a cross-cutting analysis of the relationships between Indigenous communities and mining operations across the region. Drawing on the ROSES (Reporting Standards for Systematic Evidence Syntheses) methodology, it analyzes 50 peer-reviewed articles published in English and Spanish between 2000 and 2024. The findings reveal a strong concentration of studies centered on Guatemala and Mexico, with particular attention to the Maya (Mam, Q’eqchi’, Sipakapense) and Zapotec peoples. Canadian mining companies emerge as the dominant actors, especially in gold and silver extraction. The research field is structured around a diverse set of interrelated themes, with significant emphasis on conflicts and resistance, violence and criminalization, colonial legacies and dispossession, Indigenous ontologies, and the socio-environmental impacts of extractivism. The review underscores the need to advance research on post-extractive transitions, corporate social responsibility (CSR) strategies, and gender-sensitive approaches. It also advocates for the use of participatory methodologies co-developed with Indigenous communities and highlights the importance of expanding geographical coverage to underexplored contexts such as Panama and Nicaragua.}
}
@article{PEREZPEREZ2023104398,
title = {A novel gluten knowledge base of potential biomedical and health-related interactions extracted from the literature: Using machine learning and graph analysis methodologies to reconstruct the bibliome},
journal = {Journal of Biomedical Informatics},
volume = {143},
pages = {104398},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104398},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423001193},
author = {Martín Pérez-Pérez and Tânia Ferreira and Gilberto Igrejas and Florentino Fdez-Riverola},
keywords = {Literature curation, Knowledge representation, Ontology-base methods, Relation extraction, Social media, Gluten database},
abstract = {Background
In return for their nutritional properties and broad availability, cereal crops have been associated with different alimentary disorders and symptoms, with the majority of the responsibility being attributed to gluten. Therefore, the research of gluten-related literature data continues to be produced at ever-growing rates, driven in part by the recent exploratory studies that link gluten to non-traditional diseases and the popularity of gluten-free diets, making it increasingly difficult to access and analyse practical and structured information. In this sense, the accelerated discovery of novel advances in diagnosis and treatment, as well as exploratory studies, produce a favourable scenario for disinformation and misinformation.
Objectives
Aligned with, the European Union strategy “Delivering on EU Food Safety and Nutrition in 2050″ which emphasizes the inextricable links between imbalanced diets, the increased exposure to unreliable sources of information and misleading information, and the increased dependency on reliable sources of information; this paper presents GlutKNOIS, a public and interactive literature-based database that reconstructs and represents the experimental biomedical knowledge extracted from the gluten-related literature. The developed platform includes different external database knowledge, bibliometrics statistics and social media discussion to propose a novel and enhanced way to search, visualise and analyse potential biomedical and health-related interactions in relation to the gluten domain.
Methods
For this purpose, the presented study applies a semi-supervised curation workflow that combines natural language processing techniques, machine learning algorithms, ontology-based normalization and integration approaches, named entity recognition methods, and graph knowledge reconstruction methodologies to process, classify, represent and analyse the experimental findings contained in the literature, which is also complemented by data from the social discussion.
Results and conclusions
In this sense, 5814 documents were manually annotated and 7424 were fully automatically processed to reconstruct the first online gluten-related knowledge database of evidenced health-related interactions that produce health or metabolic changes based on the literature. In addition, the automatic processing of the literature combined with the knowledge representation methodologies proposed has the potential to assist in the revision and analysis of years of gluten research. The reconstructed knowledge base is public and accessible at https://sing-group.org/glutknois/}
}
@article{ZHONG20191041,
title = {A Knowledge Base System for Operation Optimization: Design and Implementation Practice for the Polyethylene Process},
journal = {Engineering},
volume = {5},
number = {6},
pages = {1041-1048},
year = {2019},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2019.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095809919308276},
author = {Weimin Zhong and Chaoyuan Li and Xin Peng and Feng Wan and Xufeng An and Zhou Tian},
keywords = {Ontology, Operation optimization, Knowledge base system, Polyethylene process},
abstract = {Setting up a knowledge base is a helpful way to optimize the operation of the polyethylene process by improving the performance and the efficiency of reuse of information and knowledge—two critical elements in polyethylene smart manufacturing. In this paper, we propose an overall structure for a knowledge base based on practical customer demand and the mechanism of the polyethylene process. First, an ontology of the polyethylene process constructed using the seven-step method is introduced as a carrier for knowledge representation and sharing. Next, a prediction method is presented for the molecular weight distribution (MWD) based on a back propagation (BP) neural network model, by analyzing the relationships between the operating conditions and the parameters of the MWD. Based on this network, a differential evolution algorithm is introduced to optimize the operating conditions by tuning the MWD. Finally, utilizing a MySQL database and the Java programming language, a knowledge base system for the operation optimization of the polyethylene process based on a browser/server framework is realized.}
}
@article{LOPES2023101928,
title = {A broad approach to expert detection using syntactic and semantic social networks analysis in the context of Global Software Development},
journal = {Journal of Computational Science},
volume = {66},
pages = {101928},
year = {2023},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2022.101928},
url = {https://www.sciencedirect.com/science/article/pii/S1877750322002873},
author = {Tales Lopes and Victor Ströele and Regina Braga and José Maria N. David and Michael Bauer},
abstract = {Social network analysis has been widely used in different application contexts. For example, in Global Software Development, where multiple developers with diverse skills and knowledge are involved, the use of social networking models helps to understand how these developers collaborate. Finding experts who can help address critical elements or issues in a project is a challenging and critical task. It is especially true in the context of Global Software Development projects, where developers with specific skills and knowledge often need to be identified. In this sense, searching for essential members is a valuable task, as they are fundamental to the evolution of the network. This article proposes a broad solution for syntactic and semantic analysis in social networks in the Global Software Development context. In this solution, we define a model for the social network capable of capturing collaboration between developers, incorporate strategies for temporal analysis of the network, explore the network using machine learning algorithms, and propose an ontology to enrich the data semantically. We conducted three case studies using data extracted from GitHub to evaluate the proposed approach. The case studies provide evidence that our proposed method can identify specialists, highlighting their expertise and importance to the evolution of the social network.}
}
@article{DEJESUSPACHECO2022106819,
title = {Triple Bottom Line impacts of traditional Product-Service Systems models: Myth or truth? A Natural Language Understanding approach},
journal = {Environmental Impact Assessment Review},
volume = {96},
pages = {106819},
year = {2022},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2022.106819},
url = {https://www.sciencedirect.com/science/article/pii/S0195925522000853},
author = {Diego Augusto {de Jesus Pacheco} and Carla Schwengber {ten Caten} and Carlos Fernando Jung and Isaac Pergher and Julian David Hunt},
keywords = {Environmental assessment, Natural language understanding, Artificial intelligence, Product-service systems, Sustainable product-service systems, Literature review},
abstract = {Currently, there is in the literature a debate concerning the real impact of Product-Service Systems (PSS) models on society. It is now stated that PSS does not necessarily lead to sustainable solutions in practice from a Triple Bottom Line (TBL) perspective. On the other hand, a promising approach, i.e. the Sustainable Product-Service Systems (SPSS) approach, has received attention from scholars within this debate. However, due to the novelty of this discussion, there is insufficient understanding of the synergies and divergences between both approaches regarding the potential to deliver TBL solutions to society. To address these lacks, this study examines the synergies and divergences between PSS and the emerging SPSS to deliver TBL solutions to society. Qualitative and quantitative research approaches were adopted to address the research questions. First, a structured literature review was employed. The literature analysis was segmented into two distinct periods (i.e., first period from 1990 to 2009, and the second period from 2010 to 2021). Next, the Natural Language Understanding (NLU) tools based on Artificial Intelligence (AI) and Neural Networks were applied to analyse the conceptual definitions retrieved from the relevant literature and extract new knowledge regarding both approaches. Third, the patterns of new knowledge were analysed against the literature in the area leading to research findings. Overall, findings indicate that, from a TBL perspective, SPSS is an emerging and promising approach in which the environmental and social dimensions are more salient than in the traditional PSS models. The study also unveils the central concepts related to both approaches in the extant literature. The article extends the current knowledge on PSS and SPSS, guiding the research communities interested in this area and unlocking the present and future challenges towards an effective sustainable-oriented economy. This article is a pioneering study to examine how the PSS and SPSS concepts have advanced towards TBL solutions in society.}
}
@article{BADINI2025100275,
title = {Enhancing mechanical and bioinspired materials through generative AI approaches},
journal = {Next Materials},
volume = {6},
pages = {100275},
year = {2025},
issn = {2949-8228},
doi = {https://doi.org/10.1016/j.nxmate.2024.100275},
url = {https://www.sciencedirect.com/science/article/pii/S2949822824001722},
author = {Silvia Badini and Stefano Regondi and Raffaele Pugliese},
keywords = {Mechanical materials, Bioinspired materials, Additive manufacturing, Generative AI, Human-machine interaction},
abstract = {The integration of generative artificial intelligence (AI) into the design and additive manufacturing processes of mechanical and bioinspired materials has emerged as a transformative approach in engineering and material science, allowing to explore relationships across different field (e.g., mechanics-biology) or disparate domains (e.g., failure mechanics-3D printing). In addition, generative AI techniques, including generative adversarial networks (GAN), genetic algorithms, and large language models (LLMs), offer efficient and tunable solutions for optimizing material properties, reducing production costs, and accelerating the development timelines. In the field of mechanical materials design, generative AI enables the rapid generation of novel structures with enhanced mechanical performance. Instead, bioinspired materials design benefits significantly from the synergy of generative AI with bioinspired concepts and additive manufacturing. By harnessing generative algorithms and topology optimization, researchers can explore complex biological phenomena and translate them into innovative engineering solutions. Lastly, the emergence of LLMs in additive manufacturing optimization demonstrates their potential to optimize printing parameters, debug errors, and enhance productivity. This review highlights the pivotal role of generative AI in advancing materials science and engineering, unlocking new possibilities for innovation, and accelerating the development of efficient material solutions. As generative AI continues to evolve, its integration promises to revolutionize engineering design and drive the field towards unprecedented levels of efficiency, thus turns information into knowledge.}
}
@article{THEILER2020101158,
title = {Metaization concepts for monitoring-related information},
journal = {Advanced Engineering Informatics},
volume = {46},
pages = {101158},
year = {2020},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2020.101158},
url = {https://www.sciencedirect.com/science/article/pii/S1474034620301294},
author = {Michael Theiler and Stalin Ibáñez and Dmitrii Legatiuk and Kay Smarsly},
keywords = {Structural health monitoring, Building information modeling, Metamodeling, Semantic modeling},
abstract = {In recent years, structural health monitoring (SHM) has become a widely used state-of-the-art method for analyzing and assessing the condition of civil infrastructure. SHM systems are characterized by a plethora of heterogeneous system components. For optimizing, documenting, and change tracking of SHM systems, information about SHM systems needs to be formally described on a sound basis. However, with current methods, such as ontologies, description languages or metamodels, only a small subset of information inherent to SHM systems, such as information about sensors, can formally be described. This paper presents a conceptual approach towards identifying and specifying monitoring-related information. Based on a summary review of SHM modeling approaches, metaization concepts to overcome the current limitations are discussed and a robust formalism to describe SHM systems mathematically is proposed. The review methodology is based on a three-pillar concept. First, regulations, standards, and guidelines related to SHM and, second, the current research landscape is examined to identify information required for describing SHM systems, and hierarchies of terms are proposed to categorize the findings. Third, metamodel architectures, such as SHM-related ontologies, BIM-based metamodels and description languages, are reviewed with respect to formally describe SHM systems. Being part of the third pillar, mathematical metamodeling approaches based on category theory, set theory and type theory are presented, capable to describe SHM system as well as approaches suitable to couple metamodels. As an outcome of this study, besides a comprehensive review of the above directions, a strategy towards developing a metaization concept is proposed to provide a robust formalism for SHM system descriptions, aiming to advance optimization, documentation, and change tracking of SHM systems.}
}
@article{ASHOK2022102433,
title = {Ethical framework for Artificial Intelligence and Digital technologies},
journal = {International Journal of Information Management},
volume = {62},
pages = {102433},
year = {2022},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102433},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221001262},
author = {Mona Ashok and Rohit Madan and Anton Joha and Uthayasankar Sivarajah},
keywords = {Artificial Intelligence (AI) ethics, Digital ethics, Digital technologies and archetypes, PRISMA, Systematic literature review, Ontological framework},
abstract = {The use of Artificial Intelligence (AI) in Digital technologies (DT) is proliferating a profound socio-technical transformation. Governments and AI scholarship have endorsed key AI principles but lack direction at the implementation level. Through a systematic literature review of 59 papers, this paper contributes to the critical debate on the ethical use of AI in DTs beyond high-level AI principles. To our knowledge, this is the first paper that identifies 14 digital ethics implications for the use of AI in seven DT archetypes using a novel ontological framework (physical, cognitive, information, and governance). The paper presents key findings of the review and a conceptual model with twelve propositions highlighting the impact of digital ethics implications on societal impact, as moderated by DT archetypes and mediated by organisational impact. The implications of intelligibility, accountability, fairness, and autonomy (under the cognitive domain), and privacy (under the information domain) are the most widely discussed in our sample. Furthermore, ethical implications related to the governance domain are shown to be generally applicable for most DT archetypes. Implications under the physical domain are less prominent when it comes to AI diffusion with one exception (safety). The key findings and resulting conceptual model have academic and professional implications.}
}
@article{BARTLEY20221373,
title = {Tyto: A Python Tool Enabling Better Annotation Practices for Synthetic Biology Data-Sharing},
journal = {ACS Synthetic Biology},
volume = {11},
number = {3},
pages = {1373-1376},
year = {2022},
issn = {2161-5063},
doi = {https://doi.org/10.1021/acssynbio.1c00450},
url = {https://www.sciencedirect.com/science/article/pii/S2161506322001929},
author = {Bryan A. Bartley},
keywords = {synthetic biology, standards, ontology, automation, SBOL},
abstract = {As synthetic biology becomes increasingly automated and data-driven, tools that help researchers implement FAIR (findable-accessible-interoperable-reusable) data management practices are needed. Crucially, in order to support machine processing and reusability of data, it is important that data artifacts are appropriately annotated with metadata drawn from controlled vocabularies. Unfortunately, adopting standardized annotation practices is difficult for many research groups to adopt, given the set of specialized database science skills usually required to interface with ontologies. In response to this need, Take Your Terms from Ontologies (Tyto) is a lightweight Python tool that supports the use of controlled vocabularies in everyday scripting practice. While Tyto has been developed for synthetic biology applications, its utility may extend to users working in other areas of bioinformatics research as well. Tyto is available as a Python package distribution or available as source at https://github.com/SynBioDex/tyto.
}
}
@article{ZHU2023878,
title = {An information model for highway operational risk management based on the IFC-Brick schema},
journal = {International Journal of Transportation Science and Technology},
volume = {12},
number = {3},
pages = {878-890},
year = {2023},
issn = {2046-0430},
doi = {https://doi.org/10.1016/j.ijtst.2022.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S2046043022001046},
author = {Bencheng Zhu and Fujin Hou and Tao Feng and Tao Li and Cancan Song},
keywords = {Highway, Information Model, Operational Risk Management, IFC-Brick, Digital Twin},
abstract = {With the development of highways, new technologies should be continuously introduced to improve highway traffic safety. Digital twin (DT) has been an emerging field of research in recent years. To develop a digital twin management system, a data model is essential. In the field of highway operational risk management (HORM), however, the development of data models is still in its infancy. Motivated by the concept of linked data, in this paper, we attempt to propose an information model for HORM. The main achievements of this paper include data architecture, identification and classification code methods, data interaction method, and the developed system. Based on data needs analysis, the highway information model architecture for risk management is defined as five layers: basic highway products, traffic sensors and equipment, traffic rules, traffic flow, and weather. Furthermore, according to the concepts of semantic data, these five layers can be classified into three categories: highway product data, topology data, and sensor data. Although the Industry Foundation Classes (IFC) standard and Brick schema were first proposed and applied in the building domain, some of their entities and relationships can also be applied to highways. To this end, we defined some new classes, a specific ontology, and an integrated framework for HORM. Finally, a case study was carried out. Applying such information model to highways has broad potential. It changes the file-based exchange method to the data-based one, which can promote highway data exchange and applications. The proposed information model could be of great significance for HORM.}
}
@article{BURGGRAF2024254,
title = {Paving the way for automated factory planning – applying rule-based expert systems to capacity planning},
journal = {Procedia CIRP},
volume = {126},
pages = {254-259},
year = {2024},
note = {17th CIRP Conference on Intelligent Computation in Manufacturing Engineering (CIRP ICME ‘23)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.335},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124009065},
author = {Peter Burggräf and Tobias Adlon and Niklas Schäfer},
keywords = {Factory Planning, Digital Factory, Planning Automation, Rule-Based Systems, Semantic Web},
abstract = {Today, numerous different software systems aid factory planners in their tasks. Nevertheless, due to their lacking interoperability and project-specific approaches, generic support for automated decision-making is still missing. Investigating the state of the art, we conclude that knowledge-based information modeling is needed for decision-making support. However, as the identified approaches of previous works propose no general automation concepts. Therefore, we define the guiding research question as how to model processual domain knowledge for automating factory planning processes. In this paper, we propose a planning assistance on rule-based expert systems. The planning assistance is composed of an ontology-based information model, a planning model consisting of individual planning functions, and a domain-specific inference engine. We implement the planning assistance with Semantic Web technologies and validate the solution using an application example from capacity planning. Thereby, we demonstrate the applicability of rule-based expert systems for automated factory planning. Finally, implications for future research are drawn for exploring further application areas and developing anticipated hybrid solution concepts.}
}
@article{KACZMAREK2022103479,
title = {A machine learning approach for integration of spatial development plans based on natural language processing},
journal = {Sustainable Cities and Society},
volume = {76},
pages = {103479},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103479},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721007460},
author = {Iwona Kaczmarek and Adam Iwaniak and Aleksandra Świetlicka and Mateusz Piwowarczyk and Adam Nadolny},
keywords = {Spatial planning, Spatial development plan, Neural networks, LSTM, GRU, Unsupervised machine learning, Text classification},
abstract = {Spatial development plans are the basic tool for shaping spatial policy and have an impact on the implementation of the concept of sustainable development. Monitoring the implementation of plans can be difficult where no standard of plans exists that allows for obtaining comprehensive information on the arrangements of the plans, including future land development. The purpose of the research is to integrate spatial development plans by analyzing and classifying their textual content. We use machine learning methods for the processing of the text of plans and their classification. The result is a model, that classifies the texts of findings for individual areas in the plan into defined land use categories. We use machine learning methods in natural language processing for the analyzing of the text part of plans and their classification. The results indicate the best quality of the model when using neural networks. The proposed approach allows for obtaining comprehensive information on the planned land use of the area, derived from many heterogeneous planning documents. Due to the combination of textual arrangements with spatial data, it allows both for the unification of land use classification and then integration of multiple spatial development plans in spatial dimension.}
}
@article{SEARLE2023104358,
title = {Discharge summary hospital course summarisation of in patient Electronic Health Record text with clinical concept guided deep pre-trained Transformer models},
journal = {Journal of Biomedical Informatics},
volume = {141},
pages = {104358},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104358},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000795},
author = {Thomas Searle and Zina Ibrahim and James Teo and Richard J.B. Dobson},
keywords = {Clinical natural language processing, Clinical text summarisation, Pre-trained, deep learning, fine-tuned models for clinical summarisation},
abstract = {Brief Hospital Course (BHC) summaries are succinct summaries of an entire hospital encounter, embedded within discharge summaries, written by senior clinicians responsible for the overall care of a patient. Methods to automatically produce summaries from inpatient documentation would be invaluable in reducing clinician manual burden of summarising documents under high time-pressure to admit and discharge patients. Automatically producing these summaries from the inpatient course, is a complex, multi-document summarisation task, as source notes are written from various perspectives (e.g. nursing, doctor, radiology), during the course of the hospitalisation. We demonstrate a range of methods for BHC summarisation demonstrating the performance of deep learning summarisation models across extractive and abstractive summarisation scenarios. We also test a novel ensemble extractive and abstractive summarisation model that incorporates a medical concept ontology (SNOMED) as a clinical guidance signal and shows superior performance in 2 real-world clinical data sets.}
}
@article{ALVAREZCARMONA202210125,
title = {Natural language processing applied to tourism research: A systematic review and future research directions},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part B},
pages = {10125-10144},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822003615},
author = {Miguel Á. Álvarez-Carmona and Ramón Aranda and Ansel Y. Rodríguez-Gonzalez and Daniel Fajardo-Delgado and María Guadalupe Sánchez and Humberto Pérez-Espinosa and Juan Martínez-Miranda and Rafael Guerrero-Rodríguez and Lázaro Bustio-Martínez and Ángel Díaz-Pacheco},
keywords = {Tourism, Natural language processing, Systematic review},
abstract = {The social networks and the rapid development of new technologies have led to considerable changes in the tourism industry. Artificial intelligence, in particular natural language processing (NLP), presupposes a significant advantage in obtaining information on the mass content generated by online users concerning tourism services and products. This work presents a systematic review of the use of NLP in the tourism industry and research. We used the well-known PRISMA methodology, and 227 relevant studies over the last decade have been reviewed. Our analysis identified the main methodologies, tools, data sources, and other relevant features in the field. One of the principal contributions of this study is a taxonomy for using NLP in tourism. In addition, metadata were examined using a threefold approach: (i) general statistics, (ii) abstract text analysis, and (iii) keyword networks. Automatic analyses have identified six major topics in applying NLP to tourism issues and have shown that China, the United States, Thailand, and Spain share similar tourism issues or approaches.}
}
@article{PETHANI2023104282,
title = {Natural language processing for clinical notes in dentistry: A systematic review},
journal = {Journal of Biomedical Informatics},
volume = {138},
pages = {104282},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104282},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000035},
author = {Farhana Pethani and Adam G. Dunn},
keywords = {Dental informatics, Dental records, Dentistry, Information sciences, Natural language processing},
abstract = {Objective
To identify and synthesise research on applications of natural language processing (NLP) for information extraction and retrieval from clinical notes in dentistry.
Materials and methods
A predefined search strategy was applied in EMBASE, CINAHL and Medline. Studies eligible for inclusion were those that that described, evaluated, or applied NLP to clinical notes containing either human or simulated patient information. Quality of the study design and reporting was independently assessed based on a set of questions derived from relevant tools including CHecklist for critical Appraisal and data extraction for systematic Reviews of prediction Modelling Studies (CHARMS). A narrative synthesis was conducted to present the results.
Results
Of the 17 included studies, 10 developed and evaluated NLP methods and 7 described applications of NLP-based information retrieval methods in dental records. Studies were published between 2015 and 2021, most were missing key details needed for reproducibility, and there was no consistency in design or reporting. The 10 studies developing or evaluating NLP methods used document classification or entity extraction, and 4 compared NLP methods to non-NLP methods. The quality of reporting on NLP studies in dentistry has modestly improved over time.
Conclusions
Study design heterogeneity and incomplete reporting of studies currently limits our ability to synthesise NLP applications in dental records. Standardisation of reporting and improved connections between NLP methods and applied NLP in dentistry may improve how we can make use of clinical notes from dentistry in population health or decision support systems. Protocol Registration. PROSPERO CRD42021227823.}
}
@article{LEFKOVITZ2024,
title = {Direct Clinical Applications of Natural Language Processing in Common Neurological Disorders: Scoping Review},
journal = {JMIR Neurotechnology},
volume = {3},
year = {2024},
issn = {2817-092X},
doi = {https://doi.org/10.2196/51822},
url = {https://www.sciencedirect.com/science/article/pii/S2817092X24000036},
author = {Ilana Lefkovitz and Samantha Walsh and Leah J Blank and Nathalie Jetté and Benjamin R Kummer},
keywords = {natural language processing, NLP, unstructured, text, machine learning, deep learning, neurology, headache disorders, migraine, Parkinson disease, cerebrovascular disease, stroke, transient ischemic attack, epilepsy, multiple sclerosis, cardiovascular, artificial intelligence, Parkinson, neurological, neurological disorder, scoping review, diagnosis, treatment, prediction},
abstract = {Background
Natural language processing (NLP), a branch of artificial intelligence that analyzes unstructured language, is being increasingly used in health care. However, the extent to which NLP has been formally studied in neurological disorders remains unclear.
Objective
We sought to characterize studies that applied NLP to the diagnosis, prediction, or treatment of common neurological disorders.
Methods
This review followed the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) standards. The search was conducted using MEDLINE and Embase on May 11, 2022. Studies of NLP use in migraine, Parkinson disease, Alzheimer disease, stroke and transient ischemic attack, epilepsy, or multiple sclerosis were included. We excluded conference abstracts, review papers, as well as studies involving heterogeneous clinical populations or indirect clinical uses of NLP. Study characteristics were extracted and analyzed using descriptive statistics. We did not aggregate measurements of performance in our review due to the high variability in study outcomes, which is the main limitation of the study.
Results
In total, 916 studies were identified, of which 41 (4.5%) met all eligibility criteria and were included in the final review. Of the 41 included studies, the most frequently represented disorders were stroke and transient ischemic attack (n=20, 49%), followed by epilepsy (n=10, 24%), Alzheimer disease (n=6, 15%), and multiple sclerosis (n=5, 12%). We found no studies of NLP use in migraine or Parkinson disease that met our eligibility criteria. The main objective of NLP was diagnosis (n=20, 49%), followed by disease phenotyping (n=17, 41%), prognostication (n=9, 22%), and treatment (n=4, 10%). In total, 18 (44%) studies used only machine learning approaches, 6 (15%) used only rule-based methods, and 17 (41%) used both.
Conclusions
We found that NLP was most commonly applied for diagnosis, implying a potential role for NLP in augmenting diagnostic accuracy in settings with limited access to neurological expertise. We also found several gaps in neurological NLP research, with few to no studies addressing certain disorders, which may suggest additional areas of inquiry.
Trial Registration
Prospective Register of Systematic Reviews (PROSPERO) CRD42021228703; https://www.crd.york.ac.uk/PROSPERO/display_record.php?RecordID=228703}
}
@article{CIMINO2025859,
title = {Automatic simulation models generation in industrial systems: A systematic literature review and outlook towards simulation technology in the Industry 5.0},
journal = {Journal of Manufacturing Systems},
volume = {80},
pages = {859-882},
year = {2025},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2025.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S0278612525000895},
author = {Antonio Cimino and Mohaiad Elbasheer and Francesco Longo and Giovanni Mirabelli and Vittorio Solina and Pierpaolo Veltri},
keywords = {Simulation, Automatic generation, Smart manufacturing, Digital twin, Industry 5.0},
abstract = {Simulation models are a crucial enabling technology for decision support in the ongoing industrial digitalization hype. Within Industry 4.0, simulations are extensively utilized, providing insights into industrial behavior and responses. As we progress towards Industry 5.0, simulation models continue to play a pivotal role in achieving sustainable, resilient, and human-oriented industrial systems. However, a persistent challenge within Industry 4.0/5.0 is the substantial dynamism of industrial environments. This dynamic and complex landscape necessitates the development of adaptive solutions capable of swiftly responding to the volatile process requirements of modern industrial systems. To this end, Automatic Simulation Model Generation (ASMG) offers an innovative methodological framework to address this practical challenge in the development of industrial simulation models. Employing the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology, this research systematically reviews the current state-of-the-art in ASMG. Complemented by a bibliometric and content analysis of 61 articles spanning more than two decades (from 2000 to 2023), the paper evaluates ASMG’s progression and application in manufacturing through four research questions focusing on ASMG development strategies, objectives, essential data, and developing environments. Ultimately, this article provides valuable insights into ASMG perspective for industrial simulation specialists and offers guidelines for future developments in the era of Industry 5.0.}
}
@article{FENG202141,
title = {Application of natural language processing in HAZOP reports},
journal = {Process Safety and Environmental Protection},
volume = {155},
pages = {41-48},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0957582021004675},
author = {Xiayuan Feng and Yiyang Dai and Xu Ji and Li Zhou and Yagu Dang},
keywords = {NLP, Chemical process safety, HAZOP reports, Text mining, BERT},
abstract = {Accidents in chemical production usually result in fatal injuries, economic losses, and negative social impacts. To ensure personnel security in such cases, previous research has often used digital data, such as physical signals. However, the valuable textual information contained in chemical security texts, such as expert knowledge, has not yet been explored. Therefore, there is an increasing demand to mine useful information from these unstructured data. In this study, natural language processing (NLP) was applied to the hazard and operability (HAZOP) analysis reports. The classification model was trained to learn the classification of consequence severity levels in high-quality HAZOP analysis reports, which will not only ensure the consistency of the analysis results, but also help smaller chemical plants perform security analysis. In the classification model, we introduced Bidirectional Encoder Representation from Transformers (BERT), which, for word embedding, which is a powerful NLP pre-training model and significantly improved the effectiveness of the model. Through these application scenarios, the feasibility and possibility of applying NLP in chemical security text have been confirmed to a certain extent. In addition to digital data, future security managers will be able to monitor chemical production using natural language.}
}
@article{BHATTACHARJEE2024108413,
title = {Symptom-based drug prediction of lifestyle-related chronic diseases using unsupervised machine learning techniques},
journal = {Computers in Biology and Medicine},
volume = {174},
pages = {108413},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108413},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524004979},
author = {Sudipto Bhattacharjee and Banani Saha and Sudipto Saha},
keywords = {Lifestyle-related diseases, Drugs, Symptoms, Machine learning, Clustering},
abstract = {Background and objectives
Lifestyle-related diseases (LSDs) impose a substantial economic burden on patients and health care services. LSDs are chronic in nature and can directly affect the heart and lungs. Therapeutic interventions only based on symptoms can be crucial for prompt treatment initiation in LSDs, as symptoms are the first information available to clinicians. So, this work aims to apply unsupervised machine learning (ML) techniques for developing models to predict drugs from symptoms for LSDs, with a specific focus on pulmonary and heart diseases.
Methods
The drug-disease and disease-symptom associations of 143 LSDs, 1271 drugs, and 305 symptoms were used to compute direct associations between drugs and symptoms. ML models with four different algorithms – K-Means, Bisecting K-Means, Mean Shift, and Balanced Iterative Reducing and Clustering using Hierarchies (BIRCH) – were developed to cluster the drugs using symptoms as features. The optimal model was saved in a server for the development of a web application. A web application was developed to perform the prediction based on the optimal model.
Results
The Bisecting K-means model showed the best performance with a silhouette coefficient of 0.647 and generated 138 drug clusters. The drugs within the optimal clusters showed good similarity based on i) gene ontology annotations of the gene targets, ii) chemical ontology annotations, and iii) maximum common substructure of the drugs. In the web application, the model also provides a confidence score for each predicted drug while predicting from a new set of input symptoms.
Conclusion
In summary, direct associations between drugs and symptoms were computed, and those were used to develop a symptom-based drug prediction tool for LSDs with unsupervised ML models. The ML-based prediction can provide a second opinion to clinicians to aid their decision-making for early treatment of LSD patients. The web application (URL - http://bicresources.jcbose.ac.in/ssaha4/sdldpred) can provide a simple interface for all end-users to perform the ML-based prediction.}
}
@article{MORASEGURA201971,
title = {Extremo: An Eclipse plugin for modelling and meta-modelling assistance},
journal = {Science of Computer Programming},
volume = {180},
pages = {71-80},
year = {2019},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167642319300644},
author = {Ángel {Mora Segura} and Juan {de Lara}},
keywords = {Model-driven Engineering, Modelling process, Language Engineering, Modelling assistance},
abstract = {Modelling is a core activity in software development paradigms like Model-driven Engineering (MDE). Therefore, the quality of (meta-)models is crucial for the success of software projects. However, many times, modelling becomes a purely manual activity, which does not take advantage of information embedded in heterogeneous information sources, such as XML documents, ontologies, or other models and meta-models. In order to improve this situation, we present Extremo, an Eclipse plugin aimed at gathering the information stored in heterogeneous sources in a common data model, to facilitate the reuse of information chunks in the model being built. The tool covers the steps needed to incorporate this knowledge within an external modelling tool, supporting the uniform query of the heterogeneous sources and the evaluation of constraints. Flexibility of the main features (e.g., supported data formats, queries) is achieved by means of extensible mechanisms. To illustrate the usefulness of Extremo, we describe a practical case study in the financial domain and evaluate its performance and scalability.}
}
@article{LUKASIEWICZ2022103685,
title = {Inconsistency-tolerant query answering for existential rules},
journal = {Artificial Intelligence},
volume = {307},
pages = {103685},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103685},
url = {https://www.sciencedirect.com/science/article/pii/S000437022200025X},
author = {Thomas Lukasiewicz and Enrico Malizia and Maria Vanina Martinez and Cristian Molinaro and Andreas Pieris and Gerardo I. Simari},
keywords = {Rule-based ontologies, Conjunctive queries, Inconsistency, Semantics, Query answering, Computational complexity},
abstract = {Querying inconsistent knowledge bases is an intriguing problem that gave rise to a flourishing research activity in the knowledge representation and reasoning community during the last years. It has been extensively studied in the context of description logics (DLs), and its computational complexity is rather well-understood. Although DLs are popular formalisms for modeling ontologies, it is generally agreed that rule-based ontologies are well-suited for data-intensive applications, since they allow us to conveniently deal with higher-arity relations, which naturally occur in standard relational databases. The goal of this work is to perform an in-depth complexity analysis of querying inconsistent knowledge bases in the case of the main decidable classes of existential rules, based on the notions of guardedness, linearity, acyclicity, and stickiness, enriched with negative (a.k.a. denial) constraints. Our investigation concentrates on three central inconsistency-tolerant semantics: the ABox repair (AR) semantics, considered as the standard one, and its main sound approximations, the intersection of repairs (IAR) semantics and the intersection of closed repairs (ICR) semantics.}
}
@article{PRIETOGONZALEZ2019103216,
title = {Towards the automated economic assessment of newborn screening for rare diseases},
journal = {Journal of Biomedical Informatics},
volume = {95},
pages = {103216},
year = {2019},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2019.103216},
url = {https://www.sciencedirect.com/science/article/pii/S1532046419301340},
author = {David Prieto-González and Iván Castilla-Rodríguez and Evelio González and María L. Couce},
keywords = {Economic assessment, Newborn screening, Ontology, Rare diseases, Simulation},
abstract = {Objective
Economic assessments of newborn screening programs for rare diseases involve the use of models and require huge efforts to synthesize information from different sources. Sharing and automatically or semi-automatically reusing this information for new assessments would be desirable, but it is not possible nowadays due to the lack of suitable tools.
Material and methods
We designed and implemented the Rare Diseases Ontology for Simulation (RaDiOS) after performing two reviews, and critically appraising the existing data repositories on rare diseases. The first review involved previous published economic assessments, and served to identify the main parameters required to model newborn screening. The second review aimed at locating existing data repositories potentially available to inform these parameters.
Results
We found key model parameters on epidemiology, screening methods, diagnose methods, pathogenesis, treatment and follow-up tests. We also identified seven data repositories directly related to rare diseases. None of such repositories was well-suited for the automated generation of simulation models. We incorporated the identified parameters as structured classes and properties of the new ontology (RaDiOS). We carefully set the relationships among the parameters so to allow automated inference from the ontology.
Conclusions
RaDiOS is an ontology that serves as a data repository to automatically build simulation models for the economic assessment of newborn screening for rare diseases.}
}
@article{LI2025101726,
title = {Exploring the impact of intermodal transfer on simplification: Insights from signed language interpreting, subtitle translation, and native speech in TED talks},
journal = {Language Sciences},
volume = {110},
pages = {101726},
year = {2025},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2025.101726},
url = {https://www.sciencedirect.com/science/article/pii/S038800012500021X},
author = {Ruitian Li and Kanglong Liu and Andrew K.F. Cheung},
keywords = {Signed language interpreting, Subtitle translation, Intermodal transfer, Linguistic complexity and simplification, TED talks},
abstract = {This study explores translational simplification in interpreted English from American Sign Language (ASL) and subtitled English from spoken French, compared to native English speech, using a self-constructed TED Talks Comparable Intermodal Corpus. By analyzing both lexical and syntactic complexity, the findings indicate that interpreted English does not exhibit a significant reduction in lexical density compared to native English speech. In fact, interpreted English has a higher lexical density than subtitled English. However, while subtitles are simpler in terms of semantic content, they show a less pronounced reduction in lexical variation and sophistication than oral interpretations, when compared to native speech. These results are attributable to the distinct modality influences of ASL and French, combined with the condensation constraints of subtitling and the real-time processing demands of interpreting. At the syntactic level, interpreted outputs display greater phrasal coordination than subtitles, while both modalities feature higher sentence-level coordination than native speech, likely shaped by the specific constraints of the TED Talk setting. This study contributes to a more nuanced understanding of the simplification phenomenon by highlighting the unique effects of intermodal transfer. It also adds to the knowledge of the distinct constraints of signed language interpreting and subtitle translation, as well as their divergent and shared patterns of information processing.}
}
@article{WU2025103283,
title = {Harnessing the potential of multimodal EHR data: A comprehensive survey of clinical predictive modeling for intelligent healthcare},
journal = {Information Fusion},
volume = {123},
pages = {103283},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103283},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525003562},
author = {Jialun Wu and Kai He and Rui Mao and Xuequn Shang and Erik Cambria},
keywords = {Intelligent healthcare, Medical intelligence, Electronic health records, Clinical predictive modeling},
abstract = {The digitization of healthcare has led to the accumulation of vast amounts of patient data through Electronic Health Records (EHRs) systems, creating significant opportunities for advancing intelligent healthcare. Recent breakthroughs in deep learning and information fusion techniques have enabled the seamless integration of diverse data sources, providing richer insights for clinical decision-making. This review offers a comprehensive analysis of predictive modeling approaches that leverage multimodal EHR data, focusing on the latest methodologies and their practical applications. We classify the current advancements from both task-driven and method-driven perspectives, while distilling key challenges and motivations that have fueled these innovations. This exploration examines the real-world impact of advanced technologies in healthcare, addressing issues from data integration to task formulation, challenges, and method refinement. The role of information fusion in enhancing model performance is also emphasized. Building on the discussions and findings, we highlight promising future research directions critical for advancing multimodal fusion technologies in clinical predictive modeling, addressing the complex challenges of real-world clinical environments, and moving toward universal intelligence in healthcare.}
}
@article{RAMOGLOU2020e00168,
title = {“Who is an entrepreneur?” is (still) the wrong question},
journal = {Journal of Business Venturing Insights},
volume = {13},
pages = {e00168},
year = {2020},
issn = {2352-6734},
doi = {https://doi.org/10.1016/j.jbvi.2020.e00168},
url = {https://www.sciencedirect.com/science/article/pii/S235267342030024X},
author = {Stratos Ramoglou and William B. Gartner and Eric W.K. Tsang},
keywords = {Personality research, Linguistic philosophy, Conceptual analysis, Ontology, Entrepreneurial gene, Entrepreneurial agency, Entrepreneurial opportunities, Discovery approach, Actualisation approach, Non-entrepreneurs},
abstract = {The idea that there exist undiscovered entrepreneurial endowments fell into disfavor after Gartner's (1988) “‘Who is an entrepreneur?’ is the wrong question”. However, a resurgence of the “question of the entrepreneur” suggests that advances in genetics research may be the key to discovering what makes entrepreneurs distinctive. This paper draws from Wittgensteinian philosophy to offer a novel critique regarding the search for differences between entrepreneurs and non-entrepreneurs. We explain that the idea that entrepreneurs are different gains credence through misleading forms of language that 1) encourage the illusion of some causal interplay between opportunities and potential entrepreneurs, and 2) overshadow the contingent nature of entrepreneurial action. We sidestep misleading forms of thought to suggest that ontological reflection on the nature of entrepreneurial agency shows why we will never discover some “entrepreneurial gene”. Equally important, this Wittgensteinian critique demonstrates the limits of empirical research for problems that fundamentally require conceptual attention – not more determined effort or advanced research methods.}
}
@article{DEFILIPPIS2025104845,
title = {Computational strategies in nutrigenetics: Constructing a reference dataset of nutrition-associated genetic polymorphisms},
journal = {Journal of Biomedical Informatics},
volume = {167},
pages = {104845},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104845},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000747},
author = {Giovanni Maria {De Filippis} and Maria Monticelli and Alessandra Pollice and Tiziana Angrisano and Bruno {Hay Mele} and Viola Calabrò},
keywords = {Nutrigenetics, Genetic polymorphisms, Personalized nutrition, Gene-diet interactions, Data integration, MeSH ontology},
abstract = {Objective:
This study aims to create a comprehensive dataset of human genetic polymorphisms associated with nutrition by integrating data from multiple sources, including the LitVar database, PubMed, and the GWAS catalog. This consolidated resource is intended to facilitate research in nutrigenetics by providing a reliable foundation to explore genetic polymorphisms linked to nutrition-related traits.
Methods:
We developed a data integration pipeline to assemble and analyze the dataset. It performs data retrieval from LitVar and PubMed and merges the data to produce a unified dataset. Comprehensive MeSH queries are defined to extract relevant genetic associations, which are then cross-referenced with the GWAS data.
Results:
The resulting dataset aggregates extensive information on genetic polymorphisms and nutrition-related traits. Through MeSH query, we identified key genes and SNPs associated with nutrition-related traits. Cross-referencing with GWAS data provided insights on potential effects or risk alleles associated with this genetic polymorphisms. The co-occurrence analysis revealed meaningful gene-diet interactions, advancing personalized nutrition and nutrigenomics research.
Conclusion:
The dataset presented in this study consolidates and organizes information on genetic polymorphisms associated with nutrition, facilitating detailed exploration of gene-diet interactions. This resource advances personalized nutrition interventions and nutrigenomics research. The dataset is publicly accessible at https://zenodo.org/records/14052302, its adaptable structure ensures applicability in a broad range of genetic investigations.}
}
@article{CAR2018290,
title = {USING decision models to enable better irrigation Decision Support Systems},
journal = {Computers and Electronics in Agriculture},
volume = {152},
pages = {290-301},
year = {2018},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2018.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0168169917313595},
author = {Nicholas J. Car},
keywords = {Decision modelling, Irrigation, Decision Support System, Decision Modelling Notation, Ontology},
abstract = {Many attempts have been made to enhance irrigation decisions using Decision Support Systems (DSS). These have met with limited success for many reasons, one of which is well known: that DSS encode decision rules (waterbalances, financial models) narrower in scope than the criteria farmers really use to make decisions, thus their advice is of limited value or perhaps entirely irrelevant. To assist irrigation DSS designers build more flexible systems, we suggest they heed decision theory and decision modelling, separately from domain-specific DSS tasks. They may then find better ways of modelling real-world decisions which might allow for wider ranging sets of decision rules than previously. To facilitate this, we review three different decision modelling systems and with each model the seemingly straightforward irrigation decision “How much should I water today?”. In doing this we show how they can assist with wide-ranging rule integration. The systems we chose are: Decision Modelling Notation (DMN) from the business analysis community; the Decision Ontology (DO), a Semantic Web modelling system; and Decision Modelling Ontology (DMO) a formal ontology from Information Systems Engineering. We have determined that each of these modelling systems have useful aspects for irrigation DSS designers, which we list, but that they are not equally useful. Also, none of the systems provide designers with both the best modelling system and best technology & tools. We complete our work with a list of requirements for a future decision modelling system based on the intersection of the strengths of the systems investigated and our perceptions of irrigation DSS need. We believe a future system is possible to make and could serve irrigation DSS designers better than any current system. In future work, we indicate what steps might be taken with existing systems to evolve them in line with our future system requirements. Finally, we conclude with a summary of our findings.}
}
@article{MELLUSO2022103676,
title = {Enhancing Industry 4.0 standards interoperability via knowledge graphs with natural language processing},
journal = {Computers in Industry},
volume = {140},
pages = {103676},
year = {2022},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2022.103676},
url = {https://www.sciencedirect.com/science/article/pii/S0166361522000732},
author = {Nicola Melluso and Irlan Grangel-González and Gualtiero Fantoni},
keywords = {Standards, Interoperability, Natural language processing, Knowledge graphs, Industry 4.0},
abstract = {Industry 4.0 (I4.0) has brought several challenges related to the need to acquire and integrate large amounts of data from multiple sources in order to integrate these elements into an automated manufacturing system. Establishing interoperability is crucial to meet these challenges, and standards development and adoptions play a key role in achieving this. Therefore, academics and industrial stakeholders must join their forces in order to develop methods to enhance interoperability and to mitigate possible conflicts between standards. The aim of this paper is to propose an approach that enhances interoperability between standards through the combined use of Natural Language Processing (NLP) and Knowledge Graphs (KG). In particular, the proposed method is based on the measurement of semantic similarity among the textual content of standards documents belonging to different standardization frameworks. The present study contributes to the research and practice in three ways. First, it fills research gaps concerning the synergy of NLP, KGs and I4.0. Second, it provides an automatic method that improves the process of creating, curating and enriching a KG. Third, it provides qualitative and quantitative evidence of Semantic Interoperability Conflicts (SICs). The experimental results of the application of the proposed method to the I4.0 Standards Knowledge Graph (I40KG) show that different standards are still struggling to use a shared language and that there exists a strong different in the view of I4.0 proposed by the two main standardization frameworks (RAMI and IIRA). By automatically enriching the I40KG with a solid experimental approach, we are paving the way for actionable knowledge which has been extracted from the PDFs and made available in the I40KG.}
}
@article{KIM2025112269,
title = {Technological applications of social robots to create healthy and comfortable smart home environment},
journal = {Building and Environment},
volume = {267},
pages = {112269},
year = {2025},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2024.112269},
url = {https://www.sciencedirect.com/science/article/pii/S0360132324011119},
author = {Hakpyeong Kim and Minjin Kong and Seunghoon Jung and Jaewon Jeoung and Hyuna Kang and Taehoon Hong},
keywords = {Smart home, Home automation, Social robot, Emotion recognition, Personal assistance},
abstract = {The increasing demand for healthy and comfortable living environments has driven significant advancements in smart home technology. However, current developments often overlook the importance of users’ social-emotional needs and the contextual dynamics within the home environment. This study proposes the integration of social robots as a promising solution to address these gaps. By enhancing smart home functionalities, social robots offer a more holistic approach to smart home design. A comprehensive literature review was conducted using the PRISMA methodology combined with large language model-based topic modeling to identify current research trends in social robotics. The analysis revealed five key research areas: (i) social-emotional intelligence, (ii) physical embodiment, (iii) elderly care, (iv) pediatric care, and (v) therapeutic applications. The study discusses how the core functionalities of social robots can enhance user experience by positively influencing the sensing, perception, and action layers of smart home systems. The findings suggest that the evolution of smart home technology should prioritize not only functional improvements but also the social and emotional well-being of users. Integrating social robots into smart homes will foster more human-centric, interactive, and satisfying living environments.}
}
@incollection{SHOAIP201961,
title = {Chapter 4 - Reasoning methodologies in clinical decision support systems: A literature review},
editor = {Nilanjan Dey and Amira S. Ashour and Simon James Fong and Surekha Borra},
booktitle = {U-Healthcare Monitoring Systems},
publisher = {Academic Press},
pages = {61-87},
year = {2019},
series = {Advances in Ubiquitous Sensing Applications for Healthcare},
issn = {25891014},
doi = {https://doi.org/10.1016/B978-0-12-815370-3.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128153703000049},
author = {Nora Shoaip and Shaker El-Sappagh and Sherif Barakat and Mohammed Elmogy},
keywords = {Clinical decision support system (CDSS), Rule-based reasoning (RBR), Fuzzy ontology, Ontology-based fuzzy decision support system (OBFDSS), Case-based reasoning (CBR), Mamdani fuzzy inference},
abstract = {The growing significance of clinical decision support systems (CDSS) has become a positive factor influential in pushing medical care toward success. It depends on using successful and effective reasoning methodologies. This survey aims to give a brief overview of the research directions that are practiced under the domain of reasoning methodologies used in CDSS implementation. It focuses on studying the roles of fuzzy ontology and fuzzy logic in the CDSS implementation given in the scientific literature. We are trying to identify new future trends in this domain. We adopt a search methodology involving the definition of research questions, the determination of selection criteria, and the description of the search strategy. The primary questions of this review are as follows: Which reasoning techniques have been used in CDSS? What is the accuracy of using different reasoning techniques in real applications? What are the limitations of existing reasoning techniques? How to enhance the reasoning process in DSS? The manuscript describes the current published literature in Science Direct, Springer Link, PubMed, and IEEE Xplore from 2009 through November 2017. The search strategy contains four processes: screening papers, selecting papers, extracting and analyzing concepts, and identifying future trends. Our search identified 1886 papers across different electronic databases. These papers are used as an initial database. After reviewing these articles, we selected 134 relevant articles that are more interesting and suitable for the goals of this paper. These relevant articles are included in our critical analysis to find the possible future trends. The literature review showed that case-based reasoning (CBR), Mamdani fuzzy inference, and ontology systems are the most-used reasoning techniques. However, the fuzzy inference failures, the unclear and not unified methods for the fuzzy ontology construction process and tools, the limitations of existing fuzzy description logic reasoners, and the manual case adaptation process in CBR are still the main problems and might not support the clinical practice effectively. Most of these models used ontology and fuzzy logic as two separate models, and no real overlap occurs. There are some serious points to be discussed to enhance the inference of the fuzzy component. Ontology can be used to enhance the capabilities of the fuzzy inference system. Our solution is the hybridization of regular and mature crisp ontology reasoning with regular and mature Mamdani fuzzy reasoning. We expect that will be the best choice to overcome the current limitations of crisp ontology and fuzzy reasoning. In this paper, the different reasoning methodologies applied to CDSS are analyzed. We are looking to combine ontology and Mamdani fuzzy inference in a hybrid CDSS system. The hybrid model is the most logical step to improve the fuzzy expert system by adding a semantic reasoning process to its capabilities. There are many reasons for this decision. First, the fuzzy expert systems are stable and mathematically proved, and there are many fuzzy reasoners such as Mamdani, etc. In addition, crisp ontology and its special case of (standard) medical ontologies have stable, crisp description logic such as SROIQD, well-known languages such as OWL 2, and well-established reasoners.}
}
@article{GAO2024103523,
title = {Mining tourist preferences and decision support via tourism-oriented knowledge graph},
journal = {Information Processing & Management},
volume = {61},
number = {1},
pages = {103523},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103523},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323002601},
author = {Jialiang Gao and Peng Peng and Feng Lu and Christophe Claramunt and Peiyuan Qiu and Yang Xu},
keywords = {Tourism knowledge graph, Information extraction, Spatiotemporal analysis, Knowledge reasoning, Decision-making support},
abstract = {Currently, tourism management research is focused on comprehending the fluctuating tourist preferences and devising targeted development strategies through extensive analysis of heterogenous user-generated contents. However, given the online reviews of attractions involve overabundant mixed and intangible dimensions, the widely-used unsupervised text mining could be incomplete or inaccurate. Furthermore, the existing literature typically restricted to the certain types of attractions within several tourist destinations and origins, can hardly guarantee comprehensive insights. To overcome these limitations, the study proposes a novel knowledge-graph-driven framework, involving the systematic construction as well as the thorough investigation and inference of a tourism-oriented knowledge graph (TKG). Following the ontology of domain expertise, 11,296,716 structured triplets of multifaceted knowledge about 1,174,034 tourists and 20,481 attractions within all 340 city-level destinations across China are extracted from multi-source text corpus by the transferring learning on pre-training language model with 43.64–50.65 % accuracy enhancement. In virtue of TKG, a comprehensive decision-support system can be established, which bifurcates into two distinct modes of knowledge application: symbolic query and distributed reasoning. Through the implementation of multiple spatiotemporal analyses via SPARQL queries on TKG, the distribution regularities of tourist preference, causal interpretations, and their effects on destination development can be progressively detected. Refining the distributed representations of objects by injecting abundant contextual knowledge from TKG can significantly enhance the downstream inferential tasks, such as tourist demand prediction and attraction competitive intelligence.}
}
@article{DWIVEDI2024102725,
title = {Artificial intelligence (AI) futures: India-UK collaborations emerging from the 4th Royal Society Yusuf Hamied workshop},
journal = {International Journal of Information Management},
volume = {76},
pages = {102725},
year = {2024},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102725},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223001068},
author = {Yogesh K. Dwivedi and Laurie Hughes and Harshad K.D.H. Bhadeshia and Sophia Ananiadou and Anthony G. Cohn and Jacqueline M. Cole and Gareth J. Conduit and Maunendra Sankar Desarkar and Xinwei Wang},
keywords = {Artificial intelligence, ChatGPT, Generative AI, Gen AI, Large language models, Technological disruption, uncertainties, Natural Language Processing},
abstract = {“Artificial Intelligence” in all its forms has emerged as a transformative technology that is in the process of reshaping many aspects of industry and wider society at a global level. It has evolved from a concept to a technology that is driving innovation, transforming productivity and disrupting existing business models across numerous sectors. The industrial and societal impact of AI is profound and multifaceted, offering opportunities for growth, efficiency, and improved healthcare, but also raising ethical and societal challenges as the method is integrated into many aspects of human life and work. This editorial is developed by contributors of the 4th Royal Society Yusef Hamied Workshop ( in 2023 devoted to Artificial Intelligence), designed to enhance collaboration between Indian and the UK scientists and to explore future research opportunities. The insights shared at the workshop are shared here.}
}
@article{TANEJA2023104341,
title = {Developing a Knowledge Graph for Pharmacokinetic Natural Product-Drug Interactions},
journal = {Journal of Biomedical Informatics},
volume = {140},
pages = {104341},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104341},
url = {https://www.sciencedirect.com/science/article/pii/S153204642300062X},
author = {Sanya B. Taneja and Tiffany J. Callahan and Mary F. Paine and Sandra L. Kane-Gill and Halil Kilicoglu and Marcin P. Joachimiak and Richard D. Boyce},
keywords = {Biomedical ontology, Interactions, Knowledge graph, Knowledge representation, Literature-based discovery, Natural products, Pharmacokinetics},
abstract = {Background
Pharmacokinetic natural product-drug interactions (NPDIs) occur when botanical or other natural products are co-consumed with pharmaceutical drugs. With the growing use of natural products, the risk for potential NPDIs and consequent adverse events has increased. Understanding mechanisms of NPDIs is key to preventing or minimizing adverse events. Although biomedical knowledge graphs (KGs) have been widely used for drug-drug interaction applications, computational investigation of NPDIs is novel. We constructed NP-KG as a first step toward computational discovery of plausible mechanistic explanations for pharmacokinetic NPDIs that can be used to guide scientific research.
Methods
We developed a large-scale, heterogeneous KG with biomedical ontologies, linked data, and full texts of the scientific literature. To construct the KG, biomedical ontologies and drug databases were integrated with the Phenotype Knowledge Translator framework. The semantic relation extraction systems, SemRep and Integrated Network and Dynamic Reasoning Assembler, were used to extract semantic predications (subject-relation-object triples) from full texts of the scientific literature related to the exemplar natural products green tea and kratom. A literature-based graph constructed from the predications was integrated into the ontology-grounded KG to create NP-KG. NP-KG was evaluated with case studies of pharmacokinetic green tea- and kratom-drug interactions through KG path searches and meta-path discovery to determine congruent and contradictory information in NP-KG compared to ground truth data. We also conducted an error analysis to identify knowledge gaps and incorrect predications in the KG.
Results
The fully integrated NP-KG consisted of 745,512 nodes and 7,249,576 edges. Evaluation of NP-KG resulted in congruent (38.98% for green tea, 50% for kratom), contradictory (15.25% for green tea, 21.43% for kratom), and both congruent and contradictory (15.25% for green tea, 21.43% for kratom) information compared to ground truth data. Potential pharmacokinetic mechanisms for several purported NPDIs, including the green tea-raloxifene, green tea-nadolol, kratom-midazolam, kratom-quetiapine, and kratom-venlafaxine interactions were congruent with the published literature.
Conclusion
NP-KG is the first KG to integrate biomedical ontologies with full texts of the scientific literature focused on natural products. We demonstrate the application of NP-KG to identify known pharmacokinetic interactions between natural products and pharmaceutical drugs mediated by drug metabolizing enzymes and transporters. Future work will incorporate context, contradiction analysis, and embedding-based methods to enrich NP-KG. NP-KG is publicly available at https://doi.org/10.5281/zenodo.6814507. The code for relation extraction, KG construction, and hypothesis generation is available at https://github.com/sanyabt/np-kg.}
}
@article{KONETAPSOBA20231161,
title = {Interoperability approach for Hospital Information Systems based on the composition of web services},
journal = {Procedia Computer Science},
volume = {219},
pages = {1161-1168},
year = {2023},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN – International Conference on Project MANagement / HCist – International Conference on Health and Social Care Information Systems and Technologies 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.397},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923004064},
author = {Lydie Simone {KONE TAPSOBA} and Yaya TRAORE and Sadouanouan MALO},
keywords = {Web service composition, Hospital Information System, Interoperability, ontology},
abstract = {Nowadays, many health centers use Hospital Information Systems (HIS) for the daily management of center activities such as patient data management. In Burkina Faso, many HIS are used for patient data management but these systems cannot cooperate because the data sources are often heterogeneous. In order to guarantee a better diagnosis in patient management, physicians need to access these data sources in a unique way through queries. For better analysis of a patient's situation, the health worker may want to access multiple data sources that do not belong to the same health center. This is only possible if the HIS involved interoperate. In a previous work, we proposed an interoperability architecture based on semantic web services. This solution has the advantage of not modifying the current organization of health centers. Indeed, for a complex query, a composition of web services is a solution to satisfy different needs. In this paper, we detail our approach for composing semantic web services. In our approach, the functionalities of each HIS application will be implemented by a web service semantically annotated by an ontology. An ontology-based mediation service is used to enrich the physician query and the MiniCon algorithm to create a composite web service. The composite web service is executed to return the requested data.}
}
@article{NAKOS2025101378,
title = {Towards knowledge autonomy in the Companion cognitive architecture},
journal = {Cognitive Systems Research},
volume = {92},
pages = {101378},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2025.101378},
url = {https://www.sciencedirect.com/science/article/pii/S1389041725000580},
author = {Constantine Nakos and Kenneth D. Forbus},
keywords = {Companion cognitive architecture, cognitive architecture, knowledge management, knowledge representation, knowledge base, provenance},
abstract = {One of the fundamental aspects of cognitive architectures is their ability to encode and manipulate knowledge. Without a consistent, well-designed, and scalable knowledge management scheme, an architecture will be unable to move past toy problems and tackle the broader problems of cognition. Moreover, it will not be able to reach a state of knowledge autonomy, in which the architecture has the tools it needs to acquire and maintain knowledge on its own. In this paper, we document some of the challenges we have faced in developing the knowledge stack for the Companion cognitive architecture and discuss the tools, representations, and practices we have developed to overcome them. We also lay out a series of next steps that will allow Companions to play a greater role in managing their own knowledge, an important part of knowledge autonomy. It is our hope that these observations will prove useful to other cognitive architecture developers facing similar challenges.}
}
@article{ZURAWSKI20224132,
title = {Applying categories with prototypes to incomplete observations in artificial cognitive agents},
journal = {Procedia Computer Science},
volume = {207},
pages = {4132-4141},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.476},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922013722},
author = {Marcin Żurawski and Grzegorz Popek and Radoslaw Katarzyniak},
keywords = {prototype theory, artificial cognition, autonomous system, cognitive semantics, epistemic modality},
abstract = {Studies show that artificial cognitive agents can be equipped with mechanisms for learning of basic language categories. However, after a period of initial learning performed under perfect circumstances, agents get to be deployed into dynamic real world environments leading to possibly partial observations of agent's surroundings. This paper presents a general strategy for applying categories with prototypes on incomplete observational data. It is assumed that the task is carried out by an artificial agent which has autonomously developed its private ontological knowledge base using complete observations. The agent expresses its internal uncertainty about an assignment of a category to an observed object by relying on epistemic modal operators of possibility, belief, and knowledge. An underlying theory builds upon accomplishments of a theory of grounding of feasible epistemic statements in artificial cognitive agents.}
}
@article{SHAHRIAR2025101787,
title = {The role of generative artificial intelligence in digital agri-food},
journal = {Journal of Agriculture and Food Research},
volume = {20},
pages = {101787},
year = {2025},
issn = {2666-1543},
doi = {https://doi.org/10.1016/j.jafr.2025.101787},
url = {https://www.sciencedirect.com/science/article/pii/S2666154325001589},
author = {Sakib Shahriar and Maria G. Corradini and Shayan Sharif and Medhat Moussa and Rozita Dara},
keywords = {Generative artificial intelligence, Agri-food, Digital agriculture, Smart food, Food security, Food quality, Food safety, Food authenticity, Sustainable agriculture},
abstract = {The agriculture and food (agri-food) sector faces rising global concerns about its sustainability and resilience to climate events. Thus, new solutions are needed to ensure environmental and food security. Artificial Intelligence (AI) offers inventive solutions to improve agricultural and food production practices. Generative AI methods, such as generative adversarial networks (GANs), variational autoencoders, and large language models (LLMs), add to the transformative process initiated by AI and expert systems in agricultural and food-related practices to enhance productivity, sustainability, and resilience. This study categorizes generative AI approaches and their capabilities in agri-food systems and provides a comprehensive review of the current landscape of generative AI applications in the sector. It discusses the impact of these technologies on enhancing agricultural productivity, food quality, and safety, as well as sustainability, presenting potential use cases like combatting climate change and foodborne disease modeling that highlight the practical applications and benefits of generative AI in agri-food. Furthermore, it addresses the ethical implications of deploying generative AI, including privacy, security, reliability, and unbiased decision-making.}
}
@article{HAN2022103984,
title = {Classifying social determinants of health from unstructured electronic health records using deep learning-based natural language processing},
journal = {Journal of Biomedical Informatics},
volume = {127},
pages = {103984},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2021.103984},
url = {https://www.sciencedirect.com/science/article/pii/S1532046421003130},
author = {Sifei Han and Robert F. Zhang and Lingyun Shi and Russell Richie and Haixia Liu and Andrew Tseng and Wei Quan and Neal Ryan and David Brent and Fuchiang R. Tsui},
keywords = {Social determinants of health, Natural language processing, Deep learning, Electronic health records},
abstract = {Objective
Social determinants of health (SDOH) are non-medical factors that can profoundly impact patient health outcomes. However, SDOH are rarely available in structured electronic health record (EHR) data such as diagnosis codes, and more commonly found in unstructured narrative clinical notes. Hence, identifying social context from unstructured EHR data has become increasingly important. Yet, previous work on using natural language processing to automate extraction of SDOH from text (a) usually focuses on an ad hoc selection of SDOH, and (b) does not use the latest advances in deep learning. Our objective was to advance automatic extraction of SDOH from clinical text by (a) systematically creating a set of SDOH based on standard biomedical and psychiatric ontologies, and (b) training state-of-the-art deep neural networks to extract mentions of these SDOH from clinical notes.
Design
A retrospective cohort study.
Setting and participants
Data were extracted from the Medical Information Mart for Intensive Care (MIMIC-III) database. The corpus comprised 3,504 social related sentences from 2,670 clinical notes.
Methods
We developed a framework for automated classification of multiple SDOH categories. Our dataset comprised narrative clinical notes under the “Social Work” category in the MIMIC-III Clinical Database. Using standard terminologies, SNOMED-CT and DSM-IV, we systematically curated a set of 13 SDOH categories and created annotation guidelines for these. After manually annotating the 3,504 sentences, we developed and tested three deep neural network (DNN) architectures – convolutional neural network (CNN), long short-term memory (LSTM) network, and the Bidirectional Encoder Representations from Transformers (BERT) – for automated detection of eight SDOH categories. We also compared these DNNs to three baselines models: (1) cTAKES, as well as (2) L2-regularized logistic regression and (3) random forests on bags-of-words. Model evaluation metrics included micro- and macro- F1, and area under the receiver operating characteristic curve (AUC).
Results
All three DNN models accurately classified all SDOH categories (minimum micro-F1 = 0.632, minimum macro-AUC = 0.854). Compared to the CNN and LSTM, BERT performed best in most key metrics (micro-F1 = 0.690, macro-AUC = 0.907). The BERT model most effectively identified the “occupational” category (F1 = 0.774, AUC = 0.965) and least effectively identified the “non-SDOH” category (F = 0.491, AUC = 0.788). BERT outperformed cTAKES in distinguishing social vs non-social sentences (BERT F1 = 0.87 vs. cTAKES F1 = 0.06), and outperformed logistic regression (micro-F1 = 0.649, macro-AUC = 0.696) and random forest (micro-F1 = 0.502, macro-AUC = 0.523) trained on bag-of-words.
Conclusions
Our study framework with DNN models demonstrated improved performance for efficiently identifying a systematic range of SDOH categories from clinical notes in the EHR. Improved identification of patient SDOH may further improve healthcare outcomes.}
}
@article{JIA2022101706,
title = {From simple digital twin to complex digital twin Part I: A novel modeling method for multi-scale and multi-scenario digital twin},
journal = {Advanced Engineering Informatics},
volume = {53},
pages = {101706},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101706},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001641},
author = {Wenjie Jia and Wei Wang and Zhenzu Zhang},
keywords = {Complex digital twin, Simple digital twin, Digital twin modeling, Smart manufacturing},
abstract = {In recent years, the digital twin has attracted widespread attention as an important means of digitalization and intelligence. However, the digital twin is becoming more and more complex due to the expansion of need on the simulation of multi-scale and multi-scenario in reality. The instance of digital twin in references mostly concentrates a particular application, while it is still a lack of a method for constructing the complex digital twin in the total elements, the variable scale of working environments, changeable process, not even the coupling effects. In this paper, a novel modeling method for such a complex digital twin is proposed based on the standardized processing on the model division and assembly. Firstly, the complex model of digital twin is divided into several simple models according to the composition, context, component, and code in 4C architecture. Composition and context make the digital twin focus on the effective elements in a specific scale and scenario. Component and code develop the digital twin in standard-based modularization. Secondly, assemble the simple models of digital twins into the complex model through information fusion, multi-scale association and multi-scenarios iterations. Ontology establishes the complete information library of the entities on different digital twins. Knowledge graph bridges the structure relationship between the different scales of digital twins. The scenario iterations realize the behavior interaction and the accuracy calculation results. It provides an implementable method to construct a complex model of digital twin, and the reuse of components and code also enables rapid development of digital twins.}
}
@article{JAMES2025103079,
title = {R-index: a standardized representativeness metric for benchmarking diversity, equity, and inclusion in biopharmaceutical clinical trial development},
journal = {eClinicalMedicine},
volume = {80},
pages = {103079},
year = {2025},
issn = {2589-5370},
doi = {https://doi.org/10.1016/j.eclinm.2025.103079},
url = {https://www.sciencedirect.com/science/article/pii/S2589537025000112},
author = {Spencer L. James and Max Bourgognon and Patricia Pinto Vieira and Bruno Jolain and Sarah Bentouati and Emma Kipps and Assaf P. Oron and Catherine W. Gillespie and Ruma Bhagat and Altovise Ewing and Shalini Hede and Keith Dawson and Nicole Richie},
keywords = {Representativeness, Clinical trials, Advancing inclusive research},
abstract = {Summary
Background
Diversity, equity, and inclusion pertaining to race, ethnicity, and related concepts have historically been underrepresented in clinical trials for pharmaceutical drug development, although this is an increasing topic for regulators, payers, and patient advocacy groups. We aimed to develop a summary statistical measure to assess such representativeness.
Methods
A statistical measure using population demographic parameters derived from performance metrics through verbal autopsy research was proposed for using population frameworks in the UK. The summary measure, R-index, was demonstrated using simulation data with population frameworks from the UK (116 Roche UK clinical trials 2013–2022) and then using published clinical trial results (NCT02366143 [March 1, 2015–September 15, 2017], NCT04368728 [July 27, 2020–October 9, 2020], and NCT04470427 [July 27, 2020–November 25, 2020]). R-index was further proposed for use with benchmarking performance in representative trial development for internal processes, external benchmarking, and performance tracking in clinical trial development.
Findings
R-index was derived from a standardized statistical measure called the L1 norm, or Manhattan distance, and then normalized to the maximum theoretical error observed in some populations using population framework or ontology for reporting concepts such as race, ethnicity, and other dimensions of diversity used to characterize patient cohorts. R-index demonstrated desirable qualities in demonstration simulations, including a range of 0–1, ease of calculation and use, and interpretability and flexibility, as data standards in the space of inclusive research continue to develop.
Interpretation
R-index is an interpretable, accessible summary statistic that may be useful for tracking and benchmarking representativeness in inclusive research and related domains. R-index is adaptable to different population frameworks and ontologies across different settings and considerations in terms of underlying population variables.
Funding
F. Hoffmann-La Roche Ltd/Genentech, Inc.}
}
@article{COLLINGE2022104391,
title = {BIM-based construction safety risk library},
journal = {Automation in Construction},
volume = {141},
pages = {104391},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104391},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002643},
author = {William H. Collinge and Karim Farghaly and Mojgan Hadi Mosleh and Patrick Manu and Clara Man Cheung and Carlos A. Osorio-Sandoval},
keywords = {Building information modelling, BIM, Design for Safety, Prevention through design, Construction safety, Health and safety, Safety in design, Ontology, Risk scenarios},
abstract = {This paper presents a digital tool and Safety Risk library to assist designers in their health and safety work in BIM digital environments. Addressing an industry need for improved knowledge sharing and collaboration, the BIM Safety Risk library tool aligns with a Prevention through Design (PtD) approach that links safety risks to treatments via different risk scenarios. Motivated by continuing sub-optimal health and safety management processes, the research employs a conceptual framework rooted in construction guidance: structuring data via a 7-stage ontology to improve designer knowledge of issues and give access to an expanding safety knowledge base (the BIM Safety Risk Library). The tool facilitates tacit and explicit knowledge sharing in visual environments, enabling the construction industry to benefit from their health and safety data while providing an interactive learning tool for designers. The structuring of data also opens up possibilities for other digital advances (e.g. via automatic rule checking).}
}
@article{KISELEV2020373,
title = {An Overview of Massive Open Online Course Platforms: Personalization and Semantic Web Technologies and Standards},
journal = {Procedia Computer Science},
volume = {169},
pages = {373-379},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.232},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303550},
author = {Boris Kiselev and Vyacheslav Yakutenko},
keywords = {MOOC platforms, personalization, Semantic Web},
abstract = {Massive Open Online Course (MOOC) is a form of online education that provides great learning capabilities. Semantic Web technologies is an appropriate mechanism for personalization in MOOC platforms. The aim of this paper is to find out how Semantic Web is used to facilitate personalization in modern MOOC platforms. The paper describes state-of-the-art MOOC platforms from the position of personalization and Semantic Web features. We defined five personalization and five Semantic Web criteria as well as 20 MOOC platforms to review. The personalization criteria includes a personalized learning path, personalized navigation, recommendation system, personalized assessment, and personalized feedback. The Semantic Web criteria includes ontology, Resource Description Framework (RDF), Web Ontology Language (OWL), SPARQL Protocol and RDF Query Language (SPARQL), and Linked Data. The results show that most of the platforms support personalized feedback. Half of the platforms has personalized learning path tools. One third of the platforms allow personalized assessment. Three platforms recommend learning materials, and one platform allows personalized navigation. The selected platforms have poor Semantic Web technologies and standards support: three platforms use ontologies and none of the platforms supports other criteria: RDF, OWL, SPARQL, and Linked Data. Personalization tools are supported better than Semantic Web tools. Most of the platforms have no support for Semantic Web criteria. This means that currently Semantic Web is not used for personalization in the reviewed MOOC platforms.}
}
@article{BAZOGE2023,
title = {Applying Natural Language Processing to Textual Data From Clinical Data Warehouses: Systematic Review},
journal = {JMIR Medical Informatics},
volume = {11},
year = {2023},
issn = {2291-9694},
doi = {https://doi.org/10.2196/42477},
url = {https://www.sciencedirect.com/science/article/pii/S2291969423000510},
author = {Adrien Bazoge and Emmanuel Morin and Béatrice Daille and Pierre-Antoine Gourraud},
keywords = {natural language processing, data warehousing, clinical data warehouse, artificial intelligence, AI},
abstract = {Background
In recent years, health data collected during the clinical care process have been often repurposed for secondary use through clinical data warehouses (CDWs), which interconnect disparate data from different sources. A large amount of information of high clinical value is stored in unstructured text format. Natural language processing (NLP), which implements algorithms that can operate on massive unstructured textual data, has the potential to structure the data and make clinical information more accessible.
Objective
The aim of this review was to provide an overview of studies applying NLP to textual data from CDWs. It focuses on identifying the (1) NLP tasks applied to data from CDWs and (2) NLP methods used to tackle these tasks.
Methods
This review was performed according to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. We searched for relevant articles in 3 bibliographic databases: PubMed, Google Scholar, and ACL Anthology. We reviewed the titles and abstracts and included articles according to the following inclusion criteria: (1) focus on NLP applied to textual data from CDWs, (2) articles published between 1995 and 2021, and (3) written in English.
Results
We identified 1353 articles, of which 194 (14.34%) met the inclusion criteria. Among all identified NLP tasks in the included papers, information extraction from clinical text (112/194, 57.7%) and the identification of patients (51/194, 26.3%) were the most frequent tasks. To address the various tasks, symbolic methods were the most common NLP methods (124/232, 53.4%), showing that some tasks can be partially achieved with classical NLP techniques, such as regular expressions or pattern matching that exploit specialized lexica, such as drug lists and terminologies. Machine learning (70/232, 30.2%) and deep learning (38/232, 16.4%) have been increasingly used in recent years, including the most recent approaches based on transformers. NLP methods were mostly applied to English language data (153/194, 78.9%).
Conclusions
CDWs are central to the secondary use of clinical texts for research purposes. Although the use of NLP on data from CDWs is growing, there remain challenges in this field, especially with regard to languages other than English. Clinical NLP is an effective strategy for accessing, extracting, and transforming data from CDWs. Information retrieved with NLP can assist in clinical research and have an impact on clinical practice.}
}
@article{HOLE2023371,
title = {Handle with care; considerations of Braun and Clarke's approach to thematic analysis},
journal = {Qualitative Research Journal},
volume = {24},
number = {4},
pages = {371-383},
year = {2023},
issn = {1443-9883},
doi = {https://doi.org/10.1108/QRJ-08-2023-0132},
url = {https://www.sciencedirect.com/science/article/pii/S1443988323000802},
author = {Lee Hole},
keywords = {Thematic analysis, Ontology, Epistemology, Qualitative research},
abstract = {Purpose
The purpose of this paper is to support potential users of thematic analysis (as outlined by Virginia Braun and Victoria Clarke). Researchers with the intention of applying thematic analysis are advised to consider the theoretical framework of their work and how differing ontological and epistemological standpoints influences their approach to thematic analysis.
Design/methodology/approach
This paper considers aspects of the work that has been done around thematic analysis to guide future potential users. The flexibility, recipe-like use and ease of thematic analysis are discussed, along with guidance being offered to avoid the seemingly common trip hazards of navigating the approach.
Findings
Users of thematic analysis seemingly continue to cite Braun and Clarke's thematic analysis whilst repeatedly contradicting the guidance of their work.
Practical implications
Readers of this paper that intend on using thematic analysis will be redirect to further learning, personal reflection and adjustments to the way in which they engage with, utilise and report their qualitative work using Braun and Clarke's approach to thematic analysis.
Social implications
It is possible that past research that has been reported as using Braun and Clarke's approach to thematic analysis has been misinterpreted, misunderstood and misused. This as a result of researchers potentially having failed to embrace the need to engage in reflexive, epistemological and ontologically clear processes during the use of thematic analysis.
Originality/value
While Braun and Clarke's approach to thematic analysis seems to have developed a significant level of popularity and use, the finer but impactful understanding of the approach has been overlooked. Other work has been done in relation to thematic analysis but there has not been anything to support thinking and learning around the suitable, accurate and knowledgeable use of the approach.}
}
@article{LEE2025115039,
title = {Metadata schema for virtual building models in digital twins: VB schema implemented in GPT-based applications},
journal = {Energy and Buildings},
volume = {327},
pages = {115039},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115039},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824011551},
author = {Jeyoon Lee and Sungmin Yoon},
keywords = {GPT, Virtual models, Virtual buildings, Ontology, HVAC, Digital twins, Model metadata, Operation and maintenance (O&M), Built environments},
abstract = {A virtual building model (VBM) is a virtual entity that represents the physical behavior of a target building mathematically within a digital twin environment. The creation and synchronization of a VBM are achieved by utilizing various interrelated virtual sub-models, including behavior, correction, and distance models. To achieve continuous digital twinning, it is essential to manage the VBM with virtual sub-models. However, existing metadata schemas have limitations in describing VBMs representing operational building behaviors within the concept of building digital twins (DTs). Therefore, this study proposes a novel metadata schema, termed the virtual building model metadata schema (VB schema), to represent and manage VBMs in DT-built environments. The VB schema is established according to the mathematical and semantic ontology of the in-situ modeling and calibration approach for constructing and correcting virtual models during building operations, and it is linked to physical entities, data, and applications within DTs. Specifically, it involves: (1) determining classes for operational data and virtual models; (2) establishing relationships for interactions between model and data entities, between model classes, between model and physical entities, and between model and applications; (3) defining properties for each class of models; and (4) extending into the exiting metadata schema of Brick. To demonstrate the proposed VB schema, a virtual model describing supply pressure behaviors in a central heating system was developed and represented using the VB schema for DT-enabled building operations. Additionally, the VB schema was utilized for implementing generative pre-trained transformer (GPT)-based DT applications, which highlights its benefits in enhancing ontology comprehension of DTs in the context of VBMs, improving autonomous problem-solving capabilities in real building systems, and providing better interpretation of application results compared to cases where only the Brick schema was used. The VB schema is expected to enable continuous and autonomous in-situ management of VBMs for intelligent building services within the DT.}
}
@article{ARVOR2021112615,
title = {Towards user-adaptive remote sensing: Knowledge-driven automatic classification of Sentinel-2 time series},
journal = {Remote Sensing of Environment},
volume = {264},
pages = {112615},
year = {2021},
issn = {0034-4257},
doi = {https://doi.org/10.1016/j.rse.2021.112615},
url = {https://www.sciencedirect.com/science/article/pii/S0034425721003357},
author = {Damien Arvor and Julie Betbeder and Felipe R.G. Daher and Tim Blossier and Renan {Le Roux} and Samuel Corgne and Thomas Corpetti and Vinicius {de Freitas Silgueiro} and Carlos Antonio da {Silva Junior}},
keywords = {Land cover, Sentinel-2, Time series, Knowledge-driven, Ontologies, Amazon},
abstract = {Land cover mapping over large areas is essential to address a wide spectrum of socio-environmental challenges. For this reason, many global or regional land cover products are regularly released to the scientific community. Yet, the remote sensing community has not fully addressed the challenge to extract useful information from vast volumes of satellite data. Especially, major limitations concern the use of inadequate classification schemes and “black box” methods that may not match with end-users conceptualization of geographic features. In this paper, we introduce a knowledge-driven methodological approach to automatically process Sentinel-2 time series in order to produce pre-classifications that can be adapted by end-users to match their requirements. The approach relies on a conceptual framework inspired from ontologies of scientific observation and geographic information to describe the representation of geographic entities in remote sensing images. The implementation consists in a three-stage classification system including an initial stage, a dichotomous stage and a modular stage. At each stage, the system firstly relies on natural language semantic descriptions of time series of spectral signatures before assigning labels of land cover classes. The implementation was tested on 75 time series of Sentinel-2 images (i.e. 2069 images) in the Southern Brazilian Amazon to map natural vegetation and water bodies as required by a local end-user, i.e. a non-governmental organization. The results confirmed the potential of the method to accurately detect water bodies (F-score = 0.874 for bodies larger than 10 m) and map natural vegetation (max F-score = 0.875), yet emphasizing the spatial heterogeneity of accuracy results. In addition, it proved to be efficient to provide rapid estimates of degraded riparian forests at watershed level (R2 = 0.871). Finally, we discuss potential improvements both in the system's implementation, e.g. considering additional characteristics, and in the conceptual framework, e.g. moving from pixel- to object-based image analysis and evolving towards a hybrid system combining data- and knowledge-driven approaches.}
}
@article{SOARES2024109044,
title = {An approach to foster agribusiness marketing applying data analysis of social network},
journal = {Computers and Electronics in Agriculture},
volume = {222},
pages = {109044},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109044},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924004356},
author = {Nedson D. Soares and Regina Braga and José Maria N. David and Kennya B. Siqueira and Victor Stroele},
keywords = {Data analysis, Social networks, Agribusiness marketing, Dairy derivatives, Cheese market},
abstract = {Context
Applying social network data analysis to the agribusiness context can be useful to increase profitability, mainly in the dairy derivatives niche.
Problem
The dairy derivatives market needs to recover its profitability. After a 2.9 % GDP growth in 2022,1Source https://www.canalrural.com.br/noticias/pecuaria/leite/leite-confira-como-esta-o-mercado-brasileiro/ (in Portuguese).1 Canal Rural the economic projections indicated only 0.91 % in 2023. Specific strategies to foster this market need to be applied.
Objective
To collect information from social networks to find influential people who appreciate dairy derivatives and can influence new potential consumers, we present the IntelDigitalMarketing architecture. Its features encompass social network analysis, recommendations, and context propagation. Through its use, influencers and user communities can be identified who address issues related to specific domains in different social networks, and who can disseminate information to foster specific market niches.
Method
We used the Design Science Research methodology to conduct the study. The solution encompasses techniques such as complex networks, machine learning, and ontologies, to detect market trends. With IntelDigitalMarketing architecture, we processed social network data from X (formerly Twitter), Instagram, and YouTube.
Results
The results showed that the solution can search for communities of digital influencers who talk about dairy derivatives, what they talk about, and the dissemination of information on these social networks.
Contributions and impact
With the combination of techniques, we can detect new relevant relationships among users that are not detected by other similar solutions. In addition, the proposed solution is online and in real-time, making it easier to follow trends in social networks and with the potential to foster the Agribusiness market.}
}
@article{CHAUHAN2024489,
title = {KRIOTA: A framework for Knowledge-management of dynamic Reference Information and Optimal Task Assignment in hybrid edge–cloud environments to support situation-aware robot-assisted operations},
journal = {Future Generation Computer Systems},
volume = {160},
pages = {489-504},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24003157},
author = {Muhammad Aufeef Chauhan and Muhammad Ali Babar and Haifeng Shen},
keywords = {Dynamic Reference Information (DRI), Autonomous robotic system (ARS), Dynamic tasks allocation, Edge–cloud environment, Knowledge management},
abstract = {Enabling an autonomous robotic system (ARS) to be aware of its operating environment can equip the system to deal with unknown and uncertain situations. While several conceptual models have been proposed to establish the fundamental concepts of situational awareness, it remains a challenge to make an ARS situation aware, in particular using a combination of low-cost resource-constraint robots at the tactical edge and powerful remote cloud nodes. This paper proposes a dynamic reference information (DRI) based knowledge management and optimal task assignment framework that manages knowledge extracted from DRI to assess the current situation as per given mission objectives and assigns tasks to different computing nodes, which include a combination of edge robots, edge computing nodes and cloud-hosted services. The proposed framework is referred to as KRIOTA. The framework has been designed using an architecture-centric approach. We have designed ontologies to classify and structure different elements of DRI hierarchically and associate the processing components of an ARS with the DRI. We have devised algorithms for the ARS to optimally assign tasks to relevant processing components on robots, edge computing nodes and cloud-hosted services for adaptive behaviour. We have evaluated the framework by demonstrating its implementation in a testbed named RoboPatrol. We have also demonstrated the performance, effectiveness and feasibility of the KRIOTA framework.}
}
@article{TOMASZUK2020105757,
title = {The molecular entities in linked data dataset},
journal = {Data in Brief},
volume = {31},
pages = {105757},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2020.105757},
url = {https://www.sciencedirect.com/science/article/pii/S235234092030651X},
author = {Dominik Tomaszuk and Łukasz Szeremeta},
keywords = {Linked Data, Semantic Web, RDF, JSON, Cheminformatics, molecular dataset},
abstract = {The Molecular Entities in Linked Data (MEiLD) dataset comprises data of distinct atoms, molecules, ions, ion pairs, radicals, radical ions, and others that can be identifiable as separately distinguishable chemical entities. The dataset is provided in a JSON-LD format and was generated by the SDFEater, a tool that allows parsing atoms, bonds, and other molecule data. MEiLD contains 349,960 of ‘small’ chemical entities. Our dataset is based on the SDF files and is enriched with additional ontologies and line notation data. As a basis, the Molecular Entities in Linked Data dataset uses the Resource Description Framework (RDF) data model. Saving the data in such a model allows preserving the semantic relations, like hierarchical and associative, between them. To describe chemical molecules, vocabularies such as Chemical Vocabulary for Molecular Entities (CVME) and Simple Knowledge Organization System (SKOS) are used. The dataset can be beneficial, among others, for people concerned with research and development tools for cheminformatics and bioinformatics. In this paper, we describe various methods of access to our dataset. In addition to the MEiLD dataset, we publish the Shapes Constraint Language (SHACL) schema of our dataset and the CVME ontology. The data is available in Mendeley Data.}
}
@article{TEMPL2025103109,
title = {A roadmap for advancing plant phenological studies through effective open research data management},
journal = {Ecological Informatics},
volume = {87},
pages = {103109},
year = {2025},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2025.103109},
url = {https://www.sciencedirect.com/science/article/pii/S1574954125001189},
author = {Barbara Templ},
keywords = {FAIR, Phenological data life cycle, Phenological data type, Phenological data format, ORDM challenge, ORDM solution, Plant phenology ontology, Ecological metadata language},
abstract = {Phenological research, critical for understanding ecological dynamics in response to environmental changes, increasingly relies on Open Research Data Management (ORDM) to enhance scientific outcomes. Based on insights from a structured survey conducted among phenology experts, this paper explores how the adoption of FAIR principles - Findability, Accessibility, Interoperability, and Reusability - directly addresses the unique challenges of phenological data, such as inconsistent metadata, variability in data collection methods, and difficulties in data integration. This synthesis not only highlights the obstacles faced by phenologists but also proposes strategic solutions highlighting a clear call to action steering phenological research toward a more collaborative and open science future.}
}
@article{CHANDRA20245,
title = {Mental healthcare systems research during COVID-19: Lessons for shifting the paradigm post COVID-19},
journal = {Urban Governance},
volume = {4},
number = {1},
pages = {5-15},
year = {2024},
issn = {2664-3286},
doi = {https://doi.org/10.1016/j.ugj.2024.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S2664328624000056},
author = {Ajay Chandra and S. D. Sreeganga and Arkalgud Ramaprasad},
keywords = {Mental healthcare systems, Ontology, Research, COVID-19},
abstract = {The mental health effects of the Covid-19 pandemic across the globe have been significant, are ongoing, and will persist for a long time. Mental healthcare systems (MHS) to address these effects have been stressed beyond their limit. They have had to: (a) sense the developments and respond to the changing needs quickly, (b) be agile in obtaining feedback and learning from it in very short cycle times, and (c) immediately integrate their personal local experience, the reported global experience and translate the learning to practice. This intense learning cycle has spawned an enormous corpus of research on MHS during COVID-19 and shifted the paradigm of research. Lessons from the paradigm shift should be embraced and normalized in the roadmap for MHS research post COVID-19. This paper presents an ontology of MHS as a framework to systematically: (a) visualize in structured natural-English the dimensions, elements, and narratives of MHS research, (b) map the emphases and gaps in the research during COVID-19, and (c) develop a roadmap to shift the future research paradigm.}
}
@article{CLARK20201649,
title = {Models of necessity},
journal = {Beilstein Journal of Organic Chemistry},
volume = {16},
pages = {1649-1661},
year = {2020},
issn = {1860-5397},
doi = {https://doi.org/10.3762/bjoc.16.137},
url = {https://www.sciencedirect.com/science/article/pii/S1860539721020740},
author = {Timothy Clark and Martin G Hicks},
keywords = {chemical bonding, chemical ontologies, chemical structure formats, chemical structure representation, chemical structure models, language of chemistry, quantum chemistry},
abstract = {The way chemists represent chemical structures as two-dimensional sketches made up of atoms and bonds, simplifying the complex three-dimensional molecules comprising nuclei and electrons of the quantum mechanical description, is the everyday language of chemistry. This language uses models, particularly of bonding, that are not contained in the quantum mechanical description of chemical systems, but has been used to derive machine-readable formats for storing and manipulating chemical structures in digital computers. This language is fuzzy and varies from chemist to chemist but has been astonishingly successful and perhaps contributes with its fuzziness to the success of chemistry. It is this creative imagination of chemical structures that has been fundamental to the cognition of chemistry and has allowed thought experiments to take place. Within the everyday language, the model nature of these concepts is not always clear to practicing chemists, so that controversial discussions about the merits of alternative models often arise. However, the extensive use of artificial intelligence (AI) and machine learning (ML) in chemistry, with the aim of being able to make reliable predictions, will require that these models be extended to cover all relevant properties and characteristics of chemical systems. This, in turn, imposes conditions such as completeness, compactness, computational efficiency and non-redundancy on the extensions to the almost universal Lewis and VSEPR bonding models. Thus, AI and ML are likely to be important in rationalizing, extending and standardizing chemical bonding models. This will not affect the everyday language of chemistry but may help to understand the unique basis of chemical language.}
}
@article{BARBAGONZALEZ2021103546,
title = {Injecting domain knowledge in multi-objective optimization problems: A semantic approach},
journal = {Computer Standards & Interfaces},
volume = {78},
pages = {103546},
year = {2021},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2021.103546},
url = {https://www.sciencedirect.com/science/article/pii/S0920548921000416},
author = {Cristóbal Barba-González and Antonio J. Nebro and José García-Nieto and María {del Mar Roldán-García} and Ismael Navas-Delgado and José F. Aldana-Montes},
keywords = {Multi-Objective optimization, Decision making, Metaheuristics, Domain knowledge, Semantic web technologies, Ontology},
abstract = {In the field of complex problem optimization with metaheuristics, semantics has been used for modeling different aspects, such as: problem characterization, parameters, decision-maker’s preferences, or algorithms. However, there is a lack of approaches where ontologies are applied in a direct way into the optimization process, with the aim of enhancing it by allowing the systematic incorporation of additional domain knowledge. This is due to the high level of abstraction of ontologies, which makes them difficult to be mapped into the code implementing the problems and/or the specific operators of metaheuristics. In this paper, we present a strategy to inject domain knowledge (by reusing existing ontologies or creating a new one) into a problem implementation that will be optimized using a metaheuristic. Thus, this approach based on accepted ontologies enables building and exploiting complex computing systems in optimization problems. We describe a methodology to automatically induce user choices (taken from the ontology) into the problem implementations provided by the jMetal optimization framework. With the aim of illustrating our proposal, we focus on the urban domain. Concretely, we start from defining an ontology representing the domain semantics for a city (e.g., building, bridges, point of interest, routes, etc.) that allows defining a-priori preferences by a decision maker in a standard, reusable, and formal (logic-based) way. We validate our proposal with several instances of two use cases, consisting in bi-objective formulations of the Traveling Salesman Problem (TSP) and the Radio Network Design problem (RND), both in the context of an urban scenario. The results of the experiments conducted show how the semantic specification of domain constraints are effectively mapped into feasible solutions of the tackled TSP and RND scenarios. This proposal aims at representing a step forward towards the automatic modeling and adaptation of optimization problems guided by semantics, where the annotation of a human expert can be now considered during the optimization process.}
}
@article{MOHAMMADI2021916,
title = {An intelligent simulation-based framework for automated planning of concrete construction works},
journal = {Engineering, Construction and Architectural Management},
volume = {29},
number = {2},
pages = {916-939},
year = {2021},
issn = {0969-9988},
doi = {https://doi.org/10.1108/ECAM-11-2020-0971},
url = {https://www.sciencedirect.com/science/article/pii/S0969998821000916},
author = {Sina Mohammadi and Mehdi Tavakolan and Banafsheh Zahraie},
keywords = {Construction planning, Simulation, Ontology, Building information modeling, 4D BIM, Constraint-based simulation, Semantic reasoning},
abstract = {Purpose
This paper proposes an innovative intelligent simulation-based construction planning framework that introduces a new approach to simulation-based construction planning.
Design/methodology/approach
In this approach, the authors developed an ontological inference engine as an integrated part of a constraint-based simulation system that configures the construction processes, defines activities and manages resources considering a variety of requirements and constraints during the simulation. It allows for the incorporation of the latest project information and a deep level of construction planning knowledge in the planning. The construction planning knowledge is represented by an ontology and several semantic rules. Also, the proposed framework uses the project building information model (BIM) to extract information regarding the construction product and the relations between elements. The extracted information is then converted to an ontological format to be useable by the framework.
Findings
The authors implemented the framework in a case study project and tested its usefulness and capabilities. It successfully generated the construction processes, activities and required resources based on the construction product, available resources and the planning rules. It also allowed for a variety of analyses regarding different construction strategies and resource planning. Moreover, 4D BIM models that provide a very good understanding of the construction plan can be automatically generated using the proposed framework.
Originality/value
The active integration between BIM, discrete-event simulation (DES) and ontological knowledge base and inference engine defines a new class of construction simulation with expandable applications.}
}
@article{DENG2025177678,
title = {Establishment of a murine chronic proximal thoracic aortic aneurysm model by combining periaortic elastase application with oral BAPN administration},
journal = {European Journal of Pharmacology},
volume = {999},
pages = {177678},
year = {2025},
issn = {0014-2999},
doi = {https://doi.org/10.1016/j.ejphar.2025.177678},
url = {https://www.sciencedirect.com/science/article/pii/S0014299925004327},
author = {Jianqing Deng and Lei Tian and Haitao Chi and Lei Chen and Junhui Wang and Yan Xue and Qiang Zhao and Nan Zheng and Jie Dong and Jiaying Li and Wei Guo and Cangsong Xiao and Ming Yang},
keywords = {Thoracic aortic aneurysm, Experimental models, Aneurysmal pathology, Elastase, Aortic, Degenerative disease},
abstract = {This study aimed to develop a chronic proximal thoracic aortic aneurysm (PTAA) model by combining periaortic elastase application with oral administration of 3-aminopropionitrile fumarate salt (BAPN) after surgery. Sixty 8-week-old C57BL/6J male mice were divided into four groups: Sham, Sham + BAPN, Elastase, and Elastase + BAPN. High-resolution micro-ultrasound was performed on days 7, 14, 21, 28, 56, and 90 post-operation to measure aortic diameter. Histopathological, transcriptomic, and bioinformatics analyses were conducted to assess the model's relevance to human PTAA. The operative mortality rate was 10 % (6/60). During follow-up, 4 animals in the elastase + BAPN group and 1 in the elastase group died from aortic rupture. Significant continuous dilation of the proximal thoracic aorta was observed only in the elastase + BAPN group, with average dilation rates of 116.60 %, 178.99 %, and 231.90 % on days 28, 56, and 90, respectively, compared to 66.46 %, 61.13 %, and 68.73 % in the elastase group. Histopathology revealed greater aortic wall thickening, collagen deposition, MMP2 expression, elastin degradation, smooth muscle cell loss, calcification, and immune cell infiltration in the elastase + BAPN group. Transcriptomic analysis identified 3039 differentially expressed genes, enriched in immune and inflammation-related pathways. Weighted gene co-expression network analysis showed significant overlap in the Gene Ontology and Kyoto Encyclopedia of Genes and Genomes pathway enrichment results between human and murine PTAA-related gene modules which were most positively correlated with PTAA diameters. This study establishes a chronic PTAA model that mimics key features of human disease, providing a valuable tool for investigating PTAA mechanisms and developing new therapies.}
}
@article{PENG2020106360,
title = {Knowledge configuration model for fast derivation design of electronic equipment and its implementation},
journal = {Knowledge-Based Systems},
volume = {206},
pages = {106360},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106360},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120305098},
author = {Jun Peng and Wenqiang Li and Meng Wang and Yuegang Song and Xijia Qin},
keywords = {Knowledge template, Ontology, Rapid design, Derivative design, Electronic equipment},
abstract = {There has contradictions in timeliness and diversity in the design schemes of complex product systems. The traditional solidified knowledge template for the knowledge reuse purpose cannot meet the rapid design needs of complex products. This paper proposes a configurable knowledge ontology model (CKOM) and a corresponding design method for rapid derivation of complex products. Based on the combination of basic knowledge elements (BKE), customized knowledge elements (CKE), and assorted knowledge elements (AKE), a knowledge structural body (KSB) is constructed by unitizing knowledge ontology into units of knowledge. And a dynamic structured knowledge template configuration method based on physical form (Pf) configuration and virtual form (Vf) association is proposed. The knowledge configuration information is transmitted and parsed in XML form, and the virtual form is established to associate and dynamically configure the corresponding physical form knowledge structure. Based on the structured knowledge model configuration template, a rapid derivative design method for electronic equipment is proposed. The rapid derivation design process is divided into three stages: requirement analysis, index demonstration, and scheme formation. By establishing seven knowledge attributes, such as global, local, scope, retrieval, the required product knowledge elements, rule knowledge elements and tools are dynamically configured to realize the dynamic processing of design parameters based on configuration knowledge elements and then form a corresponding design scheme. Finally, a rapid derivation design system is developed and applied to electronic equipment, which verifies the rationality and effectiveness of the proposed model and method.}
}
@article{JARVENPAA201887,
title = {Formal Resource and Capability Models supporting Re-use of Manufacturing Resources},
journal = {Procedia Manufacturing},
volume = {19},
pages = {87-94},
year = {2018},
note = {Proceedings of the 6th International Conference in Through-life Engineering Services, University of Bremen, 7th and 8th November 2017},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.01.013},
url = {https://www.sciencedirect.com/science/article/pii/S2351978918300131},
author = {Eeva Järvenpää and Minna Lanz and Niko Siltala},
keywords = {Capability model, Production system representation, Adaptive manufacturing, Ontology},
abstract = {In the field of manufacturing the responsiveness has become a new strategic goal for the enterprises alongside with quality and costs. Efficient responsiveness requires production reconfiguration ranging from layout to equipment. The production system capabilities originate from the tool and equipment level. While a resource is being used, its condition and capability may change. It is crucial to consider the resources’ individual lifecycle, their actual capabilities and condition during the system design and reconfiguration. Thus, the lifecycle perspective in the capability management is of utmost importance. This paper presents the development of the Manufacturing Resource Capability Ontology (MaRCO), focusing on describing the functional capabilities of manufacturing resources. Special emphasis is placed on the lifecycle management aspect of the resource descriptions.}
}
@article{CHANDLER2022102672,
title = {Interstitial and Abyssal geographies},
journal = {Political Geography},
volume = {98},
pages = {102672},
year = {2022},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2022.102672},
url = {https://www.sciencedirect.com/science/article/pii/S0962629822000865},
author = {David Chandler and Jonathan Pugh},
abstract = {Against the backdrop of the contemporary crisis of faith in modern reasoning, work on islands and with island cultures has come to the fore in the development of alternative, non-Eurocentric, non-modern, ways of being and knowing. Much attention has surrounded a wide range of critical work associated with the ‘ontological’ or the ‘relational’ turn, highlighting interstitial, entangled, post- and more-than-human creative encounters of becoming. This paper examines the emergence of what we call ‘abyssal thought’, a related but distinctly different analytical approach drawing largely from critical Black studies. Central to abyssal approaches is the understanding of the world as ontologically inseparable from its violent forging through antiblackness. In putting coloniality at the heart of the modernist problematic, abyssal work turns to the Caribbean in particular as a gateway, door or ‘punctum’, a space of ‘abyssal geographies’, inviting a deconstruction or unmaking of the world. Exploring how, this paper draws out three key aspects of the abyssal analytic: (1) the abyssal ‘subject’ forged through the ontological violence of the making of the modern world, (2) the abyssal as a refusal of impositions of spatial and temporal fixities, and (3) the methodological approach of ‘paraontology’. Thus, its key concerns are those of refusal, deconstruction and ‘suspension’ rather than of creative becoming. In distinction to relational ontologies of interstital island work, the desire is not to save or to remake understandings of the human and the world but rather to negate them.}
}
@article{CASINI2023118409,
title = {Defeasible RDFS via rational closure},
journal = {Information Sciences},
volume = {643},
pages = {118409},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.165},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522014888},
author = {Giovanni Casini and Umberto Straccia},
keywords = {RDFS, Non-monotonic reasoning, Defeasible reasoning, Rational closure},
abstract = {In the field of non-monotonic logics, the notion of Rational Closure (RC) is acknowledged as a notable approach. In recent years, RC has gained popularity in the context of Description Logics (DLs), the logic underpinning the standard semantic Web Ontology Language OWL 2, whose main ingredients are classes, the relationship among classes and roles, which are used to describe the properties of classes. In this work, we show instead how to integrate RC within the triple language RDFS (Resource Description Framework Schema), which together with OWL 2 is a major standard semantic web ontology language. To do so, we start from ρdf, a minimal, but significant RDFS fragment that covers the essential features of RDFS, and then extend it to ρdf⊥, allowing to state that two entities are incompatible/disjoint with each other. Eventually, we propose defeasible ρdf⊥ via a typical RC construction allowing to state default class/property inclusions. Furthermore, to overcome the main weaknesses of RC in our context, i.e., the “drowning problem” (viz. the “inheritance blocking problem”), we further extend our construction by leveraging Defeasible Inheritance Networks (DIN) defining a new non-monotonic inference relation that combines the advantages of both (RC and DIN). To the best of our knowledge this is the first time of such an attempt. In summary, the main features of our approach are: (i) the defeasible ρdf⊥ we propose here remains syntactically a triple language by extending it with new predicate symbols with specific semantics; (ii) the logic is defined in such a way that any RDFS reasoner/store may handle the new predicates as ordinary terms if it does not want to take account of the extra non-monotonic capabilities; (iii) the defeasible entailment decision procedure is built on top of the ρdf⊥ entailment decision procedure, which in turn is an extension of the one for ρdf via some additional inference rules favouring a potential implementation; (iv) the computational complexity of deciding entailment in ρdf and ρdf⊥ are the same; and (v) defeasible entailment can be decided via a polynomial number of calls to an oracle deciding ground triple entailment in ρdf⊥ and, in particular, deciding defeasible entailment can be done in polynomial time.}
}
@incollection{VANDIJK202314,
title = {Dynamic system approaches to language acquisition},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {14-26},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.07041-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818630507041X},
author = {Marijn {van Dijk} and Paul {van Geert}},
keywords = {Dynamic systems theory, Trajectories, Self-organization, Emergence, Process causality, Interactions, Variability, Nonlinearity, First language acquisition, Second language acquisition},
abstract = {Dynamic systems theoretical approaches conceive of language acquisition as a complex system of interacting components or variables. In such a system, language emerges in a communicative context as a result of a process of self-organization. This theory demands a focus on the system as a whole, a radically different perspective on causality, and a renewed appreciation for intra-individual variability and nonlinear forms of change. It requires the analysis of individual trajectories. Various studies have been published that offer dynamic systems interpretations of first and second language acquisition and that have provided empirical support for the existence of its main constructs.}
}
@article{CHEN20251719,
title = {Symptoms of Asthma Extracted Through Natural Language Processing and Their Associations With Acute Asthma Exacerbation in Adults With Mild Asthma},
journal = {The Journal of Allergy and Clinical Immunology: In Practice},
volume = {13},
number = {7},
pages = {1719-1729.e7},
year = {2025},
issn = {2213-2198},
doi = {https://doi.org/10.1016/j.jaip.2025.04.031},
url = {https://www.sciencedirect.com/science/article/pii/S2213219825003897},
author = {Wansu Chen and Eric J. Puttock and Fagen Xie and William Crawford and Michael Schatz and William M. Vollmer and Stanley Xu and Robert S. Zeiger},
keywords = {, , , , , , , , , , },
abstract = {Background
Individuals with mild asthma account for 30% to 40% of asthma exacerbations requiring emergency consultation, and nearly 30% had not–well controlled or poorly controlled asthma symptoms over the previous 4 weeks.
Objective
We sought to estimate the prevalence of various asthma symptoms and assess their association with future acute asthma exacerbations (AAEs) in patients with mild asthma.
Methods
This was a retrospective cohort study. Using administrative data, we identified 198,873 adults aged 18 to 85 years, who met criteria for mild asthma between 2013 and 2018. The presence of cough, wheezing, dyspnea, and chest tightness in the 12 months before the index visit (t0) was extracted from clinical notes and patient/provider communications through natural language processing. We used Poisson regression models with robust SEs to assess the associations between symptoms and AAEs in the 12 months after t0, controlling for potential confounders.
Results
The prevalence of cough, wheezing, dyspnea, and chest tightness was 67.0%, 47.7%, 41.3%, and 13.2%, respectively. Moreover, 6.5% of patients experienced an AAE in the 12 months after t0. All four symptoms were associated with increased AAE risk in the unadjusted analysis. After adjusting for other patient characteristics, only wheezing (adjusted relative risk, 1.13; 99% CI, 1.07-1.20) and dyspnea (1.17; 1.12-1.23) were associated with an increased risk of future AAEs. The risk increased with the frequency of the documented symptoms.
Conclusion
Patients with mild asthma who exhibit symptoms of dyspnea and wheezing (especially on multiple occasions) are at an increased risk for future AAEs and may benefit from therapeutic intervention and/or trigger-avoidance education.}
}
@article{DAVIS2024100151,
title = {Knowledge graphs for seismic data and metadata},
journal = {Applied Computing and Geosciences},
volume = {21},
pages = {100151},
year = {2024},
issn = {2590-1974},
doi = {https://doi.org/10.1016/j.acags.2023.100151},
url = {https://www.sciencedirect.com/science/article/pii/S259019742300040X},
author = {William Davis and Cassandra R. Hunt},
keywords = {Seismology, Seismic data, Knowledge graphs, Ontologies, Semantic models},
abstract = {The increasing scale and diversity of seismic data, and the growing role of big data in seismology, has raised interest in methods to make data exploration more accessible. This paper presents the use of knowledge graphs (KGs) for representing seismic data and metadata to improve data exploration and analysis, focusing on usability, flexibility, and extensibility. Using constraints derived from domain knowledge in seismology, we define a semantic model of seismic station and event information used to construct the KGs. Our approach utilizes the capability of KGs to integrate data across many sources and diverse schema formats. We use schema-diverse, real-world seismic data to construct KGs with millions of nodes, and illustrate potential applications with three big-data examples. Our findings demonstrate the potential of KGs to enhance the efficiency and efficacy of seismological workflows in research and beyond, indicating a promising interdisciplinary future for this technology.}
}
@article{HAVERAAEN2020100543,
title = {Specifying with syntactic theory functors},
journal = {Journal of Logical and Algebraic Methods in Programming},
volume = {113},
pages = {100543},
year = {2020},
issn = {2352-2208},
doi = {https://doi.org/10.1016/j.jlamp.2020.100543},
url = {https://www.sciencedirect.com/science/article/pii/S2352220820300286},
author = {Magne Haveraaen and Markus Roggenbach},
keywords = {Specification languages, Reuse mechanisms, Institution-independence},
abstract = {We propose a framework, syntactic theory functors (STFs), for creating syntactic structuring mechanisms for specification languages. Good support for common reuse patterns is important for systematically developing specifications for large systems. Though immaterial to foundational theory, lack of support otherwise causes lengthy writing of boilerplate code or repeated adaptation from one context to another. We present STFs in the context of the Goguen & Burstall institution theory. This theory captures the essential structure of ontologies, modelling and formal specifications (OMS). In particular it provides powerful structuring mechanisms that are independent of the specification formalism, i.e., they are institution-independent. The presented STF framework is institution-independent as well. As such it encompasses many approaches to software and information systems. STFs subsume the standard institution-independent structuring mechanisms, and open up new ways of reusing existing and structuring new specifications. In this, STFs subsume and enrich the tool-set of ‘good practices’, which includes separation of concerns, ease of reuse of specification-text, and improved theorem proving support. STFs are aimed at structuring and reuse beyond the classical mechanisms. However, most STFs are institution-specific and support specific reuse patterns in that institution. With such institution-specific STFs it is possible to incrementally grow more complex institutions from simpler ones. This is very much needed when developing ontologies or specification languages for a new domain. In this paper, we motivate STFs with examples in Casl, the common standard algebraic specification language. We further demonstrate how STFs can ease specification through capturing repeated constructions once and for all as patterns formulated as STFs.}
}
@article{PUROHIT2025113939,
title = {VANILLA: Validated knowledge graph completion—A Normalization-based framework for Integrity, Link prediction, and Logical Accuracy},
journal = {Knowledge-Based Systems},
volume = {325},
pages = {113939},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113939},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125009840},
author = {Disha Purohit and Yashrajsinh Chudasama and Maria-Esther Vidal},
keywords = {Knowledge graphs, Knowledge graph completion, Symbolic learning, Symbolic constraint validation, Numerical learning},
abstract = {Knowledge graphs (KGs) are expressive data structures for integrating and describing heterogeneous data by unifying factual information and domain knowledge. However, under the Open World Assumption (OWA), the absence of facts does not imply falsity—only incompleteness. Inductive learning methods, particularly numerical techniques such as Knowledge Graph Embeddings (KGEs) and Graph Neural Networks (GNNs), are widely used for link prediction and classification tasks in KGs. These models excel at capturing latent patterns and exploiting structural properties at scale. Nevertheless, their performance can be significantly degraded by anomalies in KG representations—semantic inconsistencies and modeling artifacts that arise from unconstrained data integration. Such anomalies obscure the intended meaning of relations, introduce noise, and mislead numerical learning models. To address this issue, we introduce a normalization theory for KGs that enforces semantic consistency through normal forms. These forms restructure KGs to eliminate representational anomalies, ensuring that the data adheres to well-defined semantic constraints. We present VANILLA, a neuro-symbolic framework that combines symbolic rule learning, numerical inductive models, and constraint-based validation. By aligning inductive predictions with normalized, ontology-aware KG structures, VANILLA enables accurate and semantically grounded KG completion. Experimental results show that our approach significantly improves predictive performance while maintaining semantic integrity, demonstrating the value of normalization in hybrid KG learning systems. VANILLA is publicly available on GitHub https://github.com/SDM-TIB/VANILLA.}
}
@article{DALZOCHIO2022420,
title = {ELFpm: A machine learning framework for industrial machines prediction of remaining useful life},
journal = {Neurocomputing},
volume = {512},
pages = {420-442},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.083},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222011766},
author = {Jovani Dalzochio and Rafael Kunst and Jorge Luis Victória Barbosa and Henrique Damasceno Vianna and Gabriel {de Oliveira Ramos} and Edison Pignaton and Alecio Binotto and Jose Favilla},
keywords = {Framework, Machine learning, Predictive maintenance},
abstract = {The topic of predictive maintenance has great relevance in the search for the rationalization and efficiency of the industrial plants in the context of Industry 4.0. Monitoring equipment parameters and identifying behavior changes that recognize a future failure allows for anticipation of maintenance while avoiding unnecessary preventive maintenance. There are numerous studies in the literature towards the prediction of the maintenance needs of various types of equipment. However, a piece of equipment may have different behavior depending on the conditions of use or the operating environment. Thus, tools to adjust prediction algorithms to new environments are necessary. This article proposes a framework called ELFpm, designed to predict equipment failures, regardless of location or condition of use. This framework introduces an ontology to store the statistics generated by the prediction algorithms and propose a novel suitability index that selects the appropriate prediction algorithm considering the current status of the equipment. The evaluation of ELFpm considers an engine failure scenario. Results show that ELFpm can predict failures up to three hours before they occur.}
}
@article{PLIATSIOS2023100754,
title = {A systematic review on semantic interoperability in the IoE-enabled smart cities},
journal = {Internet of Things},
volume = {22},
pages = {100754},
year = {2023},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2023.100754},
url = {https://www.sciencedirect.com/science/article/pii/S254266052300077X},
author = {Antonios Pliatsios and Konstantinos Kotis and Christos Goumopoulos},
keywords = {Internet of everything, Smart cities, Semantic interoperability, Semantic web technologies, Systematic literature review, Maturity level},
abstract = {Smart cities have emerged as a result of smart interconnections of people, processes, data, and things, representing an excellent case study of the Internet of Everything (IoE) paradigm. One of the main challenges in realizing the smart city vision is how to provide seamless interoperability between the IoE entities. In this paper we conduct a systematic literature review on the use of semantic technologies to support interoperability between IoE entities in smart cities, with the goal of identifying the main trends and challenges in adopting semantic interoperability solutions for sustainable, green, and resilient smart cities. To this end, we have extracted data from selected primary studies over the last decade that address semantic interoperability issues in smart cities through related technologies and techniques such as ontologies, linked open data, knowledge graphs, ontology alignment/matching methods, and automated reasoning mechanisms. We have analyzed the maturity of this research area by exploring three research questions that focus on: i) the importance of semantic interoperability in the smart city domain; ii) the identification of semantic technologies and tools applied in the smart city domain to promote semantic interoperability; and iii) the identification of smart city application areas where semantic technologies are used to efficiently deliver smart services. The analysis provided research insights, including the introduction of a new evaluation framework that assesses semantic interoperability solutions on four maturity levels. The framework includes specific evaluation criteria for attributes such as modeling, scalability, and availability. Finally, an elaborated list of strengths, opportunities, weaknesses, and threats of semantic interoperability solutions in smart cities is provided, along with a discussion of open challenges and future work in this domain.}
}
@article{ALATRASH202253,
title = {Augmented language model with deep learning adaptation on sentiment analysis for E-learning recommendation},
journal = {Cognitive Systems Research},
volume = {75},
pages = {53-69},
year = {2022},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2022.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041722000304},
author = {Rawaa Alatrash and Rojalina Priyadarshini and Hadi Ezaldeen and Akram Alhinnawi},
keywords = {E-learning adaptation, Recommender system, Sentiment analysis, Convolutional neural network, Natural language processing, Word embeddings},
abstract = {Sentiment Analysis is considered as an important research field in text mining, and is significant in recommendation systems and e-learning environments. This research proposes a new methodology of e-learning hybrid Recommendation System Based on Sentiment Analysis (RSBSA) by leveraging tailored Natural Language Processing (NLP) and Convolutional Neural Network (CNN) techniques, to recommend appropriate e-learning materials based on learner’s preferences. Integration is done on fine-grained sentiment analysis models, to classify text reviews of e-content posted on e-learning platform. Two enhanced language models based on ‘Continuous Bag of Word’ and ‘Skip-Gram’ are introduced. Moreover, three resilient language modelsbased on the hybrid language techniques are developed to produce a superior vocabulary representation. These models were trained using various CNN models to predict ratings of resources from online reviews provided by learners. To accomplish this, a customizable dataset ‘ABHR-1′ is used, which is derived from e-content' reviews with corresponding ratings labeled [1–5]. The proposed models are evaluated and tested using ABHR-1 and two public datasets. According to the simulation results, Multiplication-Several-Channels-CNNmodel outperformed other models with an accuracy of 90.37 % for fine-grained sentiment classification on 5 discrete classes and the empirical results are compared.}
}
@article{HORTA2022100739,
title = {Detecting topic-based communities in social networks: A study in a real software development network},
journal = {Journal of Web Semantics},
volume = {74},
pages = {100739},
year = {2022},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2022.100739},
url = {https://www.sciencedirect.com/science/article/pii/S1570826822000269},
author = {Vitor A.C. Horta and Victor Ströele and Jonice Oliveira and Regina Braga and José Maria N. David and Fernanda Campos},
keywords = {Ontology, Tag enrichment, Social network, Topic-based communities, Software development},
abstract = {In social network analysis, a key issue is the detection of meaningful communities. This problem consists of finding groups of people who are both connected and semantically aligned. In the software development context, identifying communities considering both collaborations between developers and their skills can help to address critical elements or issues in a project. However, a large amount of data and the lack of data structure make it difficult to analyze these networks’ content. In this paper, we propose a framework for detecting overlapping semantic communities and their influential members. We also propose an ontology to extract topics of interest through tag enrichment in a Q&A forum. An evaluation was conducted in a large network of software developers built with Stack Overflow’s data, showing that the proposed framework and ontology can find real communities of developers. The evaluation indicates that their members are semantic aligned and still active in the detected topics of interest, and the quantitative analysis showed that the detected communities have high internal connectivity.}
}
@article{AGGARWAL2025116907,
title = {The Pistoia Alliance’s methods database project: Instrument, chromatographic data system, and vendor-agnostic digital transfer of machine-readable high-performance liquid chromatography-ultraviolet methods using the allotrope data format},
journal = {Journal of Pharmaceutical and Biomedical Analysis},
volume = {263},
pages = {116907},
year = {2025},
issn = {0731-7085},
doi = {https://doi.org/10.1016/j.jpba.2025.116907},
url = {https://www.sciencedirect.com/science/article/pii/S0731708525002481},
author = {Pankaj Aggarwal and Azzedine Dabo and Cheng Sun and Vincent Antonucci and Wolfgang Colsman and Heiko Fessenmayr and Kenneth M. Wells and Juliet McComas and Gerhard Noelken and Birthe V. Nielsen},
keywords = {Chromatographic Data System, HPLC, Allotrope Data Format, Interoperability, Precompetitive collaboration, FAIR},
abstract = {The Pistoia Alliance has successfully completed a pilot on the digital transfer of analytical High-Performance Liquid Chromatography (HPLC) methods and results between chromatography data systems (CDS) via a central data storage system using a standardized machine-readable data format, which transforms methods from paper documents to digital instructions. Critical method and result parameters in two example CDSs have been harmonized with a novel ontology and RDF-based graph data model created in this work. The authors will demonstrate how this new degree of data standardization can simplify data exchange day to day in the laboratory, and describe how the embedded semantics will position scientists to perform on-demand modern queries of data that has been automatically aggregated across vendor solutions. The developed solution was successfully tested at the analytical labs at Merck Sharp & Dohme LLC, a subsidiary of Merck & Co., Inc., Rahway, NJ, USA (MSD) and GSK, Stevenage, UK where there has been an effective transfer of HPLC information between different systems and sites to prove the concept and initial use cases. The work also begins to demonstrate the potential to realize many other use cases such as a series of critical improvements to current method transfer that eliminate the manual keying of data to reduce risk, steps, and error while improving overall flexibility.}
}
@article{ZHANG20211326,
title = {Linking data model and formula to automate KPI calculation for building performance benchmarking},
journal = {Energy Reports},
volume = {7},
pages = {1326-1337},
year = {2021},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2021.02.044},
url = {https://www.sciencedirect.com/science/article/pii/S2352484721001426},
author = {Yun-Yi Zhang and Zhen-Zhong Hu and Jia-Rui Lin and Jian-Ping Zhang},
keywords = {Automation, Building performance, KPI, Linked data, Ontology, Sensor network},
abstract = {Buildings consume a large proportion of global primary energy and building performance management requires massive data inputs. Key Performance Indicator (KPI) is a tool used for comparing different buildings while avoiding problems caused by heterogeneous data sources. However, silos of building and energy consumption data are separate, and the linkages between a KPI formula and different data sets are often non-existent. This paper develops an ontology-based approach for automatically calculating the KPI to support building energy evaluation. The proposed approach integrates building information from BIM and energy and environmental information collected by sensor networks. A KPI ontology is developed to establish a KPI formula, thereby linking static and dynamic data generated in the building operation phase. Each KPI can be defined by inputs, a formula and outputs, and the formula consists of parameters and operators. The parameters can be linked to building data or transformed into a SPARQL query. A case study is investigated based on the proposed approach, and the KPIs for energy and environment are calculated for a real building project. The result shows that this approach relates the KPI formula to the data generated in the building operation phase and can automatically give the result after defining the space and time of interest, thus supporting building performance benchmarking with massive data sets at different levels of details. This research proposes a novel approach to integrating the KPI formula and linked building data from a semantic perspective, and other researchers can use this approach as a foundation for linking data from different sources and computational methods such as formula created for building performance evaluation.}
}
@article{DELHAES2024123652,
title = {Natural language processing for participatory corporate foresight: The participant input analyzer for identifying biases and fallacies},
journal = {Technological Forecasting and Social Change},
volume = {209},
pages = {123652},
year = {2024},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2024.123652},
url = {https://www.sciencedirect.com/science/article/pii/S0040162524004505},
author = {Jörg M. Delhaes and Ana C.L. Vieira and Mónica D. Oliveira},
keywords = {Natural language processing, Participatory methods, Text mining, Biases, Fallacies, Corporate foresight},
abstract = {Corporate Foresight (CF) relies heavily on expert opinions collected through participatory processes, underpinning its results with a wide array of expertise. However, the efficient analysis and processing of comprehensive information shared by participants, which are often influenced by heuristics, biases, and fallacies, remains a challenge. Natural Language Processing (NLP) techniques offer an untapped potential for efficient, objective analysis of textual input, including identifying fallacies and biases. Yet, their application remains limited within foresight literature. This article explores the use of NLP to enhance CF. Specifically, it proposes application of NLP to help facilitators of participatory processes identify biases – e.g., desirability and undesirability – and common fallacies, like the assumptions of ceteris paribus environments and linear trends. We introduce a novel NLP tool, the ‘Participant Input Analyzer’, which by enabling analysis and visualization of sentiment, tense, and topic in textual statements, demonstrates the power of NLP in addressing those challenges. The use of NLP, as exhibited with proposed tool, has the potential to significantly advance the CF field. It can lead to more comprehensive and debiased analyses, contributing to better decision-making for corporates. This paper, therefore, serves as a steppingstone towards more rigorous, data-driven approaches in CF research and practice.}
}
@article{MIRSKI2021100856,
title = {Conventional minds: An interactivist perspective on social cognition and its enculturation},
journal = {New Ideas in Psychology},
volume = {62},
pages = {100856},
year = {2021},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2021.100856},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X21000052},
author = {Robert Mirski and Mark H. Bickhard},
keywords = {Social cognition, Theory of mind, Mindreading, Folk psychology, Enculturation, Interactivism},
abstract = {We argue that the traditional theory of mind models of social cognition face in-principle problems in accounting for enculturation of social cognition, and offer an alternative model advanced within the interactivist framework. In the critical section, we argue that theory of mind accounts’ encodingist model of mental representation renders them unable to account for enculturation. We focus on the three problems: (1) the copy problem and impossibility of internalization; (2) foundationalism and the impossibility of acquisition of culturally specific content; and (3) the frame problems and the inadequacy of mental-state attribution as a way of coordinating social interaction among (encultured) individuals. The positive section begins with a brief sketch of the theoretical basics of interactivism, followed by a more focused presentation of the interactivist model of social cognition, and concludes with a discussion of a number of issues most widely debated in the social cognition literature.}
}
@incollection{DHAMENIYA2025227,
title = {12 - Knowledge graph-based question answering (KG-QA) using natural language processing},
editor = {Rajesh Kumar Dhanaraj and M. Nalini and Malathy Sathyamoorthy and Manar Mohaisen},
booktitle = {Knowledge Graph-Based Methods for Automated Driving},
publisher = {Elsevier},
pages = {227-249},
year = {2025},
isbn = {978-0-443-30040-0},
doi = {https://doi.org/10.1016/B978-0-443-30040-0.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443300400000126},
author = {Priyanshu Dhameniya and Rashmi Yadav},
keywords = {Natural language processing, Knowledge graphsand question answering, Information retrieval, Unstructured, Entity-resolution models, Semantic-parsing, Semantic annotations, Recurrent neural networks, Entity recognition, RDF-based knowledge graphs, SPARQL},
abstract = {This chapter begins by providing an overview of knowledge graphs, their construction, and their role in representing relationships between entities. It then delves into the challenges posed by question-answering tasks and how knowledge graphs can enhance traditional QA systems by offering a structured and context-aware information base. The core of this chapter focuses on the integration of NLP techniques in KG-QA systems. It discusses the processing of natural language queries, entity recognition, and relationship extraction to bridge the gap between user queries and the structured information encapsulated in knowledge graphs. Special attention is given to semantic parsing and understanding, enabling the extraction of nuanced information from unstructured text.}
}
@article{TRAPPEY2021120511,
title = {An intelligent patent recommender adopting machine learning approach for natural language processing: A case study for smart machinery technology mining},
journal = {Technological Forecasting and Social Change},
volume = {164},
pages = {120511},
year = {2021},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120511},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520313378},
author = {Amy Trappey and Charles V. Trappey and Alex Hsieh},
keywords = {Natural language processing, Patent recommendation, Word embedding, Technology mining and trend analysis},
abstract = {Recommendation systems are widely applied in many fields, such as online customized product searches and customer-centric advertisements. This research develops the methodology for a patent recommender to discover semantically relevant patents for further technology mining and trend analysis. The proposed recommender adopts machine learning (ML) algorithms for natural language processing (NLP) to represent patent documents in vector space and to enable semantic analyses of the patent documents. The ML approach of neural network (NN) language models, trained by domain patent documents (text) as a training set, convert patent documents into vectors and, thus, can identify semantically similar patents using document similarity measures. In particular, the proposed recommender is deployed to in-depth case studies for advanced patent recommendations. The case domain of smart machinery is used to better enable smart manufacturing by incorporating innovative technologies, such as intelligent sensors, intelligent controllers, and intelligent decision making. The research uses six sub-domains in smart machinery technologies as the case studies to verify the superior accuracy and efficacy of the recommender system and methodologies.}
}
@article{LI2025107025,
title = {HAZOPCT: A HAZOP analysis completeness tool based on knowledge graph reasoning},
journal = {Process Safety and Environmental Protection},
volume = {197},
pages = {107025},
year = {2025},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2025.107025},
url = {https://www.sciencedirect.com/science/article/pii/S0957582025002927},
author = {Zhiyuan Li and Jinsong Zhao},
keywords = {Hazard and operability analysis, Knowledge graph, Graph machine learning, Process safety risk assessment, Knowledge reasoning},
abstract = {Hazard and Operability (HAZOP) analysis is widely recognized as a cornerstone of risk assessment in the chemical industry. However, the reliance on manual processes makes HAZOP analysis time-consuming, labor-intensive, and subject to variability, leading to inconsistent report quality and increased accident risks. To address these challenges, this paper introduces a HAZOP analysis Completeness Tool (HAZOPCT), which helps to review process designs for enhancing the completeness of HAZOP analysis. Initially, a HAZOP knowledge graph is constructed automatically to integrate multi-source process safety information based on a predefined ontology. Subsequently, HAZOPCT is employed to perform reasoning over the knowledge graph, thereby generating recommended conclusions from similar cases for completing HAZOP analysis. A case study focusing on the sulfur recovery process demonstrates the ability of HAZOPCT to efficiently identify hazards and operability issues that may have been overlooked in original HAZOP analysis tables. Additionally, ablation studies validate the critical role of semantic information and graph attention mechanisms in improving reasoning performance. The generated attention weight network further enhances the interpretability of HAZOPCT, offering insights into the automated reasoning process and contributing to a more transparent and reliable HAZOP analysis.}
}
@article{NHI2022,
title = {A Model of Semantic-Based Image Retrieval Using C-Tree and Neighbor Graph},
journal = {International Journal on Semantic Web and Information Systems},
volume = {18},
number = {1},
year = {2022},
issn = {1552-6283},
doi = {https://doi.org/10.4018/IJSWIS.295551},
url = {https://www.sciencedirect.com/science/article/pii/S1552628322000035},
author = {Nguyen Thi Uyen Nhi and Thanh Manh Le},
keywords = {C-Tree, Graph-CTree, Image Retrieval, k-NN, Ontology, SBIR_grCT, Semantic-Based Image Retrieval, Similar Images, SPARQL},
abstract = {ABSTRACT
The problems of image mining and semantic image retrieval play an important role in many areas of life. In this paper, a semantic-based image retrieval system is proposed that relies on the combination of C-Tree, which was built in the authors’ previous work, and a neighbor graph (called Graph-CTree) to improve accuracy. The k-Nearest Neighbor (k-NN) algorithm is used to classify a set of similar images that are retrieved on Graph-CTree to create a set of visual words. An ontology framework for images is created semi-automatically. SPARQL query is automatically generated from visual words and retrieve on ontology for semantics image. The experiment was performed on image datasets, such as COREL, WANG, ImageCLEF, and Stanford Dogs, with precision values of 0.888473, 0.766473, 0.839814, and 0.826416, respectively. These results are compared with related works on the same image dataset, showing the effectiveness of the methods proposed here.}
}
@article{ADDEPALLI2023508,
title = {Designing a semantic based common taxonomy of mechanical component degradation to enable maintenance digitalisation},
journal = {Procedia CIRP},
volume = {119},
pages = {508-513},
year = {2023},
note = {The 33rd CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S221282712300478X},
author = {Sri Addepalli and Bernadin Namoano and Oluseyi Ayodeji Oyedeji and Maryam Farsi and John Ahmet Erkoyuncu},
keywords = {Knowledge management, decision making, degradation analysis},
abstract = {Digital data management and enterprise systems have become key to support the digitalisation of maintenance activities. With traditional maintenance activities still striving for efficiencies, platforms such as the natural language processing (NLP) are supporting industries to mine textural data, not just extracting degradation terminologies but providing the maintainer with holistic insights on the degradation process. Traditionally, the degradation analysis, the first step in maintenance, is a manual process for defect characterisation, followed by failure investigation and a remaining useful life estimation. To enable digitalisation, transfer of human cognitive decision making from the physical world to the digital world is key. This paper enables this cognitive knowledge transfer through the design of a common degradation taxonomy and extracting terminology relationships to produce degradation causality with an NLP knowledge extraction approach. Further, this paper proposes and demonstrates a framework to present the data in the form of a knowledge graph populated using an application-level ontology. Use cases in the aerospace context have been used to show the power of the NLP and conceptual journey into the digitalisation of maintenance.}
}
@article{SREEJA2021107161,
title = {A unified model for egocentric video summarization: an instance-based approach},
journal = {Computers & Electrical Engineering},
volume = {92},
pages = {107161},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107161},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621001634},
author = {M.U. Sreeja and Binsu C. Kovoor},
keywords = {Egocentric videos, Ontology, Multi-video summarization, Single video summarization, Object detection},
abstract = {Video summarization generates compact representations of videos in the form of summaries. The proposed framework is a unified model for instance-driven egocentric video summarization addressing generic and query-based summarization along with multi-video summarization. The model employs deep learning for object detection and semantic web technologies in the form of ontologies for query inferences. Combining user preferences in the form of object queries has aided in producing summaries that are subjective in nature. Quantitative evaluations performed on two novel datasets namely, ‘vehicle expo’ and ‘academic inspection’ prove that the proposed framework produces remarkable results with the employment of instance-driven modules for summarization. Additional experimental analysis for shot boundary detection have been conducted based on proposed method and conventional methods establishing the significance of the instance-based model. Moreover, qualitative evaluations further ensure that the summaries are concise, representative, diverse and semantically relevant further substantiating the need for instance-driven models in video summarization.}
}
@article{HAMDAQA2022106762,
title = {iContractML 2.0: A domain-specific language for modeling and deploying smart contracts onto multiple blockchain platforms},
journal = {Information and Software Technology},
volume = {144},
pages = {106762},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106762},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921002081},
author = {Mohammad Hamdaqa and Lucas Alberto Pineda Met and Ilham Qasse},
keywords = {Smart contracts, Blockchain, Model-driven engineering, Ethereum, Hyperledger fabric, DAML, Azure},
abstract = {Context:
Smart contracts play a vital role in many fields. Despite being called smart, the development of smart contracts is a tedious task beyond defining a set of contractual rules. In addition to business knowledge, coding a smart contract requires strong technical knowledge in a multiplex of new and rapidly changing domain-specific languages and blockchain platforms.
Objectives:
The goal of this paper is to assist developers in building smart contracts independently from the language or the target blockchain platform. In which, we present our second-generation smart contract language iContractML 2.0.
Methods:
We follow a feature-oriented approach to analyze three different blockchain platforms and propose an enhanced reference model and a modeling framework for smart contracts (iContractML 2.0). Then, we evaluate the coverage and extensibility of iContractML 2.0, first through mapping the concepts of the reference models to the constructs within each of the platforms used in devising the reference model, and second through mapping its concepts to a new smart contract language not previously considered. Finally, we demonstrate the capabilities of iContractML 2.0 using five case studies from different business domains.
Results:
iContractML 2.0 extends our first generation language to support DAML, which is another standardized language for smart contracts. This makes iContractML 2.0 supports the platforms that DAML support by extension. Moreover, iContractML 2.0 supports generating the structural and deployment artifacts in addition to the smart contract behavior by implementing templates for some of the common functions. The results of evaluating the generality of the iContractML 2.0 reference model show that it is 91.7% lucid and 72.2% laconic. Moreover, the reference model is able to capture all the elements of the new language with 83.3% of the components which have a direct one-to-one mapping.
Conclusion:
iContractML 2.0 is an extensible framework that empowers developers to model and generate functional smart contract code that can be deployed onto multiple blockchain platforms.}
}
@article{FAN2025,
title = {Genome-wide transcriptional analysis of tendon tissue-related genes and pathways in the torn rotator cuff of diabetes patient},
journal = {Journal of Shoulder and Elbow Surgery},
year = {2025},
issn = {1058-2746},
doi = {https://doi.org/10.1016/j.jse.2025.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1058274625006111},
author = {Ning Fan and Aobo Wang and Tianyi Wang and Shuo Yuan and Peng Du and Lei Zang},
keywords = {Rotator cuff tear, Diabetes mellitus, RNA sequencing, Bioinformatics analysis,  Basic Science Study, Microbiology},
abstract = {Background
Diabetes is a major factor affecting rotator cuff tear (RCT) progression and healing. However, the underlying molecular mechanisms linking diabetes to RCT remain unclear. We aimed to investigate these mechanisms for developing targeted therapeutic strategies to improve tendon healing in diabetes patients.
Methods
Microarray analysis was used to profile differentially expressed mRNAs in RCT samples between three diabetes and three nondiabetes patients. Subsequently, Gene Ontology (GO) database, Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway, and interaction networks for the differentially expressed genes (DEGs) were analyzed using Search Tool for the Retrieval of Interacting Genes (STRING), Cytoscape, and packages of the R computing language. All the hub genes were verified by quantitative real-time PCR (qPCR).
Results
In total, 660 DEGs were identified, including 337 upregulated genes and 323 downregulated genes. GO annotation analysis revealed that these DEGs were mainly associated with negative regulation of growth and cellular response to zinc ion in the biological process, heparin binding and cysteine-type endopeptidase activity in the molecular function, and extracellular space and plasma membrane in the cellular component. KEGG pathway analysis showed that these DEGs were mainly involved in the Staphylococcus aureus infection, mineral absorption, phagosome, asthma and influenza A. The interaction network analysis indicated that ten hub genes, including MT1H, MT1G, MT1X, MT1M, LOX, P4HA1, EGLN3, FMOD, SLC2A1, and COMP, which are functionally involved in oxidative stress response and extracellular matrix organization. The qPCR data verified that MT1H, MT1G, MT1X, MT1M, LOX, EGLN3, FMOD, and COMP were consistent with the microarray results.
Conclusions
This study preliminarily identified different gene expression patterns between diabetes and nondiabetes patients with RCT. Bioinformatic analyses suggested several altered molecular processes, including oxidative stress response and extracellular matrix organization, may contribute to tendon degeneration and impaired healing in diabetes patients. However, further studies are required to provide direct biological evidence for these associations.}
}
@article{LOVELL2025103510,
title = {Labour will progress where māmā feel safe: Constructing birth-place decision making in primary birth centres},
journal = {Health & Place},
volume = {95},
pages = {103510},
year = {2025},
issn = {1353-8292},
doi = {https://doi.org/10.1016/j.healthplace.2025.103510},
url = {https://www.sciencedirect.com/science/article/pii/S1353829225001005},
author = {Sarah A. Lovell and Christina Ergler and Mary Kensington},
keywords = {Primary birth, Midwifery, Reproductive health, Birthplace decision-making},
abstract = {High rates of intervention in birth is a significant health issue. Primary birth centres are midwife-led sites for care with lower rates of intervention in birth than hospitals. Yet hospital births dominate birthplace decision-making in New Zealand. In-depth interviews with 24 health workers associated with four primary birth centres aim to identify how confidence in a primary centre birth is built. Thematic analysis demonstrates how midwives discursively and visually re-centred birth as a normal physiological process challenging hospital as the taken-for-granted place for care. We conclude that midwives’ neurohormonal understandings of birth builds responsiveness to the birth-place ontologies of clients.}
}
@article{CHEN2022105908,
title = {Construction and application of COVID-19 infectors activity information knowledge graph},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105908},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105908},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522006515},
author = {Liming Chen and Dong Liu and Junkai Yang and Mingyue Jiang and Shouqiang Liu and Yang Wang},
keywords = {COVID-19, NER, Knowledge graph, Application of knowledge graph, Knowledge reasoning},
abstract = {During COVID-19 prevention and control, people need to be aware of the outbreak situation in their area to avoid being inconvenienced by the outbreak and even becoming infected. Thus, this project constructs a knowledge graph with COVID-19 infector activity information, by using the official flow information of the infected people from the provincial and municipal websites. This knowledge graph is the basis of the COVID-19 applications for tracing, visualization and reporting proposes. In the implementation process, we (1) collect a dataset with the information on COVID-19 cases from the prevention and control centers, (2) extract the entity elements with a Bert + BILSTM + CRF-based model, and (3) pre-process the dataset and construct a knowledge graph with manual annotation and human-based review. Finally, we use the knowledge graph to develop a web-based application to implement the question and answer, query, transmission path tracking and the "No.0" tracing infector functions.}
}
@article{GRAPIN2024101334,
title = {Thorny issues with academic language: A perspective from scientific practice},
journal = {Linguistics and Education},
volume = {83},
pages = {101334},
year = {2024},
issn = {0898-5898},
doi = {https://doi.org/10.1016/j.linged.2024.101334},
url = {https://www.sciencedirect.com/science/article/pii/S0898589824000676},
author = {Scott E. Grapin and Lorena Llosa},
keywords = {Academic language, English learners, Disciplinary practices, STEM education, Equity, Translanguaging},
abstract = {A debate over the construct of academic language (AL) has engendered significant polarization in the field of language education. The issues at the heart of the AL debate are thorny and persistently elusive to resolve. Yet they are not unpredictable, particularly when viewed from the perspective of how scientific communities advance knowledge. In this article, we highlight two thorny issues with AL from the perspective of scientific practice: (a) the modeling issue and (b) the paradigm issue. For each issue, we discuss methodological and pedagogical implications using examples from research on language use in STEM education. Further, we analyze how the two thorny issues have manifested in other contentious debates in language education (cognitive-social debate, translanguaging vs. codeswitching). Finally, we propose ways forward in light of the thorny issues toward advancing our collective knowledge as a community of researchers and practitioners committed to the education of language minoritized students.}
}
@article{AMMI2022,
title = {Taxonomical Challenges for Cyber Incident Response Threat Intelligence:},
journal = {International Journal of Cloud Applications and Computing},
volume = {12},
number = {1},
year = {2022},
issn = {2156-1834},
doi = {https://doi.org/10.4018/IJCAC.300770},
url = {https://www.sciencedirect.com/science/article/pii/S2156183422000493},
author = {Meryem Ammi and Oluwasegun Adedugbe and Fahad Mohamed Alharby and Elhadj Benkhelifa},
keywords = {CTI Standards, CTI Taxonomies, Cyber Incident Response, Cyber Security, Cyber Threat Intelligence},
abstract = {ABSTRACT
As attackers continue to devise new means of exploiting vulnerabilities in computer systems, security personnel are doing their best to identify loopholes and threats. Analysis of threats to come up with effective mitigation techniques requires all-encompassing information about them. Security analysts can represent and share cyber threat information with semantic knowledge graphs within cyber security space to access. However, there should be no conflicting information because the response to threats must be immediate. This calls for a standardized taxonomy that is generally accepted within the cybersecurity space to represent information, ultimately making cyber threat intelligence (CTI) credible. This review looks into existing CTI-based ontologies, taxonomies, and knowledge graphs. The absence of standardized taxonomy identified could be responsible for limited taxonomy encoding and integration among existing CTI-based ontologies, as well as missing interconnections between taxonomies and existing ontologies. Hence, the development of a standardized taxonomy will enhance CTI effectiveness.}
}
@article{DERTADIAN2024104425,
title = {Towards a social harm approach in drug policy},
journal = {International Journal of Drug Policy},
volume = {127},
pages = {104425},
year = {2024},
issn = {0955-3959},
doi = {https://doi.org/10.1016/j.drugpo.2024.104425},
url = {https://www.sciencedirect.com/science/article/pii/S0955395924001105},
author = {George Christopher Dertadian and Rebecca Askew},
keywords = {Social harm, Zemiology, Crime, Drug policy, Ontology},
abstract = {In this paper, we explore how the social harm approach can be adapted within drug policy scholarship. Since the mid-2000s, a group of critical criminologists have moved beyond the concept of crime and criminology, towards the study of social harm. This turn proceeds decades of research that highlights the inequities within the criminal legal system, the formation of laws that protect the privileged and punish the disadvantaged, and the systemic challenge of the effectiveness of retribution and punishment at addressing harm in the community. The purpose of this paper is to first identify parallels between the social harm approach and critical drug scholarship, and second to advocate for the adoption of a social harm lens in drug policy scholarship. In the paper, we draw out the similarities between social harm and drug policy literatures, as well as outline what the study of social harm can bring to an analysis of drug policy. This includes a discussion on the ontology of drug crime, the myth of drug crime and the ineffective use of the crime control system in response to drug use. The paper then discusses how these conversations in critical criminology and critical drugs scholarship can be brought together to inform future drug policy research. This reflection details the link between social harm and the impingement of human flourishing, explores the role of decolonizing drug policy, advocates for the centralization of lived experience within the research process and outlines how this might align with harm reduction approaches. We conclude by arguing that the social harm approach challenges the idea that neutrality is the goal in drug policy and explicitly seeks to expand new avenues in activist research and social justice approaches to policymaking.}
}
@article{CONDE2020102232,
title = {How can wikipedia be used to support the process of automatically building multilingual domain modules? a case study.},
journal = {Information Processing & Management},
volume = {57},
number = {4},
pages = {102232},
year = {2020},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2020.102232},
url = {https://www.sciencedirect.com/science/article/pii/S0306457319313974},
author = {Angel Conde and Ana Arruarte and Mikel Larrañaga and Jon A. Elorriaga},
keywords = {Educational ontology, Technology supported learning systems, Wikipedia, domain module, machine learning, Multilingualism},
abstract = {The goal of the research here presented is to identify which aspects of Wikipedia can be exploited to support the process of automatically building Multilingual Domain Modules from textbooks. First, we have defined a representation formalism for Multilingual Domain Modules that is essential for Technology Supported Learning Systems which aim to serve a globalized society. To our knowledge, no attempt has been made at achieving domain models that consider multiple languages. Our approach combines Multilingual Educational Ontologies with Learning Objects in different languages. Wikipedia is a valuable resource to accomplish this purpose. In this scenario, we have developed LiDom Builder, a framework that uses Wikipedia as an additional knowledge base for the automatic generation of Multilingual Domain Modules from textbooks. The framework includes domain-independent term extraction methods to identify which topics of Wikipedia are related to the domain to be learnt and, also, extracts their equivalents in other languages. In order to complete the Educational Ontology, we have defined a method to extract pedagogical relationships from Wikipedia and other general-purpose knowledge bases. From this task, we highlight the extraction of relationship that will allow the sequencing of the topics in Technology Supported Learning Systems. In addition, LiDom Builder takes advantage of the structured contents of Wikipedia to identify text fragments that can be used for educational purposes, classifies them and generates their corresponding Learning Objects. The interlanguage links between topics of Wikipedia are used to create Learning Objects in other languages.}
}
@article{ANDREASEN2020101848,
title = {Natural logic knowledge bases and their graph form},
journal = {Data & Knowledge Engineering},
volume = {129},
pages = {101848},
year = {2020},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2020.101848},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X18306165},
author = {Troels Andreasen and Henrik Bulskov and Per Anker Jensen and Jørgen Fischer Nilsson},
keywords = {Natural Logic, Knowledge management applications, Ontologies, Query, Metalogic, Bioinformatics databases},
abstract = {This paper describes how knowledge bases can be represented in and reasoned with in natural logic. Natural logic is a regimented fragment of natural language possessing a well-defined logical semantics. As such, natural logic may be considered an attractive alternative among the various knowledge representation logics such as description logics. Our version of natural logic expands formal ontologies with affirmative propositions expressing a variety of relationships between concepts. It comprises (nested) restrictive relative clauses and prepositional phrases and, as a new construct, adverbial prepositional phrases. The natural logic knowledge base is to be used for deductive query answering applying inference rules. This is facilitated by introduction of Datalog as an embedding meta-logic. The inference rules are stated in Datalog and act directly on the natural logic formulations. The knowledge base propositions are decomposed into a graph form enabling path finding between concepts. The examples in the paper are derived from text source life-science descriptions.}
}
@article{ELARAB2025104058,
title = {The role of AI in emergency department triage: An integrative systematic review},
journal = {Intensive and Critical Care Nursing},
volume = {89},
pages = {104058},
year = {2025},
issn = {0964-3397},
doi = {https://doi.org/10.1016/j.iccn.2025.104058},
url = {https://www.sciencedirect.com/science/article/pii/S0964339725001193},
author = {Rabie Adel {El Arab} and Omayma Abdulaziz {Al Moosa}},
keywords = {Emergency department, Triage, Artificial intelligence, Machine learning, Risk stratification, Overcrowding, Predictive modeling, Natural language processing, Healthcare outcomes, Clinical decision support},
abstract = {Background
Overcrowding in emergency departments (EDs) leads to delayed treatments, poor patient outcomes, and increased staff workloads. Artificial intelligence (AI) and machine learning (ML) have emerged as promising tools to optimize triage. Objective: This systematic review evaluates AI/ML-driven triage and risk stratification models in EDs, focusing on predictive performance, key predictors, clinical and operational outcomes, and implementation challenges.
Methods
Following PRISMA 2020 guidelines, we systematically searched PubMed, CINAHL, Scopus, Web of Science, and IEEE Xplore for studies on AI/ML-driven ED triage published through January 2025. Two independent reviewers screened studies, extracted data, and assessed quality using PROBAST, with findings synthesized thematically.
Results
Twenty-six studies met inclusion criteria. ML-based triage models consistently outperformed traditional tools, often achieving AUCs > 0.80 for high acuity outcomes (e.g., hospital admission, ICU transfer). Key predictors included vital signs, age, arrival mode, and disease-specific markers. Incorporating free-text data via natural language processing enhances accuracy and sensitivity. Advanced ML techniques, such as gradient boosting and random forests, generally surpassed simpler models across diverse populations. Reported benefits included reduced ED overcrowding, improved resource allocation, fewer mis-triaged patients, and potential patient outcome improvements.
Conclusion
AI/ML-based triage models hold substantial promise in improving ED efficiency and patient outcomes. Prospective, multi-center trials with transparent reporting and seamless electronic health record integration are essential to confirm these benefits.
Implications for Clinical Practice
Integrating AI and ML into ED triage can enhance assessment accuracy and resource allocation. Early identification of high-risk patients supports better clinical decision-making, including critical care and ICU nurses, by streamlining patient transitions and reducing overcrowding. Explainable AI models foster trust and enable informed decisions under pressure. To realize these benefits, healthcare organizations must invest in robust infrastructure, provide comprehensive training for all clinical staff, and implement ethical, standardized practices that support interdisciplinary collaboration between ED and ICU teams.}
}
@article{DINKU2025117547,
title = {The connection between social and emotional wellbeing and Indigenous language use varies across language ecologies in Australia},
journal = {Social Science & Medicine},
volume = {364},
pages = {117547},
year = {2025},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2024.117547},
url = {https://www.sciencedirect.com/science/article/pii/S0277953624010013},
author = {Yonatan Dinku and Francis Markham and Denise Angelo and Jane Simpson},
keywords = {Aboriginal and Torres Strait Islander Peoples, Social and emotional wellbeing, Indigenous language use, Language ecology, Australia},
abstract = {This research examines relationships between social and emotional wellbeing in various language ecology contexts. Previous studies have shown a correlation between speaking an Indigenous language and improved social and emotional wellbeing among Aboriginal and Torres Strait Islander peoples within the population nationally. This study considers the rich variety of contemporary Indigenous language contexts and the extent to which traditional languages, new contact languages and English are spoken. It adopts the concept of ‘language ecologies’ — the different configurations of languages spoken in a location — to investigate how the relationship between Indigenous language use (traditional or new), and social and emotional wellbeing varies by ecology type. We classify the country geographically into five language ecology types, and use regression analysis to investigate associations between Indigenous language use and social and emotional wellbeing by language ecology type. We find heterogenous associations across different language ecologies Speaking an Indigenous language is associated with lower than average levels of wellbeing in areas where English is frequently spoken as a first language, while it is associated with greater than average wellbeing in other areas. Associations between wellbeing and speaking an Indigenous language are relatively larger in areas where traditional Indigenous languages are frequently spoken as a first language than in other areas. The findings suggest that the extent and type of wellbeing benefits from speaking an Indigenous language are dependent on the type of languages in individuals' language repertoires (person-based) and the language contexts where they live (place-based language ecologies).}
}
@article{ABBAS2024120441,
title = {A socio-technical approach to trustworthy semantic biomedical content generation and sharing},
journal = {Information Sciences},
volume = {666},
pages = {120441},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120441},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524003542},
author = {Asim Abbas and Tahir Hameed and Fazel Keshtkar and Seifedine Kadry and Syed Ahmad Chan Bukhari},
keywords = {Biomedical content, Semantic annotations, Annotation recommendation, Knowledge sharing, Socio-technical content generation, Trust},
abstract = {The rapid growth of online biomedical content has presented a notable challenge in delivering timely and precise semantic annotations. Semantic annotations play a crucial role in contextually indexing data, thereby enhancing search accuracy. This intricate process involves the utilization of multiple coded ontologies, requiring extensive technical expertise and domain knowledge. While automated ontologies face limitations in balancing accuracy and speed, expert knowledge generation is also scarce and expensive. In response to these conflicting challenges, we propose a socio-technical content generation and sharing approach named ‘Semantically,’ which actively involves biomedical experts and scientists. Additionally, ‘Semantically’ leverages schema.org for incorporating additional semantic tags to enhance search engine performance. The outcome is high-quality, machine-understandable content that not only facilitates fast and accurate searches but also instills trust due to the collaborative nature of the annotation process. ‘Semantically’ generated biomedical content was evaluated in two scenarios: (1) search based solely on initial-level annotations and (2) search incorporating additional expert-recommended annotations. ‘Semantically’ enhances the user search experience when compared to benchmark data. In the first scenario, the unigram to 5-grams strategy efficiently recognizes biomedical terms, resulting in high precision, recall, F1, and accuracy scores, all around 0.9. ‘Semantically’ code is openly accessible at https://github.com/bukharilab/Semantically.}
}
@article{KOLONIN2022180,
title = {Cognitive Architecture for Decision-Making Based on Brain Principles Programming},
journal = {Procedia Computer Science},
volume = {213},
pages = {180-189},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.054},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017458},
author = {Anton Kolonin and Andrey Kurpatov and Artem Molchanov and Gennadiy Averyanov},
keywords = {brain principles programming, cognitive architecture, formal concept analysis, functional system theory, probabilistic logic, subject domain ontology, task-driven approach},
abstract = {We describe a cognitive architecture intended to solve a wide range of problems based on the five identified principles of brain activity, with their implementation in three subsystems: logical-probabilistic inference, probabilistic formal concepts, and functional systems theory. Building an architecture involves the implementation of a task-driven approach that allows defining the target functions of applied applications as tasks formulated in terms of the operating environment corresponding to the task, expressed in the applied ontology. We provide a basic ontology for a number of practical applications as well as for the subject domain ontologies based upon it, describe the proposed architecture, and give possible examples of the execution of these applications in this architecture.}
}
@article{HOLTER2025102284,
title = {CellRex: Software platform for managing biological cell data},
journal = {SoftwareX},
volume = {31},
pages = {102284},
year = {2025},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2025.102284},
url = {https://www.sciencedirect.com/science/article/pii/S2352711025002511},
author = {Jan Hölter and Tim Rickmeyer and Christiane Thielemann},
keywords = {Research data management (RDM), Laboratory information management system (LIMS), Biological cell, Microscopic images, Electrophysiological recordings, Software-as-a-service (SaaS)},
abstract = {This work introduces the software platform CellRex, a research data management system for laboratories capable of storing, searching, and enriching data with biological metadata. CellRex addresses data management challenges by storing data in an ontology-based directory structure within the filesystem, with metadata saved as JSON files and in a document-oriented SQLite database. The framework, deployed as container services in a software-as-a-service model, features a web-based GUI and API for user interaction and machine-readable access, providing functionalities such as duplicate detection, experiment grouping, and templating. CellRex improves research efficiency and facilitates data reuse, providing a targeted solution for laboratories focused on cell analysis research.}
}
@article{SAHOO201910,
title = {ProvCaRe: Characterizing scientific reproducibility of biomedical research studies using semantic provenance metadata},
journal = {International Journal of Medical Informatics},
volume = {121},
pages = {10-18},
year = {2019},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618302697},
author = {Satya S. Sahoo and Joshua Valdez and Matthew Kim and Michael Rueschman and Susan Redline},
keywords = {Scientific reproducibility, Provenance metadata, W3C PROV specifications, ProvCaRe ontology, S3 model, Provenance-based ranking, ProvCaRe knowledge repository},
abstract = {Objective
Reproducibility of research studies is key to advancing biomedical science by building on sound results and reducing inconsistencies between published results and study data. We propose that the available data from research studies combined with provenance metadata provide a framework for evaluating scientific reproducibility. We developed the ProvCaRe platform to model, extract, and query semantic provenance information from 435, 248 published articles.
Methods
The ProvCaRe platform consists of: (1) the S3 model and a formal ontology; (2) a provenance-focused text processing workflow to generate provenance triples consisting of subject, predicate, and object using metadata extracted from articles; and (3) the ProvCaRe knowledge repository that supports “provenance-aware” hypothesis-driven search queries. A new provenance-based ranking algorithm is used to rank the articles in the search query results.
Results
The ProvCaRe knowledge repository contains 48.9 million provenance triples. Seven research hypotheses were used as search queries for evaluation and the resulting provenance triples were analyzed using five categories of provenance terms. The highest number of terms (34%) described provenance related to population cohort followed by 29% of terms describing statistical data analysis methods, and only 5% of the terms described the measurement instruments used in a study. In addition, the analysis showed that some articles included a higher number of provenance terms across multiple provenance categories suggesting a higher potential for reproducibility of these research studies.
Conclusion
The ProvCaRe knowledge repository (https://provcare.case.edu/) is one of the largest provenance resources for biomedical research studies that combines intuitive search functionality with a new provenance-based ranking feature to list articles related to a search query.}
}
@article{GHOUL2024100100,
title = {A combined AraBERT and Voting Ensemble classifier model for Arabic sentiment analysis},
journal = {Natural Language Processing Journal},
volume = {8},
pages = {100100},
year = {2024},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100100},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000487},
author = {Dhaou Ghoul and Jérémy Patrix and Gaël Lejeune and Jérôme Verny},
keywords = {Arabic language, Sentiment analysis, Ensemble learning, Machine learning, AraBERT},
abstract = {For sentiment analysis of short texts (e.g. movie reviews, tweets, etc.), one approach is to build machine learning models that can determine their tones (positive, negative, neutral). However, these natural language processing (NLP) studies are missing when there is a lack of high-quality and large-scale training data for specific languages such as Arabic. In this paper, we present three machine learning models designed to classify sentiment Arabic tweets developed for a Kaggle competition. We present a Voting Ensemble classifier taking advantage of both character-level and word-level features. We also propose an AraBERT (Arabic Bidirectional Encoder Representations from Transformers) model with preprocessing using Farasa Segmenter. Finally, we combine these first two approaches as a third approach (Voting Ensemble classifier using AraBERT embeddings). Performance measures of results show improvement over previous efforts for all models. The third model exhibits strong performance with a 73.98% F-score score. The work presented here could be useful for future studies and for new Arabic sentiment analysis online services or competitions.}
}
@article{GISSLANDER2024112,
title = {Data quality and patient characteristics in European ANCA-associated vasculitis registries: data retrieval by federated querying},
journal = {Annals of the Rheumatic Diseases},
volume = {83},
number = {1},
pages = {112-120},
year = {2024},
issn = {0003-4967},
doi = {https://doi.org/10.1136/ard-2023-224571},
url = {https://www.sciencedirect.com/science/article/pii/S0003496724003959},
author = {Karl Gisslander and Matthew Rutherford and Louis Aslett and Neil Basu and François Dradin and Lucy Hederman and Zdenka Hruskova and Hicham Kardaoui and Peter Lamprecht and Sabina Lichołai and Jacek Musial and Declan O'Sullivan and Xavier Puechal and Jennifer Scott and Mårten Segelmark and Richard Straka and Benjamin Terrier and Vladimir Tesar and Michelangelo Tesi and Augusto Vaglio and Dagmar Wandrei and Arthur White and Krzysztof Wójcik and Beyza Yaman and Mark A Little and Aladdin J Mohammad},
keywords = {systemic vasculitis, epidemiology, granulomatosis with polyangiitis, quality indicators, health care},
abstract = {Objectives
This study aims to describe the data structure and harmonisation process, explore data quality and define characteristics, treatment, and outcomes of patients across six federated antineutrophil cytoplasmic antibody-associated vasculitis (AAV) registries.
Methods
Through creation of the vasculitis-specific Findable, Accessible, Interoperable, Reusable, VASCulitis ontology, we harmonised the registries and enabled semantic interoperability. We assessed data quality across the domains of uniqueness, consistency, completeness and correctness. Aggregated data were retrieved using the semantic query language SPARQL Protocol and Resource Description Framework Query Language (SPARQL) and outcome rates were assessed through random effects meta-analysis.
Results
A total of 5282 cases of AAV were identified. Uniqueness and data-type consistency were 100% across all assessed variables. Completeness and correctness varied from 49%–100% to 60%–100%, respectively. There were 2754 (52.1%) cases classified as granulomatosis with polyangiitis (GPA), 1580 (29.9%) as microscopic polyangiitis and 937 (17.7%) as eosinophilic GPA. The pattern of organ involvement included: lung in 3281 (65.1%), ear-nose-throat in 2860 (56.7%) and kidney in 2534 (50.2%). Intravenous cyclophosphamide was used as remission induction therapy in 982 (50.7%), rituximab in 505 (17.7%) and pulsed intravenous glucocorticoid use was highly variable (11%–91%). Overall mortality and incidence rates of end-stage kidney disease were 28.8 (95% CI 19.7 to 42.2) and 24.8 (95% CI 19.7 to 31.1) per 1000 patient-years, respectively.
Conclusions
In the largest reported AAV cohort-study, we federated patient registries using semantic web technologies and highlighted concerns about data quality. The comparison of patient characteristics, treatment and outcomes was hampered by heterogeneous recruitment settings.}
}
@article{MASSET2024104869,
title = {Videography in tourism research: An analytical review},
journal = {Tourism Management},
volume = {102},
pages = {104869},
year = {2024},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2023.104869},
url = {https://www.sciencedirect.com/science/article/pii/S0261517723001516},
author = {Julie Masset and Alain Decrop and Isabelle Frochot},
keywords = {Videography, Tourism, Visual research},
abstract = {Videography is a research approach that has been used extensively in consumer research and anthropology, yet has been considered less often in tourism research. This is surprising as tourism is inextricably linked to visuals. Many tourism features also fully justify the use of films to disseminate and share research findings within the tourism community, as the variety of contexts and their multisensorial dimensions can be best represented audio-visually. This paper reviews the existing work on videography, including its epistemology, ontology, and axiology. We then provide an analytical review of 28 films connected to tourism and travel themes. We describe and analyze them using five criteria: topicality, technicality (in collecting and in editing material), theoreticality, and theatricality. We conclude with contributions related to ontology, epistemology, and axiology and recommendations on how, when and why to use this innovative research format.}
}